{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockPoolSlice.java",
  "functionName": "checkDirs",
  "functionId": "checkDirs",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
  "functionStartLine": 424,
  "functionEndLine": 428,
  "numCommitsSeen": 91,
  "timeTaken": 6833,
  "changeHistory": [
    "ec183faadcf7edaf432aca3b25d24215d505c2ec",
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16",
    "bc13dfb1426944ce45293cb8f444239a7406762c",
    "1177d4edc29f839b8df1038c4fbf37f56f56a2a0",
    "9e31bf675dd92183a9a74a66b7caf1a080581d65",
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "58dc1381e0f2582e91da13cc67a5ca9d9657d78c",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "ec183faadcf7edaf432aca3b25d24215d505c2ec": "Ybodychange",
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16": "Ybodychange",
    "bc13dfb1426944ce45293cb8f444239a7406762c": "Ymovefromfile",
    "1177d4edc29f839b8df1038c4fbf37f56f56a2a0": "Ybodychange",
    "9e31bf675dd92183a9a74a66b7caf1a080581d65": "Ybodychange",
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "58dc1381e0f2582e91da13cc67a5ca9d9657d78c": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "ec183faadcf7edaf432aca3b25d24215d505c2ec": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8845. DiskChecker should not traverse the entire tree (Chang Li via Colin P. McCabe)\n",
      "commitDate": "17/08/15 3:25 PM",
      "commitName": "ec183faadcf7edaf432aca3b25d24215d505c2ec",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "02/06/15 8:06 PM",
      "commitNameOld": "806e407ac8896c4e669dba1fcf86fa5d6fee7c6d",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 75.8,
      "commitsBetweenForRepo": 464,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,5 @@\n   void checkDirs() throws DiskErrorException {\n-    DiskChecker.checkDirs(finalizedDir);\n+    DiskChecker.checkDir(finalizedDir);\n     DiskChecker.checkDir(tmpDir);\n     DiskChecker.checkDir(rbwDir);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void checkDirs() throws DiskErrorException {\n    DiskChecker.checkDir(finalizedDir);\n    DiskChecker.checkDir(tmpDir);\n    DiskChecker.checkDir(rbwDir);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6482. Use block ID-based block layout on datanodes (James Thomas via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615223 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/14 1:41 PM",
      "commitName": "1ba3f8971433cdbc3e43fd3605065d811dab5b16",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "24/04/14 10:19 PM",
      "commitNameOld": "0931bd94be50ba5fe266d5c31e8fcfad9897bfec",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 98.64,
      "commitsBetweenForRepo": 608,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,5 @@\n   void checkDirs() throws DiskErrorException {\n-    finalizedDir.checkDirTree();\n+    DiskChecker.checkDirs(finalizedDir);\n     DiskChecker.checkDir(tmpDir);\n     DiskChecker.checkDir(rbwDir);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void checkDirs() throws DiskErrorException {\n    DiskChecker.checkDirs(finalizedDir);\n    DiskChecker.checkDir(tmpDir);\n    DiskChecker.checkDir(rbwDir);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "bc13dfb1426944ce45293cb8f444239a7406762c": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/04/12 10:38 AM",
      "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/04/12 8:48 PM",
      "commitNameOld": "a4ccb8f504e79802f1b3c69acbcbb00b2343c529",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.58,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,5 @@\n-    void checkDirs() throws DiskErrorException {\n-      finalizedDir.checkDirTree();\n-      DiskChecker.checkDir(tmpDir);\n-      DiskChecker.checkDir(rbwDir);\n-    }\n\\ No newline at end of file\n+  void checkDirs() throws DiskErrorException {\n+    finalizedDir.checkDirTree();\n+    DiskChecker.checkDir(tmpDir);\n+    DiskChecker.checkDir(rbwDir);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void checkDirs() throws DiskErrorException {\n    finalizedDir.checkDirTree();\n    DiskChecker.checkDir(tmpDir);\n    DiskChecker.checkDir(rbwDir);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
        "oldMethodName": "checkDirs",
        "newMethodName": "checkDirs"
      }
    },
    "1177d4edc29f839b8df1038c4fbf37f56f56a2a0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3005. FSVolume.decDfsUsed(..) should be synchronized.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301127 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/03/12 11:24 AM",
      "commitName": "1177d4edc29f839b8df1038c4fbf37f56f56a2a0",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "13/03/12 3:52 PM",
      "commitNameOld": "3e582c690cb8c29267c8c8a741a21eea918f0145",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.81,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,32 @@\n     private synchronized List\u003cFSVolume\u003e checkDirs() {\n       ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n       \n       // Make a copy of volumes for performing modification \n       final List\u003cFSVolume\u003e volumeList \u003d new ArrayList\u003cFSVolume\u003e(volumes);\n       \n-      for (int idx \u003d 0; idx \u003c volumeList.size(); idx++) {\n-        FSVolume fsv \u003d volumeList.get(idx);\n+      for(Iterator\u003cFSVolume\u003e i \u003d volumeList.iterator(); i.hasNext(); ) {\n+        final FSVolume fsv \u003d i.next();\n         try {\n           fsv.checkDirs();\n         } catch (DiskErrorException e) {\n           DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n           if (removedVols \u003d\u003d null) {\n-            removedVols \u003d new ArrayList\u003cFSVolume\u003e(1);\n+            removedVols \u003d new ArrayList\u003cFSVolume\u003e(2);\n           }\n           removedVols.add(fsv);\n           fsv.shutdown(); \n-          volumeList.set(idx, null); // Remove the volume\n+          i.remove(); // Remove the volume\n           numFailedVolumes++;\n         }\n       }\n       \n-      // Remove null volumes from the volumes array\n       if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n-        final List\u003cFSVolume\u003e newVols \u003d new ArrayList\u003cFSVolume\u003e();\n-        for (FSVolume vol : volumeList) {\n-          if (vol !\u003d null) {\n-            newVols.add(vol);\n-          }\n-        }\n-        volumes \u003d Collections.unmodifiableList(newVols); // Replace volume list\n+        // Replace volume list\n+        volumes \u003d Collections.unmodifiableList(volumeList);\n         DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n             + removedVols.size() + \" volumes. List of current volumes: \"\n             + this);\n       }\n \n       return removedVols;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private synchronized List\u003cFSVolume\u003e checkDirs() {\n      ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n      \n      // Make a copy of volumes for performing modification \n      final List\u003cFSVolume\u003e volumeList \u003d new ArrayList\u003cFSVolume\u003e(volumes);\n      \n      for(Iterator\u003cFSVolume\u003e i \u003d volumeList.iterator(); i.hasNext(); ) {\n        final FSVolume fsv \u003d i.next();\n        try {\n          fsv.checkDirs();\n        } catch (DiskErrorException e) {\n          DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n          if (removedVols \u003d\u003d null) {\n            removedVols \u003d new ArrayList\u003cFSVolume\u003e(2);\n          }\n          removedVols.add(fsv);\n          fsv.shutdown(); \n          i.remove(); // Remove the volume\n          numFailedVolumes++;\n        }\n      }\n      \n      if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n        // Replace volume list\n        volumes \u003d Collections.unmodifiableList(volumeList);\n        DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n            + removedVols.size() + \" volumes. List of current volumes: \"\n            + this);\n      }\n\n      return removedVols;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "9e31bf675dd92183a9a74a66b7caf1a080581d65": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3021. Use generic type to declare FSDatasetInterface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1295929 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/03/12 1:58 PM",
      "commitName": "9e31bf675dd92183a9a74a66b7caf1a080581d65",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "22/02/12 9:47 AM",
      "commitNameOld": "efbc58f30c8e8d9f26c6a82d32d53716fb2b222a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 8.17,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n     private synchronized List\u003cFSVolume\u003e checkDirs() {\n       ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n       \n       // Make a copy of volumes for performing modification \n-      final List\u003cFSVolumeInterface\u003e volumeList \u003d new ArrayList\u003cFSVolumeInterface\u003e(volumes);\n+      final List\u003cFSVolume\u003e volumeList \u003d new ArrayList\u003cFSVolume\u003e(volumes);\n       \n       for (int idx \u003d 0; idx \u003c volumeList.size(); idx++) {\n-        FSVolume fsv \u003d (FSVolume)volumeList.get(idx);\n+        FSVolume fsv \u003d volumeList.get(idx);\n         try {\n           fsv.checkDirs();\n         } catch (DiskErrorException e) {\n           DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n           if (removedVols \u003d\u003d null) {\n             removedVols \u003d new ArrayList\u003cFSVolume\u003e(1);\n           }\n           removedVols.add(fsv);\n           fsv.shutdown(); \n           volumeList.set(idx, null); // Remove the volume\n           numFailedVolumes++;\n         }\n       }\n       \n       // Remove null volumes from the volumes array\n       if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n-        List\u003cFSVolumeInterface\u003e newVols \u003d new ArrayList\u003cFSVolumeInterface\u003e();\n-        for (FSVolumeInterface vol : volumeList) {\n+        final List\u003cFSVolume\u003e newVols \u003d new ArrayList\u003cFSVolume\u003e();\n+        for (FSVolume vol : volumeList) {\n           if (vol !\u003d null) {\n             newVols.add(vol);\n           }\n         }\n         volumes \u003d Collections.unmodifiableList(newVols); // Replace volume list\n         DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n             + removedVols.size() + \" volumes. List of current volumes: \"\n             + this);\n       }\n \n       return removedVols;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private synchronized List\u003cFSVolume\u003e checkDirs() {\n      ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n      \n      // Make a copy of volumes for performing modification \n      final List\u003cFSVolume\u003e volumeList \u003d new ArrayList\u003cFSVolume\u003e(volumes);\n      \n      for (int idx \u003d 0; idx \u003c volumeList.size(); idx++) {\n        FSVolume fsv \u003d volumeList.get(idx);\n        try {\n          fsv.checkDirs();\n        } catch (DiskErrorException e) {\n          DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n          if (removedVols \u003d\u003d null) {\n            removedVols \u003d new ArrayList\u003cFSVolume\u003e(1);\n          }\n          removedVols.add(fsv);\n          fsv.shutdown(); \n          volumeList.set(idx, null); // Remove the volume\n          numFailedVolumes++;\n        }\n      }\n      \n      // Remove null volumes from the volumes array\n      if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n        final List\u003cFSVolume\u003e newVols \u003d new ArrayList\u003cFSVolume\u003e();\n        for (FSVolume vol : volumeList) {\n          if (vol !\u003d null) {\n            newVols.add(vol);\n          }\n        }\n        volumes \u003d Collections.unmodifiableList(newVols); // Replace volume list\n        DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n            + removedVols.size() + \" volumes. List of current volumes: \"\n            + this);\n      }\n\n      return removedVols;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2887. FSVolume, is a part of FSDatasetInterface implementation, should not be referred outside FSDataset.  A new FSVolumeInterface is defined.  The BlockVolumeChoosingPolicy.chooseVolume(..) method signature is also updated.  (szetszwo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1242087 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/02/12 12:58 PM",
      "commitName": "b6ffb08a467f1b5bc89e5114c462c3117b337be6",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "02/02/12 11:26 PM",
      "commitNameOld": "38ad4b503686a0d18cb2d42ffdecf06f0ba7b98f",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.56,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n     private synchronized List\u003cFSVolume\u003e checkDirs() {\n       ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n       \n       // Make a copy of volumes for performing modification \n-      List\u003cFSVolume\u003e volumeList \u003d new ArrayList\u003cFSVolume\u003e(getVolumes());\n+      final List\u003cFSVolumeInterface\u003e volumeList \u003d new ArrayList\u003cFSVolumeInterface\u003e(volumes);\n       \n       for (int idx \u003d 0; idx \u003c volumeList.size(); idx++) {\n-        FSVolume fsv \u003d volumeList.get(idx);\n+        FSVolume fsv \u003d (FSVolume)volumeList.get(idx);\n         try {\n           fsv.checkDirs();\n         } catch (DiskErrorException e) {\n           DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n           if (removedVols \u003d\u003d null) {\n             removedVols \u003d new ArrayList\u003cFSVolume\u003e(1);\n           }\n           removedVols.add(fsv);\n           fsv.shutdown(); \n           volumeList.set(idx, null); // Remove the volume\n           numFailedVolumes++;\n         }\n       }\n       \n       // Remove null volumes from the volumes array\n       if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n-        List\u003cFSVolume\u003e newVols \u003d new ArrayList\u003cFSVolume\u003e();\n-        for (FSVolume vol : volumeList) {\n+        List\u003cFSVolumeInterface\u003e newVols \u003d new ArrayList\u003cFSVolumeInterface\u003e();\n+        for (FSVolumeInterface vol : volumeList) {\n           if (vol !\u003d null) {\n             newVols.add(vol);\n           }\n         }\n         volumes \u003d Collections.unmodifiableList(newVols); // Replace volume list\n         DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n             + removedVols.size() + \" volumes. List of current volumes: \"\n             + this);\n       }\n \n       return removedVols;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private synchronized List\u003cFSVolume\u003e checkDirs() {\n      ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n      \n      // Make a copy of volumes for performing modification \n      final List\u003cFSVolumeInterface\u003e volumeList \u003d new ArrayList\u003cFSVolumeInterface\u003e(volumes);\n      \n      for (int idx \u003d 0; idx \u003c volumeList.size(); idx++) {\n        FSVolume fsv \u003d (FSVolume)volumeList.get(idx);\n        try {\n          fsv.checkDirs();\n        } catch (DiskErrorException e) {\n          DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n          if (removedVols \u003d\u003d null) {\n            removedVols \u003d new ArrayList\u003cFSVolume\u003e(1);\n          }\n          removedVols.add(fsv);\n          fsv.shutdown(); \n          volumeList.set(idx, null); // Remove the volume\n          numFailedVolumes++;\n        }\n      }\n      \n      // Remove null volumes from the volumes array\n      if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n        List\u003cFSVolumeInterface\u003e newVols \u003d new ArrayList\u003cFSVolumeInterface\u003e();\n        for (FSVolumeInterface vol : volumeList) {\n          if (vol !\u003d null) {\n            newVols.add(vol);\n          }\n        }\n        volumes \u003d Collections.unmodifiableList(newVols); // Replace volume list\n        DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n            + removedVols.size() + \" volumes. List of current volumes: \"\n            + this);\n      }\n\n      return removedVols;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private synchronized List\u003cFSVolume\u003e checkDirs() {\n      ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n      \n      // Make a copy of volumes for performing modification \n      List\u003cFSVolume\u003e volumeList \u003d new ArrayList\u003cFSVolume\u003e(getVolumes());\n      \n      for (int idx \u003d 0; idx \u003c volumeList.size(); idx++) {\n        FSVolume fsv \u003d volumeList.get(idx);\n        try {\n          fsv.checkDirs();\n        } catch (DiskErrorException e) {\n          DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n          if (removedVols \u003d\u003d null) {\n            removedVols \u003d new ArrayList\u003cFSVolume\u003e(1);\n          }\n          removedVols.add(fsv);\n          fsv.shutdown(); \n          volumeList.set(idx, null); // Remove the volume\n          numFailedVolumes++;\n        }\n      }\n      \n      // Remove null volumes from the volumes array\n      if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n        List\u003cFSVolume\u003e newVols \u003d new ArrayList\u003cFSVolume\u003e();\n        for (FSVolume vol : volumeList) {\n          if (vol !\u003d null) {\n            newVols.add(vol);\n          }\n        }\n        volumes \u003d Collections.unmodifiableList(newVols); // Replace volume list\n        DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n            + removedVols.size() + \" volumes. List of current volumes: \"\n            + this);\n      }\n\n      return removedVols;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private synchronized List\u003cFSVolume\u003e checkDirs() {\n      ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n      \n      // Make a copy of volumes for performing modification \n      List\u003cFSVolume\u003e volumeList \u003d new ArrayList\u003cFSVolume\u003e(getVolumes());\n      \n      for (int idx \u003d 0; idx \u003c volumeList.size(); idx++) {\n        FSVolume fsv \u003d volumeList.get(idx);\n        try {\n          fsv.checkDirs();\n        } catch (DiskErrorException e) {\n          DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n          if (removedVols \u003d\u003d null) {\n            removedVols \u003d new ArrayList\u003cFSVolume\u003e(1);\n          }\n          removedVols.add(fsv);\n          fsv.shutdown(); \n          volumeList.set(idx, null); // Remove the volume\n          numFailedVolumes++;\n        }\n      }\n      \n      // Remove null volumes from the volumes array\n      if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n        List\u003cFSVolume\u003e newVols \u003d new ArrayList\u003cFSVolume\u003e();\n        for (FSVolume vol : volumeList) {\n          if (vol !\u003d null) {\n            newVols.add(vol);\n          }\n        }\n        volumes \u003d Collections.unmodifiableList(newVols); // Replace volume list\n        DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n            + removedVols.size() + \" volumes. List of current volumes: \"\n            + this);\n      }\n\n      return removedVols;\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "58dc1381e0f2582e91da13cc67a5ca9d9657d78c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1692. In secure mode, Datanode process doesn\u0027t exit when disks fail. Contributed by Bharath Mundlapudi.\n        \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1136741 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/06/11 4:35 PM",
      "commitName": "58dc1381e0f2582e91da13cc67a5ca9d9657d78c",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.07,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,38 @@\n     private synchronized List\u003cFSVolume\u003e checkDirs() {\n       ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n       \n       // Make a copy of volumes for performing modification \n       List\u003cFSVolume\u003e volumeList \u003d new ArrayList\u003cFSVolume\u003e(getVolumes());\n       \n       for (int idx \u003d 0; idx \u003c volumeList.size(); idx++) {\n         FSVolume fsv \u003d volumeList.get(idx);\n         try {\n           fsv.checkDirs();\n         } catch (DiskErrorException e) {\n           DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n           if (removedVols \u003d\u003d null) {\n             removedVols \u003d new ArrayList\u003cFSVolume\u003e(1);\n           }\n-          removedVols.add(volumeList.get(idx));\n+          removedVols.add(fsv);\n+          fsv.shutdown(); \n           volumeList.set(idx, null); // Remove the volume\n           numFailedVolumes++;\n         }\n       }\n       \n       // Remove null volumes from the volumes array\n       if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n         List\u003cFSVolume\u003e newVols \u003d new ArrayList\u003cFSVolume\u003e();\n         for (FSVolume vol : volumeList) {\n           if (vol !\u003d null) {\n             newVols.add(vol);\n           }\n         }\n         volumes \u003d Collections.unmodifiableList(newVols); // Replace volume list\n         DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n             + removedVols.size() + \" volumes. List of current volumes: \"\n             + this);\n       }\n \n       return removedVols;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private synchronized List\u003cFSVolume\u003e checkDirs() {\n      ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n      \n      // Make a copy of volumes for performing modification \n      List\u003cFSVolume\u003e volumeList \u003d new ArrayList\u003cFSVolume\u003e(getVolumes());\n      \n      for (int idx \u003d 0; idx \u003c volumeList.size(); idx++) {\n        FSVolume fsv \u003d volumeList.get(idx);\n        try {\n          fsv.checkDirs();\n        } catch (DiskErrorException e) {\n          DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n          if (removedVols \u003d\u003d null) {\n            removedVols \u003d new ArrayList\u003cFSVolume\u003e(1);\n          }\n          removedVols.add(fsv);\n          fsv.shutdown(); \n          volumeList.set(idx, null); // Remove the volume\n          numFailedVolumes++;\n        }\n      }\n      \n      // Remove null volumes from the volumes array\n      if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n        List\u003cFSVolume\u003e newVols \u003d new ArrayList\u003cFSVolume\u003e();\n        for (FSVolume vol : volumeList) {\n          if (vol !\u003d null) {\n            newVols.add(vol);\n          }\n        }\n        volumes \u003d Collections.unmodifiableList(newVols); // Replace volume list\n        DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n            + removedVols.size() + \" volumes. List of current volumes: \"\n            + this);\n      }\n\n      return removedVols;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,37 @@\n+    private synchronized List\u003cFSVolume\u003e checkDirs() {\n+      ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n+      \n+      // Make a copy of volumes for performing modification \n+      List\u003cFSVolume\u003e volumeList \u003d new ArrayList\u003cFSVolume\u003e(getVolumes());\n+      \n+      for (int idx \u003d 0; idx \u003c volumeList.size(); idx++) {\n+        FSVolume fsv \u003d volumeList.get(idx);\n+        try {\n+          fsv.checkDirs();\n+        } catch (DiskErrorException e) {\n+          DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n+          if (removedVols \u003d\u003d null) {\n+            removedVols \u003d new ArrayList\u003cFSVolume\u003e(1);\n+          }\n+          removedVols.add(volumeList.get(idx));\n+          volumeList.set(idx, null); // Remove the volume\n+          numFailedVolumes++;\n+        }\n+      }\n+      \n+      // Remove null volumes from the volumes array\n+      if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n+        List\u003cFSVolume\u003e newVols \u003d new ArrayList\u003cFSVolume\u003e();\n+        for (FSVolume vol : volumeList) {\n+          if (vol !\u003d null) {\n+            newVols.add(vol);\n+          }\n+        }\n+        volumes \u003d Collections.unmodifiableList(newVols); // Replace volume list\n+        DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n+            + removedVols.size() + \" volumes. List of current volumes: \"\n+            + this);\n+      }\n+\n+      return removedVols;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private synchronized List\u003cFSVolume\u003e checkDirs() {\n      ArrayList\u003cFSVolume\u003e removedVols \u003d null;\n      \n      // Make a copy of volumes for performing modification \n      List\u003cFSVolume\u003e volumeList \u003d new ArrayList\u003cFSVolume\u003e(getVolumes());\n      \n      for (int idx \u003d 0; idx \u003c volumeList.size(); idx++) {\n        FSVolume fsv \u003d volumeList.get(idx);\n        try {\n          fsv.checkDirs();\n        } catch (DiskErrorException e) {\n          DataNode.LOG.warn(\"Removing failed volume \" + fsv + \": \",e);\n          if (removedVols \u003d\u003d null) {\n            removedVols \u003d new ArrayList\u003cFSVolume\u003e(1);\n          }\n          removedVols.add(volumeList.get(idx));\n          volumeList.set(idx, null); // Remove the volume\n          numFailedVolumes++;\n        }\n      }\n      \n      // Remove null volumes from the volumes array\n      if (removedVols !\u003d null \u0026\u0026 removedVols.size() \u003e 0) {\n        List\u003cFSVolume\u003e newVols \u003d new ArrayList\u003cFSVolume\u003e();\n        for (FSVolume vol : volumeList) {\n          if (vol !\u003d null) {\n            newVols.add(vol);\n          }\n        }\n        volumes \u003d Collections.unmodifiableList(newVols); // Replace volume list\n        DataNode.LOG.info(\"Completed FSVolumeSet.checkDirs. Removed \"\n            + removedVols.size() + \" volumes. List of current volumes: \"\n            + this);\n      }\n\n      return removedVols;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
    }
  }
}