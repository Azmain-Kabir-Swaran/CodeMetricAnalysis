{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CombineFileInputFormat.java",
  "functionName": "getMoreSplits",
  "functionId": "getMoreSplits___job-JobContext__stats-List__FileStatus____maxSize-long__minSizeNode-long__minSizeRack-long__splits-List__InputSplit__",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
  "functionStartLine": 253,
  "functionEndLine": 290,
  "numCommitsSeen": 22,
  "timeTaken": 5405,
  "changeHistory": [
    "381a4c42135916245c8992daa3d03f38e282108d",
    "ec18984252731089ab5af12b3603dcfc3d4f4593",
    "0b9ed2364a0690d62a0d51d636027acb984e3e91",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "381a4c42135916245c8992daa3d03f38e282108d": "Ybodychange",
    "ec18984252731089ab5af12b3603dcfc3d4f4593": "Ymultichange(Yparameterchange,Ybodychange)",
    "0b9ed2364a0690d62a0d51d636027acb984e3e91": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "381a4c42135916245c8992daa3d03f38e282108d": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5352. Optimize node local splits generated by CombineFileInputFormat. (sseth)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1509345 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/13 10:42 AM",
      "commitName": "381a4c42135916245c8992daa3d03f38e282108d",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "26/07/13 11:16 AM",
      "commitNameOld": "ec18984252731089ab5af12b3603dcfc3d4f4593",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 5.98,
      "commitsBetweenForRepo": 41,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   private void getMoreSplits(JobContext job, List\u003cFileStatus\u003e stats,\n                              long maxSize, long minSizeNode, long minSizeRack,\n                              List\u003cInputSplit\u003e splits)\n     throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n \n     // all blocks for all the files in input set\n     OneFileInfo[] files;\n   \n     // mapping from a rack name to the list of blocks it has\n     HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n                               new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n \n     // mapping from a block to the nodes on which it has replicas\n     HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n                               new HashMap\u003cOneBlockInfo, String[]\u003e();\n \n     // mapping from a node to the list of blocks that it contains\n-    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n-                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n+    HashMap\u003cString, Set\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n+                              new HashMap\u003cString, Set\u003cOneBlockInfo\u003e\u003e();\n     \n     files \u003d new OneFileInfo[stats.size()];\n     if (stats.size() \u003d\u003d 0) {\n       return; \n     }\n \n     // populate all the blocks for all files\n     long totLength \u003d 0;\n     int i \u003d 0;\n     for (FileStatus stat : stats) {\n       files[i] \u003d new OneFileInfo(stat, conf, isSplitable(job, stat.getPath()),\n                                  rackToBlocks, blockToNodes, nodeToBlocks,\n                                  rackToNodes, maxSize);\n       totLength +\u003d files[i].getLength();\n     }\n     createSplits(nodeToBlocks, blockToNodes, rackToBlocks, totLength, \n                  maxSize, minSizeNode, minSizeRack, splits);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void getMoreSplits(JobContext job, List\u003cFileStatus\u003e stats,\n                             long maxSize, long minSizeNode, long minSizeRack,\n                             List\u003cInputSplit\u003e splits)\n    throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n\n    // all blocks for all the files in input set\n    OneFileInfo[] files;\n  \n    // mapping from a rack name to the list of blocks it has\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n\n    // mapping from a block to the nodes on which it has replicas\n    HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n                              new HashMap\u003cOneBlockInfo, String[]\u003e();\n\n    // mapping from a node to the list of blocks that it contains\n    HashMap\u003cString, Set\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n                              new HashMap\u003cString, Set\u003cOneBlockInfo\u003e\u003e();\n    \n    files \u003d new OneFileInfo[stats.size()];\n    if (stats.size() \u003d\u003d 0) {\n      return; \n    }\n\n    // populate all the blocks for all files\n    long totLength \u003d 0;\n    int i \u003d 0;\n    for (FileStatus stat : stats) {\n      files[i] \u003d new OneFileInfo(stat, conf, isSplitable(job, stat.getPath()),\n                                 rackToBlocks, blockToNodes, nodeToBlocks,\n                                 rackToNodes, maxSize);\n      totLength +\u003d files[i].getLength();\n    }\n    createSplits(nodeToBlocks, blockToNodes, rackToBlocks, totLength, \n                 maxSize, minSizeNode, minSizeRack, splits);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
      "extendedDetails": {}
    },
    "ec18984252731089ab5af12b3603dcfc3d4f4593": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-1981. Improve getSplits performance by using listLocatedStatus. Contributed by Hairong Kuang and Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1507385 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/13 11:16 AM",
      "commitName": "ec18984252731089ab5af12b3603dcfc3d4f4593",
      "commitAuthor": "Jason Darrell Lowe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-1981. Improve getSplits performance by using listLocatedStatus. Contributed by Hairong Kuang and Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1507385 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "26/07/13 11:16 AM",
          "commitName": "ec18984252731089ab5af12b3603dcfc3d4f4593",
          "commitAuthor": "Jason Darrell Lowe",
          "commitDateOld": "27/02/13 10:49 AM",
          "commitNameOld": "0b9ed2364a0690d62a0d51d636027acb984e3e91",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 148.98,
          "commitsBetweenForRepo": 924,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,38 @@\n-  private void getMoreSplits(JobContext job, Path[] paths, \n+  private void getMoreSplits(JobContext job, List\u003cFileStatus\u003e stats,\n                              long maxSize, long minSizeNode, long minSizeRack,\n                              List\u003cInputSplit\u003e splits)\n     throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n \n     // all blocks for all the files in input set\n     OneFileInfo[] files;\n   \n     // mapping from a rack name to the list of blocks it has\n     HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n                               new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n \n     // mapping from a block to the nodes on which it has replicas\n     HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n                               new HashMap\u003cOneBlockInfo, String[]\u003e();\n \n     // mapping from a node to the list of blocks that it contains\n     HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n                               new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n     \n-    files \u003d new OneFileInfo[paths.length];\n-    if (paths.length \u003d\u003d 0) {\n+    files \u003d new OneFileInfo[stats.size()];\n+    if (stats.size() \u003d\u003d 0) {\n       return; \n     }\n \n     // populate all the blocks for all files\n     long totLength \u003d 0;\n-    for (int i \u003d 0; i \u003c paths.length; i++) {\n-      files[i] \u003d new OneFileInfo(paths[i], conf, isSplitable(job, paths[i]),\n+    int i \u003d 0;\n+    for (FileStatus stat : stats) {\n+      files[i] \u003d new OneFileInfo(stat, conf, isSplitable(job, stat.getPath()),\n                                  rackToBlocks, blockToNodes, nodeToBlocks,\n                                  rackToNodes, maxSize);\n       totLength +\u003d files[i].getLength();\n     }\n     createSplits(nodeToBlocks, blockToNodes, rackToBlocks, totLength, \n                  maxSize, minSizeNode, minSizeRack, splits);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void getMoreSplits(JobContext job, List\u003cFileStatus\u003e stats,\n                             long maxSize, long minSizeNode, long minSizeRack,\n                             List\u003cInputSplit\u003e splits)\n    throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n\n    // all blocks for all the files in input set\n    OneFileInfo[] files;\n  \n    // mapping from a rack name to the list of blocks it has\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n\n    // mapping from a block to the nodes on which it has replicas\n    HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n                              new HashMap\u003cOneBlockInfo, String[]\u003e();\n\n    // mapping from a node to the list of blocks that it contains\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n    \n    files \u003d new OneFileInfo[stats.size()];\n    if (stats.size() \u003d\u003d 0) {\n      return; \n    }\n\n    // populate all the blocks for all files\n    long totLength \u003d 0;\n    int i \u003d 0;\n    for (FileStatus stat : stats) {\n      files[i] \u003d new OneFileInfo(stat, conf, isSplitable(job, stat.getPath()),\n                                 rackToBlocks, blockToNodes, nodeToBlocks,\n                                 rackToNodes, maxSize);\n      totLength +\u003d files[i].getLength();\n    }\n    createSplits(nodeToBlocks, blockToNodes, rackToBlocks, totLength, \n                 maxSize, minSizeNode, minSizeRack, splits);\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
          "extendedDetails": {
            "oldValue": "[job-JobContext, paths-Path[], maxSize-long, minSizeNode-long, minSizeRack-long, splits-List\u003cInputSplit\u003e]",
            "newValue": "[job-JobContext, stats-List\u003cFileStatus\u003e, maxSize-long, minSizeNode-long, minSizeRack-long, splits-List\u003cInputSplit\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-1981. Improve getSplits performance by using listLocatedStatus. Contributed by Hairong Kuang and Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1507385 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "26/07/13 11:16 AM",
          "commitName": "ec18984252731089ab5af12b3603dcfc3d4f4593",
          "commitAuthor": "Jason Darrell Lowe",
          "commitDateOld": "27/02/13 10:49 AM",
          "commitNameOld": "0b9ed2364a0690d62a0d51d636027acb984e3e91",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 148.98,
          "commitsBetweenForRepo": 924,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,38 @@\n-  private void getMoreSplits(JobContext job, Path[] paths, \n+  private void getMoreSplits(JobContext job, List\u003cFileStatus\u003e stats,\n                              long maxSize, long minSizeNode, long minSizeRack,\n                              List\u003cInputSplit\u003e splits)\n     throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n \n     // all blocks for all the files in input set\n     OneFileInfo[] files;\n   \n     // mapping from a rack name to the list of blocks it has\n     HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n                               new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n \n     // mapping from a block to the nodes on which it has replicas\n     HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n                               new HashMap\u003cOneBlockInfo, String[]\u003e();\n \n     // mapping from a node to the list of blocks that it contains\n     HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n                               new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n     \n-    files \u003d new OneFileInfo[paths.length];\n-    if (paths.length \u003d\u003d 0) {\n+    files \u003d new OneFileInfo[stats.size()];\n+    if (stats.size() \u003d\u003d 0) {\n       return; \n     }\n \n     // populate all the blocks for all files\n     long totLength \u003d 0;\n-    for (int i \u003d 0; i \u003c paths.length; i++) {\n-      files[i] \u003d new OneFileInfo(paths[i], conf, isSplitable(job, paths[i]),\n+    int i \u003d 0;\n+    for (FileStatus stat : stats) {\n+      files[i] \u003d new OneFileInfo(stat, conf, isSplitable(job, stat.getPath()),\n                                  rackToBlocks, blockToNodes, nodeToBlocks,\n                                  rackToNodes, maxSize);\n       totLength +\u003d files[i].getLength();\n     }\n     createSplits(nodeToBlocks, blockToNodes, rackToBlocks, totLength, \n                  maxSize, minSizeNode, minSizeRack, splits);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void getMoreSplits(JobContext job, List\u003cFileStatus\u003e stats,\n                             long maxSize, long minSizeNode, long minSizeRack,\n                             List\u003cInputSplit\u003e splits)\n    throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n\n    // all blocks for all the files in input set\n    OneFileInfo[] files;\n  \n    // mapping from a rack name to the list of blocks it has\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n\n    // mapping from a block to the nodes on which it has replicas\n    HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n                              new HashMap\u003cOneBlockInfo, String[]\u003e();\n\n    // mapping from a node to the list of blocks that it contains\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n    \n    files \u003d new OneFileInfo[stats.size()];\n    if (stats.size() \u003d\u003d 0) {\n      return; \n    }\n\n    // populate all the blocks for all files\n    long totLength \u003d 0;\n    int i \u003d 0;\n    for (FileStatus stat : stats) {\n      files[i] \u003d new OneFileInfo(stat, conf, isSplitable(job, stat.getPath()),\n                                 rackToBlocks, blockToNodes, nodeToBlocks,\n                                 rackToNodes, maxSize);\n      totLength +\u003d files[i].getLength();\n    }\n    createSplits(nodeToBlocks, blockToNodes, rackToBlocks, totLength, \n                 maxSize, minSizeNode, minSizeRack, splits);\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
          "extendedDetails": {}
        }
      ]
    },
    "0b9ed2364a0690d62a0d51d636027acb984e3e91": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4892. Modify CombineFileInputFormat to not skew input slits\u0027 allocation on small clusters. Contributed by Bikas Saha.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1450912 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/02/13 10:49 AM",
      "commitName": "0b9ed2364a0690d62a0d51d636027acb984e3e91",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "14/11/12 4:16 PM",
      "commitNameOld": "905b17876c44634545a68300ff2f2d73fb86d3b7",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 104.77,
      "commitsBetweenForRepo": 420,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,189 +1,37 @@\n   private void getMoreSplits(JobContext job, Path[] paths, \n                              long maxSize, long minSizeNode, long minSizeRack,\n                              List\u003cInputSplit\u003e splits)\n     throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n \n     // all blocks for all the files in input set\n     OneFileInfo[] files;\n   \n     // mapping from a rack name to the list of blocks it has\n     HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n                               new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n \n     // mapping from a block to the nodes on which it has replicas\n     HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n                               new HashMap\u003cOneBlockInfo, String[]\u003e();\n \n     // mapping from a node to the list of blocks that it contains\n     HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n                               new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n     \n     files \u003d new OneFileInfo[paths.length];\n     if (paths.length \u003d\u003d 0) {\n       return; \n     }\n \n     // populate all the blocks for all files\n     long totLength \u003d 0;\n     for (int i \u003d 0; i \u003c paths.length; i++) {\n       files[i] \u003d new OneFileInfo(paths[i], conf, isSplitable(job, paths[i]),\n                                  rackToBlocks, blockToNodes, nodeToBlocks,\n                                  rackToNodes, maxSize);\n       totLength +\u003d files[i].getLength();\n     }\n-\n-    ArrayList\u003cOneBlockInfo\u003e validBlocks \u003d new ArrayList\u003cOneBlockInfo\u003e();\n-    Set\u003cString\u003e nodes \u003d new HashSet\u003cString\u003e();\n-    long curSplitSize \u003d 0;\n-\n-    // process all nodes and create splits that are local\n-    // to a node. \n-    for (Iterator\u003cMap.Entry\u003cString, \n-         List\u003cOneBlockInfo\u003e\u003e\u003e iter \u003d nodeToBlocks.entrySet().iterator(); \n-         iter.hasNext();) {\n-\n-      Map.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e one \u003d iter.next();\n-      nodes.add(one.getKey());\n-      List\u003cOneBlockInfo\u003e blocksInNode \u003d one.getValue();\n-\n-      // for each block, copy it into validBlocks. Delete it from \n-      // blockToNodes so that the same block does not appear in \n-      // two different splits.\n-      for (OneBlockInfo oneblock : blocksInNode) {\n-        if (blockToNodes.containsKey(oneblock)) {\n-          validBlocks.add(oneblock);\n-          blockToNodes.remove(oneblock);\n-          curSplitSize +\u003d oneblock.length;\n-\n-          // if the accumulated split size exceeds the maximum, then \n-          // create this split.\n-          if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n-            // create an input split and add it to the splits array\n-            addCreatedSplit(splits, nodes, validBlocks);\n-            curSplitSize \u003d 0;\n-            validBlocks.clear();\n-          }\n-        }\n-      }\n-      // if there were any blocks left over and their combined size is\n-      // larger than minSplitNode, then combine them into one split.\n-      // Otherwise add them back to the unprocessed pool. It is likely \n-      // that they will be combined with other blocks from the \n-      // same rack later on.\n-      if (minSizeNode !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d minSizeNode) {\n-        // create an input split and add it to the splits array\n-        addCreatedSplit(splits, nodes, validBlocks);\n-      } else {\n-        for (OneBlockInfo oneblock : validBlocks) {\n-          blockToNodes.put(oneblock, oneblock.hosts);\n-        }\n-      }\n-      validBlocks.clear();\n-      nodes.clear();\n-      curSplitSize \u003d 0;\n-    }\n-\n-    // if blocks in a rack are below the specified minimum size, then keep them\n-    // in \u0027overflow\u0027. After the processing of all racks is complete, these \n-    // overflow blocks will be combined into splits.\n-    ArrayList\u003cOneBlockInfo\u003e overflowBlocks \u003d new ArrayList\u003cOneBlockInfo\u003e();\n-    Set\u003cString\u003e racks \u003d new HashSet\u003cString\u003e();\n-\n-    // Process all racks over and over again until there is no more work to do.\n-    while (blockToNodes.size() \u003e 0) {\n-\n-      // Create one split for this rack before moving over to the next rack. \n-      // Come back to this rack after creating a single split for each of the \n-      // remaining racks.\n-      // Process one rack location at a time, Combine all possible blocks that\n-      // reside on this rack as one split. (constrained by minimum and maximum\n-      // split size).\n-\n-      // iterate over all racks \n-      for (Iterator\u003cMap.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e\u003e iter \u003d \n-           rackToBlocks.entrySet().iterator(); iter.hasNext();) {\n-\n-        Map.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e one \u003d iter.next();\n-        racks.add(one.getKey());\n-        List\u003cOneBlockInfo\u003e blocks \u003d one.getValue();\n-\n-        // for each block, copy it into validBlocks. Delete it from \n-        // blockToNodes so that the same block does not appear in \n-        // two different splits.\n-        boolean createdSplit \u003d false;\n-        for (OneBlockInfo oneblock : blocks) {\n-          if (blockToNodes.containsKey(oneblock)) {\n-            validBlocks.add(oneblock);\n-            blockToNodes.remove(oneblock);\n-            curSplitSize +\u003d oneblock.length;\n-      \n-            // if the accumulated split size exceeds the maximum, then \n-            // create this split.\n-            if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n-              // create an input split and add it to the splits array\n-              addCreatedSplit(splits, getHosts(racks), validBlocks);\n-              createdSplit \u003d true;\n-              break;\n-            }\n-          }\n-        }\n-\n-        // if we created a split, then just go to the next rack\n-        if (createdSplit) {\n-          curSplitSize \u003d 0;\n-          validBlocks.clear();\n-          racks.clear();\n-          continue;\n-        }\n-\n-        if (!validBlocks.isEmpty()) {\n-          if (minSizeRack !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d minSizeRack) {\n-            // if there is a minimum size specified, then create a single split\n-            // otherwise, store these blocks into overflow data structure\n-            addCreatedSplit(splits, getHosts(racks), validBlocks);\n-          } else {\n-            // There were a few blocks in this rack that \n-        \t// remained to be processed. Keep them in \u0027overflow\u0027 block list. \n-        \t// These will be combined later.\n-            overflowBlocks.addAll(validBlocks);\n-          }\n-        }\n-        curSplitSize \u003d 0;\n-        validBlocks.clear();\n-        racks.clear();\n-      }\n-    }\n-\n-    assert blockToNodes.isEmpty();\n-    assert curSplitSize \u003d\u003d 0;\n-    assert validBlocks.isEmpty();\n-    assert racks.isEmpty();\n-\n-    // Process all overflow blocks\n-    for (OneBlockInfo oneblock : overflowBlocks) {\n-      validBlocks.add(oneblock);\n-      curSplitSize +\u003d oneblock.length;\n-\n-      // This might cause an exiting rack location to be re-added,\n-      // but it should be ok.\n-      for (int i \u003d 0; i \u003c oneblock.racks.length; i++) {\n-        racks.add(oneblock.racks[i]);\n-      }\n-\n-      // if the accumulated split size exceeds the maximum, then \n-      // create this split.\n-      if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n-        // create an input split and add it to the splits array\n-        addCreatedSplit(splits, getHosts(racks), validBlocks);\n-        curSplitSize \u003d 0;\n-        validBlocks.clear();\n-        racks.clear();\n-      }\n-    }\n-\n-    // Process any remaining blocks, if any.\n-    if (!validBlocks.isEmpty()) {\n-      addCreatedSplit(splits, getHosts(racks), validBlocks);\n-    }\n+    createSplits(nodeToBlocks, blockToNodes, rackToBlocks, totLength, \n+                 maxSize, minSizeNode, minSizeRack, splits);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void getMoreSplits(JobContext job, Path[] paths, \n                             long maxSize, long minSizeNode, long minSizeRack,\n                             List\u003cInputSplit\u003e splits)\n    throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n\n    // all blocks for all the files in input set\n    OneFileInfo[] files;\n  \n    // mapping from a rack name to the list of blocks it has\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n\n    // mapping from a block to the nodes on which it has replicas\n    HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n                              new HashMap\u003cOneBlockInfo, String[]\u003e();\n\n    // mapping from a node to the list of blocks that it contains\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n    \n    files \u003d new OneFileInfo[paths.length];\n    if (paths.length \u003d\u003d 0) {\n      return; \n    }\n\n    // populate all the blocks for all files\n    long totLength \u003d 0;\n    for (int i \u003d 0; i \u003c paths.length; i++) {\n      files[i] \u003d new OneFileInfo(paths[i], conf, isSplitable(job, paths[i]),\n                                 rackToBlocks, blockToNodes, nodeToBlocks,\n                                 rackToNodes, maxSize);\n      totLength +\u003d files[i].getLength();\n    }\n    createSplits(nodeToBlocks, blockToNodes, rackToBlocks, totLength, \n                 maxSize, minSizeNode, minSizeRack, splits);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void getMoreSplits(JobContext job, Path[] paths, \n                             long maxSize, long minSizeNode, long minSizeRack,\n                             List\u003cInputSplit\u003e splits)\n    throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n\n    // all blocks for all the files in input set\n    OneFileInfo[] files;\n  \n    // mapping from a rack name to the list of blocks it has\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n\n    // mapping from a block to the nodes on which it has replicas\n    HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n                              new HashMap\u003cOneBlockInfo, String[]\u003e();\n\n    // mapping from a node to the list of blocks that it contains\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n    \n    files \u003d new OneFileInfo[paths.length];\n    if (paths.length \u003d\u003d 0) {\n      return; \n    }\n\n    // populate all the blocks for all files\n    long totLength \u003d 0;\n    for (int i \u003d 0; i \u003c paths.length; i++) {\n      files[i] \u003d new OneFileInfo(paths[i], conf, isSplitable(job, paths[i]),\n                                 rackToBlocks, blockToNodes, nodeToBlocks,\n                                 rackToNodes, maxSize);\n      totLength +\u003d files[i].getLength();\n    }\n\n    ArrayList\u003cOneBlockInfo\u003e validBlocks \u003d new ArrayList\u003cOneBlockInfo\u003e();\n    Set\u003cString\u003e nodes \u003d new HashSet\u003cString\u003e();\n    long curSplitSize \u003d 0;\n\n    // process all nodes and create splits that are local\n    // to a node. \n    for (Iterator\u003cMap.Entry\u003cString, \n         List\u003cOneBlockInfo\u003e\u003e\u003e iter \u003d nodeToBlocks.entrySet().iterator(); \n         iter.hasNext();) {\n\n      Map.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e one \u003d iter.next();\n      nodes.add(one.getKey());\n      List\u003cOneBlockInfo\u003e blocksInNode \u003d one.getValue();\n\n      // for each block, copy it into validBlocks. Delete it from \n      // blockToNodes so that the same block does not appear in \n      // two different splits.\n      for (OneBlockInfo oneblock : blocksInNode) {\n        if (blockToNodes.containsKey(oneblock)) {\n          validBlocks.add(oneblock);\n          blockToNodes.remove(oneblock);\n          curSplitSize +\u003d oneblock.length;\n\n          // if the accumulated split size exceeds the maximum, then \n          // create this split.\n          if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n            // create an input split and add it to the splits array\n            addCreatedSplit(splits, nodes, validBlocks);\n            curSplitSize \u003d 0;\n            validBlocks.clear();\n          }\n        }\n      }\n      // if there were any blocks left over and their combined size is\n      // larger than minSplitNode, then combine them into one split.\n      // Otherwise add them back to the unprocessed pool. It is likely \n      // that they will be combined with other blocks from the \n      // same rack later on.\n      if (minSizeNode !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d minSizeNode) {\n        // create an input split and add it to the splits array\n        addCreatedSplit(splits, nodes, validBlocks);\n      } else {\n        for (OneBlockInfo oneblock : validBlocks) {\n          blockToNodes.put(oneblock, oneblock.hosts);\n        }\n      }\n      validBlocks.clear();\n      nodes.clear();\n      curSplitSize \u003d 0;\n    }\n\n    // if blocks in a rack are below the specified minimum size, then keep them\n    // in \u0027overflow\u0027. After the processing of all racks is complete, these \n    // overflow blocks will be combined into splits.\n    ArrayList\u003cOneBlockInfo\u003e overflowBlocks \u003d new ArrayList\u003cOneBlockInfo\u003e();\n    Set\u003cString\u003e racks \u003d new HashSet\u003cString\u003e();\n\n    // Process all racks over and over again until there is no more work to do.\n    while (blockToNodes.size() \u003e 0) {\n\n      // Create one split for this rack before moving over to the next rack. \n      // Come back to this rack after creating a single split for each of the \n      // remaining racks.\n      // Process one rack location at a time, Combine all possible blocks that\n      // reside on this rack as one split. (constrained by minimum and maximum\n      // split size).\n\n      // iterate over all racks \n      for (Iterator\u003cMap.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e\u003e iter \u003d \n           rackToBlocks.entrySet().iterator(); iter.hasNext();) {\n\n        Map.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e one \u003d iter.next();\n        racks.add(one.getKey());\n        List\u003cOneBlockInfo\u003e blocks \u003d one.getValue();\n\n        // for each block, copy it into validBlocks. Delete it from \n        // blockToNodes so that the same block does not appear in \n        // two different splits.\n        boolean createdSplit \u003d false;\n        for (OneBlockInfo oneblock : blocks) {\n          if (blockToNodes.containsKey(oneblock)) {\n            validBlocks.add(oneblock);\n            blockToNodes.remove(oneblock);\n            curSplitSize +\u003d oneblock.length;\n      \n            // if the accumulated split size exceeds the maximum, then \n            // create this split.\n            if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n              // create an input split and add it to the splits array\n              addCreatedSplit(splits, getHosts(racks), validBlocks);\n              createdSplit \u003d true;\n              break;\n            }\n          }\n        }\n\n        // if we created a split, then just go to the next rack\n        if (createdSplit) {\n          curSplitSize \u003d 0;\n          validBlocks.clear();\n          racks.clear();\n          continue;\n        }\n\n        if (!validBlocks.isEmpty()) {\n          if (minSizeRack !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d minSizeRack) {\n            // if there is a minimum size specified, then create a single split\n            // otherwise, store these blocks into overflow data structure\n            addCreatedSplit(splits, getHosts(racks), validBlocks);\n          } else {\n            // There were a few blocks in this rack that \n        \t// remained to be processed. Keep them in \u0027overflow\u0027 block list. \n        \t// These will be combined later.\n            overflowBlocks.addAll(validBlocks);\n          }\n        }\n        curSplitSize \u003d 0;\n        validBlocks.clear();\n        racks.clear();\n      }\n    }\n\n    assert blockToNodes.isEmpty();\n    assert curSplitSize \u003d\u003d 0;\n    assert validBlocks.isEmpty();\n    assert racks.isEmpty();\n\n    // Process all overflow blocks\n    for (OneBlockInfo oneblock : overflowBlocks) {\n      validBlocks.add(oneblock);\n      curSplitSize +\u003d oneblock.length;\n\n      // This might cause an exiting rack location to be re-added,\n      // but it should be ok.\n      for (int i \u003d 0; i \u003c oneblock.racks.length; i++) {\n        racks.add(oneblock.racks[i]);\n      }\n\n      // if the accumulated split size exceeds the maximum, then \n      // create this split.\n      if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n        // create an input split and add it to the splits array\n        addCreatedSplit(splits, getHosts(racks), validBlocks);\n        curSplitSize \u003d 0;\n        validBlocks.clear();\n        racks.clear();\n      }\n    }\n\n    // Process any remaining blocks, if any.\n    if (!validBlocks.isEmpty()) {\n      addCreatedSplit(splits, getHosts(racks), validBlocks);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void getMoreSplits(JobContext job, Path[] paths, \n                             long maxSize, long minSizeNode, long minSizeRack,\n                             List\u003cInputSplit\u003e splits)\n    throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n\n    // all blocks for all the files in input set\n    OneFileInfo[] files;\n  \n    // mapping from a rack name to the list of blocks it has\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n\n    // mapping from a block to the nodes on which it has replicas\n    HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n                              new HashMap\u003cOneBlockInfo, String[]\u003e();\n\n    // mapping from a node to the list of blocks that it contains\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n    \n    files \u003d new OneFileInfo[paths.length];\n    if (paths.length \u003d\u003d 0) {\n      return; \n    }\n\n    // populate all the blocks for all files\n    long totLength \u003d 0;\n    for (int i \u003d 0; i \u003c paths.length; i++) {\n      files[i] \u003d new OneFileInfo(paths[i], conf, isSplitable(job, paths[i]),\n                                 rackToBlocks, blockToNodes, nodeToBlocks,\n                                 rackToNodes, maxSize);\n      totLength +\u003d files[i].getLength();\n    }\n\n    ArrayList\u003cOneBlockInfo\u003e validBlocks \u003d new ArrayList\u003cOneBlockInfo\u003e();\n    Set\u003cString\u003e nodes \u003d new HashSet\u003cString\u003e();\n    long curSplitSize \u003d 0;\n\n    // process all nodes and create splits that are local\n    // to a node. \n    for (Iterator\u003cMap.Entry\u003cString, \n         List\u003cOneBlockInfo\u003e\u003e\u003e iter \u003d nodeToBlocks.entrySet().iterator(); \n         iter.hasNext();) {\n\n      Map.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e one \u003d iter.next();\n      nodes.add(one.getKey());\n      List\u003cOneBlockInfo\u003e blocksInNode \u003d one.getValue();\n\n      // for each block, copy it into validBlocks. Delete it from \n      // blockToNodes so that the same block does not appear in \n      // two different splits.\n      for (OneBlockInfo oneblock : blocksInNode) {\n        if (blockToNodes.containsKey(oneblock)) {\n          validBlocks.add(oneblock);\n          blockToNodes.remove(oneblock);\n          curSplitSize +\u003d oneblock.length;\n\n          // if the accumulated split size exceeds the maximum, then \n          // create this split.\n          if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n            // create an input split and add it to the splits array\n            addCreatedSplit(splits, nodes, validBlocks);\n            curSplitSize \u003d 0;\n            validBlocks.clear();\n          }\n        }\n      }\n      // if there were any blocks left over and their combined size is\n      // larger than minSplitNode, then combine them into one split.\n      // Otherwise add them back to the unprocessed pool. It is likely \n      // that they will be combined with other blocks from the \n      // same rack later on.\n      if (minSizeNode !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d minSizeNode) {\n        // create an input split and add it to the splits array\n        addCreatedSplit(splits, nodes, validBlocks);\n      } else {\n        for (OneBlockInfo oneblock : validBlocks) {\n          blockToNodes.put(oneblock, oneblock.hosts);\n        }\n      }\n      validBlocks.clear();\n      nodes.clear();\n      curSplitSize \u003d 0;\n    }\n\n    // if blocks in a rack are below the specified minimum size, then keep them\n    // in \u0027overflow\u0027. After the processing of all racks is complete, these \n    // overflow blocks will be combined into splits.\n    ArrayList\u003cOneBlockInfo\u003e overflowBlocks \u003d new ArrayList\u003cOneBlockInfo\u003e();\n    Set\u003cString\u003e racks \u003d new HashSet\u003cString\u003e();\n\n    // Process all racks over and over again until there is no more work to do.\n    while (blockToNodes.size() \u003e 0) {\n\n      // Create one split for this rack before moving over to the next rack. \n      // Come back to this rack after creating a single split for each of the \n      // remaining racks.\n      // Process one rack location at a time, Combine all possible blocks that\n      // reside on this rack as one split. (constrained by minimum and maximum\n      // split size).\n\n      // iterate over all racks \n      for (Iterator\u003cMap.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e\u003e iter \u003d \n           rackToBlocks.entrySet().iterator(); iter.hasNext();) {\n\n        Map.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e one \u003d iter.next();\n        racks.add(one.getKey());\n        List\u003cOneBlockInfo\u003e blocks \u003d one.getValue();\n\n        // for each block, copy it into validBlocks. Delete it from \n        // blockToNodes so that the same block does not appear in \n        // two different splits.\n        boolean createdSplit \u003d false;\n        for (OneBlockInfo oneblock : blocks) {\n          if (blockToNodes.containsKey(oneblock)) {\n            validBlocks.add(oneblock);\n            blockToNodes.remove(oneblock);\n            curSplitSize +\u003d oneblock.length;\n      \n            // if the accumulated split size exceeds the maximum, then \n            // create this split.\n            if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n              // create an input split and add it to the splits array\n              addCreatedSplit(splits, getHosts(racks), validBlocks);\n              createdSplit \u003d true;\n              break;\n            }\n          }\n        }\n\n        // if we created a split, then just go to the next rack\n        if (createdSplit) {\n          curSplitSize \u003d 0;\n          validBlocks.clear();\n          racks.clear();\n          continue;\n        }\n\n        if (!validBlocks.isEmpty()) {\n          if (minSizeRack !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d minSizeRack) {\n            // if there is a minimum size specified, then create a single split\n            // otherwise, store these blocks into overflow data structure\n            addCreatedSplit(splits, getHosts(racks), validBlocks);\n          } else {\n            // There were a few blocks in this rack that \n        \t// remained to be processed. Keep them in \u0027overflow\u0027 block list. \n        \t// These will be combined later.\n            overflowBlocks.addAll(validBlocks);\n          }\n        }\n        curSplitSize \u003d 0;\n        validBlocks.clear();\n        racks.clear();\n      }\n    }\n\n    assert blockToNodes.isEmpty();\n    assert curSplitSize \u003d\u003d 0;\n    assert validBlocks.isEmpty();\n    assert racks.isEmpty();\n\n    // Process all overflow blocks\n    for (OneBlockInfo oneblock : overflowBlocks) {\n      validBlocks.add(oneblock);\n      curSplitSize +\u003d oneblock.length;\n\n      // This might cause an exiting rack location to be re-added,\n      // but it should be ok.\n      for (int i \u003d 0; i \u003c oneblock.racks.length; i++) {\n        racks.add(oneblock.racks[i]);\n      }\n\n      // if the accumulated split size exceeds the maximum, then \n      // create this split.\n      if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n        // create an input split and add it to the splits array\n        addCreatedSplit(splits, getHosts(racks), validBlocks);\n        curSplitSize \u003d 0;\n        validBlocks.clear();\n        racks.clear();\n      }\n    }\n\n    // Process any remaining blocks, if any.\n    if (!validBlocks.isEmpty()) {\n      addCreatedSplit(splits, getHosts(racks), validBlocks);\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,189 @@\n+  private void getMoreSplits(JobContext job, Path[] paths, \n+                             long maxSize, long minSizeNode, long minSizeRack,\n+                             List\u003cInputSplit\u003e splits)\n+    throws IOException {\n+    Configuration conf \u003d job.getConfiguration();\n+\n+    // all blocks for all the files in input set\n+    OneFileInfo[] files;\n+  \n+    // mapping from a rack name to the list of blocks it has\n+    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n+                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n+\n+    // mapping from a block to the nodes on which it has replicas\n+    HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n+                              new HashMap\u003cOneBlockInfo, String[]\u003e();\n+\n+    // mapping from a node to the list of blocks that it contains\n+    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n+                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n+    \n+    files \u003d new OneFileInfo[paths.length];\n+    if (paths.length \u003d\u003d 0) {\n+      return; \n+    }\n+\n+    // populate all the blocks for all files\n+    long totLength \u003d 0;\n+    for (int i \u003d 0; i \u003c paths.length; i++) {\n+      files[i] \u003d new OneFileInfo(paths[i], conf, isSplitable(job, paths[i]),\n+                                 rackToBlocks, blockToNodes, nodeToBlocks,\n+                                 rackToNodes, maxSize);\n+      totLength +\u003d files[i].getLength();\n+    }\n+\n+    ArrayList\u003cOneBlockInfo\u003e validBlocks \u003d new ArrayList\u003cOneBlockInfo\u003e();\n+    Set\u003cString\u003e nodes \u003d new HashSet\u003cString\u003e();\n+    long curSplitSize \u003d 0;\n+\n+    // process all nodes and create splits that are local\n+    // to a node. \n+    for (Iterator\u003cMap.Entry\u003cString, \n+         List\u003cOneBlockInfo\u003e\u003e\u003e iter \u003d nodeToBlocks.entrySet().iterator(); \n+         iter.hasNext();) {\n+\n+      Map.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e one \u003d iter.next();\n+      nodes.add(one.getKey());\n+      List\u003cOneBlockInfo\u003e blocksInNode \u003d one.getValue();\n+\n+      // for each block, copy it into validBlocks. Delete it from \n+      // blockToNodes so that the same block does not appear in \n+      // two different splits.\n+      for (OneBlockInfo oneblock : blocksInNode) {\n+        if (blockToNodes.containsKey(oneblock)) {\n+          validBlocks.add(oneblock);\n+          blockToNodes.remove(oneblock);\n+          curSplitSize +\u003d oneblock.length;\n+\n+          // if the accumulated split size exceeds the maximum, then \n+          // create this split.\n+          if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n+            // create an input split and add it to the splits array\n+            addCreatedSplit(splits, nodes, validBlocks);\n+            curSplitSize \u003d 0;\n+            validBlocks.clear();\n+          }\n+        }\n+      }\n+      // if there were any blocks left over and their combined size is\n+      // larger than minSplitNode, then combine them into one split.\n+      // Otherwise add them back to the unprocessed pool. It is likely \n+      // that they will be combined with other blocks from the \n+      // same rack later on.\n+      if (minSizeNode !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d minSizeNode) {\n+        // create an input split and add it to the splits array\n+        addCreatedSplit(splits, nodes, validBlocks);\n+      } else {\n+        for (OneBlockInfo oneblock : validBlocks) {\n+          blockToNodes.put(oneblock, oneblock.hosts);\n+        }\n+      }\n+      validBlocks.clear();\n+      nodes.clear();\n+      curSplitSize \u003d 0;\n+    }\n+\n+    // if blocks in a rack are below the specified minimum size, then keep them\n+    // in \u0027overflow\u0027. After the processing of all racks is complete, these \n+    // overflow blocks will be combined into splits.\n+    ArrayList\u003cOneBlockInfo\u003e overflowBlocks \u003d new ArrayList\u003cOneBlockInfo\u003e();\n+    Set\u003cString\u003e racks \u003d new HashSet\u003cString\u003e();\n+\n+    // Process all racks over and over again until there is no more work to do.\n+    while (blockToNodes.size() \u003e 0) {\n+\n+      // Create one split for this rack before moving over to the next rack. \n+      // Come back to this rack after creating a single split for each of the \n+      // remaining racks.\n+      // Process one rack location at a time, Combine all possible blocks that\n+      // reside on this rack as one split. (constrained by minimum and maximum\n+      // split size).\n+\n+      // iterate over all racks \n+      for (Iterator\u003cMap.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e\u003e iter \u003d \n+           rackToBlocks.entrySet().iterator(); iter.hasNext();) {\n+\n+        Map.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e one \u003d iter.next();\n+        racks.add(one.getKey());\n+        List\u003cOneBlockInfo\u003e blocks \u003d one.getValue();\n+\n+        // for each block, copy it into validBlocks. Delete it from \n+        // blockToNodes so that the same block does not appear in \n+        // two different splits.\n+        boolean createdSplit \u003d false;\n+        for (OneBlockInfo oneblock : blocks) {\n+          if (blockToNodes.containsKey(oneblock)) {\n+            validBlocks.add(oneblock);\n+            blockToNodes.remove(oneblock);\n+            curSplitSize +\u003d oneblock.length;\n+      \n+            // if the accumulated split size exceeds the maximum, then \n+            // create this split.\n+            if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n+              // create an input split and add it to the splits array\n+              addCreatedSplit(splits, getHosts(racks), validBlocks);\n+              createdSplit \u003d true;\n+              break;\n+            }\n+          }\n+        }\n+\n+        // if we created a split, then just go to the next rack\n+        if (createdSplit) {\n+          curSplitSize \u003d 0;\n+          validBlocks.clear();\n+          racks.clear();\n+          continue;\n+        }\n+\n+        if (!validBlocks.isEmpty()) {\n+          if (minSizeRack !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d minSizeRack) {\n+            // if there is a minimum size specified, then create a single split\n+            // otherwise, store these blocks into overflow data structure\n+            addCreatedSplit(splits, getHosts(racks), validBlocks);\n+          } else {\n+            // There were a few blocks in this rack that \n+        \t// remained to be processed. Keep them in \u0027overflow\u0027 block list. \n+        \t// These will be combined later.\n+            overflowBlocks.addAll(validBlocks);\n+          }\n+        }\n+        curSplitSize \u003d 0;\n+        validBlocks.clear();\n+        racks.clear();\n+      }\n+    }\n+\n+    assert blockToNodes.isEmpty();\n+    assert curSplitSize \u003d\u003d 0;\n+    assert validBlocks.isEmpty();\n+    assert racks.isEmpty();\n+\n+    // Process all overflow blocks\n+    for (OneBlockInfo oneblock : overflowBlocks) {\n+      validBlocks.add(oneblock);\n+      curSplitSize +\u003d oneblock.length;\n+\n+      // This might cause an exiting rack location to be re-added,\n+      // but it should be ok.\n+      for (int i \u003d 0; i \u003c oneblock.racks.length; i++) {\n+        racks.add(oneblock.racks[i]);\n+      }\n+\n+      // if the accumulated split size exceeds the maximum, then \n+      // create this split.\n+      if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n+        // create an input split and add it to the splits array\n+        addCreatedSplit(splits, getHosts(racks), validBlocks);\n+        curSplitSize \u003d 0;\n+        validBlocks.clear();\n+        racks.clear();\n+      }\n+    }\n+\n+    // Process any remaining blocks, if any.\n+    if (!validBlocks.isEmpty()) {\n+      addCreatedSplit(splits, getHosts(racks), validBlocks);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void getMoreSplits(JobContext job, Path[] paths, \n                             long maxSize, long minSizeNode, long minSizeRack,\n                             List\u003cInputSplit\u003e splits)\n    throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n\n    // all blocks for all the files in input set\n    OneFileInfo[] files;\n  \n    // mapping from a rack name to the list of blocks it has\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e rackToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n\n    // mapping from a block to the nodes on which it has replicas\n    HashMap\u003cOneBlockInfo, String[]\u003e blockToNodes \u003d \n                              new HashMap\u003cOneBlockInfo, String[]\u003e();\n\n    // mapping from a node to the list of blocks that it contains\n    HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e nodeToBlocks \u003d \n                              new HashMap\u003cString, List\u003cOneBlockInfo\u003e\u003e();\n    \n    files \u003d new OneFileInfo[paths.length];\n    if (paths.length \u003d\u003d 0) {\n      return; \n    }\n\n    // populate all the blocks for all files\n    long totLength \u003d 0;\n    for (int i \u003d 0; i \u003c paths.length; i++) {\n      files[i] \u003d new OneFileInfo(paths[i], conf, isSplitable(job, paths[i]),\n                                 rackToBlocks, blockToNodes, nodeToBlocks,\n                                 rackToNodes, maxSize);\n      totLength +\u003d files[i].getLength();\n    }\n\n    ArrayList\u003cOneBlockInfo\u003e validBlocks \u003d new ArrayList\u003cOneBlockInfo\u003e();\n    Set\u003cString\u003e nodes \u003d new HashSet\u003cString\u003e();\n    long curSplitSize \u003d 0;\n\n    // process all nodes and create splits that are local\n    // to a node. \n    for (Iterator\u003cMap.Entry\u003cString, \n         List\u003cOneBlockInfo\u003e\u003e\u003e iter \u003d nodeToBlocks.entrySet().iterator(); \n         iter.hasNext();) {\n\n      Map.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e one \u003d iter.next();\n      nodes.add(one.getKey());\n      List\u003cOneBlockInfo\u003e blocksInNode \u003d one.getValue();\n\n      // for each block, copy it into validBlocks. Delete it from \n      // blockToNodes so that the same block does not appear in \n      // two different splits.\n      for (OneBlockInfo oneblock : blocksInNode) {\n        if (blockToNodes.containsKey(oneblock)) {\n          validBlocks.add(oneblock);\n          blockToNodes.remove(oneblock);\n          curSplitSize +\u003d oneblock.length;\n\n          // if the accumulated split size exceeds the maximum, then \n          // create this split.\n          if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n            // create an input split and add it to the splits array\n            addCreatedSplit(splits, nodes, validBlocks);\n            curSplitSize \u003d 0;\n            validBlocks.clear();\n          }\n        }\n      }\n      // if there were any blocks left over and their combined size is\n      // larger than minSplitNode, then combine them into one split.\n      // Otherwise add them back to the unprocessed pool. It is likely \n      // that they will be combined with other blocks from the \n      // same rack later on.\n      if (minSizeNode !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d minSizeNode) {\n        // create an input split and add it to the splits array\n        addCreatedSplit(splits, nodes, validBlocks);\n      } else {\n        for (OneBlockInfo oneblock : validBlocks) {\n          blockToNodes.put(oneblock, oneblock.hosts);\n        }\n      }\n      validBlocks.clear();\n      nodes.clear();\n      curSplitSize \u003d 0;\n    }\n\n    // if blocks in a rack are below the specified minimum size, then keep them\n    // in \u0027overflow\u0027. After the processing of all racks is complete, these \n    // overflow blocks will be combined into splits.\n    ArrayList\u003cOneBlockInfo\u003e overflowBlocks \u003d new ArrayList\u003cOneBlockInfo\u003e();\n    Set\u003cString\u003e racks \u003d new HashSet\u003cString\u003e();\n\n    // Process all racks over and over again until there is no more work to do.\n    while (blockToNodes.size() \u003e 0) {\n\n      // Create one split for this rack before moving over to the next rack. \n      // Come back to this rack after creating a single split for each of the \n      // remaining racks.\n      // Process one rack location at a time, Combine all possible blocks that\n      // reside on this rack as one split. (constrained by minimum and maximum\n      // split size).\n\n      // iterate over all racks \n      for (Iterator\u003cMap.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e\u003e iter \u003d \n           rackToBlocks.entrySet().iterator(); iter.hasNext();) {\n\n        Map.Entry\u003cString, List\u003cOneBlockInfo\u003e\u003e one \u003d iter.next();\n        racks.add(one.getKey());\n        List\u003cOneBlockInfo\u003e blocks \u003d one.getValue();\n\n        // for each block, copy it into validBlocks. Delete it from \n        // blockToNodes so that the same block does not appear in \n        // two different splits.\n        boolean createdSplit \u003d false;\n        for (OneBlockInfo oneblock : blocks) {\n          if (blockToNodes.containsKey(oneblock)) {\n            validBlocks.add(oneblock);\n            blockToNodes.remove(oneblock);\n            curSplitSize +\u003d oneblock.length;\n      \n            // if the accumulated split size exceeds the maximum, then \n            // create this split.\n            if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n              // create an input split and add it to the splits array\n              addCreatedSplit(splits, getHosts(racks), validBlocks);\n              createdSplit \u003d true;\n              break;\n            }\n          }\n        }\n\n        // if we created a split, then just go to the next rack\n        if (createdSplit) {\n          curSplitSize \u003d 0;\n          validBlocks.clear();\n          racks.clear();\n          continue;\n        }\n\n        if (!validBlocks.isEmpty()) {\n          if (minSizeRack !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d minSizeRack) {\n            // if there is a minimum size specified, then create a single split\n            // otherwise, store these blocks into overflow data structure\n            addCreatedSplit(splits, getHosts(racks), validBlocks);\n          } else {\n            // There were a few blocks in this rack that \n        \t// remained to be processed. Keep them in \u0027overflow\u0027 block list. \n        \t// These will be combined later.\n            overflowBlocks.addAll(validBlocks);\n          }\n        }\n        curSplitSize \u003d 0;\n        validBlocks.clear();\n        racks.clear();\n      }\n    }\n\n    assert blockToNodes.isEmpty();\n    assert curSplitSize \u003d\u003d 0;\n    assert validBlocks.isEmpty();\n    assert racks.isEmpty();\n\n    // Process all overflow blocks\n    for (OneBlockInfo oneblock : overflowBlocks) {\n      validBlocks.add(oneblock);\n      curSplitSize +\u003d oneblock.length;\n\n      // This might cause an exiting rack location to be re-added,\n      // but it should be ok.\n      for (int i \u003d 0; i \u003c oneblock.racks.length; i++) {\n        racks.add(oneblock.racks[i]);\n      }\n\n      // if the accumulated split size exceeds the maximum, then \n      // create this split.\n      if (maxSize !\u003d 0 \u0026\u0026 curSplitSize \u003e\u003d maxSize) {\n        // create an input split and add it to the splits array\n        addCreatedSplit(splits, getHosts(racks), validBlocks);\n        curSplitSize \u003d 0;\n        validBlocks.clear();\n        racks.clear();\n      }\n    }\n\n    // Process any remaining blocks, if any.\n    if (!validBlocks.isEmpty()) {\n      addCreatedSplit(splits, getHosts(racks), validBlocks);\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java"
    }
  }
}