{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReaderLocalLegacy.java",
  "functionName": "doByteBufferRead",
  "functionId": "doByteBufferRead___buf-ByteBuffer",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java",
  "functionStartLine": 510,
  "functionEndLine": 558,
  "numCommitsSeen": 30,
  "timeTaken": 2063,
  "changeHistory": [
    "f308561f1d885491b88db73ac63003202056d661",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "e2c9b288b223b9fd82dc12018936e13128413492",
    "694a6721316aea14c1244447974231abc8dff0cb"
  ],
  "changeHistoryShort": {
    "f308561f1d885491b88db73ac63003202056d661": "Yfilerename",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "e2c9b288b223b9fd82dc12018936e13128413492": "Yfilerename",
    "694a6721316aea14c1244447974231abc8dff0cb": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f308561f1d885491b88db73ac63003202056d661": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
      "commitDate": "25/04/16 12:01 PM",
      "commitName": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "25/04/16 9:38 AM",
      "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private synchronized int doByteBufferRead(ByteBuffer buf) throws IOException {\n    if (verifyChecksum) {\n      assert buf.remaining() % bytesPerChecksum \u003d\u003d 0;\n    }\n    int dataRead;\n\n    int oldpos \u003d buf.position();\n    // Read as much as we can into the buffer.\n    dataRead \u003d fillBuffer(dataIn, buf);\n\n    if (dataRead \u003d\u003d -1) {\n      return -1;\n    }\n\n    if (verifyChecksum) {\n      ByteBuffer toChecksum \u003d buf.duplicate();\n      toChecksum.position(oldpos);\n      toChecksum.limit(oldpos + dataRead);\n\n      checksumBuff.clear();\n      // Equivalent to\n      // (int)Math.ceil(toChecksum.remaining() * 1.0 / bytesPerChecksum );\n      int numChunks \u003d\n          (toChecksum.remaining() + bytesPerChecksum - 1) / bytesPerChecksum;\n      checksumBuff.limit(checksumSize * numChunks);\n\n      fillBuffer(checksumIn, checksumBuff);\n      checksumBuff.flip();\n\n      checksum.verifyChunkedSums(toChecksum, checksumBuff, filename,\n          this.startOffset);\n    }\n\n    if (dataRead \u003e\u003d 0) {\n      buf.position(oldpos + Math.min(offsetFromChunkBoundary, dataRead));\n    }\n\n    if (dataRead \u003c offsetFromChunkBoundary) {\n      // yikes, didn\u0027t even get enough bytes to honour offset. This can happen\n      // even if we are verifying checksums if we are at EOF.\n      offsetFromChunkBoundary -\u003d dataRead;\n      dataRead \u003d 0;\n    } else {\n      dataRead -\u003d offsetFromChunkBoundary;\n      offsetFromChunkBoundary \u003d 0;\n    }\n\n    return dataRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java"
      }
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,49 @@\n   private synchronized int doByteBufferRead(ByteBuffer buf) throws IOException {\n     if (verifyChecksum) {\n       assert buf.remaining() % bytesPerChecksum \u003d\u003d 0;\n     }\n-    int dataRead \u003d -1;\n+    int dataRead;\n \n     int oldpos \u003d buf.position();\n     // Read as much as we can into the buffer.\n     dataRead \u003d fillBuffer(dataIn, buf);\n \n     if (dataRead \u003d\u003d -1) {\n       return -1;\n     }\n \n     if (verifyChecksum) {\n       ByteBuffer toChecksum \u003d buf.duplicate();\n       toChecksum.position(oldpos);\n       toChecksum.limit(oldpos + dataRead);\n \n       checksumBuff.clear();\n-      // Equivalent to (int)Math.ceil(toChecksum.remaining() * 1.0 / bytesPerChecksum );\n+      // Equivalent to\n+      // (int)Math.ceil(toChecksum.remaining() * 1.0 / bytesPerChecksum );\n       int numChunks \u003d\n-        (toChecksum.remaining() + bytesPerChecksum - 1) / bytesPerChecksum;\n+          (toChecksum.remaining() + bytesPerChecksum - 1) / bytesPerChecksum;\n       checksumBuff.limit(checksumSize * numChunks);\n \n       fillBuffer(checksumIn, checksumBuff);\n       checksumBuff.flip();\n \n       checksum.verifyChunkedSums(toChecksum, checksumBuff, filename,\n           this.startOffset);\n     }\n \n     if (dataRead \u003e\u003d 0) {\n       buf.position(oldpos + Math.min(offsetFromChunkBoundary, dataRead));\n     }\n \n     if (dataRead \u003c offsetFromChunkBoundary) {\n       // yikes, didn\u0027t even get enough bytes to honour offset. This can happen\n       // even if we are verifying checksums if we are at EOF.\n       offsetFromChunkBoundary -\u003d dataRead;\n       dataRead \u003d 0;\n     } else {\n       dataRead -\u003d offsetFromChunkBoundary;\n       offsetFromChunkBoundary \u003d 0;\n     }\n \n     return dataRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized int doByteBufferRead(ByteBuffer buf) throws IOException {\n    if (verifyChecksum) {\n      assert buf.remaining() % bytesPerChecksum \u003d\u003d 0;\n    }\n    int dataRead;\n\n    int oldpos \u003d buf.position();\n    // Read as much as we can into the buffer.\n    dataRead \u003d fillBuffer(dataIn, buf);\n\n    if (dataRead \u003d\u003d -1) {\n      return -1;\n    }\n\n    if (verifyChecksum) {\n      ByteBuffer toChecksum \u003d buf.duplicate();\n      toChecksum.position(oldpos);\n      toChecksum.limit(oldpos + dataRead);\n\n      checksumBuff.clear();\n      // Equivalent to\n      // (int)Math.ceil(toChecksum.remaining() * 1.0 / bytesPerChecksum );\n      int numChunks \u003d\n          (toChecksum.remaining() + bytesPerChecksum - 1) / bytesPerChecksum;\n      checksumBuff.limit(checksumSize * numChunks);\n\n      fillBuffer(checksumIn, checksumBuff);\n      checksumBuff.flip();\n\n      checksum.verifyChunkedSums(toChecksum, checksumBuff, filename,\n          this.startOffset);\n    }\n\n    if (dataRead \u003e\u003d 0) {\n      buf.position(oldpos + Math.min(offsetFromChunkBoundary, dataRead));\n    }\n\n    if (dataRead \u003c offsetFromChunkBoundary) {\n      // yikes, didn\u0027t even get enough bytes to honour offset. This can happen\n      // even if we are verifying checksums if we are at EOF.\n      offsetFromChunkBoundary -\u003d dataRead;\n      dataRead \u003d 0;\n    } else {\n      dataRead -\u003d offsetFromChunkBoundary;\n      offsetFromChunkBoundary \u003d 0;\n    }\n\n    return dataRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
      "extendedDetails": {}
    },
    "e2c9b288b223b9fd82dc12018936e13128413492": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "28/08/15 2:38 PM",
      "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/08/15 2:21 PM",
      "commitNameOld": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private synchronized int doByteBufferRead(ByteBuffer buf) throws IOException {\n    if (verifyChecksum) {\n      assert buf.remaining() % bytesPerChecksum \u003d\u003d 0;\n    }\n    int dataRead \u003d -1;\n\n    int oldpos \u003d buf.position();\n    // Read as much as we can into the buffer.\n    dataRead \u003d fillBuffer(dataIn, buf);\n\n    if (dataRead \u003d\u003d -1) {\n      return -1;\n    }\n\n    if (verifyChecksum) {\n      ByteBuffer toChecksum \u003d buf.duplicate();\n      toChecksum.position(oldpos);\n      toChecksum.limit(oldpos + dataRead);\n\n      checksumBuff.clear();\n      // Equivalent to (int)Math.ceil(toChecksum.remaining() * 1.0 / bytesPerChecksum );\n      int numChunks \u003d\n        (toChecksum.remaining() + bytesPerChecksum - 1) / bytesPerChecksum;\n      checksumBuff.limit(checksumSize * numChunks);\n\n      fillBuffer(checksumIn, checksumBuff);\n      checksumBuff.flip();\n\n      checksum.verifyChunkedSums(toChecksum, checksumBuff, filename,\n          this.startOffset);\n    }\n\n    if (dataRead \u003e\u003d 0) {\n      buf.position(oldpos + Math.min(offsetFromChunkBoundary, dataRead));\n    }\n\n    if (dataRead \u003c offsetFromChunkBoundary) {\n      // yikes, didn\u0027t even get enough bytes to honour offset. This can happen\n      // even if we are verifying checksums if we are at EOF.\n      offsetFromChunkBoundary -\u003d dataRead;\n      dataRead \u003d 0;\n    } else {\n      dataRead -\u003d offsetFromChunkBoundary;\n      offsetFromChunkBoundary \u003d 0;\n    }\n\n    return dataRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java"
      }
    },
    "694a6721316aea14c1244447974231abc8dff0cb": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4538. Allow use of legacy blockreader. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1461818 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/03/13 12:28 PM",
      "commitName": "694a6721316aea14c1244447974231abc8dff0cb",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,48 @@\n+  private synchronized int doByteBufferRead(ByteBuffer buf) throws IOException {\n+    if (verifyChecksum) {\n+      assert buf.remaining() % bytesPerChecksum \u003d\u003d 0;\n+    }\n+    int dataRead \u003d -1;\n+\n+    int oldpos \u003d buf.position();\n+    // Read as much as we can into the buffer.\n+    dataRead \u003d fillBuffer(dataIn, buf);\n+\n+    if (dataRead \u003d\u003d -1) {\n+      return -1;\n+    }\n+\n+    if (verifyChecksum) {\n+      ByteBuffer toChecksum \u003d buf.duplicate();\n+      toChecksum.position(oldpos);\n+      toChecksum.limit(oldpos + dataRead);\n+\n+      checksumBuff.clear();\n+      // Equivalent to (int)Math.ceil(toChecksum.remaining() * 1.0 / bytesPerChecksum );\n+      int numChunks \u003d\n+        (toChecksum.remaining() + bytesPerChecksum - 1) / bytesPerChecksum;\n+      checksumBuff.limit(checksumSize * numChunks);\n+\n+      fillBuffer(checksumIn, checksumBuff);\n+      checksumBuff.flip();\n+\n+      checksum.verifyChunkedSums(toChecksum, checksumBuff, filename,\n+          this.startOffset);\n+    }\n+\n+    if (dataRead \u003e\u003d 0) {\n+      buf.position(oldpos + Math.min(offsetFromChunkBoundary, dataRead));\n+    }\n+\n+    if (dataRead \u003c offsetFromChunkBoundary) {\n+      // yikes, didn\u0027t even get enough bytes to honour offset. This can happen\n+      // even if we are verifying checksums if we are at EOF.\n+      offsetFromChunkBoundary -\u003d dataRead;\n+      dataRead \u003d 0;\n+    } else {\n+      dataRead -\u003d offsetFromChunkBoundary;\n+      offsetFromChunkBoundary \u003d 0;\n+    }\n+\n+    return dataRead;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized int doByteBufferRead(ByteBuffer buf) throws IOException {\n    if (verifyChecksum) {\n      assert buf.remaining() % bytesPerChecksum \u003d\u003d 0;\n    }\n    int dataRead \u003d -1;\n\n    int oldpos \u003d buf.position();\n    // Read as much as we can into the buffer.\n    dataRead \u003d fillBuffer(dataIn, buf);\n\n    if (dataRead \u003d\u003d -1) {\n      return -1;\n    }\n\n    if (verifyChecksum) {\n      ByteBuffer toChecksum \u003d buf.duplicate();\n      toChecksum.position(oldpos);\n      toChecksum.limit(oldpos + dataRead);\n\n      checksumBuff.clear();\n      // Equivalent to (int)Math.ceil(toChecksum.remaining() * 1.0 / bytesPerChecksum );\n      int numChunks \u003d\n        (toChecksum.remaining() + bytesPerChecksum - 1) / bytesPerChecksum;\n      checksumBuff.limit(checksumSize * numChunks);\n\n      fillBuffer(checksumIn, checksumBuff);\n      checksumBuff.flip();\n\n      checksum.verifyChunkedSums(toChecksum, checksumBuff, filename,\n          this.startOffset);\n    }\n\n    if (dataRead \u003e\u003d 0) {\n      buf.position(oldpos + Math.min(offsetFromChunkBoundary, dataRead));\n    }\n\n    if (dataRead \u003c offsetFromChunkBoundary) {\n      // yikes, didn\u0027t even get enough bytes to honour offset. This can happen\n      // even if we are verifying checksums if we are at EOF.\n      offsetFromChunkBoundary -\u003d dataRead;\n      dataRead \u003d 0;\n    } else {\n      dataRead -\u003d offsetFromChunkBoundary;\n      offsetFromChunkBoundary \u003d 0;\n    }\n\n    return dataRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java"
    }
  }
}