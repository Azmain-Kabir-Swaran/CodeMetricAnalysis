{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataNode.java",
  "functionName": "transferReplicaForPipelineRecovery",
  "functionId": "transferReplicaForPipelineRecovery___b-ExtendedBlock(modifiers-final)__targets-DatanodeInfo[](modifiers-final)__targetStorageTypes-StorageType[](modifiers-final)__targetStorageIds-String[](modifiers-final)__client-String(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
  "functionStartLine": 3054,
  "functionEndLine": 3108,
  "numCommitsSeen": 358,
  "timeTaken": 3001,
  "changeHistory": [
    "15d38b1bf9fbd41658f6980c1a484dd28f746654",
    "dfcb331ba3516264398121c9f23af3a79c0509cc"
  ],
  "changeHistoryShort": {
    "15d38b1bf9fbd41658f6980c1a484dd28f746654": "Ybodychange",
    "dfcb331ba3516264398121c9f23af3a79c0509cc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "15d38b1bf9fbd41658f6980c1a484dd28f746654": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14295. Add Threadpool for DataTransfers. Contributed by David Mollitor.\n",
      "commitDate": "28/03/19 3:37 AM",
      "commitName": "15d38b1bf9fbd41658f6980c1a484dd28f746654",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "12/03/19 10:17 AM",
      "commitNameOld": "34b14061b38dccab25058dff1b8743d8a3f82734",
      "commitAuthorOld": "Stephen O\u0027Donnell",
      "daysBetweenCommits": 15.72,
      "commitsBetweenForRepo": 116,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,55 @@\n   void transferReplicaForPipelineRecovery(final ExtendedBlock b,\n       final DatanodeInfo[] targets, final StorageType[] targetStorageTypes,\n       final String[] targetStorageIds, final String client)\n       throws IOException {\n     final long storedGS;\n     final long visible;\n     final BlockConstructionStage stage;\n \n     //get replica information\n     try(AutoCloseableLock lock \u003d data.acquireDatasetLock()) {\n       Block storedBlock \u003d data.getStoredBlock(b.getBlockPoolId(),\n           b.getBlockId());\n       if (null \u003d\u003d storedBlock) {\n         throw new IOException(b + \" not found in datanode.\");\n       }\n       storedGS \u003d storedBlock.getGenerationStamp();\n       if (storedGS \u003c b.getGenerationStamp()) {\n         throw new IOException(storedGS\n             + \" \u003d storedGS \u003c b.getGenerationStamp(), b\u003d\" + b);\n       }\n       // Update the genstamp with storedGS\n       b.setGenerationStamp(storedGS);\n       if (data.isValidRbw(b)) {\n         stage \u003d BlockConstructionStage.TRANSFER_RBW;\n       } else if (data.isValidBlock(b)) {\n         stage \u003d BlockConstructionStage.TRANSFER_FINALIZED;\n       } else {\n         final String r \u003d data.getReplicaString(b.getBlockPoolId(), b.getBlockId());\n         throw new IOException(b + \" is neither a RBW nor a Finalized, r\u003d\" + r);\n       }\n       visible \u003d data.getReplicaVisibleLength(b);\n     }\n     //set visible length\n     b.setNumBytes(visible);\n \n     if (targets.length \u003e 0) {\n-      Daemon daemon \u003d new Daemon(threadGroup,\n-          new DataTransfer(targets, targetStorageTypes, targetStorageIds, b,\n-              stage, client));\n-      daemon.start();\n+      if (LOG.isDebugEnabled()) {\n+        final String xferTargetsString \u003d\n+            StringUtils.join(\" \", Arrays.asList(targets));\n+        LOG.debug(\"Transferring a replica to {}\", xferTargetsString);\n+      }\n+\n+      final DataTransfer dataTransferTask \u003d new DataTransfer(targets,\n+          targetStorageTypes, targetStorageIds, b, stage, client);\n+\n+      @SuppressWarnings(\"unchecked\")\n+      Future\u003cVoid\u003e f \u003d (Future\u003cVoid\u003e) this.xferService.submit(dataTransferTask);\n       try {\n-        daemon.join();\n-      } catch (InterruptedException e) {\n-        throw new IOException(\n-            \"Pipeline recovery for \" + b + \" is interrupted.\", e);\n+        f.get();\n+      } catch (InterruptedException | ExecutionException e) {\n+        throw new IOException(\"Pipeline recovery for \" + b + \" is interrupted.\",\n+            e);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void transferReplicaForPipelineRecovery(final ExtendedBlock b,\n      final DatanodeInfo[] targets, final StorageType[] targetStorageTypes,\n      final String[] targetStorageIds, final String client)\n      throws IOException {\n    final long storedGS;\n    final long visible;\n    final BlockConstructionStage stage;\n\n    //get replica information\n    try(AutoCloseableLock lock \u003d data.acquireDatasetLock()) {\n      Block storedBlock \u003d data.getStoredBlock(b.getBlockPoolId(),\n          b.getBlockId());\n      if (null \u003d\u003d storedBlock) {\n        throw new IOException(b + \" not found in datanode.\");\n      }\n      storedGS \u003d storedBlock.getGenerationStamp();\n      if (storedGS \u003c b.getGenerationStamp()) {\n        throw new IOException(storedGS\n            + \" \u003d storedGS \u003c b.getGenerationStamp(), b\u003d\" + b);\n      }\n      // Update the genstamp with storedGS\n      b.setGenerationStamp(storedGS);\n      if (data.isValidRbw(b)) {\n        stage \u003d BlockConstructionStage.TRANSFER_RBW;\n      } else if (data.isValidBlock(b)) {\n        stage \u003d BlockConstructionStage.TRANSFER_FINALIZED;\n      } else {\n        final String r \u003d data.getReplicaString(b.getBlockPoolId(), b.getBlockId());\n        throw new IOException(b + \" is neither a RBW nor a Finalized, r\u003d\" + r);\n      }\n      visible \u003d data.getReplicaVisibleLength(b);\n    }\n    //set visible length\n    b.setNumBytes(visible);\n\n    if (targets.length \u003e 0) {\n      if (LOG.isDebugEnabled()) {\n        final String xferTargetsString \u003d\n            StringUtils.join(\" \", Arrays.asList(targets));\n        LOG.debug(\"Transferring a replica to {}\", xferTargetsString);\n      }\n\n      final DataTransfer dataTransferTask \u003d new DataTransfer(targets,\n          targetStorageTypes, targetStorageIds, b, stage, client);\n\n      @SuppressWarnings(\"unchecked\")\n      Future\u003cVoid\u003e f \u003d (Future\u003cVoid\u003e) this.xferService.submit(dataTransferTask);\n      try {\n        f.get();\n      } catch (InterruptedException | ExecutionException e) {\n        throw new IOException(\"Pipeline recovery for \" + b + \" is interrupted.\",\n            e);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "dfcb331ba3516264398121c9f23af3a79c0509cc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13076: [SPS]: Addendum. Resolve conflicts after rebasing branch to trunk. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "dfcb331ba3516264398121c9f23af3a79c0509cc",
      "commitAuthor": "Rakesh Radhakrishnan",
      "diff": "@@ -0,0 +1,48 @@\n+  void transferReplicaForPipelineRecovery(final ExtendedBlock b,\n+      final DatanodeInfo[] targets, final StorageType[] targetStorageTypes,\n+      final String[] targetStorageIds, final String client)\n+      throws IOException {\n+    final long storedGS;\n+    final long visible;\n+    final BlockConstructionStage stage;\n+\n+    //get replica information\n+    try(AutoCloseableLock lock \u003d data.acquireDatasetLock()) {\n+      Block storedBlock \u003d data.getStoredBlock(b.getBlockPoolId(),\n+          b.getBlockId());\n+      if (null \u003d\u003d storedBlock) {\n+        throw new IOException(b + \" not found in datanode.\");\n+      }\n+      storedGS \u003d storedBlock.getGenerationStamp();\n+      if (storedGS \u003c b.getGenerationStamp()) {\n+        throw new IOException(storedGS\n+            + \" \u003d storedGS \u003c b.getGenerationStamp(), b\u003d\" + b);\n+      }\n+      // Update the genstamp with storedGS\n+      b.setGenerationStamp(storedGS);\n+      if (data.isValidRbw(b)) {\n+        stage \u003d BlockConstructionStage.TRANSFER_RBW;\n+      } else if (data.isValidBlock(b)) {\n+        stage \u003d BlockConstructionStage.TRANSFER_FINALIZED;\n+      } else {\n+        final String r \u003d data.getReplicaString(b.getBlockPoolId(), b.getBlockId());\n+        throw new IOException(b + \" is neither a RBW nor a Finalized, r\u003d\" + r);\n+      }\n+      visible \u003d data.getReplicaVisibleLength(b);\n+    }\n+    //set visible length\n+    b.setNumBytes(visible);\n+\n+    if (targets.length \u003e 0) {\n+      Daemon daemon \u003d new Daemon(threadGroup,\n+          new DataTransfer(targets, targetStorageTypes, targetStorageIds, b,\n+              stage, client));\n+      daemon.start();\n+      try {\n+        daemon.join();\n+      } catch (InterruptedException e) {\n+        throw new IOException(\n+            \"Pipeline recovery for \" + b + \" is interrupted.\", e);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void transferReplicaForPipelineRecovery(final ExtendedBlock b,\n      final DatanodeInfo[] targets, final StorageType[] targetStorageTypes,\n      final String[] targetStorageIds, final String client)\n      throws IOException {\n    final long storedGS;\n    final long visible;\n    final BlockConstructionStage stage;\n\n    //get replica information\n    try(AutoCloseableLock lock \u003d data.acquireDatasetLock()) {\n      Block storedBlock \u003d data.getStoredBlock(b.getBlockPoolId(),\n          b.getBlockId());\n      if (null \u003d\u003d storedBlock) {\n        throw new IOException(b + \" not found in datanode.\");\n      }\n      storedGS \u003d storedBlock.getGenerationStamp();\n      if (storedGS \u003c b.getGenerationStamp()) {\n        throw new IOException(storedGS\n            + \" \u003d storedGS \u003c b.getGenerationStamp(), b\u003d\" + b);\n      }\n      // Update the genstamp with storedGS\n      b.setGenerationStamp(storedGS);\n      if (data.isValidRbw(b)) {\n        stage \u003d BlockConstructionStage.TRANSFER_RBW;\n      } else if (data.isValidBlock(b)) {\n        stage \u003d BlockConstructionStage.TRANSFER_FINALIZED;\n      } else {\n        final String r \u003d data.getReplicaString(b.getBlockPoolId(), b.getBlockId());\n        throw new IOException(b + \" is neither a RBW nor a Finalized, r\u003d\" + r);\n      }\n      visible \u003d data.getReplicaVisibleLength(b);\n    }\n    //set visible length\n    b.setNumBytes(visible);\n\n    if (targets.length \u003e 0) {\n      Daemon daemon \u003d new Daemon(threadGroup,\n          new DataTransfer(targets, targetStorageTypes, targetStorageIds, b,\n              stage, client));\n      daemon.start();\n      try {\n        daemon.join();\n      } catch (InterruptedException e) {\n        throw new IOException(\n            \"Pipeline recovery for \" + b + \" is interrupted.\", e);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
    }
  }
}