{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSClient.java",
  "functionName": "getFileChecksum",
  "functionId": "getFileChecksum___src-String__length-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
  "functionStartLine": 1890,
  "functionEndLine": 1894,
  "numCommitsSeen": 471,
  "timeTaken": 15668,
  "changeHistory": [
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
    "cc1292e73acd39c1f1023ad4841ffe30176f7daf",
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
    "a337ceb74e984991dbf976236d2e785cf5921b16",
    "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
    "def9136e0259e118e6fd7b656260765d28ac9ae6",
    "4da8490b512a33a255ed27309860859388d7c168",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "f2d7a67a2c1d9dde10ed3171fdec65dff885afcc",
    "67ed59348d638d56e6752ba2c71fdcd69567546d",
    "3b54223c0f32d42a84436c670d80b791a8e9696d",
    "3671a5e16fbddbe5a0516289ce98e1305e02291c",
    "cfae13306ac0fb3f3c139d5ac511bf78cede1b77",
    "f98d8eb291be364102b5c3011ce72e8f43eab389",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf": "Ybodychange",
    "cc1292e73acd39c1f1023ad4841ffe30176f7daf": "Ybodychange",
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720": "Ybodychange",
    "a337ceb74e984991dbf976236d2e785cf5921b16": "Ybodychange",
    "e5ff0ea7ba087984262f1f27200ae5bb40d9b838": "Ybodychange",
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": "Ybodychange",
    "def9136e0259e118e6fd7b656260765d28ac9ae6": "Ybodychange",
    "4da8490b512a33a255ed27309860859388d7c168": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "f2d7a67a2c1d9dde10ed3171fdec65dff885afcc": "Ybodychange",
    "67ed59348d638d56e6752ba2c71fdcd69567546d": "Ybodychange",
    "3b54223c0f32d42a84436c670d80b791a8e9696d": "Ybodychange",
    "3671a5e16fbddbe5a0516289ce98e1305e02291c": "Ymultichange(Yparameterchange,Ybodychange)",
    "cfae13306ac0fb3f3c139d5ac511bf78cede1b77": "Ybodychange",
    "f98d8eb291be364102b5c3011ce72e8f43eab389": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13056. Expose file-level composite CRCs in HDFS which are comparable across different instances/layouts. Contributed by Dennis Huo.\n",
      "commitDate": "10/04/18 9:31 PM",
      "commitName": "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "22/03/18 11:29 AM",
      "commitNameOld": "f738d75a86602353d48a810f46919e49d1c06ade",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 19.42,
      "commitsBetweenForRepo": 247,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,5 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n-    checkOpen();\n-    Preconditions.checkArgument(length \u003e\u003d 0);\n-\n-    LocatedBlocks blockLocations \u003d null;\n-    FileChecksumHelper.FileChecksumComputer maker \u003d null;\n-    ErasureCodingPolicy ecPolicy \u003d null;\n-    if (length \u003e 0) {\n-      blockLocations \u003d getBlockLocations(src, length);\n-      ecPolicy \u003d blockLocations.getErasureCodingPolicy();\n-    }\n-\n-    maker \u003d ecPolicy !\u003d null ?\n-        new FileChecksumHelper.StripedFileNonStripedChecksumComputer(src,\n-            length, blockLocations, namenode, this, ecPolicy) :\n-        new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length,\n-            blockLocations, namenode, this);\n-\n-    maker.compute();\n-\n-    return maker.getFileChecksum();\n+    return (MD5MD5CRC32FileChecksum) getFileChecksumInternal(\n+        src, length, ChecksumCombineMode.MD5MD5CRC);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    return (MD5MD5CRC32FileChecksum) getFileChecksumInternal(\n        src, length, ChecksumCombineMode.MD5MD5CRC);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "cc1292e73acd39c1f1023ad4841ffe30176f7daf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9705. Refine the behaviour of getFileChecksum when length \u003d 0. Contributed by Kai Zheng and SammiChen.\n",
      "commitDate": "14/03/17 4:41 PM",
      "commitName": "cc1292e73acd39c1f1023ad4841ffe30176f7daf",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "01/03/17 2:36 AM",
      "commitNameOld": "82ef9accafe7318278efb169678e17065e082c8e",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 13.55,
      "commitsBetweenForRepo": 89,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,23 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n \n-    LocatedBlocks blockLocations \u003d getBlockLocations(src, length);\n+    LocatedBlocks blockLocations \u003d null;\n+    FileChecksumHelper.FileChecksumComputer maker \u003d null;\n+    ErasureCodingPolicy ecPolicy \u003d null;\n+    if (length \u003e 0) {\n+      blockLocations \u003d getBlockLocations(src, length);\n+      ecPolicy \u003d blockLocations.getErasureCodingPolicy();\n+    }\n \n-    FileChecksumHelper.FileChecksumComputer maker;\n-    ErasureCodingPolicy ecPolicy \u003d blockLocations.getErasureCodingPolicy();\n     maker \u003d ecPolicy !\u003d null ?\n         new FileChecksumHelper.StripedFileNonStripedChecksumComputer(src,\n             length, blockLocations, namenode, this, ecPolicy) :\n         new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length,\n             blockLocations, namenode, this);\n \n     maker.compute();\n \n     return maker.getFileChecksum();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n\n    LocatedBlocks blockLocations \u003d null;\n    FileChecksumHelper.FileChecksumComputer maker \u003d null;\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (length \u003e 0) {\n      blockLocations \u003d getBlockLocations(src, length);\n      ecPolicy \u003d blockLocations.getErasureCodingPolicy();\n    }\n\n    maker \u003d ecPolicy !\u003d null ?\n        new FileChecksumHelper.StripedFileNonStripedChecksumComputer(src,\n            length, blockLocations, namenode, this, ecPolicy) :\n        new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length,\n            blockLocations, namenode, this);\n\n    maker.compute();\n\n    return maker.getFileChecksum();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
      "commitDate": "26/03/16 7:58 PM",
      "commitName": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "26/03/16 9:20 AM",
      "commitNameOld": "a337ceb74e984991dbf976236d2e785cf5921b16",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.44,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,19 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n \n     LocatedBlocks blockLocations \u003d getBlockLocations(src, length);\n \n-    FileChecksumHelper.FileChecksumComputer maker \u003d\n+    FileChecksumHelper.FileChecksumComputer maker;\n+    ErasureCodingPolicy ecPolicy \u003d blockLocations.getErasureCodingPolicy();\n+    maker \u003d ecPolicy !\u003d null ?\n+        new FileChecksumHelper.StripedFileNonStripedChecksumComputer(src,\n+            length, blockLocations, namenode, this, ecPolicy) :\n         new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length,\n             blockLocations, namenode, this);\n \n     maker.compute();\n \n     return maker.getFileChecksum();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n\n    LocatedBlocks blockLocations \u003d getBlockLocations(src, length);\n\n    FileChecksumHelper.FileChecksumComputer maker;\n    ErasureCodingPolicy ecPolicy \u003d blockLocations.getErasureCodingPolicy();\n    maker \u003d ecPolicy !\u003d null ?\n        new FileChecksumHelper.StripedFileNonStripedChecksumComputer(src,\n            length, blockLocations, namenode, this, ecPolicy) :\n        new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length,\n            blockLocations, namenode, this);\n\n    maker.compute();\n\n    return maker.getFileChecksum();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "a337ceb74e984991dbf976236d2e785cf5921b16": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\"\n\nThis reverts commit e5ff0ea7ba087984262f1f27200ae5bb40d9b838.\n",
      "commitDate": "26/03/16 9:20 AM",
      "commitName": "a337ceb74e984991dbf976236d2e785cf5921b16",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "26/03/16 12:52 AM",
      "commitNameOld": "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.35,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,15 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n \n     LocatedBlocks blockLocations \u003d getBlockLocations(src, length);\n \n-    FileChecksumHelper.FileChecksumComputer maker;\n-    ErasureCodingPolicy ecPolicy \u003d blockLocations.getErasureCodingPolicy();\n-    maker \u003d ecPolicy !\u003d null ?\n-        new FileChecksumHelper.StripedFileNonStripedChecksumComputer(src,\n-            length, blockLocations, namenode, this, ecPolicy) :\n+    FileChecksumHelper.FileChecksumComputer maker \u003d\n         new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length,\n             blockLocations, namenode, this);\n \n     maker.compute();\n \n     return maker.getFileChecksum();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n\n    LocatedBlocks blockLocations \u003d getBlockLocations(src, length);\n\n    FileChecksumHelper.FileChecksumComputer maker \u003d\n        new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length,\n            blockLocations, namenode, this);\n\n    maker.compute();\n\n    return maker.getFileChecksum();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "e5ff0ea7ba087984262f1f27200ae5bb40d9b838": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
      "commitDate": "26/03/16 12:52 AM",
      "commitName": "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "19/03/16 2:02 PM",
      "commitNameOld": "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 6.45,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,19 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n \n     LocatedBlocks blockLocations \u003d getBlockLocations(src, length);\n \n-    FileChecksumHelper.FileChecksumComputer maker \u003d\n+    FileChecksumHelper.FileChecksumComputer maker;\n+    ErasureCodingPolicy ecPolicy \u003d blockLocations.getErasureCodingPolicy();\n+    maker \u003d ecPolicy !\u003d null ?\n+        new FileChecksumHelper.StripedFileNonStripedChecksumComputer(src,\n+            length, blockLocations, namenode, this, ecPolicy) :\n         new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length,\n             blockLocations, namenode, this);\n \n     maker.compute();\n \n     return maker.getFileChecksum();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n\n    LocatedBlocks blockLocations \u003d getBlockLocations(src, length);\n\n    FileChecksumHelper.FileChecksumComputer maker;\n    ErasureCodingPolicy ecPolicy \u003d blockLocations.getErasureCodingPolicy();\n    maker \u003d ecPolicy !\u003d null ?\n        new FileChecksumHelper.StripedFileNonStripedChecksumComputer(src,\n            length, blockLocations, namenode, this, ecPolicy) :\n        new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length,\n            blockLocations, namenode, this);\n\n    maker.compute();\n\n    return maker.getFileChecksum();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9733. Refactor DFSClient#getFileChecksum and DataXceiver#blockChecksum. Contributed by Kai Zheng\n",
      "commitDate": "29/02/16 9:52 PM",
      "commitName": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/02/16 10:31 AM",
      "commitNameOld": "372d1302c63c6f49f99be5766c5da9647ebd9ca6",
      "commitAuthorOld": "Masatake Iwasaki",
      "daysBetweenCommits": 17.47,
      "commitsBetweenForRepo": 116,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,174 +1,15 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n-    //get block locations for the file range\n-    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n-        length);\n-    if (null \u003d\u003d blockLocations) {\n-      throw new FileNotFoundException(\"File does not exist: \" + src);\n-    }\n-    if (blockLocations.isUnderConstruction()) {\n-      throw new IOException(\"Fail to get checksum, since file \" + src\n-          + \" is under construction.\");\n-    }\n-    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n-    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n-    int bytesPerCRC \u003d -1;\n-    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n-    long crcPerBlock \u003d 0;\n-    boolean refetchBlocks \u003d false;\n-    int lastRetriedIndex \u003d -1;\n \n-    // get block checksum for each block\n-    long remaining \u003d length;\n-    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n-      remaining \u003d Math.min(length, blockLocations.getFileLength());\n-    }\n-    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n-      if (refetchBlocks) {  // refetch to get fresh tokens\n-        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n-        if (null \u003d\u003d blockLocations) {\n-          throw new FileNotFoundException(\"File does not exist: \" + src);\n-        }\n-        if (blockLocations.isUnderConstruction()) {\n-          throw new IOException(\"Fail to get checksum, since file \" + src\n-              + \" is under construction.\");\n-        }\n-        locatedblocks \u003d blockLocations.getLocatedBlocks();\n-        refetchBlocks \u003d false;\n-      }\n-      LocatedBlock lb \u003d locatedblocks.get(i);\n-      final ExtendedBlock block \u003d lb.getBlock();\n-      if (remaining \u003c block.getNumBytes()) {\n-        block.setNumBytes(remaining);\n-      }\n-      remaining -\u003d block.getNumBytes();\n-      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n+    LocatedBlocks blockLocations \u003d getBlockLocations(src, length);\n \n-      //try each datanode location of the block\n-      final int timeout \u003d 3000 * datanodes.length +\n-          dfsClientConf.getSocketTimeout();\n-      boolean done \u003d false;\n-      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n-        DataOutputStream out \u003d null;\n-        DataInputStream in \u003d null;\n+    FileChecksumHelper.FileChecksumComputer maker \u003d\n+        new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length,\n+            blockLocations, namenode, this);\n \n-        try {\n-          //connect to a datanode\n-          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n-          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n-              smallBufferSize));\n-          in \u003d new DataInputStream(pair.in);\n+    maker.compute();\n \n-          LOG.debug(\"write to {}: {}, block\u003d{}\",\n-              datanodes[j], Op.BLOCK_CHECKSUM, block);\n-          // get block MD5\n-          new Sender(out).blockChecksum(block, lb.getBlockToken());\n-\n-          final BlockOpResponseProto reply \u003d\n-              BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n-\n-          String logInfo \u003d \"for block \" + block + \" from datanode \" +\n-              datanodes[j];\n-          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n-\n-          OpBlockChecksumResponseProto checksumData \u003d\n-              reply.getChecksumResponse();\n-\n-          //read byte-per-checksum\n-          final int bpc \u003d checksumData.getBytesPerCrc();\n-          if (i \u003d\u003d 0) { //first block\n-            bytesPerCRC \u003d bpc;\n-          }\n-          else if (bpc !\u003d bytesPerCRC) {\n-            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n-                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n-          }\n-\n-          //read crc-per-block\n-          final long cpb \u003d checksumData.getCrcPerBlock();\n-          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n-            crcPerBlock \u003d cpb;\n-          }\n-\n-          //read md5\n-          final MD5Hash md5 \u003d new MD5Hash(\n-              checksumData.getMd5().toByteArray());\n-          md5.write(md5out);\n-\n-          // read crc-type\n-          final DataChecksum.Type ct;\n-          if (checksumData.hasCrcType()) {\n-            ct \u003d PBHelperClient.convert(checksumData\n-                .getCrcType());\n-          } else {\n-            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n-                \"inferring checksum by reading first byte\");\n-            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n-          }\n-\n-          if (i \u003d\u003d 0) { // first block\n-            crcType \u003d ct;\n-          } else if (crcType !\u003d DataChecksum.Type.MIXED\n-              \u0026\u0026 crcType !\u003d ct) {\n-            // if crc types are mixed in a file\n-            crcType \u003d DataChecksum.Type.MIXED;\n-          }\n-\n-          done \u003d true;\n-\n-          if (LOG.isDebugEnabled()) {\n-            if (i \u003d\u003d 0) {\n-              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n-                  + \", crcPerBlock\u003d\" + crcPerBlock);\n-            }\n-            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n-          }\n-        } catch (InvalidBlockTokenException ibte) {\n-          if (i \u003e lastRetriedIndex) {\n-            LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n-                    + \"for file {} for block {} from datanode {}. Will retry \"\n-                    + \"the block once.\",\n-                src, block, datanodes[j]);\n-            lastRetriedIndex \u003d i;\n-            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n-            i--; // repeat at i-th block\n-            refetchBlocks \u003d true;\n-            break;\n-          }\n-        } catch (IOException ie) {\n-          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n-        } finally {\n-          IOUtils.closeStream(in);\n-          IOUtils.closeStream(out);\n-        }\n-      }\n-\n-      if (!done) {\n-        throw new IOException(\"Fail to get block MD5 for \" + block);\n-      }\n-    }\n-\n-    //compute file MD5\n-    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData());\n-    switch (crcType) {\n-    case CRC32:\n-      return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n-          crcPerBlock, fileMD5);\n-    case CRC32C:\n-      return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n-          crcPerBlock, fileMD5);\n-    default:\n-      // If there is no block allocated for the file,\n-      // return one with the magic entry that matches what previous\n-      // hdfs versions return.\n-      if (locatedblocks.size() \u003d\u003d 0) {\n-        return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n-      }\n-\n-      // we should never get here since the validity was checked\n-      // when getCrcType() was called above.\n-      return null;\n-    }\n+    return maker.getFileChecksum();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n\n    LocatedBlocks blockLocations \u003d getBlockLocations(src, length);\n\n    FileChecksumHelper.FileChecksumComputer maker \u003d\n        new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length,\n            blockLocations, namenode, this);\n\n    maker.compute();\n\n    return maker.getFileChecksum();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,172 +1,174 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n     //get block locations for the file range\n     LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n         length);\n     if (null \u003d\u003d blockLocations) {\n       throw new FileNotFoundException(\"File does not exist: \" + src);\n     }\n     if (blockLocations.isUnderConstruction()) {\n       throw new IOException(\"Fail to get checksum, since file \" + src\n           + \" is under construction.\");\n     }\n     List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n     final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n     int bytesPerCRC \u003d -1;\n     DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n     long crcPerBlock \u003d 0;\n     boolean refetchBlocks \u003d false;\n     int lastRetriedIndex \u003d -1;\n \n     // get block checksum for each block\n     long remaining \u003d length;\n     if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n       remaining \u003d Math.min(length, blockLocations.getFileLength());\n     }\n     for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n       if (refetchBlocks) {  // refetch to get fresh tokens\n         blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n         if (null \u003d\u003d blockLocations) {\n           throw new FileNotFoundException(\"File does not exist: \" + src);\n         }\n         if (blockLocations.isUnderConstruction()) {\n           throw new IOException(\"Fail to get checksum, since file \" + src\n               + \" is under construction.\");\n         }\n         locatedblocks \u003d blockLocations.getLocatedBlocks();\n         refetchBlocks \u003d false;\n       }\n       LocatedBlock lb \u003d locatedblocks.get(i);\n       final ExtendedBlock block \u003d lb.getBlock();\n       if (remaining \u003c block.getNumBytes()) {\n         block.setNumBytes(remaining);\n       }\n       remaining -\u003d block.getNumBytes();\n       final DatanodeInfo[] datanodes \u003d lb.getLocations();\n-      \n+\n       //try each datanode location of the block\n-      final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n+      final int timeout \u003d 3000 * datanodes.length +\n+          dfsClientConf.getSocketTimeout();\n       boolean done \u003d false;\n       for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n         DataOutputStream out \u003d null;\n         DataInputStream in \u003d null;\n-        \n+\n         try {\n           //connect to a datanode\n           IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n           out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n               smallBufferSize));\n           in \u003d new DataInputStream(pair.in);\n \n           LOG.debug(\"write to {}: {}, block\u003d{}\",\n               datanodes[j], Op.BLOCK_CHECKSUM, block);\n           // get block MD5\n           new Sender(out).blockChecksum(block, lb.getBlockToken());\n \n           final BlockOpResponseProto reply \u003d\n-            BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n+              BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n \n-          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n+          String logInfo \u003d \"for block \" + block + \" from datanode \" +\n+              datanodes[j];\n           DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n           OpBlockChecksumResponseProto checksumData \u003d\n-            reply.getChecksumResponse();\n+              reply.getChecksumResponse();\n \n           //read byte-per-checksum\n           final int bpc \u003d checksumData.getBytesPerCrc();\n           if (i \u003d\u003d 0) { //first block\n             bytesPerCRC \u003d bpc;\n           }\n           else if (bpc !\u003d bytesPerCRC) {\n             throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                 + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n           }\n-          \n+\n           //read crc-per-block\n           final long cpb \u003d checksumData.getCrcPerBlock();\n           if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n             crcPerBlock \u003d cpb;\n           }\n \n           //read md5\n           final MD5Hash md5 \u003d new MD5Hash(\n               checksumData.getMd5().toByteArray());\n           md5.write(md5out);\n-          \n+\n           // read crc-type\n           final DataChecksum.Type ct;\n           if (checksumData.hasCrcType()) {\n             ct \u003d PBHelperClient.convert(checksumData\n                 .getCrcType());\n           } else {\n             LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n-                      \"inferring checksum by reading first byte\");\n+                \"inferring checksum by reading first byte\");\n             ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n           }\n \n           if (i \u003d\u003d 0) { // first block\n             crcType \u003d ct;\n           } else if (crcType !\u003d DataChecksum.Type.MIXED\n               \u0026\u0026 crcType !\u003d ct) {\n             // if crc types are mixed in a file\n             crcType \u003d DataChecksum.Type.MIXED;\n           }\n \n           done \u003d true;\n \n           if (LOG.isDebugEnabled()) {\n             if (i \u003d\u003d 0) {\n               LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                   + \", crcPerBlock\u003d\" + crcPerBlock);\n             }\n             LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n           }\n         } catch (InvalidBlockTokenException ibte) {\n           if (i \u003e lastRetriedIndex) {\n             LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n-                  + \"for file {} for block {} from datanode {}. Will retry the \"\n-                  + \"block once.\",\n+                    + \"for file {} for block {} from datanode {}. Will retry \"\n+                    + \"the block once.\",\n                 src, block, datanodes[j]);\n             lastRetriedIndex \u003d i;\n             done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n             i--; // repeat at i-th block\n             refetchBlocks \u003d true;\n             break;\n           }\n         } catch (IOException ie) {\n           LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n         } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n         }\n       }\n \n       if (!done) {\n         throw new IOException(\"Fail to get block MD5 for \" + block);\n       }\n     }\n \n     //compute file MD5\n-    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n+    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData());\n     switch (crcType) {\n-      case CRC32:\n-        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n-            crcPerBlock, fileMD5);\n-      case CRC32C:\n-        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n-            crcPerBlock, fileMD5);\n-      default:\n-        // If there is no block allocated for the file,\n-        // return one with the magic entry that matches what previous\n-        // hdfs versions return.\n-        if (locatedblocks.size() \u003d\u003d 0) {\n-          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n-        }\n+    case CRC32:\n+      return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n+          crcPerBlock, fileMD5);\n+    case CRC32C:\n+      return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n+          crcPerBlock, fileMD5);\n+    default:\n+      // If there is no block allocated for the file,\n+      // return one with the magic entry that matches what previous\n+      // hdfs versions return.\n+      if (locatedblocks.size() \u003d\u003d 0) {\n+        return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n+      }\n \n-        // we should never get here since the validity was checked\n-        // when getCrcType() was called above.\n-        return null;\n+      // we should never get here since the validity was checked\n+      // when getCrcType() was called above.\n+      return null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    if (blockLocations.isUnderConstruction()) {\n      throw new IOException(\"Fail to get checksum, since file \" + src\n          + \" is under construction.\");\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n      remaining \u003d Math.min(length, blockLocations.getFileLength());\n    }\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        if (blockLocations.isUnderConstruction()) {\n          throw new IOException(\"Fail to get checksum, since file \" + src\n              + \" is under construction.\");\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n\n      //try each datanode location of the block\n      final int timeout \u003d 3000 * datanodes.length +\n          dfsClientConf.getSocketTimeout();\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n\n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              smallBufferSize));\n          in \u003d new DataInputStream(pair.in);\n\n          LOG.debug(\"write to {}: {}, block\u003d{}\",\n              datanodes[j], Op.BLOCK_CHECKSUM, block);\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n              BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" +\n              datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n              reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n\n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n\n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelperClient.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                    + \"for file {} for block {} from datanode {}. Will retry \"\n                    + \"the block once.\",\n                src, block, datanodes[j]);\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData());\n    switch (crcType) {\n    case CRC32:\n      return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n          crcPerBlock, fileMD5);\n    case CRC32C:\n      return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n          crcPerBlock, fileMD5);\n    default:\n      // If there is no block allocated for the file,\n      // return one with the magic entry that matches what previous\n      // hdfs versions return.\n      if (locatedblocks.size() \u003d\u003d 0) {\n        return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n      }\n\n      // we should never get here since the validity was checked\n      // when getCrcType() was called above.\n      return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,176 +1,172 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n     //get block locations for the file range\n     LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n         length);\n     if (null \u003d\u003d blockLocations) {\n       throw new FileNotFoundException(\"File does not exist: \" + src);\n     }\n     if (blockLocations.isUnderConstruction()) {\n       throw new IOException(\"Fail to get checksum, since file \" + src\n           + \" is under construction.\");\n     }\n     List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n     final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n     int bytesPerCRC \u003d -1;\n     DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n     long crcPerBlock \u003d 0;\n     boolean refetchBlocks \u003d false;\n     int lastRetriedIndex \u003d -1;\n \n     // get block checksum for each block\n     long remaining \u003d length;\n     if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n       remaining \u003d Math.min(length, blockLocations.getFileLength());\n     }\n     for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n       if (refetchBlocks) {  // refetch to get fresh tokens\n         blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n         if (null \u003d\u003d blockLocations) {\n           throw new FileNotFoundException(\"File does not exist: \" + src);\n         }\n         if (blockLocations.isUnderConstruction()) {\n           throw new IOException(\"Fail to get checksum, since file \" + src\n               + \" is under construction.\");\n         }\n         locatedblocks \u003d blockLocations.getLocatedBlocks();\n         refetchBlocks \u003d false;\n       }\n       LocatedBlock lb \u003d locatedblocks.get(i);\n       final ExtendedBlock block \u003d lb.getBlock();\n       if (remaining \u003c block.getNumBytes()) {\n         block.setNumBytes(remaining);\n       }\n       remaining -\u003d block.getNumBytes();\n       final DatanodeInfo[] datanodes \u003d lb.getLocations();\n       \n       //try each datanode location of the block\n       final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n       boolean done \u003d false;\n       for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n         DataOutputStream out \u003d null;\n         DataInputStream in \u003d null;\n         \n         try {\n           //connect to a datanode\n           IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n           out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n               smallBufferSize));\n           in \u003d new DataInputStream(pair.in);\n \n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"write to \" + datanodes[j] + \": \"\n-                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n-          }\n+          LOG.debug(\"write to {}: {}, block\u003d{}\",\n+              datanodes[j], Op.BLOCK_CHECKSUM, block);\n           // get block MD5\n           new Sender(out).blockChecksum(block, lb.getBlockToken());\n \n           final BlockOpResponseProto reply \u003d\n             BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n \n           String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n           DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n           OpBlockChecksumResponseProto checksumData \u003d\n             reply.getChecksumResponse();\n \n           //read byte-per-checksum\n           final int bpc \u003d checksumData.getBytesPerCrc();\n           if (i \u003d\u003d 0) { //first block\n             bytesPerCRC \u003d bpc;\n           }\n           else if (bpc !\u003d bytesPerCRC) {\n             throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                 + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n           }\n           \n           //read crc-per-block\n           final long cpb \u003d checksumData.getCrcPerBlock();\n           if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n             crcPerBlock \u003d cpb;\n           }\n \n           //read md5\n           final MD5Hash md5 \u003d new MD5Hash(\n               checksumData.getMd5().toByteArray());\n           md5.write(md5out);\n           \n           // read crc-type\n           final DataChecksum.Type ct;\n           if (checksumData.hasCrcType()) {\n             ct \u003d PBHelperClient.convert(checksumData\n                 .getCrcType());\n           } else {\n             LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                       \"inferring checksum by reading first byte\");\n             ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n           }\n \n           if (i \u003d\u003d 0) { // first block\n             crcType \u003d ct;\n           } else if (crcType !\u003d DataChecksum.Type.MIXED\n               \u0026\u0026 crcType !\u003d ct) {\n             // if crc types are mixed in a file\n             crcType \u003d DataChecksum.Type.MIXED;\n           }\n \n           done \u003d true;\n \n           if (LOG.isDebugEnabled()) {\n             if (i \u003d\u003d 0) {\n               LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                   + \", crcPerBlock\u003d\" + crcPerBlock);\n             }\n             LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n           }\n         } catch (InvalidBlockTokenException ibte) {\n           if (i \u003e lastRetriedIndex) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n-                  + \"for file \" + src + \" for block \" + block\n-                  + \" from datanode \" + datanodes[j]\n-                  + \". Will retry the block once.\");\n-            }\n+            LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n+                  + \"for file {} for block {} from datanode {}. Will retry the \"\n+                  + \"block once.\",\n+                src, block, datanodes[j]);\n             lastRetriedIndex \u003d i;\n             done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n             i--; // repeat at i-th block\n             refetchBlocks \u003d true;\n             break;\n           }\n         } catch (IOException ie) {\n           LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n         } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n         }\n       }\n \n       if (!done) {\n         throw new IOException(\"Fail to get block MD5 for \" + block);\n       }\n     }\n \n     //compute file MD5\n     final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n     switch (crcType) {\n       case CRC32:\n         return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       case CRC32C:\n         return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       default:\n         // If there is no block allocated for the file,\n         // return one with the magic entry that matches what previous\n         // hdfs versions return.\n         if (locatedblocks.size() \u003d\u003d 0) {\n           return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n         }\n \n         // we should never get here since the validity was checked\n         // when getCrcType() was called above.\n         return null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    if (blockLocations.isUnderConstruction()) {\n      throw new IOException(\"Fail to get checksum, since file \" + src\n          + \" is under construction.\");\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n      remaining \u003d Math.min(length, blockLocations.getFileLength());\n    }\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        if (blockLocations.isUnderConstruction()) {\n          throw new IOException(\"Fail to get checksum, since file \" + src\n              + \" is under construction.\");\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              smallBufferSize));\n          in \u003d new DataInputStream(pair.in);\n\n          LOG.debug(\"write to {}: {}, block\u003d{}\",\n              datanodes[j], Op.BLOCK_CHECKSUM, block);\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelperClient.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file {} for block {} from datanode {}. Will retry the \"\n                  + \"block once.\",\n                src, block, datanodes[j]);\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,172 +1,176 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n     //get block locations for the file range\n     LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n         length);\n     if (null \u003d\u003d blockLocations) {\n       throw new FileNotFoundException(\"File does not exist: \" + src);\n     }\n     if (blockLocations.isUnderConstruction()) {\n       throw new IOException(\"Fail to get checksum, since file \" + src\n           + \" is under construction.\");\n     }\n     List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n     final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n     int bytesPerCRC \u003d -1;\n     DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n     long crcPerBlock \u003d 0;\n     boolean refetchBlocks \u003d false;\n     int lastRetriedIndex \u003d -1;\n \n     // get block checksum for each block\n     long remaining \u003d length;\n     if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n       remaining \u003d Math.min(length, blockLocations.getFileLength());\n     }\n     for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n       if (refetchBlocks) {  // refetch to get fresh tokens\n         blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n         if (null \u003d\u003d blockLocations) {\n           throw new FileNotFoundException(\"File does not exist: \" + src);\n         }\n         if (blockLocations.isUnderConstruction()) {\n           throw new IOException(\"Fail to get checksum, since file \" + src\n               + \" is under construction.\");\n         }\n         locatedblocks \u003d blockLocations.getLocatedBlocks();\n         refetchBlocks \u003d false;\n       }\n       LocatedBlock lb \u003d locatedblocks.get(i);\n       final ExtendedBlock block \u003d lb.getBlock();\n       if (remaining \u003c block.getNumBytes()) {\n         block.setNumBytes(remaining);\n       }\n       remaining -\u003d block.getNumBytes();\n       final DatanodeInfo[] datanodes \u003d lb.getLocations();\n       \n       //try each datanode location of the block\n       final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n       boolean done \u003d false;\n       for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n         DataOutputStream out \u003d null;\n         DataInputStream in \u003d null;\n         \n         try {\n           //connect to a datanode\n           IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n           out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n               smallBufferSize));\n           in \u003d new DataInputStream(pair.in);\n \n-          LOG.debug(\"write to {}: {}, block\u003d{}\",\n-              datanodes[j], Op.BLOCK_CHECKSUM, block);\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"write to \" + datanodes[j] + \": \"\n+                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n+          }\n           // get block MD5\n           new Sender(out).blockChecksum(block, lb.getBlockToken());\n \n           final BlockOpResponseProto reply \u003d\n             BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n \n           String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n           DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n           OpBlockChecksumResponseProto checksumData \u003d\n             reply.getChecksumResponse();\n \n           //read byte-per-checksum\n           final int bpc \u003d checksumData.getBytesPerCrc();\n           if (i \u003d\u003d 0) { //first block\n             bytesPerCRC \u003d bpc;\n           }\n           else if (bpc !\u003d bytesPerCRC) {\n             throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                 + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n           }\n           \n           //read crc-per-block\n           final long cpb \u003d checksumData.getCrcPerBlock();\n           if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n             crcPerBlock \u003d cpb;\n           }\n \n           //read md5\n           final MD5Hash md5 \u003d new MD5Hash(\n               checksumData.getMd5().toByteArray());\n           md5.write(md5out);\n           \n           // read crc-type\n           final DataChecksum.Type ct;\n           if (checksumData.hasCrcType()) {\n             ct \u003d PBHelperClient.convert(checksumData\n                 .getCrcType());\n           } else {\n             LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                       \"inferring checksum by reading first byte\");\n             ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n           }\n \n           if (i \u003d\u003d 0) { // first block\n             crcType \u003d ct;\n           } else if (crcType !\u003d DataChecksum.Type.MIXED\n               \u0026\u0026 crcType !\u003d ct) {\n             // if crc types are mixed in a file\n             crcType \u003d DataChecksum.Type.MIXED;\n           }\n \n           done \u003d true;\n \n           if (LOG.isDebugEnabled()) {\n             if (i \u003d\u003d 0) {\n               LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                   + \", crcPerBlock\u003d\" + crcPerBlock);\n             }\n             LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n           }\n         } catch (InvalidBlockTokenException ibte) {\n           if (i \u003e lastRetriedIndex) {\n-            LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n-                  + \"for file {} for block {} from datanode {}. Will retry the \"\n-                  + \"block once.\",\n-                src, block, datanodes[j]);\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n+                  + \"for file \" + src + \" for block \" + block\n+                  + \" from datanode \" + datanodes[j]\n+                  + \". Will retry the block once.\");\n+            }\n             lastRetriedIndex \u003d i;\n             done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n             i--; // repeat at i-th block\n             refetchBlocks \u003d true;\n             break;\n           }\n         } catch (IOException ie) {\n           LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n         } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n         }\n       }\n \n       if (!done) {\n         throw new IOException(\"Fail to get block MD5 for \" + block);\n       }\n     }\n \n     //compute file MD5\n     final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n     switch (crcType) {\n       case CRC32:\n         return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       case CRC32C:\n         return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       default:\n         // If there is no block allocated for the file,\n         // return one with the magic entry that matches what previous\n         // hdfs versions return.\n         if (locatedblocks.size() \u003d\u003d 0) {\n           return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n         }\n \n         // we should never get here since the validity was checked\n         // when getCrcType() was called above.\n         return null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    if (blockLocations.isUnderConstruction()) {\n      throw new IOException(\"Fail to get checksum, since file \" + src\n          + \" is under construction.\");\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n      remaining \u003d Math.min(length, blockLocations.getFileLength());\n    }\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        if (blockLocations.isUnderConstruction()) {\n          throw new IOException(\"Fail to get checksum, since file \" + src\n              + \" is under construction.\");\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              smallBufferSize));\n          in \u003d new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelperClient.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,176 +1,172 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n     //get block locations for the file range\n     LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n         length);\n     if (null \u003d\u003d blockLocations) {\n       throw new FileNotFoundException(\"File does not exist: \" + src);\n     }\n     if (blockLocations.isUnderConstruction()) {\n       throw new IOException(\"Fail to get checksum, since file \" + src\n           + \" is under construction.\");\n     }\n     List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n     final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n     int bytesPerCRC \u003d -1;\n     DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n     long crcPerBlock \u003d 0;\n     boolean refetchBlocks \u003d false;\n     int lastRetriedIndex \u003d -1;\n \n     // get block checksum for each block\n     long remaining \u003d length;\n     if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n       remaining \u003d Math.min(length, blockLocations.getFileLength());\n     }\n     for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n       if (refetchBlocks) {  // refetch to get fresh tokens\n         blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n         if (null \u003d\u003d blockLocations) {\n           throw new FileNotFoundException(\"File does not exist: \" + src);\n         }\n         if (blockLocations.isUnderConstruction()) {\n           throw new IOException(\"Fail to get checksum, since file \" + src\n               + \" is under construction.\");\n         }\n         locatedblocks \u003d blockLocations.getLocatedBlocks();\n         refetchBlocks \u003d false;\n       }\n       LocatedBlock lb \u003d locatedblocks.get(i);\n       final ExtendedBlock block \u003d lb.getBlock();\n       if (remaining \u003c block.getNumBytes()) {\n         block.setNumBytes(remaining);\n       }\n       remaining -\u003d block.getNumBytes();\n       final DatanodeInfo[] datanodes \u003d lb.getLocations();\n       \n       //try each datanode location of the block\n       final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n       boolean done \u003d false;\n       for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n         DataOutputStream out \u003d null;\n         DataInputStream in \u003d null;\n         \n         try {\n           //connect to a datanode\n           IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n           out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n               smallBufferSize));\n           in \u003d new DataInputStream(pair.in);\n \n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"write to \" + datanodes[j] + \": \"\n-                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n-          }\n+          LOG.debug(\"write to {}: {}, block\u003d{}\",\n+              datanodes[j], Op.BLOCK_CHECKSUM, block);\n           // get block MD5\n           new Sender(out).blockChecksum(block, lb.getBlockToken());\n \n           final BlockOpResponseProto reply \u003d\n             BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n \n           String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n           DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n           OpBlockChecksumResponseProto checksumData \u003d\n             reply.getChecksumResponse();\n \n           //read byte-per-checksum\n           final int bpc \u003d checksumData.getBytesPerCrc();\n           if (i \u003d\u003d 0) { //first block\n             bytesPerCRC \u003d bpc;\n           }\n           else if (bpc !\u003d bytesPerCRC) {\n             throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                 + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n           }\n           \n           //read crc-per-block\n           final long cpb \u003d checksumData.getCrcPerBlock();\n           if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n             crcPerBlock \u003d cpb;\n           }\n \n           //read md5\n           final MD5Hash md5 \u003d new MD5Hash(\n               checksumData.getMd5().toByteArray());\n           md5.write(md5out);\n           \n           // read crc-type\n           final DataChecksum.Type ct;\n           if (checksumData.hasCrcType()) {\n             ct \u003d PBHelperClient.convert(checksumData\n                 .getCrcType());\n           } else {\n             LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                       \"inferring checksum by reading first byte\");\n             ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n           }\n \n           if (i \u003d\u003d 0) { // first block\n             crcType \u003d ct;\n           } else if (crcType !\u003d DataChecksum.Type.MIXED\n               \u0026\u0026 crcType !\u003d ct) {\n             // if crc types are mixed in a file\n             crcType \u003d DataChecksum.Type.MIXED;\n           }\n \n           done \u003d true;\n \n           if (LOG.isDebugEnabled()) {\n             if (i \u003d\u003d 0) {\n               LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                   + \", crcPerBlock\u003d\" + crcPerBlock);\n             }\n             LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n           }\n         } catch (InvalidBlockTokenException ibte) {\n           if (i \u003e lastRetriedIndex) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n-                  + \"for file \" + src + \" for block \" + block\n-                  + \" from datanode \" + datanodes[j]\n-                  + \". Will retry the block once.\");\n-            }\n+            LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n+                  + \"for file {} for block {} from datanode {}. Will retry the \"\n+                  + \"block once.\",\n+                src, block, datanodes[j]);\n             lastRetriedIndex \u003d i;\n             done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n             i--; // repeat at i-th block\n             refetchBlocks \u003d true;\n             break;\n           }\n         } catch (IOException ie) {\n           LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n         } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n         }\n       }\n \n       if (!done) {\n         throw new IOException(\"Fail to get block MD5 for \" + block);\n       }\n     }\n \n     //compute file MD5\n     final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n     switch (crcType) {\n       case CRC32:\n         return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       case CRC32C:\n         return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       default:\n         // If there is no block allocated for the file,\n         // return one with the magic entry that matches what previous\n         // hdfs versions return.\n         if (locatedblocks.size() \u003d\u003d 0) {\n           return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n         }\n \n         // we should never get here since the validity was checked\n         // when getCrcType() was called above.\n         return null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    if (blockLocations.isUnderConstruction()) {\n      throw new IOException(\"Fail to get checksum, since file \" + src\n          + \" is under construction.\");\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n      remaining \u003d Math.min(length, blockLocations.getFileLength());\n    }\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        if (blockLocations.isUnderConstruction()) {\n          throw new IOException(\"Fail to get checksum, since file \" + src\n              + \" is under construction.\");\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              smallBufferSize));\n          in \u003d new DataInputStream(pair.in);\n\n          LOG.debug(\"write to {}: {}, block\u003d{}\",\n              datanodes[j], Op.BLOCK_CHECKSUM, block);\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelperClient.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file {} for block {} from datanode {}. Will retry the \"\n                  + \"block once.\",\n                src, block, datanodes[j]);\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    if (blockLocations.isUnderConstruction()) {\n      throw new IOException(\"Fail to get checksum, since file \" + src\n          + \" is under construction.\");\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n      remaining \u003d Math.min(length, blockLocations.getFileLength());\n    }\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        if (blockLocations.isUnderConstruction()) {\n          throw new IOException(\"Fail to get checksum, since file \" + src\n              + \" is under construction.\");\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              smallBufferSize));\n          in \u003d new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelperClient.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java"
      }
    },
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8934. Move ShortCircuitShm to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "22/08/15 1:31 PM",
      "commitName": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/08/15 11:28 AM",
      "commitNameOld": "3aac4758b007a56e3d66998d457b2156effca528",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.09,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,176 +1,176 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n     //get block locations for the file range\n     LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n         length);\n     if (null \u003d\u003d blockLocations) {\n       throw new FileNotFoundException(\"File does not exist: \" + src);\n     }\n     if (blockLocations.isUnderConstruction()) {\n       throw new IOException(\"Fail to get checksum, since file \" + src\n           + \" is under construction.\");\n     }\n     List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n     final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n     int bytesPerCRC \u003d -1;\n     DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n     long crcPerBlock \u003d 0;\n     boolean refetchBlocks \u003d false;\n     int lastRetriedIndex \u003d -1;\n \n     // get block checksum for each block\n     long remaining \u003d length;\n     if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n       remaining \u003d Math.min(length, blockLocations.getFileLength());\n     }\n     for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n       if (refetchBlocks) {  // refetch to get fresh tokens\n         blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n         if (null \u003d\u003d blockLocations) {\n           throw new FileNotFoundException(\"File does not exist: \" + src);\n         }\n         if (blockLocations.isUnderConstruction()) {\n           throw new IOException(\"Fail to get checksum, since file \" + src\n               + \" is under construction.\");\n         }\n         locatedblocks \u003d blockLocations.getLocatedBlocks();\n         refetchBlocks \u003d false;\n       }\n       LocatedBlock lb \u003d locatedblocks.get(i);\n       final ExtendedBlock block \u003d lb.getBlock();\n       if (remaining \u003c block.getNumBytes()) {\n         block.setNumBytes(remaining);\n       }\n       remaining -\u003d block.getNumBytes();\n       final DatanodeInfo[] datanodes \u003d lb.getLocations();\n       \n       //try each datanode location of the block\n       final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n       boolean done \u003d false;\n       for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n         DataOutputStream out \u003d null;\n         DataInputStream in \u003d null;\n         \n         try {\n           //connect to a datanode\n           IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n           out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n               smallBufferSize));\n           in \u003d new DataInputStream(pair.in);\n \n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"write to \" + datanodes[j] + \": \"\n                 + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n           }\n           // get block MD5\n           new Sender(out).blockChecksum(block, lb.getBlockToken());\n \n           final BlockOpResponseProto reply \u003d\n-            BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n+            BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n \n           String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n           DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n           OpBlockChecksumResponseProto checksumData \u003d\n             reply.getChecksumResponse();\n \n           //read byte-per-checksum\n           final int bpc \u003d checksumData.getBytesPerCrc();\n           if (i \u003d\u003d 0) { //first block\n             bytesPerCRC \u003d bpc;\n           }\n           else if (bpc !\u003d bytesPerCRC) {\n             throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                 + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n           }\n           \n           //read crc-per-block\n           final long cpb \u003d checksumData.getCrcPerBlock();\n           if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n             crcPerBlock \u003d cpb;\n           }\n \n           //read md5\n           final MD5Hash md5 \u003d new MD5Hash(\n               checksumData.getMd5().toByteArray());\n           md5.write(md5out);\n           \n           // read crc-type\n           final DataChecksum.Type ct;\n           if (checksumData.hasCrcType()) {\n-            ct \u003d PBHelper.convert(checksumData\n+            ct \u003d PBHelperClient.convert(checksumData\n                 .getCrcType());\n           } else {\n             LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                       \"inferring checksum by reading first byte\");\n             ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n           }\n \n           if (i \u003d\u003d 0) { // first block\n             crcType \u003d ct;\n           } else if (crcType !\u003d DataChecksum.Type.MIXED\n               \u0026\u0026 crcType !\u003d ct) {\n             // if crc types are mixed in a file\n             crcType \u003d DataChecksum.Type.MIXED;\n           }\n \n           done \u003d true;\n \n           if (LOG.isDebugEnabled()) {\n             if (i \u003d\u003d 0) {\n               LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                   + \", crcPerBlock\u003d\" + crcPerBlock);\n             }\n             LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n           }\n         } catch (InvalidBlockTokenException ibte) {\n           if (i \u003e lastRetriedIndex) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                   + \"for file \" + src + \" for block \" + block\n                   + \" from datanode \" + datanodes[j]\n                   + \". Will retry the block once.\");\n             }\n             lastRetriedIndex \u003d i;\n             done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n             i--; // repeat at i-th block\n             refetchBlocks \u003d true;\n             break;\n           }\n         } catch (IOException ie) {\n           LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n         } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n         }\n       }\n \n       if (!done) {\n         throw new IOException(\"Fail to get block MD5 for \" + block);\n       }\n     }\n \n     //compute file MD5\n     final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n     switch (crcType) {\n       case CRC32:\n         return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       case CRC32C:\n         return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       default:\n         // If there is no block allocated for the file,\n         // return one with the magic entry that matches what previous\n         // hdfs versions return.\n         if (locatedblocks.size() \u003d\u003d 0) {\n           return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n         }\n \n         // we should never get here since the validity was checked\n         // when getCrcType() was called above.\n         return null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    if (blockLocations.isUnderConstruction()) {\n      throw new IOException(\"Fail to get checksum, since file \" + src\n          + \" is under construction.\");\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n      remaining \u003d Math.min(length, blockLocations.getFileLength());\n    }\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        if (blockLocations.isUnderConstruction()) {\n          throw new IOException(\"Fail to get checksum, since file \" + src\n              + \" is under construction.\");\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              smallBufferSize));\n          in \u003d new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelperClient.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "def9136e0259e118e6fd7b656260765d28ac9ae6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8150. Make getFileChecksum fail for blocks under construction (Contributed by J.Andreina)\n",
      "commitDate": "14/05/15 3:24 AM",
      "commitName": "def9136e0259e118e6fd7b656260765d28ac9ae6",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "07/05/15 11:56 PM",
      "commitNameOld": "e16f4b7f70b8675760cf5aaa471dfe29d48041e6",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 6.14,
      "commitsBetweenForRepo": 116,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,168 +1,176 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n     //get block locations for the file range\n     LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n         length);\n     if (null \u003d\u003d blockLocations) {\n       throw new FileNotFoundException(\"File does not exist: \" + src);\n     }\n+    if (blockLocations.isUnderConstruction()) {\n+      throw new IOException(\"Fail to get checksum, since file \" + src\n+          + \" is under construction.\");\n+    }\n     List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n     final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n     int bytesPerCRC \u003d -1;\n     DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n     long crcPerBlock \u003d 0;\n     boolean refetchBlocks \u003d false;\n     int lastRetriedIndex \u003d -1;\n \n     // get block checksum for each block\n     long remaining \u003d length;\n     if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n       remaining \u003d Math.min(length, blockLocations.getFileLength());\n     }\n     for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n       if (refetchBlocks) {  // refetch to get fresh tokens\n         blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n         if (null \u003d\u003d blockLocations) {\n           throw new FileNotFoundException(\"File does not exist: \" + src);\n         }\n+        if (blockLocations.isUnderConstruction()) {\n+          throw new IOException(\"Fail to get checksum, since file \" + src\n+              + \" is under construction.\");\n+        }\n         locatedblocks \u003d blockLocations.getLocatedBlocks();\n         refetchBlocks \u003d false;\n       }\n       LocatedBlock lb \u003d locatedblocks.get(i);\n       final ExtendedBlock block \u003d lb.getBlock();\n       if (remaining \u003c block.getNumBytes()) {\n         block.setNumBytes(remaining);\n       }\n       remaining -\u003d block.getNumBytes();\n       final DatanodeInfo[] datanodes \u003d lb.getLocations();\n       \n       //try each datanode location of the block\n       final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n       boolean done \u003d false;\n       for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n         DataOutputStream out \u003d null;\n         DataInputStream in \u003d null;\n         \n         try {\n           //connect to a datanode\n           IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n           out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n               smallBufferSize));\n           in \u003d new DataInputStream(pair.in);\n \n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"write to \" + datanodes[j] + \": \"\n                 + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n           }\n           // get block MD5\n           new Sender(out).blockChecksum(block, lb.getBlockToken());\n \n           final BlockOpResponseProto reply \u003d\n             BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n \n           String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n           DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n           OpBlockChecksumResponseProto checksumData \u003d\n             reply.getChecksumResponse();\n \n           //read byte-per-checksum\n           final int bpc \u003d checksumData.getBytesPerCrc();\n           if (i \u003d\u003d 0) { //first block\n             bytesPerCRC \u003d bpc;\n           }\n           else if (bpc !\u003d bytesPerCRC) {\n             throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                 + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n           }\n           \n           //read crc-per-block\n           final long cpb \u003d checksumData.getCrcPerBlock();\n           if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n             crcPerBlock \u003d cpb;\n           }\n \n           //read md5\n           final MD5Hash md5 \u003d new MD5Hash(\n               checksumData.getMd5().toByteArray());\n           md5.write(md5out);\n           \n           // read crc-type\n           final DataChecksum.Type ct;\n           if (checksumData.hasCrcType()) {\n             ct \u003d PBHelper.convert(checksumData\n                 .getCrcType());\n           } else {\n             LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                       \"inferring checksum by reading first byte\");\n             ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n           }\n \n           if (i \u003d\u003d 0) { // first block\n             crcType \u003d ct;\n           } else if (crcType !\u003d DataChecksum.Type.MIXED\n               \u0026\u0026 crcType !\u003d ct) {\n             // if crc types are mixed in a file\n             crcType \u003d DataChecksum.Type.MIXED;\n           }\n \n           done \u003d true;\n \n           if (LOG.isDebugEnabled()) {\n             if (i \u003d\u003d 0) {\n               LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                   + \", crcPerBlock\u003d\" + crcPerBlock);\n             }\n             LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n           }\n         } catch (InvalidBlockTokenException ibte) {\n           if (i \u003e lastRetriedIndex) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                   + \"for file \" + src + \" for block \" + block\n                   + \" from datanode \" + datanodes[j]\n                   + \". Will retry the block once.\");\n             }\n             lastRetriedIndex \u003d i;\n             done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n             i--; // repeat at i-th block\n             refetchBlocks \u003d true;\n             break;\n           }\n         } catch (IOException ie) {\n           LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n         } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n         }\n       }\n \n       if (!done) {\n         throw new IOException(\"Fail to get block MD5 for \" + block);\n       }\n     }\n \n     //compute file MD5\n     final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n     switch (crcType) {\n       case CRC32:\n         return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       case CRC32C:\n         return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       default:\n         // If there is no block allocated for the file,\n         // return one with the magic entry that matches what previous\n         // hdfs versions return.\n         if (locatedblocks.size() \u003d\u003d 0) {\n           return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n         }\n \n         // we should never get here since the validity was checked\n         // when getCrcType() was called above.\n         return null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    if (blockLocations.isUnderConstruction()) {\n      throw new IOException(\"Fail to get checksum, since file \" + src\n          + \" is under construction.\");\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n      remaining \u003d Math.min(length, blockLocations.getFileLength());\n    }\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        if (blockLocations.isUnderConstruction()) {\n          throw new IOException(\"Fail to get checksum, since file \" + src\n              + \" is under construction.\");\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              smallBufferSize));\n          in \u003d new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelper.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "4da8490b512a33a255ed27309860859388d7c168": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
      "commitDate": "05/05/15 3:41 PM",
      "commitName": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.23,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,168 +1,168 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n     //get block locations for the file range\n     LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n         length);\n     if (null \u003d\u003d blockLocations) {\n       throw new FileNotFoundException(\"File does not exist: \" + src);\n     }\n     List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n     final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n     int bytesPerCRC \u003d -1;\n     DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n     long crcPerBlock \u003d 0;\n     boolean refetchBlocks \u003d false;\n     int lastRetriedIndex \u003d -1;\n \n     // get block checksum for each block\n     long remaining \u003d length;\n     if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n       remaining \u003d Math.min(length, blockLocations.getFileLength());\n     }\n     for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n       if (refetchBlocks) {  // refetch to get fresh tokens\n         blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n         if (null \u003d\u003d blockLocations) {\n           throw new FileNotFoundException(\"File does not exist: \" + src);\n         }\n         locatedblocks \u003d blockLocations.getLocatedBlocks();\n         refetchBlocks \u003d false;\n       }\n       LocatedBlock lb \u003d locatedblocks.get(i);\n       final ExtendedBlock block \u003d lb.getBlock();\n       if (remaining \u003c block.getNumBytes()) {\n         block.setNumBytes(remaining);\n       }\n       remaining -\u003d block.getNumBytes();\n       final DatanodeInfo[] datanodes \u003d lb.getLocations();\n       \n       //try each datanode location of the block\n       final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n       boolean done \u003d false;\n       for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n         DataOutputStream out \u003d null;\n         DataInputStream in \u003d null;\n         \n         try {\n           //connect to a datanode\n           IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n           out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n-              HdfsServerConstants.SMALL_BUFFER_SIZE));\n+              smallBufferSize));\n           in \u003d new DataInputStream(pair.in);\n \n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"write to \" + datanodes[j] + \": \"\n                 + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n           }\n           // get block MD5\n           new Sender(out).blockChecksum(block, lb.getBlockToken());\n \n           final BlockOpResponseProto reply \u003d\n             BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n \n           String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n           DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n           OpBlockChecksumResponseProto checksumData \u003d\n             reply.getChecksumResponse();\n \n           //read byte-per-checksum\n           final int bpc \u003d checksumData.getBytesPerCrc();\n           if (i \u003d\u003d 0) { //first block\n             bytesPerCRC \u003d bpc;\n           }\n           else if (bpc !\u003d bytesPerCRC) {\n             throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                 + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n           }\n           \n           //read crc-per-block\n           final long cpb \u003d checksumData.getCrcPerBlock();\n           if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n             crcPerBlock \u003d cpb;\n           }\n \n           //read md5\n           final MD5Hash md5 \u003d new MD5Hash(\n               checksumData.getMd5().toByteArray());\n           md5.write(md5out);\n           \n           // read crc-type\n           final DataChecksum.Type ct;\n           if (checksumData.hasCrcType()) {\n             ct \u003d PBHelper.convert(checksumData\n                 .getCrcType());\n           } else {\n             LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                       \"inferring checksum by reading first byte\");\n             ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n           }\n \n           if (i \u003d\u003d 0) { // first block\n             crcType \u003d ct;\n           } else if (crcType !\u003d DataChecksum.Type.MIXED\n               \u0026\u0026 crcType !\u003d ct) {\n             // if crc types are mixed in a file\n             crcType \u003d DataChecksum.Type.MIXED;\n           }\n \n           done \u003d true;\n \n           if (LOG.isDebugEnabled()) {\n             if (i \u003d\u003d 0) {\n               LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                   + \", crcPerBlock\u003d\" + crcPerBlock);\n             }\n             LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n           }\n         } catch (InvalidBlockTokenException ibte) {\n           if (i \u003e lastRetriedIndex) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                   + \"for file \" + src + \" for block \" + block\n                   + \" from datanode \" + datanodes[j]\n                   + \". Will retry the block once.\");\n             }\n             lastRetriedIndex \u003d i;\n             done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n             i--; // repeat at i-th block\n             refetchBlocks \u003d true;\n             break;\n           }\n         } catch (IOException ie) {\n           LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n         } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n         }\n       }\n \n       if (!done) {\n         throw new IOException(\"Fail to get block MD5 for \" + block);\n       }\n     }\n \n     //compute file MD5\n     final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n     switch (crcType) {\n       case CRC32:\n         return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       case CRC32C:\n         return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       default:\n         // If there is no block allocated for the file,\n         // return one with the magic entry that matches what previous\n         // hdfs versions return.\n         if (locatedblocks.size() \u003d\u003d 0) {\n           return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n         }\n \n         // we should never get here since the validity was checked\n         // when getCrcType() was called above.\n         return null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n      remaining \u003d Math.min(length, blockLocations.getFileLength());\n    }\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              smallBufferSize));\n          in \u003d new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelper.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "01/05/15 3:12 PM",
      "commitNameOld": "d3d019c337ecc10e9c6bbefc3a97c6cd1f5283c3",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,168 +1,168 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n     //get block locations for the file range\n     LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n         length);\n     if (null \u003d\u003d blockLocations) {\n       throw new FileNotFoundException(\"File does not exist: \" + src);\n     }\n     List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n     final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n     int bytesPerCRC \u003d -1;\n     DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n     long crcPerBlock \u003d 0;\n     boolean refetchBlocks \u003d false;\n     int lastRetriedIndex \u003d -1;\n \n     // get block checksum for each block\n     long remaining \u003d length;\n     if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n       remaining \u003d Math.min(length, blockLocations.getFileLength());\n     }\n     for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n       if (refetchBlocks) {  // refetch to get fresh tokens\n         blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n         if (null \u003d\u003d blockLocations) {\n           throw new FileNotFoundException(\"File does not exist: \" + src);\n         }\n         locatedblocks \u003d blockLocations.getLocatedBlocks();\n         refetchBlocks \u003d false;\n       }\n       LocatedBlock lb \u003d locatedblocks.get(i);\n       final ExtendedBlock block \u003d lb.getBlock();\n       if (remaining \u003c block.getNumBytes()) {\n         block.setNumBytes(remaining);\n       }\n       remaining -\u003d block.getNumBytes();\n       final DatanodeInfo[] datanodes \u003d lb.getLocations();\n       \n       //try each datanode location of the block\n       final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n       boolean done \u003d false;\n       for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n         DataOutputStream out \u003d null;\n         DataInputStream in \u003d null;\n         \n         try {\n           //connect to a datanode\n           IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n           out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n-              HdfsConstants.SMALL_BUFFER_SIZE));\n+              HdfsServerConstants.SMALL_BUFFER_SIZE));\n           in \u003d new DataInputStream(pair.in);\n \n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"write to \" + datanodes[j] + \": \"\n                 + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n           }\n           // get block MD5\n           new Sender(out).blockChecksum(block, lb.getBlockToken());\n \n           final BlockOpResponseProto reply \u003d\n             BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n \n           String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n           DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n           OpBlockChecksumResponseProto checksumData \u003d\n             reply.getChecksumResponse();\n \n           //read byte-per-checksum\n           final int bpc \u003d checksumData.getBytesPerCrc();\n           if (i \u003d\u003d 0) { //first block\n             bytesPerCRC \u003d bpc;\n           }\n           else if (bpc !\u003d bytesPerCRC) {\n             throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                 + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n           }\n           \n           //read crc-per-block\n           final long cpb \u003d checksumData.getCrcPerBlock();\n           if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n             crcPerBlock \u003d cpb;\n           }\n \n           //read md5\n           final MD5Hash md5 \u003d new MD5Hash(\n               checksumData.getMd5().toByteArray());\n           md5.write(md5out);\n           \n           // read crc-type\n           final DataChecksum.Type ct;\n           if (checksumData.hasCrcType()) {\n             ct \u003d PBHelper.convert(checksumData\n                 .getCrcType());\n           } else {\n             LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                       \"inferring checksum by reading first byte\");\n             ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n           }\n \n           if (i \u003d\u003d 0) { // first block\n             crcType \u003d ct;\n           } else if (crcType !\u003d DataChecksum.Type.MIXED\n               \u0026\u0026 crcType !\u003d ct) {\n             // if crc types are mixed in a file\n             crcType \u003d DataChecksum.Type.MIXED;\n           }\n \n           done \u003d true;\n \n           if (LOG.isDebugEnabled()) {\n             if (i \u003d\u003d 0) {\n               LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                   + \", crcPerBlock\u003d\" + crcPerBlock);\n             }\n             LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n           }\n         } catch (InvalidBlockTokenException ibte) {\n           if (i \u003e lastRetriedIndex) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                   + \"for file \" + src + \" for block \" + block\n                   + \" from datanode \" + datanodes[j]\n                   + \". Will retry the block once.\");\n             }\n             lastRetriedIndex \u003d i;\n             done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n             i--; // repeat at i-th block\n             refetchBlocks \u003d true;\n             break;\n           }\n         } catch (IOException ie) {\n           LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n         } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n         }\n       }\n \n       if (!done) {\n         throw new IOException(\"Fail to get block MD5 for \" + block);\n       }\n     }\n \n     //compute file MD5\n     final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n     switch (crcType) {\n       case CRC32:\n         return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       case CRC32C:\n         return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       default:\n         // If there is no block allocated for the file,\n         // return one with the magic entry that matches what previous\n         // hdfs versions return.\n         if (locatedblocks.size() \u003d\u003d 0) {\n           return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n         }\n \n         // we should never get here since the validity was checked\n         // when getCrcType() was called above.\n         return null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n      remaining \u003d Math.min(length, blockLocations.getFileLength());\n    }\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              HdfsServerConstants.SMALL_BUFFER_SIZE));\n          in \u003d new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelper.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "10/04/15 11:40 AM",
      "commitNameOld": "7660da95cb67cbfe034aa8fa2a5bf0f8c9fdf41a",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 0.13,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,168 +1,168 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n     //get block locations for the file range\n     LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n         length);\n     if (null \u003d\u003d blockLocations) {\n       throw new FileNotFoundException(\"File does not exist: \" + src);\n     }\n     List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n     final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n     int bytesPerCRC \u003d -1;\n     DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n     long crcPerBlock \u003d 0;\n     boolean refetchBlocks \u003d false;\n     int lastRetriedIndex \u003d -1;\n \n     // get block checksum for each block\n     long remaining \u003d length;\n     if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n       remaining \u003d Math.min(length, blockLocations.getFileLength());\n     }\n     for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n       if (refetchBlocks) {  // refetch to get fresh tokens\n         blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n         if (null \u003d\u003d blockLocations) {\n           throw new FileNotFoundException(\"File does not exist: \" + src);\n         }\n         locatedblocks \u003d blockLocations.getLocatedBlocks();\n         refetchBlocks \u003d false;\n       }\n       LocatedBlock lb \u003d locatedblocks.get(i);\n       final ExtendedBlock block \u003d lb.getBlock();\n       if (remaining \u003c block.getNumBytes()) {\n         block.setNumBytes(remaining);\n       }\n       remaining -\u003d block.getNumBytes();\n       final DatanodeInfo[] datanodes \u003d lb.getLocations();\n       \n       //try each datanode location of the block\n-      final int timeout \u003d 3000 * datanodes.length + dfsClientConf.socketTimeout;\n+      final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n       boolean done \u003d false;\n       for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n         DataOutputStream out \u003d null;\n         DataInputStream in \u003d null;\n         \n         try {\n           //connect to a datanode\n           IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n           out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           in \u003d new DataInputStream(pair.in);\n \n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"write to \" + datanodes[j] + \": \"\n                 + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n           }\n           // get block MD5\n           new Sender(out).blockChecksum(block, lb.getBlockToken());\n \n           final BlockOpResponseProto reply \u003d\n             BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n \n           String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n           DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n           OpBlockChecksumResponseProto checksumData \u003d\n             reply.getChecksumResponse();\n \n           //read byte-per-checksum\n           final int bpc \u003d checksumData.getBytesPerCrc();\n           if (i \u003d\u003d 0) { //first block\n             bytesPerCRC \u003d bpc;\n           }\n           else if (bpc !\u003d bytesPerCRC) {\n             throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                 + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n           }\n           \n           //read crc-per-block\n           final long cpb \u003d checksumData.getCrcPerBlock();\n           if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n             crcPerBlock \u003d cpb;\n           }\n \n           //read md5\n           final MD5Hash md5 \u003d new MD5Hash(\n               checksumData.getMd5().toByteArray());\n           md5.write(md5out);\n           \n           // read crc-type\n           final DataChecksum.Type ct;\n           if (checksumData.hasCrcType()) {\n             ct \u003d PBHelper.convert(checksumData\n                 .getCrcType());\n           } else {\n             LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                       \"inferring checksum by reading first byte\");\n             ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n           }\n \n           if (i \u003d\u003d 0) { // first block\n             crcType \u003d ct;\n           } else if (crcType !\u003d DataChecksum.Type.MIXED\n               \u0026\u0026 crcType !\u003d ct) {\n             // if crc types are mixed in a file\n             crcType \u003d DataChecksum.Type.MIXED;\n           }\n \n           done \u003d true;\n \n           if (LOG.isDebugEnabled()) {\n             if (i \u003d\u003d 0) {\n               LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                   + \", crcPerBlock\u003d\" + crcPerBlock);\n             }\n             LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n           }\n         } catch (InvalidBlockTokenException ibte) {\n           if (i \u003e lastRetriedIndex) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                   + \"for file \" + src + \" for block \" + block\n                   + \" from datanode \" + datanodes[j]\n                   + \". Will retry the block once.\");\n             }\n             lastRetriedIndex \u003d i;\n             done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n             i--; // repeat at i-th block\n             refetchBlocks \u003d true;\n             break;\n           }\n         } catch (IOException ie) {\n           LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n         } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n         }\n       }\n \n       if (!done) {\n         throw new IOException(\"Fail to get block MD5 for \" + block);\n       }\n     }\n \n     //compute file MD5\n     final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n     switch (crcType) {\n       case CRC32:\n         return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       case CRC32C:\n         return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       default:\n         // If there is no block allocated for the file,\n         // return one with the magic entry that matches what previous\n         // hdfs versions return.\n         if (locatedblocks.size() \u003d\u003d 0) {\n           return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n         }\n \n         // we should never get here since the validity was checked\n         // when getCrcType() was called above.\n         return null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n      remaining \u003d Math.min(length, blockLocations.getFileLength());\n    }\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000*datanodes.length + dfsClientConf.getSocketTimeout();\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          in \u003d new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelper.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "f2d7a67a2c1d9dde10ed3171fdec65dff885afcc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7682. {{DistributedFileSystem#getFileChecksum}} of a snapshotted file includes non-snapshotted content. Contributed by Charles Lamb.\n",
      "commitDate": "03/03/15 6:08 PM",
      "commitName": "f2d7a67a2c1d9dde10ed3171fdec65dff885afcc",
      "commitAuthor": "Aaron T. Myers",
      "commitDateOld": "01/03/15 11:03 PM",
      "commitNameOld": "67ed59348d638d56e6752ba2c71fdcd69567546d",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 1.8,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,165 +1,168 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n     //get block locations for the file range\n     LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n         length);\n     if (null \u003d\u003d blockLocations) {\n       throw new FileNotFoundException(\"File does not exist: \" + src);\n     }\n     List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n     final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n     int bytesPerCRC \u003d -1;\n     DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n     long crcPerBlock \u003d 0;\n     boolean refetchBlocks \u003d false;\n     int lastRetriedIndex \u003d -1;\n \n     // get block checksum for each block\n     long remaining \u003d length;\n+    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n+      remaining \u003d Math.min(length, blockLocations.getFileLength());\n+    }\n     for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n       if (refetchBlocks) {  // refetch to get fresh tokens\n         blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n         if (null \u003d\u003d blockLocations) {\n           throw new FileNotFoundException(\"File does not exist: \" + src);\n         }\n         locatedblocks \u003d blockLocations.getLocatedBlocks();\n         refetchBlocks \u003d false;\n       }\n       LocatedBlock lb \u003d locatedblocks.get(i);\n       final ExtendedBlock block \u003d lb.getBlock();\n       if (remaining \u003c block.getNumBytes()) {\n         block.setNumBytes(remaining);\n       }\n       remaining -\u003d block.getNumBytes();\n       final DatanodeInfo[] datanodes \u003d lb.getLocations();\n       \n       //try each datanode location of the block\n       final int timeout \u003d 3000 * datanodes.length + dfsClientConf.socketTimeout;\n       boolean done \u003d false;\n       for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n         DataOutputStream out \u003d null;\n         DataInputStream in \u003d null;\n         \n         try {\n           //connect to a datanode\n           IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n           out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           in \u003d new DataInputStream(pair.in);\n \n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"write to \" + datanodes[j] + \": \"\n                 + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n           }\n           // get block MD5\n           new Sender(out).blockChecksum(block, lb.getBlockToken());\n \n           final BlockOpResponseProto reply \u003d\n             BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n \n           String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n           DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n           OpBlockChecksumResponseProto checksumData \u003d\n             reply.getChecksumResponse();\n \n           //read byte-per-checksum\n           final int bpc \u003d checksumData.getBytesPerCrc();\n           if (i \u003d\u003d 0) { //first block\n             bytesPerCRC \u003d bpc;\n           }\n           else if (bpc !\u003d bytesPerCRC) {\n             throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                 + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n           }\n           \n           //read crc-per-block\n           final long cpb \u003d checksumData.getCrcPerBlock();\n           if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n             crcPerBlock \u003d cpb;\n           }\n \n           //read md5\n           final MD5Hash md5 \u003d new MD5Hash(\n               checksumData.getMd5().toByteArray());\n           md5.write(md5out);\n           \n           // read crc-type\n           final DataChecksum.Type ct;\n           if (checksumData.hasCrcType()) {\n             ct \u003d PBHelper.convert(checksumData\n                 .getCrcType());\n           } else {\n             LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                       \"inferring checksum by reading first byte\");\n             ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n           }\n \n           if (i \u003d\u003d 0) { // first block\n             crcType \u003d ct;\n           } else if (crcType !\u003d DataChecksum.Type.MIXED\n               \u0026\u0026 crcType !\u003d ct) {\n             // if crc types are mixed in a file\n             crcType \u003d DataChecksum.Type.MIXED;\n           }\n \n           done \u003d true;\n \n           if (LOG.isDebugEnabled()) {\n             if (i \u003d\u003d 0) {\n               LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                   + \", crcPerBlock\u003d\" + crcPerBlock);\n             }\n             LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n           }\n         } catch (InvalidBlockTokenException ibte) {\n           if (i \u003e lastRetriedIndex) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                   + \"for file \" + src + \" for block \" + block\n                   + \" from datanode \" + datanodes[j]\n                   + \". Will retry the block once.\");\n             }\n             lastRetriedIndex \u003d i;\n             done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n             i--; // repeat at i-th block\n             refetchBlocks \u003d true;\n             break;\n           }\n         } catch (IOException ie) {\n           LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n         } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n         }\n       }\n \n       if (!done) {\n         throw new IOException(\"Fail to get block MD5 for \" + block);\n       }\n     }\n \n     //compute file MD5\n     final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n     switch (crcType) {\n       case CRC32:\n         return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       case CRC32C:\n         return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       default:\n         // If there is no block allocated for the file,\n         // return one with the magic entry that matches what previous\n         // hdfs versions return.\n         if (locatedblocks.size() \u003d\u003d 0) {\n           return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n         }\n \n         // we should never get here since the validity was checked\n         // when getCrcType() was called above.\n         return null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    if (src.contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR_SEPARATOR)) {\n      remaining \u003d Math.min(length, blockLocations.getFileLength());\n    }\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000 * datanodes.length + dfsClientConf.socketTimeout;\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          in \u003d new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelper.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "67ed59348d638d56e6752ba2c71fdcd69567546d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7439. Add BlockOpResponseProto\u0027s message to the exception messages.  Contributed by Takanobu Asanuma\n",
      "commitDate": "01/03/15 11:03 PM",
      "commitName": "67ed59348d638d56e6752ba2c71fdcd69567546d",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "21/02/15 3:38 PM",
      "commitNameOld": "8b465b4b8caed31ca9daeaae108f9a868a30a455",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 8.31,
      "commitsBetweenForRepo": 60,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,171 +1,165 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n     //get block locations for the file range\n     LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n         length);\n     if (null \u003d\u003d blockLocations) {\n       throw new FileNotFoundException(\"File does not exist: \" + src);\n     }\n     List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n     final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n     int bytesPerCRC \u003d -1;\n     DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n     long crcPerBlock \u003d 0;\n     boolean refetchBlocks \u003d false;\n     int lastRetriedIndex \u003d -1;\n \n     // get block checksum for each block\n     long remaining \u003d length;\n     for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n       if (refetchBlocks) {  // refetch to get fresh tokens\n         blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n         if (null \u003d\u003d blockLocations) {\n           throw new FileNotFoundException(\"File does not exist: \" + src);\n         }\n         locatedblocks \u003d blockLocations.getLocatedBlocks();\n         refetchBlocks \u003d false;\n       }\n       LocatedBlock lb \u003d locatedblocks.get(i);\n       final ExtendedBlock block \u003d lb.getBlock();\n       if (remaining \u003c block.getNumBytes()) {\n         block.setNumBytes(remaining);\n       }\n       remaining -\u003d block.getNumBytes();\n       final DatanodeInfo[] datanodes \u003d lb.getLocations();\n       \n       //try each datanode location of the block\n       final int timeout \u003d 3000 * datanodes.length + dfsClientConf.socketTimeout;\n       boolean done \u003d false;\n       for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n         DataOutputStream out \u003d null;\n         DataInputStream in \u003d null;\n         \n         try {\n           //connect to a datanode\n           IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n           out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           in \u003d new DataInputStream(pair.in);\n \n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"write to \" + datanodes[j] + \": \"\n                 + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n           }\n           // get block MD5\n           new Sender(out).blockChecksum(block, lb.getBlockToken());\n \n           final BlockOpResponseProto reply \u003d\n             BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n \n-          if (reply.getStatus() !\u003d Status.SUCCESS) {\n-            if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n-              throw new InvalidBlockTokenException();\n-            } else {\n-              throw new IOException(\"Bad response \" + reply + \" for block \"\n-                  + block + \" from datanode \" + datanodes[j]);\n-            }\n-          }\n-          \n+          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n+          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n+\n           OpBlockChecksumResponseProto checksumData \u003d\n             reply.getChecksumResponse();\n \n           //read byte-per-checksum\n           final int bpc \u003d checksumData.getBytesPerCrc();\n           if (i \u003d\u003d 0) { //first block\n             bytesPerCRC \u003d bpc;\n           }\n           else if (bpc !\u003d bytesPerCRC) {\n             throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                 + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n           }\n           \n           //read crc-per-block\n           final long cpb \u003d checksumData.getCrcPerBlock();\n           if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n             crcPerBlock \u003d cpb;\n           }\n \n           //read md5\n           final MD5Hash md5 \u003d new MD5Hash(\n               checksumData.getMd5().toByteArray());\n           md5.write(md5out);\n           \n           // read crc-type\n           final DataChecksum.Type ct;\n           if (checksumData.hasCrcType()) {\n             ct \u003d PBHelper.convert(checksumData\n                 .getCrcType());\n           } else {\n             LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                       \"inferring checksum by reading first byte\");\n             ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n           }\n \n           if (i \u003d\u003d 0) { // first block\n             crcType \u003d ct;\n           } else if (crcType !\u003d DataChecksum.Type.MIXED\n               \u0026\u0026 crcType !\u003d ct) {\n             // if crc types are mixed in a file\n             crcType \u003d DataChecksum.Type.MIXED;\n           }\n \n           done \u003d true;\n \n           if (LOG.isDebugEnabled()) {\n             if (i \u003d\u003d 0) {\n               LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                   + \", crcPerBlock\u003d\" + crcPerBlock);\n             }\n             LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n           }\n         } catch (InvalidBlockTokenException ibte) {\n           if (i \u003e lastRetriedIndex) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                   + \"for file \" + src + \" for block \" + block\n                   + \" from datanode \" + datanodes[j]\n                   + \". Will retry the block once.\");\n             }\n             lastRetriedIndex \u003d i;\n             done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n             i--; // repeat at i-th block\n             refetchBlocks \u003d true;\n             break;\n           }\n         } catch (IOException ie) {\n           LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n         } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n         }\n       }\n \n       if (!done) {\n         throw new IOException(\"Fail to get block MD5 for \" + block);\n       }\n     }\n \n     //compute file MD5\n     final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n     switch (crcType) {\n       case CRC32:\n         return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       case CRC32C:\n         return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n             crcPerBlock, fileMD5);\n       default:\n         // If there is no block allocated for the file,\n         // return one with the magic entry that matches what previous\n         // hdfs versions return.\n         if (locatedblocks.size() \u003d\u003d 0) {\n           return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n         }\n \n         // we should never get here since the validity was checked\n         // when getCrcType() was called above.\n         return null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000 * datanodes.length + dfsClientConf.socketTimeout;\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          in \u003d new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n\n          String logInfo \u003d \"for block \" + block + \" from datanode \" + datanodes[j];\n          DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelper.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "3b54223c0f32d42a84436c670d80b791a8e9696d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/07/14 11:10 AM",
      "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "30/05/14 5:12 PM",
      "commitNameOld": "880a0c673c74a128a01c72b60695f05327f5e961",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 44.75,
      "commitsBetweenForRepo": 266,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,171 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n       throws IOException {\n     checkOpen();\n     Preconditions.checkArgument(length \u003e\u003d 0);\n-    return getFileChecksum(src, length, clientName, namenode,\n-        socketFactory, dfsClientConf.socketTimeout, getDataEncryptionKey(),\n-        dfsClientConf.connectToDnViaHostname);\n+    //get block locations for the file range\n+    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n+        length);\n+    if (null \u003d\u003d blockLocations) {\n+      throw new FileNotFoundException(\"File does not exist: \" + src);\n+    }\n+    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n+    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n+    int bytesPerCRC \u003d -1;\n+    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n+    long crcPerBlock \u003d 0;\n+    boolean refetchBlocks \u003d false;\n+    int lastRetriedIndex \u003d -1;\n+\n+    // get block checksum for each block\n+    long remaining \u003d length;\n+    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n+      if (refetchBlocks) {  // refetch to get fresh tokens\n+        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n+        if (null \u003d\u003d blockLocations) {\n+          throw new FileNotFoundException(\"File does not exist: \" + src);\n+        }\n+        locatedblocks \u003d blockLocations.getLocatedBlocks();\n+        refetchBlocks \u003d false;\n+      }\n+      LocatedBlock lb \u003d locatedblocks.get(i);\n+      final ExtendedBlock block \u003d lb.getBlock();\n+      if (remaining \u003c block.getNumBytes()) {\n+        block.setNumBytes(remaining);\n+      }\n+      remaining -\u003d block.getNumBytes();\n+      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n+      \n+      //try each datanode location of the block\n+      final int timeout \u003d 3000 * datanodes.length + dfsClientConf.socketTimeout;\n+      boolean done \u003d false;\n+      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n+        DataOutputStream out \u003d null;\n+        DataInputStream in \u003d null;\n+        \n+        try {\n+          //connect to a datanode\n+          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n+          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n+              HdfsConstants.SMALL_BUFFER_SIZE));\n+          in \u003d new DataInputStream(pair.in);\n+\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"write to \" + datanodes[j] + \": \"\n+                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n+          }\n+          // get block MD5\n+          new Sender(out).blockChecksum(block, lb.getBlockToken());\n+\n+          final BlockOpResponseProto reply \u003d\n+            BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n+\n+          if (reply.getStatus() !\u003d Status.SUCCESS) {\n+            if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n+              throw new InvalidBlockTokenException();\n+            } else {\n+              throw new IOException(\"Bad response \" + reply + \" for block \"\n+                  + block + \" from datanode \" + datanodes[j]);\n+            }\n+          }\n+          \n+          OpBlockChecksumResponseProto checksumData \u003d\n+            reply.getChecksumResponse();\n+\n+          //read byte-per-checksum\n+          final int bpc \u003d checksumData.getBytesPerCrc();\n+          if (i \u003d\u003d 0) { //first block\n+            bytesPerCRC \u003d bpc;\n+          }\n+          else if (bpc !\u003d bytesPerCRC) {\n+            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n+                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n+          }\n+          \n+          //read crc-per-block\n+          final long cpb \u003d checksumData.getCrcPerBlock();\n+          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n+            crcPerBlock \u003d cpb;\n+          }\n+\n+          //read md5\n+          final MD5Hash md5 \u003d new MD5Hash(\n+              checksumData.getMd5().toByteArray());\n+          md5.write(md5out);\n+          \n+          // read crc-type\n+          final DataChecksum.Type ct;\n+          if (checksumData.hasCrcType()) {\n+            ct \u003d PBHelper.convert(checksumData\n+                .getCrcType());\n+          } else {\n+            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n+                      \"inferring checksum by reading first byte\");\n+            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n+          }\n+\n+          if (i \u003d\u003d 0) { // first block\n+            crcType \u003d ct;\n+          } else if (crcType !\u003d DataChecksum.Type.MIXED\n+              \u0026\u0026 crcType !\u003d ct) {\n+            // if crc types are mixed in a file\n+            crcType \u003d DataChecksum.Type.MIXED;\n+          }\n+\n+          done \u003d true;\n+\n+          if (LOG.isDebugEnabled()) {\n+            if (i \u003d\u003d 0) {\n+              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n+                  + \", crcPerBlock\u003d\" + crcPerBlock);\n+            }\n+            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n+          }\n+        } catch (InvalidBlockTokenException ibte) {\n+          if (i \u003e lastRetriedIndex) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n+                  + \"for file \" + src + \" for block \" + block\n+                  + \" from datanode \" + datanodes[j]\n+                  + \". Will retry the block once.\");\n+            }\n+            lastRetriedIndex \u003d i;\n+            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n+            i--; // repeat at i-th block\n+            refetchBlocks \u003d true;\n+            break;\n+          }\n+        } catch (IOException ie) {\n+          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n+        } finally {\n+          IOUtils.closeStream(in);\n+          IOUtils.closeStream(out);\n+        }\n+      }\n+\n+      if (!done) {\n+        throw new IOException(\"Fail to get block MD5 for \" + block);\n+      }\n+    }\n+\n+    //compute file MD5\n+    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n+    switch (crcType) {\n+      case CRC32:\n+        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n+            crcPerBlock, fileMD5);\n+      case CRC32C:\n+        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n+            crcPerBlock, fileMD5);\n+      default:\n+        // If there is no block allocated for the file,\n+        // return one with the magic entry that matches what previous\n+        // hdfs versions return.\n+        if (locatedblocks.size() \u003d\u003d 0) {\n+          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n+        }\n+\n+        // we should never get here since the validity was checked\n+        // when getCrcType() was called above.\n+        return null;\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    //get block locations for the file range\n    LocatedBlocks blockLocations \u003d callGetBlockLocations(namenode, src, 0,\n        length);\n    if (null \u003d\u003d blockLocations) {\n      throw new FileNotFoundException(\"File does not exist: \" + src);\n    }\n    List\u003cLocatedBlock\u003e locatedblocks \u003d blockLocations.getLocatedBlocks();\n    final DataOutputBuffer md5out \u003d new DataOutputBuffer();\n    int bytesPerCRC \u003d -1;\n    DataChecksum.Type crcType \u003d DataChecksum.Type.DEFAULT;\n    long crcPerBlock \u003d 0;\n    boolean refetchBlocks \u003d false;\n    int lastRetriedIndex \u003d -1;\n\n    // get block checksum for each block\n    long remaining \u003d length;\n    for(int i \u003d 0; i \u003c locatedblocks.size() \u0026\u0026 remaining \u003e 0; i++) {\n      if (refetchBlocks) {  // refetch to get fresh tokens\n        blockLocations \u003d callGetBlockLocations(namenode, src, 0, length);\n        if (null \u003d\u003d blockLocations) {\n          throw new FileNotFoundException(\"File does not exist: \" + src);\n        }\n        locatedblocks \u003d blockLocations.getLocatedBlocks();\n        refetchBlocks \u003d false;\n      }\n      LocatedBlock lb \u003d locatedblocks.get(i);\n      final ExtendedBlock block \u003d lb.getBlock();\n      if (remaining \u003c block.getNumBytes()) {\n        block.setNumBytes(remaining);\n      }\n      remaining -\u003d block.getNumBytes();\n      final DatanodeInfo[] datanodes \u003d lb.getLocations();\n      \n      //try each datanode location of the block\n      final int timeout \u003d 3000 * datanodes.length + dfsClientConf.socketTimeout;\n      boolean done \u003d false;\n      for(int j \u003d 0; !done \u0026\u0026 j \u003c datanodes.length; j++) {\n        DataOutputStream out \u003d null;\n        DataInputStream in \u003d null;\n        \n        try {\n          //connect to a datanode\n          IOStreamPair pair \u003d connectToDN(datanodes[j], timeout, lb);\n          out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          in \u003d new DataInputStream(pair.in);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"write to \" + datanodes[j] + \": \"\n                + Op.BLOCK_CHECKSUM + \", block\u003d\" + block);\n          }\n          // get block MD5\n          new Sender(out).blockChecksum(block, lb.getBlockToken());\n\n          final BlockOpResponseProto reply \u003d\n            BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n\n          if (reply.getStatus() !\u003d Status.SUCCESS) {\n            if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException();\n            } else {\n              throw new IOException(\"Bad response \" + reply + \" for block \"\n                  + block + \" from datanode \" + datanodes[j]);\n            }\n          }\n          \n          OpBlockChecksumResponseProto checksumData \u003d\n            reply.getChecksumResponse();\n\n          //read byte-per-checksum\n          final int bpc \u003d checksumData.getBytesPerCrc();\n          if (i \u003d\u003d 0) { //first block\n            bytesPerCRC \u003d bpc;\n          }\n          else if (bpc !\u003d bytesPerCRC) {\n            throw new IOException(\"Byte-per-checksum not matched: bpc\u003d\" + bpc\n                + \" but bytesPerCRC\u003d\" + bytesPerCRC);\n          }\n          \n          //read crc-per-block\n          final long cpb \u003d checksumData.getCrcPerBlock();\n          if (locatedblocks.size() \u003e 1 \u0026\u0026 i \u003d\u003d 0) {\n            crcPerBlock \u003d cpb;\n          }\n\n          //read md5\n          final MD5Hash md5 \u003d new MD5Hash(\n              checksumData.getMd5().toByteArray());\n          md5.write(md5out);\n          \n          // read crc-type\n          final DataChecksum.Type ct;\n          if (checksumData.hasCrcType()) {\n            ct \u003d PBHelper.convert(checksumData\n                .getCrcType());\n          } else {\n            LOG.debug(\"Retrieving checksum from an earlier-version DataNode: \" +\n                      \"inferring checksum by reading first byte\");\n            ct \u003d inferChecksumTypeByReading(lb, datanodes[j]);\n          }\n\n          if (i \u003d\u003d 0) { // first block\n            crcType \u003d ct;\n          } else if (crcType !\u003d DataChecksum.Type.MIXED\n              \u0026\u0026 crcType !\u003d ct) {\n            // if crc types are mixed in a file\n            crcType \u003d DataChecksum.Type.MIXED;\n          }\n\n          done \u003d true;\n\n          if (LOG.isDebugEnabled()) {\n            if (i \u003d\u003d 0) {\n              LOG.debug(\"set bytesPerCRC\u003d\" + bytesPerCRC\n                  + \", crcPerBlock\u003d\" + crcPerBlock);\n            }\n            LOG.debug(\"got reply from \" + datanodes[j] + \": md5\u003d\" + md5);\n          }\n        } catch (InvalidBlockTokenException ibte) {\n          if (i \u003e lastRetriedIndex) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Got access token error in response to OP_BLOCK_CHECKSUM \"\n                  + \"for file \" + src + \" for block \" + block\n                  + \" from datanode \" + datanodes[j]\n                  + \". Will retry the block once.\");\n            }\n            lastRetriedIndex \u003d i;\n            done \u003d true; // actually it\u0027s not done; but we\u0027ll retry\n            i--; // repeat at i-th block\n            refetchBlocks \u003d true;\n            break;\n          }\n        } catch (IOException ie) {\n          LOG.warn(\"src\u003d\" + src + \", datanodes[\"+j+\"]\u003d\" + datanodes[j], ie);\n        } finally {\n          IOUtils.closeStream(in);\n          IOUtils.closeStream(out);\n        }\n      }\n\n      if (!done) {\n        throw new IOException(\"Fail to get block MD5 for \" + block);\n      }\n    }\n\n    //compute file MD5\n    final MD5Hash fileMD5 \u003d MD5Hash.digest(md5out.getData()); \n    switch (crcType) {\n      case CRC32:\n        return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      case CRC32C:\n        return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,\n            crcPerBlock, fileMD5);\n      default:\n        // If there is no block allocated for the file,\n        // return one with the magic entry that matches what previous\n        // hdfs versions return.\n        if (locatedblocks.size() \u003d\u003d 0) {\n          return new MD5MD5CRC32GzipFileChecksum(0, 0, fileMD5);\n        }\n\n        // we should never get here since the validity was checked\n        // when getCrcType() was called above.\n        return null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "3671a5e16fbddbe5a0516289ce98e1305e02291c": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-5899. Support incremental data copy in DistCp. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596931 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/05/14 11:17 AM",
      "commitName": "3671a5e16fbddbe5a0516289ce98e1305e02291c",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-5899. Support incremental data copy in DistCp. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596931 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/05/14 11:17 AM",
          "commitName": "3671a5e16fbddbe5a0516289ce98e1305e02291c",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "21/05/14 6:57 AM",
          "commitNameOld": "ac23a55547716df29b3e25c98a113399e184d9d1",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 1.18,
          "commitsBetweenForRepo": 10,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,6 +1,8 @@\n-  public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n+  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n+      throws IOException {\n     checkOpen();\n-    return getFileChecksum(src, clientName, namenode, socketFactory,\n-        dfsClientConf.socketTimeout, getDataEncryptionKey(),\n+    Preconditions.checkArgument(length \u003e\u003d 0);\n+    return getFileChecksum(src, length, clientName, namenode,\n+        socketFactory, dfsClientConf.socketTimeout, getDataEncryptionKey(),\n         dfsClientConf.connectToDnViaHostname);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    return getFileChecksum(src, length, clientName, namenode,\n        socketFactory, dfsClientConf.socketTimeout, getDataEncryptionKey(),\n        dfsClientConf.connectToDnViaHostname);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldValue": "[src-String]",
            "newValue": "[src-String, length-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-5899. Support incremental data copy in DistCp. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596931 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/05/14 11:17 AM",
          "commitName": "3671a5e16fbddbe5a0516289ce98e1305e02291c",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "21/05/14 6:57 AM",
          "commitNameOld": "ac23a55547716df29b3e25c98a113399e184d9d1",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 1.18,
          "commitsBetweenForRepo": 10,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,6 +1,8 @@\n-  public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n+  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n+      throws IOException {\n     checkOpen();\n-    return getFileChecksum(src, clientName, namenode, socketFactory,\n-        dfsClientConf.socketTimeout, getDataEncryptionKey(),\n+    Preconditions.checkArgument(length \u003e\u003d 0);\n+    return getFileChecksum(src, length, clientName, namenode,\n+        socketFactory, dfsClientConf.socketTimeout, getDataEncryptionKey(),\n         dfsClientConf.connectToDnViaHostname);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src, long length)\n      throws IOException {\n    checkOpen();\n    Preconditions.checkArgument(length \u003e\u003d 0);\n    return getFileChecksum(src, length, clientName, namenode,\n        socketFactory, dfsClientConf.socketTimeout, getDataEncryptionKey(),\n        dfsClientConf.connectToDnViaHostname);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "cfae13306ac0fb3f3c139d5ac511bf78cede1b77": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4403. DFSClient can infer checksum type when not provided by reading first byte. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1436730 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/01/13 6:59 PM",
      "commitName": "cfae13306ac0fb3f3c139d5ac511bf78cede1b77",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "09/01/13 3:30 PM",
      "commitNameOld": "7e599d9e3b852954a5a21b4738817c7aabfa1bc8",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 12.14,
      "commitsBetweenForRepo": 70,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,6 +1,6 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n     checkOpen();\n-    return getFileChecksum(src, namenode, socketFactory,\n+    return getFileChecksum(src, clientName, namenode, socketFactory,\n         dfsClientConf.socketTimeout, getDataEncryptionKey(),\n         dfsClientConf.connectToDnViaHostname);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n    checkOpen();\n    return getFileChecksum(src, clientName, namenode, socketFactory,\n        dfsClientConf.socketTimeout, getDataEncryptionKey(),\n        dfsClientConf.connectToDnViaHostname);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "f98d8eb291be364102b5c3011ce72e8f43eab389": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3150. Add option for clients to contact DNs via hostname. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1373094 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/08/12 1:59 PM",
      "commitName": "f98d8eb291be364102b5c3011ce72e8f43eab389",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "07/08/12 9:40 AM",
      "commitNameOld": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 7.18,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,6 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n     checkOpen();\n     return getFileChecksum(src, namenode, socketFactory,\n-        dfsClientConf.socketTimeout, getDataEncryptionKey());\n+        dfsClientConf.socketTimeout, getDataEncryptionKey(),\n+        dfsClientConf.connectToDnViaHostname);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n    checkOpen();\n    return getFileChecksum(src, namenode, socketFactory,\n        dfsClientConf.socketTimeout, getDataEncryptionKey(),\n        dfsClientConf.connectToDnViaHostname);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "18/07/12 4:21 PM",
      "commitNameOld": "0a6806ce8c946b26eceac7d16b467c54c453df84",
      "commitAuthorOld": "Daryn Sharp",
      "daysBetweenCommits": 19.72,
      "commitsBetweenForRepo": 79,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,4 +1,5 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n     checkOpen();\n-    return getFileChecksum(src, namenode, socketFactory, dfsClientConf.socketTimeout);    \n+    return getFileChecksum(src, namenode, socketFactory,\n+        dfsClientConf.socketTimeout, getDataEncryptionKey());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n    checkOpen();\n    return getFileChecksum(src, namenode, socketFactory,\n        dfsClientConf.socketTimeout, getDataEncryptionKey());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n    checkOpen();\n    return getFileChecksum(src, namenode, socketFactory, dfsClientConf.socketTimeout);    \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n    checkOpen();\n    return getFileChecksum(src, namenode, socketFactory, dfsClientConf.socketTimeout);    \n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSClient.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java"
      }
    },
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2092. Remove some object references to Configuration in DFSClient.  Contributed by Bharath Mundlapudi\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/06/11 3:24 PM",
      "commitName": "fd9997989c1f1c6f806c57a806e7225ca599fc0c",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 11.02,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,4 +1,4 @@\n   public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n     checkOpen();\n-    return getFileChecksum(src, namenode, socketFactory, socketTimeout);    \n+    return getFileChecksum(src, namenode, socketFactory, dfsClientConf.socketTimeout);    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n    checkOpen();\n    return getFileChecksum(src, namenode, socketFactory, dfsClientConf.socketTimeout);    \n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,4 @@\n+  public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n+    checkOpen();\n+    return getFileChecksum(src, namenode, socketFactory, socketTimeout);    \n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {\n    checkOpen();\n    return getFileChecksum(src, namenode, socketFactory, socketTimeout);    \n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSClient.java"
    }
  }
}