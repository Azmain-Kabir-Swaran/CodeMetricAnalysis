{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeDescriptor.java",
  "functionName": "dumpDatanode",
  "functionId": "dumpDatanode",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
  "functionStartLine": 967,
  "functionEndLine": 986,
  "numCommitsSeen": 118,
  "timeTaken": 2862,
  "changeHistory": [
    "57a84c0d149b693c913416975cafe6de4e23c321",
    "31c91706f7d17da006ef2d6c541f8dd092fae077"
  ],
  "changeHistoryShort": {
    "57a84c0d149b693c913416975cafe6de4e23c321": "Ybodychange",
    "31c91706f7d17da006ef2d6c541f8dd092fae077": "Yintroduced"
  },
  "changeHistoryDetails": {
    "57a84c0d149b693c913416975cafe6de4e23c321": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7369. Erasure coding: distribute recovery work for striped blocks to DataNode. Contributed by Zhe Zhang.\n",
      "commitDate": "26/05/15 11:43 AM",
      "commitName": "57a84c0d149b693c913416975cafe6de4e23c321",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 11:32 AM",
      "commitNameOld": "f05c21285ef23b6a973d69f045b1cb46c5abc039",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,20 @@\n   public String dumpDatanode() {\n     StringBuilder sb \u003d new StringBuilder(super.dumpDatanode());\n     int repl \u003d replicateBlocks.size();\n     if (repl \u003e 0) {\n       sb.append(\" \").append(repl).append(\" blocks to be replicated;\");\n     }\n+    int ec \u003d erasurecodeBlocks.size();\n+    if(ec \u003e 0) {\n+      sb.append(\" \").append(ec).append(\" blocks to be erasure coded;\");\n+    }\n     int inval \u003d invalidateBlocks.size();\n     if (inval \u003e 0) {\n       sb.append(\" \").append(inval).append(\" blocks to be invalidated;\");      \n     }\n     int recover \u003d recoverBlocks.size();\n     if (recover \u003e 0) {\n       sb.append(\" \").append(recover).append(\" blocks to be recovered;\");\n     }\n     return sb.toString();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder sb \u003d new StringBuilder(super.dumpDatanode());\n    int repl \u003d replicateBlocks.size();\n    if (repl \u003e 0) {\n      sb.append(\" \").append(repl).append(\" blocks to be replicated;\");\n    }\n    int ec \u003d erasurecodeBlocks.size();\n    if(ec \u003e 0) {\n      sb.append(\" \").append(ec).append(\" blocks to be erasure coded;\");\n    }\n    int inval \u003d invalidateBlocks.size();\n    if (inval \u003e 0) {\n      sb.append(\" \").append(inval).append(\" blocks to be invalidated;\");      \n    }\n    int recover \u003d recoverBlocks.size();\n    if (recover \u003e 0) {\n      sb.append(\" \").append(recover).append(\" blocks to be recovered;\");\n    }\n    return sb.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    },
    "31c91706f7d17da006ef2d6c541f8dd092fae077": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-1972. Fencing mechanism for block invalidations and replications. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1221608 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/11 8:32 PM",
      "commitName": "31c91706f7d17da006ef2d6c541f8dd092fae077",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,16 @@\n+  public String dumpDatanode() {\n+    StringBuilder sb \u003d new StringBuilder(super.dumpDatanode());\n+    int repl \u003d replicateBlocks.size();\n+    if (repl \u003e 0) {\n+      sb.append(\" \").append(repl).append(\" blocks to be replicated;\");\n+    }\n+    int inval \u003d invalidateBlocks.size();\n+    if (inval \u003e 0) {\n+      sb.append(\" \").append(inval).append(\" blocks to be invalidated;\");      \n+    }\n+    int recover \u003d recoverBlocks.size();\n+    if (recover \u003e 0) {\n+      sb.append(\" \").append(recover).append(\" blocks to be recovered;\");\n+    }\n+    return sb.toString();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder sb \u003d new StringBuilder(super.dumpDatanode());\n    int repl \u003d replicateBlocks.size();\n    if (repl \u003e 0) {\n      sb.append(\" \").append(repl).append(\" blocks to be replicated;\");\n    }\n    int inval \u003d invalidateBlocks.size();\n    if (inval \u003e 0) {\n      sb.append(\" \").append(inval).append(\" blocks to be invalidated;\");      \n    }\n    int recover \u003d recoverBlocks.size();\n    if (recover \u003e 0) {\n      sb.append(\" \").append(recover).append(\" blocks to be recovered;\");\n    }\n    return sb.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java"
    }
  }
}