{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStorage.java",
  "functionName": "getParallelVolumeLoadThreadsNum",
  "functionId": "getParallelVolumeLoadThreadsNum___dataDirs-int__conf-Configuration",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
  "functionStartLine": 348,
  "functionEndLine": 359,
  "numCommitsSeen": 75,
  "timeTaken": 1865,
  "changeHistory": [
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a",
    "66289a3bf403f307844ea0b6ceed35b603d12c0b"
  ],
  "changeHistoryShort": {
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a": "Ybodychange",
    "66289a3bf403f307844ea0b6ceed35b603d12c0b": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12997. Move logging to slf4j in BlockPoolSliceStorage and Storage. Contributed by Ajay Kumar.\n",
      "commitDate": "01/02/18 10:45 AM",
      "commitName": "b3ae11d59790bb08b81848e9f944db7d3afbbd8a",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "8239e3afb31d3c4485817d4b8b8b195b554acbe7",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 47.7,
      "commitsBetweenForRepo": 240,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,12 @@\n   static int getParallelVolumeLoadThreadsNum(int dataDirs, Configuration conf) {\n     final String key\n         \u003d DFSConfigKeys.DFS_DATANODE_PARALLEL_VOLUME_LOAD_THREADS_NUM_KEY;\n     final int n \u003d conf.getInt(key, dataDirs);\n     if (n \u003c 1) {\n       throw new HadoopIllegalArgumentException(key + \" \u003d \" + n + \" \u003c 1\");\n     }\n     final int min \u003d Math.min(n, dataDirs);\n-    LOG.info(\"Using \" + min + \" threads to upgrade data directories (\"\n-        + key + \"\u003d\" + n + \", dataDirs\u003d\" + dataDirs + \")\");\n+    LOG.info(\"Using {} threads to upgrade data directories ({}\u003d{}, \"\n+        + \"dataDirs\u003d{})\", min, key, n, dataDirs);\n     return min;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int getParallelVolumeLoadThreadsNum(int dataDirs, Configuration conf) {\n    final String key\n        \u003d DFSConfigKeys.DFS_DATANODE_PARALLEL_VOLUME_LOAD_THREADS_NUM_KEY;\n    final int n \u003d conf.getInt(key, dataDirs);\n    if (n \u003c 1) {\n      throw new HadoopIllegalArgumentException(key + \" \u003d \" + n + \" \u003c 1\");\n    }\n    final int min \u003d Math.min(n, dataDirs);\n    LOG.info(\"Using {} threads to upgrade data directories ({}\u003d{}, \"\n        + \"dataDirs\u003d{})\", min, key, n, dataDirs);\n    return min;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {}
    },
    "66289a3bf403f307844ea0b6ceed35b603d12c0b": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8578. On upgrade, Datanode should process all storage/data dirs in parallel.  Contributed by vinayakumarb and szetszwo\n",
      "commitDate": "22/02/16 3:01 PM",
      "commitName": "66289a3bf403f307844ea0b6ceed35b603d12c0b",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "diff": "@@ -0,0 +1,12 @@\n+  static int getParallelVolumeLoadThreadsNum(int dataDirs, Configuration conf) {\n+    final String key\n+        \u003d DFSConfigKeys.DFS_DATANODE_PARALLEL_VOLUME_LOAD_THREADS_NUM_KEY;\n+    final int n \u003d conf.getInt(key, dataDirs);\n+    if (n \u003c 1) {\n+      throw new HadoopIllegalArgumentException(key + \" \u003d \" + n + \" \u003c 1\");\n+    }\n+    final int min \u003d Math.min(n, dataDirs);\n+    LOG.info(\"Using \" + min + \" threads to upgrade data directories (\"\n+        + key + \"\u003d\" + n + \", dataDirs\u003d\" + dataDirs + \")\");\n+    return min;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  static int getParallelVolumeLoadThreadsNum(int dataDirs, Configuration conf) {\n    final String key\n        \u003d DFSConfigKeys.DFS_DATANODE_PARALLEL_VOLUME_LOAD_THREADS_NUM_KEY;\n    final int n \u003d conf.getInt(key, dataDirs);\n    if (n \u003c 1) {\n      throw new HadoopIllegalArgumentException(key + \" \u003d \" + n + \" \u003c 1\");\n    }\n    final int min \u003d Math.min(n, dataDirs);\n    LOG.info(\"Using \" + min + \" threads to upgrade data directories (\"\n        + key + \"\u003d\" + n + \", dataDirs\u003d\" + dataDirs + \")\");\n    return min;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java"
    }
  }
}