{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetAsyncDiskService.java",
  "functionName": "updateDeletedBlockId",
  "functionId": "updateDeletedBlockId___block-ExtendedBlock",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java",
  "functionStartLine": 342,
  "functionEndLine": 359,
  "numCommitsSeen": 21,
  "timeTaken": 1290,
  "changeHistory": [
    "6dae6d12ec5abb716e1501cd4e18b10ae7809b94"
  ],
  "changeHistoryShort": {
    "6dae6d12ec5abb716e1501cd4e18b10ae7809b94": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6dae6d12ec5abb716e1501cd4e18b10ae7809b94": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-6833.  DirectoryScanner should not register a deleting block with memory of DataNode.  Contributed by Shinichi Yamashita\n",
      "commitDate": "12/03/15 11:25 AM",
      "commitName": "6dae6d12ec5abb716e1501cd4e18b10ae7809b94",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "diff": "@@ -0,0 +1,18 @@\n+  private synchronized void updateDeletedBlockId(ExtendedBlock block) {\n+    Set\u003cLong\u003e blockIds \u003d deletedBlockIds.get(block.getBlockPoolId());\n+    if (blockIds \u003d\u003d null) {\n+      blockIds \u003d new HashSet\u003cLong\u003e();\n+      deletedBlockIds.put(block.getBlockPoolId(), blockIds);\n+    }\n+    blockIds.add(block.getBlockId());\n+    numDeletedBlocks++;\n+    if (numDeletedBlocks \u003d\u003d MAX_DELETED_BLOCKS) {\n+      for (Entry\u003cString, Set\u003cLong\u003e\u003e e : deletedBlockIds.entrySet()) {\n+        String bpid \u003d e.getKey();\n+        Set\u003cLong\u003e bs \u003d e.getValue();\n+        fsdatasetImpl.removeDeletedBlocks(bpid, bs);\n+        bs.clear();\n+      }\n+      numDeletedBlocks \u003d 0;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void updateDeletedBlockId(ExtendedBlock block) {\n    Set\u003cLong\u003e blockIds \u003d deletedBlockIds.get(block.getBlockPoolId());\n    if (blockIds \u003d\u003d null) {\n      blockIds \u003d new HashSet\u003cLong\u003e();\n      deletedBlockIds.put(block.getBlockPoolId(), blockIds);\n    }\n    blockIds.add(block.getBlockId());\n    numDeletedBlocks++;\n    if (numDeletedBlocks \u003d\u003d MAX_DELETED_BLOCKS) {\n      for (Entry\u003cString, Set\u003cLong\u003e\u003e e : deletedBlockIds.entrySet()) {\n        String bpid \u003d e.getKey();\n        Set\u003cLong\u003e bs \u003d e.getValue();\n        fsdatasetImpl.removeDeletedBlocks(bpid, bs);\n        bs.clear();\n      }\n      numDeletedBlocks \u003d 0;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java"
    }
  }
}