{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WebHdfsFileSystem.java",
  "functionName": "initializeInputStream",
  "functionId": "initializeInputStream___conn-HttpURLConnection",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
  "functionStartLine": 2432,
  "functionEndLine": 2455,
  "numCommitsSeen": 73,
  "timeTaken": 2353,
  "changeHistory": [
    "867048c3e4b20ece0039a876def129fa5eb9234f"
  ],
  "changeHistoryShort": {
    "867048c3e4b20ece0039a876def129fa5eb9234f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "867048c3e4b20ece0039a876def129fa5eb9234f": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7163. WebHdfsFileSystem should retry reads according to the configured retry policy. Contributed by Eric Payne.\n",
      "commitDate": "22/12/15 12:08 PM",
      "commitName": "867048c3e4b20ece0039a876def129fa5eb9234f",
      "commitAuthor": "Kihwal Lee",
      "diff": "@@ -0,0 +1,24 @@\n+    InputStream initializeInputStream(HttpURLConnection conn)\n+        throws IOException {\n+      // Cache the resolved URL so that it can be used in the event of\n+      // a future seek operation.\n+      resolvedUrl \u003d removeOffsetParam(conn.getURL());\n+      final String cl \u003d conn.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n+      InputStream inStream \u003d conn.getInputStream();\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"open file: \" + conn.getURL());\n+      }\n+      if (cl !\u003d null) {\n+        long streamLength \u003d Long.parseLong(cl);\n+        fileLength \u003d pos + streamLength;\n+        // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n+        // the reads so the transfer blocks until the server times out\n+        inStream \u003d new BoundedInputStream(inStream, streamLength);\n+      } else {\n+        fileLength \u003d getHdfsFileStatus(path).getLen();\n+      }\n+      // Wrapping in BufferedInputStream because it is more performant than\n+      // BoundedInputStream by itself.\n+      runnerState \u003d RunnerState.OPEN;\n+      return new BufferedInputStream(inStream, bufferSize);\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    InputStream initializeInputStream(HttpURLConnection conn)\n        throws IOException {\n      // Cache the resolved URL so that it can be used in the event of\n      // a future seek operation.\n      resolvedUrl \u003d removeOffsetParam(conn.getURL());\n      final String cl \u003d conn.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n      InputStream inStream \u003d conn.getInputStream();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"open file: \" + conn.getURL());\n      }\n      if (cl !\u003d null) {\n        long streamLength \u003d Long.parseLong(cl);\n        fileLength \u003d pos + streamLength;\n        // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n        // the reads so the transfer blocks until the server times out\n        inStream \u003d new BoundedInputStream(inStream, streamLength);\n      } else {\n        fileLength \u003d getHdfsFileStatus(path).getLen();\n      }\n      // Wrapping in BufferedInputStream because it is more performant than\n      // BoundedInputStream by itself.\n      runnerState \u003d RunnerState.OPEN;\n      return new BufferedInputStream(inStream, bufferSize);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java"
    }
  }
}