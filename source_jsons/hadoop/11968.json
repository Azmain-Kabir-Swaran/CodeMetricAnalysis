{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "getBlockInputStream",
  "functionId": "getBlockInputStream___b-ExtendedBlock__seekOffset-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 810,
  "functionEndLine": 827,
  "numCommitsSeen": 230,
  "timeTaken": 12166,
  "changeHistory": [
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8",
    "8a77a224c734bea0eb490f30c908748458c190c3",
    "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a",
    "29b411d5f065f177de05a69d2822836209f813c6",
    "86c9862bec0248d671e657aa56094a2919b8ac14",
    "d085eb15d7ca7b43a69bd70bad4e2ea601ba2ae0",
    "b2d5ed36bcb80e2581191dcdc3976e825c959142",
    "bbb24fbf5d220fbe137d43651ba3802a9806b1a3",
    "bc13dfb1426944ce45293cb8f444239a7406762c",
    "11cf658d0a8c3fb0f0822c9fc60f18f2ae5bf629",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8": "Ybodychange",
    "8a77a224c734bea0eb490f30c908748458c190c3": "Ybodychange",
    "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a": "Ybodychange",
    "29b411d5f065f177de05a69d2822836209f813c6": "Ybodychange",
    "86c9862bec0248d671e657aa56094a2919b8ac14": "Ybodychange",
    "d085eb15d7ca7b43a69bd70bad4e2ea601ba2ae0": "Ybodychange",
    "b2d5ed36bcb80e2581191dcdc3976e825c959142": "Ybodychange",
    "bbb24fbf5d220fbe137d43651ba3802a9806b1a3": "Ybodychange",
    "bc13dfb1426944ce45293cb8f444239a7406762c": "Ymovefromfile",
    "11cf658d0a8c3fb0f0822c9fc60f18f2ae5bf629": "Ymultichange(Ymodifierchange,Ybodychange)",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15150. Introduce read write lock to Datanode. Contributed Stephen O\u0027Donnell.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "11/02/20 8:00 AM",
      "commitName": "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "28/01/20 10:10 AM",
      "commitNameOld": "1839c467f60cbb8592d446694ec3d7710cda5142",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 13.91,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public InputStream getBlockInputStream(ExtendedBlock b,\n       long seekOffset) throws IOException {\n \n     ReplicaInfo info;\n-    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n+    try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n       info \u003d volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n     }\n \n     if (info !\u003d null \u0026\u0026 info.getVolume().isTransientStorage()) {\n       ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n       datanode.getMetrics().incrRamDiskBlocksReadHits();\n     }\n \n     if (info \u003d\u003d null) {\n       throw new IOException(\"No data exists for block \" + b);\n     }\n     return getBlockInputStreamWithCheckingPmemCache(info, b, seekOffset);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n\n    ReplicaInfo info;\n    try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n      info \u003d volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n    }\n\n    if (info !\u003d null \u0026\u0026 info.getVolume().isTransientStorage()) {\n      ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n      datanode.getMetrics().incrRamDiskBlocksReadHits();\n    }\n\n    if (info \u003d\u003d null) {\n      throw new IOException(\"No data exists for block \" + b);\n    }\n    return getBlockInputStreamWithCheckingPmemCache(info, b, seekOffset);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "8a77a224c734bea0eb490f30c908748458c190c3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13359. DataXceiver hung due to the lock in FsDatasetImpl#getBlockInputStream. Contributed by Yiqun Lin.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "09/08/19 6:40 PM",
      "commitName": "8a77a224c734bea0eb490f30c908748458c190c3",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "08/08/19 1:36 PM",
      "commitNameOld": "b0799148cf6e92be540f5665bb571418b916d789",
      "commitAuthorOld": "Stephen O\u0027Donnell",
      "daysBetweenCommits": 1.21,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public InputStream getBlockInputStream(ExtendedBlock b,\n       long seekOffset) throws IOException {\n \n     ReplicaInfo info;\n-    synchronized(this) {\n+    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       info \u003d volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n     }\n \n     if (info !\u003d null \u0026\u0026 info.getVolume().isTransientStorage()) {\n       ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n       datanode.getMetrics().incrRamDiskBlocksReadHits();\n     }\n \n     if (info \u003d\u003d null) {\n       throw new IOException(\"No data exists for block \" + b);\n     }\n     return getBlockInputStreamWithCheckingPmemCache(info, b, seekOffset);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n\n    ReplicaInfo info;\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      info \u003d volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n    }\n\n    if (info !\u003d null \u0026\u0026 info.getVolume().isTransientStorage()) {\n      ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n      datanode.getMetrics().incrRamDiskBlocksReadHits();\n    }\n\n    if (info \u003d\u003d null) {\n      throw new IOException(\"No data exists for block \" + b);\n    }\n    return getBlockInputStreamWithCheckingPmemCache(info, b, seekOffset);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14355 : Implement HDFS cache on SCM by using pure java mapped byte buffer. Contributed by Feilong He.\n",
      "commitDate": "30/03/19 11:33 PM",
      "commitName": "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "13/02/19 10:11 AM",
      "commitNameOld": "29b411d5f065f177de05a69d2822836209f813c6",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 45.52,
      "commitsBetweenForRepo": 382,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,18 @@\n   public InputStream getBlockInputStream(ExtendedBlock b,\n       long seekOffset) throws IOException {\n \n     ReplicaInfo info;\n     synchronized(this) {\n       info \u003d volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n     }\n \n     if (info !\u003d null \u0026\u0026 info.getVolume().isTransientStorage()) {\n       ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n       datanode.getMetrics().incrRamDiskBlocksReadHits();\n     }\n \n-    if (info !\u003d null) {\n-      return info.getDataInputStream(seekOffset);\n-    } else {\n+    if (info \u003d\u003d null) {\n       throw new IOException(\"No data exists for block \" + b);\n     }\n+    return getBlockInputStreamWithCheckingPmemCache(info, b, seekOffset);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n\n    ReplicaInfo info;\n    synchronized(this) {\n      info \u003d volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n    }\n\n    if (info !\u003d null \u0026\u0026 info.getVolume().isTransientStorage()) {\n      ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n      datanode.getMetrics().incrRamDiskBlocksReadHits();\n    }\n\n    if (info \u003d\u003d null) {\n      throw new IOException(\"No data exists for block \" + b);\n    }\n    return getBlockInputStreamWithCheckingPmemCache(info, b, seekOffset);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "29b411d5f065f177de05a69d2822836209f813c6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14263. Remove unnecessary block file exists check from FsDatasetImpl#getBlockInputStream(). Contributed by Surendra Singh Lilhore\n",
      "commitDate": "13/02/19 10:11 AM",
      "commitName": "29b411d5f065f177de05a69d2822836209f813c6",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "01/10/18 6:43 PM",
      "commitNameOld": "5689355783de005ebc604f4403dc5129a286bfca",
      "commitAuthorOld": "Yiqun Lin",
      "daysBetweenCommits": 134.69,
      "commitsBetweenForRepo": 1009,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   public InputStream getBlockInputStream(ExtendedBlock b,\n       long seekOffset) throws IOException {\n \n     ReplicaInfo info;\n     synchronized(this) {\n       info \u003d volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n     }\n \n     if (info !\u003d null \u0026\u0026 info.getVolume().isTransientStorage()) {\n       ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n       datanode.getMetrics().incrRamDiskBlocksReadHits();\n     }\n \n-    if(info !\u003d null \u0026\u0026 info.blockDataExists()) {\n+    if (info !\u003d null) {\n       return info.getDataInputStream(seekOffset);\n     } else {\n       throw new IOException(\"No data exists for block \" + b);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n\n    ReplicaInfo info;\n    synchronized(this) {\n      info \u003d volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n    }\n\n    if (info !\u003d null \u0026\u0026 info.getVolume().isTransientStorage()) {\n      ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n      datanode.getMetrics().incrRamDiskBlocksReadHits();\n    }\n\n    if (info !\u003d null) {\n      return info.getDataInputStream(seekOffset);\n    } else {\n      throw new IOException(\"No data exists for block \" + b);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "86c9862bec0248d671e657aa56094a2919b8ac14": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "13/09/16 12:54 PM",
      "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "10/09/16 6:22 PM",
      "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.77,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,19 @@\n   public InputStream getBlockInputStream(ExtendedBlock b,\n       long seekOffset) throws IOException {\n-    File blockFile \u003d getBlockFileNoExistsCheck(b, true);\n-    if (isNativeIOAvailable) {\n-      return NativeIO.getShareDeleteFileInputStream(blockFile, seekOffset);\n+\n+    ReplicaInfo info;\n+    synchronized(this) {\n+      info \u003d volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n+    }\n+\n+    if (info !\u003d null \u0026\u0026 info.getVolume().isTransientStorage()) {\n+      ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n+      datanode.getMetrics().incrRamDiskBlocksReadHits();\n+    }\n+\n+    if(info !\u003d null \u0026\u0026 info.blockDataExists()) {\n+      return info.getDataInputStream(seekOffset);\n     } else {\n-      try {\n-        return openAndSeek(blockFile, seekOffset);\n-      } catch (FileNotFoundException fnfe) {\n-        throw new IOException(\"Block \" + b + \" is not valid. \" +\n-            \"Expected block file at \" + blockFile + \" does not exist.\");\n-      }\n+      throw new IOException(\"No data exists for block \" + b);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n\n    ReplicaInfo info;\n    synchronized(this) {\n      info \u003d volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());\n    }\n\n    if (info !\u003d null \u0026\u0026 info.getVolume().isTransientStorage()) {\n      ramDiskReplicaTracker.touch(b.getBlockPoolId(), b.getBlockId());\n      datanode.getMetrics().incrRamDiskBlocksReadHits();\n    }\n\n    if(info !\u003d null \u0026\u0026 info.blockDataExists()) {\n      return info.getDataInputStream(seekOffset);\n    } else {\n      throw new IOException(\"No data exists for block \" + b);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "d085eb15d7ca7b43a69bd70bad4e2ea601ba2ae0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7696. In FsDatasetImpl, the getBlockInputStream(..) and getTmpInputStreams(..) methods may leak file descriptors.\n",
      "commitDate": "02/02/15 1:38 PM",
      "commitName": "d085eb15d7ca7b43a69bd70bad4e2ea601ba2ae0",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "28/01/15 4:00 PM",
      "commitNameOld": "5a0051f4da6e102846d795a7965a6a18216d74f7",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 4.9,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,14 @@\n   public InputStream getBlockInputStream(ExtendedBlock b,\n       long seekOffset) throws IOException {\n     File blockFile \u003d getBlockFileNoExistsCheck(b, true);\n     if (isNativeIOAvailable) {\n       return NativeIO.getShareDeleteFileInputStream(blockFile, seekOffset);\n     } else {\n-      RandomAccessFile blockInFile;\n       try {\n-        blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n+        return openAndSeek(blockFile, seekOffset);\n       } catch (FileNotFoundException fnfe) {\n         throw new IOException(\"Block \" + b + \" is not valid. \" +\n             \"Expected block file at \" + blockFile + \" does not exist.\");\n       }\n-\n-      if (seekOffset \u003e 0) {\n-        blockInFile.seek(seekOffset);\n-      }\n-      return new FileInputStream(blockInFile.getFD());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n    File blockFile \u003d getBlockFileNoExistsCheck(b, true);\n    if (isNativeIOAvailable) {\n      return NativeIO.getShareDeleteFileInputStream(blockFile, seekOffset);\n    } else {\n      try {\n        return openAndSeek(blockFile, seekOffset);\n      } catch (FileNotFoundException fnfe) {\n        throw new IOException(\"Block \" + b + \" is not valid. \" +\n            \"Expected block file at \" + blockFile + \" does not exist.\");\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "b2d5ed36bcb80e2581191dcdc3976e825c959142": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7100. Make eviction scheme pluggable. (Arpit Agarwal)\n",
      "commitDate": "20/09/14 1:25 PM",
      "commitName": "b2d5ed36bcb80e2581191dcdc3976e825c959142",
      "commitAuthor": "arp",
      "commitDateOld": "19/09/14 10:02 AM",
      "commitNameOld": "222bf0fe6706ee43964fd39b8315c1a339fbc84a",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.14,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   public InputStream getBlockInputStream(ExtendedBlock b,\n       long seekOffset) throws IOException {\n-    File blockFile \u003d getBlockFileNoExistsCheck(b);\n+    File blockFile \u003d getBlockFileNoExistsCheck(b, true);\n     if (isNativeIOAvailable) {\n       return NativeIO.getShareDeleteFileInputStream(blockFile, seekOffset);\n     } else {\n       RandomAccessFile blockInFile;\n       try {\n         blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n       } catch (FileNotFoundException fnfe) {\n         throw new IOException(\"Block \" + b + \" is not valid. \" +\n             \"Expected block file at \" + blockFile + \" does not exist.\");\n       }\n \n       if (seekOffset \u003e 0) {\n         blockInFile.seek(seekOffset);\n       }\n       return new FileInputStream(blockInFile.getFD());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n    File blockFile \u003d getBlockFileNoExistsCheck(b, true);\n    if (isNativeIOAvailable) {\n      return NativeIO.getShareDeleteFileInputStream(blockFile, seekOffset);\n    } else {\n      RandomAccessFile blockInFile;\n      try {\n        blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n      } catch (FileNotFoundException fnfe) {\n        throw new IOException(\"Block \" + b + \" is not valid. \" +\n            \"Expected block file at \" + blockFile + \" does not exist.\");\n      }\n\n      if (seekOffset \u003e 0) {\n        blockInFile.seek(seekOffset);\n      }\n      return new FileInputStream(blockInFile.getFD());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "bbb24fbf5d220fbe137d43651ba3802a9806b1a3": {
      "type": "Ybodychange",
      "commitMessage": "Merge trunk into branch.\n\nConflicts resolved:\nC       hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestShortCircuitLocalRead.java\n!     C hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java\n\n(thanks to Colin for help resolving)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1462652 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/03/13 2:33 PM",
      "commitName": "bbb24fbf5d220fbe137d43651ba3802a9806b1a3",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "15/02/13 5:12 PM",
      "commitNameOld": "f61581501aa774fd67cf1ac72693ee88285e87e1",
      "commitAuthorOld": "",
      "daysBetweenCommits": 41.85,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,20 @@\n   public InputStream getBlockInputStream(ExtendedBlock b,\n       long seekOffset) throws IOException {\n     File blockFile \u003d getBlockFileNoExistsCheck(b);\n-    RandomAccessFile blockInFile;\n-    try {\n-      blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n-    } catch (FileNotFoundException fnfe) {\n-      throw new IOException(\"Block \" + b + \" is not valid. \" +\n-          \"Expected block file at \" + blockFile + \" does not exist.\");\n-    }\n+    if (isNativeIOAvailable) {\n+      return NativeIO.getShareDeleteFileInputStream(blockFile, seekOffset);\n+    } else {\n+      RandomAccessFile blockInFile;\n+      try {\n+        blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n+      } catch (FileNotFoundException fnfe) {\n+        throw new IOException(\"Block \" + b + \" is not valid. \" +\n+            \"Expected block file at \" + blockFile + \" does not exist.\");\n+      }\n \n-    if (seekOffset \u003e 0) {\n-      blockInFile.seek(seekOffset);\n+      if (seekOffset \u003e 0) {\n+        blockInFile.seek(seekOffset);\n+      }\n+      return new FileInputStream(blockInFile.getFD());\n     }\n-    return new FileInputStream(blockInFile.getFD());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n    File blockFile \u003d getBlockFileNoExistsCheck(b);\n    if (isNativeIOAvailable) {\n      return NativeIO.getShareDeleteFileInputStream(blockFile, seekOffset);\n    } else {\n      RandomAccessFile blockInFile;\n      try {\n        blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n      } catch (FileNotFoundException fnfe) {\n        throw new IOException(\"Block \" + b + \" is not valid. \" +\n            \"Expected block file at \" + blockFile + \" does not exist.\");\n      }\n\n      if (seekOffset \u003e 0) {\n        blockInFile.seek(seekOffset);\n      }\n      return new FileInputStream(blockInFile.getFD());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "bc13dfb1426944ce45293cb8f444239a7406762c": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/04/12 10:38 AM",
      "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/04/12 8:48 PM",
      "commitNameOld": "a4ccb8f504e79802f1b3c69acbcbb00b2343c529",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.58,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n    File blockFile \u003d getBlockFileNoExistsCheck(b);\n    RandomAccessFile blockInFile;\n    try {\n      blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    } catch (FileNotFoundException fnfe) {\n      throw new IOException(\"Block \" + b + \" is not valid. \" +\n          \"Expected block file at \" + blockFile + \" does not exist.\");\n    }\n\n    if (seekOffset \u003e 0) {\n      blockInFile.seek(seekOffset);\n    }\n    return new FileInputStream(blockInFile.getFD());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
        "oldMethodName": "getBlockInputStream",
        "newMethodName": "getBlockInputStream"
      }
    },
    "11cf658d0a8c3fb0f0822c9fc60f18f2ae5bf629": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-2533. Remove needless synchronization on some FSDataSet methods. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196902 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/11/11 6:00 PM",
      "commitName": "11cf658d0a8c3fb0f0822c9fc60f18f2ae5bf629",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2533. Remove needless synchronization on some FSDataSet methods. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196902 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "02/11/11 6:00 PM",
          "commitName": "11cf658d0a8c3fb0f0822c9fc60f18f2ae5bf629",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "27/10/11 3:47 PM",
          "commitNameOld": "221aadbc5b35b043fbc62c417b0edc029db9d004",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 6.09,
          "commitsBetweenForRepo": 82,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,16 @@\n-  public synchronized InputStream getBlockInputStream(ExtendedBlock b,\n+  public InputStream getBlockInputStream(ExtendedBlock b,\n       long seekOffset) throws IOException {\n-    File blockFile \u003d getBlockFile(b);\n-    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n+    File blockFile \u003d getBlockFileNoExistsCheck(b);\n+    RandomAccessFile blockInFile;\n+    try {\n+      blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n+    } catch (FileNotFoundException fnfe) {\n+      throw new IOException(\"Block \" + b + \" is not valid. \" +\n+          \"Expected block file at \" + blockFile + \" does not exist.\");\n+    }\n+\n     if (seekOffset \u003e 0) {\n       blockInFile.seek(seekOffset);\n     }\n     return new FileInputStream(blockInFile.getFD());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n    File blockFile \u003d getBlockFileNoExistsCheck(b);\n    RandomAccessFile blockInFile;\n    try {\n      blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    } catch (FileNotFoundException fnfe) {\n      throw new IOException(\"Block \" + b + \" is not valid. \" +\n          \"Expected block file at \" + blockFile + \" does not exist.\");\n    }\n\n    if (seekOffset \u003e 0) {\n      blockInFile.seek(seekOffset);\n    }\n    return new FileInputStream(blockInFile.getFD());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2533. Remove needless synchronization on some FSDataSet methods. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196902 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "02/11/11 6:00 PM",
          "commitName": "11cf658d0a8c3fb0f0822c9fc60f18f2ae5bf629",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "27/10/11 3:47 PM",
          "commitNameOld": "221aadbc5b35b043fbc62c417b0edc029db9d004",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 6.09,
          "commitsBetweenForRepo": 82,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,16 @@\n-  public synchronized InputStream getBlockInputStream(ExtendedBlock b,\n+  public InputStream getBlockInputStream(ExtendedBlock b,\n       long seekOffset) throws IOException {\n-    File blockFile \u003d getBlockFile(b);\n-    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n+    File blockFile \u003d getBlockFileNoExistsCheck(b);\n+    RandomAccessFile blockInFile;\n+    try {\n+      blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n+    } catch (FileNotFoundException fnfe) {\n+      throw new IOException(\"Block \" + b + \" is not valid. \" +\n+          \"Expected block file at \" + blockFile + \" does not exist.\");\n+    }\n+\n     if (seekOffset \u003e 0) {\n       blockInFile.seek(seekOffset);\n     }\n     return new FileInputStream(blockInFile.getFD());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n    File blockFile \u003d getBlockFileNoExistsCheck(b);\n    RandomAccessFile blockInFile;\n    try {\n      blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    } catch (FileNotFoundException fnfe) {\n      throw new IOException(\"Block \" + b + \" is not valid. \" +\n          \"Expected block file at \" + blockFile + \" does not exist.\");\n    }\n\n    if (seekOffset \u003e 0) {\n      blockInFile.seek(seekOffset);\n    }\n    return new FileInputStream(blockInFile.getFD());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
          "extendedDetails": {}
        }
      ]
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n    File blockFile \u003d getBlockFile(b);\n    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    if (seekOffset \u003e 0) {\n      blockInFile.seek(seekOffset);\n    }\n    return new FileInputStream(blockInFile.getFD());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n    File blockFile \u003d getBlockFile(b);\n    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    if (seekOffset \u003e 0) {\n      blockInFile.seek(seekOffset);\n    }\n    return new FileInputStream(blockInFile.getFD());\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,9 @@\n+  public synchronized InputStream getBlockInputStream(ExtendedBlock b,\n+      long seekOffset) throws IOException {\n+    File blockFile \u003d getBlockFile(b);\n+    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n+    if (seekOffset \u003e 0) {\n+      blockInFile.seek(seekOffset);\n+    }\n+    return new FileInputStream(blockInFile.getFD());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized InputStream getBlockInputStream(ExtendedBlock b,\n      long seekOffset) throws IOException {\n    File blockFile \u003d getBlockFile(b);\n    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    if (seekOffset \u003e 0) {\n      blockInFile.seek(seekOffset);\n    }\n    return new FileInputStream(blockInFile.getFD());\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
    }
  }
}