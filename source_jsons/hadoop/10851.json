{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StripedBlockReconstructor.java",
  "functionName": "reconstruct",
  "functionId": "reconstruct",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
  "functionStartLine": 87,
  "functionEndLine": 121,
  "numCommitsSeen": 18,
  "timeTaken": 3384,
  "changeHistory": [
    "9367c25dbdfedf60cdbd65611281cf9c667829e6",
    "73aed34dffa5e79f6f819137b69054c1dee2d4dd",
    "d749cf65e1ab0e0daf5be86931507183f189e855",
    "3c18a53cbd2efabb2ad108d63a0b0b558424115f"
  ],
  "changeHistoryShort": {
    "9367c25dbdfedf60cdbd65611281cf9c667829e6": "Ybodychange",
    "73aed34dffa5e79f6f819137b69054c1dee2d4dd": "Ybodychange",
    "d749cf65e1ab0e0daf5be86931507183f189e855": "Ymultichange(Ymovefromfile,Ybodychange,Yrename)",
    "3c18a53cbd2efabb2ad108d63a0b0b558424115f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "9367c25dbdfedf60cdbd65611281cf9c667829e6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12482. Provide a configuration to adjust the weight of EC recovery tasks to adjust the speed of recovery. (lei)\n",
      "commitDate": "31/10/17 9:58 PM",
      "commitName": "9367c25dbdfedf60cdbd65611281cf9c667829e6",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "16/10/17 7:44 PM",
      "commitNameOld": "31ebccc96238136560f4210bdf6766fe18e0650c",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 15.09,
      "commitsBetweenForRepo": 94,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,35 @@\n   void reconstruct() throws IOException {\n     while (getPositionInBlock() \u003c getMaxTargetLength()) {\n+      DataNodeFaultInjector.get().stripedBlockReconstruction();\n       long remaining \u003d getMaxTargetLength() - getPositionInBlock();\n       final int toReconstructLen \u003d\n           (int) Math.min(getStripedReader().getBufferSize(), remaining);\n \n       long start \u003d Time.monotonicNow();\n       // step1: read from minimum source DNs required for reconstruction.\n       // The returned success list is the source DNs we do real read from\n       getStripedReader().readMinimumSources(toReconstructLen);\n       long readEnd \u003d Time.monotonicNow();\n \n       // step2: decode to reconstruct targets\n       reconstructTargets(toReconstructLen);\n       long decodeEnd \u003d Time.monotonicNow();\n \n       // step3: transfer data\n       if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n         String error \u003d \"Transfer failed for all targets.\";\n         throw new IOException(error);\n       }\n       long writeEnd \u003d Time.monotonicNow();\n \n       // Only the succeed reconstructions are recorded.\n       final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n       metrics.incrECReconstructionReadTime(readEnd - start);\n       metrics.incrECReconstructionDecodingTime(decodeEnd - readEnd);\n       metrics.incrECReconstructionWriteTime(writeEnd - decodeEnd);\n \n       updatePositionInBlock(toReconstructLen);\n \n       clearBuffers();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void reconstruct() throws IOException {\n    while (getPositionInBlock() \u003c getMaxTargetLength()) {\n      DataNodeFaultInjector.get().stripedBlockReconstruction();\n      long remaining \u003d getMaxTargetLength() - getPositionInBlock();\n      final int toReconstructLen \u003d\n          (int) Math.min(getStripedReader().getBufferSize(), remaining);\n\n      long start \u003d Time.monotonicNow();\n      // step1: read from minimum source DNs required for reconstruction.\n      // The returned success list is the source DNs we do real read from\n      getStripedReader().readMinimumSources(toReconstructLen);\n      long readEnd \u003d Time.monotonicNow();\n\n      // step2: decode to reconstruct targets\n      reconstructTargets(toReconstructLen);\n      long decodeEnd \u003d Time.monotonicNow();\n\n      // step3: transfer data\n      if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n        String error \u003d \"Transfer failed for all targets.\";\n        throw new IOException(error);\n      }\n      long writeEnd \u003d Time.monotonicNow();\n\n      // Only the succeed reconstructions are recorded.\n      final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n      metrics.incrECReconstructionReadTime(readEnd - start);\n      metrics.incrECReconstructionDecodingTime(decodeEnd - readEnd);\n      metrics.incrECReconstructionWriteTime(writeEnd - decodeEnd);\n\n      updatePositionInBlock(toReconstructLen);\n\n      clearBuffers();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
      "extendedDetails": {}
    },
    "73aed34dffa5e79f6f819137b69054c1dee2d4dd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12409. Add metrics of execution time of different stages in EC recovery task. (Lei (Eddy) Xu)\n",
      "commitDate": "13/09/17 5:14 PM",
      "commitName": "73aed34dffa5e79f6f819137b69054c1dee2d4dd",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "28/07/17 10:50 AM",
      "commitNameOld": "77791e4c36ddc9305306c83806bf486d4d32575d",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 47.27,
      "commitsBetweenForRepo": 466,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,34 @@\n   void reconstruct() throws IOException {\n     while (getPositionInBlock() \u003c getMaxTargetLength()) {\n       long remaining \u003d getMaxTargetLength() - getPositionInBlock();\n       final int toReconstructLen \u003d\n           (int) Math.min(getStripedReader().getBufferSize(), remaining);\n \n+      long start \u003d Time.monotonicNow();\n       // step1: read from minimum source DNs required for reconstruction.\n       // The returned success list is the source DNs we do real read from\n       getStripedReader().readMinimumSources(toReconstructLen);\n+      long readEnd \u003d Time.monotonicNow();\n \n       // step2: decode to reconstruct targets\n       reconstructTargets(toReconstructLen);\n+      long decodeEnd \u003d Time.monotonicNow();\n \n       // step3: transfer data\n       if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n         String error \u003d \"Transfer failed for all targets.\";\n         throw new IOException(error);\n       }\n+      long writeEnd \u003d Time.monotonicNow();\n+\n+      // Only the succeed reconstructions are recorded.\n+      final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n+      metrics.incrECReconstructionReadTime(readEnd - start);\n+      metrics.incrECReconstructionDecodingTime(decodeEnd - readEnd);\n+      metrics.incrECReconstructionWriteTime(writeEnd - decodeEnd);\n \n       updatePositionInBlock(toReconstructLen);\n \n       clearBuffers();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void reconstruct() throws IOException {\n    while (getPositionInBlock() \u003c getMaxTargetLength()) {\n      long remaining \u003d getMaxTargetLength() - getPositionInBlock();\n      final int toReconstructLen \u003d\n          (int) Math.min(getStripedReader().getBufferSize(), remaining);\n\n      long start \u003d Time.monotonicNow();\n      // step1: read from minimum source DNs required for reconstruction.\n      // The returned success list is the source DNs we do real read from\n      getStripedReader().readMinimumSources(toReconstructLen);\n      long readEnd \u003d Time.monotonicNow();\n\n      // step2: decode to reconstruct targets\n      reconstructTargets(toReconstructLen);\n      long decodeEnd \u003d Time.monotonicNow();\n\n      // step3: transfer data\n      if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n        String error \u003d \"Transfer failed for all targets.\";\n        throw new IOException(error);\n      }\n      long writeEnd \u003d Time.monotonicNow();\n\n      // Only the succeed reconstructions are recorded.\n      final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n      metrics.incrECReconstructionReadTime(readEnd - start);\n      metrics.incrECReconstructionDecodingTime(decodeEnd - readEnd);\n      metrics.incrECReconstructionWriteTime(writeEnd - decodeEnd);\n\n      updatePositionInBlock(toReconstructLen);\n\n      clearBuffers();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
      "extendedDetails": {}
    },
    "d749cf65e1ab0e0daf5be86931507183f189e855": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange,Yrename)",
      "commitMessage": "HDFS-9833. Erasure coding: recomputing block checksum on the fly by reconstructing the missed/corrupt block data. Contributed by Rakesh R.\n",
      "commitDate": "01/06/16 9:56 PM",
      "commitName": "d749cf65e1ab0e0daf5be86931507183f189e855",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-9833. Erasure coding: recomputing block checksum on the fly by reconstructing the missed/corrupt block data. Contributed by Rakesh R.\n",
          "commitDate": "01/06/16 9:56 PM",
          "commitName": "d749cf65e1ab0e0daf5be86931507183f189e855",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "31/05/16 5:54 PM",
          "commitNameOld": "8ceb06e2392763726210f96bb1c176e6a9fe7b53",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 1.17,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,24 @@\n-  void reconstructAndTransfer() throws IOException {\n-    while (positionInBlock \u003c stripedWriter.getMaxTargetLength()) {\n-      long remaining \u003d stripedWriter.getMaxTargetLength() - positionInBlock;\n+  void reconstruct() throws IOException {\n+    while (getPositionInBlock() \u003c getMaxTargetLength()) {\n+      long remaining \u003d getMaxTargetLength() - getPositionInBlock();\n       final int toReconstructLen \u003d\n-          (int) Math.min(stripedReader.getBufferSize(), remaining);\n+          (int) Math.min(getStripedReader().getBufferSize(), remaining);\n+\n       // step1: read from minimum source DNs required for reconstruction.\n       // The returned success list is the source DNs we do real read from\n-      stripedReader.readMinimumSources(toReconstructLen);\n+      getStripedReader().readMinimumSources(toReconstructLen);\n \n       // step2: decode to reconstruct targets\n       reconstructTargets(toReconstructLen);\n \n       // step3: transfer data\n       if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n         String error \u003d \"Transfer failed for all targets.\";\n         throw new IOException(error);\n       }\n \n-      positionInBlock +\u003d toReconstructLen;\n+      updatePositionInBlock(toReconstructLen);\n \n       clearBuffers();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void reconstruct() throws IOException {\n    while (getPositionInBlock() \u003c getMaxTargetLength()) {\n      long remaining \u003d getMaxTargetLength() - getPositionInBlock();\n      final int toReconstructLen \u003d\n          (int) Math.min(getStripedReader().getBufferSize(), remaining);\n\n      // step1: read from minimum source DNs required for reconstruction.\n      // The returned success list is the source DNs we do real read from\n      getStripedReader().readMinimumSources(toReconstructLen);\n\n      // step2: decode to reconstruct targets\n      reconstructTargets(toReconstructLen);\n\n      // step3: transfer data\n      if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n        String error \u003d \"Transfer failed for all targets.\";\n        throw new IOException(error);\n      }\n\n      updatePositionInBlock(toReconstructLen);\n\n      clearBuffers();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructor.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
            "oldMethodName": "reconstructAndTransfer",
            "newMethodName": "reconstruct"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9833. Erasure coding: recomputing block checksum on the fly by reconstructing the missed/corrupt block data. Contributed by Rakesh R.\n",
          "commitDate": "01/06/16 9:56 PM",
          "commitName": "d749cf65e1ab0e0daf5be86931507183f189e855",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "31/05/16 5:54 PM",
          "commitNameOld": "8ceb06e2392763726210f96bb1c176e6a9fe7b53",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 1.17,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,24 @@\n-  void reconstructAndTransfer() throws IOException {\n-    while (positionInBlock \u003c stripedWriter.getMaxTargetLength()) {\n-      long remaining \u003d stripedWriter.getMaxTargetLength() - positionInBlock;\n+  void reconstruct() throws IOException {\n+    while (getPositionInBlock() \u003c getMaxTargetLength()) {\n+      long remaining \u003d getMaxTargetLength() - getPositionInBlock();\n       final int toReconstructLen \u003d\n-          (int) Math.min(stripedReader.getBufferSize(), remaining);\n+          (int) Math.min(getStripedReader().getBufferSize(), remaining);\n+\n       // step1: read from minimum source DNs required for reconstruction.\n       // The returned success list is the source DNs we do real read from\n-      stripedReader.readMinimumSources(toReconstructLen);\n+      getStripedReader().readMinimumSources(toReconstructLen);\n \n       // step2: decode to reconstruct targets\n       reconstructTargets(toReconstructLen);\n \n       // step3: transfer data\n       if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n         String error \u003d \"Transfer failed for all targets.\";\n         throw new IOException(error);\n       }\n \n-      positionInBlock +\u003d toReconstructLen;\n+      updatePositionInBlock(toReconstructLen);\n \n       clearBuffers();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void reconstruct() throws IOException {\n    while (getPositionInBlock() \u003c getMaxTargetLength()) {\n      long remaining \u003d getMaxTargetLength() - getPositionInBlock();\n      final int toReconstructLen \u003d\n          (int) Math.min(getStripedReader().getBufferSize(), remaining);\n\n      // step1: read from minimum source DNs required for reconstruction.\n      // The returned success list is the source DNs we do real read from\n      getStripedReader().readMinimumSources(toReconstructLen);\n\n      // step2: decode to reconstruct targets\n      reconstructTargets(toReconstructLen);\n\n      // step3: transfer data\n      if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n        String error \u003d \"Transfer failed for all targets.\";\n        throw new IOException(error);\n      }\n\n      updatePositionInBlock(toReconstructLen);\n\n      clearBuffers();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "HDFS-9833. Erasure coding: recomputing block checksum on the fly by reconstructing the missed/corrupt block data. Contributed by Rakesh R.\n",
          "commitDate": "01/06/16 9:56 PM",
          "commitName": "d749cf65e1ab0e0daf5be86931507183f189e855",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "31/05/16 5:54 PM",
          "commitNameOld": "8ceb06e2392763726210f96bb1c176e6a9fe7b53",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 1.17,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,24 @@\n-  void reconstructAndTransfer() throws IOException {\n-    while (positionInBlock \u003c stripedWriter.getMaxTargetLength()) {\n-      long remaining \u003d stripedWriter.getMaxTargetLength() - positionInBlock;\n+  void reconstruct() throws IOException {\n+    while (getPositionInBlock() \u003c getMaxTargetLength()) {\n+      long remaining \u003d getMaxTargetLength() - getPositionInBlock();\n       final int toReconstructLen \u003d\n-          (int) Math.min(stripedReader.getBufferSize(), remaining);\n+          (int) Math.min(getStripedReader().getBufferSize(), remaining);\n+\n       // step1: read from minimum source DNs required for reconstruction.\n       // The returned success list is the source DNs we do real read from\n-      stripedReader.readMinimumSources(toReconstructLen);\n+      getStripedReader().readMinimumSources(toReconstructLen);\n \n       // step2: decode to reconstruct targets\n       reconstructTargets(toReconstructLen);\n \n       // step3: transfer data\n       if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n         String error \u003d \"Transfer failed for all targets.\";\n         throw new IOException(error);\n       }\n \n-      positionInBlock +\u003d toReconstructLen;\n+      updatePositionInBlock(toReconstructLen);\n \n       clearBuffers();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void reconstruct() throws IOException {\n    while (getPositionInBlock() \u003c getMaxTargetLength()) {\n      long remaining \u003d getMaxTargetLength() - getPositionInBlock();\n      final int toReconstructLen \u003d\n          (int) Math.min(getStripedReader().getBufferSize(), remaining);\n\n      // step1: read from minimum source DNs required for reconstruction.\n      // The returned success list is the source DNs we do real read from\n      getStripedReader().readMinimumSources(toReconstructLen);\n\n      // step2: decode to reconstruct targets\n      reconstructTargets(toReconstructLen);\n\n      // step3: transfer data\n      if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n        String error \u003d \"Transfer failed for all targets.\";\n        throw new IOException(error);\n      }\n\n      updatePositionInBlock(toReconstructLen);\n\n      clearBuffers();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
          "extendedDetails": {
            "oldValue": "reconstructAndTransfer",
            "newValue": "reconstruct"
          }
        }
      ]
    },
    "3c18a53cbd2efabb2ad108d63a0b0b558424115f": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9719. Refactoring ErasureCodingWorker into smaller reusable constructs. Contributed by Kai Zheng.\n",
      "commitDate": "06/04/16 10:50 PM",
      "commitName": "3c18a53cbd2efabb2ad108d63a0b0b558424115f",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,23 @@\n+  void reconstructAndTransfer() throws IOException {\n+    while (positionInBlock \u003c stripedWriter.getMaxTargetLength()) {\n+      long remaining \u003d stripedWriter.getMaxTargetLength() - positionInBlock;\n+      final int toReconstructLen \u003d\n+          (int) Math.min(stripedReader.getBufferSize(), remaining);\n+      // step1: read from minimum source DNs required for reconstruction.\n+      // The returned success list is the source DNs we do real read from\n+      stripedReader.readMinimumSources(toReconstructLen);\n+\n+      // step2: decode to reconstruct targets\n+      reconstructTargets(toReconstructLen);\n+\n+      // step3: transfer data\n+      if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n+        String error \u003d \"Transfer failed for all targets.\";\n+        throw new IOException(error);\n+      }\n+\n+      positionInBlock +\u003d toReconstructLen;\n+\n+      clearBuffers();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void reconstructAndTransfer() throws IOException {\n    while (positionInBlock \u003c stripedWriter.getMaxTargetLength()) {\n      long remaining \u003d stripedWriter.getMaxTargetLength() - positionInBlock;\n      final int toReconstructLen \u003d\n          (int) Math.min(stripedReader.getBufferSize(), remaining);\n      // step1: read from minimum source DNs required for reconstruction.\n      // The returned success list is the source DNs we do real read from\n      stripedReader.readMinimumSources(toReconstructLen);\n\n      // step2: decode to reconstruct targets\n      reconstructTargets(toReconstructLen);\n\n      // step3: transfer data\n      if (stripedWriter.transferData2Targets() \u003d\u003d 0) {\n        String error \u003d \"Transfer failed for all targets.\";\n        throw new IOException(error);\n      }\n\n      positionInBlock +\u003d toReconstructLen;\n\n      clearBuffers();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructor.java"
    }
  }
}