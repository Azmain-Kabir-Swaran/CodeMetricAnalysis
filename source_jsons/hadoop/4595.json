{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ClientNamenodeProtocolServerSideTranslatorPB.java",
  "functionName": "getAdditionalDatanode",
  "functionId": "getAdditionalDatanode___controller-RpcController__req-GetAdditionalDatanodeRequestProto",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
  "functionStartLine": 610,
  "functionEndLine": 632,
  "numCommitsSeen": 141,
  "timeTaken": 4245,
  "changeHistory": [
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "e2c9b288b223b9fd82dc12018936e13128413492",
    "f05c21285ef23b6a973d69f045b1cb46c5abc039",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
    "eb9f1b670726e1af03f2e940ce2696b880964972",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9",
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e",
    "48da033901d3471ef176a94104158546152353e9"
  ],
  "changeHistoryShort": {
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ybodychange",
    "e2c9b288b223b9fd82dc12018936e13128413492": "Ybodychange",
    "f05c21285ef23b6a973d69f045b1cb46c5abc039": "Ybodychange",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": "Ybodychange",
    "eb9f1b670726e1af03f2e940ce2696b880964972": "Ybodychange",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": "Ybodychange",
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e": "Ybodychange",
    "48da033901d3471ef176a94104158546152353e9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/08/15 2:38 PM",
      "commitNameOld": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 24.18,
      "commitsBetweenForRepo": 143,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n       RpcController controller, GetAdditionalDatanodeRequestProto req)\n       throws ServiceException {\n     try {\n       List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n       List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageUuidsList();\n       List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n       LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n           req.getFileId(), PBHelperClient.convert(req.getBlk()),\n-          PBHelper.convert(existingList.toArray(\n+          PBHelperClient.convert(existingList.toArray(\n               new DatanodeInfoProto[existingList.size()])),\n           existingStorageIDsList.toArray(\n               new String[existingStorageIDsList.size()]),\n-          PBHelper.convert(excludesList.toArray(\n-              new DatanodeInfoProto[excludesList.size()])), \n+          PBHelperClient.convert(excludesList.toArray(\n+              new DatanodeInfoProto[excludesList.size()])),\n           req.getNumAdditionalNodes(), req.getClientName());\n       return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n-          PBHelper.convert(result))\n+          PBHelperClient.convert(result))\n           .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n      RpcController controller, GetAdditionalDatanodeRequestProto req)\n      throws ServiceException {\n    try {\n      List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n      List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageUuidsList();\n      List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n      LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n          req.getFileId(), PBHelperClient.convert(req.getBlk()),\n          PBHelperClient.convert(existingList.toArray(\n              new DatanodeInfoProto[existingList.size()])),\n          existingStorageIDsList.toArray(\n              new String[existingStorageIDsList.size()]),\n          PBHelperClient.convert(excludesList.toArray(\n              new DatanodeInfoProto[excludesList.size()])),\n          req.getNumAdditionalNodes(), req.getClientName());\n      return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n          PBHelperClient.convert(result))\n          .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "e2c9b288b223b9fd82dc12018936e13128413492": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "28/08/15 2:38 PM",
      "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "22/08/15 1:31 PM",
      "commitNameOld": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.05,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n       RpcController controller, GetAdditionalDatanodeRequestProto req)\n       throws ServiceException {\n     try {\n       List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n       List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageUuidsList();\n       List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n       LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n-          req.getFileId(), PBHelper.convert(req.getBlk()),\n+          req.getFileId(), PBHelperClient.convert(req.getBlk()),\n           PBHelper.convert(existingList.toArray(\n               new DatanodeInfoProto[existingList.size()])),\n           existingStorageIDsList.toArray(\n               new String[existingStorageIDsList.size()]),\n           PBHelper.convert(excludesList.toArray(\n               new DatanodeInfoProto[excludesList.size()])), \n           req.getNumAdditionalNodes(), req.getClientName());\n       return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n           PBHelper.convert(result))\n           .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n      RpcController controller, GetAdditionalDatanodeRequestProto req)\n      throws ServiceException {\n    try {\n      List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n      List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageUuidsList();\n      List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n      LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n          req.getFileId(), PBHelperClient.convert(req.getBlk()),\n          PBHelper.convert(existingList.toArray(\n              new DatanodeInfoProto[existingList.size()])),\n          existingStorageIDsList.toArray(\n              new String[existingStorageIDsList.size()]),\n          PBHelper.convert(excludesList.toArray(\n              new DatanodeInfoProto[excludesList.size()])), \n          req.getNumAdditionalNodes(), req.getClientName());\n      return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n          PBHelper.convert(result))\n          .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "f05c21285ef23b6a973d69f045b1cb46c5abc039": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7853. Erasure coding: extend LocatedBlocks to support reading from striped files. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:32 AM",
      "commitName": "f05c21285ef23b6a973d69f045b1cb46c5abc039",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 24.06,
      "commitsBetweenForRepo": 279,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n       RpcController controller, GetAdditionalDatanodeRequestProto req)\n       throws ServiceException {\n     try {\n       List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n       List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageUuidsList();\n       List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n       LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n           req.getFileId(), PBHelper.convert(req.getBlk()),\n           PBHelper.convert(existingList.toArray(\n               new DatanodeInfoProto[existingList.size()])),\n           existingStorageIDsList.toArray(\n               new String[existingStorageIDsList.size()]),\n           PBHelper.convert(excludesList.toArray(\n               new DatanodeInfoProto[excludesList.size()])), \n           req.getNumAdditionalNodes(), req.getClientName());\n       return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n-          PBHelper.convert(result))\n+          PBHelper.convertLocatedBlock(result))\n           .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n      RpcController controller, GetAdditionalDatanodeRequestProto req)\n      throws ServiceException {\n    try {\n      List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n      List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageUuidsList();\n      List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n      LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n          req.getFileId(), PBHelper.convert(req.getBlk()),\n          PBHelper.convert(existingList.toArray(\n              new DatanodeInfoProto[existingList.size()])),\n          existingStorageIDsList.toArray(\n              new String[existingStorageIDsList.size()]),\n          PBHelper.convert(excludesList.toArray(\n              new DatanodeInfoProto[excludesList.size()])), \n          req.getNumAdditionalNodes(), req.getClientName());\n      return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n          PBHelper.convertLocatedBlock(result))\n          .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1593634 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/14 3:36 PM",
      "commitName": "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "19/02/14 11:57 AM",
      "commitNameOld": "72c214c89bed15e1c4b97c1f922d564f54491fed",
      "commitAuthorOld": "",
      "daysBetweenCommits": 79.11,
      "commitsBetweenForRepo": 576,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n       RpcController controller, GetAdditionalDatanodeRequestProto req)\n       throws ServiceException {\n     try {\n       List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n       List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageUuidsList();\n       List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n       LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n-          PBHelper.convert(req.getBlk()),\n+          req.getFileId(), PBHelper.convert(req.getBlk()),\n           PBHelper.convert(existingList.toArray(\n               new DatanodeInfoProto[existingList.size()])),\n           existingStorageIDsList.toArray(\n               new String[existingStorageIDsList.size()]),\n           PBHelper.convert(excludesList.toArray(\n               new DatanodeInfoProto[excludesList.size()])), \n           req.getNumAdditionalNodes(), req.getClientName());\n       return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n           PBHelper.convert(result))\n           .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n      RpcController controller, GetAdditionalDatanodeRequestProto req)\n      throws ServiceException {\n    try {\n      List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n      List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageUuidsList();\n      List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n      LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n          req.getFileId(), PBHelper.convert(req.getBlk()),\n          PBHelper.convert(existingList.toArray(\n              new DatanodeInfoProto[existingList.size()])),\n          existingStorageIDsList.toArray(\n              new String[existingStorageIDsList.size()]),\n          PBHelper.convert(excludesList.toArray(\n              new DatanodeInfoProto[excludesList.size()])), \n          req.getNumAdditionalNodes(), req.getClientName());\n      return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n          PBHelper.convert(result))\n          .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "eb9f1b670726e1af03f2e940ce2696b880964972": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5232. Protocol changes to transmit StorageUuid.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1525153 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/09/13 3:06 PM",
      "commitName": "eb9f1b670726e1af03f2e940ce2696b880964972",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "18/09/13 8:12 AM",
      "commitNameOld": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.29,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n       RpcController controller, GetAdditionalDatanodeRequestProto req)\n       throws ServiceException {\n     try {\n       List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n-      List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageIDsList();\n+      List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageUuidsList();\n       List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n       LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n           PBHelper.convert(req.getBlk()),\n           PBHelper.convert(existingList.toArray(\n               new DatanodeInfoProto[existingList.size()])),\n           existingStorageIDsList.toArray(\n               new String[existingStorageIDsList.size()]),\n           PBHelper.convert(excludesList.toArray(\n               new DatanodeInfoProto[excludesList.size()])), \n           req.getNumAdditionalNodes(), req.getClientName());\n       return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n           PBHelper.convert(result))\n           .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n      RpcController controller, GetAdditionalDatanodeRequestProto req)\n      throws ServiceException {\n    try {\n      List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n      List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageUuidsList();\n      List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n      LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n          PBHelper.convert(req.getBlk()),\n          PBHelper.convert(existingList.toArray(\n              new DatanodeInfoProto[existingList.size()])),\n          existingStorageIDsList.toArray(\n              new String[existingStorageIDsList.size()]),\n          PBHelper.convert(excludesList.toArray(\n              new DatanodeInfoProto[excludesList.size()])), \n          req.getNumAdditionalNodes(), req.getClientName());\n      return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n          PBHelper.convert(result))\n          .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4990. Change BlockPlacementPolicy to choose storages instead of datanodes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1524444 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/13 8:12 AM",
      "commitName": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "03/09/13 7:03 AM",
      "commitNameOld": "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 15.05,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,23 @@\n   public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n       RpcController controller, GetAdditionalDatanodeRequestProto req)\n       throws ServiceException {\n     try {\n       List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n+      List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageIDsList();\n       List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n-      LocatedBlock result \u003d server.getAdditionalDatanode(\n-          req.getSrc(), PBHelper.convert(req.getBlk()),\n+      LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n+          PBHelper.convert(req.getBlk()),\n           PBHelper.convert(existingList.toArray(\n               new DatanodeInfoProto[existingList.size()])),\n+          existingStorageIDsList.toArray(\n+              new String[existingStorageIDsList.size()]),\n           PBHelper.convert(excludesList.toArray(\n               new DatanodeInfoProto[excludesList.size()])), \n-              req.getNumAdditionalNodes(), req.getClientName());\n+          req.getNumAdditionalNodes(), req.getClientName());\n       return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n           PBHelper.convert(result))\n           .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n      RpcController controller, GetAdditionalDatanodeRequestProto req)\n      throws ServiceException {\n    try {\n      List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n      List\u003cString\u003e existingStorageIDsList \u003d req.getExistingStorageIDsList();\n      List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n      LocatedBlock result \u003d server.getAdditionalDatanode(req.getSrc(),\n          PBHelper.convert(req.getBlk()),\n          PBHelper.convert(existingList.toArray(\n              new DatanodeInfoProto[existingList.size()])),\n          existingStorageIDsList.toArray(\n              new String[existingStorageIDsList.size()]),\n          PBHelper.convert(excludesList.toArray(\n              new DatanodeInfoProto[excludesList.size()])), \n          req.getNumAdditionalNodes(), req.getClientName());\n      return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n          PBHelper.convert(result))\n          .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e": {
      "type": "Ybodychange",
      "commitMessage": "    HDFS-2669 Enable protobuf rpc for ClientNamenodeProtocol\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1214128 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/12/11 1:27 AM",
      "commitName": "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e",
      "commitAuthor": "Sanjay Radia",
      "commitDateOld": "13/12/11 3:31 PM",
      "commitNameOld": "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.41,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,20 @@\n   public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n       RpcController controller, GetAdditionalDatanodeRequestProto req)\n       throws ServiceException {\n     try {\n+      List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n+      List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n+      LocatedBlock result \u003d server.getAdditionalDatanode(\n+          req.getSrc(), PBHelper.convert(req.getBlk()),\n+          PBHelper.convert(existingList.toArray(\n+              new DatanodeInfoProto[existingList.size()])),\n+          PBHelper.convert(excludesList.toArray(\n+              new DatanodeInfoProto[excludesList.size()])), \n+              req.getNumAdditionalNodes(), req.getClientName());\n       return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n-          PBHelper.convert(\n-              server.getAdditionalDatanode(req.getSrc(),\n-                  PBHelper.convert(req.getBlk()), \n-                  PBHelper.convert((DatanodeInfoProto[]) req.getExistingsList()\n-                      .toArray()), PBHelper\n-                  .convert((DatanodeInfoProto[]) req.getExcludesList()\n-                      .toArray()), req.getNumAdditionalNodes(), req\n-                  .getClientName())))\n+          PBHelper.convert(result))\n           .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n      RpcController controller, GetAdditionalDatanodeRequestProto req)\n      throws ServiceException {\n    try {\n      List\u003cDatanodeInfoProto\u003e existingList \u003d req.getExistingsList();\n      List\u003cDatanodeInfoProto\u003e excludesList \u003d req.getExcludesList();\n      LocatedBlock result \u003d server.getAdditionalDatanode(\n          req.getSrc(), PBHelper.convert(req.getBlk()),\n          PBHelper.convert(existingList.toArray(\n              new DatanodeInfoProto[existingList.size()])),\n          PBHelper.convert(excludesList.toArray(\n              new DatanodeInfoProto[excludesList.size()])), \n              req.getNumAdditionalNodes(), req.getClientName());\n      return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n          PBHelper.convert(result))\n          .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "48da033901d3471ef176a94104158546152353e9": {
      "type": "Yintroduced",
      "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/11 9:36 PM",
      "commitName": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthor": "Sanjay Radia",
      "diff": "@@ -0,0 +1,18 @@\n+  public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n+      RpcController controller, GetAdditionalDatanodeRequestProto req)\n+      throws ServiceException {\n+    try {\n+      return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n+          PBHelper.convert(\n+              server.getAdditionalDatanode(req.getSrc(),\n+                  PBHelper.convert(req.getBlk()), \n+                  PBHelper.convert((DatanodeInfoProto[]) req.getExistingsList()\n+                      .toArray()), PBHelper\n+                  .convert((DatanodeInfoProto[]) req.getExcludesList()\n+                      .toArray()), req.getNumAdditionalNodes(), req\n+                  .getClientName())))\n+          .build();\n+    } catch (IOException e) {\n+      throw new ServiceException(e);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public GetAdditionalDatanodeResponseProto getAdditionalDatanode(\n      RpcController controller, GetAdditionalDatanodeRequestProto req)\n      throws ServiceException {\n    try {\n      return GetAdditionalDatanodeResponseProto.newBuilder().setBlock(\n          PBHelper.convert(\n              server.getAdditionalDatanode(req.getSrc(),\n                  PBHelper.convert(req.getBlk()), \n                  PBHelper.convert((DatanodeInfoProto[]) req.getExistingsList()\n                      .toArray()), PBHelper\n                  .convert((DatanodeInfoProto[]) req.getExcludesList()\n                      .toArray()), req.getNumAdditionalNodes(), req\n                  .getClientName())))\n          .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java"
    }
  }
}