{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LeafQueue.java",
  "functionName": "assignContainers",
  "functionId": "assignContainers___clusterResource-Resource__candidates-CandidateNodeSet__FiCaSchedulerNode____currentResourceLimits-ResourceLimits__schedulingMode-SchedulingMode",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
  "functionStartLine": 1040,
  "functionEndLine": 1203,
  "numCommitsSeen": 763,
  "timeTaken": 25429,
  "changeHistory": [
    "cdb2107066a2d8557270888c0a9a75f29a6853bf",
    "c2731d4b6399f88f76341ed697e80652ed1b61ea",
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8",
    "09763925025a3709e6098186348e1afd80cb9f71",
    "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7",
    "9c3fc3ef2865164aa5f121793ac914cfeb21a181",
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
    "945c0958bb8df3dd9d5f1467f1216d2e6b0ee3d8",
    "de3b4aac561258ad242a3c5ed1c919428893fd4c",
    "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
    "e0d131f055ee126052ad4d0f7b0d192e6c730188",
    "4d92aefd35d4517d9435d81bafdec0d77905a7a0",
    "fc055a3cbe9545cf1c59421641c7b296aa33f953",
    "ae14e5d07f1b6702a5160637438028bb03d9387e",
    "fa7a43529d529f0006c8033c2003f15b9b93f103",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
    "6cb0af3c39a5d49cb2f7911ee21363a9542ca2d7",
    "89cab1ba5f0671f8ef30dbe7432079c18362b434",
    "e5003be907acef87c2770e3f2914953f62017b0e",
    "ba2313d6145a1234777938a747187373f4cd58d9",
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
    "1ea36299a47af302379ae0750b571ec021eb54ad",
    "d497f6ea2be559aa31ed76f37ae949dbfabe2a51",
    "189a63a719c63b67a1783a280bfc2f72dcb55277",
    "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5",
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
    "586348e4cbf197188057d6b843a6701cfffdaff3",
    "487374b7fe0c92fc7eb1406c568952722b5d5b15",
    "14dd647c556016d351f425ee956ccf800ccb9ce2",
    "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb",
    "f2ea555ac6c06a3f2f6559731f48711fff05d3f1",
    "9c22065109a77681bc2534063eabe8692fbcb3cd",
    "4ce0e4bf2e91278bbc33f4a1c44c7929627b5d6e",
    "025f1719472282a30aa26ae3e235e404f04ba932",
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c",
    "eff5d9b17e0853e82968a695b498b4be37148a05",
    "8eb3be63f598daae01f0a0c09eab5086881f153d",
    "2051fd5ee29e99df6fe79c70b0c7c8c0c1cc131f",
    "4a14264ddb28d4cfd06ec4f70b42e3174a1d888c",
    "520033b1cd81c76b38fcdcfcfeed16158db4bbba",
    "106e2e27ffb81f816ae627fa1712f5db5fb36002",
    "453926397182078c65a4428eb5de5a90d6af6448",
    "fb5b96dfc324f999e8b3698288c110a1c3b71c30",
    "e1fdf62123625e4ba399af02f8aad500637d29d1",
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76",
    "ef1a619a4df3a612eb293a6e8e1e952eaef18eba",
    "21c9116309d8482e7e28522cd7386e65415b15e9",
    "4a343c9d4ab4c993b545f0c1062c6b5449b065f0",
    "b8f0836f9420e71652404c41471653bb15f62a48",
    "b8102dbdf8b4dc2e99bc7c58f4085a7313830a2d",
    "f24dcb3449c77da665058427bc7fa480cad507fc",
    "6b608aad7d52b524fa94955a538e8b3524d42d93",
    "b9a5fd51904a074a7a33f38266378f0f6f97b531",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517"
  ],
  "changeHistoryShort": {
    "cdb2107066a2d8557270888c0a9a75f29a6853bf": "Ybodychange",
    "c2731d4b6399f88f76341ed697e80652ed1b61ea": "Ybodychange",
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8": "Ybodychange",
    "09763925025a3709e6098186348e1afd80cb9f71": "Ybodychange",
    "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7": "Ybodychange",
    "9c3fc3ef2865164aa5f121793ac914cfeb21a181": "Ybodychange",
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949": "Ymultichange(Yparameterchange,Ybodychange)",
    "945c0958bb8df3dd9d5f1467f1216d2e6b0ee3d8": "Ybodychange",
    "de3b4aac561258ad242a3c5ed1c919428893fd4c": "Ymultichange(Yparameterchange,Ybodychange)",
    "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa": "Ymultichange(Ymodifierchange,Ybodychange)",
    "e0d131f055ee126052ad4d0f7b0d192e6c730188": "Ybodychange",
    "4d92aefd35d4517d9435d81bafdec0d77905a7a0": "Ybodychange",
    "fc055a3cbe9545cf1c59421641c7b296aa33f953": "Ybodychange",
    "ae14e5d07f1b6702a5160637438028bb03d9387e": "Ybodychange",
    "fa7a43529d529f0006c8033c2003f15b9b93f103": "Ybodychange",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": "Ybodychange",
    "6cb0af3c39a5d49cb2f7911ee21363a9542ca2d7": "Ybodychange",
    "89cab1ba5f0671f8ef30dbe7432079c18362b434": "Ybodychange",
    "e5003be907acef87c2770e3f2914953f62017b0e": "Ybodychange",
    "ba2313d6145a1234777938a747187373f4cd58d9": "Ybodychange",
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b": "Ybodychange",
    "1ea36299a47af302379ae0750b571ec021eb54ad": "Ybodychange",
    "d497f6ea2be559aa31ed76f37ae949dbfabe2a51": "Ybodychange",
    "189a63a719c63b67a1783a280bfc2f72dcb55277": "Ybodychange",
    "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5": "Ybodychange",
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5": "Ymultichange(Yparameterchange,Ybodychange)",
    "586348e4cbf197188057d6b843a6701cfffdaff3": "Ybodychange",
    "487374b7fe0c92fc7eb1406c568952722b5d5b15": "Ymultichange(Yparameterchange,Ybodychange)",
    "14dd647c556016d351f425ee956ccf800ccb9ce2": "Ymultichange(Yparameterchange,Ybodychange)",
    "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb": "Ybodychange",
    "f2ea555ac6c06a3f2f6559731f48711fff05d3f1": "Ybodychange",
    "9c22065109a77681bc2534063eabe8692fbcb3cd": "Ymultichange(Yparameterchange,Ybodychange)",
    "4ce0e4bf2e91278bbc33f4a1c44c7929627b5d6e": "Ybodychange",
    "025f1719472282a30aa26ae3e235e404f04ba932": "Ybodychange",
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c": "Ybodychange",
    "eff5d9b17e0853e82968a695b498b4be37148a05": "Ybodychange",
    "8eb3be63f598daae01f0a0c09eab5086881f153d": "Ybodychange",
    "2051fd5ee29e99df6fe79c70b0c7c8c0c1cc131f": "Ybodychange",
    "4a14264ddb28d4cfd06ec4f70b42e3174a1d888c": "Ybodychange",
    "520033b1cd81c76b38fcdcfcfeed16158db4bbba": "Ybodychange",
    "106e2e27ffb81f816ae627fa1712f5db5fb36002": "Ybodychange",
    "453926397182078c65a4428eb5de5a90d6af6448": "Ybodychange",
    "fb5b96dfc324f999e8b3698288c110a1c3b71c30": "Ybodychange",
    "e1fdf62123625e4ba399af02f8aad500637d29d1": "Yfilerename",
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76": "Ymultichange(Yparameterchange,Ybodychange)",
    "ef1a619a4df3a612eb293a6e8e1e952eaef18eba": "Ybodychange",
    "21c9116309d8482e7e28522cd7386e65415b15e9": "Ybodychange",
    "4a343c9d4ab4c993b545f0c1062c6b5449b065f0": "Ymultichange(Yreturntypechange,Ybodychange)",
    "b8f0836f9420e71652404c41471653bb15f62a48": "Ybodychange",
    "b8102dbdf8b4dc2e99bc7c58f4085a7313830a2d": "Ybodychange",
    "f24dcb3449c77da665058427bc7fa480cad507fc": "Ybodychange",
    "6b608aad7d52b524fa94955a538e8b3524d42d93": "Ybodychange",
    "b9a5fd51904a074a7a33f38266378f0f6f97b531": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yintroduced"
  },
  "changeHistoryDetails": {
    "cdb2107066a2d8557270888c0a9a75f29a6853bf": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9879. Allow multiple leaf queues with the same name in CapacityScheduler. Contributed by Gergely Pollak.\n",
      "commitDate": "25/03/20 4:20 AM",
      "commitName": "cdb2107066a2d8557270888c0a9a75f29a6853bf",
      "commitAuthor": "Sunil G",
      "commitDateOld": "28/01/20 7:54 PM",
      "commitNameOld": "e578e52aae01248507e089b406fe038ab8e84207",
      "commitAuthorOld": "Eric Badger",
      "daysBetweenCommits": 56.31,
      "commitsBetweenForRepo": 177,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,164 +1,164 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n       CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n \n     // Check for reserved resources, try to allocate reserved container first.\n     CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n         candidates, currentResourceLimits, schedulingMode);\n     if (null !\u003d assignment) {\n       return assignment;\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n+          getParent().getQueuePath(), getQueuePath(), ActivityState.REJECTED,\n           ActivityDiagnosticConstant.QUEUE_NOT_ABLE_TO_ACCESS_PARTITION);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + candidates\n             .getPartition());\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+          getParent().getQueuePath(), getQueuePath(), ActivityState.SKIPPED,\n           ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n     boolean needAssignToQueueCheck \u003d true;\n     IteratorSelector sel \u003d new IteratorSelector();\n     sel.setPartition(candidates.getPartition());\n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n          orderingPolicy.getAssignmentIterator(sel);\n          assignmentIterator.hasNext(); ) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n           node, SystemClock.getInstance().getTime(), application);\n \n       // Check queue max-capacity limit\n       Resource appReserved \u003d application.getCurrentReservation();\n       if (needAssignToQueueCheck) {\n         if (!super.canAssignToThisQueue(clusterResource,\n             candidates.getPartition(), currentResourceLimits, appReserved,\n             schedulingMode)) {\n           ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n               activitiesManager, node, application, application.getPriority(),\n               ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n           ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              getParent().getQueueName(), getQueueName(),\n+              getParent().getQueuePath(), getQueuePath(),\n               ActivityState.REJECTED,\n               ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n           return CSAssignment.NULL_ASSIGNMENT;\n         }\n         // If there was no reservation and canAssignToThisQueue returned\n         // true, there is no reason to check further.\n         if (!this.reservationsContinueLooking\n             || appReserved.equals(Resources.none())) {\n           needAssignToQueueCheck \u003d false;\n         }\n       }\n \n       CachedUserLimit cul \u003d userLimits.get(application.getUser());\n       Resource cachedUserLimit \u003d null;\n       if (cul !\u003d null) {\n         cachedUserLimit \u003d cul.userLimit;\n       }\n       Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n           clusterResource, candidates.getPartition(), schedulingMode,\n           cachedUserLimit);\n       if (cul \u003d\u003d null) {\n         cul \u003d new CachedUserLimit(userLimit);\n         userLimits.put(application.getUser(), cul);\n       }\n       // Check user limit\n       boolean userAssignable \u003d true;\n       if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n         userAssignable \u003d false;\n       } else {\n         userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n             userLimit, application, candidates.getPartition(),\n             currentResourceLimits);\n         if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n           cul.canAssign \u003d false;\n           cul.reservation \u003d appReserved;\n         }\n       }\n       if (!userAssignable) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n             activitiesManager, node, application, application.getPriority(),\n             ActivityDiagnosticConstant.QUEUE_HIT_USER_MAX_CAPACITY_LIMIT);\n         continue;\n       }\n \n       // Try to schedule\n       assignment \u003d application.assignContainers(clusterResource,\n           candidates, currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \" + application\n             .getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(),\n+            getParent().getQueuePath(), getQueuePath(),\n             ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n         return assignment;\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.OTHER) {\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n+            getParent().getQueuePath(), getQueuePath(), ActivityState.REJECTED,\n             () -\u003e ActivityDiagnosticConstant.QUEUE_DO_NOT_HAVE_ENOUGH_HEADROOM\n                 + \" from \" + application.getApplicationId());\n         return assignment;\n       } else{\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+            getParent().getQueuePath(), getQueuePath(), ActivityState.SKIPPED,\n             ActivityDiagnosticConstant.QUEUE_SKIPPED_TO_RESPECT_FIFO);\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n     ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+        getParent().getQueuePath(), getQueuePath(), ActivityState.SKIPPED,\n         ActivityDiagnosticConstant.EMPTY);\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n\n    // Check for reserved resources, try to allocate reserved container first.\n    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n        candidates, currentResourceLimits, schedulingMode);\n    if (null !\u003d assignment) {\n      return assignment;\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueuePath(), getQueuePath(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.QUEUE_NOT_ABLE_TO_ACCESS_PARTITION);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + candidates\n            .getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueuePath(), getQueuePath(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n    boolean needAssignToQueueCheck \u003d true;\n    IteratorSelector sel \u003d new IteratorSelector();\n    sel.setPartition(candidates.getPartition());\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(sel);\n         assignmentIterator.hasNext(); ) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node, SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      Resource appReserved \u003d application.getCurrentReservation();\n      if (needAssignToQueueCheck) {\n        if (!super.canAssignToThisQueue(clusterResource,\n            candidates.getPartition(), currentResourceLimits, appReserved,\n            schedulingMode)) {\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueuePath(), getQueuePath(),\n              ActivityState.REJECTED,\n              ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n        // If there was no reservation and canAssignToThisQueue returned\n        // true, there is no reason to check further.\n        if (!this.reservationsContinueLooking\n            || appReserved.equals(Resources.none())) {\n          needAssignToQueueCheck \u003d false;\n        }\n      }\n\n      CachedUserLimit cul \u003d userLimits.get(application.getUser());\n      Resource cachedUserLimit \u003d null;\n      if (cul !\u003d null) {\n        cachedUserLimit \u003d cul.userLimit;\n      }\n      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n          clusterResource, candidates.getPartition(), schedulingMode,\n          cachedUserLimit);\n      if (cul \u003d\u003d null) {\n        cul \u003d new CachedUserLimit(userLimit);\n        userLimits.put(application.getUser(), cul);\n      }\n      // Check user limit\n      boolean userAssignable \u003d true;\n      if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n        userAssignable \u003d false;\n      } else {\n        userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n            userLimit, application, candidates.getPartition(),\n            currentResourceLimits);\n        if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n          cul.canAssign \u003d false;\n          cul.reservation \u003d appReserved;\n        }\n      }\n      if (!userAssignable) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.QUEUE_HIT_USER_MAX_CAPACITY_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      assignment \u003d application.assignContainers(clusterResource,\n          candidates, currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \" + application\n            .getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueuePath(), getQueuePath(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueuePath(), getQueuePath(), ActivityState.REJECTED,\n            () -\u003e ActivityDiagnosticConstant.QUEUE_DO_NOT_HAVE_ENOUGH_HEADROOM\n                + \" from \" + application.getApplicationId());\n        return assignment;\n      } else{\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueuePath(), getQueuePath(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.QUEUE_SKIPPED_TO_RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueuePath(), getQueuePath(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "c2731d4b6399f88f76341ed697e80652ed1b61ea": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9730. Support forcing configured partitions to be exclusive based on app node label\n",
      "commitDate": "24/09/19 1:51 PM",
      "commitName": "c2731d4b6399f88f76341ed697e80652ed1b61ea",
      "commitAuthor": "Jonathan Hung",
      "commitDateOld": "02/09/19 11:23 PM",
      "commitNameOld": "03489124ea1b8d5648ade5e3563e39b5bc323384",
      "commitAuthorOld": "bibinchundatt",
      "daysBetweenCommits": 21.6,
      "commitsBetweenForRepo": 180,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,162 +1,164 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n       CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n \n     // Check for reserved resources, try to allocate reserved container first.\n     CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n         candidates, currentResourceLimits, schedulingMode);\n     if (null !\u003d assignment) {\n       return assignment;\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n           ActivityDiagnosticConstant.QUEUE_NOT_ABLE_TO_ACCESS_PARTITION);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + candidates\n             .getPartition());\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n           ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n     boolean needAssignToQueueCheck \u003d true;\n+    IteratorSelector sel \u003d new IteratorSelector();\n+    sel.setPartition(candidates.getPartition());\n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n-         orderingPolicy.getAssignmentIterator();\n+         orderingPolicy.getAssignmentIterator(sel);\n          assignmentIterator.hasNext(); ) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n           node, SystemClock.getInstance().getTime(), application);\n \n       // Check queue max-capacity limit\n       Resource appReserved \u003d application.getCurrentReservation();\n       if (needAssignToQueueCheck) {\n         if (!super.canAssignToThisQueue(clusterResource,\n             candidates.getPartition(), currentResourceLimits, appReserved,\n             schedulingMode)) {\n           ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n               activitiesManager, node, application, application.getPriority(),\n               ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n           ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n               getParent().getQueueName(), getQueueName(),\n               ActivityState.REJECTED,\n               ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n           return CSAssignment.NULL_ASSIGNMENT;\n         }\n         // If there was no reservation and canAssignToThisQueue returned\n         // true, there is no reason to check further.\n         if (!this.reservationsContinueLooking\n             || appReserved.equals(Resources.none())) {\n           needAssignToQueueCheck \u003d false;\n         }\n       }\n \n       CachedUserLimit cul \u003d userLimits.get(application.getUser());\n       Resource cachedUserLimit \u003d null;\n       if (cul !\u003d null) {\n         cachedUserLimit \u003d cul.userLimit;\n       }\n       Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n           clusterResource, candidates.getPartition(), schedulingMode,\n           cachedUserLimit);\n       if (cul \u003d\u003d null) {\n         cul \u003d new CachedUserLimit(userLimit);\n         userLimits.put(application.getUser(), cul);\n       }\n       // Check user limit\n       boolean userAssignable \u003d true;\n       if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n         userAssignable \u003d false;\n       } else {\n         userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n             userLimit, application, candidates.getPartition(),\n             currentResourceLimits);\n         if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n           cul.canAssign \u003d false;\n           cul.reservation \u003d appReserved;\n         }\n       }\n       if (!userAssignable) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n             activitiesManager, node, application, application.getPriority(),\n             ActivityDiagnosticConstant.QUEUE_HIT_USER_MAX_CAPACITY_LIMIT);\n         continue;\n       }\n \n       // Try to schedule\n       assignment \u003d application.assignContainers(clusterResource,\n           candidates, currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \" + application\n             .getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(),\n             ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n         return assignment;\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.OTHER) {\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n             () -\u003e ActivityDiagnosticConstant.QUEUE_DO_NOT_HAVE_ENOUGH_HEADROOM\n                 + \" from \" + application.getApplicationId());\n         return assignment;\n       } else{\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n             ActivityDiagnosticConstant.QUEUE_SKIPPED_TO_RESPECT_FIFO);\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n     ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n         getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n         ActivityDiagnosticConstant.EMPTY);\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n\n    // Check for reserved resources, try to allocate reserved container first.\n    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n        candidates, currentResourceLimits, schedulingMode);\n    if (null !\u003d assignment) {\n      return assignment;\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.QUEUE_NOT_ABLE_TO_ACCESS_PARTITION);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + candidates\n            .getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n    boolean needAssignToQueueCheck \u003d true;\n    IteratorSelector sel \u003d new IteratorSelector();\n    sel.setPartition(candidates.getPartition());\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(sel);\n         assignmentIterator.hasNext(); ) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node, SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      Resource appReserved \u003d application.getCurrentReservation();\n      if (needAssignToQueueCheck) {\n        if (!super.canAssignToThisQueue(clusterResource,\n            candidates.getPartition(), currentResourceLimits, appReserved,\n            schedulingMode)) {\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(),\n              ActivityState.REJECTED,\n              ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n        // If there was no reservation and canAssignToThisQueue returned\n        // true, there is no reason to check further.\n        if (!this.reservationsContinueLooking\n            || appReserved.equals(Resources.none())) {\n          needAssignToQueueCheck \u003d false;\n        }\n      }\n\n      CachedUserLimit cul \u003d userLimits.get(application.getUser());\n      Resource cachedUserLimit \u003d null;\n      if (cul !\u003d null) {\n        cachedUserLimit \u003d cul.userLimit;\n      }\n      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n          clusterResource, candidates.getPartition(), schedulingMode,\n          cachedUserLimit);\n      if (cul \u003d\u003d null) {\n        cul \u003d new CachedUserLimit(userLimit);\n        userLimits.put(application.getUser(), cul);\n      }\n      // Check user limit\n      boolean userAssignable \u003d true;\n      if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n        userAssignable \u003d false;\n      } else {\n        userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n            userLimit, application, candidates.getPartition(),\n            currentResourceLimits);\n        if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n          cul.canAssign \u003d false;\n          cul.reservation \u003d appReserved;\n        }\n      }\n      if (!userAssignable) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.QUEUE_HIT_USER_MAX_CAPACITY_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      assignment \u003d application.assignContainers(clusterResource,\n          candidates, currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \" + application\n            .getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n            () -\u003e ActivityDiagnosticConstant.QUEUE_DO_NOT_HAVE_ENOUGH_HEADROOM\n                + \" from \" + application.getApplicationId());\n        return assignment;\n      } else{\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.QUEUE_SKIPPED_TO_RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9664. Improve response of scheduler/app activities for better understanding. Contributed by Tao Yang.\n",
      "commitDate": "29/08/19 3:14 AM",
      "commitName": "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "28/08/19 1:40 PM",
      "commitNameOld": "6f2226a013daf30b00c6676a5f12160ac0b197b7",
      "commitAuthorOld": "Eric E Payne",
      "daysBetweenCommits": 0.57,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,161 +1,162 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n       CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n \n     // Check for reserved resources, try to allocate reserved container first.\n     CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n         candidates, currentResourceLimits, schedulingMode);\n     if (null !\u003d assignment) {\n       return assignment;\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n-          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + \" \"\n-              + candidates.getPartition());\n+          ActivityDiagnosticConstant.QUEUE_NOT_ABLE_TO_ACCESS_PARTITION);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + candidates\n             .getPartition());\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n           ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n     boolean needAssignToQueueCheck \u003d true;\n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n          orderingPolicy.getAssignmentIterator();\n          assignmentIterator.hasNext(); ) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n           node, SystemClock.getInstance().getTime(), application);\n \n       // Check queue max-capacity limit\n       Resource appReserved \u003d application.getCurrentReservation();\n       if (needAssignToQueueCheck) {\n         if (!super.canAssignToThisQueue(clusterResource,\n             candidates.getPartition(), currentResourceLimits, appReserved,\n             schedulingMode)) {\n           ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n               activitiesManager, node, application, application.getPriority(),\n-              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n+              ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n           ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-              ActivityDiagnosticConstant.EMPTY);\n+              getParent().getQueueName(), getQueueName(),\n+              ActivityState.REJECTED,\n+              ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n           return CSAssignment.NULL_ASSIGNMENT;\n         }\n         // If there was no reservation and canAssignToThisQueue returned\n         // true, there is no reason to check further.\n         if (!this.reservationsContinueLooking\n             || appReserved.equals(Resources.none())) {\n           needAssignToQueueCheck \u003d false;\n         }\n       }\n \n       CachedUserLimit cul \u003d userLimits.get(application.getUser());\n       Resource cachedUserLimit \u003d null;\n       if (cul !\u003d null) {\n         cachedUserLimit \u003d cul.userLimit;\n       }\n       Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n           clusterResource, candidates.getPartition(), schedulingMode,\n           cachedUserLimit);\n       if (cul \u003d\u003d null) {\n         cul \u003d new CachedUserLimit(userLimit);\n         userLimits.put(application.getUser(), cul);\n       }\n       // Check user limit\n       boolean userAssignable \u003d true;\n       if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n         userAssignable \u003d false;\n       } else {\n         userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n             userLimit, application, candidates.getPartition(),\n             currentResourceLimits);\n         if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n           cul.canAssign \u003d false;\n           cul.reservation \u003d appReserved;\n         }\n       }\n       if (!userAssignable) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n             activitiesManager, node, application, application.getPriority(),\n-            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n+            ActivityDiagnosticConstant.QUEUE_HIT_USER_MAX_CAPACITY_LIMIT);\n         continue;\n       }\n \n       // Try to schedule\n       assignment \u003d application.assignContainers(clusterResource,\n           candidates, currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \" + application\n             .getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(),\n             ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n         return assignment;\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.OTHER) {\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-            ActivityDiagnosticConstant.QUEUE_SKIPPED_HEADROOM);\n+            getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n+            () -\u003e ActivityDiagnosticConstant.QUEUE_DO_NOT_HAVE_ENOUGH_HEADROOM\n+                + \" from \" + application.getApplicationId());\n         return assignment;\n       } else{\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-            ActivityDiagnosticConstant.RESPECT_FIFO);\n+            ActivityDiagnosticConstant.QUEUE_SKIPPED_TO_RESPECT_FIFO);\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n     ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n         getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n         ActivityDiagnosticConstant.EMPTY);\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n\n    // Check for reserved resources, try to allocate reserved container first.\n    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n        candidates, currentResourceLimits, schedulingMode);\n    if (null !\u003d assignment) {\n      return assignment;\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.QUEUE_NOT_ABLE_TO_ACCESS_PARTITION);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + candidates\n            .getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n    boolean needAssignToQueueCheck \u003d true;\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext(); ) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node, SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      Resource appReserved \u003d application.getCurrentReservation();\n      if (needAssignToQueueCheck) {\n        if (!super.canAssignToThisQueue(clusterResource,\n            candidates.getPartition(), currentResourceLimits, appReserved,\n            schedulingMode)) {\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(),\n              ActivityState.REJECTED,\n              ActivityDiagnosticConstant.QUEUE_HIT_MAX_CAPACITY_LIMIT);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n        // If there was no reservation and canAssignToThisQueue returned\n        // true, there is no reason to check further.\n        if (!this.reservationsContinueLooking\n            || appReserved.equals(Resources.none())) {\n          needAssignToQueueCheck \u003d false;\n        }\n      }\n\n      CachedUserLimit cul \u003d userLimits.get(application.getUser());\n      Resource cachedUserLimit \u003d null;\n      if (cul !\u003d null) {\n        cachedUserLimit \u003d cul.userLimit;\n      }\n      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n          clusterResource, candidates.getPartition(), schedulingMode,\n          cachedUserLimit);\n      if (cul \u003d\u003d null) {\n        cul \u003d new CachedUserLimit(userLimit);\n        userLimits.put(application.getUser(), cul);\n      }\n      // Check user limit\n      boolean userAssignable \u003d true;\n      if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n        userAssignable \u003d false;\n      } else {\n        userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n            userLimit, application, candidates.getPartition(),\n            currentResourceLimits);\n        if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n          cul.canAssign \u003d false;\n          cul.reservation \u003d appReserved;\n        }\n      }\n      if (!userAssignable) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.QUEUE_HIT_USER_MAX_CAPACITY_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      assignment \u003d application.assignContainers(clusterResource,\n          candidates, currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \" + application\n            .getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n            () -\u003e ActivityDiagnosticConstant.QUEUE_DO_NOT_HAVE_ENOUGH_HEADROOM\n                + \" from \" + application.getApplicationId());\n        return assignment;\n      } else{\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.QUEUE_SKIPPED_TO_RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "09763925025a3709e6098186348e1afd80cb9f71": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9590. Correct incompatible, incomplete and redundant activities. Contributed by Tao Yang.\n",
      "commitDate": "06/06/19 6:59 AM",
      "commitName": "09763925025a3709e6098186348e1afd80cb9f71",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "06/05/19 5:00 AM",
      "commitNameOld": "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 31.08,
      "commitsBetweenForRepo": 197,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,158 +1,161 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n       CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n \n     // Check for reserved resources, try to allocate reserved container first.\n     CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n         candidates, currentResourceLimits, schedulingMode);\n     if (null !\u003d assignment) {\n       return assignment;\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n           ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + \" \"\n               + candidates.getPartition());\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + candidates\n             .getPartition());\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n           ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n     boolean needAssignToQueueCheck \u003d true;\n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n          orderingPolicy.getAssignmentIterator();\n          assignmentIterator.hasNext(); ) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n           node, SystemClock.getInstance().getTime(), application);\n \n       // Check queue max-capacity limit\n       Resource appReserved \u003d application.getCurrentReservation();\n       if (needAssignToQueueCheck) {\n         if (!super.canAssignToThisQueue(clusterResource,\n             candidates.getPartition(), currentResourceLimits, appReserved,\n             schedulingMode)) {\n           ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n               activitiesManager, node, application, application.getPriority(),\n               ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n           ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n               getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n               ActivityDiagnosticConstant.EMPTY);\n           return CSAssignment.NULL_ASSIGNMENT;\n         }\n         // If there was no reservation and canAssignToThisQueue returned\n         // true, there is no reason to check further.\n         if (!this.reservationsContinueLooking\n             || appReserved.equals(Resources.none())) {\n           needAssignToQueueCheck \u003d false;\n         }\n       }\n \n       CachedUserLimit cul \u003d userLimits.get(application.getUser());\n       Resource cachedUserLimit \u003d null;\n       if (cul !\u003d null) {\n         cachedUserLimit \u003d cul.userLimit;\n       }\n       Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n           clusterResource, candidates.getPartition(), schedulingMode,\n           cachedUserLimit);\n       if (cul \u003d\u003d null) {\n         cul \u003d new CachedUserLimit(userLimit);\n         userLimits.put(application.getUser(), cul);\n       }\n       // Check user limit\n       boolean userAssignable \u003d true;\n       if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n         userAssignable \u003d false;\n       } else {\n         userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n             userLimit, application, candidates.getPartition(),\n             currentResourceLimits);\n         if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n           cul.canAssign \u003d false;\n           cul.reservation \u003d appReserved;\n         }\n       }\n       if (!userAssignable) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n             activitiesManager, node, application, application.getPriority(),\n             ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n         continue;\n       }\n \n       // Try to schedule\n       assignment \u003d application.assignContainers(clusterResource,\n           candidates, currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \" + application\n             .getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(),\n             ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n         return assignment;\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.OTHER) {\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+            ActivityDiagnosticConstant.QUEUE_SKIPPED_HEADROOM);\n         return assignment;\n       } else{\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n             ActivityDiagnosticConstant.RESPECT_FIFO);\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n     ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n         getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n         ActivityDiagnosticConstant.EMPTY);\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n\n    // Check for reserved resources, try to allocate reserved container first.\n    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n        candidates, currentResourceLimits, schedulingMode);\n    if (null !\u003d assignment) {\n      return assignment;\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + \" \"\n              + candidates.getPartition());\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + candidates\n            .getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n    boolean needAssignToQueueCheck \u003d true;\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext(); ) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node, SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      Resource appReserved \u003d application.getCurrentReservation();\n      if (needAssignToQueueCheck) {\n        if (!super.canAssignToThisQueue(clusterResource,\n            candidates.getPartition(), currentResourceLimits, appReserved,\n            schedulingMode)) {\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n              ActivityDiagnosticConstant.EMPTY);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n        // If there was no reservation and canAssignToThisQueue returned\n        // true, there is no reason to check further.\n        if (!this.reservationsContinueLooking\n            || appReserved.equals(Resources.none())) {\n          needAssignToQueueCheck \u003d false;\n        }\n      }\n\n      CachedUserLimit cul \u003d userLimits.get(application.getUser());\n      Resource cachedUserLimit \u003d null;\n      if (cul !\u003d null) {\n        cachedUserLimit \u003d cul.userLimit;\n      }\n      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n          clusterResource, candidates.getPartition(), schedulingMode,\n          cachedUserLimit);\n      if (cul \u003d\u003d null) {\n        cul \u003d new CachedUserLimit(userLimit);\n        userLimits.put(application.getUser(), cul);\n      }\n      // Check user limit\n      boolean userAssignable \u003d true;\n      if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n        userAssignable \u003d false;\n      } else {\n        userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n            userLimit, application, candidates.getPartition(),\n            currentResourceLimits);\n        if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n          cul.canAssign \u003d false;\n          cul.reservation \u003d appReserved;\n        }\n      }\n      if (!userAssignable) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      assignment \u003d application.assignContainers(clusterResource,\n          candidates, currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \" + application\n            .getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.QUEUE_SKIPPED_HEADROOM);\n        return assignment;\n      } else{\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9440. Improve diagnostics for scheduler and app activities. Contributed by Tao Yang.\n",
      "commitDate": "06/05/19 5:00 AM",
      "commitName": "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "23/04/19 12:40 PM",
      "commitNameOld": "c504eee0c29276a385ff68ce456f08150aa25e80",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 12.68,
      "commitsBetweenForRepo": 54,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,158 +1,158 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n       CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n \n     // Check for reserved resources, try to allocate reserved container first.\n     CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n         candidates, currentResourceLimits, schedulingMode);\n     if (null !\u003d assignment) {\n       return assignment;\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n-          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + candidates\n-              .getPartition());\n+          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + \" \"\n+              + candidates.getPartition());\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + candidates\n             .getPartition());\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n           ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n     boolean needAssignToQueueCheck \u003d true;\n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n          orderingPolicy.getAssignmentIterator();\n          assignmentIterator.hasNext(); ) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n           node, SystemClock.getInstance().getTime(), application);\n \n       // Check queue max-capacity limit\n       Resource appReserved \u003d application.getCurrentReservation();\n       if (needAssignToQueueCheck) {\n         if (!super.canAssignToThisQueue(clusterResource,\n             candidates.getPartition(), currentResourceLimits, appReserved,\n             schedulingMode)) {\n           ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n               activitiesManager, node, application, application.getPriority(),\n               ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n           ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n               getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n               ActivityDiagnosticConstant.EMPTY);\n           return CSAssignment.NULL_ASSIGNMENT;\n         }\n         // If there was no reservation and canAssignToThisQueue returned\n         // true, there is no reason to check further.\n         if (!this.reservationsContinueLooking\n             || appReserved.equals(Resources.none())) {\n           needAssignToQueueCheck \u003d false;\n         }\n       }\n \n       CachedUserLimit cul \u003d userLimits.get(application.getUser());\n       Resource cachedUserLimit \u003d null;\n       if (cul !\u003d null) {\n         cachedUserLimit \u003d cul.userLimit;\n       }\n       Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n           clusterResource, candidates.getPartition(), schedulingMode,\n           cachedUserLimit);\n       if (cul \u003d\u003d null) {\n         cul \u003d new CachedUserLimit(userLimit);\n         userLimits.put(application.getUser(), cul);\n       }\n       // Check user limit\n       boolean userAssignable \u003d true;\n       if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n         userAssignable \u003d false;\n       } else {\n         userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n             userLimit, application, candidates.getPartition(),\n             currentResourceLimits);\n         if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n           cul.canAssign \u003d false;\n           cul.reservation \u003d appReserved;\n         }\n       }\n       if (!userAssignable) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n             activitiesManager, node, application, application.getPriority(),\n             ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n         continue;\n       }\n \n       // Try to schedule\n       assignment \u003d application.assignContainers(clusterResource,\n           candidates, currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \" + application\n             .getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(),\n             ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n         return assignment;\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.OTHER) {\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n         return assignment;\n       } else{\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n             ActivityDiagnosticConstant.RESPECT_FIFO);\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n     ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n         getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n         ActivityDiagnosticConstant.EMPTY);\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n\n    // Check for reserved resources, try to allocate reserved container first.\n    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n        candidates, currentResourceLimits, schedulingMode);\n    if (null !\u003d assignment) {\n      return assignment;\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + \" \"\n              + candidates.getPartition());\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + candidates\n            .getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n    boolean needAssignToQueueCheck \u003d true;\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext(); ) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node, SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      Resource appReserved \u003d application.getCurrentReservation();\n      if (needAssignToQueueCheck) {\n        if (!super.canAssignToThisQueue(clusterResource,\n            candidates.getPartition(), currentResourceLimits, appReserved,\n            schedulingMode)) {\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n              ActivityDiagnosticConstant.EMPTY);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n        // If there was no reservation and canAssignToThisQueue returned\n        // true, there is no reason to check further.\n        if (!this.reservationsContinueLooking\n            || appReserved.equals(Resources.none())) {\n          needAssignToQueueCheck \u003d false;\n        }\n      }\n\n      CachedUserLimit cul \u003d userLimits.get(application.getUser());\n      Resource cachedUserLimit \u003d null;\n      if (cul !\u003d null) {\n        cachedUserLimit \u003d cul.userLimit;\n      }\n      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n          clusterResource, candidates.getPartition(), schedulingMode,\n          cachedUserLimit);\n      if (cul \u003d\u003d null) {\n        cul \u003d new CachedUserLimit(userLimit);\n        userLimits.put(application.getUser(), cul);\n      }\n      // Check user limit\n      boolean userAssignable \u003d true;\n      if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n        userAssignable \u003d false;\n      } else {\n        userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n            userLimit, application, candidates.getPartition(),\n            currentResourceLimits);\n        if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n          cul.canAssign \u003d false;\n          cul.reservation \u003d appReserved;\n        }\n      }\n      if (!userAssignable) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      assignment \u003d application.assignContainers(clusterResource,\n          candidates, currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \" + application\n            .getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        return assignment;\n      } else{\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "9c3fc3ef2865164aa5f121793ac914cfeb21a181": {
      "type": "Ybodychange",
      "commitMessage": "YARN-7494. Add muti-node lookup mechanism and pluggable nodes sorting policies to optimize placement decision. Contributed by Sunil Govindan.\n",
      "commitDate": "21/08/18 7:42 AM",
      "commitName": "9c3fc3ef2865164aa5f121793ac914cfeb21a181",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "12/06/18 8:35 AM",
      "commitNameOld": "652bcbb3e4950758e61ce123ecc1798ae2b60a7f",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 69.96,
      "commitsBetweenForRepo": 500,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,156 +1,158 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n       CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n \n     // Check for reserved resources, try to allocate reserved container first.\n     CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n         candidates, currentResourceLimits, schedulingMode);\n     if (null !\u003d assignment) {\n       return assignment;\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n           ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + candidates\n               .getPartition());\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + candidates\n             .getPartition());\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n           ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n     boolean needAssignToQueueCheck \u003d true;\n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n          orderingPolicy.getAssignmentIterator();\n          assignmentIterator.hasNext(); ) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n-          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n+          node, SystemClock.getInstance().getTime(), application);\n \n       // Check queue max-capacity limit\n       Resource appReserved \u003d application.getCurrentReservation();\n       if (needAssignToQueueCheck) {\n-        if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n-            currentResourceLimits, appReserved, schedulingMode)) {\n+        if (!super.canAssignToThisQueue(clusterResource,\n+            candidates.getPartition(), currentResourceLimits, appReserved,\n+            schedulingMode)) {\n           ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n               activitiesManager, node, application, application.getPriority(),\n               ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n           ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n               getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n               ActivityDiagnosticConstant.EMPTY);\n           return CSAssignment.NULL_ASSIGNMENT;\n         }\n         // If there was no reservation and canAssignToThisQueue returned\n         // true, there is no reason to check further.\n         if (!this.reservationsContinueLooking\n             || appReserved.equals(Resources.none())) {\n           needAssignToQueueCheck \u003d false;\n         }\n       }\n \n       CachedUserLimit cul \u003d userLimits.get(application.getUser());\n       Resource cachedUserLimit \u003d null;\n       if (cul !\u003d null) {\n         cachedUserLimit \u003d cul.userLimit;\n       }\n       Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n           clusterResource, candidates.getPartition(), schedulingMode,\n           cachedUserLimit);\n       if (cul \u003d\u003d null) {\n         cul \u003d new CachedUserLimit(userLimit);\n         userLimits.put(application.getUser(), cul);\n       }\n       // Check user limit\n       boolean userAssignable \u003d true;\n       if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n         userAssignable \u003d false;\n       } else {\n         userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n-            userLimit, application, node.getPartition(), currentResourceLimits);\n+            userLimit, application, candidates.getPartition(),\n+            currentResourceLimits);\n         if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n           cul.canAssign \u003d false;\n           cul.reservation \u003d appReserved;\n         }\n       }\n       if (!userAssignable) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n             activitiesManager, node, application, application.getPriority(),\n             ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n         continue;\n       }\n \n       // Try to schedule\n       assignment \u003d application.assignContainers(clusterResource,\n           candidates, currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \" + application\n             .getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(),\n             ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n         return assignment;\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.OTHER) {\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n         return assignment;\n       } else{\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n             ActivityDiagnosticConstant.RESPECT_FIFO);\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n     ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n         getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n         ActivityDiagnosticConstant.EMPTY);\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n\n    // Check for reserved resources, try to allocate reserved container first.\n    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n        candidates, currentResourceLimits, schedulingMode);\n    if (null !\u003d assignment) {\n      return assignment;\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + candidates\n              .getPartition());\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + candidates\n            .getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n    boolean needAssignToQueueCheck \u003d true;\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext(); ) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node, SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      Resource appReserved \u003d application.getCurrentReservation();\n      if (needAssignToQueueCheck) {\n        if (!super.canAssignToThisQueue(clusterResource,\n            candidates.getPartition(), currentResourceLimits, appReserved,\n            schedulingMode)) {\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n              ActivityDiagnosticConstant.EMPTY);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n        // If there was no reservation and canAssignToThisQueue returned\n        // true, there is no reason to check further.\n        if (!this.reservationsContinueLooking\n            || appReserved.equals(Resources.none())) {\n          needAssignToQueueCheck \u003d false;\n        }\n      }\n\n      CachedUserLimit cul \u003d userLimits.get(application.getUser());\n      Resource cachedUserLimit \u003d null;\n      if (cul !\u003d null) {\n        cachedUserLimit \u003d cul.userLimit;\n      }\n      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n          clusterResource, candidates.getPartition(), schedulingMode,\n          cachedUserLimit);\n      if (cul \u003d\u003d null) {\n        cul \u003d new CachedUserLimit(userLimit);\n        userLimits.put(application.getUser(), cul);\n      }\n      // Check user limit\n      boolean userAssignable \u003d true;\n      if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n        userAssignable \u003d false;\n      } else {\n        userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n            userLimit, application, candidates.getPartition(),\n            currentResourceLimits);\n        if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n          cul.canAssign \u003d false;\n          cul.reservation \u003d appReserved;\n        }\n      }\n      if (!userAssignable) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      assignment \u003d application.assignContainers(clusterResource,\n          candidates, currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \" + application\n            .getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        return assignment;\n      } else{\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-7437. Rename PlacementSet and SchedulingPlacementSet. (Wangda Tan via kkaranasos)\n",
      "commitDate": "09/11/17 1:01 PM",
      "commitName": "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
      "commitAuthor": "Konstantinos Karanasos",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-7437. Rename PlacementSet and SchedulingPlacementSet. (Wangda Tan via kkaranasos)\n",
          "commitDate": "09/11/17 1:01 PM",
          "commitName": "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
          "commitAuthor": "Konstantinos Karanasos",
          "commitDateOld": "06/11/17 9:38 PM",
          "commitNameOld": "13fa2d4e3e55a849dcd7e472750f3e0422cc2ac9",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 2.64,
          "commitsBetweenForRepo": 20,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,154 +1,156 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n-      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n-    SchedulingMode schedulingMode) {\n+      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n+      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n-    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n+    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: partition\u003d\" + ps.getPartition()\n+      LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n-    setPreemptionAllowed(currentResourceLimits, ps.getPartition());\n+    setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n \n     // Check for reserved resources, try to allocate reserved container first.\n     CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n-        ps, currentResourceLimits, schedulingMode);\n+        candidates, currentResourceLimits, schedulingMode);\n     if (null !\u003d assignment) {\n       return assignment;\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n-        \u0026\u0026 !accessibleToPartition(ps.getPartition())) {\n+        \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n-          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + ps\n+          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + candidates\n               .getPartition());\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n-    if (!hasPendingResourceRequest(ps.getPartition(), clusterResource,\n+    if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n-            + schedulingMode.name() + \" node-partition\u003d\" + ps.getPartition());\n+            + schedulingMode.name() + \" node-partition\u003d\" + candidates\n+            .getPartition());\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n           ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n     boolean needAssignToQueueCheck \u003d true;\n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n          orderingPolicy.getAssignmentIterator();\n          assignmentIterator.hasNext(); ) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n           node.getNodeID(), SystemClock.getInstance().getTime(), application);\n \n       // Check queue max-capacity limit\n       Resource appReserved \u003d application.getCurrentReservation();\n       if (needAssignToQueueCheck) {\n         if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n             currentResourceLimits, appReserved, schedulingMode)) {\n           ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n               activitiesManager, node, application, application.getPriority(),\n               ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n           ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n               getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n               ActivityDiagnosticConstant.EMPTY);\n           return CSAssignment.NULL_ASSIGNMENT;\n         }\n         // If there was no reservation and canAssignToThisQueue returned\n         // true, there is no reason to check further.\n         if (!this.reservationsContinueLooking\n             || appReserved.equals(Resources.none())) {\n           needAssignToQueueCheck \u003d false;\n         }\n       }\n \n       CachedUserLimit cul \u003d userLimits.get(application.getUser());\n       Resource cachedUserLimit \u003d null;\n       if (cul !\u003d null) {\n         cachedUserLimit \u003d cul.userLimit;\n       }\n       Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n-          clusterResource, ps.getPartition(), schedulingMode, cachedUserLimit);\n+          clusterResource, candidates.getPartition(), schedulingMode,\n+          cachedUserLimit);\n       if (cul \u003d\u003d null) {\n         cul \u003d new CachedUserLimit(userLimit);\n         userLimits.put(application.getUser(), cul);\n       }\n       // Check user limit\n       boolean userAssignable \u003d true;\n       if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n         userAssignable \u003d false;\n       } else {\n         userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n             userLimit, application, node.getPartition(), currentResourceLimits);\n         if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n           cul.canAssign \u003d false;\n           cul.reservation \u003d appReserved;\n         }\n       }\n       if (!userAssignable) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n             activitiesManager, node, application, application.getPriority(),\n             ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n         continue;\n       }\n \n       // Try to schedule\n       assignment \u003d application.assignContainers(clusterResource,\n-          ps, currentResourceLimits, schedulingMode, null);\n+          candidates, currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \" + application\n             .getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(),\n             ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n         return assignment;\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.OTHER) {\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n         return assignment;\n       } else{\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n             ActivityDiagnosticConstant.RESPECT_FIFO);\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n     ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n         getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n         ActivityDiagnosticConstant.EMPTY);\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n\n    // Check for reserved resources, try to allocate reserved container first.\n    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n        candidates, currentResourceLimits, schedulingMode);\n    if (null !\u003d assignment) {\n      return assignment;\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + candidates\n              .getPartition());\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + candidates\n            .getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n    boolean needAssignToQueueCheck \u003d true;\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext(); ) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      Resource appReserved \u003d application.getCurrentReservation();\n      if (needAssignToQueueCheck) {\n        if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n            currentResourceLimits, appReserved, schedulingMode)) {\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n              ActivityDiagnosticConstant.EMPTY);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n        // If there was no reservation and canAssignToThisQueue returned\n        // true, there is no reason to check further.\n        if (!this.reservationsContinueLooking\n            || appReserved.equals(Resources.none())) {\n          needAssignToQueueCheck \u003d false;\n        }\n      }\n\n      CachedUserLimit cul \u003d userLimits.get(application.getUser());\n      Resource cachedUserLimit \u003d null;\n      if (cul !\u003d null) {\n        cachedUserLimit \u003d cul.userLimit;\n      }\n      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n          clusterResource, candidates.getPartition(), schedulingMode,\n          cachedUserLimit);\n      if (cul \u003d\u003d null) {\n        cul \u003d new CachedUserLimit(userLimit);\n        userLimits.put(application.getUser(), cul);\n      }\n      // Check user limit\n      boolean userAssignable \u003d true;\n      if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n        userAssignable \u003d false;\n      } else {\n        userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n            userLimit, application, node.getPartition(), currentResourceLimits);\n        if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n          cul.canAssign \u003d false;\n          cul.reservation \u003d appReserved;\n        }\n      }\n      if (!userAssignable) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      assignment \u003d application.assignContainers(clusterResource,\n          candidates, currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \" + application\n            .getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        return assignment;\n      } else{\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, ps-PlacementSet\u003cFiCaSchedulerNode\u003e, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode]",
            "newValue": "[clusterResource-Resource, candidates-CandidateNodeSet\u003cFiCaSchedulerNode\u003e, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-7437. Rename PlacementSet and SchedulingPlacementSet. (Wangda Tan via kkaranasos)\n",
          "commitDate": "09/11/17 1:01 PM",
          "commitName": "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
          "commitAuthor": "Konstantinos Karanasos",
          "commitDateOld": "06/11/17 9:38 PM",
          "commitNameOld": "13fa2d4e3e55a849dcd7e472750f3e0422cc2ac9",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 2.64,
          "commitsBetweenForRepo": 20,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,154 +1,156 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n-      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n-    SchedulingMode schedulingMode) {\n+      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n+      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n-    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n+    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: partition\u003d\" + ps.getPartition()\n+      LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n-    setPreemptionAllowed(currentResourceLimits, ps.getPartition());\n+    setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n \n     // Check for reserved resources, try to allocate reserved container first.\n     CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n-        ps, currentResourceLimits, schedulingMode);\n+        candidates, currentResourceLimits, schedulingMode);\n     if (null !\u003d assignment) {\n       return assignment;\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n-        \u0026\u0026 !accessibleToPartition(ps.getPartition())) {\n+        \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n-          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + ps\n+          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + candidates\n               .getPartition());\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n-    if (!hasPendingResourceRequest(ps.getPartition(), clusterResource,\n+    if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n-            + schedulingMode.name() + \" node-partition\u003d\" + ps.getPartition());\n+            + schedulingMode.name() + \" node-partition\u003d\" + candidates\n+            .getPartition());\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n           ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n     boolean needAssignToQueueCheck \u003d true;\n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n          orderingPolicy.getAssignmentIterator();\n          assignmentIterator.hasNext(); ) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n           node.getNodeID(), SystemClock.getInstance().getTime(), application);\n \n       // Check queue max-capacity limit\n       Resource appReserved \u003d application.getCurrentReservation();\n       if (needAssignToQueueCheck) {\n         if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n             currentResourceLimits, appReserved, schedulingMode)) {\n           ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n               activitiesManager, node, application, application.getPriority(),\n               ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n           ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n               getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n               ActivityDiagnosticConstant.EMPTY);\n           return CSAssignment.NULL_ASSIGNMENT;\n         }\n         // If there was no reservation and canAssignToThisQueue returned\n         // true, there is no reason to check further.\n         if (!this.reservationsContinueLooking\n             || appReserved.equals(Resources.none())) {\n           needAssignToQueueCheck \u003d false;\n         }\n       }\n \n       CachedUserLimit cul \u003d userLimits.get(application.getUser());\n       Resource cachedUserLimit \u003d null;\n       if (cul !\u003d null) {\n         cachedUserLimit \u003d cul.userLimit;\n       }\n       Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n-          clusterResource, ps.getPartition(), schedulingMode, cachedUserLimit);\n+          clusterResource, candidates.getPartition(), schedulingMode,\n+          cachedUserLimit);\n       if (cul \u003d\u003d null) {\n         cul \u003d new CachedUserLimit(userLimit);\n         userLimits.put(application.getUser(), cul);\n       }\n       // Check user limit\n       boolean userAssignable \u003d true;\n       if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n         userAssignable \u003d false;\n       } else {\n         userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n             userLimit, application, node.getPartition(), currentResourceLimits);\n         if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n           cul.canAssign \u003d false;\n           cul.reservation \u003d appReserved;\n         }\n       }\n       if (!userAssignable) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n             activitiesManager, node, application, application.getPriority(),\n             ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n         continue;\n       }\n \n       // Try to schedule\n       assignment \u003d application.assignContainers(clusterResource,\n-          ps, currentResourceLimits, schedulingMode, null);\n+          candidates, currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \" + application\n             .getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(),\n             ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n         return assignment;\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.OTHER) {\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n         return assignment;\n       } else{\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n             ActivityDiagnosticConstant.RESPECT_FIFO);\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n     ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n         getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n         ActivityDiagnosticConstant.EMPTY);\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: partition\u003d\" + candidates.getPartition()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, candidates.getPartition());\n\n    // Check for reserved resources, try to allocate reserved container first.\n    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n        candidates, currentResourceLimits, schedulingMode);\n    if (null !\u003d assignment) {\n      return assignment;\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(candidates.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + candidates\n              .getPartition());\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(candidates.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + candidates\n            .getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n    boolean needAssignToQueueCheck \u003d true;\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext(); ) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      Resource appReserved \u003d application.getCurrentReservation();\n      if (needAssignToQueueCheck) {\n        if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n            currentResourceLimits, appReserved, schedulingMode)) {\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n              ActivityDiagnosticConstant.EMPTY);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n        // If there was no reservation and canAssignToThisQueue returned\n        // true, there is no reason to check further.\n        if (!this.reservationsContinueLooking\n            || appReserved.equals(Resources.none())) {\n          needAssignToQueueCheck \u003d false;\n        }\n      }\n\n      CachedUserLimit cul \u003d userLimits.get(application.getUser());\n      Resource cachedUserLimit \u003d null;\n      if (cul !\u003d null) {\n        cachedUserLimit \u003d cul.userLimit;\n      }\n      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n          clusterResource, candidates.getPartition(), schedulingMode,\n          cachedUserLimit);\n      if (cul \u003d\u003d null) {\n        cul \u003d new CachedUserLimit(userLimit);\n        userLimits.put(application.getUser(), cul);\n      }\n      // Check user limit\n      boolean userAssignable \u003d true;\n      if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n        userAssignable \u003d false;\n      } else {\n        userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n            userLimit, application, node.getPartition(), currentResourceLimits);\n        if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n          cul.canAssign \u003d false;\n          cul.reservation \u003d appReserved;\n        }\n      }\n      if (!userAssignable) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      assignment \u003d application.assignContainers(clusterResource,\n          candidates, currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \" + application\n            .getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        return assignment;\n      } else{\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "945c0958bb8df3dd9d5f1467f1216d2e6b0ee3d8": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6775. CapacityScheduler: Improvements to assignContainers, avoid unnecessary canAssignToUser/Queue calls. (Nathan Roberts via wangda)\n\nChange-Id: I84ccd54200ccbaae23018ef320028e42b4c3509a\n",
      "commitDate": "13/07/17 10:30 AM",
      "commitName": "945c0958bb8df3dd9d5f1467f1216d2e6b0ee3d8",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "22/06/17 11:50 PM",
      "commitNameOld": "ca13b224b2feb9c44de861da9cbba8dd2a12cb35",
      "commitAuthorOld": "Sunil G",
      "daysBetweenCommits": 20.44,
      "commitsBetweenForRepo": 89,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,126 +1,154 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n       PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n     SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: partition\u003d\" + ps.getPartition()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     setPreemptionAllowed(currentResourceLimits, ps.getPartition());\n \n     // Check for reserved resources, try to allocate reserved container first.\n     CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n         ps, currentResourceLimits, schedulingMode);\n     if (null !\u003d assignment) {\n       return assignment;\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(ps.getPartition())) {\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n           ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + ps\n               .getPartition());\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(ps.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + ps.getPartition());\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n           ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n+    Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n+    boolean needAssignToQueueCheck \u003d true;\n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n          orderingPolicy.getAssignmentIterator();\n          assignmentIterator.hasNext(); ) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n           node.getNodeID(), SystemClock.getInstance().getTime(), application);\n \n       // Check queue max-capacity limit\n-      if (!super.canAssignToThisQueue(clusterResource, ps.getPartition(),\n-          currentResourceLimits, application.getCurrentReservation(),\n-          schedulingMode)) {\n-        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n-            activitiesManager, node, application, application.getPriority(),\n-            ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-            ActivityDiagnosticConstant.EMPTY);\n-        return CSAssignment.NULL_ASSIGNMENT;\n+      Resource appReserved \u003d application.getCurrentReservation();\n+      if (needAssignToQueueCheck) {\n+        if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n+            currentResourceLimits, appReserved, schedulingMode)) {\n+          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n+              activitiesManager, node, application, application.getPriority(),\n+              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n+          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+              ActivityDiagnosticConstant.EMPTY);\n+          return CSAssignment.NULL_ASSIGNMENT;\n+        }\n+        // If there was no reservation and canAssignToThisQueue returned\n+        // true, there is no reason to check further.\n+        if (!this.reservationsContinueLooking\n+            || appReserved.equals(Resources.none())) {\n+          needAssignToQueueCheck \u003d false;\n+        }\n       }\n \n+      CachedUserLimit cul \u003d userLimits.get(application.getUser());\n+      Resource cachedUserLimit \u003d null;\n+      if (cul !\u003d null) {\n+        cachedUserLimit \u003d cul.userLimit;\n+      }\n       Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n-          clusterResource, ps.getPartition(), schedulingMode);\n-\n+          clusterResource, ps.getPartition(), schedulingMode, cachedUserLimit);\n+      if (cul \u003d\u003d null) {\n+        cul \u003d new CachedUserLimit(userLimit);\n+        userLimits.put(application.getUser(), cul);\n+      }\n       // Check user limit\n-      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n-          application, ps.getPartition(), currentResourceLimits)) {\n+      boolean userAssignable \u003d true;\n+      if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n+        userAssignable \u003d false;\n+      } else {\n+        userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n+            userLimit, application, node.getPartition(), currentResourceLimits);\n+        if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n+          cul.canAssign \u003d false;\n+          cul.reservation \u003d appReserved;\n+        }\n+      }\n+      if (!userAssignable) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n             activitiesManager, node, application, application.getPriority(),\n             ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n         continue;\n       }\n \n       // Try to schedule\n       assignment \u003d application.assignContainers(clusterResource,\n           ps, currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \" + application\n             .getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(),\n             ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n         return assignment;\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.OTHER) {\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n         return assignment;\n       } else{\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n             getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n             ActivityDiagnosticConstant.RESPECT_FIFO);\n         ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n             activitiesManager, application.getApplicationId(),\n             ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n     ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n         getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n         ActivityDiagnosticConstant.EMPTY);\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n    SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: partition\u003d\" + ps.getPartition()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, ps.getPartition());\n\n    // Check for reserved resources, try to allocate reserved container first.\n    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n        ps, currentResourceLimits, schedulingMode);\n    if (null !\u003d assignment) {\n      return assignment;\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(ps.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + ps\n              .getPartition());\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(ps.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + ps.getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    Map\u003cString, CachedUserLimit\u003e userLimits \u003d new HashMap\u003c\u003e();\n    boolean needAssignToQueueCheck \u003d true;\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext(); ) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      Resource appReserved \u003d application.getCurrentReservation();\n      if (needAssignToQueueCheck) {\n        if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n            currentResourceLimits, appReserved, schedulingMode)) {\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n              ActivityDiagnosticConstant.EMPTY);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n        // If there was no reservation and canAssignToThisQueue returned\n        // true, there is no reason to check further.\n        if (!this.reservationsContinueLooking\n            || appReserved.equals(Resources.none())) {\n          needAssignToQueueCheck \u003d false;\n        }\n      }\n\n      CachedUserLimit cul \u003d userLimits.get(application.getUser());\n      Resource cachedUserLimit \u003d null;\n      if (cul !\u003d null) {\n        cachedUserLimit \u003d cul.userLimit;\n      }\n      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n          clusterResource, ps.getPartition(), schedulingMode, cachedUserLimit);\n      if (cul \u003d\u003d null) {\n        cul \u003d new CachedUserLimit(userLimit);\n        userLimits.put(application.getUser(), cul);\n      }\n      // Check user limit\n      boolean userAssignable \u003d true;\n      if (!cul.canAssign \u0026\u0026 Resources.fitsIn(appReserved, cul.reservation)) {\n        userAssignable \u003d false;\n      } else {\n        userAssignable \u003d canAssignToUser(clusterResource, application.getUser(),\n            userLimit, application, node.getPartition(), currentResourceLimits);\n        if (!userAssignable \u0026\u0026 Resources.fitsIn(cul.reservation, appReserved)) {\n          cul.canAssign \u003d false;\n          cul.reservation \u003d appReserved;\n        }\n      }\n      if (!userAssignable) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      assignment \u003d application.assignContainers(clusterResource,\n          ps, currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \" + application\n            .getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        return assignment;\n      } else{\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "de3b4aac561258ad242a3c5ed1c919428893fd4c": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
      "commitDate": "07/11/16 10:14 AM",
      "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
          "commitDate": "07/11/16 10:14 AM",
          "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
          "commitAuthor": "Jian He",
          "commitDateOld": "04/11/16 3:37 AM",
          "commitNameOld": "19b3779ae7455230ed89971141b2667eae624aab",
          "commitAuthorOld": "Sunil",
          "daysBetweenCommits": 3.32,
          "commitsBetweenForRepo": 47,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,166 +1,126 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n-      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n-      SchedulingMode schedulingMode) {\n-    try {\n-      writeLock.lock();\n-      updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n+      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n+    SchedulingMode schedulingMode) {\n+    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n+    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n \n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\n-            \"assignContainers: node\u003d\" + node.getNodeName() + \" #applications\u003d\"\n-                + orderingPolicy.getNumSchedulableEntities());\n-      }\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"assignContainers: partition\u003d\" + ps.getPartition()\n+          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n+    }\n \n-      setPreemptionAllowed(currentResourceLimits, node.getPartition());\n+    setPreemptionAllowed(currentResourceLimits, ps.getPartition());\n \n-      // Check for reserved resources\n-      RMContainer reservedContainer \u003d node.getReservedContainer();\n-      if (reservedContainer !\u003d null) {\n-        FiCaSchedulerApp application \u003d getApplication(\n-            reservedContainer.getApplicationAttemptId());\n+    // Check for reserved resources, try to allocate reserved container first.\n+    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n+        ps, currentResourceLimits, schedulingMode);\n+    if (null !\u003d assignment) {\n+      return assignment;\n+    }\n \n-        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n-            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n-\n-        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n-            node, currentResourceLimits, schedulingMode, reservedContainer);\n-        handleExcessReservedContainer(clusterResource, assignment, node,\n-            application);\n-        killToPreemptContainers(clusterResource, node, assignment);\n-        return assignment;\n-      }\n-\n-      // if our queue cannot access this node, just return\n-      if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n-          \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n-            ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + node\n-                .getPartition());\n-        return CSAssignment.NULL_ASSIGNMENT;\n-      }\n-\n-      // Check if this queue need more resource, simply skip allocation if this\n-      // queue doesn\u0027t need more resources.\n-      if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n-          schedulingMode)) {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n-              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n-              + schedulingMode.name() + \" node-partition\u003d\" + node\n+    // if our queue cannot access this node, just return\n+    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n+        \u0026\u0026 !accessibleToPartition(ps.getPartition())) {\n+      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n+          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + ps\n               .getPartition());\n-        }\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-            ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n-        return CSAssignment.NULL_ASSIGNMENT;\n-      }\n+      return CSAssignment.NULL_ASSIGNMENT;\n+    }\n \n-      for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n-           orderingPolicy.getAssignmentIterator();\n-           assignmentIterator.hasNext(); ) {\n-        FiCaSchedulerApp application \u003d assignmentIterator.next();\n-\n-        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n-            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n-\n-        // Check queue max-capacity limit\n-        if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n-            currentResourceLimits, application.getCurrentReservation(),\n-            schedulingMode)) {\n-          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n-              activitiesManager, node, application, application.getPriority(),\n-              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-              ActivityDiagnosticConstant.EMPTY);\n-          return CSAssignment.NULL_ASSIGNMENT;\n-        }\n-\n-        Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n-            clusterResource, node.getPartition(), schedulingMode);\n-\n-        // Check user limit\n-        if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n-            application, node.getPartition(), currentResourceLimits)) {\n-          application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n-              \"User capacity has reached its maximum limit.\");\n-          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n-              activitiesManager, node, application, application.getPriority(),\n-              ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n-          continue;\n-        }\n-\n-        // Try to schedule\n-        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n-            node, currentResourceLimits, schedulingMode, null);\n-\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"post-assignContainers for application \" + application\n-              .getApplicationId());\n-          application.showRequests();\n-        }\n-\n-        // Did we schedule or reserve a container?\n-        Resource assigned \u003d assignment.getResource();\n-\n-        handleExcessReservedContainer(clusterResource, assignment, node,\n-            application);\n-        killToPreemptContainers(clusterResource, node, assignment);\n-\n-        if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n-            Resources.none())) {\n-          // Get reserved or allocated container from application\n-          RMContainer reservedOrAllocatedRMContainer \u003d\n-              application.getRMContainer(assignment.getAssignmentInformation()\n-                  .getFirstAllocatedOrReservedContainerId());\n-\n-          // Book-keeping\n-          // Note: Update headroom to account for current allocation too...\n-          allocateResource(clusterResource, application, assigned,\n-              node.getPartition(), reservedOrAllocatedRMContainer,\n-              assignment.isIncreasedAllocation());\n-\n-          // Update reserved metrics\n-          Resource reservedRes \u003d\n-              assignment.getAssignmentInformation().getReserved();\n-          if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n-            incReservedResource(node.getPartition(), reservedRes);\n-          }\n-\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              getParent().getQueueName(), getQueueName(),\n-              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-\n-          // Done\n-          return assignment;\n-        } else if (assignment.getSkippedType()\n-            \u003d\u003d CSAssignment.SkippedType.OTHER) {\n-          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n-              activitiesManager, application.getApplicationId(),\n-              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n-          application.updateNodeInfoForAMDiagnostics(node);\n-        } else if (assignment.getSkippedType()\n-            \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n-          return assignment;\n-        } else{\n-          // If we don\u0027t allocate anything, and it is not skipped by application,\n-          // we will return to respect FIFO of applications\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-              ActivityDiagnosticConstant.RESPECT_FIFO);\n-          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n-              activitiesManager, application.getApplicationId(),\n-              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n-          return CSAssignment.NULL_ASSIGNMENT;\n-        }\n+    // Check if this queue need more resource, simply skip allocation if this\n+    // queue doesn\u0027t need more resources.\n+    if (!hasPendingResourceRequest(ps.getPartition(), clusterResource,\n+        schedulingMode)) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n+            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+            + schedulingMode.name() + \" node-partition\u003d\" + ps.getPartition());\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-          ActivityDiagnosticConstant.EMPTY);\n-\n+          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n-    } finally {\n-      writeLock.unlock();\n     }\n+\n+    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n+         orderingPolicy.getAssignmentIterator();\n+         assignmentIterator.hasNext(); ) {\n+      FiCaSchedulerApp application \u003d assignmentIterator.next();\n+\n+      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n+          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n+\n+      // Check queue max-capacity limit\n+      if (!super.canAssignToThisQueue(clusterResource, ps.getPartition(),\n+          currentResourceLimits, application.getCurrentReservation(),\n+          schedulingMode)) {\n+        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n+            activitiesManager, node, application, application.getPriority(),\n+            ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+            ActivityDiagnosticConstant.EMPTY);\n+        return CSAssignment.NULL_ASSIGNMENT;\n+      }\n+\n+      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n+          clusterResource, ps.getPartition(), schedulingMode);\n+\n+      // Check user limit\n+      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n+          application, ps.getPartition(), currentResourceLimits)) {\n+        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n+            \"User capacity has reached its maximum limit.\");\n+        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n+            activitiesManager, node, application, application.getPriority(),\n+            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n+        continue;\n+      }\n+\n+      // Try to schedule\n+      assignment \u003d application.assignContainers(clusterResource,\n+          ps, currentResourceLimits, schedulingMode, null);\n+\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"post-assignContainers for application \" + application\n+            .getApplicationId());\n+        application.showRequests();\n+      }\n+\n+      // Did we schedule or reserve a container?\n+      Resource assigned \u003d assignment.getResource();\n+\n+      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n+          Resources.none())) {\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(),\n+            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n+        return assignment;\n+      } else if (assignment.getSkippedType()\n+          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n+        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n+            activitiesManager, application.getApplicationId(),\n+            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n+        application.updateNodeInfoForAMDiagnostics(node);\n+      } else if (assignment.getSkippedType()\n+          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n+        return assignment;\n+      } else{\n+        // If we don\u0027t allocate anything, and it is not skipped by application,\n+        // we will return to respect FIFO of applications\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+            ActivityDiagnosticConstant.RESPECT_FIFO);\n+        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n+            activitiesManager, application.getApplicationId(),\n+            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n+        return CSAssignment.NULL_ASSIGNMENT;\n+      }\n+    }\n+    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+        ActivityDiagnosticConstant.EMPTY);\n+\n+    return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n    SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: partition\u003d\" + ps.getPartition()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, ps.getPartition());\n\n    // Check for reserved resources, try to allocate reserved container first.\n    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n        ps, currentResourceLimits, schedulingMode);\n    if (null !\u003d assignment) {\n      return assignment;\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(ps.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + ps\n              .getPartition());\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(ps.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + ps.getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext(); ) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, ps.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n\n      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n          clusterResource, ps.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, ps.getPartition(), currentResourceLimits)) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      assignment \u003d application.assignContainers(clusterResource,\n          ps, currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \" + application\n            .getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        return assignment;\n      } else{\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode]",
            "newValue": "[clusterResource-Resource, ps-PlacementSet\u003cFiCaSchedulerNode\u003e, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
          "commitDate": "07/11/16 10:14 AM",
          "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
          "commitAuthor": "Jian He",
          "commitDateOld": "04/11/16 3:37 AM",
          "commitNameOld": "19b3779ae7455230ed89971141b2667eae624aab",
          "commitAuthorOld": "Sunil",
          "daysBetweenCommits": 3.32,
          "commitsBetweenForRepo": 47,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,166 +1,126 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n-      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n-      SchedulingMode schedulingMode) {\n-    try {\n-      writeLock.lock();\n-      updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n+      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n+    SchedulingMode schedulingMode) {\n+    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n+    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n \n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\n-            \"assignContainers: node\u003d\" + node.getNodeName() + \" #applications\u003d\"\n-                + orderingPolicy.getNumSchedulableEntities());\n-      }\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"assignContainers: partition\u003d\" + ps.getPartition()\n+          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n+    }\n \n-      setPreemptionAllowed(currentResourceLimits, node.getPartition());\n+    setPreemptionAllowed(currentResourceLimits, ps.getPartition());\n \n-      // Check for reserved resources\n-      RMContainer reservedContainer \u003d node.getReservedContainer();\n-      if (reservedContainer !\u003d null) {\n-        FiCaSchedulerApp application \u003d getApplication(\n-            reservedContainer.getApplicationAttemptId());\n+    // Check for reserved resources, try to allocate reserved container first.\n+    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n+        ps, currentResourceLimits, schedulingMode);\n+    if (null !\u003d assignment) {\n+      return assignment;\n+    }\n \n-        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n-            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n-\n-        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n-            node, currentResourceLimits, schedulingMode, reservedContainer);\n-        handleExcessReservedContainer(clusterResource, assignment, node,\n-            application);\n-        killToPreemptContainers(clusterResource, node, assignment);\n-        return assignment;\n-      }\n-\n-      // if our queue cannot access this node, just return\n-      if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n-          \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n-            ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + node\n-                .getPartition());\n-        return CSAssignment.NULL_ASSIGNMENT;\n-      }\n-\n-      // Check if this queue need more resource, simply skip allocation if this\n-      // queue doesn\u0027t need more resources.\n-      if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n-          schedulingMode)) {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n-              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n-              + schedulingMode.name() + \" node-partition\u003d\" + node\n+    // if our queue cannot access this node, just return\n+    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n+        \u0026\u0026 !accessibleToPartition(ps.getPartition())) {\n+      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n+          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + ps\n               .getPartition());\n-        }\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-            ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n-        return CSAssignment.NULL_ASSIGNMENT;\n-      }\n+      return CSAssignment.NULL_ASSIGNMENT;\n+    }\n \n-      for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n-           orderingPolicy.getAssignmentIterator();\n-           assignmentIterator.hasNext(); ) {\n-        FiCaSchedulerApp application \u003d assignmentIterator.next();\n-\n-        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n-            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n-\n-        // Check queue max-capacity limit\n-        if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n-            currentResourceLimits, application.getCurrentReservation(),\n-            schedulingMode)) {\n-          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n-              activitiesManager, node, application, application.getPriority(),\n-              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-              ActivityDiagnosticConstant.EMPTY);\n-          return CSAssignment.NULL_ASSIGNMENT;\n-        }\n-\n-        Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n-            clusterResource, node.getPartition(), schedulingMode);\n-\n-        // Check user limit\n-        if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n-            application, node.getPartition(), currentResourceLimits)) {\n-          application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n-              \"User capacity has reached its maximum limit.\");\n-          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n-              activitiesManager, node, application, application.getPriority(),\n-              ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n-          continue;\n-        }\n-\n-        // Try to schedule\n-        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n-            node, currentResourceLimits, schedulingMode, null);\n-\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"post-assignContainers for application \" + application\n-              .getApplicationId());\n-          application.showRequests();\n-        }\n-\n-        // Did we schedule or reserve a container?\n-        Resource assigned \u003d assignment.getResource();\n-\n-        handleExcessReservedContainer(clusterResource, assignment, node,\n-            application);\n-        killToPreemptContainers(clusterResource, node, assignment);\n-\n-        if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n-            Resources.none())) {\n-          // Get reserved or allocated container from application\n-          RMContainer reservedOrAllocatedRMContainer \u003d\n-              application.getRMContainer(assignment.getAssignmentInformation()\n-                  .getFirstAllocatedOrReservedContainerId());\n-\n-          // Book-keeping\n-          // Note: Update headroom to account for current allocation too...\n-          allocateResource(clusterResource, application, assigned,\n-              node.getPartition(), reservedOrAllocatedRMContainer,\n-              assignment.isIncreasedAllocation());\n-\n-          // Update reserved metrics\n-          Resource reservedRes \u003d\n-              assignment.getAssignmentInformation().getReserved();\n-          if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n-            incReservedResource(node.getPartition(), reservedRes);\n-          }\n-\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              getParent().getQueueName(), getQueueName(),\n-              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-\n-          // Done\n-          return assignment;\n-        } else if (assignment.getSkippedType()\n-            \u003d\u003d CSAssignment.SkippedType.OTHER) {\n-          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n-              activitiesManager, application.getApplicationId(),\n-              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n-          application.updateNodeInfoForAMDiagnostics(node);\n-        } else if (assignment.getSkippedType()\n-            \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n-          return assignment;\n-        } else{\n-          // If we don\u0027t allocate anything, and it is not skipped by application,\n-          // we will return to respect FIFO of applications\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-              ActivityDiagnosticConstant.RESPECT_FIFO);\n-          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n-              activitiesManager, application.getApplicationId(),\n-              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n-          return CSAssignment.NULL_ASSIGNMENT;\n-        }\n+    // Check if this queue need more resource, simply skip allocation if this\n+    // queue doesn\u0027t need more resources.\n+    if (!hasPendingResourceRequest(ps.getPartition(), clusterResource,\n+        schedulingMode)) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n+            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+            + schedulingMode.name() + \" node-partition\u003d\" + ps.getPartition());\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-          ActivityDiagnosticConstant.EMPTY);\n-\n+          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n-    } finally {\n-      writeLock.unlock();\n     }\n+\n+    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n+         orderingPolicy.getAssignmentIterator();\n+         assignmentIterator.hasNext(); ) {\n+      FiCaSchedulerApp application \u003d assignmentIterator.next();\n+\n+      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n+          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n+\n+      // Check queue max-capacity limit\n+      if (!super.canAssignToThisQueue(clusterResource, ps.getPartition(),\n+          currentResourceLimits, application.getCurrentReservation(),\n+          schedulingMode)) {\n+        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n+            activitiesManager, node, application, application.getPriority(),\n+            ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+            ActivityDiagnosticConstant.EMPTY);\n+        return CSAssignment.NULL_ASSIGNMENT;\n+      }\n+\n+      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n+          clusterResource, ps.getPartition(), schedulingMode);\n+\n+      // Check user limit\n+      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n+          application, ps.getPartition(), currentResourceLimits)) {\n+        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n+            \"User capacity has reached its maximum limit.\");\n+        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n+            activitiesManager, node, application, application.getPriority(),\n+            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n+        continue;\n+      }\n+\n+      // Try to schedule\n+      assignment \u003d application.assignContainers(clusterResource,\n+          ps, currentResourceLimits, schedulingMode, null);\n+\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"post-assignContainers for application \" + application\n+            .getApplicationId());\n+        application.showRequests();\n+      }\n+\n+      // Did we schedule or reserve a container?\n+      Resource assigned \u003d assignment.getResource();\n+\n+      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n+          Resources.none())) {\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(),\n+            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n+        return assignment;\n+      } else if (assignment.getSkippedType()\n+          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n+        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n+            activitiesManager, application.getApplicationId(),\n+            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n+        application.updateNodeInfoForAMDiagnostics(node);\n+      } else if (assignment.getSkippedType()\n+          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n+        return assignment;\n+      } else{\n+        // If we don\u0027t allocate anything, and it is not skipped by application,\n+        // we will return to respect FIFO of applications\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+            ActivityDiagnosticConstant.RESPECT_FIFO);\n+        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n+            activitiesManager, application.getApplicationId(),\n+            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n+        return CSAssignment.NULL_ASSIGNMENT;\n+      }\n+    }\n+    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+        ActivityDiagnosticConstant.EMPTY);\n+\n+    return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n    SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: partition\u003d\" + ps.getPartition()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, ps.getPartition());\n\n    // Check for reserved resources, try to allocate reserved container first.\n    CSAssignment assignment \u003d allocateFromReservedContainer(clusterResource,\n        ps, currentResourceLimits, schedulingMode);\n    if (null !\u003d assignment) {\n      return assignment;\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(ps.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + ps\n              .getPartition());\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(ps.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + ps.getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext(); ) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, ps.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n\n      Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n          clusterResource, ps.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, ps.getPartition(), currentResourceLimits)) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node, application, application.getPriority(),\n            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      assignment \u003d application.assignContainers(clusterResource,\n          ps, currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \" + application\n            .getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        return assignment;\n      } else{\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "YARN-3140. Improve locks in AbstractCSQueue/LeafQueue/ParentQueue. Contributed by Wangda Tan\n",
      "commitDate": "20/09/16 12:03 AM",
      "commitName": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-3140. Improve locks in AbstractCSQueue/LeafQueue/ParentQueue. Contributed by Wangda Tan\n",
          "commitDate": "20/09/16 12:03 AM",
          "commitName": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthor": "Jian He",
          "commitDateOld": "16/09/16 10:05 PM",
          "commitNameOld": "4174b9756c8c7877797545c4356b1f40df603ec5",
          "commitAuthorOld": "Naganarasimha",
          "daysBetweenCommits": 3.08,
          "commitsBetweenForRepo": 11,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,165 +1,166 @@\n-  public synchronized CSAssignment assignContainers(Resource clusterResource,\n+  public CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n-    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n+    try {\n+      writeLock.lock();\n+      updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n-    }\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\n+            \"assignContainers: node\u003d\" + node.getNodeName() + \" #applications\u003d\"\n+                + orderingPolicy.getNumSchedulableEntities());\n+      }\n \n-    setPreemptionAllowed(currentResourceLimits, node.getPartition());\n+      setPreemptionAllowed(currentResourceLimits, node.getPartition());\n \n-    // Check for reserved resources\n-    RMContainer reservedContainer \u003d node.getReservedContainer();\n-    if (reservedContainer !\u003d null) {\n-      FiCaSchedulerApp application \u003d\n-          getApplication(reservedContainer.getApplicationAttemptId());\n+      // Check for reserved resources\n+      RMContainer reservedContainer \u003d node.getReservedContainer();\n+      if (reservedContainer !\u003d null) {\n+        FiCaSchedulerApp application \u003d getApplication(\n+            reservedContainer.getApplicationAttemptId());\n \n-      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n-          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n+        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n+            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n \n-      synchronized (application) {\n-        CSAssignment assignment \u003d\n-            application.assignContainers(clusterResource, node,\n-                currentResourceLimits, schedulingMode, reservedContainer);\n+        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n+            node, currentResourceLimits, schedulingMode, reservedContainer);\n         handleExcessReservedContainer(clusterResource, assignment, node,\n             application);\n         killToPreemptContainers(clusterResource, node, assignment);\n         return assignment;\n       }\n-    }\n \n-    // if our queue cannot access this node, just return\n-    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n-        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n-      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n-          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + node\n+      // if our queue cannot access this node, just return\n+      if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n+          \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n+            ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + node\n+                .getPartition());\n+        return CSAssignment.NULL_ASSIGNMENT;\n+      }\n+\n+      // Check if this queue need more resource, simply skip allocation if this\n+      // queue doesn\u0027t need more resources.\n+      if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n+          schedulingMode)) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n+              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+              + schedulingMode.name() + \" node-partition\u003d\" + node\n               .getPartition());\n-      return CSAssignment.NULL_ASSIGNMENT;\n-    }\n+        }\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+            ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n+        return CSAssignment.NULL_ASSIGNMENT;\n+      }\n \n-    // Check if this queue need more resource, simply skip allocation if this\n-    // queue doesn\u0027t need more resources.\n-    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n-        schedulingMode)) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n-            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n-            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n+      for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n+           orderingPolicy.getAssignmentIterator();\n+           assignmentIterator.hasNext(); ) {\n+        FiCaSchedulerApp application \u003d assignmentIterator.next();\n+\n+        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n+            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n+\n+        // Check queue max-capacity limit\n+        if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n+            currentResourceLimits, application.getCurrentReservation(),\n+            schedulingMode)) {\n+          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n+              activitiesManager, node, application, application.getPriority(),\n+              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n+          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+              ActivityDiagnosticConstant.EMPTY);\n+          return CSAssignment.NULL_ASSIGNMENT;\n+        }\n+\n+        Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n+            clusterResource, node.getPartition(), schedulingMode);\n+\n+        // Check user limit\n+        if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n+            application, node.getPartition(), currentResourceLimits)) {\n+          application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n+              \"User capacity has reached its maximum limit.\");\n+          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n+              activitiesManager, node, application, application.getPriority(),\n+              ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n+          continue;\n+        }\n+\n+        // Try to schedule\n+        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n+            node, currentResourceLimits, schedulingMode, null);\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"post-assignContainers for application \" + application\n+              .getApplicationId());\n+          application.showRequests();\n+        }\n+\n+        // Did we schedule or reserve a container?\n+        Resource assigned \u003d assignment.getResource();\n+\n+        handleExcessReservedContainer(clusterResource, assignment, node,\n+            application);\n+        killToPreemptContainers(clusterResource, node, assignment);\n+\n+        if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n+            Resources.none())) {\n+          // Get reserved or allocated container from application\n+          RMContainer reservedOrAllocatedRMContainer \u003d\n+              application.getRMContainer(assignment.getAssignmentInformation()\n+                  .getFirstAllocatedOrReservedContainerId());\n+\n+          // Book-keeping\n+          // Note: Update headroom to account for current allocation too...\n+          allocateResource(clusterResource, application, assigned,\n+              node.getPartition(), reservedOrAllocatedRMContainer,\n+              assignment.isIncreasedAllocation());\n+\n+          // Update reserved metrics\n+          Resource reservedRes \u003d\n+              assignment.getAssignmentInformation().getReserved();\n+          if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n+            incReservedResource(node.getPartition(), reservedRes);\n+          }\n+\n+          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+              getParent().getQueueName(), getQueueName(),\n+              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n+\n+          // Done\n+          return assignment;\n+        } else if (assignment.getSkippedType()\n+            \u003d\u003d CSAssignment.SkippedType.OTHER) {\n+          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n+              activitiesManager, application.getApplicationId(),\n+              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n+          application.updateNodeInfoForAMDiagnostics(node);\n+        } else if (assignment.getSkippedType()\n+            \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n+          return assignment;\n+        } else{\n+          // If we don\u0027t allocate anything, and it is not skipped by application,\n+          // we will return to respect FIFO of applications\n+          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+              ActivityDiagnosticConstant.RESPECT_FIFO);\n+          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n+              activitiesManager, application.getApplicationId(),\n+              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n+          return CSAssignment.NULL_ASSIGNMENT;\n+        }\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n+          ActivityDiagnosticConstant.EMPTY);\n+\n       return CSAssignment.NULL_ASSIGNMENT;\n+    } finally {\n+      writeLock.unlock();\n     }\n-\n-    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n-        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n-      FiCaSchedulerApp application \u003d assignmentIterator.next();\n-\n-      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n-          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n-\n-      // Check queue max-capacity limit\n-      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n-          currentResourceLimits, application.getCurrentReservation(),\n-          schedulingMode)) {\n-        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n-            activitiesManager, node,\n-            application, application.getPriority(),\n-            ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-            ActivityDiagnosticConstant.EMPTY);\n-        return CSAssignment.NULL_ASSIGNMENT;\n-      }\n-\n-      Resource userLimit \u003d\n-          computeUserLimitAndSetHeadroom(application, clusterResource,\n-              node.getPartition(), schedulingMode);\n-\n-      // Check user limit\n-      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n-          application, node.getPartition(), currentResourceLimits)) {\n-        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n-            \"User capacity has reached its maximum limit.\");\n-        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n-            activitiesManager, node,\n-            application, application.getPriority(),\n-            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n-        continue;\n-      }\n-\n-      // Try to schedule\n-      CSAssignment assignment \u003d\n-          application.assignContainers(clusterResource, node,\n-              currentResourceLimits, schedulingMode, null);\n-\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"post-assignContainers for application \"\n-            + application.getApplicationId());\n-        application.showRequests();\n-      }\n-\n-      // Did we schedule or reserve a container?\n-      Resource assigned \u003d assignment.getResource();\n-      \n-      handleExcessReservedContainer(clusterResource, assignment, node,\n-          application);\n-      killToPreemptContainers(clusterResource, node, assignment);\n-\n-      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n-          Resources.none())) {\n-        // Get reserved or allocated container from application\n-        RMContainer reservedOrAllocatedRMContainer \u003d\n-            application.getRMContainer(assignment.getAssignmentInformation()\n-                .getFirstAllocatedOrReservedContainerId());\n-\n-        // Book-keeping\n-        // Note: Update headroom to account for current allocation too...\n-        allocateResource(clusterResource, application, assigned,\n-            node.getPartition(), reservedOrAllocatedRMContainer,\n-            assignment.isIncreasedAllocation());\n-\n-        // Update reserved metrics\n-        Resource reservedRes \u003d assignment.getAssignmentInformation()\n-            .getReserved();\n-        if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n-          incReservedResource(node.getPartition(), reservedRes);\n-        }\n-\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.ACCEPTED,\n-            ActivityDiagnosticConstant.EMPTY);\n-\n-        // Done\n-        return assignment;\n-      } else if (assignment.getSkippedType()\n-          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n-        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n-            activitiesManager, application.getApplicationId(),\n-            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n-        application.updateNodeInfoForAMDiagnostics(node);\n-      } else if(assignment.getSkippedType()\n-          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n-        return assignment;\n-      } else {\n-        // If we don\u0027t allocate anything, and it is not skipped by application,\n-        // we will return to respect FIFO of applications\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-            ActivityDiagnosticConstant.RESPECT_FIFO);\n-        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n-            activitiesManager, application.getApplicationId(),\n-            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n-        return CSAssignment.NULL_ASSIGNMENT;\n-      }\n-    }\n-    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-        ActivityDiagnosticConstant.EMPTY);\n-\n-    return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    try {\n      writeLock.lock();\n      updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\n            \"assignContainers: node\u003d\" + node.getNodeName() + \" #applications\u003d\"\n                + orderingPolicy.getNumSchedulableEntities());\n      }\n\n      setPreemptionAllowed(currentResourceLimits, node.getPartition());\n\n      // Check for reserved resources\n      RMContainer reservedContainer \u003d node.getReservedContainer();\n      if (reservedContainer !\u003d null) {\n        FiCaSchedulerApp application \u003d getApplication(\n            reservedContainer.getApplicationAttemptId());\n\n        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n\n        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n            node, currentResourceLimits, schedulingMode, reservedContainer);\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        killToPreemptContainers(clusterResource, node, assignment);\n        return assignment;\n      }\n\n      // if our queue cannot access this node, just return\n      if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n          \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n            ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + node\n                .getPartition());\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n\n      // Check if this queue need more resource, simply skip allocation if this\n      // queue doesn\u0027t need more resources.\n      if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n          schedulingMode)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n              + schedulingMode.name() + \" node-partition\u003d\" + node\n              .getPartition());\n        }\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n\n      for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n           orderingPolicy.getAssignmentIterator();\n           assignmentIterator.hasNext(); ) {\n        FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n\n        // Check queue max-capacity limit\n        if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n            currentResourceLimits, application.getCurrentReservation(),\n            schedulingMode)) {\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n              ActivityDiagnosticConstant.EMPTY);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n\n        Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n            clusterResource, node.getPartition(), schedulingMode);\n\n        // Check user limit\n        if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n            application, node.getPartition(), currentResourceLimits)) {\n          application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n              \"User capacity has reached its maximum limit.\");\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n          continue;\n        }\n\n        // Try to schedule\n        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n            node, currentResourceLimits, schedulingMode, null);\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"post-assignContainers for application \" + application\n              .getApplicationId());\n          application.showRequests();\n        }\n\n        // Did we schedule or reserve a container?\n        Resource assigned \u003d assignment.getResource();\n\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        killToPreemptContainers(clusterResource, node, assignment);\n\n        if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n            Resources.none())) {\n          // Get reserved or allocated container from application\n          RMContainer reservedOrAllocatedRMContainer \u003d\n              application.getRMContainer(assignment.getAssignmentInformation()\n                  .getFirstAllocatedOrReservedContainerId());\n\n          // Book-keeping\n          // Note: Update headroom to account for current allocation too...\n          allocateResource(clusterResource, application, assigned,\n              node.getPartition(), reservedOrAllocatedRMContainer,\n              assignment.isIncreasedAllocation());\n\n          // Update reserved metrics\n          Resource reservedRes \u003d\n              assignment.getAssignmentInformation().getReserved();\n          if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n            incReservedResource(node.getPartition(), reservedRes);\n          }\n\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(),\n              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n\n          // Done\n          return assignment;\n        } else if (assignment.getSkippedType()\n            \u003d\u003d CSAssignment.SkippedType.OTHER) {\n          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n              activitiesManager, application.getApplicationId(),\n              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n          application.updateNodeInfoForAMDiagnostics(node);\n        } else if (assignment.getSkippedType()\n            \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n          return assignment;\n        } else{\n          // If we don\u0027t allocate anything, and it is not skipped by application,\n          // we will return to respect FIFO of applications\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n              ActivityDiagnosticConstant.RESPECT_FIFO);\n          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n              activitiesManager, application.getApplicationId(),\n              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.EMPTY);\n\n      return CSAssignment.NULL_ASSIGNMENT;\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3140. Improve locks in AbstractCSQueue/LeafQueue/ParentQueue. Contributed by Wangda Tan\n",
          "commitDate": "20/09/16 12:03 AM",
          "commitName": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthor": "Jian He",
          "commitDateOld": "16/09/16 10:05 PM",
          "commitNameOld": "4174b9756c8c7877797545c4356b1f40df603ec5",
          "commitAuthorOld": "Naganarasimha",
          "daysBetweenCommits": 3.08,
          "commitsBetweenForRepo": 11,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,165 +1,166 @@\n-  public synchronized CSAssignment assignContainers(Resource clusterResource,\n+  public CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n-    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n+    try {\n+      writeLock.lock();\n+      updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n-    }\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\n+            \"assignContainers: node\u003d\" + node.getNodeName() + \" #applications\u003d\"\n+                + orderingPolicy.getNumSchedulableEntities());\n+      }\n \n-    setPreemptionAllowed(currentResourceLimits, node.getPartition());\n+      setPreemptionAllowed(currentResourceLimits, node.getPartition());\n \n-    // Check for reserved resources\n-    RMContainer reservedContainer \u003d node.getReservedContainer();\n-    if (reservedContainer !\u003d null) {\n-      FiCaSchedulerApp application \u003d\n-          getApplication(reservedContainer.getApplicationAttemptId());\n+      // Check for reserved resources\n+      RMContainer reservedContainer \u003d node.getReservedContainer();\n+      if (reservedContainer !\u003d null) {\n+        FiCaSchedulerApp application \u003d getApplication(\n+            reservedContainer.getApplicationAttemptId());\n \n-      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n-          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n+        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n+            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n \n-      synchronized (application) {\n-        CSAssignment assignment \u003d\n-            application.assignContainers(clusterResource, node,\n-                currentResourceLimits, schedulingMode, reservedContainer);\n+        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n+            node, currentResourceLimits, schedulingMode, reservedContainer);\n         handleExcessReservedContainer(clusterResource, assignment, node,\n             application);\n         killToPreemptContainers(clusterResource, node, assignment);\n         return assignment;\n       }\n-    }\n \n-    // if our queue cannot access this node, just return\n-    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n-        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n-      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n-          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + node\n+      // if our queue cannot access this node, just return\n+      if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n+          \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n+            ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + node\n+                .getPartition());\n+        return CSAssignment.NULL_ASSIGNMENT;\n+      }\n+\n+      // Check if this queue need more resource, simply skip allocation if this\n+      // queue doesn\u0027t need more resources.\n+      if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n+          schedulingMode)) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n+              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+              + schedulingMode.name() + \" node-partition\u003d\" + node\n               .getPartition());\n-      return CSAssignment.NULL_ASSIGNMENT;\n-    }\n+        }\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+            ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n+        return CSAssignment.NULL_ASSIGNMENT;\n+      }\n \n-    // Check if this queue need more resource, simply skip allocation if this\n-    // queue doesn\u0027t need more resources.\n-    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n-        schedulingMode)) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n-            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n-            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n+      for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n+           orderingPolicy.getAssignmentIterator();\n+           assignmentIterator.hasNext(); ) {\n+        FiCaSchedulerApp application \u003d assignmentIterator.next();\n+\n+        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n+            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n+\n+        // Check queue max-capacity limit\n+        if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n+            currentResourceLimits, application.getCurrentReservation(),\n+            schedulingMode)) {\n+          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n+              activitiesManager, node, application, application.getPriority(),\n+              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n+          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+              ActivityDiagnosticConstant.EMPTY);\n+          return CSAssignment.NULL_ASSIGNMENT;\n+        }\n+\n+        Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n+            clusterResource, node.getPartition(), schedulingMode);\n+\n+        // Check user limit\n+        if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n+            application, node.getPartition(), currentResourceLimits)) {\n+          application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n+              \"User capacity has reached its maximum limit.\");\n+          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n+              activitiesManager, node, application, application.getPriority(),\n+              ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n+          continue;\n+        }\n+\n+        // Try to schedule\n+        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n+            node, currentResourceLimits, schedulingMode, null);\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"post-assignContainers for application \" + application\n+              .getApplicationId());\n+          application.showRequests();\n+        }\n+\n+        // Did we schedule or reserve a container?\n+        Resource assigned \u003d assignment.getResource();\n+\n+        handleExcessReservedContainer(clusterResource, assignment, node,\n+            application);\n+        killToPreemptContainers(clusterResource, node, assignment);\n+\n+        if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n+            Resources.none())) {\n+          // Get reserved or allocated container from application\n+          RMContainer reservedOrAllocatedRMContainer \u003d\n+              application.getRMContainer(assignment.getAssignmentInformation()\n+                  .getFirstAllocatedOrReservedContainerId());\n+\n+          // Book-keeping\n+          // Note: Update headroom to account for current allocation too...\n+          allocateResource(clusterResource, application, assigned,\n+              node.getPartition(), reservedOrAllocatedRMContainer,\n+              assignment.isIncreasedAllocation());\n+\n+          // Update reserved metrics\n+          Resource reservedRes \u003d\n+              assignment.getAssignmentInformation().getReserved();\n+          if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n+            incReservedResource(node.getPartition(), reservedRes);\n+          }\n+\n+          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+              getParent().getQueueName(), getQueueName(),\n+              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n+\n+          // Done\n+          return assignment;\n+        } else if (assignment.getSkippedType()\n+            \u003d\u003d CSAssignment.SkippedType.OTHER) {\n+          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n+              activitiesManager, application.getApplicationId(),\n+              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n+          application.updateNodeInfoForAMDiagnostics(node);\n+        } else if (assignment.getSkippedType()\n+            \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n+          return assignment;\n+        } else{\n+          // If we don\u0027t allocate anything, and it is not skipped by application,\n+          // we will return to respect FIFO of applications\n+          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+              ActivityDiagnosticConstant.RESPECT_FIFO);\n+          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n+              activitiesManager, application.getApplicationId(),\n+              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n+          return CSAssignment.NULL_ASSIGNMENT;\n+        }\n       }\n       ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n           getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n+          ActivityDiagnosticConstant.EMPTY);\n+\n       return CSAssignment.NULL_ASSIGNMENT;\n+    } finally {\n+      writeLock.unlock();\n     }\n-\n-    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n-        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n-      FiCaSchedulerApp application \u003d assignmentIterator.next();\n-\n-      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n-          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n-\n-      // Check queue max-capacity limit\n-      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n-          currentResourceLimits, application.getCurrentReservation(),\n-          schedulingMode)) {\n-        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n-            activitiesManager, node,\n-            application, application.getPriority(),\n-            ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-            ActivityDiagnosticConstant.EMPTY);\n-        return CSAssignment.NULL_ASSIGNMENT;\n-      }\n-\n-      Resource userLimit \u003d\n-          computeUserLimitAndSetHeadroom(application, clusterResource,\n-              node.getPartition(), schedulingMode);\n-\n-      // Check user limit\n-      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n-          application, node.getPartition(), currentResourceLimits)) {\n-        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n-            \"User capacity has reached its maximum limit.\");\n-        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n-            activitiesManager, node,\n-            application, application.getPriority(),\n-            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n-        continue;\n-      }\n-\n-      // Try to schedule\n-      CSAssignment assignment \u003d\n-          application.assignContainers(clusterResource, node,\n-              currentResourceLimits, schedulingMode, null);\n-\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"post-assignContainers for application \"\n-            + application.getApplicationId());\n-        application.showRequests();\n-      }\n-\n-      // Did we schedule or reserve a container?\n-      Resource assigned \u003d assignment.getResource();\n-      \n-      handleExcessReservedContainer(clusterResource, assignment, node,\n-          application);\n-      killToPreemptContainers(clusterResource, node, assignment);\n-\n-      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n-          Resources.none())) {\n-        // Get reserved or allocated container from application\n-        RMContainer reservedOrAllocatedRMContainer \u003d\n-            application.getRMContainer(assignment.getAssignmentInformation()\n-                .getFirstAllocatedOrReservedContainerId());\n-\n-        // Book-keeping\n-        // Note: Update headroom to account for current allocation too...\n-        allocateResource(clusterResource, application, assigned,\n-            node.getPartition(), reservedOrAllocatedRMContainer,\n-            assignment.isIncreasedAllocation());\n-\n-        // Update reserved metrics\n-        Resource reservedRes \u003d assignment.getAssignmentInformation()\n-            .getReserved();\n-        if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n-          incReservedResource(node.getPartition(), reservedRes);\n-        }\n-\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.ACCEPTED,\n-            ActivityDiagnosticConstant.EMPTY);\n-\n-        // Done\n-        return assignment;\n-      } else if (assignment.getSkippedType()\n-          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n-        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n-            activitiesManager, application.getApplicationId(),\n-            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n-        application.updateNodeInfoForAMDiagnostics(node);\n-      } else if(assignment.getSkippedType()\n-          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n-        return assignment;\n-      } else {\n-        // If we don\u0027t allocate anything, and it is not skipped by application,\n-        // we will return to respect FIFO of applications\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-            ActivityDiagnosticConstant.RESPECT_FIFO);\n-        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n-            activitiesManager, application.getApplicationId(),\n-            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n-        return CSAssignment.NULL_ASSIGNMENT;\n-      }\n-    }\n-    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n-        ActivityDiagnosticConstant.EMPTY);\n-\n-    return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    try {\n      writeLock.lock();\n      updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\n            \"assignContainers: node\u003d\" + node.getNodeName() + \" #applications\u003d\"\n                + orderingPolicy.getNumSchedulableEntities());\n      }\n\n      setPreemptionAllowed(currentResourceLimits, node.getPartition());\n\n      // Check for reserved resources\n      RMContainer reservedContainer \u003d node.getReservedContainer();\n      if (reservedContainer !\u003d null) {\n        FiCaSchedulerApp application \u003d getApplication(\n            reservedContainer.getApplicationAttemptId());\n\n        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n\n        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n            node, currentResourceLimits, schedulingMode, reservedContainer);\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        killToPreemptContainers(clusterResource, node, assignment);\n        return assignment;\n      }\n\n      // if our queue cannot access this node, just return\n      if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n          \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n            ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + node\n                .getPartition());\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n\n      // Check if this queue need more resource, simply skip allocation if this\n      // queue doesn\u0027t need more resources.\n      if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n          schedulingMode)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n              + schedulingMode.name() + \" node-partition\u003d\" + node\n              .getPartition());\n        }\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n\n      for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n           orderingPolicy.getAssignmentIterator();\n           assignmentIterator.hasNext(); ) {\n        FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n        ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n            node.getNodeID(), SystemClock.getInstance().getTime(), application);\n\n        // Check queue max-capacity limit\n        if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n            currentResourceLimits, application.getCurrentReservation(),\n            schedulingMode)) {\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n              ActivityDiagnosticConstant.EMPTY);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n\n        Resource userLimit \u003d computeUserLimitAndSetHeadroom(application,\n            clusterResource, node.getPartition(), schedulingMode);\n\n        // Check user limit\n        if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n            application, node.getPartition(), currentResourceLimits)) {\n          application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n              \"User capacity has reached its maximum limit.\");\n          ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n              activitiesManager, node, application, application.getPriority(),\n              ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n          continue;\n        }\n\n        // Try to schedule\n        CSAssignment assignment \u003d application.assignContainers(clusterResource,\n            node, currentResourceLimits, schedulingMode, null);\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"post-assignContainers for application \" + application\n              .getApplicationId());\n          application.showRequests();\n        }\n\n        // Did we schedule or reserve a container?\n        Resource assigned \u003d assignment.getResource();\n\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        killToPreemptContainers(clusterResource, node, assignment);\n\n        if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n            Resources.none())) {\n          // Get reserved or allocated container from application\n          RMContainer reservedOrAllocatedRMContainer \u003d\n              application.getRMContainer(assignment.getAssignmentInformation()\n                  .getFirstAllocatedOrReservedContainerId());\n\n          // Book-keeping\n          // Note: Update headroom to account for current allocation too...\n          allocateResource(clusterResource, application, assigned,\n              node.getPartition(), reservedOrAllocatedRMContainer,\n              assignment.isIncreasedAllocation());\n\n          // Update reserved metrics\n          Resource reservedRes \u003d\n              assignment.getAssignmentInformation().getReserved();\n          if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n            incReservedResource(node.getPartition(), reservedRes);\n          }\n\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(),\n              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n\n          // Done\n          return assignment;\n        } else if (assignment.getSkippedType()\n            \u003d\u003d CSAssignment.SkippedType.OTHER) {\n          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n              activitiesManager, application.getApplicationId(),\n              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n          application.updateNodeInfoForAMDiagnostics(node);\n        } else if (assignment.getSkippedType()\n            \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n          return assignment;\n        } else{\n          // If we don\u0027t allocate anything, and it is not skipped by application,\n          // we will return to respect FIFO of applications\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n              ActivityDiagnosticConstant.RESPECT_FIFO);\n          ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n              activitiesManager, application.getApplicationId(),\n              ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n          return CSAssignment.NULL_ASSIGNMENT;\n        }\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.EMPTY);\n\n      return CSAssignment.NULL_ASSIGNMENT;\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "e0d131f055ee126052ad4d0f7b0d192e6c730188": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4091. Add REST API to retrieve scheduler activity. (Chen Ge and Sunil G via wangda)\n",
      "commitDate": "05/08/16 10:27 AM",
      "commitName": "e0d131f055ee126052ad4d0f7b0d192e6c730188",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "03/08/16 11:53 AM",
      "commitNameOld": "4d92aefd35d4517d9435d81bafdec0d77905a7a0",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 1.94,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,124 +1,165 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     setPreemptionAllowed(currentResourceLimits, node.getPartition());\n \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d\n           getApplication(reservedContainer.getApplicationAttemptId());\n+\n+      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n+          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n+\n       synchronized (application) {\n         CSAssignment assignment \u003d\n             application.assignContainers(clusterResource, node,\n                 currentResourceLimits, schedulingMode, reservedContainer);\n         handleExcessReservedContainer(clusterResource, assignment, node,\n             application);\n         killToPreemptContainers(clusterResource, node, assignment);\n         return assignment;\n       }\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n+      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n+          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + node\n+              .getPartition());\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n+      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n+      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n+          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n+\n       // Check queue max-capacity limit\n       if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n           currentResourceLimits, application.getCurrentReservation(),\n           schedulingMode)) {\n+        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n+            activitiesManager, node,\n+            application, application.getPriority(),\n+            ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+            ActivityDiagnosticConstant.EMPTY);\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n-      \n+\n       Resource userLimit \u003d\n           computeUserLimitAndSetHeadroom(application, clusterResource,\n               node.getPartition(), schedulingMode);\n \n       // Check user limit\n       if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n           application, node.getPartition(), currentResourceLimits)) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n+        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n+            activitiesManager, node,\n+            application, application.getPriority(),\n+            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n         continue;\n       }\n \n       // Try to schedule\n       CSAssignment assignment \u003d\n           application.assignContainers(clusterResource, node,\n               currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n             + application.getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n       \n       handleExcessReservedContainer(clusterResource, assignment, node,\n           application);\n       killToPreemptContainers(clusterResource, node, assignment);\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         // Get reserved or allocated container from application\n         RMContainer reservedOrAllocatedRMContainer \u003d\n             application.getRMContainer(assignment.getAssignmentInformation()\n                 .getFirstAllocatedOrReservedContainerId());\n \n         // Book-keeping\n         // Note: Update headroom to account for current allocation too...\n         allocateResource(clusterResource, application, assigned,\n             node.getPartition(), reservedOrAllocatedRMContainer,\n             assignment.isIncreasedAllocation());\n \n         // Update reserved metrics\n         Resource reservedRes \u003d assignment.getAssignmentInformation()\n             .getReserved();\n         if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n           incReservedResource(node.getPartition(), reservedRes);\n         }\n \n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.ACCEPTED,\n+            ActivityDiagnosticConstant.EMPTY);\n+\n         // Done\n         return assignment;\n       } else if (assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.OTHER) {\n+        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n+            activitiesManager, application.getApplicationId(),\n+            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else if(assignment.getSkippedType()\n           \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n         return assignment;\n       } else {\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+            ActivityDiagnosticConstant.RESPECT_FIFO);\n+        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n+            activitiesManager, application.getApplicationId(),\n+            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n+    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n+        ActivityDiagnosticConstant.EMPTY);\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, node.getPartition());\n\n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d\n          getApplication(reservedContainer.getApplicationAttemptId());\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n\n      synchronized (application) {\n        CSAssignment assignment \u003d\n            application.assignContainers(clusterResource, node,\n                currentResourceLimits, schedulingMode, reservedContainer);\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        killToPreemptContainers(clusterResource, node, assignment);\n        return assignment;\n      }\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.REJECTED,\n          ActivityDiagnosticConstant.NOT_ABLE_TO_ACCESS_PARTITION + node\n              .getPartition());\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n          getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n          ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      ActivitiesLogger.APP.startAppAllocationRecording(activitiesManager,\n          node.getNodeID(), SystemClock.getInstance().getTime(), application);\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node,\n            application, application.getPriority(),\n            ActivityDiagnosticConstant.QUEUE_MAX_CAPACITY_LIMIT);\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n\n      Resource userLimit \u003d\n          computeUserLimitAndSetHeadroom(application, clusterResource,\n              node.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, node.getPartition(), currentResourceLimits)) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue(\n            activitiesManager, node,\n            application, application.getPriority(),\n            ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);\n        continue;\n      }\n\n      // Try to schedule\n      CSAssignment assignment \u003d\n          application.assignContainers(clusterResource, node,\n              currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n            + application.getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n      \n      handleExcessReservedContainer(clusterResource, assignment, node,\n          application);\n      killToPreemptContainers(clusterResource, node, assignment);\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        // Get reserved or allocated container from application\n        RMContainer reservedOrAllocatedRMContainer \u003d\n            application.getRMContainer(assignment.getAssignmentInformation()\n                .getFirstAllocatedOrReservedContainerId());\n\n        // Book-keeping\n        // Note: Update headroom to account for current allocation too...\n        allocateResource(clusterResource, application, assigned,\n            node.getPartition(), reservedOrAllocatedRMContainer,\n            assignment.isIncreasedAllocation());\n\n        // Update reserved metrics\n        Resource reservedRes \u003d assignment.getAssignmentInformation()\n            .getReserved();\n        if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n          incReservedResource(node.getPartition(), reservedRes);\n        }\n\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.ACCEPTED,\n            ActivityDiagnosticConstant.EMPTY);\n\n        // Done\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if(assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        return assignment;\n      } else {\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n            ActivityDiagnosticConstant.RESPECT_FIFO);\n        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(\n            activitiesManager, application.getApplicationId(),\n            ActivityState.SKIPPED, ActivityDiagnosticConstant.EMPTY);\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n        getParent().getQueueName(), getQueueName(), ActivityState.SKIPPED,\n        ActivityDiagnosticConstant.EMPTY);\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "4d92aefd35d4517d9435d81bafdec0d77905a7a0": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4280. CapacityScheduler reservations may not prevent indefinite postponement on a busy cluster. Contributed by Kuhu Shukla\n",
      "commitDate": "03/08/16 11:53 AM",
      "commitName": "4d92aefd35d4517d9435d81bafdec0d77905a7a0",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "26/07/16 2:54 PM",
      "commitNameOld": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 7.87,
      "commitsBetweenForRepo": 56,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,120 +1,124 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     setPreemptionAllowed(currentResourceLimits, node.getPartition());\n \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d\n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         CSAssignment assignment \u003d\n             application.assignContainers(clusterResource, node,\n                 currentResourceLimits, schedulingMode, reservedContainer);\n         handleExcessReservedContainer(clusterResource, assignment, node,\n             application);\n         killToPreemptContainers(clusterResource, node, assignment);\n         return assignment;\n       }\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       // Check queue max-capacity limit\n       if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n           currentResourceLimits, application.getCurrentReservation(),\n           schedulingMode)) {\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n       \n       Resource userLimit \u003d\n           computeUserLimitAndSetHeadroom(application, clusterResource,\n               node.getPartition(), schedulingMode);\n \n       // Check user limit\n       if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n           application, node.getPartition(), currentResourceLimits)) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         continue;\n       }\n \n       // Try to schedule\n       CSAssignment assignment \u003d\n           application.assignContainers(clusterResource, node,\n               currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n             + application.getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n       \n       handleExcessReservedContainer(clusterResource, assignment, node,\n           application);\n       killToPreemptContainers(clusterResource, node, assignment);\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         // Get reserved or allocated container from application\n         RMContainer reservedOrAllocatedRMContainer \u003d\n             application.getRMContainer(assignment.getAssignmentInformation()\n                 .getFirstAllocatedOrReservedContainerId());\n \n         // Book-keeping\n         // Note: Update headroom to account for current allocation too...\n         allocateResource(clusterResource, application, assigned,\n             node.getPartition(), reservedOrAllocatedRMContainer,\n             assignment.isIncreasedAllocation());\n \n         // Update reserved metrics\n         Resource reservedRes \u003d assignment.getAssignmentInformation()\n             .getReserved();\n         if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n           incReservedResource(node.getPartition(), reservedRes);\n         }\n \n         // Done\n         return assignment;\n-      } else if (assignment.getSkipped()) {\n+      } else if (assignment.getSkippedType()\n+          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n         application.updateNodeInfoForAMDiagnostics(node);\n+      } else if(assignment.getSkippedType()\n+          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n+        return assignment;\n       } else {\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, node.getPartition());\n\n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d\n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        CSAssignment assignment \u003d\n            application.assignContainers(clusterResource, node,\n                currentResourceLimits, schedulingMode, reservedContainer);\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        killToPreemptContainers(clusterResource, node, assignment);\n        return assignment;\n      }\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n      \n      Resource userLimit \u003d\n          computeUserLimitAndSetHeadroom(application, clusterResource,\n              node.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, node.getPartition(), currentResourceLimits)) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        continue;\n      }\n\n      // Try to schedule\n      CSAssignment assignment \u003d\n          application.assignContainers(clusterResource, node,\n              currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n            + application.getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n      \n      handleExcessReservedContainer(clusterResource, assignment, node,\n          application);\n      killToPreemptContainers(clusterResource, node, assignment);\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        // Get reserved or allocated container from application\n        RMContainer reservedOrAllocatedRMContainer \u003d\n            application.getRMContainer(assignment.getAssignmentInformation()\n                .getFirstAllocatedOrReservedContainerId());\n\n        // Book-keeping\n        // Note: Update headroom to account for current allocation too...\n        allocateResource(clusterResource, application, assigned,\n            node.getPartition(), reservedOrAllocatedRMContainer,\n            assignment.isIncreasedAllocation());\n\n        // Update reserved metrics\n        Resource reservedRes \u003d assignment.getAssignmentInformation()\n            .getReserved();\n        if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n          incReservedResource(node.getPartition(), reservedRes);\n        }\n\n        // Done\n        return assignment;\n      } else if (assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.OTHER) {\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else if(assignment.getSkippedType()\n          \u003d\u003d CSAssignment.SkippedType.QUEUE_LIMIT) {\n        return assignment;\n      } else {\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "fc055a3cbe9545cf1c59421641c7b296aa33f953": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4865. Track Reserved resources in ResourceUsage and QueueCapacities. (Sunil G via wangda)\n",
      "commitDate": "29/03/16 5:07 PM",
      "commitName": "fc055a3cbe9545cf1c59421641c7b296aa33f953",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 5:02 PM",
      "commitNameOld": "ae14e5d07f1b6702a5160637438028bb03d9387e",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 13.0,
      "commitsBetweenForRepo": 64,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,113 +1,120 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     setPreemptionAllowed(currentResourceLimits, node.getPartition());\n \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d\n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         CSAssignment assignment \u003d\n             application.assignContainers(clusterResource, node,\n                 currentResourceLimits, schedulingMode, reservedContainer);\n         handleExcessReservedContainer(clusterResource, assignment, node,\n             application);\n         killToPreemptContainers(clusterResource, node, assignment);\n         return assignment;\n       }\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       // Check queue max-capacity limit\n       if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n           currentResourceLimits, application.getCurrentReservation(),\n           schedulingMode)) {\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n       \n       Resource userLimit \u003d\n           computeUserLimitAndSetHeadroom(application, clusterResource,\n               node.getPartition(), schedulingMode);\n \n       // Check user limit\n       if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n           application, node.getPartition(), currentResourceLimits)) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         continue;\n       }\n \n       // Try to schedule\n       CSAssignment assignment \u003d\n           application.assignContainers(clusterResource, node,\n               currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n             + application.getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n       \n       handleExcessReservedContainer(clusterResource, assignment, node,\n           application);\n       killToPreemptContainers(clusterResource, node, assignment);\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         // Get reserved or allocated container from application\n         RMContainer reservedOrAllocatedRMContainer \u003d\n             application.getRMContainer(assignment.getAssignmentInformation()\n                 .getFirstAllocatedOrReservedContainerId());\n \n         // Book-keeping\n         // Note: Update headroom to account for current allocation too...\n         allocateResource(clusterResource, application, assigned,\n             node.getPartition(), reservedOrAllocatedRMContainer,\n             assignment.isIncreasedAllocation());\n \n+        // Update reserved metrics\n+        Resource reservedRes \u003d assignment.getAssignmentInformation()\n+            .getReserved();\n+        if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n+          incReservedResource(node.getPartition(), reservedRes);\n+        }\n+\n         // Done\n         return assignment;\n       } else if (assignment.getSkipped()) {\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else {\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, node.getPartition());\n\n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d\n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        CSAssignment assignment \u003d\n            application.assignContainers(clusterResource, node,\n                currentResourceLimits, schedulingMode, reservedContainer);\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        killToPreemptContainers(clusterResource, node, assignment);\n        return assignment;\n      }\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n      \n      Resource userLimit \u003d\n          computeUserLimitAndSetHeadroom(application, clusterResource,\n              node.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, node.getPartition(), currentResourceLimits)) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        continue;\n      }\n\n      // Try to schedule\n      CSAssignment assignment \u003d\n          application.assignContainers(clusterResource, node,\n              currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n            + application.getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n      \n      handleExcessReservedContainer(clusterResource, assignment, node,\n          application);\n      killToPreemptContainers(clusterResource, node, assignment);\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        // Get reserved or allocated container from application\n        RMContainer reservedOrAllocatedRMContainer \u003d\n            application.getRMContainer(assignment.getAssignmentInformation()\n                .getFirstAllocatedOrReservedContainerId());\n\n        // Book-keeping\n        // Note: Update headroom to account for current allocation too...\n        allocateResource(clusterResource, application, assigned,\n            node.getPartition(), reservedOrAllocatedRMContainer,\n            assignment.isIncreasedAllocation());\n\n        // Update reserved metrics\n        Resource reservedRes \u003d assignment.getAssignmentInformation()\n            .getReserved();\n        if (reservedRes !\u003d null \u0026\u0026 !reservedRes.equals(Resources.none())) {\n          incReservedResource(node.getPartition(), reservedRes);\n        }\n\n        // Done\n        return assignment;\n      } else if (assignment.getSkipped()) {\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else {\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "ae14e5d07f1b6702a5160637438028bb03d9387e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4108. CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n\n(cherry picked from commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913)\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "ae14e5d07f1b6702a5160637438028bb03d9387e",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 5:02 PM",
      "commitNameOld": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,113 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n+    setPreemptionAllowed(currentResourceLimits, node.getPartition());\n+\n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d\n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         CSAssignment assignment \u003d\n             application.assignContainers(clusterResource, node,\n                 currentResourceLimits, schedulingMode, reservedContainer);\n         handleExcessReservedContainer(clusterResource, assignment, node,\n             application);\n+        killToPreemptContainers(clusterResource, node, assignment);\n         return assignment;\n       }\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       // Check queue max-capacity limit\n       if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n           currentResourceLimits, application.getCurrentReservation(),\n           schedulingMode)) {\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n       \n       Resource userLimit \u003d\n           computeUserLimitAndSetHeadroom(application, clusterResource,\n               node.getPartition(), schedulingMode);\n \n       // Check user limit\n       if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n           application, node.getPartition(), currentResourceLimits)) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         continue;\n       }\n \n       // Try to schedule\n       CSAssignment assignment \u003d\n           application.assignContainers(clusterResource, node,\n               currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n             + application.getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n       \n       handleExcessReservedContainer(clusterResource, assignment, node,\n           application);\n+      killToPreemptContainers(clusterResource, node, assignment);\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         // Get reserved or allocated container from application\n         RMContainer reservedOrAllocatedRMContainer \u003d\n             application.getRMContainer(assignment.getAssignmentInformation()\n                 .getFirstAllocatedOrReservedContainerId());\n \n         // Book-keeping\n         // Note: Update headroom to account for current allocation too...\n         allocateResource(clusterResource, application, assigned,\n             node.getPartition(), reservedOrAllocatedRMContainer,\n             assignment.isIncreasedAllocation());\n \n         // Done\n         return assignment;\n       } else if (assignment.getSkipped()) {\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else {\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, node.getPartition());\n\n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d\n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        CSAssignment assignment \u003d\n            application.assignContainers(clusterResource, node,\n                currentResourceLimits, schedulingMode, reservedContainer);\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        killToPreemptContainers(clusterResource, node, assignment);\n        return assignment;\n      }\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n      \n      Resource userLimit \u003d\n          computeUserLimitAndSetHeadroom(application, clusterResource,\n              node.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, node.getPartition(), currentResourceLimits)) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        continue;\n      }\n\n      // Try to schedule\n      CSAssignment assignment \u003d\n          application.assignContainers(clusterResource, node,\n              currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n            + application.getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n      \n      handleExcessReservedContainer(clusterResource, assignment, node,\n          application);\n      killToPreemptContainers(clusterResource, node, assignment);\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        // Get reserved or allocated container from application\n        RMContainer reservedOrAllocatedRMContainer \u003d\n            application.getRMContainer(assignment.getAssignmentInformation()\n                .getFirstAllocatedOrReservedContainerId());\n\n        // Book-keeping\n        // Note: Update headroom to account for current allocation too...\n        allocateResource(clusterResource, application, assigned,\n            node.getPartition(), reservedOrAllocatedRMContainer,\n            assignment.isIncreasedAllocation());\n\n        // Done\n        return assignment;\n      } else if (assignment.getSkipped()) {\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else {\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "fa7a43529d529f0006c8033c2003f15b9b93f103": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\"\n\nThis reverts commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913.\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 4:59 PM",
      "commitNameOld": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,113 +1,109 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n-    setPreemptionAllowed(currentResourceLimits, node.getPartition());\n-\n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d\n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         CSAssignment assignment \u003d\n             application.assignContainers(clusterResource, node,\n                 currentResourceLimits, schedulingMode, reservedContainer);\n         handleExcessReservedContainer(clusterResource, assignment, node,\n             application);\n-        killToPreemptContainers(clusterResource, node, assignment);\n         return assignment;\n       }\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       // Check queue max-capacity limit\n       if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n           currentResourceLimits, application.getCurrentReservation(),\n           schedulingMode)) {\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n       \n       Resource userLimit \u003d\n           computeUserLimitAndSetHeadroom(application, clusterResource,\n               node.getPartition(), schedulingMode);\n \n       // Check user limit\n       if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n           application, node.getPartition(), currentResourceLimits)) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         continue;\n       }\n \n       // Try to schedule\n       CSAssignment assignment \u003d\n           application.assignContainers(clusterResource, node,\n               currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n             + application.getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n       \n       handleExcessReservedContainer(clusterResource, assignment, node,\n           application);\n-      killToPreemptContainers(clusterResource, node, assignment);\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         // Get reserved or allocated container from application\n         RMContainer reservedOrAllocatedRMContainer \u003d\n             application.getRMContainer(assignment.getAssignmentInformation()\n                 .getFirstAllocatedOrReservedContainerId());\n \n         // Book-keeping\n         // Note: Update headroom to account for current allocation too...\n         allocateResource(clusterResource, application, assigned,\n             node.getPartition(), reservedOrAllocatedRMContainer,\n             assignment.isIncreasedAllocation());\n \n         // Done\n         return assignment;\n       } else if (assignment.getSkipped()) {\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else {\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d\n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        CSAssignment assignment \u003d\n            application.assignContainers(clusterResource, node,\n                currentResourceLimits, schedulingMode, reservedContainer);\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        return assignment;\n      }\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n      \n      Resource userLimit \u003d\n          computeUserLimitAndSetHeadroom(application, clusterResource,\n              node.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, node.getPartition(), currentResourceLimits)) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        continue;\n      }\n\n      // Try to schedule\n      CSAssignment assignment \u003d\n          application.assignContainers(clusterResource, node,\n              currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n            + application.getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n      \n      handleExcessReservedContainer(clusterResource, assignment, node,\n          application);\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        // Get reserved or allocated container from application\n        RMContainer reservedOrAllocatedRMContainer \u003d\n            application.getRMContainer(assignment.getAssignmentInformation()\n                .getFirstAllocatedOrReservedContainerId());\n\n        // Book-keeping\n        // Note: Update headroom to account for current allocation too...\n        allocateResource(clusterResource, application, assigned,\n            node.getPartition(), reservedOrAllocatedRMContainer,\n            assignment.isIncreasedAllocation());\n\n        // Done\n        return assignment;\n      } else if (assignment.getSkipped()) {\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else {\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": {
      "type": "Ybodychange",
      "commitMessage": "CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n",
      "commitDate": "16/03/16 4:59 PM",
      "commitName": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "29/01/16 12:22 PM",
      "commitNameOld": "f4a57d4a531e793373fe3118d644871a3b9ae0b1",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 47.15,
      "commitsBetweenForRepo": 307,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,113 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n+    setPreemptionAllowed(currentResourceLimits, node.getPartition());\n+\n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d\n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         CSAssignment assignment \u003d\n             application.assignContainers(clusterResource, node,\n                 currentResourceLimits, schedulingMode, reservedContainer);\n         handleExcessReservedContainer(clusterResource, assignment, node,\n             application);\n+        killToPreemptContainers(clusterResource, node, assignment);\n         return assignment;\n       }\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       // Check queue max-capacity limit\n       if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n           currentResourceLimits, application.getCurrentReservation(),\n           schedulingMode)) {\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n       \n       Resource userLimit \u003d\n           computeUserLimitAndSetHeadroom(application, clusterResource,\n               node.getPartition(), schedulingMode);\n \n       // Check user limit\n       if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n           application, node.getPartition(), currentResourceLimits)) {\n         application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n             \"User capacity has reached its maximum limit.\");\n         continue;\n       }\n \n       // Try to schedule\n       CSAssignment assignment \u003d\n           application.assignContainers(clusterResource, node,\n               currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n             + application.getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n       \n       handleExcessReservedContainer(clusterResource, assignment, node,\n           application);\n+      killToPreemptContainers(clusterResource, node, assignment);\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         // Get reserved or allocated container from application\n         RMContainer reservedOrAllocatedRMContainer \u003d\n             application.getRMContainer(assignment.getAssignmentInformation()\n                 .getFirstAllocatedOrReservedContainerId());\n \n         // Book-keeping\n         // Note: Update headroom to account for current allocation too...\n         allocateResource(clusterResource, application, assigned,\n             node.getPartition(), reservedOrAllocatedRMContainer,\n             assignment.isIncreasedAllocation());\n \n         // Done\n         return assignment;\n       } else if (assignment.getSkipped()) {\n         application.updateNodeInfoForAMDiagnostics(node);\n       } else {\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    setPreemptionAllowed(currentResourceLimits, node.getPartition());\n\n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d\n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        CSAssignment assignment \u003d\n            application.assignContainers(clusterResource, node,\n                currentResourceLimits, schedulingMode, reservedContainer);\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        killToPreemptContainers(clusterResource, node, assignment);\n        return assignment;\n      }\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n      \n      Resource userLimit \u003d\n          computeUserLimitAndSetHeadroom(application, clusterResource,\n              node.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, node.getPartition(), currentResourceLimits)) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        continue;\n      }\n\n      // Try to schedule\n      CSAssignment assignment \u003d\n          application.assignContainers(clusterResource, node,\n              currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n            + application.getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n      \n      handleExcessReservedContainer(clusterResource, assignment, node,\n          application);\n      killToPreemptContainers(clusterResource, node, assignment);\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        // Get reserved or allocated container from application\n        RMContainer reservedOrAllocatedRMContainer \u003d\n            application.getRMContainer(assignment.getAssignmentInformation()\n                .getFirstAllocatedOrReservedContainerId());\n\n        // Book-keeping\n        // Note: Update headroom to account for current allocation too...\n        allocateResource(clusterResource, application, assigned,\n            node.getPartition(), reservedOrAllocatedRMContainer,\n            assignment.isIncreasedAllocation());\n\n        // Done\n        return assignment;\n      } else if (assignment.getSkipped()) {\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else {\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "6cb0af3c39a5d49cb2f7911ee21363a9542ca2d7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3946. Update exact reason as to why a submitted app is in ACCEPTED state to app\u0027s diagnostic message. (Naganarasimha G R via wangda)\n",
      "commitDate": "14/12/15 10:52 AM",
      "commitName": "6cb0af3c39a5d49cb2f7911ee21363a9542ca2d7",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "20/11/15 3:55 PM",
      "commitNameOld": "2346fa3141bf28f25a90b6a426a1d3a3982e464f",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 23.79,
      "commitsBetweenForRepo": 150,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,105 +1,109 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d\n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         CSAssignment assignment \u003d\n             application.assignContainers(clusterResource, node,\n                 currentResourceLimits, schedulingMode, reservedContainer);\n         handleExcessReservedContainer(clusterResource, assignment, node,\n             application);\n         return assignment;\n       }\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       // Check queue max-capacity limit\n       if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n           currentResourceLimits, application.getCurrentReservation(),\n           schedulingMode)) {\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n       \n       Resource userLimit \u003d\n           computeUserLimitAndSetHeadroom(application, clusterResource,\n               node.getPartition(), schedulingMode);\n \n       // Check user limit\n       if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n           application, node.getPartition(), currentResourceLimits)) {\n+        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n+            \"User capacity has reached its maximum limit.\");\n         continue;\n       }\n \n       // Try to schedule\n       CSAssignment assignment \u003d\n           application.assignContainers(clusterResource, node,\n               currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n             + application.getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n       \n       handleExcessReservedContainer(clusterResource, assignment, node,\n           application);\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         // Get reserved or allocated container from application\n         RMContainer reservedOrAllocatedRMContainer \u003d\n             application.getRMContainer(assignment.getAssignmentInformation()\n                 .getFirstAllocatedOrReservedContainerId());\n \n         // Book-keeping\n         // Note: Update headroom to account for current allocation too...\n         allocateResource(clusterResource, application, assigned,\n             node.getPartition(), reservedOrAllocatedRMContainer,\n             assignment.isIncreasedAllocation());\n \n         // Done\n         return assignment;\n-      } else if (!assignment.getSkipped()) {\n+      } else if (assignment.getSkipped()) {\n+        application.updateNodeInfoForAMDiagnostics(node);\n+      } else {\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d\n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        CSAssignment assignment \u003d\n            application.assignContainers(clusterResource, node,\n                currentResourceLimits, schedulingMode, reservedContainer);\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        return assignment;\n      }\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n      \n      Resource userLimit \u003d\n          computeUserLimitAndSetHeadroom(application, clusterResource,\n              node.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, node.getPartition(), currentResourceLimits)) {\n        application.updateAMContainerDiagnostics(AMState.ACTIVATED,\n            \"User capacity has reached its maximum limit.\");\n        continue;\n      }\n\n      // Try to schedule\n      CSAssignment assignment \u003d\n          application.assignContainers(clusterResource, node,\n              currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n            + application.getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n      \n      handleExcessReservedContainer(clusterResource, assignment, node,\n          application);\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        // Get reserved or allocated container from application\n        RMContainer reservedOrAllocatedRMContainer \u003d\n            application.getRMContainer(assignment.getAssignmentInformation()\n                .getFirstAllocatedOrReservedContainerId());\n\n        // Book-keeping\n        // Note: Update headroom to account for current allocation too...\n        allocateResource(clusterResource, application, assigned,\n            node.getPartition(), reservedOrAllocatedRMContainer,\n            assignment.isIncreasedAllocation());\n\n        // Done\n        return assignment;\n      } else if (assignment.getSkipped()) {\n        application.updateNodeInfoForAMDiagnostics(node);\n      } else {\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "89cab1ba5f0671f8ef30dbe7432079c18362b434": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1651. CapacityScheduler side changes to support container resize. Contributed by Wangda Tan\n",
      "commitDate": "23/09/15 1:29 PM",
      "commitName": "89cab1ba5f0671f8ef30dbe7432079c18362b434",
      "commitAuthor": "Jian He",
      "commitDateOld": "21/09/15 8:54 PM",
      "commitNameOld": "dfd807afab0fae3839c9cc5d552aa0304444f956",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 1.69,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,102 +1,105 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d\n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         CSAssignment assignment \u003d\n             application.assignContainers(clusterResource, node,\n                 currentResourceLimits, schedulingMode, reservedContainer);\n-        handleExcessReservedContainer(clusterResource, assignment);\n+        handleExcessReservedContainer(clusterResource, assignment, node,\n+            application);\n         return assignment;\n       }\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       // Check queue max-capacity limit\n       if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n           currentResourceLimits, application.getCurrentReservation(),\n           schedulingMode)) {\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n       \n       Resource userLimit \u003d\n           computeUserLimitAndSetHeadroom(application, clusterResource,\n               node.getPartition(), schedulingMode);\n \n       // Check user limit\n       if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n           application, node.getPartition(), currentResourceLimits)) {\n         continue;\n       }\n \n       // Try to schedule\n       CSAssignment assignment \u003d\n           application.assignContainers(clusterResource, node,\n               currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n             + application.getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n       \n-      handleExcessReservedContainer(clusterResource, assignment);\n+      handleExcessReservedContainer(clusterResource, assignment, node,\n+          application);\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         // Get reserved or allocated container from application\n         RMContainer reservedOrAllocatedRMContainer \u003d\n             application.getRMContainer(assignment.getAssignmentInformation()\n                 .getFirstAllocatedOrReservedContainerId());\n \n         // Book-keeping\n         // Note: Update headroom to account for current allocation too...\n         allocateResource(clusterResource, application, assigned,\n-            node.getPartition(), reservedOrAllocatedRMContainer);\n+            node.getPartition(), reservedOrAllocatedRMContainer,\n+            assignment.isIncreasedAllocation());\n \n         // Done\n         return assignment;\n       } else if (!assignment.getSkipped()) {\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d\n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        CSAssignment assignment \u003d\n            application.assignContainers(clusterResource, node,\n                currentResourceLimits, schedulingMode, reservedContainer);\n        handleExcessReservedContainer(clusterResource, assignment, node,\n            application);\n        return assignment;\n      }\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n      \n      Resource userLimit \u003d\n          computeUserLimitAndSetHeadroom(application, clusterResource,\n              node.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, node.getPartition(), currentResourceLimits)) {\n        continue;\n      }\n\n      // Try to schedule\n      CSAssignment assignment \u003d\n          application.assignContainers(clusterResource, node,\n              currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n            + application.getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n      \n      handleExcessReservedContainer(clusterResource, assignment, node,\n          application);\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        // Get reserved or allocated container from application\n        RMContainer reservedOrAllocatedRMContainer \u003d\n            application.getRMContainer(assignment.getAssignmentInformation()\n                .getFirstAllocatedOrReservedContainerId());\n\n        // Book-keeping\n        // Note: Update headroom to account for current allocation too...\n        allocateResource(clusterResource, application, assigned,\n            node.getPartition(), reservedOrAllocatedRMContainer,\n            assignment.isIncreasedAllocation());\n\n        // Done\n        return assignment;\n      } else if (!assignment.getSkipped()) {\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "e5003be907acef87c2770e3f2914953f62017b0e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4026. Refactored ContainerAllocator to accept a list of priorites rather than a single priority. Contributed by Wangda Tan\n",
      "commitDate": "12/08/15 3:07 PM",
      "commitName": "e5003be907acef87c2770e3f2914953f62017b0e",
      "commitAuthor": "Jian He",
      "commitDateOld": "10/08/15 2:54 PM",
      "commitNameOld": "cf9d3c925608e8bc650d43975382ed3014081057",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 2.01,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,102 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d\n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n-        CSAssignment assignment \u003d application.assignReservedContainer(node, reservedContainer,\n-            clusterResource, schedulingMode);\n+        CSAssignment assignment \u003d\n+            application.assignContainers(clusterResource, node,\n+                currentResourceLimits, schedulingMode, reservedContainer);\n         handleExcessReservedContainer(clusterResource, assignment);\n         return assignment;\n       }\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       // Check queue max-capacity limit\n       if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n           currentResourceLimits, application.getCurrentReservation(),\n           schedulingMode)) {\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n       \n       Resource userLimit \u003d\n           computeUserLimitAndSetHeadroom(application, clusterResource,\n               node.getPartition(), schedulingMode);\n \n       // Check user limit\n       if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n           application, node.getPartition(), currentResourceLimits)) {\n         continue;\n       }\n \n       // Try to schedule\n       CSAssignment assignment \u003d\n           application.assignContainers(clusterResource, node,\n-              currentResourceLimits, schedulingMode);\n+              currentResourceLimits, schedulingMode, null);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n             + application.getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n       \n       handleExcessReservedContainer(clusterResource, assignment);\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         // Get reserved or allocated container from application\n         RMContainer reservedOrAllocatedRMContainer \u003d\n             application.getRMContainer(assignment.getAssignmentInformation()\n                 .getFirstAllocatedOrReservedContainerId());\n \n         // Book-keeping\n         // Note: Update headroom to account for current allocation too...\n         allocateResource(clusterResource, application, assigned,\n             node.getPartition(), reservedOrAllocatedRMContainer);\n \n         // Done\n         return assignment;\n       } else if (!assignment.getSkipped()) {\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n         return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n \n     return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d\n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        CSAssignment assignment \u003d\n            application.assignContainers(clusterResource, node,\n                currentResourceLimits, schedulingMode, reservedContainer);\n        handleExcessReservedContainer(clusterResource, assignment);\n        return assignment;\n      }\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n      \n      Resource userLimit \u003d\n          computeUserLimitAndSetHeadroom(application, clusterResource,\n              node.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, node.getPartition(), currentResourceLimits)) {\n        continue;\n      }\n\n      // Try to schedule\n      CSAssignment assignment \u003d\n          application.assignContainers(clusterResource, node,\n              currentResourceLimits, schedulingMode, null);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n            + application.getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n      \n      handleExcessReservedContainer(clusterResource, assignment);\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        // Get reserved or allocated container from application\n        RMContainer reservedOrAllocatedRMContainer \u003d\n            application.getRMContainer(assignment.getAssignmentInformation()\n                .getFirstAllocatedOrReservedContainerId());\n\n        // Book-keeping\n        // Note: Update headroom to account for current allocation too...\n        allocateResource(clusterResource, application, assigned,\n            node.getPartition(), reservedOrAllocatedRMContainer);\n\n        // Done\n        return assignment;\n      } else if (!assignment.getSkipped()) {\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "ba2313d6145a1234777938a747187373f4cd58d9": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3983. Refactored CapacityScheduleri#FiCaSchedulerApp to easier extend container allocation logic. Contributed by Wangda Tan\n",
      "commitDate": "05/08/15 1:47 PM",
      "commitName": "ba2313d6145a1234777938a747187373f4cd58d9",
      "commitAuthor": "Jian He",
      "commitDateOld": "24/07/15 2:00 PM",
      "commitNameOld": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 11.99,
      "commitsBetweenForRepo": 56,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,101 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d\n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         CSAssignment assignment \u003d application.assignReservedContainer(node, reservedContainer,\n             clusterResource, schedulingMode);\n         handleExcessReservedContainer(clusterResource, assignment);\n         return assignment;\n       }\n     }\n \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n-      return NULL_ASSIGNMENT;\n+      return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n         schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n-      return NULL_ASSIGNMENT;\n+      return CSAssignment.NULL_ASSIGNMENT;\n     }\n \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n \n       // Check queue max-capacity limit\n       if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n           currentResourceLimits, application.getCurrentReservation(),\n           schedulingMode)) {\n-        return NULL_ASSIGNMENT;\n+        return CSAssignment.NULL_ASSIGNMENT;\n       }\n       \n       Resource userLimit \u003d\n           computeUserLimitAndSetHeadroom(application, clusterResource,\n               node.getPartition(), schedulingMode);\n \n       // Check user limit\n       if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n           application, node.getPartition(), currentResourceLimits)) {\n         continue;\n       }\n \n       // Try to schedule\n       CSAssignment assignment \u003d\n           application.assignContainers(clusterResource, node,\n               currentResourceLimits, schedulingMode);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n             + application.getApplicationId());\n         application.showRequests();\n       }\n \n       // Did we schedule or reserve a container?\n       Resource assigned \u003d assignment.getResource();\n       \n       handleExcessReservedContainer(clusterResource, assignment);\n \n       if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n           Resources.none())) {\n         // Get reserved or allocated container from application\n         RMContainer reservedOrAllocatedRMContainer \u003d\n             application.getRMContainer(assignment.getAssignmentInformation()\n                 .getFirstAllocatedOrReservedContainerId());\n \n         // Book-keeping\n         // Note: Update headroom to account for current allocation too...\n         allocateResource(clusterResource, application, assigned,\n             node.getPartition(), reservedOrAllocatedRMContainer);\n \n         // Done\n         return assignment;\n       } else if (!assignment.getSkipped()) {\n         // If we don\u0027t allocate anything, and it is not skipped by application,\n         // we will return to respect FIFO of applications\n-        return NULL_ASSIGNMENT;\n+        return CSAssignment.NULL_ASSIGNMENT;\n       }\n     }\n \n-    return NULL_ASSIGNMENT;\n+    return CSAssignment.NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d\n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        CSAssignment assignment \u003d application.assignReservedContainer(node, reservedContainer,\n            clusterResource, schedulingMode);\n        handleExcessReservedContainer(clusterResource, assignment);\n        return assignment;\n      }\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return CSAssignment.NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n      \n      Resource userLimit \u003d\n          computeUserLimitAndSetHeadroom(application, clusterResource,\n              node.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, node.getPartition(), currentResourceLimits)) {\n        continue;\n      }\n\n      // Try to schedule\n      CSAssignment assignment \u003d\n          application.assignContainers(clusterResource, node,\n              currentResourceLimits, schedulingMode);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n            + application.getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n      \n      handleExcessReservedContainer(clusterResource, assignment);\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        // Get reserved or allocated container from application\n        RMContainer reservedOrAllocatedRMContainer \u003d\n            application.getRMContainer(assignment.getAssignmentInformation()\n                .getFirstAllocatedOrReservedContainerId());\n\n        // Book-keeping\n        // Note: Update headroom to account for current allocation too...\n        allocateResource(clusterResource, application, assigned,\n            node.getPartition(), reservedOrAllocatedRMContainer);\n\n        // Done\n        return assignment;\n      } else if (!assignment.getSkipped()) {\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        return CSAssignment.NULL_ASSIGNMENT;\n      }\n    }\n\n    return CSAssignment.NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
      "commitDate": "24/07/15 2:00 PM",
      "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
      "commitAuthor": "Jian He",
      "commitDateOld": "22/07/15 11:54 AM",
      "commitNameOld": "76ec26de8099dc48ce3812c595b7ab857a600442",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 2.09,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,235 +1,101 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n-    \n-    if(LOG.isDebugEnabled()) {\n+\n+    if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" #applications\u003d\" + \n-        orderingPolicy.getNumSchedulableEntities());\n+          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n     }\n-    \n+\n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n-      FiCaSchedulerApp application \u003d \n+      FiCaSchedulerApp application \u003d\n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n-        return assignReservedContainer(application, node, reservedContainer,\n+        CSAssignment assignment \u003d application.assignReservedContainer(node, reservedContainer,\n             clusterResource, schedulingMode);\n+        handleExcessReservedContainer(clusterResource, assignment);\n+        return assignment;\n       }\n     }\n-    \n+\n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return NULL_ASSIGNMENT;\n     }\n-    \n+\n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n-    if (!hasPendingResourceRequest(node.getPartition(),\n-        clusterResource, schedulingMode)) {\n+    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n+        schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return NULL_ASSIGNMENT;\n     }\n-    \n+\n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n-        orderingPolicy.getAssignmentIterator();\n-        assignmentIterator.hasNext();) {\n+        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n-      if(LOG.isDebugEnabled()) {\n-        LOG.debug(\"pre-assignContainers for application \"\n-        + application.getApplicationId());\n-        application.showRequests();\n+\n+      // Check queue max-capacity limit\n+      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n+          currentResourceLimits, application.getCurrentReservation(),\n+          schedulingMode)) {\n+        return NULL_ASSIGNMENT;\n       }\n       \n-      // Check if application needs more resource, skip if it doesn\u0027t need more.\n-      if (!application.hasPendingResourceRequest(resourceCalculator,\n-          node.getPartition(), clusterResource, schedulingMode)) {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n-              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n-              + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n-        }\n+      Resource userLimit \u003d\n+          computeUserLimitAndSetHeadroom(application, clusterResource,\n+              node.getPartition(), schedulingMode);\n+\n+      // Check user limit\n+      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n+          application, node.getPartition(), currentResourceLimits)) {\n         continue;\n       }\n \n-      synchronized (application) {\n-        // Check if this resource is on the blacklist\n-        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n-          continue;\n-        }\n-        \n-        // Schedule in priority order\n-        for (Priority priority : application.getPriorities()) {\n-          ResourceRequest anyRequest \u003d\n-              application.getResourceRequest(priority, ResourceRequest.ANY);\n-          if (null \u003d\u003d anyRequest) {\n-            continue;\n-          }\n-          \n-          // Required resource\n-          Resource required \u003d anyRequest.getCapability();\n+      // Try to schedule\n+      CSAssignment assignment \u003d\n+          application.assignContainers(clusterResource, node,\n+              currentResourceLimits, schedulingMode);\n \n-          // Do we need containers at this \u0027priority\u0027?\n-          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n-            continue;\n-          }\n-          \n-          // AM container allocation doesn\u0027t support non-exclusive allocation to\n-          // avoid painful of preempt an AM container\n-          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n-            RMAppAttempt rmAppAttempt \u003d\n-                csContext.getRMContext().getRMApps()\n-                    .get(application.getApplicationId()).getCurrentAppAttempt();\n-            if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n-                \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n-              if (LOG.isDebugEnabled()) {\n-                LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n-                    + application.getApplicationAttemptId()\n-                    + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n-              }\n-              break;\n-            }\n-          }\n-          \n-          // Is the node-label-expression of this offswitch resource request\n-          // matches the node\u0027s label?\n-          // If not match, jump to next priority.\n-          if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n-              anyRequest, node.getPartition(), schedulingMode)) {\n-            continue;\n-          }\n-          \n-          if (!this.reservationsContinueLooking) {\n-            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n-              if (LOG.isDebugEnabled()) {\n-                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n-              }\n-              continue;\n-            }\n-          }\n-          \n-          // Compute user-limit \u0026 set headroom\n-          // Note: We compute both user-limit \u0026 headroom with the highest \n-          //       priority request as the target. \n-          //       This works since we never assign lower priority requests\n-          //       before all higher priority ones are serviced.\n-          Resource userLimit \u003d \n-              computeUserLimitAndSetHeadroom(application, clusterResource, \n-                  required, node.getPartition(), schedulingMode);\n-\n-          // Check queue max-capacity limit\n-          if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n-              currentResourceLimits, required,\n-              application.getCurrentReservation(), schedulingMode)) {\n-            return NULL_ASSIGNMENT;\n-          }\n-\n-          // Check user limit\n-          if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n-              application, node.getPartition(), currentResourceLimits)) {\n-            break;\n-          }\n-\n-          // Inform the application it is about to get a scheduling opportunity\n-          application.addSchedulingOpportunity(priority);\n-          \n-          // Increase missed-non-partitioned-resource-request-opportunity.\n-          // This is to make sure non-partitioned-resource-request will prefer\n-          // to be allocated to non-partitioned nodes\n-          int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n-          if (anyRequest.getNodeLabelExpression().equals(\n-              RMNodeLabelsManager.NO_LABEL)) {\n-            missedNonPartitionedRequestSchedulingOpportunity \u003d\n-                application\n-                    .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n-          }\n-          \n-          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n-            // Before doing allocation, we need to check scheduling opportunity to\n-            // make sure : non-partitioned resource request should be scheduled to\n-            // non-partitioned partition first.\n-            if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n-                .getNumClusterNodes()) {\n-              if (LOG.isDebugEnabled()) {\n-                LOG.debug(\"Skip app_attempt\u003d\"\n-                    + application.getApplicationAttemptId()\n-                    + \" priority\u003d\"\n-                    + priority\n-                    + \" because missed-non-partitioned-resource-request\"\n-                    + \" opportunity under requred:\"\n-                    + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n-                    + \" required\u003d\"\n-                    + scheduler.getNumClusterNodes());\n-              }\n-\n-              break;\n-            }\n-          }\n-          \n-          // Try to schedule\n-          CSAssignment assignment \u003d\n-            assignContainersOnNode(clusterResource, node, application, priority,\n-                null, schedulingMode, currentResourceLimits);\n-\n-          // Did the application skip this node?\n-          if (assignment.getSkipped()) {\n-            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n-            application.subtractSchedulingOpportunity(priority);\n-            continue;\n-          }\n-          \n-          // Did we schedule or reserve a container?\n-          Resource assigned \u003d assignment.getResource();\n-          if (Resources.greaterThan(\n-              resourceCalculator, clusterResource, assigned, Resources.none())) {\n-            // Get reserved or allocated container from application\n-            RMContainer reservedOrAllocatedRMContainer \u003d\n-                application.getRMContainer(assignment\n-                    .getAssignmentInformation()\n-                    .getFirstAllocatedOrReservedContainerId());\n-\n-            // Book-keeping \n-            // Note: Update headroom to account for current allocation too...\n-            allocateResource(clusterResource, application, assigned,\n-                node.getPartition(), reservedOrAllocatedRMContainer);\n-            \n-            // Don\u0027t reset scheduling opportunities for offswitch assignments\n-            // otherwise the app will be delayed for each non-local assignment.\n-            // This helps apps with many off-cluster requests schedule faster.\n-            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n-              if (LOG.isDebugEnabled()) {\n-                LOG.debug(\"Resetting scheduling opportunities\");\n-              }\n-              application.resetSchedulingOpportunities(priority);\n-            }\n-            // Non-exclusive scheduling opportunity is different: we need reset\n-            // it every time to make sure non-labeled resource request will be\n-            // most likely allocated on non-labeled nodes first. \n-            application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n-            \n-            // Done\n-            return assignment;\n-          } else {\n-            // Do not assign out of order w.r.t priorities\n-            break;\n-          }\n-        }\n-      }\n-\n-      if(LOG.isDebugEnabled()) {\n+      if (LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n-          + application.getApplicationId());\n+            + application.getApplicationId());\n+        application.showRequests();\n       }\n-      application.showRequests();\n-    }\n-  \n-    return NULL_ASSIGNMENT;\n \n+      // Did we schedule or reserve a container?\n+      Resource assigned \u003d assignment.getResource();\n+      \n+      handleExcessReservedContainer(clusterResource, assignment);\n+\n+      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n+          Resources.none())) {\n+        // Get reserved or allocated container from application\n+        RMContainer reservedOrAllocatedRMContainer \u003d\n+            application.getRMContainer(assignment.getAssignmentInformation()\n+                .getFirstAllocatedOrReservedContainerId());\n+\n+        // Book-keeping\n+        // Note: Update headroom to account for current allocation too...\n+        allocateResource(clusterResource, application, assigned,\n+            node.getPartition(), reservedOrAllocatedRMContainer);\n+\n+        // Done\n+        return assignment;\n+      } else if (!assignment.getSkipped()) {\n+        // If we don\u0027t allocate anything, and it is not skipped by application,\n+        // we will return to respect FIFO of applications\n+        return NULL_ASSIGNMENT;\n+      }\n+    }\n+\n+    return NULL_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" #applications\u003d\" + orderingPolicy.getNumSchedulableEntities());\n    }\n\n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d\n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        CSAssignment assignment \u003d application.assignReservedContainer(node, reservedContainer,\n            clusterResource, schedulingMode);\n        handleExcessReservedContainer(clusterResource, assignment);\n        return assignment;\n      }\n    }\n\n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return NULL_ASSIGNMENT;\n    }\n\n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(), clusterResource,\n        schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return NULL_ASSIGNMENT;\n    }\n\n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator(); assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n\n      // Check queue max-capacity limit\n      if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n          currentResourceLimits, application.getCurrentReservation(),\n          schedulingMode)) {\n        return NULL_ASSIGNMENT;\n      }\n      \n      Resource userLimit \u003d\n          computeUserLimitAndSetHeadroom(application, clusterResource,\n              node.getPartition(), schedulingMode);\n\n      // Check user limit\n      if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n          application, node.getPartition(), currentResourceLimits)) {\n        continue;\n      }\n\n      // Try to schedule\n      CSAssignment assignment \u003d\n          application.assignContainers(clusterResource, node,\n              currentResourceLimits, schedulingMode);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n            + application.getApplicationId());\n        application.showRequests();\n      }\n\n      // Did we schedule or reserve a container?\n      Resource assigned \u003d assignment.getResource();\n      \n      handleExcessReservedContainer(clusterResource, assignment);\n\n      if (Resources.greaterThan(resourceCalculator, clusterResource, assigned,\n          Resources.none())) {\n        // Get reserved or allocated container from application\n        RMContainer reservedOrAllocatedRMContainer \u003d\n            application.getRMContainer(assignment.getAssignmentInformation()\n                .getFirstAllocatedOrReservedContainerId());\n\n        // Book-keeping\n        // Note: Update headroom to account for current allocation too...\n        allocateResource(clusterResource, application, assigned,\n            node.getPartition(), reservedOrAllocatedRMContainer);\n\n        // Done\n        return assignment;\n      } else if (!assignment.getSkipped()) {\n        // If we don\u0027t allocate anything, and it is not skipped by application,\n        // we will return to respect FIFO of applications\n        return NULL_ASSIGNMENT;\n      }\n    }\n\n    return NULL_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "1ea36299a47af302379ae0750b571ec021eb54ad": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3116. RM notifies NM whether a container is an AM container or normal task container. Contributed by Giovanni Matteo Fumarola.\n",
      "commitDate": "10/07/15 6:58 PM",
      "commitName": "1ea36299a47af302379ae0750b571ec021eb54ad",
      "commitAuthor": "Zhijie Shen",
      "commitDateOld": "27/06/15 9:34 PM",
      "commitNameOld": "b543d1a390a67e5e92fea67d3a2635058c29e9da",
      "commitAuthorOld": "Devaraj K",
      "daysBetweenCommits": 12.89,
      "commitsBetweenForRepo": 95,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,234 +1,235 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + \n         orderingPolicy.getNumSchedulableEntities());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n             clusterResource, schedulingMode);\n       }\n     }\n     \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return NULL_ASSIGNMENT;\n     }\n     \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(),\n         clusterResource, schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return NULL_ASSIGNMENT;\n     }\n     \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n       \n       // Check if application needs more resource, skip if it doesn\u0027t need more.\n       if (!application.hasPendingResourceRequest(resourceCalculator,\n           node.getPartition(), clusterResource, schedulingMode)) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n               + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n               + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n         }\n         continue;\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           \n           // AM container allocation doesn\u0027t support non-exclusive allocation to\n           // avoid painful of preempt an AM container\n           if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n             RMAppAttempt rmAppAttempt \u003d\n                 csContext.getRMContext().getRMApps()\n                     .get(application.getApplicationId()).getCurrentAppAttempt();\n-            if (null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n+            if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n+                \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                     + application.getApplicationAttemptId()\n                     + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n               }\n               break;\n             }\n           }\n           \n           // Is the node-label-expression of this offswitch resource request\n           // matches the node\u0027s label?\n           // If not match, jump to next priority.\n           if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n               anyRequest, node.getPartition(), schedulingMode)) {\n             continue;\n           }\n           \n           if (!this.reservationsContinueLooking) {\n             if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required, node.getPartition(), schedulingMode);\n \n           // Check queue max-capacity limit\n           if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n               currentResourceLimits, required,\n               application.getCurrentReservation(), schedulingMode)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n               application, node.getPartition(), currentResourceLimits)) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Increase missed-non-partitioned-resource-request-opportunity.\n           // This is to make sure non-partitioned-resource-request will prefer\n           // to be allocated to non-partitioned nodes\n           int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n           if (anyRequest.getNodeLabelExpression().equals(\n               RMNodeLabelsManager.NO_LABEL)) {\n             missedNonPartitionedRequestSchedulingOpportunity \u003d\n                 application\n                     .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n           }\n           \n           if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n             // Before doing allocation, we need to check scheduling opportunity to\n             // make sure : non-partitioned resource request should be scheduled to\n             // non-partitioned partition first.\n             if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n                 .getNumClusterNodes()) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Skip app_attempt\u003d\"\n                     + application.getApplicationAttemptId()\n                     + \" priority\u003d\"\n                     + priority\n                     + \" because missed-non-partitioned-resource-request\"\n                     + \" opportunity under requred:\"\n                     + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n                     + \" required\u003d\"\n                     + scheduler.getNumClusterNodes());\n               }\n \n               break;\n             }\n           }\n           \n           // Try to schedule\n           CSAssignment assignment \u003d\n             assignContainersOnNode(clusterResource, node, application, priority,\n                 null, schedulingMode, currentResourceLimits);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n             // Get reserved or allocated container from application\n             RMContainer reservedOrAllocatedRMContainer \u003d\n                 application.getRMContainer(assignment\n                     .getAssignmentInformation()\n                     .getFirstAllocatedOrReservedContainerId());\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n                 node.getPartition(), reservedOrAllocatedRMContainer);\n             \n             // Don\u0027t reset scheduling opportunities for offswitch assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n             // Non-exclusive scheduling opportunity is different: we need reset\n             // it every time to make sure non-labeled resource request will be\n             // most likely allocated on non-labeled nodes first. \n             application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + \n        orderingPolicy.getNumSchedulableEntities());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource, schedulingMode);\n      }\n    }\n    \n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(),\n        clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return NULL_ASSIGNMENT;\n    }\n    \n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator();\n        assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n      \n      // Check if application needs more resource, skip if it doesn\u0027t need more.\n      if (!application.hasPendingResourceRequest(resourceCalculator,\n          node.getPartition(), clusterResource, schedulingMode)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n              + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n        }\n        continue;\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          \n          // AM container allocation doesn\u0027t support non-exclusive allocation to\n          // avoid painful of preempt an AM container\n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            RMAppAttempt rmAppAttempt \u003d\n                csContext.getRMContext().getRMApps()\n                    .get(application.getApplicationId()).getCurrentAppAttempt();\n            if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n                \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n              }\n              break;\n            }\n          }\n          \n          // Is the node-label-expression of this offswitch resource request\n          // matches the node\u0027s label?\n          // If not match, jump to next priority.\n          if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n              anyRequest, node.getPartition(), schedulingMode)) {\n            continue;\n          }\n          \n          if (!this.reservationsContinueLooking) {\n            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, node.getPartition(), schedulingMode);\n\n          // Check queue max-capacity limit\n          if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n              currentResourceLimits, required,\n              application.getCurrentReservation(), schedulingMode)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n              application, node.getPartition(), currentResourceLimits)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Increase missed-non-partitioned-resource-request-opportunity.\n          // This is to make sure non-partitioned-resource-request will prefer\n          // to be allocated to non-partitioned nodes\n          int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n          if (anyRequest.getNodeLabelExpression().equals(\n              RMNodeLabelsManager.NO_LABEL)) {\n            missedNonPartitionedRequestSchedulingOpportunity \u003d\n                application\n                    .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n          }\n          \n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            // Before doing allocation, we need to check scheduling opportunity to\n            // make sure : non-partitioned resource request should be scheduled to\n            // non-partitioned partition first.\n            if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n                .getNumClusterNodes()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \" priority\u003d\"\n                    + priority\n                    + \" because missed-non-partitioned-resource-request\"\n                    + \" opportunity under requred:\"\n                    + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n                    + \" required\u003d\"\n                    + scheduler.getNumClusterNodes());\n              }\n\n              break;\n            }\n          }\n          \n          // Try to schedule\n          CSAssignment assignment \u003d\n            assignContainersOnNode(clusterResource, node, application, priority,\n                null, schedulingMode, currentResourceLimits);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n            // Get reserved or allocated container from application\n            RMContainer reservedOrAllocatedRMContainer \u003d\n                application.getRMContainer(assignment\n                    .getAssignmentInformation()\n                    .getFirstAllocatedOrReservedContainerId());\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getPartition(), reservedOrAllocatedRMContainer);\n            \n            // Don\u0027t reset scheduling opportunities for offswitch assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            // Non-exclusive scheduling opportunity is different: we need reset\n            // it every time to make sure non-labeled resource request will be\n            // most likely allocated on non-labeled nodes first. \n            application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "d497f6ea2be559aa31ed76f37ae949dbfabe2a51": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2498. Respect labels in preemption policy of capacity scheduler for inter-queue preemption. Contributed by Wangda Tan\n",
      "commitDate": "24/04/15 5:03 PM",
      "commitName": "d497f6ea2be559aa31ed76f37ae949dbfabe2a51",
      "commitAuthor": "Jian He",
      "commitDateOld": "23/04/15 7:39 AM",
      "commitNameOld": "189a63a719c63b67a1783a280bfc2f72dcb55277",
      "commitAuthorOld": "tgraves",
      "daysBetweenCommits": 1.39,
      "commitsBetweenForRepo": 28,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,229 +1,234 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + \n         orderingPolicy.getNumSchedulableEntities());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n             clusterResource, schedulingMode);\n       }\n     }\n     \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return NULL_ASSIGNMENT;\n     }\n     \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(),\n         clusterResource, schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return NULL_ASSIGNMENT;\n     }\n     \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n       \n       // Check if application needs more resource, skip if it doesn\u0027t need more.\n       if (!application.hasPendingResourceRequest(resourceCalculator,\n           node.getPartition(), clusterResource, schedulingMode)) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n               + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n               + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n         }\n         continue;\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           \n           // AM container allocation doesn\u0027t support non-exclusive allocation to\n           // avoid painful of preempt an AM container\n           if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n             RMAppAttempt rmAppAttempt \u003d\n                 csContext.getRMContext().getRMApps()\n                     .get(application.getApplicationId()).getCurrentAppAttempt();\n             if (null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                     + application.getApplicationAttemptId()\n                     + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n               }\n               break;\n             }\n           }\n           \n           // Is the node-label-expression of this offswitch resource request\n           // matches the node\u0027s label?\n           // If not match, jump to next priority.\n           if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n               anyRequest, node.getPartition(), schedulingMode)) {\n             continue;\n           }\n           \n           if (!this.reservationsContinueLooking) {\n             if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required, node.getPartition(), schedulingMode);\n \n           // Check queue max-capacity limit\n           if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n               currentResourceLimits, required,\n               application.getCurrentReservation(), schedulingMode)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n               application, node.getPartition(), currentResourceLimits)) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Increase missed-non-partitioned-resource-request-opportunity.\n           // This is to make sure non-partitioned-resource-request will prefer\n           // to be allocated to non-partitioned nodes\n           int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n           if (anyRequest.getNodeLabelExpression().equals(\n               RMNodeLabelsManager.NO_LABEL)) {\n             missedNonPartitionedRequestSchedulingOpportunity \u003d\n                 application\n                     .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n           }\n           \n           if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n             // Before doing allocation, we need to check scheduling opportunity to\n             // make sure : non-partitioned resource request should be scheduled to\n             // non-partitioned partition first.\n             if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n                 .getNumClusterNodes()) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Skip app_attempt\u003d\"\n                     + application.getApplicationAttemptId()\n                     + \" priority\u003d\"\n                     + priority\n                     + \" because missed-non-partitioned-resource-request\"\n                     + \" opportunity under requred:\"\n                     + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n                     + \" required\u003d\"\n                     + scheduler.getNumClusterNodes());\n               }\n \n               break;\n             }\n           }\n           \n           // Try to schedule\n           CSAssignment assignment \u003d\n             assignContainersOnNode(clusterResource, node, application, priority,\n                 null, schedulingMode, currentResourceLimits);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n+            // Get reserved or allocated container from application\n+            RMContainer reservedOrAllocatedRMContainer \u003d\n+                application.getRMContainer(assignment\n+                    .getAssignmentInformation()\n+                    .getFirstAllocatedOrReservedContainerId());\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n-                node.getPartition());\n+                node.getPartition(), reservedOrAllocatedRMContainer);\n             \n             // Don\u0027t reset scheduling opportunities for offswitch assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n             // Non-exclusive scheduling opportunity is different: we need reset\n             // it every time to make sure non-labeled resource request will be\n             // most likely allocated on non-labeled nodes first. \n             application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + \n        orderingPolicy.getNumSchedulableEntities());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource, schedulingMode);\n      }\n    }\n    \n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(),\n        clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return NULL_ASSIGNMENT;\n    }\n    \n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator();\n        assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n      \n      // Check if application needs more resource, skip if it doesn\u0027t need more.\n      if (!application.hasPendingResourceRequest(resourceCalculator,\n          node.getPartition(), clusterResource, schedulingMode)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n              + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n        }\n        continue;\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          \n          // AM container allocation doesn\u0027t support non-exclusive allocation to\n          // avoid painful of preempt an AM container\n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            RMAppAttempt rmAppAttempt \u003d\n                csContext.getRMContext().getRMApps()\n                    .get(application.getApplicationId()).getCurrentAppAttempt();\n            if (null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n              }\n              break;\n            }\n          }\n          \n          // Is the node-label-expression of this offswitch resource request\n          // matches the node\u0027s label?\n          // If not match, jump to next priority.\n          if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n              anyRequest, node.getPartition(), schedulingMode)) {\n            continue;\n          }\n          \n          if (!this.reservationsContinueLooking) {\n            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, node.getPartition(), schedulingMode);\n\n          // Check queue max-capacity limit\n          if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n              currentResourceLimits, required,\n              application.getCurrentReservation(), schedulingMode)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n              application, node.getPartition(), currentResourceLimits)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Increase missed-non-partitioned-resource-request-opportunity.\n          // This is to make sure non-partitioned-resource-request will prefer\n          // to be allocated to non-partitioned nodes\n          int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n          if (anyRequest.getNodeLabelExpression().equals(\n              RMNodeLabelsManager.NO_LABEL)) {\n            missedNonPartitionedRequestSchedulingOpportunity \u003d\n                application\n                    .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n          }\n          \n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            // Before doing allocation, we need to check scheduling opportunity to\n            // make sure : non-partitioned resource request should be scheduled to\n            // non-partitioned partition first.\n            if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n                .getNumClusterNodes()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \" priority\u003d\"\n                    + priority\n                    + \" because missed-non-partitioned-resource-request\"\n                    + \" opportunity under requred:\"\n                    + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n                    + \" required\u003d\"\n                    + scheduler.getNumClusterNodes());\n              }\n\n              break;\n            }\n          }\n          \n          // Try to schedule\n          CSAssignment assignment \u003d\n            assignContainersOnNode(clusterResource, node, application, priority,\n                null, schedulingMode, currentResourceLimits);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n            // Get reserved or allocated container from application\n            RMContainer reservedOrAllocatedRMContainer \u003d\n                application.getRMContainer(assignment\n                    .getAssignmentInformation()\n                    .getFirstAllocatedOrReservedContainerId());\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getPartition(), reservedOrAllocatedRMContainer);\n            \n            // Don\u0027t reset scheduling opportunities for offswitch assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            // Non-exclusive scheduling opportunity is different: we need reset\n            // it every time to make sure non-labeled resource request will be\n            // most likely allocated on non-labeled nodes first. \n            application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "189a63a719c63b67a1783a280bfc2f72dcb55277": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3434. Interaction between reservations and userlimit can result in significant ULF violation\n",
      "commitDate": "23/04/15 7:39 AM",
      "commitName": "189a63a719c63b67a1783a280bfc2f72dcb55277",
      "commitAuthor": "tgraves",
      "commitDateOld": "21/04/15 8:06 PM",
      "commitNameOld": "bdd90110e6904b59746812d9a093924a65e72280",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 1.48,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,229 +1,229 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + \n         orderingPolicy.getNumSchedulableEntities());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n             clusterResource, schedulingMode);\n       }\n     }\n     \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return NULL_ASSIGNMENT;\n     }\n     \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(),\n         clusterResource, schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return NULL_ASSIGNMENT;\n     }\n     \n     for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n         orderingPolicy.getAssignmentIterator();\n         assignmentIterator.hasNext();) {\n       FiCaSchedulerApp application \u003d assignmentIterator.next();\n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n       \n       // Check if application needs more resource, skip if it doesn\u0027t need more.\n       if (!application.hasPendingResourceRequest(resourceCalculator,\n           node.getPartition(), clusterResource, schedulingMode)) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n               + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n               + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n         }\n         continue;\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           \n           // AM container allocation doesn\u0027t support non-exclusive allocation to\n           // avoid painful of preempt an AM container\n           if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n             RMAppAttempt rmAppAttempt \u003d\n                 csContext.getRMContext().getRMApps()\n                     .get(application.getApplicationId()).getCurrentAppAttempt();\n             if (null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                     + application.getApplicationAttemptId()\n                     + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n               }\n               break;\n             }\n           }\n           \n           // Is the node-label-expression of this offswitch resource request\n           // matches the node\u0027s label?\n           // If not match, jump to next priority.\n           if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n               anyRequest, node.getPartition(), schedulingMode)) {\n             continue;\n           }\n           \n           if (!this.reservationsContinueLooking) {\n             if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n-                  required, node.getPartition(), schedulingMode);          \n-          \n+                  required, node.getPartition(), schedulingMode);\n+\n           // Check queue max-capacity limit\n           if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n-              this.currentResourceLimits, required,\n+              currentResourceLimits, required,\n               application.getCurrentReservation(), schedulingMode)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n-              application, true, node.getPartition())) {\n+              application, node.getPartition(), currentResourceLimits)) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Increase missed-non-partitioned-resource-request-opportunity.\n           // This is to make sure non-partitioned-resource-request will prefer\n           // to be allocated to non-partitioned nodes\n           int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n           if (anyRequest.getNodeLabelExpression().equals(\n               RMNodeLabelsManager.NO_LABEL)) {\n             missedNonPartitionedRequestSchedulingOpportunity \u003d\n                 application\n                     .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n           }\n           \n           if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n             // Before doing allocation, we need to check scheduling opportunity to\n             // make sure : non-partitioned resource request should be scheduled to\n             // non-partitioned partition first.\n             if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n                 .getNumClusterNodes()) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Skip app_attempt\u003d\"\n                     + application.getApplicationAttemptId()\n                     + \" priority\u003d\"\n                     + priority\n                     + \" because missed-non-partitioned-resource-request\"\n                     + \" opportunity under requred:\"\n                     + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n                     + \" required\u003d\"\n                     + scheduler.getNumClusterNodes());\n               }\n \n               break;\n             }\n           }\n           \n           // Try to schedule\n-          CSAssignment assignment \u003d  \n-            assignContainersOnNode(clusterResource, node, application, priority, \n-                null, schedulingMode);\n+          CSAssignment assignment \u003d\n+            assignContainersOnNode(clusterResource, node, application, priority,\n+                null, schedulingMode, currentResourceLimits);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n                 node.getPartition());\n             \n             // Don\u0027t reset scheduling opportunities for offswitch assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n             // Non-exclusive scheduling opportunity is different: we need reset\n             // it every time to make sure non-labeled resource request will be\n             // most likely allocated on non-labeled nodes first. \n             application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + \n        orderingPolicy.getNumSchedulableEntities());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource, schedulingMode);\n      }\n    }\n    \n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(),\n        clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return NULL_ASSIGNMENT;\n    }\n    \n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator();\n        assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n      \n      // Check if application needs more resource, skip if it doesn\u0027t need more.\n      if (!application.hasPendingResourceRequest(resourceCalculator,\n          node.getPartition(), clusterResource, schedulingMode)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n              + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n        }\n        continue;\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          \n          // AM container allocation doesn\u0027t support non-exclusive allocation to\n          // avoid painful of preempt an AM container\n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            RMAppAttempt rmAppAttempt \u003d\n                csContext.getRMContext().getRMApps()\n                    .get(application.getApplicationId()).getCurrentAppAttempt();\n            if (null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n              }\n              break;\n            }\n          }\n          \n          // Is the node-label-expression of this offswitch resource request\n          // matches the node\u0027s label?\n          // If not match, jump to next priority.\n          if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n              anyRequest, node.getPartition(), schedulingMode)) {\n            continue;\n          }\n          \n          if (!this.reservationsContinueLooking) {\n            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, node.getPartition(), schedulingMode);\n\n          // Check queue max-capacity limit\n          if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n              currentResourceLimits, required,\n              application.getCurrentReservation(), schedulingMode)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n              application, node.getPartition(), currentResourceLimits)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Increase missed-non-partitioned-resource-request-opportunity.\n          // This is to make sure non-partitioned-resource-request will prefer\n          // to be allocated to non-partitioned nodes\n          int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n          if (anyRequest.getNodeLabelExpression().equals(\n              RMNodeLabelsManager.NO_LABEL)) {\n            missedNonPartitionedRequestSchedulingOpportunity \u003d\n                application\n                    .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n          }\n          \n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            // Before doing allocation, we need to check scheduling opportunity to\n            // make sure : non-partitioned resource request should be scheduled to\n            // non-partitioned partition first.\n            if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n                .getNumClusterNodes()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \" priority\u003d\"\n                    + priority\n                    + \" because missed-non-partitioned-resource-request\"\n                    + \" opportunity under requred:\"\n                    + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n                    + \" required\u003d\"\n                    + scheduler.getNumClusterNodes());\n              }\n\n              break;\n            }\n          }\n          \n          // Try to schedule\n          CSAssignment assignment \u003d\n            assignContainersOnNode(clusterResource, node, application, priority,\n                null, schedulingMode, currentResourceLimits);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getPartition());\n            \n            // Don\u0027t reset scheduling opportunities for offswitch assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            // Non-exclusive scheduling opportunity is different: we need reset\n            // it every time to make sure non-labeled resource request will be\n            // most likely allocated on non-labeled nodes first. \n            application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3463. Integrate OrderingPolicy Framework with CapacityScheduler. (Craig Welch via wangda)\n",
      "commitDate": "20/04/15 5:12 PM",
      "commitName": "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "17/04/15 1:36 PM",
      "commitNameOld": "d573f09fb93dbb711d504620af5d73840ea063a6",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 3.15,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,227 +1,229 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" #applications\u003d\" + activeApplications.size());\n+        + \" #applications\u003d\" + \n+        orderingPolicy.getNumSchedulableEntities());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n             clusterResource, schedulingMode);\n       }\n     }\n     \n     // if our queue cannot access this node, just return\n     if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n         \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n       return NULL_ASSIGNMENT;\n     }\n     \n     // Check if this queue need more resource, simply skip allocation if this\n     // queue doesn\u0027t need more resources.\n     if (!hasPendingResourceRequest(node.getPartition(),\n         clusterResource, schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n       }\n       return NULL_ASSIGNMENT;\n     }\n     \n-    // Try to assign containers to applications in order\n-    for (FiCaSchedulerApp application : activeApplications) {\n-      \n+    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n+        orderingPolicy.getAssignmentIterator();\n+        assignmentIterator.hasNext();) {\n+      FiCaSchedulerApp application \u003d assignmentIterator.next();\n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n       \n       // Check if application needs more resource, skip if it doesn\u0027t need more.\n       if (!application.hasPendingResourceRequest(resourceCalculator,\n           node.getPartition(), clusterResource, schedulingMode)) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n               + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n               + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n         }\n         continue;\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           \n           // AM container allocation doesn\u0027t support non-exclusive allocation to\n           // avoid painful of preempt an AM container\n           if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n             RMAppAttempt rmAppAttempt \u003d\n                 csContext.getRMContext().getRMApps()\n                     .get(application.getApplicationId()).getCurrentAppAttempt();\n             if (null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                     + application.getApplicationAttemptId()\n                     + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n               }\n               break;\n             }\n           }\n           \n           // Is the node-label-expression of this offswitch resource request\n           // matches the node\u0027s label?\n           // If not match, jump to next priority.\n           if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n               anyRequest, node.getPartition(), schedulingMode)) {\n             continue;\n           }\n           \n           if (!this.reservationsContinueLooking) {\n             if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required, node.getPartition(), schedulingMode);          \n           \n           // Check queue max-capacity limit\n           if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n               this.currentResourceLimits, required,\n               application.getCurrentReservation(), schedulingMode)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n               application, true, node.getPartition())) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Increase missed-non-partitioned-resource-request-opportunity.\n           // This is to make sure non-partitioned-resource-request will prefer\n           // to be allocated to non-partitioned nodes\n           int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n           if (anyRequest.getNodeLabelExpression().equals(\n               RMNodeLabelsManager.NO_LABEL)) {\n             missedNonPartitionedRequestSchedulingOpportunity \u003d\n                 application\n                     .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n           }\n           \n           if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n             // Before doing allocation, we need to check scheduling opportunity to\n             // make sure : non-partitioned resource request should be scheduled to\n             // non-partitioned partition first.\n             if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n                 .getNumClusterNodes()) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Skip app_attempt\u003d\"\n                     + application.getApplicationAttemptId()\n                     + \" priority\u003d\"\n                     + priority\n                     + \" because missed-non-partitioned-resource-request\"\n                     + \" opportunity under requred:\"\n                     + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n                     + \" required\u003d\"\n                     + scheduler.getNumClusterNodes());\n               }\n \n               break;\n             }\n           }\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null, schedulingMode);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n                 node.getPartition());\n             \n             // Don\u0027t reset scheduling opportunities for offswitch assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n             // Non-exclusive scheduling opportunity is different: we need reset\n             // it every time to make sure non-labeled resource request will be\n             // most likely allocated on non-labeled nodes first. \n             application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + \n        orderingPolicy.getNumSchedulableEntities());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource, schedulingMode);\n      }\n    }\n    \n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(),\n        clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return NULL_ASSIGNMENT;\n    }\n    \n    for (Iterator\u003cFiCaSchedulerApp\u003e assignmentIterator \u003d\n        orderingPolicy.getAssignmentIterator();\n        assignmentIterator.hasNext();) {\n      FiCaSchedulerApp application \u003d assignmentIterator.next();\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n      \n      // Check if application needs more resource, skip if it doesn\u0027t need more.\n      if (!application.hasPendingResourceRequest(resourceCalculator,\n          node.getPartition(), clusterResource, schedulingMode)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n              + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n        }\n        continue;\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          \n          // AM container allocation doesn\u0027t support non-exclusive allocation to\n          // avoid painful of preempt an AM container\n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            RMAppAttempt rmAppAttempt \u003d\n                csContext.getRMContext().getRMApps()\n                    .get(application.getApplicationId()).getCurrentAppAttempt();\n            if (null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n              }\n              break;\n            }\n          }\n          \n          // Is the node-label-expression of this offswitch resource request\n          // matches the node\u0027s label?\n          // If not match, jump to next priority.\n          if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n              anyRequest, node.getPartition(), schedulingMode)) {\n            continue;\n          }\n          \n          if (!this.reservationsContinueLooking) {\n            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, node.getPartition(), schedulingMode);          \n          \n          // Check queue max-capacity limit\n          if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n              this.currentResourceLimits, required,\n              application.getCurrentReservation(), schedulingMode)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n              application, true, node.getPartition())) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Increase missed-non-partitioned-resource-request-opportunity.\n          // This is to make sure non-partitioned-resource-request will prefer\n          // to be allocated to non-partitioned nodes\n          int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n          if (anyRequest.getNodeLabelExpression().equals(\n              RMNodeLabelsManager.NO_LABEL)) {\n            missedNonPartitionedRequestSchedulingOpportunity \u003d\n                application\n                    .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n          }\n          \n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            // Before doing allocation, we need to check scheduling opportunity to\n            // make sure : non-partitioned resource request should be scheduled to\n            // non-partitioned partition first.\n            if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n                .getNumClusterNodes()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \" priority\u003d\"\n                    + priority\n                    + \" because missed-non-partitioned-resource-request\"\n                    + \" opportunity under requred:\"\n                    + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n                    + \" required\u003d\"\n                    + scheduler.getNumClusterNodes());\n              }\n\n              break;\n            }\n          }\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null, schedulingMode);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getPartition());\n            \n            // Don\u0027t reset scheduling opportunities for offswitch assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            // Non-exclusive scheduling opportunity is different: we need reset\n            // it every time to make sure non-labeled resource request will be\n            // most likely allocated on non-labeled nodes first. \n            application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3361. CapacityScheduler side changes to support non-exclusive node labels. Contributed by Wangda Tan\n",
      "commitDate": "14/04/15 11:45 AM",
      "commitName": "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3361. CapacityScheduler side changes to support non-exclusive node labels. Contributed by Wangda Tan\n",
          "commitDate": "14/04/15 11:45 AM",
          "commitName": "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
          "commitAuthor": "Jian He",
          "commitDateOld": "09/04/15 11:38 PM",
          "commitNameOld": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
          "commitAuthorOld": "Xuan",
          "daysBetweenCommits": 4.51,
          "commitsBetweenForRepo": 30,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,152 +1,227 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n-      FiCaSchedulerNode node, ResourceLimits currentResourceLimits) {\n+      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n+      SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n-    // if our queue cannot access this node, just return\n-    if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n-        node.getLabels())) {\n-      return NULL_ASSIGNMENT;\n-    }\n-    \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n-            clusterResource);\n+            clusterResource, schedulingMode);\n       }\n     }\n     \n+    // if our queue cannot access this node, just return\n+    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n+        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n+      return NULL_ASSIGNMENT;\n+    }\n+    \n+    // Check if this queue need more resource, simply skip allocation if this\n+    // queue doesn\u0027t need more resources.\n+    if (!hasPendingResourceRequest(node.getPartition(),\n+        clusterResource, schedulingMode)) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n+            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n+      }\n+      return NULL_ASSIGNMENT;\n+    }\n+    \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n+      \n+      // Check if application needs more resource, skip if it doesn\u0027t need more.\n+      if (!application.hasPendingResourceRequest(resourceCalculator,\n+          node.getPartition(), clusterResource, schedulingMode)) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n+              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+              + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n+        }\n+        continue;\n+      }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           \n+          // AM container allocation doesn\u0027t support non-exclusive allocation to\n+          // avoid painful of preempt an AM container\n+          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n+            RMAppAttempt rmAppAttempt \u003d\n+                csContext.getRMContext().getRMApps()\n+                    .get(application.getApplicationId()).getCurrentAppAttempt();\n+            if (null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n+                    + application.getApplicationAttemptId()\n+                    + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n+              }\n+              break;\n+            }\n+          }\n+          \n           // Is the node-label-expression of this offswitch resource request\n           // matches the node\u0027s label?\n           // If not match, jump to next priority.\n-          if (!checkResourceRequestMatchingNodeLabel(anyRequest, node)) {\n+          if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n+              anyRequest, node.getPartition(), schedulingMode)) {\n             continue;\n           }\n           \n           if (!this.reservationsContinueLooking) {\n             if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n-          Set\u003cString\u003e requestedNodeLabels \u003d\n-              getRequestLabelSetByExpression(anyRequest\n-                  .getNodeLabelExpression());\n-\n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n-                  required, requestedNodeLabels);          \n+                  required, node.getPartition(), schedulingMode);          \n           \n           // Check queue max-capacity limit\n-          if (!super.canAssignToThisQueue(clusterResource, node.getLabels(),\n-              this.currentResourceLimits, required, application.getCurrentReservation())) {\n+          if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n+              this.currentResourceLimits, required,\n+              application.getCurrentReservation(), schedulingMode)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n-              application, true, requestedNodeLabels)) {\n+              application, true, node.getPartition())) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n+          // Increase missed-non-partitioned-resource-request-opportunity.\n+          // This is to make sure non-partitioned-resource-request will prefer\n+          // to be allocated to non-partitioned nodes\n+          int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n+          if (anyRequest.getNodeLabelExpression().equals(\n+              RMNodeLabelsManager.NO_LABEL)) {\n+            missedNonPartitionedRequestSchedulingOpportunity \u003d\n+                application\n+                    .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+          }\n+          \n+          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n+            // Before doing allocation, we need to check scheduling opportunity to\n+            // make sure : non-partitioned resource request should be scheduled to\n+            // non-partitioned partition first.\n+            if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n+                .getNumClusterNodes()) {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"Skip app_attempt\u003d\"\n+                    + application.getApplicationAttemptId()\n+                    + \" priority\u003d\"\n+                    + priority\n+                    + \" because missed-non-partitioned-resource-request\"\n+                    + \" opportunity under requred:\"\n+                    + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n+                    + \" required\u003d\"\n+                    + scheduler.getNumClusterNodes());\n+              }\n+\n+              break;\n+            }\n+          }\n+          \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n-                null);\n+                null, schedulingMode);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n-                node.getLabels());\n+                node.getPartition());\n             \n-            // Don\u0027t reset scheduling opportunities for non-local assignments\n+            // Don\u0027t reset scheduling opportunities for offswitch assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n+            // Non-exclusive scheduling opportunity is different: we need reset\n+            // it every time to make sure non-labeled resource request will be\n+            // most likely allocated on non-labeled nodes first. \n+            application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource, schedulingMode);\n      }\n    }\n    \n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(),\n        clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n      \n      // Check if application needs more resource, skip if it doesn\u0027t need more.\n      if (!application.hasPendingResourceRequest(resourceCalculator,\n          node.getPartition(), clusterResource, schedulingMode)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n              + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n        }\n        continue;\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          \n          // AM container allocation doesn\u0027t support non-exclusive allocation to\n          // avoid painful of preempt an AM container\n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            RMAppAttempt rmAppAttempt \u003d\n                csContext.getRMContext().getRMApps()\n                    .get(application.getApplicationId()).getCurrentAppAttempt();\n            if (null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n              }\n              break;\n            }\n          }\n          \n          // Is the node-label-expression of this offswitch resource request\n          // matches the node\u0027s label?\n          // If not match, jump to next priority.\n          if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n              anyRequest, node.getPartition(), schedulingMode)) {\n            continue;\n          }\n          \n          if (!this.reservationsContinueLooking) {\n            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, node.getPartition(), schedulingMode);          \n          \n          // Check queue max-capacity limit\n          if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n              this.currentResourceLimits, required,\n              application.getCurrentReservation(), schedulingMode)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n              application, true, node.getPartition())) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Increase missed-non-partitioned-resource-request-opportunity.\n          // This is to make sure non-partitioned-resource-request will prefer\n          // to be allocated to non-partitioned nodes\n          int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n          if (anyRequest.getNodeLabelExpression().equals(\n              RMNodeLabelsManager.NO_LABEL)) {\n            missedNonPartitionedRequestSchedulingOpportunity \u003d\n                application\n                    .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n          }\n          \n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            // Before doing allocation, we need to check scheduling opportunity to\n            // make sure : non-partitioned resource request should be scheduled to\n            // non-partitioned partition first.\n            if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n                .getNumClusterNodes()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \" priority\u003d\"\n                    + priority\n                    + \" because missed-non-partitioned-resource-request\"\n                    + \" opportunity under requred:\"\n                    + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n                    + \" required\u003d\"\n                    + scheduler.getNumClusterNodes());\n              }\n\n              break;\n            }\n          }\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null, schedulingMode);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getPartition());\n            \n            // Don\u0027t reset scheduling opportunities for offswitch assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            // Non-exclusive scheduling opportunity is different: we need reset\n            // it every time to make sure non-labeled resource request will be\n            // most likely allocated on non-labeled nodes first. \n            application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, currentResourceLimits-ResourceLimits]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3361. CapacityScheduler side changes to support non-exclusive node labels. Contributed by Wangda Tan\n",
          "commitDate": "14/04/15 11:45 AM",
          "commitName": "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
          "commitAuthor": "Jian He",
          "commitDateOld": "09/04/15 11:38 PM",
          "commitNameOld": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
          "commitAuthorOld": "Xuan",
          "daysBetweenCommits": 4.51,
          "commitsBetweenForRepo": 30,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,152 +1,227 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n-      FiCaSchedulerNode node, ResourceLimits currentResourceLimits) {\n+      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n+      SchedulingMode schedulingMode) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n-    // if our queue cannot access this node, just return\n-    if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n-        node.getLabels())) {\n-      return NULL_ASSIGNMENT;\n-    }\n-    \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n-            clusterResource);\n+            clusterResource, schedulingMode);\n       }\n     }\n     \n+    // if our queue cannot access this node, just return\n+    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n+        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n+      return NULL_ASSIGNMENT;\n+    }\n+    \n+    // Check if this queue need more resource, simply skip allocation if this\n+    // queue doesn\u0027t need more resources.\n+    if (!hasPendingResourceRequest(node.getPartition(),\n+        clusterResource, schedulingMode)) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n+            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n+      }\n+      return NULL_ASSIGNMENT;\n+    }\n+    \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n+      \n+      // Check if application needs more resource, skip if it doesn\u0027t need more.\n+      if (!application.hasPendingResourceRequest(resourceCalculator,\n+          node.getPartition(), clusterResource, schedulingMode)) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n+              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+              + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n+        }\n+        continue;\n+      }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           \n+          // AM container allocation doesn\u0027t support non-exclusive allocation to\n+          // avoid painful of preempt an AM container\n+          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n+            RMAppAttempt rmAppAttempt \u003d\n+                csContext.getRMContext().getRMApps()\n+                    .get(application.getApplicationId()).getCurrentAppAttempt();\n+            if (null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n+                    + application.getApplicationAttemptId()\n+                    + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n+              }\n+              break;\n+            }\n+          }\n+          \n           // Is the node-label-expression of this offswitch resource request\n           // matches the node\u0027s label?\n           // If not match, jump to next priority.\n-          if (!checkResourceRequestMatchingNodeLabel(anyRequest, node)) {\n+          if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n+              anyRequest, node.getPartition(), schedulingMode)) {\n             continue;\n           }\n           \n           if (!this.reservationsContinueLooking) {\n             if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n-          Set\u003cString\u003e requestedNodeLabels \u003d\n-              getRequestLabelSetByExpression(anyRequest\n-                  .getNodeLabelExpression());\n-\n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n-                  required, requestedNodeLabels);          \n+                  required, node.getPartition(), schedulingMode);          \n           \n           // Check queue max-capacity limit\n-          if (!super.canAssignToThisQueue(clusterResource, node.getLabels(),\n-              this.currentResourceLimits, required, application.getCurrentReservation())) {\n+          if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n+              this.currentResourceLimits, required,\n+              application.getCurrentReservation(), schedulingMode)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n-              application, true, requestedNodeLabels)) {\n+              application, true, node.getPartition())) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n+          // Increase missed-non-partitioned-resource-request-opportunity.\n+          // This is to make sure non-partitioned-resource-request will prefer\n+          // to be allocated to non-partitioned nodes\n+          int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n+          if (anyRequest.getNodeLabelExpression().equals(\n+              RMNodeLabelsManager.NO_LABEL)) {\n+            missedNonPartitionedRequestSchedulingOpportunity \u003d\n+                application\n+                    .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+          }\n+          \n+          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n+            // Before doing allocation, we need to check scheduling opportunity to\n+            // make sure : non-partitioned resource request should be scheduled to\n+            // non-partitioned partition first.\n+            if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n+                .getNumClusterNodes()) {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"Skip app_attempt\u003d\"\n+                    + application.getApplicationAttemptId()\n+                    + \" priority\u003d\"\n+                    + priority\n+                    + \" because missed-non-partitioned-resource-request\"\n+                    + \" opportunity under requred:\"\n+                    + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n+                    + \" required\u003d\"\n+                    + scheduler.getNumClusterNodes());\n+              }\n+\n+              break;\n+            }\n+          }\n+          \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n-                null);\n+                null, schedulingMode);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n-                node.getLabels());\n+                node.getPartition());\n             \n-            // Don\u0027t reset scheduling opportunities for non-local assignments\n+            // Don\u0027t reset scheduling opportunities for offswitch assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n+            // Non-exclusive scheduling opportunity is different: we need reset\n+            // it every time to make sure non-labeled resource request will be\n+            // most likely allocated on non-labeled nodes first. \n+            application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource, schedulingMode);\n      }\n    }\n    \n    // if our queue cannot access this node, just return\n    if (schedulingMode \u003d\u003d SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY\n        \u0026\u0026 !accessibleToPartition(node.getPartition())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check if this queue need more resource, simply skip allocation if this\n    // queue doesn\u0027t need more resources.\n    if (!hasPendingResourceRequest(node.getPartition(),\n        clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip this queue\u003d\" + getQueuePath()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-partition\u003d\" + node.getPartition());\n      }\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n      \n      // Check if application needs more resource, skip if it doesn\u0027t need more.\n      if (!application.hasPendingResourceRequest(resourceCalculator,\n          node.getPartition(), clusterResource, schedulingMode)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Skip app_attempt\u003d\" + application.getApplicationAttemptId()\n              + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n              + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n        }\n        continue;\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          \n          // AM container allocation doesn\u0027t support non-exclusive allocation to\n          // avoid painful of preempt an AM container\n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            RMAppAttempt rmAppAttempt \u003d\n                csContext.getRMContext().getRMApps()\n                    .get(application.getApplicationId()).getCurrentAppAttempt();\n            if (null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n              }\n              break;\n            }\n          }\n          \n          // Is the node-label-expression of this offswitch resource request\n          // matches the node\u0027s label?\n          // If not match, jump to next priority.\n          if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n              anyRequest, node.getPartition(), schedulingMode)) {\n            continue;\n          }\n          \n          if (!this.reservationsContinueLooking) {\n            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, node.getPartition(), schedulingMode);          \n          \n          // Check queue max-capacity limit\n          if (!super.canAssignToThisQueue(clusterResource, node.getPartition(),\n              this.currentResourceLimits, required,\n              application.getCurrentReservation(), schedulingMode)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n              application, true, node.getPartition())) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Increase missed-non-partitioned-resource-request-opportunity.\n          // This is to make sure non-partitioned-resource-request will prefer\n          // to be allocated to non-partitioned nodes\n          int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n          if (anyRequest.getNodeLabelExpression().equals(\n              RMNodeLabelsManager.NO_LABEL)) {\n            missedNonPartitionedRequestSchedulingOpportunity \u003d\n                application\n                    .addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n          }\n          \n          if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n            // Before doing allocation, we need to check scheduling opportunity to\n            // make sure : non-partitioned resource request should be scheduled to\n            // non-partitioned partition first.\n            if (missedNonPartitionedRequestSchedulingOpportunity \u003c scheduler\n                .getNumClusterNodes()) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skip app_attempt\u003d\"\n                    + application.getApplicationAttemptId()\n                    + \" priority\u003d\"\n                    + priority\n                    + \" because missed-non-partitioned-resource-request\"\n                    + \" opportunity under requred:\"\n                    + \" Now\u003d\" + missedNonPartitionedRequestSchedulingOpportunity\n                    + \" required\u003d\"\n                    + scheduler.getNumClusterNodes());\n              }\n\n              break;\n            }\n          }\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null, schedulingMode);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getPartition());\n            \n            // Don\u0027t reset scheduling opportunities for offswitch assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            // Non-exclusive scheduling opportunity is different: we need reset\n            // it every time to make sure non-labeled resource request will be\n            // most likely allocated on non-labeled nodes first. \n            application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "586348e4cbf197188057d6b843a6701cfffdaff3": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3356. Capacity Scheduler FiCaSchedulerApp should use ResourceUsage to track used-resources-by-label. Contributed by Wangda Tan\n",
      "commitDate": "20/03/15 1:54 PM",
      "commitName": "586348e4cbf197188057d6b843a6701cfffdaff3",
      "commitAuthor": "Jian He",
      "commitDateOld": "17/03/15 9:30 PM",
      "commitNameOld": "658097d6da1b1aac8e01db459f0c3b456e99652f",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 2.68,
      "commitsBetweenForRepo": 42,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,144 +1,152 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits) {\n     updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // if our queue cannot access this node, just return\n     if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n         node.getLabels())) {\n       return NULL_ASSIGNMENT;\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n             clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n+          \n+          // Is the node-label-expression of this offswitch resource request\n+          // matches the node\u0027s label?\n+          // If not match, jump to next priority.\n+          if (!checkResourceRequestMatchingNodeLabel(anyRequest, node)) {\n+            continue;\n+          }\n+          \n           if (!this.reservationsContinueLooking) {\n             if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n           Set\u003cString\u003e requestedNodeLabels \u003d\n               getRequestLabelSetByExpression(anyRequest\n                   .getNodeLabelExpression());\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required, requestedNodeLabels);          \n           \n           // Check queue max-capacity limit\n           if (!super.canAssignToThisQueue(clusterResource, node.getLabels(),\n               this.currentResourceLimits, required, application.getCurrentReservation())) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n-          if (!assignToUser(clusterResource, application.getUser(), userLimit,\n+          if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n               application, true, requestedNodeLabels)) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n                 node.getLabels());\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // if our queue cannot access this node, just return\n    if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n        node.getLabels())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          \n          // Is the node-label-expression of this offswitch resource request\n          // matches the node\u0027s label?\n          // If not match, jump to next priority.\n          if (!checkResourceRequestMatchingNodeLabel(anyRequest, node)) {\n            continue;\n          }\n          \n          if (!this.reservationsContinueLooking) {\n            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          Set\u003cString\u003e requestedNodeLabels \u003d\n              getRequestLabelSetByExpression(anyRequest\n                  .getNodeLabelExpression());\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, requestedNodeLabels);          \n          \n          // Check queue max-capacity limit\n          if (!super.canAssignToThisQueue(clusterResource, node.getLabels(),\n              this.currentResourceLimits, required, application.getCurrentReservation())) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!canAssignToUser(clusterResource, application.getUser(), userLimit,\n              application, true, requestedNodeLabels)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getLabels());\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "487374b7fe0c92fc7eb1406c568952722b5d5b15": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
      "commitDate": "17/03/15 10:24 AM",
      "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
          "commitDate": "17/03/15 10:24 AM",
          "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
          "commitAuthor": "Jian He",
          "commitDateOld": "03/03/15 11:49 AM",
          "commitNameOld": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 13.9,
          "commitsBetweenForRepo": 109,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,144 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n-      FiCaSchedulerNode node, boolean needToUnreserve,\n-      ResourceLimits currentResourceLimits) {\n-    this.currentResourceLimits \u003d currentResourceLimits;\n+      FiCaSchedulerNode node, ResourceLimits currentResourceLimits) {\n+    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // if our queue cannot access this node, just return\n     if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n         node.getLabels())) {\n       return NULL_ASSIGNMENT;\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n             clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           if (!this.reservationsContinueLooking) {\n-            if (!needContainers(application, priority, required)) {\n+            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n           Set\u003cString\u003e requestedNodeLabels \u003d\n               getRequestLabelSetByExpression(anyRequest\n                   .getNodeLabelExpression());\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required, requestedNodeLabels);          \n           \n           // Check queue max-capacity limit\n-          if (!canAssignToThisQueue(clusterResource, required,\n-              node.getLabels(), application, true)) {\n+          if (!super.canAssignToThisQueue(clusterResource, node.getLabels(),\n+              this.currentResourceLimits, required, application.getCurrentReservation())) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(clusterResource, application.getUser(), userLimit,\n               application, true, requestedNodeLabels)) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n-                null, needToUnreserve);\n+                null);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n                 node.getLabels());\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // if our queue cannot access this node, just return\n    if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n        node.getLabels())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          if (!this.reservationsContinueLooking) {\n            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          Set\u003cString\u003e requestedNodeLabels \u003d\n              getRequestLabelSetByExpression(anyRequest\n                  .getNodeLabelExpression());\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, requestedNodeLabels);          \n          \n          // Check queue max-capacity limit\n          if (!super.canAssignToThisQueue(clusterResource, node.getLabels(),\n              this.currentResourceLimits, required, application.getCurrentReservation())) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(clusterResource, application.getUser(), userLimit,\n              application, true, requestedNodeLabels)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getLabels());\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, needToUnreserve-boolean, currentResourceLimits-ResourceLimits]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, currentResourceLimits-ResourceLimits]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
          "commitDate": "17/03/15 10:24 AM",
          "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
          "commitAuthor": "Jian He",
          "commitDateOld": "03/03/15 11:49 AM",
          "commitNameOld": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 13.9,
          "commitsBetweenForRepo": 109,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,144 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n-      FiCaSchedulerNode node, boolean needToUnreserve,\n-      ResourceLimits currentResourceLimits) {\n-    this.currentResourceLimits \u003d currentResourceLimits;\n+      FiCaSchedulerNode node, ResourceLimits currentResourceLimits) {\n+    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // if our queue cannot access this node, just return\n     if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n         node.getLabels())) {\n       return NULL_ASSIGNMENT;\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n             clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           if (!this.reservationsContinueLooking) {\n-            if (!needContainers(application, priority, required)) {\n+            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n           Set\u003cString\u003e requestedNodeLabels \u003d\n               getRequestLabelSetByExpression(anyRequest\n                   .getNodeLabelExpression());\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required, requestedNodeLabels);          \n           \n           // Check queue max-capacity limit\n-          if (!canAssignToThisQueue(clusterResource, required,\n-              node.getLabels(), application, true)) {\n+          if (!super.canAssignToThisQueue(clusterResource, node.getLabels(),\n+              this.currentResourceLimits, required, application.getCurrentReservation())) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(clusterResource, application.getUser(), userLimit,\n               application, true, requestedNodeLabels)) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n-                null, needToUnreserve);\n+                null);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n                 node.getLabels());\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits) {\n    updateCurrentResourceLimits(currentResourceLimits, clusterResource);\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // if our queue cannot access this node, just return\n    if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n        node.getLabels())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          if (!this.reservationsContinueLooking) {\n            if (!shouldAllocOrReserveNewContainer(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          Set\u003cString\u003e requestedNodeLabels \u003d\n              getRequestLabelSetByExpression(anyRequest\n                  .getNodeLabelExpression());\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, requestedNodeLabels);          \n          \n          // Check queue max-capacity limit\n          if (!super.canAssignToThisQueue(clusterResource, node.getLabels(),\n              this.currentResourceLimits, required, application.getCurrentReservation())) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(clusterResource, application.getUser(), userLimit,\n              application, true, requestedNodeLabels)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getLabels());\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "14dd647c556016d351f425ee956ccf800ccb9ce2": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3265. Fixed a deadlock in CapacityScheduler by always passing a queue\u0027s available resource-limit from the parent queue. Contributed by Wangda Tan.\n",
      "commitDate": "02/03/15 5:52 PM",
      "commitName": "14dd647c556016d351f425ee956ccf800ccb9ce2",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3265. Fixed a deadlock in CapacityScheduler by always passing a queue\u0027s available resource-limit from the parent queue. Contributed by Wangda Tan.\n",
          "commitDate": "02/03/15 5:52 PM",
          "commitName": "14dd647c556016d351f425ee956ccf800ccb9ce2",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "12/02/15 2:58 PM",
          "commitNameOld": "18a594257e052e8f10a03e5594e6cc6901dc56be",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 18.12,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,143 +1,145 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n-      FiCaSchedulerNode node, boolean needToUnreserve) {\n-\n+      FiCaSchedulerNode node, boolean needToUnreserve,\n+      ResourceLimits currentResourceLimits) {\n+    this.currentResourceLimits \u003d currentResourceLimits;\n+    \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // if our queue cannot access this node, just return\n     if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n         node.getLabels())) {\n       return NULL_ASSIGNMENT;\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n             clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           if (!this.reservationsContinueLooking) {\n             if (!needContainers(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n           Set\u003cString\u003e requestedNodeLabels \u003d\n               getRequestLabelSetByExpression(anyRequest\n                   .getNodeLabelExpression());\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required, requestedNodeLabels);          \n           \n           // Check queue max-capacity limit\n           if (!canAssignToThisQueue(clusterResource, required,\n               node.getLabels(), application, true)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(clusterResource, application.getUser(), userLimit,\n               application, true, requestedNodeLabels)) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null, needToUnreserve);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n                 node.getLabels());\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, boolean needToUnreserve,\n      ResourceLimits currentResourceLimits) {\n    this.currentResourceLimits \u003d currentResourceLimits;\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // if our queue cannot access this node, just return\n    if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n        node.getLabels())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          if (!this.reservationsContinueLooking) {\n            if (!needContainers(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          Set\u003cString\u003e requestedNodeLabels \u003d\n              getRequestLabelSetByExpression(anyRequest\n                  .getNodeLabelExpression());\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, requestedNodeLabels);          \n          \n          // Check queue max-capacity limit\n          if (!canAssignToThisQueue(clusterResource, required,\n              node.getLabels(), application, true)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(clusterResource, application.getUser(), userLimit,\n              application, true, requestedNodeLabels)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null, needToUnreserve);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getLabels());\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, needToUnreserve-boolean]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, needToUnreserve-boolean, currentResourceLimits-ResourceLimits]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3265. Fixed a deadlock in CapacityScheduler by always passing a queue\u0027s available resource-limit from the parent queue. Contributed by Wangda Tan.\n",
          "commitDate": "02/03/15 5:52 PM",
          "commitName": "14dd647c556016d351f425ee956ccf800ccb9ce2",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "12/02/15 2:58 PM",
          "commitNameOld": "18a594257e052e8f10a03e5594e6cc6901dc56be",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 18.12,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,143 +1,145 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n-      FiCaSchedulerNode node, boolean needToUnreserve) {\n-\n+      FiCaSchedulerNode node, boolean needToUnreserve,\n+      ResourceLimits currentResourceLimits) {\n+    this.currentResourceLimits \u003d currentResourceLimits;\n+    \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // if our queue cannot access this node, just return\n     if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n         node.getLabels())) {\n       return NULL_ASSIGNMENT;\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n             clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           if (!this.reservationsContinueLooking) {\n             if (!needContainers(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n           Set\u003cString\u003e requestedNodeLabels \u003d\n               getRequestLabelSetByExpression(anyRequest\n                   .getNodeLabelExpression());\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required, requestedNodeLabels);          \n           \n           // Check queue max-capacity limit\n           if (!canAssignToThisQueue(clusterResource, required,\n               node.getLabels(), application, true)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(clusterResource, application.getUser(), userLimit,\n               application, true, requestedNodeLabels)) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null, needToUnreserve);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n                 node.getLabels());\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, boolean needToUnreserve,\n      ResourceLimits currentResourceLimits) {\n    this.currentResourceLimits \u003d currentResourceLimits;\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // if our queue cannot access this node, just return\n    if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n        node.getLabels())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          if (!this.reservationsContinueLooking) {\n            if (!needContainers(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          Set\u003cString\u003e requestedNodeLabels \u003d\n              getRequestLabelSetByExpression(anyRequest\n                  .getNodeLabelExpression());\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, requestedNodeLabels);          \n          \n          // Check queue max-capacity limit\n          if (!canAssignToThisQueue(clusterResource, required,\n              node.getLabels(), application, true)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(clusterResource, application.getUser(), userLimit,\n              application, true, requestedNodeLabels)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null, needToUnreserve);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getLabels());\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2920. Changed CapacityScheduler to kill containers on nodes where node labels are changed. Contributed by  Wangda Tan\n",
      "commitDate": "22/12/14 4:51 PM",
      "commitName": "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb",
      "commitAuthor": "Jian He",
      "commitDateOld": "15/10/14 6:33 PM",
      "commitNameOld": "f2ea555ac6c06a3f2f6559731f48711fff05d3f1",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 67.97,
      "commitsBetweenForRepo": 558,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,143 +1,143 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, boolean needToUnreserve) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // if our queue cannot access this node, just return\n     if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n-        labelManager.getLabelsOnNode(node.getNodeID()))) {\n+        node.getLabels())) {\n       return NULL_ASSIGNMENT;\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n             clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           if (!this.reservationsContinueLooking) {\n             if (!needContainers(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n           \n           Set\u003cString\u003e requestedNodeLabels \u003d\n               getRequestLabelSetByExpression(anyRequest\n                   .getNodeLabelExpression());\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required, requestedNodeLabels);          \n           \n           // Check queue max-capacity limit\n           if (!canAssignToThisQueue(clusterResource, required,\n-              labelManager.getLabelsOnNode(node.getNodeID()), application, true)) {\n+              node.getLabels(), application, true)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(clusterResource, application.getUser(), userLimit,\n               application, true, requestedNodeLabels)) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null, needToUnreserve);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned,\n-                labelManager.getLabelsOnNode(node.getNodeID()));\n+                node.getLabels());\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, boolean needToUnreserve) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // if our queue cannot access this node, just return\n    if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n        node.getLabels())) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          if (!this.reservationsContinueLooking) {\n            if (!needContainers(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          Set\u003cString\u003e requestedNodeLabels \u003d\n              getRequestLabelSetByExpression(anyRequest\n                  .getNodeLabelExpression());\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, requestedNodeLabels);          \n          \n          // Check queue max-capacity limit\n          if (!canAssignToThisQueue(clusterResource, required,\n              node.getLabels(), application, true)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(clusterResource, application.getUser(), userLimit,\n              application, true, requestedNodeLabels)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null, needToUnreserve);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                node.getLabels());\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "f2ea555ac6c06a3f2f6559731f48711fff05d3f1": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2496. Enhanced Capacity Scheduler to have basic support for allocating resources based on node-labels. Contributed by Wangda Tan.\nYARN-2500. Ehnaced ResourceManager to support schedulers allocating resources based on node-labels. Contributed by Wangda Tan.\n",
      "commitDate": "15/10/14 6:33 PM",
      "commitName": "f2ea555ac6c06a3f2f6559731f48711fff05d3f1",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "07/10/14 1:45 PM",
      "commitNameOld": "30d56fdbb40d06c4e267d6c314c8c767a7adc6a3",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 8.2,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,131 +1,143 @@\n   public synchronized CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, boolean needToUnreserve) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n+    // if our queue cannot access this node, just return\n+    if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n+        labelManager.getLabelsOnNode(node.getNodeID()))) {\n+      return NULL_ASSIGNMENT;\n+    }\n+    \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n             clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n           if (!this.reservationsContinueLooking) {\n             if (!needContainers(application, priority, required)) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n               }\n               continue;\n             }\n           }\n+          \n+          Set\u003cString\u003e requestedNodeLabels \u003d\n+              getRequestLabelSetByExpression(anyRequest\n+                  .getNodeLabelExpression());\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n-                  required);          \n+                  required, requestedNodeLabels);          \n           \n           // Check queue max-capacity limit\n-          if (!assignToQueue(clusterResource, required, application, true)) {\n+          if (!canAssignToThisQueue(clusterResource, required,\n+              labelManager.getLabelsOnNode(node.getNodeID()), application, true)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(clusterResource, application.getUser(), userLimit,\n-              application, true)) {\n+              application, true, requestedNodeLabels)) {\n             break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null, needToUnreserve);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n-            allocateResource(clusterResource, application, assigned);\n+            allocateResource(clusterResource, application, assigned,\n+                labelManager.getLabelsOnNode(node.getNodeID()));\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Resetting scheduling opportunities\");\n               }\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, boolean needToUnreserve) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // if our queue cannot access this node, just return\n    if (!SchedulerUtils.checkQueueAccessToNode(accessibleLabels,\n        labelManager.getLabelsOnNode(node.getNodeID()))) {\n      return NULL_ASSIGNMENT;\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          if (!this.reservationsContinueLooking) {\n            if (!needContainers(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n          \n          Set\u003cString\u003e requestedNodeLabels \u003d\n              getRequestLabelSetByExpression(anyRequest\n                  .getNodeLabelExpression());\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required, requestedNodeLabels);          \n          \n          // Check queue max-capacity limit\n          if (!canAssignToThisQueue(clusterResource, required,\n              labelManager.getLabelsOnNode(node.getNodeID()), application, true)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(clusterResource, application.getUser(), userLimit,\n              application, true, requestedNodeLabels)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null, needToUnreserve);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned,\n                labelManager.getLabelsOnNode(node.getNodeID()));\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "9c22065109a77681bc2534063eabe8692fbcb3cd": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-1769. CapacityScheduler: Improve reservations. Contributed by Thomas Graves\n",
      "commitDate": "29/09/14 7:12 AM",
      "commitName": "9c22065109a77681bc2534063eabe8692fbcb3cd",
      "commitAuthor": "Jason Lowe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-1769. CapacityScheduler: Improve reservations. Contributed by Thomas Graves\n",
          "commitDate": "29/09/14 7:12 AM",
          "commitName": "9c22065109a77681bc2534063eabe8692fbcb3cd",
          "commitAuthor": "Jason Lowe",
          "commitDateOld": "14/08/14 11:00 PM",
          "commitNameOld": "7360cec692be5dcc3377ae5082fe22870caac96b",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 45.34,
          "commitsBetweenForRepo": 409,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,119 +1,131 @@\n-  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n+  public synchronized CSAssignment assignContainers(Resource clusterResource,\n+      FiCaSchedulerNode node, boolean needToUnreserve) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n           clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n-          if (!needContainers(application, priority, required)) {\n+          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n+          if (!this.reservationsContinueLooking) {\n+            if (!needContainers(application, priority, required)) {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n+              }\n+              continue;\n+            }\n+          }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n-          if (!assignToQueue(clusterResource, required)) {\n+          if (!assignToQueue(clusterResource, required, application, true)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n-          if (!assignToUser(\n-              clusterResource, application.getUser(), userLimit)) {\n-            break; \n+          if (!assignToUser(clusterResource, application.getUser(), userLimit,\n+              application, true)) {\n+            break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n-                null);\n+                null, needToUnreserve);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"Resetting scheduling opportunities\");\n+              }\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, boolean needToUnreserve) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n          clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          if (!this.reservationsContinueLooking) {\n            if (!needContainers(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required, application, true)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(clusterResource, application.getUser(), userLimit,\n              application, true)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null, needToUnreserve);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, needToUnreserve-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-1769. CapacityScheduler: Improve reservations. Contributed by Thomas Graves\n",
          "commitDate": "29/09/14 7:12 AM",
          "commitName": "9c22065109a77681bc2534063eabe8692fbcb3cd",
          "commitAuthor": "Jason Lowe",
          "commitDateOld": "14/08/14 11:00 PM",
          "commitNameOld": "7360cec692be5dcc3377ae5082fe22870caac96b",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 45.34,
          "commitsBetweenForRepo": 409,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,119 +1,131 @@\n-  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n+  public synchronized CSAssignment assignContainers(Resource clusterResource,\n+      FiCaSchedulerNode node, boolean needToUnreserve) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n           clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           ResourceRequest anyRequest \u003d\n               application.getResourceRequest(priority, ResourceRequest.ANY);\n           if (null \u003d\u003d anyRequest) {\n             continue;\n           }\n           \n           // Required resource\n           Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n-          if (!needContainers(application, priority, required)) {\n+          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n             continue;\n           }\n+          if (!this.reservationsContinueLooking) {\n+            if (!needContainers(application, priority, required)) {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n+              }\n+              continue;\n+            }\n+          }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n-          if (!assignToQueue(clusterResource, required)) {\n+          if (!assignToQueue(clusterResource, required, application, true)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n-          if (!assignToUser(\n-              clusterResource, application.getUser(), userLimit)) {\n-            break; \n+          if (!assignToUser(clusterResource, application.getUser(), userLimit,\n+              application, true)) {\n+            break;\n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n-                null);\n+                null, needToUnreserve);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"Resetting scheduling opportunities\");\n+              }\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, boolean needToUnreserve) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n          clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (application.getTotalRequiredResources(priority) \u003c\u003d 0) {\n            continue;\n          }\n          if (!this.reservationsContinueLooking) {\n            if (!needContainers(application, priority, required)) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n              }\n              continue;\n            }\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required, application, true)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(clusterResource, application.getUser(), userLimit,\n              application, true)) {\n            break;\n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null, needToUnreserve);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Resetting scheduling opportunities\");\n              }\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "4ce0e4bf2e91278bbc33f4a1c44c7929627b5d6e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1444. Fix CapacityScheduler to deal with cases where applications specify host/rack requests without off-switch request. Contributed by Wangda Tan.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1576751 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/03/14 7:36 AM",
      "commitName": "4ce0e4bf2e91278bbc33f4a1c44c7929627b5d6e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "20/02/14 6:20 PM",
      "commitNameOld": "772ead791c17b0b7415cce0934366113cdbe9379",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 19.51,
      "commitsBetweenForRepo": 177,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,115 +1,119 @@\n   assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n           clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n+          ResourceRequest anyRequest \u003d\n+              application.getResourceRequest(priority, ResourceRequest.ANY);\n+          if (null \u003d\u003d anyRequest) {\n+            continue;\n+          }\n+          \n           // Required resource\n-          Resource required \u003d \n-              application.getResourceRequest(\n-                  priority, ResourceRequest.ANY).getCapability();\n+          Resource required \u003d anyRequest.getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(\n               clusterResource, application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n          clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest \u003d\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n          if (null \u003d\u003d anyRequest) {\n            continue;\n          }\n          \n          // Required resource\n          Resource required \u003d anyRequest.getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(\n              clusterResource, application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "025f1719472282a30aa26ae3e235e404f04ba932": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1333. Support blacklisting in the Fair Scheduler (Tsuyoshi Ozawa via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1535899 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/10/13 5:27 PM",
      "commitName": "025f1719472282a30aa26ae3e235e404f04ba932",
      "commitAuthor": "Sanford Ryza",
      "commitDateOld": "21/10/13 11:45 AM",
      "commitNameOld": "dc523bd18247df232fe814aed7062a116242ab04",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 4.24,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,115 +1,115 @@\n   assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n           clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n-        if (FiCaSchedulerUtils.isBlacklisted(application, node, LOG)) {\n+        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(\n                   priority, ResourceRequest.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(\n               clusterResource, application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n          clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(\n                  priority, ResourceRequest.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(\n              clusterResource, application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1008. MiniYARNCluster with multiple nodemanagers, all nodes have same key for allocations. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1517563 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/08/13 8:39 AM",
      "commitName": "942e2ebaa54306ffc5b0ffb403e552764a40d58c",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "22/07/13 4:49 PM",
      "commitNameOld": "5b3bb05fbeb7ed4671f4d3a59677788f7fda43d0",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 34.66,
      "commitsBetweenForRepo": 214,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,115 +1,115 @@\n   assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n+      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       synchronized (application) {\n         return assignReservedContainer(application, node, reservedContainer,\n           clusterResource);\n       }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (FiCaSchedulerUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(\n                   priority, ResourceRequest.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(\n               clusterResource, application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n          clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (FiCaSchedulerUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(\n                  priority, ResourceRequest.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(\n              clusterResource, application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "eff5d9b17e0853e82968a695b498b4be37148a05": {
      "type": "Ybodychange",
      "commitMessage": "YARN-845. RM crash with NPE on NODE_UPDATE (Mayank Bansal via bikas)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1499886 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/07/13 4:31 PM",
      "commitName": "eff5d9b17e0853e82968a695b498b4be37148a05",
      "commitAuthor": "Bikas Saha",
      "commitDateOld": "29/06/13 1:18 PM",
      "commitNameOld": "8eb3be63f598daae01f0a0c09eab5086881f153d",
      "commitAuthorOld": "Luke Lu",
      "daysBetweenCommits": 5.13,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,115 @@\n   assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n-      return \n-          assignReservedContainer(application, node, reservedContainer, \n-              clusterResource); \n+      synchronized (application) {\n+        return assignReservedContainer(application, node, reservedContainer,\n+          clusterResource);\n+      }\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n         if (FiCaSchedulerUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(\n                   priority, ResourceRequest.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(\n               clusterResource, application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n          clusterResource);\n      }\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (FiCaSchedulerUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(\n                  priority, ResourceRequest.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(\n              clusterResource, application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "8eb3be63f598daae01f0a0c09eab5086881f153d": {
      "type": "Ybodychange",
      "commitMessage": "YARN-877. Support resource blacklisting for FifoScheduler. (Junping Du via llu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1498021 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/06/13 1:18 PM",
      "commitName": "8eb3be63f598daae01f0a0c09eab5086881f153d",
      "commitAuthor": "Luke Lu",
      "commitDateOld": "21/06/13 11:28 AM",
      "commitNameOld": "37d7935a9d7b86635c9c1ffc03f88b49857f88a0",
      "commitAuthorOld": "Bikas Saha",
      "daysBetweenCommits": 8.08,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,114 @@\n   assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return \n           assignReservedContainer(application, node, reservedContainer, \n               clusterResource); \n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Check if this resource is on the blacklist\n-        if (isBlacklisted(application, node)) {\n+        if (FiCaSchedulerUtils.isBlacklisted(application, node, LOG)) {\n           continue;\n         }\n         \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(\n                   priority, ResourceRequest.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(\n               clusterResource, application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return \n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource); \n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (FiCaSchedulerUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(\n                  priority, ResourceRequest.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(\n              clusterResource, application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "2051fd5ee29e99df6fe79c70b0c7c8c0c1cc131f": {
      "type": "Ybodychange",
      "commitMessage": "YARN-750. Allow for black-listing resources in YARN API and Impl in CS (acmurthy via bikas)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1490392 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/06/13 11:46 AM",
      "commitName": "2051fd5ee29e99df6fe79c70b0c7c8c0c1cc131f",
      "commitAuthor": "Bikas Saha",
      "commitDateOld": "03/06/13 5:34 PM",
      "commitNameOld": "d33534c4fb35cb82ff8d56abeeb63a949e72a031",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 2.76,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,114 @@\n   assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return \n           assignReservedContainer(application, node, reservedContainer, \n               clusterResource); \n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n+        // Check if this resource is on the blacklist\n+        if (isBlacklisted(application, node)) {\n+          continue;\n+        }\n+        \n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(\n                   priority, ResourceRequest.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(\n               clusterResource, application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did the application skip this node?\n           if (assignment.getSkipped()) {\n             // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n             application.subtractSchedulingOpportunity(priority);\n             continue;\n           }\n           \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return \n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource); \n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (isBlacklisted(application, node)) {\n          continue;\n        }\n        \n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(\n                  priority, ResourceRequest.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(\n              clusterResource, application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "4a14264ddb28d4cfd06ec4f70b42e3174a1d888c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-398. Make it possible to specify hard locality constraints in resource requests for CapacityScheduler. Contributed by Arun C. Murthy.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489087 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/06/13 11:13 AM",
      "commitName": "4a14264ddb28d4cfd06ec4f70b42e3174a1d888c",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "01/06/13 2:43 PM",
      "commitNameOld": "a2c42330047bf955a6a585dcddf798920d4c8640",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 1.85,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,102 +1,109 @@\n   assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return \n           assignReservedContainer(application, node, reservedContainer, \n               clusterResource); \n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(\n                   priority, ResourceRequest.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(\n               clusterResource, application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n+          // Did the application skip this node?\n+          if (assignment.getSkipped()) {\n+            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n+            application.subtractSchedulingOpportunity(priority);\n+            continue;\n+          }\n+          \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return \n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource); \n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(\n                  priority, ResourceRequest.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(\n              clusterResource, application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did the application skip this node?\n          if (assignment.getSkipped()) {\n            // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n            application.subtractSchedulingOpportunity(priority);\n            continue;\n          }\n          \n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "520033b1cd81c76b38fcdcfcfeed16158db4bbba": {
      "type": "Ybodychange",
      "commitMessage": "YARN-450. Define value for * in the scheduling protocol (Zhijie Shen via bikas)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1462271 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/03/13 12:44 PM",
      "commitName": "520033b1cd81c76b38fcdcfcfeed16158db4bbba",
      "commitAuthor": "Bikas Saha",
      "commitDateOld": "27/03/13 11:38 AM",
      "commitNameOld": "d0bbff6c32592cb5d49d7be8d8a7346788a9ba19",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 1.05,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,102 @@\n   assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return \n           assignReservedContainer(application, node, reservedContainer, \n               clusterResource); \n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n-              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n+              application.getResourceRequest(\n+                  priority, ResourceRequest.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(\n               clusterResource, application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return \n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource); \n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(\n                  priority, ResourceRequest.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(\n              clusterResource, application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "106e2e27ffb81f816ae627fa1712f5db5fb36002": {
      "type": "Ybodychange",
      "commitMessage": "YARN-325. RM CapacityScheduler can deadlock when getQueueInfo() is called and a container is completing (Arun C Murthy via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431070 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:00 PM",
      "commitName": "106e2e27ffb81f816ae627fa1712f5db5fb36002",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "08/01/13 9:08 PM",
      "commitNameOld": "453926397182078c65a4428eb5de5a90d6af6448",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.66,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,101 @@\n   assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n-      return new CSAssignment(\n+      return \n           assignReservedContainer(application, node, reservedContainer, \n-              clusterResource),\n-          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n-                                // for reserved containers\n+              clusterResource); \n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(\n               clusterResource, application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(\n               resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return \n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource); \n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(\n              clusterResource, application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "453926397182078c65a4428eb5de5a90d6af6448": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2. Enhanced CapacityScheduler to account for CPU alongwith memory for multi-dimensional resource scheduling. Contributed by Arun C. Murthy.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430682 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 9:08 PM",
      "commitName": "453926397182078c65a4428eb5de5a90d6af6448",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "07/11/12 1:56 PM",
      "commitNameOld": "fb5b96dfc324f999e8b3698288c110a1c3b71c30",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 62.3,
      "commitsBetweenForRepo": 257,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,103 @@\n   assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return new CSAssignment(\n           assignReservedContainer(application, node, reservedContainer, \n               clusterResource),\n           NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                 // for reserved containers\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n-          if (!assignToUser(application.getUser(), userLimit)) {\n+          if (!assignToUser(\n+              clusterResource, application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n-          if (Resources.greaterThan(assigned, Resources.none())) {\n+          if (Resources.greaterThan(\n+              resourceCalculator, clusterResource, assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Don\u0027t reset scheduling opportunities for non-local assignments\n             // otherwise the app will be delayed for each non-local assignment.\n             // This helps apps with many off-cluster requests schedule faster.\n             if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n               application.resetSchedulingOpportunities(priority);\n             }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return new CSAssignment(\n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource),\n          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                // for reserved containers\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(\n              clusterResource, application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "fb5b96dfc324f999e8b3698288c110a1c3b71c30": {
      "type": "Ybodychange",
      "commitMessage": "YARN-201. Fix CapacityScheduler to be less conservative for starved off-switch requests. Contributed by Jason Lowe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1406834 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/11/12 1:56 PM",
      "commitName": "fb5b96dfc324f999e8b3698288c110a1c3b71c30",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/10/12 7:16 AM",
      "commitNameOld": "90ba993bc72e374f99c44d0770f55aeaa8342f2d",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 14.32,
      "commitsBetweenForRepo": 68,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,97 +1,101 @@\n   assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return new CSAssignment(\n           assignReservedContainer(application, node, reservedContainer, \n               clusterResource),\n           NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                 // for reserved containers\n     }\n     \n     // Try to assign containers to applications in order\n     for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n-            // Reset scheduling opportunities\n-            application.resetSchedulingOpportunities(priority);\n+            // Don\u0027t reset scheduling opportunities for non-local assignments\n+            // otherwise the app will be delayed for each non-local assignment.\n+            // This helps apps with many off-cluster requests schedule faster.\n+            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n+              application.resetSchedulingOpportunities(priority);\n+            }\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return new CSAssignment(\n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource),\n          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                // for reserved containers\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Don\u0027t reset scheduling opportunities for non-local assignments\n            // otherwise the app will be delayed for each non-local assignment.\n            // This helps apps with many off-cluster requests schedule faster.\n            if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n              application.resetSchedulingOpportunities(priority);\n            }\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "e1fdf62123625e4ba399af02f8aad500637d29d1": {
      "type": "Yfilerename",
      "commitMessage": "YARN-1. Promote YARN to be a sub-project of Apache Hadoop.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370666 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 10:22 PM",
      "commitName": "e1fdf62123625e4ba399af02f8aad500637d29d1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "07/08/12 7:53 PM",
      "commitNameOld": "34554d1e11ee1d5b564d7d9ed3e6d55931d72749",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return new CSAssignment(\n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource),\n          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                // for reserved containers\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
      }
    },
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-4440. Changed SchedulerApp and SchedulerNode to be a minimal interface to allow schedulers to maintain their own.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/07/12 6:43 PM",
      "commitName": "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76",
      "commitAuthor": "Arun Murthy",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-4440. Changed SchedulerApp and SchedulerNode to be a minimal interface to allow schedulers to maintain their own.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362332 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/07/12 6:43 PM",
          "commitName": "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "10/07/12 2:26 PM",
          "commitNameOld": "3bfb26ad3b5ac46f992a632541c97ca2bc897638",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 6.18,
          "commitsBetweenForRepo": 55,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,97 +1,97 @@\n-  assignContainers(Resource clusterResource, SchedulerNode node) {\n+  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n-      SchedulerApp application \u003d \n+      FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return new CSAssignment(\n           assignReservedContainer(application, node, reservedContainer, \n               clusterResource),\n           NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                 // for reserved containers\n     }\n     \n     // Try to assign containers to applications in order\n-    for (SchedulerApp application : activeApplications) {\n+    for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Reset scheduling opportunities\n             application.resetSchedulingOpportunities(priority);\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return new CSAssignment(\n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource),\n          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                // for reserved containers\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-SchedulerNode]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-4440. Changed SchedulerApp and SchedulerNode to be a minimal interface to allow schedulers to maintain their own.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362332 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/07/12 6:43 PM",
          "commitName": "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "10/07/12 2:26 PM",
          "commitNameOld": "3bfb26ad3b5ac46f992a632541c97ca2bc897638",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 6.18,
          "commitsBetweenForRepo": 55,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,97 +1,97 @@\n-  assignContainers(Resource clusterResource, SchedulerNode node) {\n+  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n-      SchedulerApp application \u003d \n+      FiCaSchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return new CSAssignment(\n           assignReservedContainer(application, node, reservedContainer, \n               clusterResource),\n           NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                 // for reserved containers\n     }\n     \n     // Try to assign containers to applications in order\n-    for (SchedulerApp application : activeApplications) {\n+    for (FiCaSchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Compute user-limit \u0026 set headroom\n           // Note: We compute both user-limit \u0026 headroom with the highest \n           //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n               computeUserLimitAndSetHeadroom(application, clusterResource, \n                   required);          \n           \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n \n           // Did we schedule or reserve a container?\n           Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(assigned, Resources.none())) {\n \n             // Book-keeping \n             // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Reset scheduling opportunities\n             application.resetSchedulingOpportunities(priority);\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  assignContainers(Resource clusterResource, FiCaSchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return new CSAssignment(\n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource),\n          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                // for reserved containers\n    }\n    \n    // Try to assign containers to applications in order\n    for (FiCaSchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "ef1a619a4df3a612eb293a6e8e1e952eaef18eba": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3752. Modified application limits to include queue max-capacities besides the usual user limits. Contributed by Arun C Murthy.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1239422 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/02/12 4:41 PM",
      "commitName": "ef1a619a4df3a612eb293a6e8e1e952eaef18eba",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "27/01/12 5:32 PM",
      "commitNameOld": "5262b7ba4d018562d4e7d60772af4ddc3d770a23",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 4.96,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,97 +1,97 @@\n   assignContainers(Resource clusterResource, SchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       SchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return new CSAssignment(\n           assignReservedContainer(application, node, reservedContainer, \n               clusterResource),\n           NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                 // for reserved containers\n     }\n     \n     // Try to assign containers to applications in order\n     for (SchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n         application.showRequests();\n       }\n \n       synchronized (application) {\n         // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n-          // Compute \u0026 set headroom\n-          // Note: We set the headroom with the highest priority request \n-          //       as the target. \n+          // Compute user-limit \u0026 set headroom\n+          // Note: We compute both user-limit \u0026 headroom with the highest \n+          //       priority request as the target. \n           //       This works since we never assign lower priority requests\n           //       before all higher priority ones are serviced.\n           Resource userLimit \u003d \n-              computeAndSetUserResourceLimit(application, clusterResource, \n-                  required);\n-\n+              computeUserLimitAndSetHeadroom(application, clusterResource, \n+                  required);          \n+          \n           // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n           // Check user limit\n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n-          \n-          Resource assigned \u003d assignment.getResource();\n-          \n+\n           // Did we schedule or reserve a container?\n+          Resource assigned \u003d assignment.getResource();\n           if (Resources.greaterThan(assigned, Resources.none())) {\n \n-            // Book-keeping\n+            // Book-keeping \n+            // Note: Update headroom to account for current allocation too...\n             allocateResource(clusterResource, application, assigned);\n             \n             // Reset scheduling opportunities\n             application.resetSchedulingOpportunities(priority);\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      SchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return new CSAssignment(\n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource),\n          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                // for reserved containers\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute user-limit \u0026 set headroom\n          // Note: We compute both user-limit \u0026 headroom with the highest \n          //       priority request as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeUserLimitAndSetHeadroom(application, clusterResource, \n                  required);          \n          \n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n\n          // Did we schedule or reserve a container?\n          Resource assigned \u003d assignment.getResource();\n          if (Resources.greaterThan(assigned, Resources.none())) {\n\n            // Book-keeping \n            // Note: Update headroom to account for current allocation too...\n            allocateResource(clusterResource, application, assigned);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "21c9116309d8482e7e28522cd7386e65415b15e9": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3713. Fixed the way head-room is allocated to applications by CapacityScheduler so that it deducts current-usage per user and not per-application. Contributed by Arun C Murthy.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1235989 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/12 3:31 PM",
      "commitName": "21c9116309d8482e7e28522cd7386e65415b15e9",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "25/01/12 10:17 AM",
      "commitNameOld": "9d1621da52fd7f4ee68f80fdbf420180a42b5b1d",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.22,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,97 @@\n   assignContainers(Resource clusterResource, SchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       SchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return new CSAssignment(\n           assignReservedContainer(application, node, reservedContainer, \n               clusterResource),\n           NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                 // for reserved containers\n     }\n     \n     // Try to assign containers to applications in order\n     for (SchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n+        application.showRequests();\n       }\n-      application.showRequests();\n \n       synchronized (application) {\n-        computeAndSetUserResourceLimit(application, clusterResource);\n-        \n+        // Schedule in priority order\n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n-          // Are we going over limits by allocating to this application?\n-          // Maximum Capacity of the queue\n+          // Compute \u0026 set headroom\n+          // Note: We set the headroom with the highest priority request \n+          //       as the target. \n+          //       This works since we never assign lower priority requests\n+          //       before all higher priority ones are serviced.\n+          Resource userLimit \u003d \n+              computeAndSetUserResourceLimit(application, clusterResource, \n+                  required);\n+\n+          // Check queue max-capacity limit\n           if (!assignToQueue(clusterResource, required)) {\n             return NULL_ASSIGNMENT;\n           }\n \n-          // User limits\n-          Resource userLimit \u003d \n-            computeUserLimit(application, clusterResource, required); \n+          // Check user limit\n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n           \n           Resource assigned \u003d assignment.getResource();\n-            \n+          \n           // Did we schedule or reserve a container?\n           if (Resources.greaterThan(assigned, Resources.none())) {\n \n             // Book-keeping\n             allocateResource(clusterResource, application, assigned);\n             \n             // Reset scheduling opportunities\n             application.resetSchedulingOpportunities(priority);\n             \n             // Done\n             return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      SchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return new CSAssignment(\n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource),\n          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                // for reserved containers\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n        application.showRequests();\n      }\n\n      synchronized (application) {\n        // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Compute \u0026 set headroom\n          // Note: We set the headroom with the highest priority request \n          //       as the target. \n          //       This works since we never assign lower priority requests\n          //       before all higher priority ones are serviced.\n          Resource userLimit \u003d \n              computeAndSetUserResourceLimit(application, clusterResource, \n                  required);\n\n          // Check queue max-capacity limit\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // Check user limit\n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n          \n          Resource assigned \u003d assignment.getResource();\n          \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n\n            // Book-keeping\n            allocateResource(clusterResource, application, assigned);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "4a343c9d4ab4c993b545f0c1062c6b5449b065f0": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "MAPREDUCE-3641. Making CapacityScheduler more conservative so as to assign only one off-switch container in a single scheduling iteration. Contributed by Arun C Murthy.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1232182 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/01/12 1:56 PM",
      "commitName": "4a343c9d4ab4c993b545f0c1062c6b5449b065f0",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "MAPREDUCE-3641. Making CapacityScheduler more conservative so as to assign only one off-switch container in a single scheduling iteration. Contributed by Arun C Murthy.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1232182 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/01/12 1:56 PM",
          "commitName": "4a343c9d4ab4c993b545f0c1062c6b5449b065f0",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "12/12/11 4:20 PM",
          "commitNameOld": "e52291ea8871e2de421692fdfd6fbaabeca60eb4",
          "commitAuthorOld": "Mahadev Konar",
          "daysBetweenCommits": 34.9,
          "commitsBetweenForRepo": 165,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,90 +1,92 @@\n   assignContainers(Resource clusterResource, SchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       SchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n-      return assignReservedContainer(application, node, reservedContainer, \n-          clusterResource);\n+      return new CSAssignment(\n+          assignReservedContainer(application, node, reservedContainer, \n+              clusterResource),\n+          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n+                                // for reserved containers\n     }\n     \n     // Try to assign containers to applications in order\n     for (SchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n       }\n       application.showRequests();\n \n       synchronized (application) {\n         computeAndSetUserResourceLimit(application, clusterResource);\n         \n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Are we going over limits by allocating to this application?\n           // Maximum Capacity of the queue\n           if (!assignToQueue(clusterResource, required)) {\n-            return Resources.none();\n+            return NULL_ASSIGNMENT;\n           }\n \n           // User limits\n           Resource userLimit \u003d \n             computeUserLimit(application, clusterResource, required); \n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n-          Resource assigned \u003d \n+          CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n-  \n+          \n+          Resource assigned \u003d assignment.getResource();\n+            \n           // Did we schedule or reserve a container?\n           if (Resources.greaterThan(assigned, Resources.none())) {\n-            Resource assignedResource \u003d \n-              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n             // Book-keeping\n-            allocateResource(clusterResource, \n-                application, assignedResource);\n+            allocateResource(clusterResource, application, assigned);\n             \n             // Reset scheduling opportunities\n             application.resetSchedulingOpportunities(priority);\n             \n             // Done\n-            return assignedResource; \n+            return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n-    return Resources.none();\n+    return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      SchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return new CSAssignment(\n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource),\n          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                // for reserved containers\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n      }\n      application.showRequests();\n\n      synchronized (application) {\n        computeAndSetUserResourceLimit(application, clusterResource);\n        \n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Are we going over limits by allocating to this application?\n          // Maximum Capacity of the queue\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // User limits\n          Resource userLimit \u003d \n            computeUserLimit(application, clusterResource, required); \n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n          \n          Resource assigned \u003d assignment.getResource();\n            \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n\n            // Book-keeping\n            allocateResource(clusterResource, application, assigned);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "Resource",
            "newValue": "CSAssignment"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-3641. Making CapacityScheduler more conservative so as to assign only one off-switch container in a single scheduling iteration. Contributed by Arun C Murthy.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1232182 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/01/12 1:56 PM",
          "commitName": "4a343c9d4ab4c993b545f0c1062c6b5449b065f0",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "12/12/11 4:20 PM",
          "commitNameOld": "e52291ea8871e2de421692fdfd6fbaabeca60eb4",
          "commitAuthorOld": "Mahadev Konar",
          "daysBetweenCommits": 34.9,
          "commitsBetweenForRepo": 165,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,90 +1,92 @@\n   assignContainers(Resource clusterResource, SchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       SchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n-      return assignReservedContainer(application, node, reservedContainer, \n-          clusterResource);\n+      return new CSAssignment(\n+          assignReservedContainer(application, node, reservedContainer, \n+              clusterResource),\n+          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n+                                // for reserved containers\n     }\n     \n     // Try to assign containers to applications in order\n     for (SchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n       }\n       application.showRequests();\n \n       synchronized (application) {\n         computeAndSetUserResourceLimit(application, clusterResource);\n         \n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Are we going over limits by allocating to this application?\n           // Maximum Capacity of the queue\n           if (!assignToQueue(clusterResource, required)) {\n-            return Resources.none();\n+            return NULL_ASSIGNMENT;\n           }\n \n           // User limits\n           Resource userLimit \u003d \n             computeUserLimit(application, clusterResource, required); \n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n-          Resource assigned \u003d \n+          CSAssignment assignment \u003d  \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n-  \n+          \n+          Resource assigned \u003d assignment.getResource();\n+            \n           // Did we schedule or reserve a container?\n           if (Resources.greaterThan(assigned, Resources.none())) {\n-            Resource assignedResource \u003d \n-              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n             // Book-keeping\n-            allocateResource(clusterResource, \n-                application, assignedResource);\n+            allocateResource(clusterResource, application, assigned);\n             \n             // Reset scheduling opportunities\n             application.resetSchedulingOpportunities(priority);\n             \n             // Done\n-            return assignedResource; \n+            return assignment;\n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n-    return Resources.none();\n+    return NULL_ASSIGNMENT;\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      SchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return new CSAssignment(\n          assignReservedContainer(application, node, reservedContainer, \n              clusterResource),\n          NodeType.NODE_LOCAL); // Don\u0027t care about locality constraints \n                                // for reserved containers\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n      }\n      application.showRequests();\n\n      synchronized (application) {\n        computeAndSetUserResourceLimit(application, clusterResource);\n        \n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Are we going over limits by allocating to this application?\n          // Maximum Capacity of the queue\n          if (!assignToQueue(clusterResource, required)) {\n            return NULL_ASSIGNMENT;\n          }\n\n          // User limits\n          Resource userLimit \u003d \n            computeUserLimit(application, clusterResource, required); \n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          CSAssignment assignment \u003d  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n          \n          Resource assigned \u003d assignment.getResource();\n            \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n\n            // Book-keeping\n            allocateResource(clusterResource, application, assigned);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignment;\n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return NULL_ASSIGNMENT;\n\n  }",
          "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "b8f0836f9420e71652404c41471653bb15f62a48": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3126. Fixed a corner case in CapacityScheduler where headroom wasn\u0027t updated on changes to cluster size.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1182000 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/10/11 11:24 AM",
      "commitName": "b8f0836f9420e71652404c41471653bb15f62a48",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "06/10/11 10:27 PM",
      "commitNameOld": "b8102dbdf8b4dc2e99bc7c58f4085a7313830a2d",
      "commitAuthorOld": "Mahadev Konar",
      "daysBetweenCommits": 4.54,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,90 @@\n   assignContainers(Resource clusterResource, SchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       SchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return assignReservedContainer(application, node, reservedContainer, \n           clusterResource);\n     }\n     \n     // Try to assign containers to applications in order\n     for (SchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n       }\n       application.showRequests();\n \n       synchronized (application) {\n-        Resource userLimit \u003d \n-          computeUserLimit(application, clusterResource, Resources.none());\n-        setUserResourceLimit(application, userLimit);\n+        computeAndSetUserResourceLimit(application, clusterResource);\n         \n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Are we going over limits by allocating to this application?\n           // Maximum Capacity of the queue\n           if (!assignToQueue(clusterResource, required)) {\n             return Resources.none();\n           }\n \n           // User limits\n-          userLimit \u003d \n+          Resource userLimit \u003d \n             computeUserLimit(application, clusterResource, required); \n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           Resource assigned \u003d \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n   \n           // Did we schedule or reserve a container?\n           if (Resources.greaterThan(assigned, Resources.none())) {\n             Resource assignedResource \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n             // Book-keeping\n             allocateResource(clusterResource, \n                 application, assignedResource);\n             \n             // Reset scheduling opportunities\n             application.resetSchedulingOpportunities(priority);\n             \n             // Done\n             return assignedResource; \n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return Resources.none();\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      SchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return assignReservedContainer(application, node, reservedContainer, \n          clusterResource);\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n      }\n      application.showRequests();\n\n      synchronized (application) {\n        computeAndSetUserResourceLimit(application, clusterResource);\n        \n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Are we going over limits by allocating to this application?\n          // Maximum Capacity of the queue\n          if (!assignToQueue(clusterResource, required)) {\n            return Resources.none();\n          }\n\n          // User limits\n          Resource userLimit \u003d \n            computeUserLimit(application, clusterResource, required); \n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          Resource assigned \u003d \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n  \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n            Resource assignedResource \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n            // Book-keeping\n            allocateResource(clusterResource, \n                application, assignedResource);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignedResource; \n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return Resources.none();\n\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "b8102dbdf8b4dc2e99bc7c58f4085a7313830a2d": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2794. [MR-279] Incorrect metrics value for AvailableGB per queue per user. (John George via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179936 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/10/11 10:27 PM",
      "commitName": "b8102dbdf8b4dc2e99bc7c58f4085a7313830a2d",
      "commitAuthor": "Mahadev Konar",
      "commitDateOld": "05/10/11 4:56 AM",
      "commitNameOld": "f24dcb3449c77da665058427bc7fa480cad507fc",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 1.73,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,92 @@\n   assignContainers(Resource clusterResource, SchedulerNode node) {\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" #applications\u003d\" + activeApplications.size());\n     }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       SchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return assignReservedContainer(application, node, reservedContainer, \n           clusterResource);\n     }\n     \n     // Try to assign containers to applications in order\n     for (SchedulerApp application : activeApplications) {\n       \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"pre-assignContainers for application \"\n         + application.getApplicationId());\n       }\n       application.showRequests();\n \n       synchronized (application) {\n         Resource userLimit \u003d \n           computeUserLimit(application, clusterResource, Resources.none());\n         setUserResourceLimit(application, userLimit);\n         \n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Are we going over limits by allocating to this application?\n           // Maximum Capacity of the queue\n           if (!assignToQueue(clusterResource, required)) {\n             return Resources.none();\n           }\n \n           // User limits\n           userLimit \u003d \n             computeUserLimit(application, clusterResource, required); \n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           Resource assigned \u003d \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n   \n           // Did we schedule or reserve a container?\n           if (Resources.greaterThan(assigned, Resources.none())) {\n             Resource assignedResource \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n             // Book-keeping\n             allocateResource(clusterResource, \n-                application.getUser(), assignedResource);\n+                application, assignedResource);\n             \n             // Reset scheduling opportunities\n             application.resetSchedulingOpportunities(priority);\n             \n             // Done\n             return assignedResource; \n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n       }\n       application.showRequests();\n     }\n   \n     return Resources.none();\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      SchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return assignReservedContainer(application, node, reservedContainer, \n          clusterResource);\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n      }\n      application.showRequests();\n\n      synchronized (application) {\n        Resource userLimit \u003d \n          computeUserLimit(application, clusterResource, Resources.none());\n        setUserResourceLimit(application, userLimit);\n        \n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Are we going over limits by allocating to this application?\n          // Maximum Capacity of the queue\n          if (!assignToQueue(clusterResource, required)) {\n            return Resources.none();\n          }\n\n          // User limits\n          userLimit \u003d \n            computeUserLimit(application, clusterResource, required); \n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          Resource assigned \u003d \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n  \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n            Resource assignedResource \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n            // Book-keeping\n            allocateResource(clusterResource, \n                application, assignedResource);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignedResource; \n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return Resources.none();\n\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "f24dcb3449c77da665058427bc7fa480cad507fc": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2907. Changed log level for various messages in ResourceManager from INFO to DEBUG. Contributed by Ravi Prakash.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179178 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/11 4:56 AM",
      "commitName": "f24dcb3449c77da665058427bc7fa480cad507fc",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "03/10/11 4:21 PM",
      "commitNameOld": "12743d2169f5a24a9b3be07c9e9dcc3f2f1001f0",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 1.52,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,92 @@\n   assignContainers(Resource clusterResource, SchedulerNode node) {\n \n-    LOG.info(\"DEBUG --- assignContainers:\" +\n-        \" node\u003d\" + node.getHostName() + \n-        \" #applications\u003d\" + activeApplications.size());\n+    if(LOG.isDebugEnabled()) {\n+      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n+        + \" #applications\u003d\" + activeApplications.size());\n+    }\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       SchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return assignReservedContainer(application, node, reservedContainer, \n           clusterResource);\n     }\n     \n     // Try to assign containers to applications in order\n     for (SchedulerApp application : activeApplications) {\n       \n-      LOG.info(\"DEBUG --- pre-assignContainers for application \"\n-          + application.getApplicationId());\n+      if(LOG.isDebugEnabled()) {\n+        LOG.debug(\"pre-assignContainers for application \"\n+        + application.getApplicationId());\n+      }\n       application.showRequests();\n \n       synchronized (application) {\n         Resource userLimit \u003d \n           computeUserLimit(application, clusterResource, Resources.none());\n         setUserResourceLimit(application, userLimit);\n         \n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Are we going over limits by allocating to this application?\n           // Maximum Capacity of the queue\n           if (!assignToQueue(clusterResource, required)) {\n             return Resources.none();\n           }\n \n           // User limits\n           userLimit \u003d \n             computeUserLimit(application, clusterResource, required); \n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           Resource assigned \u003d \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n   \n           // Did we schedule or reserve a container?\n           if (Resources.greaterThan(assigned, Resources.none())) {\n             Resource assignedResource \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n             // Book-keeping\n             allocateResource(clusterResource, \n                 application.getUser(), assignedResource);\n             \n             // Reset scheduling opportunities\n             application.resetSchedulingOpportunities(priority);\n             \n             // Done\n             return assignedResource; \n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n-      LOG.info(\"DEBUG --- post-assignContainers for application \"\n+      if(LOG.isDebugEnabled()) {\n+        LOG.debug(\"post-assignContainers for application \"\n           + application.getApplicationId());\n+      }\n       application.showRequests();\n     }\n   \n     return Resources.none();\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" #applications\u003d\" + activeApplications.size());\n    }\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      SchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return assignReservedContainer(application, node, reservedContainer, \n          clusterResource);\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : activeApplications) {\n      \n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"pre-assignContainers for application \"\n        + application.getApplicationId());\n      }\n      application.showRequests();\n\n      synchronized (application) {\n        Resource userLimit \u003d \n          computeUserLimit(application, clusterResource, Resources.none());\n        setUserResourceLimit(application, userLimit);\n        \n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Are we going over limits by allocating to this application?\n          // Maximum Capacity of the queue\n          if (!assignToQueue(clusterResource, required)) {\n            return Resources.none();\n          }\n\n          // User limits\n          userLimit \u003d \n            computeUserLimit(application, clusterResource, required); \n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          Resource assigned \u003d \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n  \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n            Resource assignedResource \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n            // Book-keeping\n            allocateResource(clusterResource, \n                application.getUser(), assignedResource);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignedResource; \n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"post-assignContainers for application \"\n          + application.getApplicationId());\n      }\n      application.showRequests();\n    }\n  \n    return Resources.none();\n\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "6b608aad7d52b524fa94955a538e8b3524d42d93": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2697. Enhance CapacityScheduler to cap concurrently running applications per-queue \u0026 per-user.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165403 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/09/11 12:49 PM",
      "commitName": "6b608aad7d52b524fa94955a538e8b3524d42d93",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "31/08/11 12:52 PM",
      "commitNameOld": "b9a5fd51904a074a7a33f38266378f0f6f97b531",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 5.0,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,87 @@\n   assignContainers(Resource clusterResource, SchedulerNode node) {\n \n     LOG.info(\"DEBUG --- assignContainers:\" +\n         \" node\u003d\" + node.getHostName() + \n-        \" #applications\u003d\" + applications.size());\n+        \" #applications\u003d\" + activeApplications.size());\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       SchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return assignReservedContainer(application, node, reservedContainer, \n           clusterResource);\n     }\n     \n     // Try to assign containers to applications in order\n-    for (SchedulerApp application : applications) {\n+    for (SchedulerApp application : activeApplications) {\n       \n       LOG.info(\"DEBUG --- pre-assignContainers for application \"\n           + application.getApplicationId());\n       application.showRequests();\n \n       synchronized (application) {\n         Resource userLimit \u003d \n           computeUserLimit(application, clusterResource, Resources.none());\n         setUserResourceLimit(application, userLimit);\n         \n         for (Priority priority : application.getPriorities()) {\n           // Required resource\n           Resource required \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n           if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Are we going over limits by allocating to this application?\n           // Maximum Capacity of the queue\n           if (!assignToQueue(clusterResource, required)) {\n             return Resources.none();\n           }\n \n           // User limits\n           userLimit \u003d \n             computeUserLimit(application, clusterResource, required); \n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           Resource assigned \u003d \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n   \n           // Did we schedule or reserve a container?\n           if (Resources.greaterThan(assigned, Resources.none())) {\n             Resource assignedResource \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n             // Book-keeping\n             allocateResource(clusterResource, \n                 application.getUser(), assignedResource);\n             \n             // Reset scheduling opportunities\n             application.resetSchedulingOpportunities(priority);\n             \n             // Done\n             return assignedResource; \n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       LOG.info(\"DEBUG --- post-assignContainers for application \"\n           + application.getApplicationId());\n       application.showRequests();\n     }\n   \n     return Resources.none();\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    LOG.info(\"DEBUG --- assignContainers:\" +\n        \" node\u003d\" + node.getHostName() + \n        \" #applications\u003d\" + activeApplications.size());\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      SchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return assignReservedContainer(application, node, reservedContainer, \n          clusterResource);\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : activeApplications) {\n      \n      LOG.info(\"DEBUG --- pre-assignContainers for application \"\n          + application.getApplicationId());\n      application.showRequests();\n\n      synchronized (application) {\n        Resource userLimit \u003d \n          computeUserLimit(application, clusterResource, Resources.none());\n        setUserResourceLimit(application, userLimit);\n        \n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Are we going over limits by allocating to this application?\n          // Maximum Capacity of the queue\n          if (!assignToQueue(clusterResource, required)) {\n            return Resources.none();\n          }\n\n          // User limits\n          userLimit \u003d \n            computeUserLimit(application, clusterResource, required); \n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          Resource assigned \u003d \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n  \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n            Resource assignedResource \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n            // Book-keeping\n            allocateResource(clusterResource, \n                application.getUser(), assignedResource);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignedResource; \n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      LOG.info(\"DEBUG --- post-assignContainers for application \"\n          + application.getApplicationId());\n      application.showRequests();\n    }\n  \n    return Resources.none();\n\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "b9a5fd51904a074a7a33f38266378f0f6f97b531": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2917. Fixed corner case in container reservation which led to starvation and hung jobs. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1163768 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/08/11 12:52 PM",
      "commitName": "b9a5fd51904a074a7a33f38266378f0f6f97b531",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 6.82,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,88 +1,87 @@\n   assignContainers(Resource clusterResource, SchedulerNode node) {\n \n     LOG.info(\"DEBUG --- assignContainers:\" +\n         \" node\u003d\" + node.getHostName() + \n         \" #applications\u003d\" + applications.size());\n     \n     // Check for reserved resources\n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       SchedulerApp application \u003d \n           getApplication(reservedContainer.getApplicationAttemptId());\n       return assignReservedContainer(application, node, reservedContainer, \n           clusterResource);\n     }\n     \n     // Try to assign containers to applications in order\n     for (SchedulerApp application : applications) {\n       \n       LOG.info(\"DEBUG --- pre-assignContainers for application \"\n           + application.getApplicationId());\n       application.showRequests();\n \n       synchronized (application) {\n         Resource userLimit \u003d \n           computeUserLimit(application, clusterResource, Resources.none());\n         setUserResourceLimit(application, userLimit);\n         \n         for (Priority priority : application.getPriorities()) {\n+          // Required resource\n+          Resource required \u003d \n+              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n           // Do we need containers at this \u0027priority\u0027?\n-          if (!needContainers(application, priority)) {\n+          if (!needContainers(application, priority, required)) {\n             continue;\n           }\n \n           // Are we going over limits by allocating to this application?\n-          ResourceRequest required \u003d \n-            application.getResourceRequest(priority, RMNode.ANY);\n-\n           // Maximum Capacity of the queue\n-          if (!assignToQueue(clusterResource, required.getCapability())) {\n+          if (!assignToQueue(clusterResource, required)) {\n             return Resources.none();\n           }\n \n           // User limits\n           userLimit \u003d \n-            computeUserLimit(application, clusterResource, \n-                required.getCapability());\n+            computeUserLimit(application, clusterResource, required); \n           if (!assignToUser(application.getUser(), userLimit)) {\n             break; \n           }\n \n           // Inform the application it is about to get a scheduling opportunity\n           application.addSchedulingOpportunity(priority);\n           \n           // Try to schedule\n           Resource assigned \u003d \n             assignContainersOnNode(clusterResource, node, application, priority, \n                 null);\n   \n           // Did we schedule or reserve a container?\n           if (Resources.greaterThan(assigned, Resources.none())) {\n             Resource assignedResource \u003d \n               application.getResourceRequest(priority, RMNode.ANY).getCapability();\n \n             // Book-keeping\n             allocateResource(clusterResource, \n                 application.getUser(), assignedResource);\n             \n             // Reset scheduling opportunities\n             application.resetSchedulingOpportunities(priority);\n             \n             // Done\n             return assignedResource; \n           } else {\n             // Do not assign out of order w.r.t priorities\n             break;\n           }\n         }\n       }\n \n       LOG.info(\"DEBUG --- post-assignContainers for application \"\n           + application.getApplicationId());\n       application.showRequests();\n     }\n   \n     return Resources.none();\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    LOG.info(\"DEBUG --- assignContainers:\" +\n        \" node\u003d\" + node.getHostName() + \n        \" #applications\u003d\" + applications.size());\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      SchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return assignReservedContainer(application, node, reservedContainer, \n          clusterResource);\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : applications) {\n      \n      LOG.info(\"DEBUG --- pre-assignContainers for application \"\n          + application.getApplicationId());\n      application.showRequests();\n\n      synchronized (application) {\n        Resource userLimit \u003d \n          computeUserLimit(application, clusterResource, Resources.none());\n        setUserResourceLimit(application, userLimit);\n        \n        for (Priority priority : application.getPriorities()) {\n          // Required resource\n          Resource required \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority, required)) {\n            continue;\n          }\n\n          // Are we going over limits by allocating to this application?\n          // Maximum Capacity of the queue\n          if (!assignToQueue(clusterResource, required)) {\n            return Resources.none();\n          }\n\n          // User limits\n          userLimit \u003d \n            computeUserLimit(application, clusterResource, required); \n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          Resource assigned \u003d \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n  \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n            Resource assignedResource \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n            // Book-keeping\n            allocateResource(clusterResource, \n                application.getUser(), assignedResource);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignedResource; \n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      LOG.info(\"DEBUG --- post-assignContainers for application \"\n          + application.getApplicationId());\n      application.showRequests();\n    }\n  \n    return Resources.none();\n\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    LOG.info(\"DEBUG --- assignContainers:\" +\n        \" node\u003d\" + node.getHostName() + \n        \" #applications\u003d\" + applications.size());\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      SchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return assignReservedContainer(application, node, reservedContainer, \n          clusterResource);\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : applications) {\n      \n      LOG.info(\"DEBUG --- pre-assignContainers for application \"\n          + application.getApplicationId());\n      application.showRequests();\n\n      synchronized (application) {\n        Resource userLimit \u003d \n          computeUserLimit(application, clusterResource, Resources.none());\n        setUserResourceLimit(application, userLimit);\n        \n        for (Priority priority : application.getPriorities()) {\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority)) {\n            continue;\n          }\n\n          // Are we going over limits by allocating to this application?\n          ResourceRequest required \u003d \n            application.getResourceRequest(priority, RMNode.ANY);\n\n          // Maximum Capacity of the queue\n          if (!assignToQueue(clusterResource, required.getCapability())) {\n            return Resources.none();\n          }\n\n          // User limits\n          userLimit \u003d \n            computeUserLimit(application, clusterResource, \n                required.getCapability());\n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          Resource assigned \u003d \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n  \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n            Resource assignedResource \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n            // Book-keeping\n            allocateResource(clusterResource, \n                application.getUser(), assignedResource);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignedResource; \n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      LOG.info(\"DEBUG --- post-assignContainers for application \"\n          + application.getApplicationId());\n      application.showRequests();\n    }\n  \n    return Resources.none();\n\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
        "newPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,88 @@\n+  assignContainers(Resource clusterResource, SchedulerNode node) {\n+\n+    LOG.info(\"DEBUG --- assignContainers:\" +\n+        \" node\u003d\" + node.getHostName() + \n+        \" #applications\u003d\" + applications.size());\n+    \n+    // Check for reserved resources\n+    RMContainer reservedContainer \u003d node.getReservedContainer();\n+    if (reservedContainer !\u003d null) {\n+      SchedulerApp application \u003d \n+          getApplication(reservedContainer.getApplicationAttemptId());\n+      return assignReservedContainer(application, node, reservedContainer, \n+          clusterResource);\n+    }\n+    \n+    // Try to assign containers to applications in order\n+    for (SchedulerApp application : applications) {\n+      \n+      LOG.info(\"DEBUG --- pre-assignContainers for application \"\n+          + application.getApplicationId());\n+      application.showRequests();\n+\n+      synchronized (application) {\n+        Resource userLimit \u003d \n+          computeUserLimit(application, clusterResource, Resources.none());\n+        setUserResourceLimit(application, userLimit);\n+        \n+        for (Priority priority : application.getPriorities()) {\n+\n+          // Do we need containers at this \u0027priority\u0027?\n+          if (!needContainers(application, priority)) {\n+            continue;\n+          }\n+\n+          // Are we going over limits by allocating to this application?\n+          ResourceRequest required \u003d \n+            application.getResourceRequest(priority, RMNode.ANY);\n+\n+          // Maximum Capacity of the queue\n+          if (!assignToQueue(clusterResource, required.getCapability())) {\n+            return Resources.none();\n+          }\n+\n+          // User limits\n+          userLimit \u003d \n+            computeUserLimit(application, clusterResource, \n+                required.getCapability());\n+          if (!assignToUser(application.getUser(), userLimit)) {\n+            break; \n+          }\n+\n+          // Inform the application it is about to get a scheduling opportunity\n+          application.addSchedulingOpportunity(priority);\n+          \n+          // Try to schedule\n+          Resource assigned \u003d \n+            assignContainersOnNode(clusterResource, node, application, priority, \n+                null);\n+  \n+          // Did we schedule or reserve a container?\n+          if (Resources.greaterThan(assigned, Resources.none())) {\n+            Resource assignedResource \u003d \n+              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n+\n+            // Book-keeping\n+            allocateResource(clusterResource, \n+                application.getUser(), assignedResource);\n+            \n+            // Reset scheduling opportunities\n+            application.resetSchedulingOpportunities(priority);\n+            \n+            // Done\n+            return assignedResource; \n+          } else {\n+            // Do not assign out of order w.r.t priorities\n+            break;\n+          }\n+        }\n+      }\n+\n+      LOG.info(\"DEBUG --- post-assignContainers for application \"\n+          + application.getApplicationId());\n+      application.showRequests();\n+    }\n+  \n+    return Resources.none();\n+\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  assignContainers(Resource clusterResource, SchedulerNode node) {\n\n    LOG.info(\"DEBUG --- assignContainers:\" +\n        \" node\u003d\" + node.getHostName() + \n        \" #applications\u003d\" + applications.size());\n    \n    // Check for reserved resources\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      SchedulerApp application \u003d \n          getApplication(reservedContainer.getApplicationAttemptId());\n      return assignReservedContainer(application, node, reservedContainer, \n          clusterResource);\n    }\n    \n    // Try to assign containers to applications in order\n    for (SchedulerApp application : applications) {\n      \n      LOG.info(\"DEBUG --- pre-assignContainers for application \"\n          + application.getApplicationId());\n      application.showRequests();\n\n      synchronized (application) {\n        Resource userLimit \u003d \n          computeUserLimit(application, clusterResource, Resources.none());\n        setUserResourceLimit(application, userLimit);\n        \n        for (Priority priority : application.getPriorities()) {\n\n          // Do we need containers at this \u0027priority\u0027?\n          if (!needContainers(application, priority)) {\n            continue;\n          }\n\n          // Are we going over limits by allocating to this application?\n          ResourceRequest required \u003d \n            application.getResourceRequest(priority, RMNode.ANY);\n\n          // Maximum Capacity of the queue\n          if (!assignToQueue(clusterResource, required.getCapability())) {\n            return Resources.none();\n          }\n\n          // User limits\n          userLimit \u003d \n            computeUserLimit(application, clusterResource, \n                required.getCapability());\n          if (!assignToUser(application.getUser(), userLimit)) {\n            break; \n          }\n\n          // Inform the application it is about to get a scheduling opportunity\n          application.addSchedulingOpportunity(priority);\n          \n          // Try to schedule\n          Resource assigned \u003d \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null);\n  \n          // Did we schedule or reserve a container?\n          if (Resources.greaterThan(assigned, Resources.none())) {\n            Resource assignedResource \u003d \n              application.getResourceRequest(priority, RMNode.ANY).getCapability();\n\n            // Book-keeping\n            allocateResource(clusterResource, \n                application.getUser(), assignedResource);\n            \n            // Reset scheduling opportunities\n            application.resetSchedulingOpportunities(priority);\n            \n            // Done\n            return assignedResource; \n          } else {\n            // Do not assign out of order w.r.t priorities\n            break;\n          }\n        }\n      }\n\n      LOG.info(\"DEBUG --- post-assignContainers for application \"\n          + application.getApplicationId());\n      application.showRequests();\n    }\n  \n    return Resources.none();\n\n  }",
      "path": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
    }
  }
}