{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WriteCtx.java",
  "functionName": "loadData",
  "functionId": "loadData",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
  "functionStartLine": 229,
  "functionEndLine": 242,
  "numCommitsSeen": 12,
  "timeTaken": 1125,
  "changeHistory": [
    "151fca5032719e561226ef278e002739073c23ec",
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
    "28e3d09230971b32f74284311931525cb7ad1b7c"
  ],
  "changeHistoryShort": {
    "151fca5032719e561226ef278e002739073c23ec": "Ybodychange",
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53": "Ybodychange",
    "28e3d09230971b32f74284311931525cb7ad1b7c": "Yintroduced"
  },
  "changeHistoryDetails": {
    "151fca5032719e561226ef278e002739073c23ec": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9092. Nfs silently drops overlapping write requests and causes data copying to fail. Contributed by Yongjun Zhang.\n",
      "commitDate": "28/09/15 6:45 PM",
      "commitName": "151fca5032719e561226ef278e002739073c23ec",
      "commitAuthor": "Yongjun Zhang",
      "commitDateOld": "11/12/14 3:40 PM",
      "commitNameOld": "f6f2a3f1c73266bfedd802eacde60d8b19b81015",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 291.09,
      "commitsBetweenForRepo": 2202,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,14 @@\n   private void loadData() throws IOException {\n     Preconditions.checkState(data \u003d\u003d null);\n     byte[] rawData \u003d new byte[count];\n     raf.seek(dumpFileOffset);\n     int size \u003d raf.read(rawData, 0, count);\n     if (size !\u003d count) {\n       throw new IOException(\"Data count is \" + count + \", but read back \"\n           + size + \"bytes\");\n     }\n-    data \u003d ByteBuffer.wrap(rawData);\n+    synchronized(this) {\n+      data \u003d ByteBuffer.wrap(rawData);\n+      trimData();\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void loadData() throws IOException {\n    Preconditions.checkState(data \u003d\u003d null);\n    byte[] rawData \u003d new byte[count];\n    raf.seek(dumpFileOffset);\n    int size \u003d raf.read(rawData, 0, count);\n    if (size !\u003d count) {\n      throw new IOException(\"Data count is \" + count + \", but read back \"\n          + size + \"bytes\");\n    }\n    synchronized(this) {\n      data \u003d ByteBuffer.wrap(rawData);\n      trimData();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
      "extendedDetails": {}
    },
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5259. Support client which combines appended data with old data before sends it to NFS server. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529730 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/10/13 7:57 PM",
      "commitName": "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "23/09/13 1:02 PM",
      "commitNameOld": "28e3d09230971b32f74284311931525cb7ad1b7c",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 13.29,
      "commitsBetweenForRepo": 110,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,11 @@\n   private void loadData() throws IOException {\n     Preconditions.checkState(data \u003d\u003d null);\n-    data \u003d new byte[count];\n+    byte[] rawData \u003d new byte[count];\n     raf.seek(dumpFileOffset);\n-    int size \u003d raf.read(data, 0, count);\n+    int size \u003d raf.read(rawData, 0, count);\n     if (size !\u003d count) {\n       throw new IOException(\"Data count is \" + count + \", but read back \"\n           + size + \"bytes\");\n     }\n+    data \u003d ByteBuffer.wrap(rawData);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void loadData() throws IOException {\n    Preconditions.checkState(data \u003d\u003d null);\n    byte[] rawData \u003d new byte[count];\n    raf.seek(dumpFileOffset);\n    int size \u003d raf.read(rawData, 0, count);\n    if (size !\u003d count) {\n      throw new IOException(\"Data count is \" + count + \", but read back \"\n          + size + \"bytes\");\n    }\n    data \u003d ByteBuffer.wrap(rawData);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
      "extendedDetails": {}
    },
    "28e3d09230971b32f74284311931525cb7ad1b7c": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4971. Move IO operations out of locking in OpenFileCtx. Contributed by Jing Zhao and Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525681 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/09/13 1:02 PM",
      "commitName": "28e3d09230971b32f74284311931525cb7ad1b7c",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,10 @@\n+  private void loadData() throws IOException {\n+    Preconditions.checkState(data \u003d\u003d null);\n+    data \u003d new byte[count];\n+    raf.seek(dumpFileOffset);\n+    int size \u003d raf.read(data, 0, count);\n+    if (size !\u003d count) {\n+      throw new IOException(\"Data count is \" + count + \", but read back \"\n+          + size + \"bytes\");\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void loadData() throws IOException {\n    Preconditions.checkState(data \u003d\u003d null);\n    data \u003d new byte[count];\n    raf.seek(dumpFileOffset);\n    int size \u003d raf.read(data, 0, count);\n    if (size !\u003d count) {\n      throw new IOException(\"Data count is \" + count + \", but read back \"\n          + size + \"bytes\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java"
    }
  }
}