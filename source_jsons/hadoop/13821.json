{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "computeReconstructionWorkForBlocks",
  "functionId": "computeReconstructionWorkForBlocks___blocksToReconstruct-List__List__BlockInfo____",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 1986,
  "functionEndLine": 2071,
  "numCommitsSeen": 792,
  "timeTaken": 13292,
  "changeHistory": [
    "8a7c54995a86fe8f63e5eff09d4be240518c8434",
    "fb8932a727f757b2e9c1c61a18145878d0eb77bd",
    "4ca3a6b21a3a25acf16d026c699154047b1f686b",
    "d331762f24b3f22f609366740c9c4f449edc61ac",
    "a2a5d7b5bca715835d92816e7b267b59f7270708",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
    "ae9c61ff0a90b070a5b7b2e7160d726e92c8eacf",
    "73b86a5046fe3262dde7b05be46b18575e35fd5f"
  ],
  "changeHistoryShort": {
    "8a7c54995a86fe8f63e5eff09d4be240518c8434": "Ybodychange",
    "fb8932a727f757b2e9c1c61a18145878d0eb77bd": "Ybodychange",
    "4ca3a6b21a3a25acf16d026c699154047b1f686b": "Ybodychange",
    "d331762f24b3f22f609366740c9c4f449edc61ac": "Ybodychange",
    "a2a5d7b5bca715835d92816e7b267b59f7270708": "Ybodychange",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": "Ybodychange",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ybodychange",
    "a0fb2eff9b71e2e2c0e53262773b34bed82585d4": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
    "ae9c61ff0a90b070a5b7b2e7160d726e92c8eacf": "Ybodychange",
    "73b86a5046fe3262dde7b05be46b18575e35fd5f": "Ybodychange"
  },
  "changeHistoryDetails": {
    "8a7c54995a86fe8f63e5eff09d4be240518c8434": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15159. Prevent adding same DN multiple times in PendingReconstructionBlocks. Contributed by hemanthboyina.\n",
      "commitDate": "15/03/20 3:46 AM",
      "commitName": "8a7c54995a86fe8f63e5eff09d4be240518c8434",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "27/02/20 11:01 AM",
      "commitNameOld": "429da635ec70f9abe5ab71e24c9f2eec0aa36e18",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 16.66,
      "commitsBetweenForRepo": 54,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,86 @@\n   int computeReconstructionWorkForBlocks(\n       List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n     int scheduledWork \u003d 0;\n     List\u003cBlockReconstructionWork\u003e reconWork \u003d new ArrayList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n       synchronized (neededReconstruction) {\n         for (int priority \u003d 0; priority \u003c blocksToReconstruct\n             .size(); priority++) {\n           for (BlockInfo block : blocksToReconstruct.get(priority)) {\n             BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                 priority);\n             if (rw !\u003d null) {\n               reconWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     // Step 2: choose target nodes for each reconstruction task\n     for (BlockReconstructionWork rw : reconWork) {\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n       final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e(rw.getContainingNodes());\n \n+      // Exclude all nodes which already exists as targets for the block\n+      List\u003cDatanodeStorageInfo\u003e targets \u003d\n+          pendingReconstruction.getTargets(rw.getBlock());\n+      if (targets !\u003d null) {\n+        for (DatanodeStorageInfo dn : targets) {\n+          excludedNodes.add(dn.getDatanodeDescriptor());\n+        }\n+      }\n+\n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n       final BlockPlacementPolicy placementPolicy \u003d\n           placementPolicies.getPolicy(rw.getBlock().getBlockType());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n       for (BlockReconstructionWork rw : reconWork) {\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets \u003d\u003d null || targets.length \u003d\u003d 0) {\n           rw.resetTargets();\n           continue;\n         }\n \n         synchronized (neededReconstruction) {\n           if (validateReconstructionWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     if (blockLog.isDebugEnabled()) {\n       // log which blocks have been scheduled for reconstruction\n       for (BlockReconstructionWork rw : reconWork) {\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n             targetList.append(\u0027 \u0027).append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n \n       blockLog.debug(\n           \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n           neededReconstruction.size(), pendingReconstruction.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  int computeReconstructionWorkForBlocks(\n      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockReconstructionWork\u003e reconWork \u003d new ArrayList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReconstruction) {\n        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n            .size(); priority++) {\n          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                priority);\n            if (rw !\u003d null) {\n              reconWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each reconstruction task\n    for (BlockReconstructionWork rw : reconWork) {\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e(rw.getContainingNodes());\n\n      // Exclude all nodes which already exists as targets for the block\n      List\u003cDatanodeStorageInfo\u003e targets \u003d\n          pendingReconstruction.getTargets(rw.getBlock());\n      if (targets !\u003d null) {\n        for (DatanodeStorageInfo dn : targets) {\n          excludedNodes.add(dn.getDatanodeDescriptor());\n        }\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().getBlockType());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for (BlockReconstructionWork rw : reconWork) {\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets \u003d\u003d null || targets.length \u003d\u003d 0) {\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReconstruction) {\n          if (validateReconstructionWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isDebugEnabled()) {\n      // log which blocks have been scheduled for reconstruction\n      for (BlockReconstructionWork rw : reconWork) {\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027).append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n\n      blockLog.debug(\n          \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n          neededReconstruction.size(), pendingReconstruction.size());\n    }\n\n    return scheduledWork;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "fb8932a727f757b2e9c1c61a18145878d0eb77bd": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16029. Consecutive StringBuilder.append can be reused. Contributed by Ayush Saxena.\n",
      "commitDate": "11/01/19 10:54 AM",
      "commitName": "fb8932a727f757b2e9c1c61a18145878d0eb77bd",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "28/11/18 11:25 AM",
      "commitNameOld": "4ca3a6b21a3a25acf16d026c699154047b1f686b",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 43.98,
      "commitsBetweenForRepo": 270,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,77 @@\n   int computeReconstructionWorkForBlocks(\n       List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n     int scheduledWork \u003d 0;\n     List\u003cBlockReconstructionWork\u003e reconWork \u003d new ArrayList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n       synchronized (neededReconstruction) {\n         for (int priority \u003d 0; priority \u003c blocksToReconstruct\n             .size(); priority++) {\n           for (BlockInfo block : blocksToReconstruct.get(priority)) {\n             BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                 priority);\n             if (rw !\u003d null) {\n               reconWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     // Step 2: choose target nodes for each reconstruction task\n     for (BlockReconstructionWork rw : reconWork) {\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n       final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e(rw.getContainingNodes());\n \n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n       final BlockPlacementPolicy placementPolicy \u003d\n           placementPolicies.getPolicy(rw.getBlock().getBlockType());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n       for (BlockReconstructionWork rw : reconWork) {\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets \u003d\u003d null || targets.length \u003d\u003d 0) {\n           rw.resetTargets();\n           continue;\n         }\n \n         synchronized (neededReconstruction) {\n           if (validateReconstructionWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     if (blockLog.isDebugEnabled()) {\n       // log which blocks have been scheduled for reconstruction\n       for (BlockReconstructionWork rw : reconWork) {\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n-            targetList.append(\u0027 \u0027);\n-            targetList.append(target.getDatanodeDescriptor());\n+            targetList.append(\u0027 \u0027).append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n \n       blockLog.debug(\n           \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n           neededReconstruction.size(), pendingReconstruction.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  int computeReconstructionWorkForBlocks(\n      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockReconstructionWork\u003e reconWork \u003d new ArrayList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReconstruction) {\n        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n            .size(); priority++) {\n          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                priority);\n            if (rw !\u003d null) {\n              reconWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each reconstruction task\n    for (BlockReconstructionWork rw : reconWork) {\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e(rw.getContainingNodes());\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().getBlockType());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for (BlockReconstructionWork rw : reconWork) {\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets \u003d\u003d null || targets.length \u003d\u003d 0) {\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReconstruction) {\n          if (validateReconstructionWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isDebugEnabled()) {\n      // log which blocks have been scheduled for reconstruction\n      for (BlockReconstructionWork rw : reconWork) {\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027).append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n\n      blockLog.debug(\n          \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n          neededReconstruction.size(), pendingReconstruction.size());\n    }\n\n    return scheduledWork;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4ca3a6b21a3a25acf16d026c699154047b1f686b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14108. Performance improvement in BlockManager Data Structures. Contributed by Beluga Behr.\n",
      "commitDate": "28/11/18 11:25 AM",
      "commitName": "4ca3a6b21a3a25acf16d026c699154047b1f686b",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "05/11/18 9:38 PM",
      "commitNameOld": "ffc9c50e074aeca804674c6e1e6b0f1eb629e230",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 22.57,
      "commitsBetweenForRepo": 159,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,78 @@\n   int computeReconstructionWorkForBlocks(\n       List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n     int scheduledWork \u003d 0;\n-    List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n+    List\u003cBlockReconstructionWork\u003e reconWork \u003d new ArrayList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n       synchronized (neededReconstruction) {\n         for (int priority \u003d 0; priority \u003c blocksToReconstruct\n             .size(); priority++) {\n           for (BlockInfo block : blocksToReconstruct.get(priority)) {\n             BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                 priority);\n             if (rw !\u003d null) {\n               reconWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     // Step 2: choose target nodes for each reconstruction task\n-    final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n-    for(BlockReconstructionWork rw : reconWork){\n+    for (BlockReconstructionWork rw : reconWork) {\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n-      excludedNodes.clear();\n-      for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n-        excludedNodes.add(dn);\n-      }\n+      final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e(rw.getContainingNodes());\n \n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n       final BlockPlacementPolicy placementPolicy \u003d\n           placementPolicies.getPolicy(rw.getBlock().getBlockType());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n-      for(BlockReconstructionWork rw : reconWork){\n+      for (BlockReconstructionWork rw : reconWork) {\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n-        if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n+        if (targets \u003d\u003d null || targets.length \u003d\u003d 0) {\n           rw.resetTargets();\n           continue;\n         }\n \n         synchronized (neededReconstruction) {\n           if (validateReconstructionWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     if (blockLog.isDebugEnabled()) {\n       // log which blocks have been scheduled for reconstruction\n-      for(BlockReconstructionWork rw : reconWork){\n+      for (BlockReconstructionWork rw : reconWork) {\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n             targetList.append(\u0027 \u0027);\n             targetList.append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n \n       blockLog.debug(\n           \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n           neededReconstruction.size(), pendingReconstruction.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  int computeReconstructionWorkForBlocks(\n      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockReconstructionWork\u003e reconWork \u003d new ArrayList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReconstruction) {\n        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n            .size(); priority++) {\n          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                priority);\n            if (rw !\u003d null) {\n              reconWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each reconstruction task\n    for (BlockReconstructionWork rw : reconWork) {\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e(rw.getContainingNodes());\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().getBlockType());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for (BlockReconstructionWork rw : reconWork) {\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets \u003d\u003d null || targets.length \u003d\u003d 0) {\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReconstruction) {\n          if (validateReconstructionWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isDebugEnabled()) {\n      // log which blocks have been scheduled for reconstruction\n      for (BlockReconstructionWork rw : reconWork) {\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027);\n            targetList.append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n\n      blockLog.debug(\n          \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n          neededReconstruction.size(), pendingReconstruction.size());\n    }\n\n    return scheduledWork;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "d331762f24b3f22f609366740c9c4f449edc61ac": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12832. INode.getFullPathName may throw ArrayIndexOutOfBoundsException lead to NameNode exit. Contribuited by Konstantin Shvachko.",
      "commitDate": "28/11/17 5:14 PM",
      "commitName": "d331762f24b3f22f609366740c9c4f449edc61ac",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "03/11/17 1:16 PM",
      "commitNameOld": "4d2dce40bbe5242953774e6a2fc0dc9111cf5ed0",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 25.21,
      "commitsBetweenForRepo": 215,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,82 @@\n   int computeReconstructionWorkForBlocks(\n       List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n     int scheduledWork \u003d 0;\n     List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n       synchronized (neededReconstruction) {\n         for (int priority \u003d 0; priority \u003c blocksToReconstruct\n             .size(); priority++) {\n           for (BlockInfo block : blocksToReconstruct.get(priority)) {\n             BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                 priority);\n             if (rw !\u003d null) {\n               reconWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     // Step 2: choose target nodes for each reconstruction task\n     final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n     for(BlockReconstructionWork rw : reconWork){\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n       excludedNodes.clear();\n       for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n         excludedNodes.add(dn);\n       }\n \n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n-      // It is costly to extract the filename for which chooseTargets is called,\n-      // so for now we pass in the block collection itself.\n       final BlockPlacementPolicy placementPolicy \u003d\n           placementPolicies.getPolicy(rw.getBlock().getBlockType());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n       for(BlockReconstructionWork rw : reconWork){\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n           rw.resetTargets();\n           continue;\n         }\n \n         synchronized (neededReconstruction) {\n           if (validateReconstructionWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     if (blockLog.isDebugEnabled()) {\n       // log which blocks have been scheduled for reconstruction\n       for(BlockReconstructionWork rw : reconWork){\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n             targetList.append(\u0027 \u0027);\n             targetList.append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n \n       blockLog.debug(\n           \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n           neededReconstruction.size(), pendingReconstruction.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  int computeReconstructionWorkForBlocks(\n      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReconstruction) {\n        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n            .size(); priority++) {\n          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                priority);\n            if (rw !\u003d null) {\n              reconWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each reconstruction task\n    final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n    for(BlockReconstructionWork rw : reconWork){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n        excludedNodes.add(dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().getBlockType());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for(BlockReconstructionWork rw : reconWork){\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReconstruction) {\n          if (validateReconstructionWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isDebugEnabled()) {\n      // log which blocks have been scheduled for reconstruction\n      for(BlockReconstructionWork rw : reconWork){\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027);\n            targetList.append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n\n      blockLog.debug(\n          \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n          neededReconstruction.size(), pendingReconstruction.size());\n    }\n\n    return scheduledWork;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "a2a5d7b5bca715835d92816e7b267b59f7270708": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10759. Change fsimage bool isStriped from boolean to an enum. Contributed by Ewan Higgs.\n",
      "commitDate": "18/01/17 1:31 PM",
      "commitName": "a2a5d7b5bca715835d92816e7b267b59f7270708",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "05/12/16 10:54 AM",
      "commitNameOld": "1b5cceaffbdde50a87ede81552dc380832db8e79",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 44.11,
      "commitsBetweenForRepo": 218,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,84 @@\n   int computeReconstructionWorkForBlocks(\n       List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n     int scheduledWork \u003d 0;\n     List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n       synchronized (neededReconstruction) {\n         for (int priority \u003d 0; priority \u003c blocksToReconstruct\n             .size(); priority++) {\n           for (BlockInfo block : blocksToReconstruct.get(priority)) {\n             BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                 priority);\n             if (rw !\u003d null) {\n               reconWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     // Step 2: choose target nodes for each reconstruction task\n     final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n     for(BlockReconstructionWork rw : reconWork){\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n       excludedNodes.clear();\n       for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n         excludedNodes.add(dn);\n       }\n \n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n       // It is costly to extract the filename for which chooseTargets is called,\n       // so for now we pass in the block collection itself.\n       final BlockPlacementPolicy placementPolicy \u003d\n-          placementPolicies.getPolicy(rw.getBlock().isStriped());\n+          placementPolicies.getPolicy(rw.getBlock().getBlockType());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n       for(BlockReconstructionWork rw : reconWork){\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n           rw.resetTargets();\n           continue;\n         }\n \n         synchronized (neededReconstruction) {\n           if (validateReconstructionWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     if (blockLog.isDebugEnabled()) {\n       // log which blocks have been scheduled for reconstruction\n       for(BlockReconstructionWork rw : reconWork){\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n             targetList.append(\u0027 \u0027);\n             targetList.append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n \n       blockLog.debug(\n           \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n           neededReconstruction.size(), pendingReconstruction.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  int computeReconstructionWorkForBlocks(\n      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReconstruction) {\n        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n            .size(); priority++) {\n          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                priority);\n            if (rw !\u003d null) {\n              reconWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each reconstruction task\n    final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n    for(BlockReconstructionWork rw : reconWork){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n        excludedNodes.add(dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().getBlockType());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for(BlockReconstructionWork rw : reconWork){\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReconstruction) {\n          if (validateReconstructionWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isDebugEnabled()) {\n      // log which blocks have been scheduled for reconstruction\n      for(BlockReconstructionWork rw : reconWork){\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027);\n            targetList.append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n\n      blockLog.debug(\n          \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n          neededReconstruction.size(), pendingReconstruction.size());\n    }\n\n    return scheduledWork;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9869. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-2]. Contributed by Rakesh R.\n",
      "commitDate": "25/04/16 10:01 PM",
      "commitName": "5865fe2bf01284993572ea60b3ec3bf8b4492818",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "17/04/16 6:28 PM",
      "commitNameOld": "67523ffcf491f4f2db5335899c00a174d0caaa9b",
      "commitAuthorOld": "Walter Su",
      "daysBetweenCommits": 8.15,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,84 @@\n   int computeReconstructionWorkForBlocks(\n       List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n     int scheduledWork \u003d 0;\n     List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n       synchronized (neededReconstruction) {\n         for (int priority \u003d 0; priority \u003c blocksToReconstruct\n             .size(); priority++) {\n           for (BlockInfo block : blocksToReconstruct.get(priority)) {\n             BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                 priority);\n             if (rw !\u003d null) {\n               reconWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     // Step 2: choose target nodes for each reconstruction task\n     final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n     for(BlockReconstructionWork rw : reconWork){\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n       excludedNodes.clear();\n       for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n         excludedNodes.add(dn);\n       }\n \n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n       // It is costly to extract the filename for which chooseTargets is called,\n       // so for now we pass in the block collection itself.\n       final BlockPlacementPolicy placementPolicy \u003d\n           placementPolicies.getPolicy(rw.getBlock().isStriped());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n       for(BlockReconstructionWork rw : reconWork){\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n           rw.resetTargets();\n           continue;\n         }\n \n         synchronized (neededReconstruction) {\n           if (validateReconstructionWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     if (blockLog.isDebugEnabled()) {\n       // log which blocks have been scheduled for reconstruction\n       for(BlockReconstructionWork rw : reconWork){\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n             targetList.append(\u0027 \u0027);\n             targetList.append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n \n       blockLog.debug(\n-          \"BLOCK* neededReconstruction \u003d {} pendingReplications \u003d {}\",\n-          neededReconstruction.size(), pendingReplications.size());\n+          \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n+          neededReconstruction.size(), pendingReconstruction.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  int computeReconstructionWorkForBlocks(\n      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReconstruction) {\n        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n            .size(); priority++) {\n          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                priority);\n            if (rw !\u003d null) {\n              reconWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each reconstruction task\n    final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n    for(BlockReconstructionWork rw : reconWork){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n        excludedNodes.add(dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().isStriped());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for(BlockReconstructionWork rw : reconWork){\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReconstruction) {\n          if (validateReconstructionWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isDebugEnabled()) {\n      // log which blocks have been scheduled for reconstruction\n      for(BlockReconstructionWork rw : reconWork){\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027);\n            targetList.append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n\n      blockLog.debug(\n          \"BLOCK* neededReconstruction \u003d {} pendingReconstruction \u003d {}\",\n          neededReconstruction.size(), pendingReconstruction.size());\n    }\n\n    return scheduledWork;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,84 @@\n   int computeReconstructionWorkForBlocks(\n       List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n     int scheduledWork \u003d 0;\n     List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n-      synchronized (neededReplications) {\n+      synchronized (neededReconstruction) {\n         for (int priority \u003d 0; priority \u003c blocksToReconstruct\n             .size(); priority++) {\n           for (BlockInfo block : blocksToReconstruct.get(priority)) {\n             BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                 priority);\n             if (rw !\u003d null) {\n               reconWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     // Step 2: choose target nodes for each reconstruction task\n     final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n     for(BlockReconstructionWork rw : reconWork){\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n       excludedNodes.clear();\n       for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n         excludedNodes.add(dn);\n       }\n \n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n       // It is costly to extract the filename for which chooseTargets is called,\n       // so for now we pass in the block collection itself.\n       final BlockPlacementPolicy placementPolicy \u003d\n           placementPolicies.getPolicy(rw.getBlock().isStriped());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n       for(BlockReconstructionWork rw : reconWork){\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n           rw.resetTargets();\n           continue;\n         }\n \n-        synchronized (neededReplications) {\n+        synchronized (neededReconstruction) {\n           if (validateReconstructionWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     if (blockLog.isDebugEnabled()) {\n-      // log which blocks have been scheduled for replication\n+      // log which blocks have been scheduled for reconstruction\n       for(BlockReconstructionWork rw : reconWork){\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n             targetList.append(\u0027 \u0027);\n             targetList.append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n \n-      blockLog.debug(\"BLOCK* neededReplications \u003d {} pendingReplications \u003d {}\",\n-          neededReplications.size(), pendingReplications.size());\n+      blockLog.debug(\n+          \"BLOCK* neededReconstruction \u003d {} pendingReplications \u003d {}\",\n+          neededReconstruction.size(), pendingReplications.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  int computeReconstructionWorkForBlocks(\n      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReconstruction) {\n        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n            .size(); priority++) {\n          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                priority);\n            if (rw !\u003d null) {\n              reconWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each reconstruction task\n    final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n    for(BlockReconstructionWork rw : reconWork){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n        excludedNodes.add(dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().isStriped());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for(BlockReconstructionWork rw : reconWork){\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReconstruction) {\n          if (validateReconstructionWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isDebugEnabled()) {\n      // log which blocks have been scheduled for reconstruction\n      for(BlockReconstructionWork rw : reconWork){\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027);\n            targetList.append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n\n      blockLog.debug(\n          \"BLOCK* neededReconstruction \u003d {} pendingReplications \u003d {}\",\n          neededReconstruction.size(), pendingReplications.size());\n    }\n\n    return scheduledWork;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "a0fb2eff9b71e2e2c0e53262773b34bed82585d4": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
      "commitDate": "09/02/16 2:43 PM",
      "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
          "commitDate": "09/02/16 2:43 PM",
          "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "02/02/16 11:23 AM",
          "commitNameOld": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 7.14,
          "commitsBetweenForRepo": 51,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,80 +1,83 @@\n-  int computeRecoveryWorkForBlocks(List\u003cList\u003cBlockInfo\u003e\u003e blocksToRecover) {\n+  int computeReconstructionWorkForBlocks(\n+      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n     int scheduledWork \u003d 0;\n-    List\u003cBlockRecoveryWork\u003e recovWork \u003d new LinkedList\u003c\u003e();\n+    List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n       synchronized (neededReplications) {\n-        for (int priority \u003d 0; priority \u003c blocksToRecover.size(); priority++) {\n-          for (BlockInfo block : blocksToRecover.get(priority)) {\n-            BlockRecoveryWork rw \u003d scheduleRecovery(block, priority);\n+        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n+            .size(); priority++) {\n+          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n+            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n+                priority);\n             if (rw !\u003d null) {\n-              recovWork.add(rw);\n+              reconWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n-    // Step 2: choose target nodes for each recovery task\n+    // Step 2: choose target nodes for each reconstruction task\n     final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n-    for(BlockRecoveryWork rw : recovWork){\n+    for(BlockReconstructionWork rw : reconWork){\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n       excludedNodes.clear();\n       for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n         excludedNodes.add(dn);\n       }\n \n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n       // It is costly to extract the filename for which chooseTargets is called,\n       // so for now we pass in the block collection itself.\n       final BlockPlacementPolicy placementPolicy \u003d\n           placementPolicies.getPolicy(rw.getBlock().isStriped());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n-      for(BlockRecoveryWork rw : recovWork){\n+      for(BlockReconstructionWork rw : reconWork){\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n           rw.resetTargets();\n           continue;\n         }\n \n         synchronized (neededReplications) {\n-          if (validateRecoveryWork(rw)) {\n+          if (validateReconstructionWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     if (blockLog.isDebugEnabled()) {\n       // log which blocks have been scheduled for replication\n-      for(BlockRecoveryWork rw : recovWork){\n+      for(BlockReconstructionWork rw : reconWork){\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n             targetList.append(\u0027 \u0027);\n             targetList.append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n \n       blockLog.debug(\"BLOCK* neededReplications \u003d {} pendingReplications \u003d {}\",\n           neededReplications.size(), pendingReplications.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  int computeReconstructionWorkForBlocks(\n      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReplications) {\n        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n            .size(); priority++) {\n          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                priority);\n            if (rw !\u003d null) {\n              reconWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each reconstruction task\n    final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n    for(BlockReconstructionWork rw : reconWork){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n        excludedNodes.add(dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().isStriped());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for(BlockReconstructionWork rw : reconWork){\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReplications) {\n          if (validateReconstructionWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isDebugEnabled()) {\n      // log which blocks have been scheduled for replication\n      for(BlockReconstructionWork rw : reconWork){\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027);\n            targetList.append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n\n      blockLog.debug(\"BLOCK* neededReplications \u003d {} pendingReplications \u003d {}\",\n          neededReplications.size(), pendingReplications.size());\n    }\n\n    return scheduledWork;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "computeRecoveryWorkForBlocks",
            "newValue": "computeReconstructionWorkForBlocks"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
          "commitDate": "09/02/16 2:43 PM",
          "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "02/02/16 11:23 AM",
          "commitNameOld": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 7.14,
          "commitsBetweenForRepo": 51,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,80 +1,83 @@\n-  int computeRecoveryWorkForBlocks(List\u003cList\u003cBlockInfo\u003e\u003e blocksToRecover) {\n+  int computeReconstructionWorkForBlocks(\n+      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n     int scheduledWork \u003d 0;\n-    List\u003cBlockRecoveryWork\u003e recovWork \u003d new LinkedList\u003c\u003e();\n+    List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n       synchronized (neededReplications) {\n-        for (int priority \u003d 0; priority \u003c blocksToRecover.size(); priority++) {\n-          for (BlockInfo block : blocksToRecover.get(priority)) {\n-            BlockRecoveryWork rw \u003d scheduleRecovery(block, priority);\n+        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n+            .size(); priority++) {\n+          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n+            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n+                priority);\n             if (rw !\u003d null) {\n-              recovWork.add(rw);\n+              reconWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n-    // Step 2: choose target nodes for each recovery task\n+    // Step 2: choose target nodes for each reconstruction task\n     final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n-    for(BlockRecoveryWork rw : recovWork){\n+    for(BlockReconstructionWork rw : reconWork){\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n       excludedNodes.clear();\n       for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n         excludedNodes.add(dn);\n       }\n \n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n       // It is costly to extract the filename for which chooseTargets is called,\n       // so for now we pass in the block collection itself.\n       final BlockPlacementPolicy placementPolicy \u003d\n           placementPolicies.getPolicy(rw.getBlock().isStriped());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n-      for(BlockRecoveryWork rw : recovWork){\n+      for(BlockReconstructionWork rw : reconWork){\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n           rw.resetTargets();\n           continue;\n         }\n \n         synchronized (neededReplications) {\n-          if (validateRecoveryWork(rw)) {\n+          if (validateReconstructionWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     if (blockLog.isDebugEnabled()) {\n       // log which blocks have been scheduled for replication\n-      for(BlockRecoveryWork rw : recovWork){\n+      for(BlockReconstructionWork rw : reconWork){\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n             targetList.append(\u0027 \u0027);\n             targetList.append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n \n       blockLog.debug(\"BLOCK* neededReplications \u003d {} pendingReplications \u003d {}\",\n           neededReplications.size(), pendingReplications.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  int computeReconstructionWorkForBlocks(\n      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReplications) {\n        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n            .size(); priority++) {\n          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                priority);\n            if (rw !\u003d null) {\n              reconWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each reconstruction task\n    final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n    for(BlockReconstructionWork rw : reconWork){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n        excludedNodes.add(dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().isStriped());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for(BlockReconstructionWork rw : reconWork){\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReplications) {\n          if (validateReconstructionWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isDebugEnabled()) {\n      // log which blocks have been scheduled for replication\n      for(BlockReconstructionWork rw : reconWork){\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027);\n            targetList.append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n\n      blockLog.debug(\"BLOCK* neededReplications \u003d {} pendingReplications \u003d {}\",\n          neededReplications.size(), pendingReplications.size());\n    }\n\n    return scheduledWork;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[blocksToRecover-List\u003cList\u003cBlockInfo\u003e\u003e]",
            "newValue": "[blocksToReconstruct-List\u003cList\u003cBlockInfo\u003e\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
          "commitDate": "09/02/16 2:43 PM",
          "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "02/02/16 11:23 AM",
          "commitNameOld": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 7.14,
          "commitsBetweenForRepo": 51,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,80 +1,83 @@\n-  int computeRecoveryWorkForBlocks(List\u003cList\u003cBlockInfo\u003e\u003e blocksToRecover) {\n+  int computeReconstructionWorkForBlocks(\n+      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n     int scheduledWork \u003d 0;\n-    List\u003cBlockRecoveryWork\u003e recovWork \u003d new LinkedList\u003c\u003e();\n+    List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n       synchronized (neededReplications) {\n-        for (int priority \u003d 0; priority \u003c blocksToRecover.size(); priority++) {\n-          for (BlockInfo block : blocksToRecover.get(priority)) {\n-            BlockRecoveryWork rw \u003d scheduleRecovery(block, priority);\n+        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n+            .size(); priority++) {\n+          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n+            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n+                priority);\n             if (rw !\u003d null) {\n-              recovWork.add(rw);\n+              reconWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n-    // Step 2: choose target nodes for each recovery task\n+    // Step 2: choose target nodes for each reconstruction task\n     final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n-    for(BlockRecoveryWork rw : recovWork){\n+    for(BlockReconstructionWork rw : reconWork){\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n       excludedNodes.clear();\n       for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n         excludedNodes.add(dn);\n       }\n \n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n       // It is costly to extract the filename for which chooseTargets is called,\n       // so for now we pass in the block collection itself.\n       final BlockPlacementPolicy placementPolicy \u003d\n           placementPolicies.getPolicy(rw.getBlock().isStriped());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n-      for(BlockRecoveryWork rw : recovWork){\n+      for(BlockReconstructionWork rw : reconWork){\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n           rw.resetTargets();\n           continue;\n         }\n \n         synchronized (neededReplications) {\n-          if (validateRecoveryWork(rw)) {\n+          if (validateReconstructionWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     if (blockLog.isDebugEnabled()) {\n       // log which blocks have been scheduled for replication\n-      for(BlockRecoveryWork rw : recovWork){\n+      for(BlockReconstructionWork rw : reconWork){\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n             targetList.append(\u0027 \u0027);\n             targetList.append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n \n       blockLog.debug(\"BLOCK* neededReplications \u003d {} pendingReplications \u003d {}\",\n           neededReplications.size(), pendingReplications.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  int computeReconstructionWorkForBlocks(\n      List\u003cList\u003cBlockInfo\u003e\u003e blocksToReconstruct) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockReconstructionWork\u003e reconWork \u003d new LinkedList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReplications) {\n        for (int priority \u003d 0; priority \u003c blocksToReconstruct\n            .size(); priority++) {\n          for (BlockInfo block : blocksToReconstruct.get(priority)) {\n            BlockReconstructionWork rw \u003d scheduleReconstruction(block,\n                priority);\n            if (rw !\u003d null) {\n              reconWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each reconstruction task\n    final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n    for(BlockReconstructionWork rw : reconWork){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n        excludedNodes.add(dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().isStriped());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for(BlockReconstructionWork rw : reconWork){\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReplications) {\n          if (validateReconstructionWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isDebugEnabled()) {\n      // log which blocks have been scheduled for replication\n      for(BlockReconstructionWork rw : reconWork){\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027);\n            targetList.append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n\n      blockLog.debug(\"BLOCK* neededReplications \u003d {} pendingReplications \u003d {}\",\n          neededReplications.size(), pendingReplications.size());\n    }\n\n    return scheduledWork;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "ae9c61ff0a90b070a5b7b2e7160d726e92c8eacf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9618. Fix mismatch between log level and guard in BlockManager#computeRecoveryWorkForBlocks (iwasakims)\n",
      "commitDate": "21/01/16 9:11 PM",
      "commitName": "ae9c61ff0a90b070a5b7b2e7160d726e92c8eacf",
      "commitAuthor": "Masatake Iwasaki",
      "commitDateOld": "21/01/16 11:13 AM",
      "commitNameOld": "c304890c8c7782d835896859f5b7f60b96c306c0",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.42,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,81 +1,80 @@\n   int computeRecoveryWorkForBlocks(List\u003cList\u003cBlockInfo\u003e\u003e blocksToRecover) {\n     int scheduledWork \u003d 0;\n     List\u003cBlockRecoveryWork\u003e recovWork \u003d new LinkedList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n       synchronized (neededReplications) {\n         for (int priority \u003d 0; priority \u003c blocksToRecover.size(); priority++) {\n           for (BlockInfo block : blocksToRecover.get(priority)) {\n             BlockRecoveryWork rw \u003d scheduleRecovery(block, priority);\n             if (rw !\u003d null) {\n               recovWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     // Step 2: choose target nodes for each recovery task\n     final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n     for(BlockRecoveryWork rw : recovWork){\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n       excludedNodes.clear();\n       for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n         excludedNodes.add(dn);\n       }\n \n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n       // It is costly to extract the filename for which chooseTargets is called,\n       // so for now we pass in the block collection itself.\n       final BlockPlacementPolicy placementPolicy \u003d\n           placementPolicies.getPolicy(rw.getBlock().isStriped());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n       for(BlockRecoveryWork rw : recovWork){\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n           rw.resetTargets();\n           continue;\n         }\n \n         synchronized (neededReplications) {\n           if (validateRecoveryWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n-    if (blockLog.isInfoEnabled()) {\n+    if (blockLog.isDebugEnabled()) {\n       // log which blocks have been scheduled for replication\n       for(BlockRecoveryWork rw : recovWork){\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n             targetList.append(\u0027 \u0027);\n             targetList.append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n-    }\n-    if (blockLog.isDebugEnabled()) {\n+\n       blockLog.debug(\"BLOCK* neededReplications \u003d {} pendingReplications \u003d {}\",\n           neededReplications.size(), pendingReplications.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  int computeRecoveryWorkForBlocks(List\u003cList\u003cBlockInfo\u003e\u003e blocksToRecover) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockRecoveryWork\u003e recovWork \u003d new LinkedList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReplications) {\n        for (int priority \u003d 0; priority \u003c blocksToRecover.size(); priority++) {\n          for (BlockInfo block : blocksToRecover.get(priority)) {\n            BlockRecoveryWork rw \u003d scheduleRecovery(block, priority);\n            if (rw !\u003d null) {\n              recovWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each recovery task\n    final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n    for(BlockRecoveryWork rw : recovWork){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n        excludedNodes.add(dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().isStriped());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for(BlockRecoveryWork rw : recovWork){\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReplications) {\n          if (validateRecoveryWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isDebugEnabled()) {\n      // log which blocks have been scheduled for replication\n      for(BlockRecoveryWork rw : recovWork){\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027);\n            targetList.append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n\n      blockLog.debug(\"BLOCK* neededReplications \u003d {} pendingReplications \u003d {}\",\n          neededReplications.size(), pendingReplications.size());\n    }\n\n    return scheduledWork;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "73b86a5046fe3262dde7b05be46b18575e35fd5f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8988. Use LightWeightHashSet instead of LightWeightLinkedSet in BlockManager#excessReplicateMap. (yliu)\n",
      "commitDate": "11/10/15 11:40 PM",
      "commitName": "73b86a5046fe3262dde7b05be46b18575e35fd5f",
      "commitAuthor": "yliu",
      "commitDateOld": "23/09/15 1:34 PM",
      "commitNameOld": "c09dc258a8f64fab852bf6f26187163480dbee3c",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 18.42,
      "commitsBetweenForRepo": 126,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,81 @@\n   int computeRecoveryWorkForBlocks(List\u003cList\u003cBlockInfo\u003e\u003e blocksToRecover) {\n-    int requiredReplication, numEffectiveReplicas;\n-    List\u003cDatanodeDescriptor\u003e containingNodes;\n-    BlockCollection bc;\n-    int additionalReplRequired;\n-\n     int scheduledWork \u003d 0;\n     List\u003cBlockRecoveryWork\u003e recovWork \u003d new LinkedList\u003c\u003e();\n \n     // Step 1: categorize at-risk blocks into replication and EC tasks\n     namesystem.writeLock();\n     try {\n       synchronized (neededReplications) {\n         for (int priority \u003d 0; priority \u003c blocksToRecover.size(); priority++) {\n           for (BlockInfo block : blocksToRecover.get(priority)) {\n             BlockRecoveryWork rw \u003d scheduleRecovery(block, priority);\n             if (rw !\u003d null) {\n               recovWork.add(rw);\n             }\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     // Step 2: choose target nodes for each recovery task\n     final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n     for(BlockRecoveryWork rw : recovWork){\n       // Exclude all of the containing nodes from being targets.\n       // This list includes decommissioning or corrupt nodes.\n       excludedNodes.clear();\n       for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n         excludedNodes.add(dn);\n       }\n \n       // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n       // It is costly to extract the filename for which chooseTargets is called,\n       // so for now we pass in the block collection itself.\n       final BlockPlacementPolicy placementPolicy \u003d\n           placementPolicies.getPolicy(rw.getBlock().isStriped());\n       rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n     }\n \n     // Step 3: add tasks to the DN\n     namesystem.writeLock();\n     try {\n       for(BlockRecoveryWork rw : recovWork){\n         final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n           rw.resetTargets();\n           continue;\n         }\n \n         synchronized (neededReplications) {\n           if (validateRecoveryWork(rw)) {\n             scheduledWork++;\n           }\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n \n     if (blockLog.isInfoEnabled()) {\n       // log which blocks have been scheduled for replication\n       for(BlockRecoveryWork rw : recovWork){\n         DatanodeStorageInfo[] targets \u003d rw.getTargets();\n         if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n           StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n           for (DatanodeStorageInfo target : targets) {\n             targetList.append(\u0027 \u0027);\n             targetList.append(target.getDatanodeDescriptor());\n           }\n           blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n               rw.getBlock(), targetList);\n         }\n       }\n     }\n     if (blockLog.isDebugEnabled()) {\n       blockLog.debug(\"BLOCK* neededReplications \u003d {} pendingReplications \u003d {}\",\n           neededReplications.size(), pendingReplications.size());\n     }\n \n     return scheduledWork;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  int computeRecoveryWorkForBlocks(List\u003cList\u003cBlockInfo\u003e\u003e blocksToRecover) {\n    int scheduledWork \u003d 0;\n    List\u003cBlockRecoveryWork\u003e recovWork \u003d new LinkedList\u003c\u003e();\n\n    // Step 1: categorize at-risk blocks into replication and EC tasks\n    namesystem.writeLock();\n    try {\n      synchronized (neededReplications) {\n        for (int priority \u003d 0; priority \u003c blocksToRecover.size(); priority++) {\n          for (BlockInfo block : blocksToRecover.get(priority)) {\n            BlockRecoveryWork rw \u003d scheduleRecovery(block, priority);\n            if (rw !\u003d null) {\n              recovWork.add(rw);\n            }\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    // Step 2: choose target nodes for each recovery task\n    final Set\u003cNode\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n    for(BlockRecoveryWork rw : recovWork){\n      // Exclude all of the containing nodes from being targets.\n      // This list includes decommissioning or corrupt nodes.\n      excludedNodes.clear();\n      for (DatanodeDescriptor dn : rw.getContainingNodes()) {\n        excludedNodes.add(dn);\n      }\n\n      // choose replication targets: NOT HOLDING THE GLOBAL LOCK\n      // It is costly to extract the filename for which chooseTargets is called,\n      // so for now we pass in the block collection itself.\n      final BlockPlacementPolicy placementPolicy \u003d\n          placementPolicies.getPolicy(rw.getBlock().isStriped());\n      rw.chooseTargets(placementPolicy, storagePolicySuite, excludedNodes);\n    }\n\n    // Step 3: add tasks to the DN\n    namesystem.writeLock();\n    try {\n      for(BlockRecoveryWork rw : recovWork){\n        final DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if(targets \u003d\u003d null || targets.length \u003d\u003d 0){\n          rw.resetTargets();\n          continue;\n        }\n\n        synchronized (neededReplications) {\n          if (validateRecoveryWork(rw)) {\n            scheduledWork++;\n          }\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n\n    if (blockLog.isInfoEnabled()) {\n      // log which blocks have been scheduled for replication\n      for(BlockRecoveryWork rw : recovWork){\n        DatanodeStorageInfo[] targets \u003d rw.getTargets();\n        if (targets !\u003d null \u0026\u0026 targets.length !\u003d 0) {\n          StringBuilder targetList \u003d new StringBuilder(\"datanode(s)\");\n          for (DatanodeStorageInfo target : targets) {\n            targetList.append(\u0027 \u0027);\n            targetList.append(target.getDatanodeDescriptor());\n          }\n          blockLog.debug(\"BLOCK* ask {} to replicate {} to {}\", rw.getSrcNodes(),\n              rw.getBlock(), targetList);\n        }\n      }\n    }\n    if (blockLog.isDebugEnabled()) {\n      blockLog.debug(\"BLOCK* neededReplications \u003d {} pendingReplications \u003d {}\",\n          neededReplications.size(), pendingReplications.size());\n    }\n\n    return scheduledWork;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    }
  }
}