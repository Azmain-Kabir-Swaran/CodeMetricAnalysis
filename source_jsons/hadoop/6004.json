{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "NamenodeFsck.java",
  "functionName": "copyBlock",
  "functionId": "copyBlock___dfs-DFSClient(modifiers-final)__lblock-LocatedBlock__fos-OutputStream",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
  "functionStartLine": 1047,
  "functionEndLine": 1133,
  "numCommitsSeen": 128,
  "timeTaken": 11529,
  "changeHistory": [
    "99e201dfe2295be830efcc80be34706802da30be",
    "b09cfad43268765f0b35af115b82ddb8ac87a3a4",
    "5d748bd056a32f2c6922514cd0c5b31d866a9919",
    "188f65287d5b2f26a8862c88198f83ac59035016",
    "37e23ce45c592f3c9c48a08a52a5f46787f6c0e9",
    "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "ed78b14ebc9a21bb57ccd088e8b49bfa457a396f",
    "3aac4758b007a56e3d66998d457b2156effca528",
    "f85cc14eb49a46e81d2edcdc1ffe4d0852f193a5",
    "3b54223c0f32d42a84436c670d80b791a8e9696d",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
    "9a4030e0e84a688c12daa21fe9a165808c3eca70",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
    "fab2cbc2c1fa7b592e27a186411dcc4a67ea2bc2",
    "837e17b2eac1471d93e2eff395272063b265fee7",
    "239b2742d0e80d13c970fd062af4930e672fe903",
    "32052a1e3a8007b5348dc42415861aeb859ebc5a",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "21fdf16b0d866dfd9eef22515be5da5f1cd9ac59",
    "be7dd8333a7e56e732171db0781786987de03195",
    "40fe96546fcd68696076db67053f30d38a39a0d5",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "dd86860633d2ed64705b669a75bf318442ed6225",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "99e201dfe2295be830efcc80be34706802da30be": "Ybodychange",
    "b09cfad43268765f0b35af115b82ddb8ac87a3a4": "Ybodychange",
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": "Ybodychange",
    "188f65287d5b2f26a8862c88198f83ac59035016": "Ybodychange",
    "37e23ce45c592f3c9c48a08a52a5f46787f6c0e9": "Ybodychange",
    "e01c6ea688e62f25c4310e771a0cd85b53a5fb87": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "ed78b14ebc9a21bb57ccd088e8b49bfa457a396f": "Ybodychange",
    "3aac4758b007a56e3d66998d457b2156effca528": "Ybodychange",
    "f85cc14eb49a46e81d2edcdc1ffe4d0852f193a5": "Ymultichange(Ybodychange,Yparametermetachange)",
    "3b54223c0f32d42a84436c670d80b791a8e9696d": "Ybodychange",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": "Ybodychange",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": "Ybodychange",
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6": "Ybodychange",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": "Ybodychange",
    "9a4030e0e84a688c12daa21fe9a165808c3eca70": "Ybodychange",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": "Ybodychange",
    "fab2cbc2c1fa7b592e27a186411dcc4a67ea2bc2": "Ybodychange",
    "837e17b2eac1471d93e2eff395272063b265fee7": "Ybodychange",
    "239b2742d0e80d13c970fd062af4930e672fe903": "Ybodychange",
    "32052a1e3a8007b5348dc42415861aeb859ebc5a": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ybodychange",
    "21fdf16b0d866dfd9eef22515be5da5f1cd9ac59": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "40fe96546fcd68696076db67053f30d38a39a0d5": "Ybodychange",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "dd86860633d2ed64705b669a75bf318442ed6225": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "99e201dfe2295be830efcc80be34706802da30be": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14105. Replace TreeSet in NamenodeFsck with HashSet. Contributed by Beluga Behr.\n",
      "commitDate": "30/11/18 11:07 AM",
      "commitName": "99e201dfe2295be830efcc80be34706802da30be",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "30/11/18 10:47 AM",
      "commitNameOld": "b09cfad43268765f0b35af115b82ddb8ac87a3a4",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,87 @@\n   private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n-                         OutputStream fos) throws Exception {\n+      OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n-    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n+    Set\u003cDatanodeInfo\u003e deadNodes \u003d new HashSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null;\n     ExtendedBlock block \u003d lblock.getBlock();\n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n             setLength(block.getNumBytes()).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n             setConfiguration(namenode.getConf()).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n               public Peer newConnectedPeer(InetSocketAddress addr,\n                   Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n                   s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                   s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                   peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                         dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                         blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n \n     long bytesRead \u003d 0L;\n     try {\n       bytesRead \u003d copyBock(blockReader, fos);\n     } catch (Exception e) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock(),\n           e);\n     } finally {\n       blockReader.close();\n     }\n \n     if (bytesRead !\u003d block.getNumBytes()) {\n       throw new IOException(\"Recorded block size is \" + block.getNumBytes()\n           + \", but datanode returned \" + bytesRead + \" bytes\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n      OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    Set\u003cDatanodeInfo\u003e deadNodes \u003d new HashSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null;\n    ExtendedBlock block \u003d lblock.getBlock();\n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n\n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(block.getNumBytes()).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.getConf()).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                  peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n\n    long bytesRead \u003d 0L;\n    try {\n      bytesRead \u003d copyBock(blockReader, fos);\n    } catch (Exception e) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock(),\n          e);\n    } finally {\n      blockReader.close();\n    }\n\n    if (bytesRead !\u003d block.getNumBytes()) {\n      throw new IOException(\"Recorded block size is \" + block.getNumBytes()\n          + \", but datanode returned \" + bytesRead + \" bytes\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "b09cfad43268765f0b35af115b82ddb8ac87a3a4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14106. Refactor NamenodeFsck#copyBlock. Contributed by Beluga Behr.\n",
      "commitDate": "30/11/18 10:47 AM",
      "commitName": "b09cfad43268765f0b35af115b82ddb8ac87a3a4",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "05/11/18 9:38 PM",
      "commitNameOld": "ffc9c50e074aeca804674c6e1e6b0f1eb629e230",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 24.55,
      "commitsBetweenForRepo": 187,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,87 @@\n   private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null;\n     ExtendedBlock block \u003d lblock.getBlock();\n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n             setLength(block.getNumBytes()).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n             setConfiguration(namenode.getConf()).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n               public Peer newConnectedPeer(InetSocketAddress addr,\n                   Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n                   s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                   s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                   peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                         dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                         blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n-    byte[] buf \u003d new byte[1024];\n-    int cnt \u003d 0;\n-    boolean success \u003d true;\n-    long bytesRead \u003d 0;\n+\n+    long bytesRead \u003d 0L;\n     try {\n-      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n-        fos.write(buf, 0, cnt);\n-        bytesRead +\u003d cnt;\n-      }\n-      if ( bytesRead !\u003d block.getNumBytes() ) {\n-        throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n-                              \", but datanode returned \" +bytesRead+\" bytes\");\n-      }\n+      bytesRead \u003d copyBock(blockReader, fos);\n     } catch (Exception e) {\n-      LOG.error(\"Error reading block\", e);\n-      success \u003d false;\n+      throw new Exception(\"Could not copy block data for \" + lblock.getBlock(),\n+          e);\n     } finally {\n       blockReader.close();\n     }\n-    if (!success) {\n-      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n+\n+    if (bytesRead !\u003d block.getNumBytes()) {\n+      throw new IOException(\"Recorded block size is \" + block.getNumBytes()\n+          + \", but datanode returned \" + bytesRead + \" bytes\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null;\n    ExtendedBlock block \u003d lblock.getBlock();\n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n\n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(block.getNumBytes()).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.getConf()).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                  peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n\n    long bytesRead \u003d 0L;\n    try {\n      bytesRead \u003d copyBock(blockReader, fos);\n    } catch (Exception e) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock(),\n          e);\n    } finally {\n      blockReader.close();\n    }\n\n    if (bytesRead !\u003d block.getNumBytes()) {\n      throw new IOException(\"Recorded block size is \" + block.getNumBytes()\n          + \", but datanode returned \" + bytesRead + \" bytes\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13702. Remove HTrace hooks from DFSClient to reduce CPU usage. Contributed by Todd Lipcon.\n",
      "commitDate": "02/07/18 3:11 AM",
      "commitName": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "08/12/17 11:46 AM",
      "commitNameOld": "ef7d334d364378070880e647eaf8bac2f12561ee",
      "commitAuthorOld": "Manoj Govindassamy",
      "daysBetweenCommits": 205.6,
      "commitsBetweenForRepo": 1972,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,95 +1,94 @@\n   private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null;\n     ExtendedBlock block \u003d lblock.getBlock();\n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n             setLength(block.getNumBytes()).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n             setConfiguration(namenode.getConf()).\n-            setTracer(tracer).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n               public Peer newConnectedPeer(InetSocketAddress addr,\n                   Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n                   s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                   s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                   peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                         dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                         blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       blockReader.close();\n     }\n     if (!success) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null;\n    ExtendedBlock block \u003d lblock.getBlock();\n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n\n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(block.getNumBytes()).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.getConf()).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                  peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "188f65287d5b2f26a8862c88198f83ac59035016": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6520. hdfs fsck passes invalid length value when creating BlockReader (Xiao Chen via cmccabe)\n",
      "commitDate": "06/04/16 11:28 AM",
      "commitName": "188f65287d5b2f26a8862c88198f83ac59035016",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "30/03/16 1:37 PM",
      "commitNameOld": "37e23ce45c592f3c9c48a08a52a5f46787f6c0e9",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 6.91,
      "commitsBetweenForRepo": 48,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,95 +1,95 @@\n   private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null;\n     ExtendedBlock block \u003d lblock.getBlock();\n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n-            setLength(-1).\n+            setLength(block.getNumBytes()).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n             setConfiguration(namenode.getConf()).\n             setTracer(tracer).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n               public Peer newConnectedPeer(InetSocketAddress addr,\n                   Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n                   s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                   s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                   peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                         dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                         blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       blockReader.close();\n     }\n     if (!success) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null;\n    ExtendedBlock block \u003d lblock.getBlock();\n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n\n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(block.getNumBytes()).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.getConf()).\n            setTracer(tracer).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                  peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "37e23ce45c592f3c9c48a08a52a5f46787f6c0e9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10223. peerFromSocketAndKey performs SASL exchange before setting connection timeouts (cmccabe)\n",
      "commitDate": "30/03/16 1:37 PM",
      "commitName": "37e23ce45c592f3c9c48a08a52a5f46787f6c0e9",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 19.73,
      "commitsBetweenForRepo": 99,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,95 +1,95 @@\n   private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null;\n     ExtendedBlock block \u003d lblock.getBlock();\n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n             setLength(-1).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n             setConfiguration(namenode.getConf()).\n             setTracer(tracer).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n               public Peer newConnectedPeer(InetSocketAddress addr,\n                   Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n                   s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                   s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                   peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                         dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n-                        blockToken, datanodeId);\n+                        blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       blockReader.close();\n     }\n     if (!success) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null;\n    ExtendedBlock block \u003d lblock.getBlock();\n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n\n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.getConf()).\n            setTracer(tracer).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                  peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "e01c6ea688e62f25c4310e771a0cd85b53a5fb87": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1477. Support reconfiguring dfs.heartbeat.interval and dfs.namenode.heartbeat.recheck-interval without NN restart. (Contributed by Xiaobing Zhou)\n",
      "commitDate": "10/03/16 7:03 PM",
      "commitName": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "24/02/16 7:42 PM",
      "commitNameOld": "6979cbfc1f4c28440816b56f5624765872b0be49",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 14.97,
      "commitsBetweenForRepo": 97,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,95 +1,95 @@\n   private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null;\n     ExtendedBlock block \u003d lblock.getBlock();\n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n             setLength(-1).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n-            setConfiguration(namenode.conf).\n+            setConfiguration(namenode.getConf()).\n             setTracer(tracer).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n               public Peer newConnectedPeer(InetSocketAddress addr,\n                   Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n                   s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                   s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                   peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                         dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                         blockToken, datanodeId);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       blockReader.close();\n     }\n     if (!success) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null;\n    ExtendedBlock block \u003d lblock.getBlock();\n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n\n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.getConf()).\n            setTracer(tracer).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                  peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "17/09/15 2:18 PM",
      "commitNameOld": "9eee97508f350ed4629abb04e7781514ffa04070",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 10.73,
      "commitsBetweenForRepo": 76,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,95 @@\n   private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null;\n     ExtendedBlock block \u003d lblock.getBlock();\n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n             setLength(-1).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n             setConfiguration(namenode.conf).\n+            setTracer(tracer).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n               public Peer newConnectedPeer(InetSocketAddress addr,\n                   Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n                   s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                   s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                   peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                         dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                         blockToken, datanodeId);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       blockReader.close();\n     }\n     if (!success) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null;\n    ExtendedBlock block \u003d lblock.getBlock();\n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n\n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.conf).\n            setTracer(tracer).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                  peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "ed78b14ebc9a21bb57ccd088e8b49bfa457a396f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9002. Move o.a.h.hdfs.net/*Peer classes to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "03/09/15 3:32 PM",
      "commitName": "ed78b14ebc9a21bb57ccd088e8b49bfa457a396f",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "27/08/15 1:03 PM",
      "commitNameOld": "f97a0f8c2cdad0668a3892319f6969fafc2f04cd",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 7.1,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,94 @@\n   private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null;\n     ExtendedBlock block \u003d lblock.getBlock();\n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n             setLength(-1).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n             setConfiguration(namenode.conf).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n               public Peer newConnectedPeer(InetSocketAddress addr,\n                   Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n                   s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                   s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n-                  peer \u003d TcpPeerServer.peerFromSocketAndKey(\n+                  peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                         dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                         blockToken, datanodeId);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       blockReader.close();\n     }\n     if (!success) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null;\n    ExtendedBlock block \u003d lblock.getBlock();\n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n\n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.conf).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                  peer \u003d DFSUtilClient.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "3aac4758b007a56e3d66998d457b2156effca528": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8803. Move DfsClientConf to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "19/08/15 11:28 AM",
      "commitName": "3aac4758b007a56e3d66998d457b2156effca528",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "06/08/15 10:21 AM",
      "commitNameOld": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 13.05,
      "commitsBetweenForRepo": 63,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,94 @@\n   private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null;\n     ExtendedBlock block \u003d lblock.getBlock();\n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n-        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n+        if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n             setLength(-1).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n             setConfiguration(namenode.conf).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n               public Peer newConnectedPeer(InetSocketAddress addr,\n                   Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n-                  s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n-                  s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n+                  s.connect(addr, HdfsConstants.READ_TIMEOUT);\n+                  s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                   peer \u003d TcpPeerServer.peerFromSocketAndKey(\n                         dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                         blockToken, datanodeId);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       blockReader.close();\n     }\n     if (!success) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null;\n    ExtendedBlock block \u003d lblock.getBlock();\n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n\n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.conf).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                  peer \u003d TcpPeerServer.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "f85cc14eb49a46e81d2edcdc1ffe4d0852f193a5": {
      "type": "Ymultichange(Ybodychange,Yparametermetachange)",
      "commitMessage": "HDFS-7073. Allow falling back to a non-SASL connection on DataTransferProtocol in several edge cases. Contributed by Chris Nauroth.\n",
      "commitDate": "19/09/14 9:23 PM",
      "commitName": "f85cc14eb49a46e81d2edcdc1ffe4d0852f193a5",
      "commitAuthor": "cnauroth",
      "subchanges": [
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7073. Allow falling back to a non-SASL connection on DataTransferProtocol in several edge cases. Contributed by Chris Nauroth.\n",
          "commitDate": "19/09/14 9:23 PM",
          "commitName": "f85cc14eb49a46e81d2edcdc1ffe4d0852f193a5",
          "commitAuthor": "cnauroth",
          "commitDateOld": "16/07/14 4:55 PM",
          "commitNameOld": "c477a166e18e122b101c372b1c0a2f362e53866d",
          "commitAuthorOld": "Allen Wittenauer",
          "daysBetweenCommits": 65.19,
          "commitsBetweenForRepo": 626,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,93 +1,94 @@\n-  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n+  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n             setLength(-1).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n             setConfiguration(namenode.conf).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n               public Peer newConnectedPeer(InetSocketAddress addr,\n                   Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n                   s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n                   s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n-                  peer \u003d TcpPeerServer.peerFromSocketAndKey(saslClient, s,\n-                        NamenodeFsck.this, blockToken, datanodeId);\n+                  peer \u003d TcpPeerServer.peerFromSocketAndKey(\n+                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n+                        blockToken, datanodeId);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       blockReader.close();\n     }\n     if (!success) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.conf).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n                  peer \u003d TcpPeerServer.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HDFS-7073. Allow falling back to a non-SASL connection on DataTransferProtocol in several edge cases. Contributed by Chris Nauroth.\n",
          "commitDate": "19/09/14 9:23 PM",
          "commitName": "f85cc14eb49a46e81d2edcdc1ffe4d0852f193a5",
          "commitAuthor": "cnauroth",
          "commitDateOld": "16/07/14 4:55 PM",
          "commitNameOld": "c477a166e18e122b101c372b1c0a2f362e53866d",
          "commitAuthorOld": "Allen Wittenauer",
          "daysBetweenCommits": 65.19,
          "commitsBetweenForRepo": 626,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,93 +1,94 @@\n-  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n+  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n             setLength(-1).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n             setConfiguration(namenode.conf).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n               public Peer newConnectedPeer(InetSocketAddress addr,\n                   Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n                   s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n                   s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n-                  peer \u003d TcpPeerServer.peerFromSocketAndKey(saslClient, s,\n-                        NamenodeFsck.this, blockToken, datanodeId);\n+                  peer \u003d TcpPeerServer.peerFromSocketAndKey(\n+                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n+                        blockToken, datanodeId);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       blockReader.close();\n     }\n     if (!success) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.conf).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n                  peer \u003d TcpPeerServer.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
          "extendedDetails": {
            "oldValue": "[dfs-DFSClient, lblock-LocatedBlock, fos-OutputStream]",
            "newValue": "[dfs-DFSClient(modifiers-final), lblock-LocatedBlock, fos-OutputStream]"
          }
        }
      ]
    },
    "3b54223c0f32d42a84436c670d80b791a8e9696d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/07/14 11:10 AM",
      "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "03/05/14 4:02 AM",
      "commitNameOld": "b2f65c276da2c4420a0974a7e2d75e081abf5d63",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 72.3,
      "commitsBetweenForRepo": 427,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,93 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         String file \u003d BlockReaderFactory.getFileName(targetAddr,\n             block.getBlockPoolId(), block.getBlockId());\n         blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n             setFileName(file).\n             setBlock(block).\n             setBlockToken(lblock.getBlockToken()).\n             setStartOffset(0).\n             setLength(-1).\n             setVerifyChecksum(true).\n             setClientName(\"fsck\").\n             setDatanodeInfo(chosenNode).\n             setInetSocketAddress(targetAddr).\n             setCachingStrategy(CachingStrategy.newDropBehind()).\n             setClientCacheContext(dfs.getClientContext()).\n             setConfiguration(namenode.conf).\n             setRemotePeerFactory(new RemotePeerFactory() {\n               @Override\n-              public Peer newConnectedPeer(InetSocketAddress addr)\n+              public Peer newConnectedPeer(InetSocketAddress addr,\n+                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                   throws IOException {\n                 Peer peer \u003d null;\n                 Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                 try {\n                   s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n                   s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n-                  peer \u003d TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n-                        getDataEncryptionKey());\n+                  peer \u003d TcpPeerServer.peerFromSocketAndKey(saslClient, s,\n+                        NamenodeFsck.this, blockToken, datanodeId);\n                 } finally {\n                   if (peer \u003d\u003d null) {\n                     IOUtils.closeQuietly(s);\n                   }\n                 }\n                 return peer;\n               }\n             }).\n             build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n       }\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       blockReader.close();\n     }\n     if (!success) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.conf).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token\u003cBlockTokenIdentifier\u003e blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n                  peer \u003d TcpPeerServer.peerFromSocketAndKey(saslClient, s,\n                        NamenodeFsck.this, blockToken, datanodeId);\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 11:08 AM",
      "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "02/12/13 8:31 AM",
      "commitNameOld": "13331a6863184d862f1252cb1084e4b1e12f10a0",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 72.11,
      "commitsBetweenForRepo": 410,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,79 +1,92 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n-    Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n-    while (s \u003d\u003d null) {\n+    while (blockReader \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n-        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n-        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n-        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n-        \n-        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n-            block.getBlockId());\n-        blockReader \u003d BlockReaderFactory.newBlockReader(dfs.getConf(),\n-            file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n-            TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n-                getDataEncryptionKey()), chosenNode, null, null, null, \n-                false, CachingStrategy.newDropBehind());\n-        \n+        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n+            block.getBlockPoolId(), block.getBlockId());\n+        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n+            setFileName(file).\n+            setBlock(block).\n+            setBlockToken(lblock.getBlockToken()).\n+            setStartOffset(0).\n+            setLength(-1).\n+            setVerifyChecksum(true).\n+            setClientName(\"fsck\").\n+            setDatanodeInfo(chosenNode).\n+            setInetSocketAddress(targetAddr).\n+            setCachingStrategy(CachingStrategy.newDropBehind()).\n+            setClientCacheContext(dfs.getClientContext()).\n+            setConfiguration(namenode.conf).\n+            setRemotePeerFactory(new RemotePeerFactory() {\n+              @Override\n+              public Peer newConnectedPeer(InetSocketAddress addr)\n+                  throws IOException {\n+                Peer peer \u003d null;\n+                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n+                try {\n+                  s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n+                  s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n+                  peer \u003d TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n+                        getDataEncryptionKey());\n+                } finally {\n+                  if (peer \u003d\u003d null) {\n+                    IOUtils.closeQuietly(s);\n+                  }\n+                }\n+                return peer;\n+              }\n+            }).\n+            build();\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n-        if (s !\u003d null) {\n-          try {\n-            s.close();\n-          } catch (IOException iex) {\n-          }\n-        }\n-        s \u003d null;\n       }\n     }\n-    if (blockReader \u003d\u003d null) {\n-      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n-    }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n-      try {s.close(); } catch (Exception e1) {}\n+      blockReader.close();\n     }\n-    if (!success)\n+    if (!success) {\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (blockReader \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file \u003d BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader \u003d new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.conf).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr)\n                  throws IOException {\n                Peer peer \u003d null;\n                Socket s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n                  peer \u003d TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                        getDataEncryptionKey());\n                } finally {\n                  if (peer \u003d\u003d null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4817.  Make HDFS advisory caching configurable on a per-file basis.  (Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505753 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/13 11:15 AM",
      "commitName": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "21/06/13 5:41 PM",
      "commitNameOld": "1087be1df4707bbadf8b25735513e140dde883bc",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 30.73,
      "commitsBetweenForRepo": 138,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,79 +1,79 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n             block.getBlockId());\n         blockReader \u003d BlockReaderFactory.newBlockReader(dfs.getConf(),\n             file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n             TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n-                getDataEncryptionKey()),\n-            chosenNode, null, null, null, false);\n+                getDataEncryptionKey()), chosenNode, null, null, null, \n+                false, CachingStrategy.newDropBehind());\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(dfs.getConf(),\n            file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n            TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                getDataEncryptionKey()), chosenNode, null, null, null, \n                false, CachingStrategy.newDropBehind());\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4914. Use DFSClient.Conf instead of Configuration.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1494854 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/06/13 9:43 PM",
      "commitName": "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "09/05/13 5:03 PM",
      "commitNameOld": "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 41.19,
      "commitsBetweenForRepo": 285,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,79 +1,79 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n             block.getBlockId());\n-        blockReader \u003d BlockReaderFactory.newBlockReader(\n-            conf, file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n+        blockReader \u003d BlockReaderFactory.newBlockReader(dfs.getConf(),\n+            file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n             TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                 getDataEncryptionKey()),\n             chosenNode, null, null, null, false);\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(dfs.getConf(),\n            file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n            TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                getDataEncryptionKey()),\n            chosenNode, null, null, null, false);\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4661. A few little code cleanups of some HDFS-347-related code. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1480839 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/13 5:03 PM",
      "commitName": "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "11/01/13 3:52 PM",
      "commitNameOld": "9a4030e0e84a688c12daa21fe9a165808c3eca70",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 118.01,
      "commitsBetweenForRepo": 693,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,79 +1,79 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n             block.getBlockId());\n         blockReader \u003d BlockReaderFactory.newBlockReader(\n             conf, file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n             TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                 getDataEncryptionKey()),\n-            chosenNode, null, false);\n+            chosenNode, null, null, null, false);\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(\n            conf, file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n            TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                getDataEncryptionKey()),\n            chosenNode, null, null, null, false);\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "9a4030e0e84a688c12daa21fe9a165808c3eca70": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4356. BlockReaderLocal should use passed file descriptors rather than paths. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1432335 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/01/13 3:52 PM",
      "commitName": "9a4030e0e84a688c12daa21fe9a165808c3eca70",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "09/01/13 1:34 PM",
      "commitNameOld": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 2.1,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,79 +1,79 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n             block.getBlockId());\n         blockReader \u003d BlockReaderFactory.newBlockReader(\n             conf, file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n             TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                 getDataEncryptionKey()),\n-            chosenNode);\n+            chosenNode, null, false);\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(\n            conf, file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n            TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                getDataEncryptionKey()),\n            chosenNode, null, false);\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:34 PM",
      "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "08/01/13 6:41 PM",
      "commitNameOld": "fab2cbc2c1fa7b592e27a186411dcc4a67ea2bc2",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,79 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n             block.getBlockId());\n         blockReader \u003d BlockReaderFactory.newBlockReader(\n-            conf, s, file, block, lblock\n-            .getBlockToken(), 0, -1,\n-            namenode.getRpcServer().getDataEncryptionKey());\n+            conf, file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n+            TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n+                getDataEncryptionKey()),\n+            chosenNode);\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(\n            conf, file, block, lblock.getBlockToken(), 0, -1, true, \"fsck\",\n            TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                getDataEncryptionKey()),\n            chosenNode);\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "fab2cbc2c1fa7b592e27a186411dcc4a67ea2bc2": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c -1428729 . for reverting HDFS-4352. Encapsulate arguments to BlockReaderFactory in a class\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430663 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 6:41 PM",
      "commitName": "fab2cbc2c1fa7b592e27a186411dcc4a67ea2bc2",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/01/13 6:39 PM",
      "commitNameOld": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,78 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n+        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n+            block.getBlockId());\n         blockReader \u003d BlockReaderFactory.newBlockReader(\n-          new BlockReaderFactory.Params(new Conf(conf)).\n-            setSocket(s).setBlock(block).\n-            setFile(BlockReaderFactory.getFileName(targetAddr, \n-                block.getBlockPoolId(), block.getBlockId())).\n-            setBlockToken(lblock.getBlockToken()).\n-            setEncryptionKey(namenode.getRpcServer().getDataEncryptionKey()).\n-            setLen(-1));\n+            conf, s, file, block, lblock\n+            .getBlockToken(), 0, -1,\n+            namenode.getRpcServer().getDataEncryptionKey());\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(\n            conf, s, file, block, lblock\n            .getBlockToken(), 0, -1,\n            namenode.getRpcServer().getDataEncryptionKey());\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "837e17b2eac1471d93e2eff395272063b265fee7": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 6:39 PM",
      "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/01/13 12:44 PM",
      "commitNameOld": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.25,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,80 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         blockReader \u003d BlockReaderFactory.newBlockReader(\n           new BlockReaderFactory.Params(new Conf(conf)).\n-            setPeer(TcpPeerServer.peerFromSocketAndKey(s,\n-                namenode.getRpcServer().getDataEncryptionKey())).\n-            setBlock(block).\n+            setSocket(s).setBlock(block).\n             setFile(BlockReaderFactory.getFileName(targetAddr, \n                 block.getBlockPoolId(), block.getBlockId())).\n             setBlockToken(lblock.getBlockToken()).\n-            setDatanodeID(chosenNode));\n+            setEncryptionKey(namenode.getRpcServer().getDataEncryptionKey()).\n+            setLen(-1));\n+        \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        blockReader \u003d BlockReaderFactory.newBlockReader(\n          new BlockReaderFactory.Params(new Conf(conf)).\n            setSocket(s).setBlock(block).\n            setFile(BlockReaderFactory.getFileName(targetAddr, \n                block.getBlockPoolId(), block.getBlockId())).\n            setBlockToken(lblock.getBlockToken()).\n            setEncryptionKey(namenode.getRpcServer().getDataEncryptionKey()).\n            setLen(-1));\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "239b2742d0e80d13c970fd062af4930e672fe903": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 12:44 PM",
      "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "03/01/13 10:59 PM",
      "commitNameOld": "32052a1e3a8007b5348dc42415861aeb859ebc5a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.57,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,80 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         blockReader \u003d BlockReaderFactory.newBlockReader(\n           new BlockReaderFactory.Params(new Conf(conf)).\n-            setSocket(s).setBlock(block).\n+            setPeer(TcpPeerServer.peerFromSocketAndKey(s,\n+                namenode.getRpcServer().getDataEncryptionKey())).\n+            setBlock(block).\n             setFile(BlockReaderFactory.getFileName(targetAddr, \n                 block.getBlockPoolId(), block.getBlockId())).\n             setBlockToken(lblock.getBlockToken()).\n-            setEncryptionKey(namenode.getRpcServer().getDataEncryptionKey()).\n-            setLen(-1));\n-        \n+            setDatanodeID(chosenNode));\n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        blockReader \u003d BlockReaderFactory.newBlockReader(\n          new BlockReaderFactory.Params(new Conf(conf)).\n            setPeer(TcpPeerServer.peerFromSocketAndKey(s,\n                namenode.getRpcServer().getDataEncryptionKey())).\n            setBlock(block).\n            setFile(BlockReaderFactory.getFileName(targetAddr, \n                block.getBlockPoolId(), block.getBlockId())).\n            setBlockToken(lblock.getBlockToken()).\n            setDatanodeID(chosenNode));\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "32052a1e3a8007b5348dc42415861aeb859ebc5a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4352. Encapsulate arguments to BlockReaderFactory in a class. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1428729 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/01/13 10:59 PM",
      "commitName": "32052a1e3a8007b5348dc42415861aeb859ebc5a",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "07/08/12 9:40 AM",
      "commitNameOld": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 149.6,
      "commitsBetweenForRepo": 789,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,80 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n-        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n-            block.getBlockId());\n         blockReader \u003d BlockReaderFactory.newBlockReader(\n-            conf, s, file, block, lblock\n-            .getBlockToken(), 0, -1,\n-            namenode.getRpcServer().getDataEncryptionKey());\n+          new BlockReaderFactory.Params(new Conf(conf)).\n+            setSocket(s).setBlock(block).\n+            setFile(BlockReaderFactory.getFileName(targetAddr, \n+                block.getBlockPoolId(), block.getBlockId())).\n+            setBlockToken(lblock.getBlockToken()).\n+            setEncryptionKey(namenode.getRpcServer().getDataEncryptionKey()).\n+            setLen(-1));\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        blockReader \u003d BlockReaderFactory.newBlockReader(\n          new BlockReaderFactory.Params(new Conf(conf)).\n            setSocket(s).setBlock(block).\n            setFile(BlockReaderFactory.getFileName(targetAddr, \n                block.getBlockPoolId(), block.getBlockId())).\n            setBlockToken(lblock.getBlockToken()).\n            setEncryptionKey(namenode.getRpcServer().getDataEncryptionKey()).\n            setLen(-1));\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "12/07/12 12:01 PM",
      "commitNameOld": "4a5ba3b7bd2360fd9605863630b477d362874e1e",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 25.9,
      "commitsBetweenForRepo": 133,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,78 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n             block.getBlockId());\n         blockReader \u003d BlockReaderFactory.newBlockReader(\n             conf, s, file, block, lblock\n-            .getBlockToken(), 0, -1);\n+            .getBlockToken(), 0, -1,\n+            namenode.getRpcServer().getDataEncryptionKey());\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(\n            conf, s, file, block, lblock\n            .getBlockToken(), 0, -1,\n            namenode.getRpcServer().getDataEncryptionKey());\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "21fdf16b0d866dfd9eef22515be5da5f1cd9ac59": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3548. NamenodeFsck.copyBlock fails to create a Block Reader. Contributed by Colin Patrick McCabe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1358822 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/07/12 12:41 PM",
      "commitName": "21fdf16b0d866dfd9eef22515be5da5f1cd9ac59",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "11/06/12 6:55 PM",
      "commitNameOld": "543f86631bf07053a045d5dabcad16fb8f9eff97",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 26.74,
      "commitsBetweenForRepo": 119,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,77 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n-          throw new IOException(\"Could not obtain block \" + lblock);\n+          throw new IOException(\"Could not obtain block \" + lblock, ie);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n-        s \u003d new Socket();\n+        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n             block.getBlockId());\n         blockReader \u003d BlockReaderFactory.newBlockReader(\n             conf, s, file, block, lblock\n             .getBlockToken(), 0, -1);\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n-      e.printStackTrace();\n+      LOG.error(\"Error reading block\", e);\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d NetUtils.getDefaultSocketFactory(conf).createSocket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(\n            conf, s, file, block, lblock\n            .getBlockToken(), 0, -1);\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "22/03/12 2:11 PM",
      "commitNameOld": "4feef863721ba88c9cbf4557502e2082dfca7c40",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 10.04,
      "commitsBetweenForRepo": 66,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,77 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n-        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getName());\n+        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d new Socket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n             block.getBlockId());\n         blockReader \u003d BlockReaderFactory.newBlockReader(\n             conf, s, file, block, lblock\n             .getBlockToken(), 0, -1);\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       e.printStackTrace();\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d new Socket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(\n            conf, s, file, block, lblock\n            .getBlockToken(), 0, -1);\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "40fe96546fcd68696076db67053f30d38a39a0d5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2129. Simplify BlockReader to not inherit from FSInputChecker. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196976 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/11/11 11:54 PM",
      "commitName": "40fe96546fcd68696076db67053f30d38a39a0d5",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "05/09/11 5:41 PM",
      "commitNameOld": "b0632df93ae5d00180b21983d960d50a45f8fb7a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 58.26,
      "commitsBetweenForRepo": 492,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,77 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getName());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d new Socket();\n         s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n             block.getBlockId());\n-        blockReader \u003d BlockReaderFactory.newBlockReader(s, file, block, lblock\n-            .getBlockToken(), 0, -1, conf.getInt(\"io.file.buffer.size\", 4096));\n+        blockReader \u003d BlockReaderFactory.newBlockReader(\n+            conf, s, file, block, lblock\n+            .getBlockToken(), 0, -1);\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       e.printStackTrace();\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getName());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d new Socket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(\n            conf, s, file, block, lblock\n            .getBlockToken(), 0, -1);\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1620. Rename HdfsConstants -\u003e HdfsServerConstants, FSConstants -\u003e HdfsConstants. (Harsh J Chouraria via atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165096 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/11 12:30 PM",
      "commitName": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 10.8,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,76 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getName());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d new Socket();\n-        s.connect(targetAddr, HdfsConstants.READ_TIMEOUT);\n-        s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n+        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n+        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         \n         String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n             block.getBlockId());\n         blockReader \u003d BlockReaderFactory.newBlockReader(s, file, block, lblock\n             .getBlockToken(), 0, -1, conf.getInt(\"io.file.buffer.size\", 4096));\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       e.printStackTrace();\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getName());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d new Socket();\n        s.connect(targetAddr, HdfsServerConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(s, file, block, lblock\n            .getBlockToken(), 0, -1, conf.getInt(\"io.file.buffer.size\", 4096));\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getName());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d new Socket();\n        s.connect(targetAddr, HdfsConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(s, file, block, lblock\n            .getBlockToken(), 0, -1, conf.getInt(\"io.file.buffer.size\", 4096));\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getName());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d new Socket();\n        s.connect(targetAddr, HdfsConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(s, file, block, lblock\n            .getBlockToken(), 0, -1, conf.getInt(\"io.file.buffer.size\", 4096));\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java"
      }
    },
    "dd86860633d2ed64705b669a75bf318442ed6225": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2260. Refactor BlockReader into an interface and implementation. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159004 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/08/11 8:02 PM",
      "commitName": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "08/08/11 7:27 AM",
      "commitNameOld": "498e2901036134d6a028e333caa4cc51558dcf5b",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 9.52,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,76 @@\n   private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                          OutputStream fos) throws Exception {\n     int failures \u003d 0;\n     InetSocketAddress targetAddr \u003d null;\n     TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n     Socket s \u003d null;\n     BlockReader blockReader \u003d null; \n     ExtendedBlock block \u003d lblock.getBlock(); \n \n     while (s \u003d\u003d null) {\n       DatanodeInfo chosenNode;\n       \n       try {\n         chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n         targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getName());\n       }  catch (IOException ie) {\n         if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n           throw new IOException(\"Could not obtain block \" + lblock);\n         }\n         LOG.info(\"Could not obtain block from any node:  \" + ie);\n         try {\n           Thread.sleep(10000);\n         }  catch (InterruptedException iex) {\n         }\n         deadNodes.clear();\n         failures++;\n         continue;\n       }\n       try {\n         s \u003d new Socket();\n         s.connect(targetAddr, HdfsConstants.READ_TIMEOUT);\n         s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n         \n-        String file \u003d BlockReader.getFileName(targetAddr, block.getBlockPoolId(),\n+        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n             block.getBlockId());\n-        blockReader \u003d BlockReader.newBlockReader(s, file, block, lblock\n+        blockReader \u003d BlockReaderFactory.newBlockReader(s, file, block, lblock\n             .getBlockToken(), 0, -1, conf.getInt(\"io.file.buffer.size\", 4096));\n         \n       }  catch (IOException ex) {\n         // Put chosen node into dead list, continue\n         LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n         deadNodes.add(chosenNode);\n         if (s !\u003d null) {\n           try {\n             s.close();\n           } catch (IOException iex) {\n           }\n         }\n         s \u003d null;\n       }\n     }\n     if (blockReader \u003d\u003d null) {\n       throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n     }\n     byte[] buf \u003d new byte[1024];\n     int cnt \u003d 0;\n     boolean success \u003d true;\n     long bytesRead \u003d 0;\n     try {\n       while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n         fos.write(buf, 0, cnt);\n         bytesRead +\u003d cnt;\n       }\n       if ( bytesRead !\u003d block.getNumBytes() ) {\n         throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                               \", but datanode returned \" +bytesRead+\" bytes\");\n       }\n     } catch (Exception e) {\n       e.printStackTrace();\n       success \u003d false;\n     } finally {\n       try {s.close(); } catch (Exception e1) {}\n     }\n     if (!success)\n       throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getName());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d new Socket();\n        s.connect(targetAddr, HdfsConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReaderFactory.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReaderFactory.newBlockReader(s, file, block, lblock\n            .getBlockToken(), 0, -1, conf.getInt(\"io.file.buffer.size\", 4096));\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,76 @@\n+  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n+                         OutputStream fos) throws Exception {\n+    int failures \u003d 0;\n+    InetSocketAddress targetAddr \u003d null;\n+    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n+    Socket s \u003d null;\n+    BlockReader blockReader \u003d null; \n+    ExtendedBlock block \u003d lblock.getBlock(); \n+\n+    while (s \u003d\u003d null) {\n+      DatanodeInfo chosenNode;\n+      \n+      try {\n+        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n+        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getName());\n+      }  catch (IOException ie) {\n+        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n+          throw new IOException(\"Could not obtain block \" + lblock);\n+        }\n+        LOG.info(\"Could not obtain block from any node:  \" + ie);\n+        try {\n+          Thread.sleep(10000);\n+        }  catch (InterruptedException iex) {\n+        }\n+        deadNodes.clear();\n+        failures++;\n+        continue;\n+      }\n+      try {\n+        s \u003d new Socket();\n+        s.connect(targetAddr, HdfsConstants.READ_TIMEOUT);\n+        s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n+        \n+        String file \u003d BlockReader.getFileName(targetAddr, block.getBlockPoolId(),\n+            block.getBlockId());\n+        blockReader \u003d BlockReader.newBlockReader(s, file, block, lblock\n+            .getBlockToken(), 0, -1, conf.getInt(\"io.file.buffer.size\", 4096));\n+        \n+      }  catch (IOException ex) {\n+        // Put chosen node into dead list, continue\n+        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n+        deadNodes.add(chosenNode);\n+        if (s !\u003d null) {\n+          try {\n+            s.close();\n+          } catch (IOException iex) {\n+          }\n+        }\n+        s \u003d null;\n+      }\n+    }\n+    if (blockReader \u003d\u003d null) {\n+      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n+    }\n+    byte[] buf \u003d new byte[1024];\n+    int cnt \u003d 0;\n+    boolean success \u003d true;\n+    long bytesRead \u003d 0;\n+    try {\n+      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n+        fos.write(buf, 0, cnt);\n+        bytesRead +\u003d cnt;\n+      }\n+      if ( bytesRead !\u003d block.getNumBytes() ) {\n+        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n+                              \", but datanode returned \" +bytesRead+\" bytes\");\n+      }\n+    } catch (Exception e) {\n+      e.printStackTrace();\n+      success \u003d false;\n+    } finally {\n+      try {s.close(); } catch (Exception e1) {}\n+    }\n+    if (!success)\n+      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures \u003d 0;\n    InetSocketAddress targetAddr \u003d null;\n    TreeSet\u003cDatanodeInfo\u003e deadNodes \u003d new TreeSet\u003cDatanodeInfo\u003e();\n    Socket s \u003d null;\n    BlockReader blockReader \u003d null; \n    ExtendedBlock block \u003d lblock.getBlock(); \n\n    while (s \u003d\u003d null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode \u003d bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr \u003d NetUtils.createSocketAddr(chosenNode.getName());\n      }  catch (IOException ie) {\n        if (failures \u003e\u003d DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        s \u003d new Socket();\n        s.connect(targetAddr, HdfsConstants.READ_TIMEOUT);\n        s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n        \n        String file \u003d BlockReader.getFileName(targetAddr, block.getBlockPoolId(),\n            block.getBlockId());\n        blockReader \u003d BlockReader.newBlockReader(s, file, block, lblock\n            .getBlockToken(), 0, -1, conf.getInt(\"io.file.buffer.size\", 4096));\n        \n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n        if (s !\u003d null) {\n          try {\n            s.close();\n          } catch (IOException iex) {\n          }\n        }\n        s \u003d null;\n      }\n    }\n    if (blockReader \u003d\u003d null) {\n      throw new Exception(\"Could not open data stream for \" + lblock.getBlock());\n    }\n    byte[] buf \u003d new byte[1024];\n    int cnt \u003d 0;\n    boolean success \u003d true;\n    long bytesRead \u003d 0;\n    try {\n      while ((cnt \u003d blockReader.read(buf, 0, buf.length)) \u003e 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead +\u003d cnt;\n      }\n      if ( bytesRead !\u003d block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      success \u003d false;\n    } finally {\n      try {s.close(); } catch (Exception e1) {}\n    }\n    if (!success)\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java"
    }
  }
}