{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "processIncrementalBlockReport",
  "functionId": "processIncrementalBlockReport___nodeID-DatanodeID(modifiers-final)__srdb-StorageReceivedDeletedBlocks(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 4259,
  "functionEndLine": 4279,
  "numCommitsSeen": 598,
  "timeTaken": 15852,
  "changeHistory": [
    "4ab0c8f96a41c573cc1f1e71c18871d243f952b9",
    "f741476146574550a1a208d58ef8be76639e5ddc",
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
    "47ef869fa790dd096b576697c4245d2f3a3193fa",
    "abf833a7b228fff2bca4f69cd9df99d532380038",
    "2d4ae3d18bc530fa9f81ee616db8af3395705fb9",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
    "e226b5b40d716b6d363c43a8783766b72734e347",
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
    "fe8c3dc2b80a2c127e7aed0d3beb41dcfd8f7eac",
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b"
  ],
  "changeHistoryShort": {
    "4ab0c8f96a41c573cc1f1e71c18871d243f952b9": "Ybodychange",
    "f741476146574550a1a208d58ef8be76639e5ddc": "Ybodychange",
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4": "Ybodychange",
    "47ef869fa790dd096b576697c4245d2f3a3193fa": "Ybodychange",
    "abf833a7b228fff2bca4f69cd9df99d532380038": "Ybodychange",
    "2d4ae3d18bc530fa9f81ee616db8af3395705fb9": "Ybodychange",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": "Ybodychange",
    "e226b5b40d716b6d363c43a8783766b72734e347": "Ybodychange",
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": "Ybodychange",
    "fe8c3dc2b80a2c127e7aed0d3beb41dcfd8f7eac": "Ybodychange",
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "4ab0c8f96a41c573cc1f1e71c18871d243f952b9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12619. Do not catch and throw unchecked exceptions if IBRs fail to process. Contributed by Wei-Chiu Chuang.\n",
      "commitDate": "19/10/17 6:17 AM",
      "commitName": "4ab0c8f96a41c573cc1f1e71c18871d243f952b9",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "10/10/17 2:23 PM",
      "commitNameOld": "78af6cdc5359404139665d81447f28d26b7bb43b",
      "commitAuthorOld": "Manoj Govindassamy",
      "daysBetweenCommits": 8.66,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,21 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n       final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n     if (node \u003d\u003d null || !node.isRegistered()) {\n       blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n               + \" is received from dead or unregistered node {}\", nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n+\n+    boolean successful \u003d false;\n     try {\n       processIncrementalBlockReport(node, srdb);\n-    } catch (Exception ex) {\n-      node.setForceRegistration(true);\n-      throw ex;\n+      successful \u003d true;\n+    } finally {\n+      if (!successful) {\n+        node.setForceRegistration(true);\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isRegistered()) {\n      blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node {}\", nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n\n    boolean successful \u003d false;\n    try {\n      processIncrementalBlockReport(node, srdb);\n      successful \u003d true;\n    } finally {\n      if (!successful) {\n        node.setForceRegistration(true);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "f741476146574550a1a208d58ef8be76639e5ddc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9198. Coalesce IBR processing in the NN. (Daryn Sharp via umamahesh)\n",
      "commitDate": "16/12/15 6:16 PM",
      "commitName": "f741476146574550a1a208d58ef8be76639e5ddc",
      "commitAuthor": "Uma Mahesh",
      "commitDateOld": "15/12/15 10:47 AM",
      "commitNameOld": "8602692338d6f493647205e0241e4116211fab75",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.31,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,17 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n       final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n-    int received \u003d 0;\n-    int deleted \u003d 0;\n-    int receiving \u003d 0;\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-    if (node \u003d\u003d null || !node.isAlive()) {\n+    if (node \u003d\u003d null || !node.isRegistered()) {\n       blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n               + \" is received from dead or unregistered node {}\", nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n-\n-    DatanodeStorageInfo storageInfo \u003d\n-        node.getStorageInfo(srdb.getStorage().getStorageID());\n-    if (storageInfo \u003d\u003d null) {\n-      // The DataNode is reporting an unknown storage. Usually the NN learns\n-      // about new storages from heartbeats but during NN restart we may\n-      // receive a block report or incremental report before the heartbeat.\n-      // We must handle this for protocol compatibility. This issue was\n-      // uncovered by HDFS-6094.\n-      storageInfo \u003d node.updateStorage(srdb.getStorage());\n+    try {\n+      processIncrementalBlockReport(node, srdb);\n+    } catch (Exception ex) {\n+      node.setForceRegistration(true);\n+      throw ex;\n     }\n-\n-    for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n-      switch (rdbi.getStatus()) {\n-      case DELETED_BLOCK:\n-        removeStoredBlock(storageInfo, rdbi.getBlock(), node);\n-        deleted++;\n-        break;\n-      case RECEIVED_BLOCK:\n-        addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n-        received++;\n-        break;\n-      case RECEIVING_BLOCK:\n-        receiving++;\n-        processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n-                                      ReplicaState.RBW, null);\n-        break;\n-      default:\n-        String msg \u003d \n-          \"Unknown block status code reported by \" + nodeID +\n-          \": \" + rdbi;\n-        blockLog.warn(msg);\n-        assert false : msg; // if assertions are enabled, throw.\n-        break;\n-      }\n-      blockLog.debug(\"BLOCK* block {}: {} is received from {}\",\n-          rdbi.getStatus(), rdbi.getBlock(), nodeID);\n-    }\n-    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: from \"\n-            + \"{} receiving: {}, received: {}, deleted: {}\", nodeID, receiving,\n-        received, deleted);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isRegistered()) {\n      blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node {}\", nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n    try {\n      processIncrementalBlockReport(node, srdb);\n    } catch (Exception ex) {\n      node.setForceRegistration(true);\n      throw ex;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9223. Code cleanup for DatanodeDescriptor and HeartbeatManager. Contributed by Jing Zhao.\n",
      "commitDate": "14/10/15 4:17 PM",
      "commitName": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "13/10/15 11:00 PM",
      "commitNameOld": "2a987243423eb5c7e191de2ba969b7591a441c70",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.72,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n       final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n     int received \u003d 0;\n     int deleted \u003d 0;\n     int receiving \u003d 0;\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-    if (node \u003d\u003d null || !node.isAlive) {\n+    if (node \u003d\u003d null || !node.isAlive()) {\n       blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n               + \" is received from dead or unregistered node {}\", nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n \n     DatanodeStorageInfo storageInfo \u003d\n         node.getStorageInfo(srdb.getStorage().getStorageID());\n     if (storageInfo \u003d\u003d null) {\n       // The DataNode is reporting an unknown storage. Usually the NN learns\n       // about new storages from heartbeats but during NN restart we may\n       // receive a block report or incremental report before the heartbeat.\n       // We must handle this for protocol compatibility. This issue was\n       // uncovered by HDFS-6094.\n       storageInfo \u003d node.updateStorage(srdb.getStorage());\n     }\n \n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n         removeStoredBlock(storageInfo, rdbi.getBlock(), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:\n         addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n         received++;\n         break;\n       case RECEIVING_BLOCK:\n         receiving++;\n         processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                       ReplicaState.RBW, null);\n         break;\n       default:\n         String msg \u003d \n           \"Unknown block status code reported by \" + nodeID +\n           \": \" + rdbi;\n         blockLog.warn(msg);\n         assert false : msg; // if assertions are enabled, throw.\n         break;\n       }\n       blockLog.debug(\"BLOCK* block {}: {} is received from {}\",\n           rdbi.getStatus(), rdbi.getBlock(), nodeID);\n     }\n     blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: from \"\n             + \"{} receiving: {}, received: {}, deleted: {}\", nodeID, receiving,\n         received, deleted);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    int received \u003d 0;\n    int deleted \u003d 0;\n    int receiving \u003d 0;\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isAlive()) {\n      blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node {}\", nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n\n    DatanodeStorageInfo storageInfo \u003d\n        node.getStorageInfo(srdb.getStorage().getStorageID());\n    if (storageInfo \u003d\u003d null) {\n      // The DataNode is reporting an unknown storage. Usually the NN learns\n      // about new storages from heartbeats but during NN restart we may\n      // receive a block report or incremental report before the heartbeat.\n      // We must handle this for protocol compatibility. This issue was\n      // uncovered by HDFS-6094.\n      storageInfo \u003d node.updateStorage(srdb.getStorage());\n    }\n\n    for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n      switch (rdbi.getStatus()) {\n      case DELETED_BLOCK:\n        removeStoredBlock(storageInfo, rdbi.getBlock(), node);\n        deleted++;\n        break;\n      case RECEIVED_BLOCK:\n        addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n        received++;\n        break;\n      case RECEIVING_BLOCK:\n        receiving++;\n        processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                      ReplicaState.RBW, null);\n        break;\n      default:\n        String msg \u003d \n          \"Unknown block status code reported by \" + nodeID +\n          \": \" + rdbi;\n        blockLog.warn(msg);\n        assert false : msg; // if assertions are enabled, throw.\n        break;\n      }\n      blockLog.debug(\"BLOCK* block {}: {} is received from {}\",\n          rdbi.getStatus(), rdbi.getBlock(), nodeID);\n    }\n    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: from \"\n            + \"{} receiving: {}, received: {}, deleted: {}\", nodeID, receiving,\n        received, deleted);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "47ef869fa790dd096b576697c4245d2f3a3193fa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8428. Erasure Coding: Fix the NullPointerException when deleting file. Contributed by Yi Liu.\n",
      "commitDate": "26/05/15 12:02 PM",
      "commitName": "47ef869fa790dd096b576697c4245d2f3a3193fa",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 12:02 PM",
      "commitNameOld": "91c81fdc24709b3caf1f6281c8879ffee08db956",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n       final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n     int received \u003d 0;\n     int deleted \u003d 0;\n     int receiving \u003d 0;\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n     if (node \u003d\u003d null || !node.isAlive) {\n       blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n               + \" is received from dead or unregistered node {}\", nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n \n     DatanodeStorageInfo storageInfo \u003d\n         node.getStorageInfo(srdb.getStorage().getStorageID());\n     if (storageInfo \u003d\u003d null) {\n       // The DataNode is reporting an unknown storage. Usually the NN learns\n       // about new storages from heartbeats but during NN restart we may\n       // receive a block report or incremental report before the heartbeat.\n       // We must handle this for protocol compatibility. This issue was\n       // uncovered by HDFS-6094.\n       storageInfo \u003d node.updateStorage(srdb.getStorage());\n     }\n \n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n-        removeStoredBlock(storageInfo, getStoredBlock(rdbi.getBlock()), node);\n+        removeStoredBlock(storageInfo, rdbi.getBlock(), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:\n         addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n         received++;\n         break;\n       case RECEIVING_BLOCK:\n         receiving++;\n         processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                       ReplicaState.RBW, null);\n         break;\n       default:\n         String msg \u003d \n           \"Unknown block status code reported by \" + nodeID +\n           \": \" + rdbi;\n         blockLog.warn(msg);\n         assert false : msg; // if assertions are enabled, throw.\n         break;\n       }\n       blockLog.debug(\"BLOCK* block {}: {} is received from {}\",\n           rdbi.getStatus(), rdbi.getBlock(), nodeID);\n     }\n     blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: from \"\n             + \"{} receiving: {}, received: {}, deleted: {}\", nodeID, receiving,\n         received, deleted);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    int received \u003d 0;\n    int deleted \u003d 0;\n    int receiving \u003d 0;\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isAlive) {\n      blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node {}\", nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n\n    DatanodeStorageInfo storageInfo \u003d\n        node.getStorageInfo(srdb.getStorage().getStorageID());\n    if (storageInfo \u003d\u003d null) {\n      // The DataNode is reporting an unknown storage. Usually the NN learns\n      // about new storages from heartbeats but during NN restart we may\n      // receive a block report or incremental report before the heartbeat.\n      // We must handle this for protocol compatibility. This issue was\n      // uncovered by HDFS-6094.\n      storageInfo \u003d node.updateStorage(srdb.getStorage());\n    }\n\n    for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n      switch (rdbi.getStatus()) {\n      case DELETED_BLOCK:\n        removeStoredBlock(storageInfo, rdbi.getBlock(), node);\n        deleted++;\n        break;\n      case RECEIVED_BLOCK:\n        addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n        received++;\n        break;\n      case RECEIVING_BLOCK:\n        receiving++;\n        processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                      ReplicaState.RBW, null);\n        break;\n      default:\n        String msg \u003d \n          \"Unknown block status code reported by \" + nodeID +\n          \": \" + rdbi;\n        blockLog.warn(msg);\n        assert false : msg; // if assertions are enabled, throw.\n        break;\n      }\n      blockLog.debug(\"BLOCK* block {}: {} is received from {}\",\n          rdbi.getStatus(), rdbi.getBlock(), nodeID);\n    }\n    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: from \"\n            + \"{} receiving: {}, received: {}, deleted: {}\", nodeID, receiving,\n        received, deleted);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "abf833a7b228fff2bca4f69cd9df99d532380038": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7907. Erasure Coding: track invalid, corrupt, and under-recovery striped blocks in NameNode. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:43 AM",
      "commitName": "abf833a7b228fff2bca4f69cd9df99d532380038",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:43 AM",
      "commitNameOld": "ea2e60fbcc79c65ec571224bd3f57c262a5d9114",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n       final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n     int received \u003d 0;\n     int deleted \u003d 0;\n     int receiving \u003d 0;\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n     if (node \u003d\u003d null || !node.isAlive) {\n       blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n               + \" is received from dead or unregistered node {}\", nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n \n     DatanodeStorageInfo storageInfo \u003d\n         node.getStorageInfo(srdb.getStorage().getStorageID());\n     if (storageInfo \u003d\u003d null) {\n       // The DataNode is reporting an unknown storage. Usually the NN learns\n       // about new storages from heartbeats but during NN restart we may\n       // receive a block report or incremental report before the heartbeat.\n       // We must handle this for protocol compatibility. This issue was\n       // uncovered by HDFS-6094.\n       storageInfo \u003d node.updateStorage(srdb.getStorage());\n     }\n \n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n-        removeStoredBlock(storageInfo, rdbi.getBlock(), node);\n+        removeStoredBlock(storageInfo, getStoredBlock(rdbi.getBlock()), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:\n         addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n         received++;\n         break;\n       case RECEIVING_BLOCK:\n         receiving++;\n         processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                       ReplicaState.RBW, null);\n         break;\n       default:\n         String msg \u003d \n           \"Unknown block status code reported by \" + nodeID +\n           \": \" + rdbi;\n         blockLog.warn(msg);\n         assert false : msg; // if assertions are enabled, throw.\n         break;\n       }\n       blockLog.debug(\"BLOCK* block {}: {} is received from {}\",\n           rdbi.getStatus(), rdbi.getBlock(), nodeID);\n     }\n     blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: from \"\n             + \"{} receiving: {}, received: {}, deleted: {}\", nodeID, receiving,\n         received, deleted);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    int received \u003d 0;\n    int deleted \u003d 0;\n    int receiving \u003d 0;\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isAlive) {\n      blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node {}\", nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n\n    DatanodeStorageInfo storageInfo \u003d\n        node.getStorageInfo(srdb.getStorage().getStorageID());\n    if (storageInfo \u003d\u003d null) {\n      // The DataNode is reporting an unknown storage. Usually the NN learns\n      // about new storages from heartbeats but during NN restart we may\n      // receive a block report or incremental report before the heartbeat.\n      // We must handle this for protocol compatibility. This issue was\n      // uncovered by HDFS-6094.\n      storageInfo \u003d node.updateStorage(srdb.getStorage());\n    }\n\n    for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n      switch (rdbi.getStatus()) {\n      case DELETED_BLOCK:\n        removeStoredBlock(storageInfo, getStoredBlock(rdbi.getBlock()), node);\n        deleted++;\n        break;\n      case RECEIVED_BLOCK:\n        addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n        received++;\n        break;\n      case RECEIVING_BLOCK:\n        receiving++;\n        processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                      ReplicaState.RBW, null);\n        break;\n      default:\n        String msg \u003d \n          \"Unknown block status code reported by \" + nodeID +\n          \": \" + rdbi;\n        blockLog.warn(msg);\n        assert false : msg; // if assertions are enabled, throw.\n        break;\n      }\n      blockLog.debug(\"BLOCK* block {}: {} is received from {}\",\n          rdbi.getStatus(), rdbi.getBlock(), nodeID);\n    }\n    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: from \"\n            + \"{} receiving: {}, received: {}, deleted: {}\", nodeID, receiving,\n        received, deleted);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "2d4ae3d18bc530fa9f81ee616db8af3395705fb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8245. Standby namenode doesn\u0027t process DELETED_BLOCK if the addblock request is in edit log. Contributed by Rushabh S Shah.\n",
      "commitDate": "08/05/15 2:36 PM",
      "commitName": "2d4ae3d18bc530fa9f81ee616db8af3395705fb9",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "07/05/15 11:36 AM",
      "commitNameOld": "f9427f1760cce7e0befc3e066cebd0912652a411",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 1.13,
      "commitsBetweenForRepo": 60,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n       final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n     int received \u003d 0;\n     int deleted \u003d 0;\n     int receiving \u003d 0;\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n     if (node \u003d\u003d null || !node.isAlive) {\n       blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n               + \" is received from dead or unregistered node {}\", nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n \n     DatanodeStorageInfo storageInfo \u003d\n         node.getStorageInfo(srdb.getStorage().getStorageID());\n     if (storageInfo \u003d\u003d null) {\n       // The DataNode is reporting an unknown storage. Usually the NN learns\n       // about new storages from heartbeats but during NN restart we may\n       // receive a block report or incremental report before the heartbeat.\n       // We must handle this for protocol compatibility. This issue was\n       // uncovered by HDFS-6094.\n       storageInfo \u003d node.updateStorage(srdb.getStorage());\n     }\n \n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n-        removeStoredBlock(rdbi.getBlock(), node);\n+        removeStoredBlock(storageInfo, rdbi.getBlock(), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:\n         addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n         received++;\n         break;\n       case RECEIVING_BLOCK:\n         receiving++;\n         processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                       ReplicaState.RBW, null);\n         break;\n       default:\n         String msg \u003d \n           \"Unknown block status code reported by \" + nodeID +\n           \": \" + rdbi;\n         blockLog.warn(msg);\n         assert false : msg; // if assertions are enabled, throw.\n         break;\n       }\n       blockLog.debug(\"BLOCK* block {}: {} is received from {}\",\n           rdbi.getStatus(), rdbi.getBlock(), nodeID);\n     }\n     blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: from \"\n             + \"{} receiving: {}, received: {}, deleted: {}\", nodeID, receiving,\n         received, deleted);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    int received \u003d 0;\n    int deleted \u003d 0;\n    int receiving \u003d 0;\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isAlive) {\n      blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node {}\", nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n\n    DatanodeStorageInfo storageInfo \u003d\n        node.getStorageInfo(srdb.getStorage().getStorageID());\n    if (storageInfo \u003d\u003d null) {\n      // The DataNode is reporting an unknown storage. Usually the NN learns\n      // about new storages from heartbeats but during NN restart we may\n      // receive a block report or incremental report before the heartbeat.\n      // We must handle this for protocol compatibility. This issue was\n      // uncovered by HDFS-6094.\n      storageInfo \u003d node.updateStorage(srdb.getStorage());\n    }\n\n    for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n      switch (rdbi.getStatus()) {\n      case DELETED_BLOCK:\n        removeStoredBlock(storageInfo, rdbi.getBlock(), node);\n        deleted++;\n        break;\n      case RECEIVED_BLOCK:\n        addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n        received++;\n        break;\n      case RECEIVING_BLOCK:\n        receiving++;\n        processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                      ReplicaState.RBW, null);\n        break;\n      default:\n        String msg \u003d \n          \"Unknown block status code reported by \" + nodeID +\n          \": \" + rdbi;\n        blockLog.warn(msg);\n        assert false : msg; // if assertions are enabled, throw.\n        break;\n      }\n      blockLog.debug(\"BLOCK* block {}: {} is received from {}\",\n          rdbi.getStatus(), rdbi.getBlock(), nodeID);\n    }\n    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: from \"\n            + \"{} receiving: {}, received: {}, deleted: {}\", nodeID, receiving,\n        received, deleted);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7712. Switch blockStateChangeLog to use slf4j.\n",
      "commitDate": "03/02/15 3:01 PM",
      "commitName": "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/01/15 11:33 AM",
      "commitNameOld": "951b3608a8cb1d9063b9be9c740b524c137b816f",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.14,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,55 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n       final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n     int received \u003d 0;\n     int deleted \u003d 0;\n     int receiving \u003d 0;\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n     if (node \u003d\u003d null || !node.isAlive) {\n-      blockLog\n-          .warn(\"BLOCK* processIncrementalBlockReport\"\n-              + \" is received from dead or unregistered node \"\n-              + nodeID);\n+      blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n+              + \" is received from dead or unregistered node {}\", nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n \n     DatanodeStorageInfo storageInfo \u003d\n         node.getStorageInfo(srdb.getStorage().getStorageID());\n     if (storageInfo \u003d\u003d null) {\n       // The DataNode is reporting an unknown storage. Usually the NN learns\n       // about new storages from heartbeats but during NN restart we may\n       // receive a block report or incremental report before the heartbeat.\n       // We must handle this for protocol compatibility. This issue was\n       // uncovered by HDFS-6094.\n       storageInfo \u003d node.updateStorage(srdb.getStorage());\n     }\n \n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n         removeStoredBlock(rdbi.getBlock(), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:\n         addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n         received++;\n         break;\n       case RECEIVING_BLOCK:\n         receiving++;\n         processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                       ReplicaState.RBW, null);\n         break;\n       default:\n         String msg \u003d \n           \"Unknown block status code reported by \" + nodeID +\n           \": \" + rdbi;\n         blockLog.warn(msg);\n         assert false : msg; // if assertions are enabled, throw.\n         break;\n       }\n-      if (blockLog.isDebugEnabled()) {\n-        blockLog.debug(\"BLOCK* block \"\n-            + (rdbi.getStatus()) + \": \" + rdbi.getBlock()\n-            + \" is received from \" + nodeID);\n-      }\n+      blockLog.debug(\"BLOCK* block {}: {} is received from {}\",\n+          rdbi.getStatus(), rdbi.getBlock(), nodeID);\n     }\n-    if (blockLog.isDebugEnabled()) {\n-      blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n-        + nodeID + \" receiving: \" + receiving + \", \" + \" received: \" + received\n-        + \", \" + \" deleted: \" + deleted);\n-    }\n+    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: from \"\n+            + \"{} receiving: {}, received: {}, deleted: {}\", nodeID, receiving,\n+        received, deleted);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    int received \u003d 0;\n    int deleted \u003d 0;\n    int receiving \u003d 0;\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isAlive) {\n      blockLog.warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node {}\", nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n\n    DatanodeStorageInfo storageInfo \u003d\n        node.getStorageInfo(srdb.getStorage().getStorageID());\n    if (storageInfo \u003d\u003d null) {\n      // The DataNode is reporting an unknown storage. Usually the NN learns\n      // about new storages from heartbeats but during NN restart we may\n      // receive a block report or incremental report before the heartbeat.\n      // We must handle this for protocol compatibility. This issue was\n      // uncovered by HDFS-6094.\n      storageInfo \u003d node.updateStorage(srdb.getStorage());\n    }\n\n    for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n      switch (rdbi.getStatus()) {\n      case DELETED_BLOCK:\n        removeStoredBlock(rdbi.getBlock(), node);\n        deleted++;\n        break;\n      case RECEIVED_BLOCK:\n        addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n        received++;\n        break;\n      case RECEIVING_BLOCK:\n        receiving++;\n        processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                      ReplicaState.RBW, null);\n        break;\n      default:\n        String msg \u003d \n          \"Unknown block status code reported by \" + nodeID +\n          \": \" + rdbi;\n        blockLog.warn(msg);\n        assert false : msg; // if assertions are enabled, throw.\n        break;\n      }\n      blockLog.debug(\"BLOCK* block {}: {} is received from {}\",\n          rdbi.getStatus(), rdbi.getBlock(), nodeID);\n    }\n    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: from \"\n            + \"{} receiving: {}, received: {}, deleted: {}\", nodeID, receiving,\n        received, deleted);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "e226b5b40d716b6d363c43a8783766b72734e347": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7213. processIncrementalBlockReport performance degradation.\nContributed by Eric Payne.\n",
      "commitDate": "28/10/14 12:54 PM",
      "commitName": "e226b5b40d716b6d363c43a8783766b72734e347",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "28/10/14 12:26 PM",
      "commitNameOld": "371a3b87ed346732ed58a4faab0c6c1db57c86ed",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,62 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n       final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n     int received \u003d 0;\n     int deleted \u003d 0;\n     int receiving \u003d 0;\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n     if (node \u003d\u003d null || !node.isAlive) {\n       blockLog\n           .warn(\"BLOCK* processIncrementalBlockReport\"\n               + \" is received from dead or unregistered node \"\n               + nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n \n     DatanodeStorageInfo storageInfo \u003d\n         node.getStorageInfo(srdb.getStorage().getStorageID());\n     if (storageInfo \u003d\u003d null) {\n       // The DataNode is reporting an unknown storage. Usually the NN learns\n       // about new storages from heartbeats but during NN restart we may\n       // receive a block report or incremental report before the heartbeat.\n       // We must handle this for protocol compatibility. This issue was\n       // uncovered by HDFS-6094.\n       storageInfo \u003d node.updateStorage(srdb.getStorage());\n     }\n \n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n         removeStoredBlock(rdbi.getBlock(), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:\n         addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n         received++;\n         break;\n       case RECEIVING_BLOCK:\n         receiving++;\n         processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                       ReplicaState.RBW, null);\n         break;\n       default:\n         String msg \u003d \n           \"Unknown block status code reported by \" + nodeID +\n           \": \" + rdbi;\n         blockLog.warn(msg);\n         assert false : msg; // if assertions are enabled, throw.\n         break;\n       }\n       if (blockLog.isDebugEnabled()) {\n         blockLog.debug(\"BLOCK* block \"\n             + (rdbi.getStatus()) + \": \" + rdbi.getBlock()\n             + \" is received from \" + nodeID);\n       }\n     }\n-    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n+    if (blockLog.isDebugEnabled()) {\n+      blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n         + nodeID + \" receiving: \" + receiving + \", \" + \" received: \" + received\n         + \", \" + \" deleted: \" + deleted);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    int received \u003d 0;\n    int deleted \u003d 0;\n    int receiving \u003d 0;\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isAlive) {\n      blockLog\n          .warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node \"\n              + nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n\n    DatanodeStorageInfo storageInfo \u003d\n        node.getStorageInfo(srdb.getStorage().getStorageID());\n    if (storageInfo \u003d\u003d null) {\n      // The DataNode is reporting an unknown storage. Usually the NN learns\n      // about new storages from heartbeats but during NN restart we may\n      // receive a block report or incremental report before the heartbeat.\n      // We must handle this for protocol compatibility. This issue was\n      // uncovered by HDFS-6094.\n      storageInfo \u003d node.updateStorage(srdb.getStorage());\n    }\n\n    for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n      switch (rdbi.getStatus()) {\n      case DELETED_BLOCK:\n        removeStoredBlock(rdbi.getBlock(), node);\n        deleted++;\n        break;\n      case RECEIVED_BLOCK:\n        addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n        received++;\n        break;\n      case RECEIVING_BLOCK:\n        receiving++;\n        processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                      ReplicaState.RBW, null);\n        break;\n      default:\n        String msg \u003d \n          \"Unknown block status code reported by \" + nodeID +\n          \": \" + rdbi;\n        blockLog.warn(msg);\n        assert false : msg; // if assertions are enabled, throw.\n        break;\n      }\n      if (blockLog.isDebugEnabled()) {\n        blockLog.debug(\"BLOCK* block \"\n            + (rdbi.getStatus()) + \": \" + rdbi.getBlock()\n            + \" is received from \" + nodeID);\n      }\n    }\n    if (blockLog.isDebugEnabled()) {\n      blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n        + nodeID + \" receiving: \" + receiving + \", \" + \" received: \" + received\n        + \", \" + \" deleted: \" + deleted);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/14 9:58 AM",
      "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "31/07/14 6:05 PM",
      "commitNameOld": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.66,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,60 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n       final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n     int received \u003d 0;\n     int deleted \u003d 0;\n     int receiving \u003d 0;\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n     if (node \u003d\u003d null || !node.isAlive) {\n       blockLog\n           .warn(\"BLOCK* processIncrementalBlockReport\"\n               + \" is received from dead or unregistered node \"\n               + nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n \n-    if (node.getStorageInfo(srdb.getStorage().getStorageID()) \u003d\u003d null) {\n+    DatanodeStorageInfo storageInfo \u003d\n+        node.getStorageInfo(srdb.getStorage().getStorageID());\n+    if (storageInfo \u003d\u003d null) {\n       // The DataNode is reporting an unknown storage. Usually the NN learns\n       // about new storages from heartbeats but during NN restart we may\n       // receive a block report or incremental report before the heartbeat.\n       // We must handle this for protocol compatibility. This issue was\n       // uncovered by HDFS-6094.\n-      node.updateStorage(srdb.getStorage());\n+      storageInfo \u003d node.updateStorage(srdb.getStorage());\n     }\n \n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n         removeStoredBlock(rdbi.getBlock(), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:\n-        addBlock(node, srdb.getStorage().getStorageID(),\n-            rdbi.getBlock(), rdbi.getDelHints());\n+        addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n         received++;\n         break;\n       case RECEIVING_BLOCK:\n         receiving++;\n-        processAndHandleReportedBlock(node, srdb.getStorage().getStorageID(),\n-            rdbi.getBlock(), ReplicaState.RBW, null);\n+        processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n+                                      ReplicaState.RBW, null);\n         break;\n       default:\n         String msg \u003d \n           \"Unknown block status code reported by \" + nodeID +\n           \": \" + rdbi;\n         blockLog.warn(msg);\n         assert false : msg; // if assertions are enabled, throw.\n         break;\n       }\n       if (blockLog.isDebugEnabled()) {\n         blockLog.debug(\"BLOCK* block \"\n             + (rdbi.getStatus()) + \": \" + rdbi.getBlock()\n             + \" is received from \" + nodeID);\n       }\n     }\n     blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n         + nodeID + \" receiving: \" + receiving + \", \" + \" received: \" + received\n         + \", \" + \" deleted: \" + deleted);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    int received \u003d 0;\n    int deleted \u003d 0;\n    int receiving \u003d 0;\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isAlive) {\n      blockLog\n          .warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node \"\n              + nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n\n    DatanodeStorageInfo storageInfo \u003d\n        node.getStorageInfo(srdb.getStorage().getStorageID());\n    if (storageInfo \u003d\u003d null) {\n      // The DataNode is reporting an unknown storage. Usually the NN learns\n      // about new storages from heartbeats but during NN restart we may\n      // receive a block report or incremental report before the heartbeat.\n      // We must handle this for protocol compatibility. This issue was\n      // uncovered by HDFS-6094.\n      storageInfo \u003d node.updateStorage(srdb.getStorage());\n    }\n\n    for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n      switch (rdbi.getStatus()) {\n      case DELETED_BLOCK:\n        removeStoredBlock(rdbi.getBlock(), node);\n        deleted++;\n        break;\n      case RECEIVED_BLOCK:\n        addBlock(storageInfo, rdbi.getBlock(), rdbi.getDelHints());\n        received++;\n        break;\n      case RECEIVING_BLOCK:\n        receiving++;\n        processAndHandleReportedBlock(storageInfo, rdbi.getBlock(),\n                                      ReplicaState.RBW, null);\n        break;\n      default:\n        String msg \u003d \n          \"Unknown block status code reported by \" + nodeID +\n          \": \" + rdbi;\n        blockLog.warn(msg);\n        assert false : msg; // if assertions are enabled, throw.\n        break;\n      }\n      if (blockLog.isDebugEnabled()) {\n        blockLog.debug(\"BLOCK* block \"\n            + (rdbi.getStatus()) + \": \" + rdbi.getBlock()\n            + \" is received from \" + nodeID);\n      }\n    }\n    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n        + nodeID + \" receiving: \" + receiving + \", \" + \" received: \" + received\n        + \", \" + \" deleted: \" + deleted);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "fe8c3dc2b80a2c127e7aed0d3beb41dcfd8f7eac": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5672. TestHASafeMode#testSafeBlockTracking fails in trunk. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1581994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/03/14 11:36 AM",
      "commitName": "fe8c3dc2b80a2c127e7aed0d3beb41dcfd8f7eac",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/03/14 4:32 PM",
      "commitNameOld": "c2ef7e239eb0e81cf8a3e971378e9e696202de67",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 1.79,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,59 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n       final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n     int received \u003d 0;\n     int deleted \u003d 0;\n     int receiving \u003d 0;\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n     if (node \u003d\u003d null || !node.isAlive) {\n       blockLog\n           .warn(\"BLOCK* processIncrementalBlockReport\"\n               + \" is received from dead or unregistered node \"\n               + nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n \n     if (node.getStorageInfo(srdb.getStorage().getStorageID()) \u003d\u003d null) {\n       // The DataNode is reporting an unknown storage. Usually the NN learns\n       // about new storages from heartbeats but during NN restart we may\n       // receive a block report or incremental report before the heartbeat.\n       // We must handle this for protocol compatibility. This issue was\n-      // uncovered by HDFS-6904.\n+      // uncovered by HDFS-6094.\n       node.updateStorage(srdb.getStorage());\n     }\n \n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n         removeStoredBlock(rdbi.getBlock(), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:\n         addBlock(node, srdb.getStorage().getStorageID(),\n             rdbi.getBlock(), rdbi.getDelHints());\n         received++;\n         break;\n       case RECEIVING_BLOCK:\n         receiving++;\n         processAndHandleReportedBlock(node, srdb.getStorage().getStorageID(),\n             rdbi.getBlock(), ReplicaState.RBW, null);\n         break;\n       default:\n         String msg \u003d \n           \"Unknown block status code reported by \" + nodeID +\n           \": \" + rdbi;\n         blockLog.warn(msg);\n         assert false : msg; // if assertions are enabled, throw.\n         break;\n       }\n       if (blockLog.isDebugEnabled()) {\n         blockLog.debug(\"BLOCK* block \"\n             + (rdbi.getStatus()) + \": \" + rdbi.getBlock()\n             + \" is received from \" + nodeID);\n       }\n     }\n     blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n         + nodeID + \" receiving: \" + receiving + \", \" + \" received: \" + received\n         + \", \" + \" deleted: \" + deleted);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    int received \u003d 0;\n    int deleted \u003d 0;\n    int receiving \u003d 0;\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isAlive) {\n      blockLog\n          .warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node \"\n              + nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n\n    if (node.getStorageInfo(srdb.getStorage().getStorageID()) \u003d\u003d null) {\n      // The DataNode is reporting an unknown storage. Usually the NN learns\n      // about new storages from heartbeats but during NN restart we may\n      // receive a block report or incremental report before the heartbeat.\n      // We must handle this for protocol compatibility. This issue was\n      // uncovered by HDFS-6094.\n      node.updateStorage(srdb.getStorage());\n    }\n\n    for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n      switch (rdbi.getStatus()) {\n      case DELETED_BLOCK:\n        removeStoredBlock(rdbi.getBlock(), node);\n        deleted++;\n        break;\n      case RECEIVED_BLOCK:\n        addBlock(node, srdb.getStorage().getStorageID(),\n            rdbi.getBlock(), rdbi.getDelHints());\n        received++;\n        break;\n      case RECEIVING_BLOCK:\n        receiving++;\n        processAndHandleReportedBlock(node, srdb.getStorage().getStorageID(),\n            rdbi.getBlock(), ReplicaState.RBW, null);\n        break;\n      default:\n        String msg \u003d \n          \"Unknown block status code reported by \" + nodeID +\n          \": \" + rdbi;\n        blockLog.warn(msg);\n        assert false : msg; // if assertions are enabled, throw.\n        break;\n      }\n      if (blockLog.isDebugEnabled()) {\n        blockLog.debug(\"BLOCK* block \"\n            + (rdbi.getStatus()) + \": \" + rdbi.getBlock()\n            + \" is received from \" + nodeID);\n      }\n    }\n    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n        + nodeID + \" receiving: \" + receiving + \", \" + \" received: \" + received\n        + \", \" + \" deleted: \" + deleted);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6094. The same block can be counted twice towards safe mode threshold. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1578478 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/03/14 10:37 AM",
      "commitName": "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6094. The same block can be counted twice towards safe mode threshold. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1578478 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/03/14 10:37 AM",
          "commitName": "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "19/02/14 8:38 PM",
          "commitNameOld": "55aec006f499fcf4ec98568d594b0585836cfa5e",
          "commitAuthorOld": "",
          "daysBetweenCommits": 25.54,
          "commitsBetweenForRepo": 225,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,59 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n-      final String poolId, final StorageReceivedDeletedBlocks srdb)\n-      throws IOException {\n+      final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n     int received \u003d 0;\n     int deleted \u003d 0;\n     int receiving \u003d 0;\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n     if (node \u003d\u003d null || !node.isAlive) {\n       blockLog\n           .warn(\"BLOCK* processIncrementalBlockReport\"\n               + \" is received from dead or unregistered node \"\n               + nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n \n+    if (node.getStorageInfo(srdb.getStorage().getStorageID()) \u003d\u003d null) {\n+      // The DataNode is reporting an unknown storage. Usually the NN learns\n+      // about new storages from heartbeats but during NN restart we may\n+      // receive a block report or incremental report before the heartbeat.\n+      // We must handle this for protocol compatibility. This issue was\n+      // uncovered by HDFS-6904.\n+      node.updateStorage(srdb.getStorage());\n+    }\n+\n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n         removeStoredBlock(rdbi.getBlock(), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:\n-        addBlock(node, srdb.getStorageID(), rdbi.getBlock(), rdbi.getDelHints());\n+        addBlock(node, srdb.getStorage().getStorageID(),\n+            rdbi.getBlock(), rdbi.getDelHints());\n         received++;\n         break;\n       case RECEIVING_BLOCK:\n         receiving++;\n-        processAndHandleReportedBlock(node, srdb.getStorageID(), rdbi.getBlock(),\n-            ReplicaState.RBW, null);\n+        processAndHandleReportedBlock(node, srdb.getStorage().getStorageID(),\n+            rdbi.getBlock(), ReplicaState.RBW, null);\n         break;\n       default:\n         String msg \u003d \n           \"Unknown block status code reported by \" + nodeID +\n           \": \" + rdbi;\n         blockLog.warn(msg);\n         assert false : msg; // if assertions are enabled, throw.\n         break;\n       }\n       if (blockLog.isDebugEnabled()) {\n         blockLog.debug(\"BLOCK* block \"\n             + (rdbi.getStatus()) + \": \" + rdbi.getBlock()\n             + \" is received from \" + nodeID);\n       }\n     }\n     blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n         + nodeID + \" receiving: \" + receiving + \", \" + \" received: \" + received\n         + \", \" + \" deleted: \" + deleted);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    int received \u003d 0;\n    int deleted \u003d 0;\n    int receiving \u003d 0;\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isAlive) {\n      blockLog\n          .warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node \"\n              + nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n\n    if (node.getStorageInfo(srdb.getStorage().getStorageID()) \u003d\u003d null) {\n      // The DataNode is reporting an unknown storage. Usually the NN learns\n      // about new storages from heartbeats but during NN restart we may\n      // receive a block report or incremental report before the heartbeat.\n      // We must handle this for protocol compatibility. This issue was\n      // uncovered by HDFS-6904.\n      node.updateStorage(srdb.getStorage());\n    }\n\n    for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n      switch (rdbi.getStatus()) {\n      case DELETED_BLOCK:\n        removeStoredBlock(rdbi.getBlock(), node);\n        deleted++;\n        break;\n      case RECEIVED_BLOCK:\n        addBlock(node, srdb.getStorage().getStorageID(),\n            rdbi.getBlock(), rdbi.getDelHints());\n        received++;\n        break;\n      case RECEIVING_BLOCK:\n        receiving++;\n        processAndHandleReportedBlock(node, srdb.getStorage().getStorageID(),\n            rdbi.getBlock(), ReplicaState.RBW, null);\n        break;\n      default:\n        String msg \u003d \n          \"Unknown block status code reported by \" + nodeID +\n          \": \" + rdbi;\n        blockLog.warn(msg);\n        assert false : msg; // if assertions are enabled, throw.\n        break;\n      }\n      if (blockLog.isDebugEnabled()) {\n        blockLog.debug(\"BLOCK* block \"\n            + (rdbi.getStatus()) + \": \" + rdbi.getBlock()\n            + \" is received from \" + nodeID);\n      }\n    }\n    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n        + nodeID + \" receiving: \" + receiving + \", \" + \" received: \" + received\n        + \", \" + \" deleted: \" + deleted);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[nodeID-DatanodeID(modifiers-final), poolId-String(modifiers-final), srdb-StorageReceivedDeletedBlocks(modifiers-final)]",
            "newValue": "[nodeID-DatanodeID(modifiers-final), srdb-StorageReceivedDeletedBlocks(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6094. The same block can be counted twice towards safe mode threshold. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1578478 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/03/14 10:37 AM",
          "commitName": "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "19/02/14 8:38 PM",
          "commitNameOld": "55aec006f499fcf4ec98568d594b0585836cfa5e",
          "commitAuthorOld": "",
          "daysBetweenCommits": 25.54,
          "commitsBetweenForRepo": 225,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,59 @@\n   public void processIncrementalBlockReport(final DatanodeID nodeID,\n-      final String poolId, final StorageReceivedDeletedBlocks srdb)\n-      throws IOException {\n+      final StorageReceivedDeletedBlocks srdb) throws IOException {\n     assert namesystem.hasWriteLock();\n     int received \u003d 0;\n     int deleted \u003d 0;\n     int receiving \u003d 0;\n     final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n     if (node \u003d\u003d null || !node.isAlive) {\n       blockLog\n           .warn(\"BLOCK* processIncrementalBlockReport\"\n               + \" is received from dead or unregistered node \"\n               + nodeID);\n       throw new IOException(\n           \"Got incremental block report from unregistered or dead node\");\n     }\n \n+    if (node.getStorageInfo(srdb.getStorage().getStorageID()) \u003d\u003d null) {\n+      // The DataNode is reporting an unknown storage. Usually the NN learns\n+      // about new storages from heartbeats but during NN restart we may\n+      // receive a block report or incremental report before the heartbeat.\n+      // We must handle this for protocol compatibility. This issue was\n+      // uncovered by HDFS-6904.\n+      node.updateStorage(srdb.getStorage());\n+    }\n+\n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n         removeStoredBlock(rdbi.getBlock(), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:\n-        addBlock(node, srdb.getStorageID(), rdbi.getBlock(), rdbi.getDelHints());\n+        addBlock(node, srdb.getStorage().getStorageID(),\n+            rdbi.getBlock(), rdbi.getDelHints());\n         received++;\n         break;\n       case RECEIVING_BLOCK:\n         receiving++;\n-        processAndHandleReportedBlock(node, srdb.getStorageID(), rdbi.getBlock(),\n-            ReplicaState.RBW, null);\n+        processAndHandleReportedBlock(node, srdb.getStorage().getStorageID(),\n+            rdbi.getBlock(), ReplicaState.RBW, null);\n         break;\n       default:\n         String msg \u003d \n           \"Unknown block status code reported by \" + nodeID +\n           \": \" + rdbi;\n         blockLog.warn(msg);\n         assert false : msg; // if assertions are enabled, throw.\n         break;\n       }\n       if (blockLog.isDebugEnabled()) {\n         blockLog.debug(\"BLOCK* block \"\n             + (rdbi.getStatus()) + \": \" + rdbi.getBlock()\n             + \" is received from \" + nodeID);\n       }\n     }\n     blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n         + nodeID + \" receiving: \" + receiving + \", \" + \" received: \" + received\n         + \", \" + \" deleted: \" + deleted);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void processIncrementalBlockReport(final DatanodeID nodeID,\n      final StorageReceivedDeletedBlocks srdb) throws IOException {\n    assert namesystem.hasWriteLock();\n    int received \u003d 0;\n    int deleted \u003d 0;\n    int receiving \u003d 0;\n    final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n    if (node \u003d\u003d null || !node.isAlive) {\n      blockLog\n          .warn(\"BLOCK* processIncrementalBlockReport\"\n              + \" is received from dead or unregistered node \"\n              + nodeID);\n      throw new IOException(\n          \"Got incremental block report from unregistered or dead node\");\n    }\n\n    if (node.getStorageInfo(srdb.getStorage().getStorageID()) \u003d\u003d null) {\n      // The DataNode is reporting an unknown storage. Usually the NN learns\n      // about new storages from heartbeats but during NN restart we may\n      // receive a block report or incremental report before the heartbeat.\n      // We must handle this for protocol compatibility. This issue was\n      // uncovered by HDFS-6904.\n      node.updateStorage(srdb.getStorage());\n    }\n\n    for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n      switch (rdbi.getStatus()) {\n      case DELETED_BLOCK:\n        removeStoredBlock(rdbi.getBlock(), node);\n        deleted++;\n        break;\n      case RECEIVED_BLOCK:\n        addBlock(node, srdb.getStorage().getStorageID(),\n            rdbi.getBlock(), rdbi.getDelHints());\n        received++;\n        break;\n      case RECEIVING_BLOCK:\n        receiving++;\n        processAndHandleReportedBlock(node, srdb.getStorage().getStorageID(),\n            rdbi.getBlock(), ReplicaState.RBW, null);\n        break;\n      default:\n        String msg \u003d \n          \"Unknown block status code reported by \" + nodeID +\n          \": \" + rdbi;\n        blockLog.warn(msg);\n        assert false : msg; // if assertions are enabled, throw.\n        break;\n      }\n      if (blockLog.isDebugEnabled()) {\n        blockLog.debug(\"BLOCK* block \"\n            + (rdbi.getStatus()) + \": \" + rdbi.getBlock()\n            + \" is received from \" + nodeID);\n      }\n    }\n    blockLog.debug(\"*BLOCK* NameNode.processIncrementalBlockReport: \" + \"from \"\n        + nodeID + \" receiving: \" + receiving + \", \" + \" received: \" + received\n        + \", \" + \" deleted: \" + deleted);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}