{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "NNStorageRetentionManager.java",
  "functionName": "purgeOldStorage",
  "functionId": "purgeOldStorage___nnf-NameNodeFile",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
  "functionStartLine": 111,
  "functionEndLine": 170,
  "numCommitsSeen": 28,
  "timeTaken": 3977,
  "changeHistory": [
    "098ec2b11ff3f677eb823f75b147a1ac8dbf959e",
    "e3d2e4c156851de7dac16154521a2e06ea83ec7b",
    "4f9bbaa301194e3d20972a10f51638c7f4d121f0",
    "512a18a8d92305a34f3037064ceabdc5aff1f8bf",
    "8c62c46046656c01b327c378e89d57b4bf37e16e",
    "e6b5b015613973d5960c9aeae4866e587d3ecf1d",
    "557ffe2101325438f15dbb218128d327984ecb11",
    "92cb6b093c7e3a39083c0497d80bd7e4eeae9c7f",
    "7f0a99f6e63fcbc4e6971b719d235af1cf5cb514",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "23762da4fa17ce6ea7b70722147977123a28a7e6",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63"
  ],
  "changeHistoryShort": {
    "098ec2b11ff3f677eb823f75b147a1ac8dbf959e": "Ybodychange",
    "e3d2e4c156851de7dac16154521a2e06ea83ec7b": "Ybodychange",
    "4f9bbaa301194e3d20972a10f51638c7f4d121f0": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "512a18a8d92305a34f3037064ceabdc5aff1f8bf": "Ybodychange",
    "8c62c46046656c01b327c378e89d57b4bf37e16e": "Ybodychange",
    "e6b5b015613973d5960c9aeae4866e587d3ecf1d": "Ybodychange",
    "557ffe2101325438f15dbb218128d327984ecb11": "Ybodychange",
    "92cb6b093c7e3a39083c0497d80bd7e4eeae9c7f": "Ybodychange",
    "7f0a99f6e63fcbc4e6971b719d235af1cf5cb514": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "23762da4fa17ce6ea7b70722147977123a28a7e6": "Ybodychange",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": "Yintroduced"
  },
  "changeHistoryDetails": {
    "098ec2b11ff3f677eb823f75b147a1ac8dbf959e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10519. Add a configuration option to enable in-progress edit log tailing. Contributed by Jiayi Zhou.\n",
      "commitDate": "27/07/16 5:55 PM",
      "commitName": "098ec2b11ff3f677eb823f75b147a1ac8dbf959e",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "13/05/14 6:15 PM",
      "commitNameOld": "97f58955a6045b373ab73653bf26ab5922b00cf3",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 805.99,
      "commitsBetweenForRepo": 6351,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   void purgeOldStorage(NameNodeFile nnf) throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n         new FSImageTransactionalStorageInspector(EnumSet.of(nnf));\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n     \n     if (nnf \u003d\u003d NameNodeFile.IMAGE_ROLLBACK) {\n       // do not purge edits for IMAGE_ROLLBACK.\n       return;\n     }\n \n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n     // reflects the state up to and including N. However, we also\n     // provide a \"cushion\" of older txns that we keep, which is\n     // handy for HA, where a remote node may not have as many\n     // new images.\n     //\n     // First, determine the target number of extra transactions to retain based\n     // on the configured amount.\n     long minimumRequiredTxId \u003d minImageTxId + 1;\n     long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n     \n     ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n-    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n+    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false, false);\n     Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n       @Override\n       public int compare(EditLogInputStream a, EditLogInputStream b) {\n         return ComparisonChain.start()\n             .compare(a.getFirstTxId(), b.getFirstTxId())\n             .compare(a.getLastTxId(), b.getLastTxId())\n             .result();\n       }\n     });\n \n     // Remove from consideration any edit logs that are in fact required.\n     while (editLogs.size() \u003e 0 \u0026\u0026\n         editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n       editLogs.remove(editLogs.size() - 1);\n     }\n     \n     // Next, adjust the number of transactions to retain if doing so would mean\n     // keeping too many segments around.\n     while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n       purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n       editLogs.remove(0);\n     }\n     \n     // Finally, ensure that we\u0027re not trying to purge any transactions that we\n     // actually need.\n     if (purgeLogsFrom \u003e minimumRequiredTxId) {\n       throw new AssertionError(\"Should not purge more edits than required to \"\n           + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n           + minimumRequiredTxId);\n     }\n     \n     purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void purgeOldStorage(NameNodeFile nnf) throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector(EnumSet.of(nnf));\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    \n    if (nnf \u003d\u003d NameNodeFile.IMAGE_ROLLBACK) {\n      // do not purge edits for IMAGE_ROLLBACK.\n      return;\n    }\n\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N. However, we also\n    // provide a \"cushion\" of older txns that we keep, which is\n    // handy for HA, where a remote node may not have as many\n    // new images.\n    //\n    // First, determine the target number of extra transactions to retain based\n    // on the configured amount.\n    long minimumRequiredTxId \u003d minImageTxId + 1;\n    long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n    \n    ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false, false);\n    Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n      @Override\n      public int compare(EditLogInputStream a, EditLogInputStream b) {\n        return ComparisonChain.start()\n            .compare(a.getFirstTxId(), b.getFirstTxId())\n            .compare(a.getLastTxId(), b.getLastTxId())\n            .result();\n      }\n    });\n\n    // Remove from consideration any edit logs that are in fact required.\n    while (editLogs.size() \u003e 0 \u0026\u0026\n        editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n      editLogs.remove(editLogs.size() - 1);\n    }\n    \n    // Next, adjust the number of transactions to retain if doing so would mean\n    // keeping too many segments around.\n    while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n      purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n      editLogs.remove(0);\n    }\n    \n    // Finally, ensure that we\u0027re not trying to purge any transactions that we\n    // actually need.\n    if (purgeLogsFrom \u003e minimumRequiredTxId) {\n      throw new AssertionError(\"Should not purge more edits than required to \"\n          + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n          + minimumRequiredTxId);\n    }\n    \n    purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
      "extendedDetails": {}
    },
    "e3d2e4c156851de7dac16154521a2e06ea83ec7b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6000. Avoid saving namespace when starting rolling upgrade. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1571840 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/02/14 1:58 PM",
      "commitName": "e3d2e4c156851de7dac16154521a2e06ea83ec7b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "14/02/14 4:28 PM",
      "commitNameOld": "470d4253b246670f220eec81dd617ba0ee979623",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 10.9,
      "commitsBetweenForRepo": 96,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   void purgeOldStorage(NameNodeFile nnf) throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n-        new FSImageTransactionalStorageInspector(nnf);\n+        new FSImageTransactionalStorageInspector(EnumSet.of(nnf));\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n     \n     if (nnf \u003d\u003d NameNodeFile.IMAGE_ROLLBACK) {\n       // do not purge edits for IMAGE_ROLLBACK.\n       return;\n     }\n \n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n     // reflects the state up to and including N. However, we also\n     // provide a \"cushion\" of older txns that we keep, which is\n     // handy for HA, where a remote node may not have as many\n     // new images.\n     //\n     // First, determine the target number of extra transactions to retain based\n     // on the configured amount.\n     long minimumRequiredTxId \u003d minImageTxId + 1;\n     long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n     \n     ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n     purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n     Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n       @Override\n       public int compare(EditLogInputStream a, EditLogInputStream b) {\n         return ComparisonChain.start()\n             .compare(a.getFirstTxId(), b.getFirstTxId())\n             .compare(a.getLastTxId(), b.getLastTxId())\n             .result();\n       }\n     });\n \n     // Remove from consideration any edit logs that are in fact required.\n     while (editLogs.size() \u003e 0 \u0026\u0026\n         editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n       editLogs.remove(editLogs.size() - 1);\n     }\n     \n     // Next, adjust the number of transactions to retain if doing so would mean\n     // keeping too many segments around.\n     while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n       purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n       editLogs.remove(0);\n     }\n     \n     // Finally, ensure that we\u0027re not trying to purge any transactions that we\n     // actually need.\n     if (purgeLogsFrom \u003e minimumRequiredTxId) {\n       throw new AssertionError(\"Should not purge more edits than required to \"\n           + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n           + minimumRequiredTxId);\n     }\n     \n     purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void purgeOldStorage(NameNodeFile nnf) throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector(EnumSet.of(nnf));\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    \n    if (nnf \u003d\u003d NameNodeFile.IMAGE_ROLLBACK) {\n      // do not purge edits for IMAGE_ROLLBACK.\n      return;\n    }\n\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N. However, we also\n    // provide a \"cushion\" of older txns that we keep, which is\n    // handy for HA, where a remote node may not have as many\n    // new images.\n    //\n    // First, determine the target number of extra transactions to retain based\n    // on the configured amount.\n    long minimumRequiredTxId \u003d minImageTxId + 1;\n    long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n    \n    ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n    Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n      @Override\n      public int compare(EditLogInputStream a, EditLogInputStream b) {\n        return ComparisonChain.start()\n            .compare(a.getFirstTxId(), b.getFirstTxId())\n            .compare(a.getLastTxId(), b.getLastTxId())\n            .result();\n      }\n    });\n\n    // Remove from consideration any edit logs that are in fact required.\n    while (editLogs.size() \u003e 0 \u0026\u0026\n        editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n      editLogs.remove(editLogs.size() - 1);\n    }\n    \n    // Next, adjust the number of transactions to retain if doing so would mean\n    // keeping too many segments around.\n    while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n      purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n      editLogs.remove(0);\n    }\n    \n    // Finally, ensure that we\u0027re not trying to purge any transactions that we\n    // actually need.\n    if (purgeLogsFrom \u003e minimumRequiredTxId) {\n      throw new AssertionError(\"Should not purge more edits than required to \"\n          + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n          + minimumRequiredTxId);\n    }\n    \n    purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
      "extendedDetails": {}
    },
    "4f9bbaa301194e3d20972a10f51638c7f4d121f0": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-5889. When starting rolling upgrade, create a fs image for rollback so that the standby namenode can create checkpoints during upgrade.  Contributed by szetszwo \u0026 jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1567861 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/02/14 12:04 AM",
      "commitName": "4f9bbaa301194e3d20972a10f51638c7f4d121f0",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5889. When starting rolling upgrade, create a fs image for rollback so that the standby namenode can create checkpoints during upgrade.  Contributed by szetszwo \u0026 jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1567861 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/02/14 12:04 AM",
          "commitName": "4f9bbaa301194e3d20972a10f51638c7f4d121f0",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "10/12/13 5:32 PM",
          "commitNameOld": "512a18a8d92305a34f3037064ceabdc5aff1f8bf",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 64.27,
          "commitsBetweenForRepo": 334,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,54 +1,60 @@\n-  public void purgeOldStorage() throws IOException {\n+  void purgeOldStorage(NameNodeFile nnf) throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n-      new FSImageTransactionalStorageInspector();\n+        new FSImageTransactionalStorageInspector(nnf);\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n+    \n+    if (nnf \u003d\u003d NameNodeFile.IMAGE_ROLLBACK) {\n+      // do not purge edits for IMAGE_ROLLBACK.\n+      return;\n+    }\n+\n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n     // reflects the state up to and including N. However, we also\n     // provide a \"cushion\" of older txns that we keep, which is\n     // handy for HA, where a remote node may not have as many\n     // new images.\n     //\n     // First, determine the target number of extra transactions to retain based\n     // on the configured amount.\n     long minimumRequiredTxId \u003d minImageTxId + 1;\n     long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n     \n     ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n     purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n     Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n       @Override\n       public int compare(EditLogInputStream a, EditLogInputStream b) {\n         return ComparisonChain.start()\n             .compare(a.getFirstTxId(), b.getFirstTxId())\n             .compare(a.getLastTxId(), b.getLastTxId())\n             .result();\n       }\n     });\n \n     // Remove from consideration any edit logs that are in fact required.\n     while (editLogs.size() \u003e 0 \u0026\u0026\n         editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n       editLogs.remove(editLogs.size() - 1);\n     }\n     \n     // Next, adjust the number of transactions to retain if doing so would mean\n     // keeping too many segments around.\n     while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n       purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n       editLogs.remove(0);\n     }\n     \n     // Finally, ensure that we\u0027re not trying to purge any transactions that we\n     // actually need.\n     if (purgeLogsFrom \u003e minimumRequiredTxId) {\n       throw new AssertionError(\"Should not purge more edits than required to \"\n           + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n           + minimumRequiredTxId);\n     }\n     \n     purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void purgeOldStorage(NameNodeFile nnf) throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector(nnf);\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    \n    if (nnf \u003d\u003d NameNodeFile.IMAGE_ROLLBACK) {\n      // do not purge edits for IMAGE_ROLLBACK.\n      return;\n    }\n\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N. However, we also\n    // provide a \"cushion\" of older txns that we keep, which is\n    // handy for HA, where a remote node may not have as many\n    // new images.\n    //\n    // First, determine the target number of extra transactions to retain based\n    // on the configured amount.\n    long minimumRequiredTxId \u003d minImageTxId + 1;\n    long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n    \n    ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n    Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n      @Override\n      public int compare(EditLogInputStream a, EditLogInputStream b) {\n        return ComparisonChain.start()\n            .compare(a.getFirstTxId(), b.getFirstTxId())\n            .compare(a.getLastTxId(), b.getLastTxId())\n            .result();\n      }\n    });\n\n    // Remove from consideration any edit logs that are in fact required.\n    while (editLogs.size() \u003e 0 \u0026\u0026\n        editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n      editLogs.remove(editLogs.size() - 1);\n    }\n    \n    // Next, adjust the number of transactions to retain if doing so would mean\n    // keeping too many segments around.\n    while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n      purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n      editLogs.remove(0);\n    }\n    \n    // Finally, ensure that we\u0027re not trying to purge any transactions that we\n    // actually need.\n    if (purgeLogsFrom \u003e minimumRequiredTxId) {\n      throw new AssertionError(\"Should not purge more edits than required to \"\n          + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n          + minimumRequiredTxId);\n    }\n    \n    purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[nnf-NameNodeFile]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5889. When starting rolling upgrade, create a fs image for rollback so that the standby namenode can create checkpoints during upgrade.  Contributed by szetszwo \u0026 jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1567861 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/02/14 12:04 AM",
          "commitName": "4f9bbaa301194e3d20972a10f51638c7f4d121f0",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "10/12/13 5:32 PM",
          "commitNameOld": "512a18a8d92305a34f3037064ceabdc5aff1f8bf",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 64.27,
          "commitsBetweenForRepo": 334,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,54 +1,60 @@\n-  public void purgeOldStorage() throws IOException {\n+  void purgeOldStorage(NameNodeFile nnf) throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n-      new FSImageTransactionalStorageInspector();\n+        new FSImageTransactionalStorageInspector(nnf);\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n+    \n+    if (nnf \u003d\u003d NameNodeFile.IMAGE_ROLLBACK) {\n+      // do not purge edits for IMAGE_ROLLBACK.\n+      return;\n+    }\n+\n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n     // reflects the state up to and including N. However, we also\n     // provide a \"cushion\" of older txns that we keep, which is\n     // handy for HA, where a remote node may not have as many\n     // new images.\n     //\n     // First, determine the target number of extra transactions to retain based\n     // on the configured amount.\n     long minimumRequiredTxId \u003d minImageTxId + 1;\n     long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n     \n     ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n     purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n     Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n       @Override\n       public int compare(EditLogInputStream a, EditLogInputStream b) {\n         return ComparisonChain.start()\n             .compare(a.getFirstTxId(), b.getFirstTxId())\n             .compare(a.getLastTxId(), b.getLastTxId())\n             .result();\n       }\n     });\n \n     // Remove from consideration any edit logs that are in fact required.\n     while (editLogs.size() \u003e 0 \u0026\u0026\n         editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n       editLogs.remove(editLogs.size() - 1);\n     }\n     \n     // Next, adjust the number of transactions to retain if doing so would mean\n     // keeping too many segments around.\n     while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n       purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n       editLogs.remove(0);\n     }\n     \n     // Finally, ensure that we\u0027re not trying to purge any transactions that we\n     // actually need.\n     if (purgeLogsFrom \u003e minimumRequiredTxId) {\n       throw new AssertionError(\"Should not purge more edits than required to \"\n           + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n           + minimumRequiredTxId);\n     }\n     \n     purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void purgeOldStorage(NameNodeFile nnf) throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector(nnf);\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    \n    if (nnf \u003d\u003d NameNodeFile.IMAGE_ROLLBACK) {\n      // do not purge edits for IMAGE_ROLLBACK.\n      return;\n    }\n\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N. However, we also\n    // provide a \"cushion\" of older txns that we keep, which is\n    // handy for HA, where a remote node may not have as many\n    // new images.\n    //\n    // First, determine the target number of extra transactions to retain based\n    // on the configured amount.\n    long minimumRequiredTxId \u003d minImageTxId + 1;\n    long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n    \n    ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n    Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n      @Override\n      public int compare(EditLogInputStream a, EditLogInputStream b) {\n        return ComparisonChain.start()\n            .compare(a.getFirstTxId(), b.getFirstTxId())\n            .compare(a.getLastTxId(), b.getLastTxId())\n            .result();\n      }\n    });\n\n    // Remove from consideration any edit logs that are in fact required.\n    while (editLogs.size() \u003e 0 \u0026\u0026\n        editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n      editLogs.remove(editLogs.size() - 1);\n    }\n    \n    // Next, adjust the number of transactions to retain if doing so would mean\n    // keeping too many segments around.\n    while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n      purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n      editLogs.remove(0);\n    }\n    \n    // Finally, ensure that we\u0027re not trying to purge any transactions that we\n    // actually need.\n    if (purgeLogsFrom \u003e minimumRequiredTxId) {\n      throw new AssertionError(\"Should not purge more edits than required to \"\n          + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n          + minimumRequiredTxId);\n    }\n    \n    purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5889. When starting rolling upgrade, create a fs image for rollback so that the standby namenode can create checkpoints during upgrade.  Contributed by szetszwo \u0026 jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1567861 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/02/14 12:04 AM",
          "commitName": "4f9bbaa301194e3d20972a10f51638c7f4d121f0",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "10/12/13 5:32 PM",
          "commitNameOld": "512a18a8d92305a34f3037064ceabdc5aff1f8bf",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 64.27,
          "commitsBetweenForRepo": 334,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,54 +1,60 @@\n-  public void purgeOldStorage() throws IOException {\n+  void purgeOldStorage(NameNodeFile nnf) throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n-      new FSImageTransactionalStorageInspector();\n+        new FSImageTransactionalStorageInspector(nnf);\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n+    \n+    if (nnf \u003d\u003d NameNodeFile.IMAGE_ROLLBACK) {\n+      // do not purge edits for IMAGE_ROLLBACK.\n+      return;\n+    }\n+\n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n     // reflects the state up to and including N. However, we also\n     // provide a \"cushion\" of older txns that we keep, which is\n     // handy for HA, where a remote node may not have as many\n     // new images.\n     //\n     // First, determine the target number of extra transactions to retain based\n     // on the configured amount.\n     long minimumRequiredTxId \u003d minImageTxId + 1;\n     long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n     \n     ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n     purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n     Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n       @Override\n       public int compare(EditLogInputStream a, EditLogInputStream b) {\n         return ComparisonChain.start()\n             .compare(a.getFirstTxId(), b.getFirstTxId())\n             .compare(a.getLastTxId(), b.getLastTxId())\n             .result();\n       }\n     });\n \n     // Remove from consideration any edit logs that are in fact required.\n     while (editLogs.size() \u003e 0 \u0026\u0026\n         editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n       editLogs.remove(editLogs.size() - 1);\n     }\n     \n     // Next, adjust the number of transactions to retain if doing so would mean\n     // keeping too many segments around.\n     while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n       purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n       editLogs.remove(0);\n     }\n     \n     // Finally, ensure that we\u0027re not trying to purge any transactions that we\n     // actually need.\n     if (purgeLogsFrom \u003e minimumRequiredTxId) {\n       throw new AssertionError(\"Should not purge more edits than required to \"\n           + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n           + minimumRequiredTxId);\n     }\n     \n     purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void purgeOldStorage(NameNodeFile nnf) throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector(nnf);\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    \n    if (nnf \u003d\u003d NameNodeFile.IMAGE_ROLLBACK) {\n      // do not purge edits for IMAGE_ROLLBACK.\n      return;\n    }\n\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N. However, we also\n    // provide a \"cushion\" of older txns that we keep, which is\n    // handy for HA, where a remote node may not have as many\n    // new images.\n    //\n    // First, determine the target number of extra transactions to retain based\n    // on the configured amount.\n    long minimumRequiredTxId \u003d minImageTxId + 1;\n    long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n    \n    ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n    Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n      @Override\n      public int compare(EditLogInputStream a, EditLogInputStream b) {\n        return ComparisonChain.start()\n            .compare(a.getFirstTxId(), b.getFirstTxId())\n            .compare(a.getLastTxId(), b.getLastTxId())\n            .result();\n      }\n    });\n\n    // Remove from consideration any edit logs that are in fact required.\n    while (editLogs.size() \u003e 0 \u0026\u0026\n        editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n      editLogs.remove(editLogs.size() - 1);\n    }\n    \n    // Next, adjust the number of transactions to retain if doing so would mean\n    // keeping too many segments around.\n    while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n      purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n      editLogs.remove(0);\n    }\n    \n    // Finally, ensure that we\u0027re not trying to purge any transactions that we\n    // actually need.\n    if (purgeLogsFrom \u003e minimumRequiredTxId) {\n      throw new AssertionError(\"Should not purge more edits than required to \"\n          + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n          + minimumRequiredTxId);\n    }\n    \n    purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "512a18a8d92305a34f3037064ceabdc5aff1f8bf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5074. Allow starting up from an fsimage checkpoint in the middle of a segment. Contributed by Todd Lipcon.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1550016 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/12/13 5:32 PM",
      "commitName": "512a18a8d92305a34f3037064ceabdc5aff1f8bf",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "22/05/13 12:37 PM",
      "commitNameOld": "8c62c46046656c01b327c378e89d57b4bf37e16e",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 202.25,
      "commitsBetweenForRepo": 1253,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   public void purgeOldStorage() throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n       new FSImageTransactionalStorageInspector();\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n     // reflects the state up to and including N. However, we also\n     // provide a \"cushion\" of older txns that we keep, which is\n     // handy for HA, where a remote node may not have as many\n     // new images.\n     //\n     // First, determine the target number of extra transactions to retain based\n     // on the configured amount.\n     long minimumRequiredTxId \u003d minImageTxId + 1;\n     long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n     \n     ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n-    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false, false);\n+    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n     Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n       @Override\n       public int compare(EditLogInputStream a, EditLogInputStream b) {\n         return ComparisonChain.start()\n             .compare(a.getFirstTxId(), b.getFirstTxId())\n             .compare(a.getLastTxId(), b.getLastTxId())\n             .result();\n       }\n     });\n \n     // Remove from consideration any edit logs that are in fact required.\n     while (editLogs.size() \u003e 0 \u0026\u0026\n         editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n       editLogs.remove(editLogs.size() - 1);\n     }\n     \n     // Next, adjust the number of transactions to retain if doing so would mean\n     // keeping too many segments around.\n     while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n       purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n       editLogs.remove(0);\n     }\n     \n     // Finally, ensure that we\u0027re not trying to purge any transactions that we\n     // actually need.\n     if (purgeLogsFrom \u003e minimumRequiredTxId) {\n       throw new AssertionError(\"Should not purge more edits than required to \"\n           + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n           + minimumRequiredTxId);\n     }\n     \n     purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void purgeOldStorage() throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n      new FSImageTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N. However, we also\n    // provide a \"cushion\" of older txns that we keep, which is\n    // handy for HA, where a remote node may not have as many\n    // new images.\n    //\n    // First, determine the target number of extra transactions to retain based\n    // on the configured amount.\n    long minimumRequiredTxId \u003d minImageTxId + 1;\n    long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n    \n    ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n    Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n      @Override\n      public int compare(EditLogInputStream a, EditLogInputStream b) {\n        return ComparisonChain.start()\n            .compare(a.getFirstTxId(), b.getFirstTxId())\n            .compare(a.getLastTxId(), b.getLastTxId())\n            .result();\n      }\n    });\n\n    // Remove from consideration any edit logs that are in fact required.\n    while (editLogs.size() \u003e 0 \u0026\u0026\n        editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n      editLogs.remove(editLogs.size() - 1);\n    }\n    \n    // Next, adjust the number of transactions to retain if doing so would mean\n    // keeping too many segments around.\n    while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n      purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n      editLogs.remove(0);\n    }\n    \n    // Finally, ensure that we\u0027re not trying to purge any transactions that we\n    // actually need.\n    if (purgeLogsFrom \u003e minimumRequiredTxId) {\n      throw new AssertionError(\"Should not purge more edits than required to \"\n          + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n          + minimumRequiredTxId);\n    }\n    \n    purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
      "extendedDetails": {}
    },
    "8c62c46046656c01b327c378e89d57b4bf37e16e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4298. StorageRetentionManager spews warnings when used with QJM. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1485371 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/05/13 12:37 PM",
      "commitName": "8c62c46046656c01b327c378e89d57b4bf37e16e",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/04/13 4:29 PM",
      "commitNameOld": "e6b5b015613973d5960c9aeae4866e587d3ecf1d",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 27.84,
      "commitsBetweenForRepo": 167,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   public void purgeOldStorage() throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n       new FSImageTransactionalStorageInspector();\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n     // reflects the state up to and including N. However, we also\n     // provide a \"cushion\" of older txns that we keep, which is\n     // handy for HA, where a remote node may not have as many\n     // new images.\n     //\n     // First, determine the target number of extra transactions to retain based\n     // on the configured amount.\n     long minimumRequiredTxId \u003d minImageTxId + 1;\n     long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n     \n     ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n-    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n+    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false, false);\n     Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n       @Override\n       public int compare(EditLogInputStream a, EditLogInputStream b) {\n         return ComparisonChain.start()\n             .compare(a.getFirstTxId(), b.getFirstTxId())\n             .compare(a.getLastTxId(), b.getLastTxId())\n             .result();\n       }\n     });\n \n     // Remove from consideration any edit logs that are in fact required.\n     while (editLogs.size() \u003e 0 \u0026\u0026\n         editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n       editLogs.remove(editLogs.size() - 1);\n     }\n     \n     // Next, adjust the number of transactions to retain if doing so would mean\n     // keeping too many segments around.\n     while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n       purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n       editLogs.remove(0);\n     }\n     \n     // Finally, ensure that we\u0027re not trying to purge any transactions that we\n     // actually need.\n     if (purgeLogsFrom \u003e minimumRequiredTxId) {\n       throw new AssertionError(\"Should not purge more edits than required to \"\n           + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n           + minimumRequiredTxId);\n     }\n     \n     purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void purgeOldStorage() throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n      new FSImageTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N. However, we also\n    // provide a \"cushion\" of older txns that we keep, which is\n    // handy for HA, where a remote node may not have as many\n    // new images.\n    //\n    // First, determine the target number of extra transactions to retain based\n    // on the configured amount.\n    long minimumRequiredTxId \u003d minImageTxId + 1;\n    long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n    \n    ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false, false);\n    Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n      @Override\n      public int compare(EditLogInputStream a, EditLogInputStream b) {\n        return ComparisonChain.start()\n            .compare(a.getFirstTxId(), b.getFirstTxId())\n            .compare(a.getLastTxId(), b.getLastTxId())\n            .result();\n      }\n    });\n\n    // Remove from consideration any edit logs that are in fact required.\n    while (editLogs.size() \u003e 0 \u0026\u0026\n        editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n      editLogs.remove(editLogs.size() - 1);\n    }\n    \n    // Next, adjust the number of transactions to retain if doing so would mean\n    // keeping too many segments around.\n    while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n      purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n      editLogs.remove(0);\n    }\n    \n    // Finally, ensure that we\u0027re not trying to purge any transactions that we\n    // actually need.\n    if (purgeLogsFrom \u003e minimumRequiredTxId) {\n      throw new AssertionError(\"Should not purge more edits than required to \"\n          + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n          + minimumRequiredTxId);\n    }\n    \n    purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
      "extendedDetails": {}
    },
    "e6b5b015613973d5960c9aeae4866e587d3ecf1d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4739. NN can miscalculate the number of extra edit log segments to retain. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1471769 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/04/13 4:29 PM",
      "commitName": "e6b5b015613973d5960c9aeae4866e587d3ecf1d",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "15/10/12 6:59 PM",
      "commitNameOld": "557ffe2101325438f15dbb218128d327984ecb11",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 190.9,
      "commitsBetweenForRepo": 934,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,54 @@\n   public void purgeOldStorage() throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n       new FSImageTransactionalStorageInspector();\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n     // reflects the state up to and including N. However, we also\n     // provide a \"cushion\" of older txns that we keep, which is\n     // handy for HA, where a remote node may not have as many\n     // new images.\n     //\n     // First, determine the target number of extra transactions to retain based\n     // on the configured amount.\n     long minimumRequiredTxId \u003d minImageTxId + 1;\n     long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n     \n     ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n     purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n     Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n       @Override\n       public int compare(EditLogInputStream a, EditLogInputStream b) {\n         return ComparisonChain.start()\n             .compare(a.getFirstTxId(), b.getFirstTxId())\n             .compare(a.getLastTxId(), b.getLastTxId())\n             .result();\n       }\n     });\n+\n+    // Remove from consideration any edit logs that are in fact required.\n+    while (editLogs.size() \u003e 0 \u0026\u0026\n+        editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n+      editLogs.remove(editLogs.size() - 1);\n+    }\n     \n     // Next, adjust the number of transactions to retain if doing so would mean\n     // keeping too many segments around.\n     while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n-      purgeLogsFrom \u003d editLogs.get(0).getFirstTxId();\n+      purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n       editLogs.remove(0);\n     }\n     \n     // Finally, ensure that we\u0027re not trying to purge any transactions that we\n     // actually need.\n     if (purgeLogsFrom \u003e minimumRequiredTxId) {\n       throw new AssertionError(\"Should not purge more edits than required to \"\n           + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n           + minimumRequiredTxId);\n     }\n     \n     purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void purgeOldStorage() throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n      new FSImageTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N. However, we also\n    // provide a \"cushion\" of older txns that we keep, which is\n    // handy for HA, where a remote node may not have as many\n    // new images.\n    //\n    // First, determine the target number of extra transactions to retain based\n    // on the configured amount.\n    long minimumRequiredTxId \u003d minImageTxId + 1;\n    long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n    \n    ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n    Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n      @Override\n      public int compare(EditLogInputStream a, EditLogInputStream b) {\n        return ComparisonChain.start()\n            .compare(a.getFirstTxId(), b.getFirstTxId())\n            .compare(a.getLastTxId(), b.getLastTxId())\n            .result();\n      }\n    });\n\n    // Remove from consideration any edit logs that are in fact required.\n    while (editLogs.size() \u003e 0 \u0026\u0026\n        editLogs.get(editLogs.size() - 1).getFirstTxId() \u003e\u003d minimumRequiredTxId) {\n      editLogs.remove(editLogs.size() - 1);\n    }\n    \n    // Next, adjust the number of transactions to retain if doing so would mean\n    // keeping too many segments around.\n    while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n      purgeLogsFrom \u003d editLogs.get(0).getLastTxId() + 1;\n      editLogs.remove(0);\n    }\n    \n    // Finally, ensure that we\u0027re not trying to purge any transactions that we\n    // actually need.\n    if (purgeLogsFrom \u003e minimumRequiredTxId) {\n      throw new AssertionError(\"Should not purge more edits than required to \"\n          + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n          + minimumRequiredTxId);\n    }\n    \n    purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
      "extendedDetails": {}
    },
    "557ffe2101325438f15dbb218128d327984ecb11": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2946. HA: Put a cap on the number of completed edits files retained by the NN. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1398609 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/10/12 6:59 PM",
      "commitName": "557ffe2101325438f15dbb218128d327984ecb11",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/08/12 11:52 AM",
      "commitNameOld": "92cb6b093c7e3a39083c0497d80bd7e4eeae9c7f",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 52.3,
      "commitsBetweenForRepo": 304,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,48 @@\n   public void purgeOldStorage() throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n       new FSImageTransactionalStorageInspector();\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n     // reflects the state up to and including N. However, we also\n     // provide a \"cushion\" of older txns that we keep, which is\n     // handy for HA, where a remote node may not have as many\n     // new images.\n-    long purgeLogsFrom \u003d Math.max(0, minImageTxId + 1 - numExtraEditsToRetain);\n+    //\n+    // First, determine the target number of extra transactions to retain based\n+    // on the configured amount.\n+    long minimumRequiredTxId \u003d minImageTxId + 1;\n+    long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n+    \n+    ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n+    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n+    Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n+      @Override\n+      public int compare(EditLogInputStream a, EditLogInputStream b) {\n+        return ComparisonChain.start()\n+            .compare(a.getFirstTxId(), b.getFirstTxId())\n+            .compare(a.getLastTxId(), b.getLastTxId())\n+            .result();\n+      }\n+    });\n+    \n+    // Next, adjust the number of transactions to retain if doing so would mean\n+    // keeping too many segments around.\n+    while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n+      purgeLogsFrom \u003d editLogs.get(0).getFirstTxId();\n+      editLogs.remove(0);\n+    }\n+    \n+    // Finally, ensure that we\u0027re not trying to purge any transactions that we\n+    // actually need.\n+    if (purgeLogsFrom \u003e minimumRequiredTxId) {\n+      throw new AssertionError(\"Should not purge more edits than required to \"\n+          + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n+          + minimumRequiredTxId);\n+    }\n+    \n     purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void purgeOldStorage() throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n      new FSImageTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N. However, we also\n    // provide a \"cushion\" of older txns that we keep, which is\n    // handy for HA, where a remote node may not have as many\n    // new images.\n    //\n    // First, determine the target number of extra transactions to retain based\n    // on the configured amount.\n    long minimumRequiredTxId \u003d minImageTxId + 1;\n    long purgeLogsFrom \u003d Math.max(0, minimumRequiredTxId - numExtraEditsToRetain);\n    \n    ArrayList\u003cEditLogInputStream\u003e editLogs \u003d new ArrayList\u003cEditLogInputStream\u003e();\n    purgeableLogs.selectInputStreams(editLogs, purgeLogsFrom, false);\n    Collections.sort(editLogs, new Comparator\u003cEditLogInputStream\u003e() {\n      @Override\n      public int compare(EditLogInputStream a, EditLogInputStream b) {\n        return ComparisonChain.start()\n            .compare(a.getFirstTxId(), b.getFirstTxId())\n            .compare(a.getLastTxId(), b.getLastTxId())\n            .result();\n      }\n    });\n    \n    // Next, adjust the number of transactions to retain if doing so would mean\n    // keeping too many segments around.\n    while (editLogs.size() \u003e maxExtraEditsSegmentsToRetain) {\n      purgeLogsFrom \u003d editLogs.get(0).getFirstTxId();\n      editLogs.remove(0);\n    }\n    \n    // Finally, ensure that we\u0027re not trying to purge any transactions that we\n    // actually need.\n    if (purgeLogsFrom \u003e minimumRequiredTxId) {\n      throw new AssertionError(\"Should not purge more edits than required to \"\n          + \"restore: \" + purgeLogsFrom + \" should be \u003c\u003d \"\n          + minimumRequiredTxId);\n    }\n    \n    purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
      "extendedDetails": {}
    },
    "92cb6b093c7e3a39083c0497d80bd7e4eeae9c7f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3678. Edit log files are never being purged from 2NN. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1377046 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/12 11:52 AM",
      "commitName": "92cb6b093c7e3a39083c0497d80bd7e4eeae9c7f",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "06/02/12 5:39 PM",
      "commitNameOld": "7f0a99f6e63fcbc4e6971b719d235af1cf5cb514",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 199.72,
      "commitsBetweenForRepo": 1271,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   public void purgeOldStorage() throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n       new FSImageTransactionalStorageInspector();\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n     // reflects the state up to and including N. However, we also\n     // provide a \"cushion\" of older txns that we keep, which is\n     // handy for HA, where a remote node may not have as many\n     // new images.\n     long purgeLogsFrom \u003d Math.max(0, minImageTxId + 1 - numExtraEditsToRetain);\n-    editLog.purgeLogsOlderThan(purgeLogsFrom);\n+    purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void purgeOldStorage() throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n      new FSImageTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N. However, we also\n    // provide a \"cushion\" of older txns that we keep, which is\n    // handy for HA, where a remote node may not have as many\n    // new images.\n    long purgeLogsFrom \u003d Math.max(0, minImageTxId + 1 - numExtraEditsToRetain);\n    purgeableLogs.purgeLogsOlderThan(purgeLogsFrom);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
      "extendedDetails": {}
    },
    "7f0a99f6e63fcbc4e6971b719d235af1cf5cb514": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2794. Active NN may purge edit log files before standby NN has a chance to read them. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1241317 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/02/12 5:39 PM",
      "commitName": "7f0a99f6e63fcbc4e6971b719d235af1cf5cb514",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 166.06,
      "commitsBetweenForRepo": 1089,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,16 @@\n   public void purgeOldStorage() throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n       new FSImageTransactionalStorageInspector();\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n-    // reflects the state up to and including N.\n-    editLog.purgeLogsOlderThan(minImageTxId + 1);\n+    // reflects the state up to and including N. However, we also\n+    // provide a \"cushion\" of older txns that we keep, which is\n+    // handy for HA, where a remote node may not have as many\n+    // new images.\n+    long purgeLogsFrom \u003d Math.max(0, minImageTxId + 1 - numExtraEditsToRetain);\n+    editLog.purgeLogsOlderThan(purgeLogsFrom);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void purgeOldStorage() throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n      new FSImageTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N. However, we also\n    // provide a \"cushion\" of older txns that we keep, which is\n    // handy for HA, where a remote node may not have as many\n    // new images.\n    long purgeLogsFrom \u003d Math.max(0, minImageTxId + 1 - numExtraEditsToRetain);\n    editLog.purgeLogsOlderThan(purgeLogsFrom);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void purgeOldStorage() throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n      new FSImageTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N.\n    editLog.purgeLogsOlderThan(minImageTxId + 1);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void purgeOldStorage() throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n      new FSImageTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N.\n    editLog.purgeLogsOlderThan(minImageTxId + 1);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java"
      }
    },
    "23762da4fa17ce6ea7b70722147977123a28a7e6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2225. Refactor file management so it\u0027s not in classes which should be generic. Contributed by Ivan Kelly.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154029 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/08/11 2:56 PM",
      "commitName": "23762da4fa17ce6ea7b70722147977123a28a7e6",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "29/07/11 9:28 AM",
      "commitNameOld": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 6.23,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,12 @@\n   public void purgeOldStorage() throws IOException {\n     FSImageTransactionalStorageInspector inspector \u003d\n       new FSImageTransactionalStorageInspector();\n     storage.inspectStorageDirs(inspector);\n \n     long minImageTxId \u003d getImageTxIdToRetain(inspector);\n     purgeCheckpointsOlderThan(inspector, minImageTxId);\n     // If fsimage_N is the image we want to keep, then we need to keep\n     // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n     // reflects the state up to and including N.\n-    editLog.purgeLogsOlderThan(minImageTxId + 1, purger);\n+    editLog.purgeLogsOlderThan(minImageTxId + 1);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void purgeOldStorage() throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n      new FSImageTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N.\n    editLog.purgeLogsOlderThan(minImageTxId + 1);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java",
      "extendedDetails": {}
    },
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/07/11 9:28 AM",
      "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,12 @@\n+  public void purgeOldStorage() throws IOException {\n+    FSImageTransactionalStorageInspector inspector \u003d\n+      new FSImageTransactionalStorageInspector();\n+    storage.inspectStorageDirs(inspector);\n+\n+    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n+    purgeCheckpointsOlderThan(inspector, minImageTxId);\n+    // If fsimage_N is the image we want to keep, then we need to keep\n+    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n+    // reflects the state up to and including N.\n+    editLog.purgeLogsOlderThan(minImageTxId + 1, purger);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void purgeOldStorage() throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n      new FSImageTransactionalStorageInspector();\n    storage.inspectStorageDirs(inspector);\n\n    long minImageTxId \u003d getImageTxIdToRetain(inspector);\n    purgeCheckpointsOlderThan(inspector, minImageTxId);\n    // If fsimage_N is the image we want to keep, then we need to keep\n    // all txns \u003e N. We can remove anything \u003c N+1, since fsimage_N\n    // reflects the state up to and including N.\n    editLog.purgeLogsOlderThan(minImageTxId + 1, purger);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java"
    }
  }
}