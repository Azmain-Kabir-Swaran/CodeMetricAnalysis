{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirWriteFileOp.java",
  "functionName": "unprotectedRemoveBlock",
  "functionId": "unprotectedRemoveBlock___fsd-FSDirectory__path-String__iip-INodesInPath__fileNode-INodeFile__block-Block",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
  "functionStartLine": 75,
  "functionEndLine": 100,
  "numCommitsSeen": 674,
  "timeTaken": 18208,
  "changeHistory": [
    "09d63d5a192b5d6b172f94ff6c94da348fd49ea6",
    "830eb252aaa4fec7ef2ec38cb66f669e8e1ecaa5",
    "164cbe643988f878f0f4100a4de51783e5b6738e",
    "e535e0f05b5fbd087c93238deb888cc985254b4c",
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835",
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75",
    "d62b63d297bff12d93de560dd50ddd48743b851d",
    "c17439c2ddd921b63b1635e6f1cba634b8da8557",
    "e5afac5896a1a88e152746598527d91f73cbb724",
    "6d5da9484185ca9f585195d6da069b9cd5be4044",
    "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
    "5dae97a584d30cef3e34141edfaca49c4ec57913",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
    "0689363343a281a6f7f6f395227668bddc2663eb",
    "8df119da214babde03e73243c7ca4cfe6d0ca562",
    "ce68f410b05a58ad05965f32ad7f5b246b363a75",
    "11c073134afc878619c37c95935d6a3098a21f17",
    "e2a618e1cc3fb99115547af6540932860dc6766e",
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
    "2372e394dd99d69d396327d5a5e172953a8b8c6a",
    "34413c2000d9262faa37fde88a72939587edc776",
    "7ee5ce3176a74d217551b5981f809a56c719424b",
    "9a0651b4b86727910ae29d055aac6a23490b5ed3",
    "ad06a087131d69d173d8e03dce5c97650a530f2e",
    "191db6a9073e8660440c85d2c1a65e2a48b4b45c",
    "71071b904d0c9aec7b3713d41740f24182e81c36",
    "10dc6b09272dbf2022907681e134104e7d418021",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "09d63d5a192b5d6b172f94ff6c94da348fd49ea6": "Ybodychange",
    "830eb252aaa4fec7ef2ec38cb66f669e8e1ecaa5": "Ybodychange",
    "164cbe643988f878f0f4100a4de51783e5b6738e": "Ybodychange",
    "e535e0f05b5fbd087c93238deb888cc985254b4c": "Ybodychange",
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835": "Ybodychange",
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75": "Yparameterchange",
    "d62b63d297bff12d93de560dd50ddd48743b851d": "Yparameterchange",
    "c17439c2ddd921b63b1635e6f1cba634b8da8557": "Ybodychange",
    "e5afac5896a1a88e152746598527d91f73cbb724": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "6d5da9484185ca9f585195d6da069b9cd5be4044": "Ybodychange",
    "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f": "Ybodychange",
    "5dae97a584d30cef3e34141edfaca49c4ec57913": "Ybodychange",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": "Ybodychange",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": "Ymultichange(Yparameterchange,Ybodychange)",
    "0689363343a281a6f7f6f395227668bddc2663eb": "Ybodychange",
    "8df119da214babde03e73243c7ca4cfe6d0ca562": "Ybodychange",
    "ce68f410b05a58ad05965f32ad7f5b246b363a75": "Ymultichange(Yparameterchange,Ybodychange)",
    "11c073134afc878619c37c95935d6a3098a21f17": "Ymultichange(Yreturntypechange,Ybodychange)",
    "e2a618e1cc3fb99115547af6540932860dc6766e": "Ybodychange",
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd": "Ybodychange",
    "2372e394dd99d69d396327d5a5e172953a8b8c6a": "Ybodychange",
    "34413c2000d9262faa37fde88a72939587edc776": "Ybodychange",
    "7ee5ce3176a74d217551b5981f809a56c719424b": "Ybodychange",
    "9a0651b4b86727910ae29d055aac6a23490b5ed3": "Ybodychange",
    "ad06a087131d69d173d8e03dce5c97650a530f2e": "Ybodychange",
    "191db6a9073e8660440c85d2c1a65e2a48b4b45c": "Yintroduced",
    "71071b904d0c9aec7b3713d41740f24182e81c36": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Yexceptionschange,Ybodychange)",
    "10dc6b09272dbf2022907681e134104e7d418021": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "09d63d5a192b5d6b172f94ff6c94da348fd49ea6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5177. blocksScheduled count should be decremented for abandoned blocks (Contributed by Vinayakumar B)\n",
      "commitDate": "29/03/16 11:22 PM",
      "commitName": "09d63d5a192b5d6b172f94ff6c94da348fd49ea6",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "14/03/16 9:54 AM",
      "commitNameOld": "5644137adad30c84e40d2c4719627b3aabc73628",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 15.56,
      "commitsBetweenForRepo": 88,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,26 @@\n   static boolean unprotectedRemoveBlock(\n       FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     BlockInfo uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n+    if (uc.getUnderConstructionFeature() !\u003d null) {\n+      DatanodeStorageInfo.decrementBlocksScheduled(uc\n+          .getUnderConstructionFeature().getExpectedStorageLocations());\n+    }\n     fsd.getBlockManager().removeBlockFromMap(uc);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n         fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfo uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    if (uc.getUnderConstructionFeature() !\u003d null) {\n      DatanodeStorageInfo.decrementBlocksScheduled(uc\n          .getUnderConstructionFeature().getExpectedStorageLocations());\n    }\n    fsd.getBlockManager().removeBlockFromMap(uc);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n        fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "830eb252aaa4fec7ef2ec38cb66f669e8e1ecaa5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9485. Make BlockManager#removeFromExcessReplicateMap accept BlockInfo instead of Block. Contributed by Mingliang Liu.\n",
      "commitDate": "01/12/15 1:05 PM",
      "commitName": "830eb252aaa4fec7ef2ec38cb66f669e8e1ecaa5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "21/10/15 4:58 PM",
      "commitNameOld": "3dadf369d550c2ae393b751cb5a184dbfe2814df",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 40.88,
      "commitsBetweenForRepo": 314,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   static boolean unprotectedRemoveBlock(\n       FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     BlockInfo uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n-    fsd.getBlockManager().removeBlockFromMap(block);\n+    fsd.getBlockManager().removeBlockFromMap(uc);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n         fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfo uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    fsd.getBlockManager().removeBlockFromMap(uc);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n        fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "164cbe643988f878f0f4100a4de51783e5b6738e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8909. Erasure coding: update BlockInfoContiguousUC and BlockInfoStripedUC to use BlockUnderConstructionFeature. Contributed by Jing Zhao.\n",
      "commitDate": "27/08/15 1:02 AM",
      "commitName": "164cbe643988f878f0f4100a4de51783e5b6738e",
      "commitAuthor": "Walter Su",
      "commitDateOld": "26/08/15 6:11 PM",
      "commitNameOld": "067ec8c2b14fb0929dc348b763383838e06ff8a5",
      "commitAuthorOld": "Walter Su",
      "daysBetweenCommits": 0.29,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   static boolean unprotectedRemoveBlock(\n       FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n-    BlockInfoUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n+    BlockInfo uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n     fsd.getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n         fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfo uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    fsd.getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n        fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "e535e0f05b5fbd087c93238deb888cc985254b4c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8801. Convert BlockInfoUnderConstruction as a feature. Contributed by Jing Zhao.\n",
      "commitDate": "17/08/15 11:28 AM",
      "commitName": "e535e0f05b5fbd087c93238deb888cc985254b4c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "11/08/15 10:28 AM",
      "commitNameOld": "1fc3c779a422bafdb86ad1a5b2349802dda1cb62",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 6.04,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   static boolean unprotectedRemoveBlock(\n       FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n-    BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n+    BlockInfo uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n     fsd.getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                     fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfo uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    fsd.getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                    fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8499. Refactor BlockInfo class hierarchy with static helper class. Contributed by Zhe Zhang.\"\n\nThis reverts commit c17439c2ddd921b63b1635e6f1cba634b8da8557.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "f4c523b69ba55b1fd35e8995c3011a9f546ac835",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "21/07/15 5:25 PM",
      "commitNameOld": "31f117138a00794de4951ee8433e304d72b04094",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 15.71,
      "commitsBetweenForRepo": 92,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   static boolean unprotectedRemoveBlock(\n       FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n-    BlockInfoUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n+    BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n     fsd.getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                     fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    fsd.getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                    fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75": {
      "type": "Yparameterchange",
      "commitMessage": "Revert \"HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\"\n\nThis reverts commit d62b63d297bff12d93de560dd50ddd48743b851d.\n",
      "commitDate": "07/07/15 10:13 AM",
      "commitName": "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "06/07/15 3:54 PM",
      "commitNameOld": "d62b63d297bff12d93de560dd50ddd48743b851d",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.76,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   static boolean unprotectedRemoveBlock(\n       FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n-      BlockInfo block) throws IOException {\n+      Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     BlockInfoUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n     fsd.getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                     fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfoUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    fsd.getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                    fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {
        "oldValue": "[fsd-FSDirectory, path-String, iip-INodesInPath, fileNode-INodeFile, block-BlockInfo]",
        "newValue": "[fsd-FSDirectory, path-String, iip-INodesInPath, fileNode-INodeFile, block-Block]"
      }
    },
    "d62b63d297bff12d93de560dd50ddd48743b851d": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\n",
      "commitDate": "06/07/15 3:54 PM",
      "commitName": "d62b63d297bff12d93de560dd50ddd48743b851d",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/06/15 11:38 AM",
      "commitNameOld": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 24.18,
      "commitsBetweenForRepo": 153,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   static boolean unprotectedRemoveBlock(\n       FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n-      Block block) throws IOException {\n+      BlockInfo block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     BlockInfoUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n     fsd.getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                     fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      BlockInfo block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfoUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    fsd.getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                    fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {
        "oldValue": "[fsd-FSDirectory, path-String, iip-INodesInPath, fileNode-INodeFile, block-Block]",
        "newValue": "[fsd-FSDirectory, path-String, iip-INodesInPath, fileNode-INodeFile, block-BlockInfo]"
      }
    },
    "c17439c2ddd921b63b1635e6f1cba634b8da8557": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8499. Refactor BlockInfo class hierarchy with static helper class. Contributed by Zhe Zhang.\n",
      "commitDate": "12/06/15 11:38 AM",
      "commitName": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/05/15 3:42 PM",
      "commitNameOld": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 15.83,
      "commitsBetweenForRepo": 122,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   static boolean unprotectedRemoveBlock(\n       FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n-    BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n+    BlockInfoUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n     fsd.getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                     fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfoUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    fsd.getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                    fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "e5afac5896a1a88e152746598527d91f73cbb724": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-8394. Move getAdditionalBlock() and related functionalities into a separate class. Contributed by Haohui Mai.\n",
      "commitDate": "15/05/15 7:09 PM",
      "commitName": "e5afac5896a1a88e152746598527d91f73cbb724",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-8394. Move getAdditionalBlock() and related functionalities into a separate class. Contributed by Haohui Mai.\n",
          "commitDate": "15/05/15 7:09 PM",
          "commitName": "e5afac5896a1a88e152746598527d91f73cbb724",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/05/15 4:14 PM",
          "commitNameOld": "8f378733423a5244461df79a92c00239514b8b93",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.12,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,22 @@\n-  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n-      INodeFile fileNode, Block block) throws IOException {\n+  static boolean unprotectedRemoveBlock(\n+      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n+      Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n-    getBlockManager().removeBlockFromMap(block);\n+    fsd.getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n-        fileNode.getPreferredBlockReplication(), true);\n+    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n+                    fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    fsd.getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                    fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
            "oldMethodName": "unprotectedRemoveBlock",
            "newMethodName": "unprotectedRemoveBlock"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-8394. Move getAdditionalBlock() and related functionalities into a separate class. Contributed by Haohui Mai.\n",
          "commitDate": "15/05/15 7:09 PM",
          "commitName": "e5afac5896a1a88e152746598527d91f73cbb724",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/05/15 4:14 PM",
          "commitNameOld": "8f378733423a5244461df79a92c00239514b8b93",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.12,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,22 @@\n-  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n-      INodeFile fileNode, Block block) throws IOException {\n+  static boolean unprotectedRemoveBlock(\n+      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n+      Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n-    getBlockManager().removeBlockFromMap(block);\n+    fsd.getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n-        fileNode.getPreferredBlockReplication(), true);\n+    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n+                    fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    fsd.getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                    fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8394. Move getAdditionalBlock() and related functionalities into a separate class. Contributed by Haohui Mai.\n",
          "commitDate": "15/05/15 7:09 PM",
          "commitName": "e5afac5896a1a88e152746598527d91f73cbb724",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/05/15 4:14 PM",
          "commitNameOld": "8f378733423a5244461df79a92c00239514b8b93",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.12,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,22 @@\n-  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n-      INodeFile fileNode, Block block) throws IOException {\n+  static boolean unprotectedRemoveBlock(\n+      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n+      Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n-    getBlockManager().removeBlockFromMap(block);\n+    fsd.getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n-        fileNode.getPreferredBlockReplication(), true);\n+    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n+                    fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    fsd.getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                    fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8394. Move getAdditionalBlock() and related functionalities into a separate class. Contributed by Haohui Mai.\n",
          "commitDate": "15/05/15 7:09 PM",
          "commitName": "e5afac5896a1a88e152746598527d91f73cbb724",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/05/15 4:14 PM",
          "commitNameOld": "8f378733423a5244461df79a92c00239514b8b93",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.12,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,22 @@\n-  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n-      INodeFile fileNode, Block block) throws IOException {\n+  static boolean unprotectedRemoveBlock(\n+      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n+      Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n-    getBlockManager().removeBlockFromMap(block);\n+    fsd.getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n-        fileNode.getPreferredBlockReplication(), true);\n+    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n+                    fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static boolean unprotectedRemoveBlock(\n      FSDirectory fsd, String path, INodesInPath iip, INodeFile fileNode,\n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    fsd.getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    fsd.updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n                    fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
          "extendedDetails": {
            "oldValue": "[path-String, iip-INodesInPath, fileNode-INodeFile, block-Block]",
            "newValue": "[fsd-FSDirectory, path-String, iip-INodesInPath, fileNode-INodeFile, block-Block]"
          }
        }
      ]
    },
    "6d5da9484185ca9f585195d6da069b9cd5be4044": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8255. Rename getBlockReplication to getPreferredBlockReplication. (Contributed by Zhe Zhang)\n",
      "commitDate": "12/05/15 6:29 AM",
      "commitName": "6d5da9484185ca9f585195d6da069b9cd5be4044",
      "commitAuthor": "yliu",
      "commitDateOld": "08/05/15 11:09 PM",
      "commitNameOld": "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.31,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n       INodeFile fileNode, Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n     if (uc \u003d\u003d null) {\n       return false;\n     }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n-        fileNode.getBlockReplication(), true);\n+        fileNode.getPreferredBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n      INodeFile fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n        fileNode.getPreferredBlockReplication(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8327. Compute storage type quotas in INodeFile.computeQuotaDeltaForTruncate(). Contributed by Haohui Mai.\n",
      "commitDate": "08/05/15 11:09 PM",
      "commitName": "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.55,
      "commitsBetweenForRepo": 131,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n       INodeFile fileNode, Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n-    boolean removed \u003d fileNode.removeLastBlock(block);\n-    if (!removed) {\n+    BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n+    if (uc \u003d\u003d null) {\n       return false;\n     }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n         fileNode.getBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n      INodeFile fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    BlockInfoContiguousUnderConstruction uc \u003d fileNode.removeLastBlock(block);\n    if (uc \u003d\u003d null) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n        fileNode.getBlockReplication(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "5dae97a584d30cef3e34141edfaca49c4ec57913": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
      "commitDate": "11/02/15 10:41 AM",
      "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "08/02/15 11:51 AM",
      "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.95,
      "commitsBetweenForRepo": 40,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,21 @@\n   boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n       INodeFile fileNode, Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     boolean removed \u003d fileNode.removeLastBlock(block);\n     if (!removed) {\n       return false;\n     }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    updateCount(iip, 0, -fileNode.getPreferredBlockDiskspace(), true);\n+    updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n+        fileNode.getBlockReplication(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n      INodeFile fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    boolean removed \u003d fileNode.removeLastBlock(block);\n    if (!removed) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    updateCount(iip, 0, -fileNode.getPreferredBlockSize(),\n        fileNode.getBlockReplication(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3689. Add support for variable length block. Contributed by Jing Zhao.\n",
      "commitDate": "27/01/15 12:58 PM",
      "commitName": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "19/01/15 10:28 AM",
      "commitNameOld": "e843a0a8cee5c704a5d28cf14b5a4050094d341b",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 8.1,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n       INodeFile fileNode, Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     boolean removed \u003d fileNode.removeLastBlock(block);\n     if (!removed) {\n       return false;\n     }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n+    updateCount(iip, 0, -fileNode.getPreferredBlockDiskspace(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n      INodeFile fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    boolean removed \u003d fileNode.removeLastBlock(block);\n    if (!removed) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    updateCount(iip, 0, -fileNode.getPreferredBlockDiskspace(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
      "commitDate": "12/12/14 3:13 PM",
      "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
          "commitDate": "12/12/14 3:13 PM",
          "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "09/12/14 11:37 AM",
          "commitNameOld": "5776a41da08af653206bb94d7c76c9c4dcce059a",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 3.15,
          "commitsBetweenForRepo": 33,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,20 @@\n-  boolean unprotectedRemoveBlock(String path,\n+  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n       INodeFile fileNode, Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     boolean removed \u003d fileNode.removeLastBlock(block);\n     if (!removed) {\n       return false;\n     }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    final INodesInPath iip \u003d getINodesInPath4Write(path, true);\n     updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n      INodeFile fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    boolean removed \u003d fileNode.removeLastBlock(block);\n    if (!removed) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[path-String, fileNode-INodeFile, block-Block]",
            "newValue": "[path-String, iip-INodesInPath, fileNode-INodeFile, block-Block]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
          "commitDate": "12/12/14 3:13 PM",
          "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "09/12/14 11:37 AM",
          "commitNameOld": "5776a41da08af653206bb94d7c76c9c4dcce059a",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 3.15,
          "commitsBetweenForRepo": 33,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,20 @@\n-  boolean unprotectedRemoveBlock(String path,\n+  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n       INodeFile fileNode, Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     boolean removed \u003d fileNode.removeLastBlock(block);\n     if (!removed) {\n       return false;\n     }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    final INodesInPath iip \u003d getINodesInPath4Write(path, true);\n     updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean unprotectedRemoveBlock(String path, INodesInPath iip,\n      INodeFile fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    boolean removed \u003d fileNode.removeLastBlock(block);\n    if (!removed) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    },
    "0689363343a281a6f7f6f395227668bddc2663eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6304. Consolidate the logic of path resolution in FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1591411 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/04/14 10:44 AM",
      "commitName": "0689363343a281a6f7f6f395227668bddc2663eb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/04/14 7:05 PM",
      "commitNameOld": "10a037cccb00c9f791da394bf2dc05985fb80612",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 5.65,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   boolean unprotectedRemoveBlock(String path,\n       INodeFile fileNode, Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     // fileNode should be under construction\n     boolean removed \u003d fileNode.removeLastBlock(block);\n     if (!removed) {\n       return false;\n     }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n+    final INodesInPath iip \u003d getINodesInPath4Write(path, true);\n     updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean unprotectedRemoveBlock(String path,\n      INodeFile fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    boolean removed \u003d fileNode.removeLastBlock(block);\n    if (!removed) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    final INodesInPath iip \u003d getINodesInPath4Write(path, true);\n    updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "8df119da214babde03e73243c7ca4cfe6d0ca562": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5537. Remove FileWithSnapshot interface.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546184 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 12:32 PM",
      "commitName": "8df119da214babde03e73243c7ca4cfe6d0ca562",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "26/11/13 10:33 AM",
      "commitNameOld": "82ff2d3f2e569879500d851f4d67dfa2d02b5c9b",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.08,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   boolean unprotectedRemoveBlock(String path,\n       INodeFile fileNode, Block block) throws IOException {\n-    Preconditions.checkArgument(fileNode.isUnderConstruction());\n     // modify file-\u003e block and blocksMap\n+    // fileNode should be under construction\n     boolean removed \u003d fileNode.removeLastBlock(block);\n     if (!removed) {\n       return false;\n     }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n     updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean unprotectedRemoveBlock(String path,\n      INodeFile fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    // fileNode should be under construction\n    boolean removed \u003d fileNode.removeLastBlock(block);\n    if (!removed) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n    updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "ce68f410b05a58ad05965f32ad7f5b246b363a75": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5285. Flatten INodeFile hierarchy: Replace INodeFileUnderConstruction and INodeFileUnderConstructionWithSnapshot with FileUnderContructionFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544389 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/13 5:39 PM",
      "commitName": "ce68f410b05a58ad05965f32ad7f5b246b363a75",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5285. Flatten INodeFile hierarchy: Replace INodeFileUnderConstruction and INodeFileUnderConstructionWithSnapshot with FileUnderContructionFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544389 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/11/13 5:39 PM",
          "commitName": "ce68f410b05a58ad05965f32ad7f5b246b363a75",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "20/11/13 7:17 PM",
          "commitNameOld": "5f458ef23f097c784f12a973b326f7e1254ae0b2",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.93,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,21 @@\n   boolean unprotectedRemoveBlock(String path,\n-      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n+      INodeFile fileNode, Block block) throws IOException {\n+    Preconditions.checkArgument(fileNode.isUnderConstruction());\n     // modify file-\u003e block and blocksMap\n     boolean removed \u003d fileNode.removeLastBlock(block);\n     if (!removed) {\n       return false;\n     }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n     updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean unprotectedRemoveBlock(String path,\n      INodeFile fileNode, Block block) throws IOException {\n    Preconditions.checkArgument(fileNode.isUnderConstruction());\n    // modify file-\u003e block and blocksMap\n    boolean removed \u003d fileNode.removeLastBlock(block);\n    if (!removed) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n    updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[path-String, fileNode-INodeFileUnderConstruction, block-Block]",
            "newValue": "[path-String, fileNode-INodeFile, block-Block]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5285. Flatten INodeFile hierarchy: Replace INodeFileUnderConstruction and INodeFileUnderConstructionWithSnapshot with FileUnderContructionFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544389 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/11/13 5:39 PM",
          "commitName": "ce68f410b05a58ad05965f32ad7f5b246b363a75",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "20/11/13 7:17 PM",
          "commitNameOld": "5f458ef23f097c784f12a973b326f7e1254ae0b2",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.93,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,21 @@\n   boolean unprotectedRemoveBlock(String path,\n-      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n+      INodeFile fileNode, Block block) throws IOException {\n+    Preconditions.checkArgument(fileNode.isUnderConstruction());\n     // modify file-\u003e block and blocksMap\n     boolean removed \u003d fileNode.removeLastBlock(block);\n     if (!removed) {\n       return false;\n     }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n     updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean unprotectedRemoveBlock(String path,\n      INodeFile fileNode, Block block) throws IOException {\n    Preconditions.checkArgument(fileNode.isUnderConstruction());\n    // modify file-\u003e block and blocksMap\n    boolean removed \u003d fileNode.removeLastBlock(block);\n    if (!removed) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n    updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    },
    "11c073134afc878619c37c95935d6a3098a21f17": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-5008. Make ClientProtocol#abandonBlock() idempotent. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505761 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/13 11:22 AM",
      "commitName": "11c073134afc878619c37c95935d6a3098a21f17",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5008. Make ClientProtocol#abandonBlock() idempotent. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505761 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/13 11:22 AM",
          "commitName": "11c073134afc878619c37c95935d6a3098a21f17",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "17/07/13 5:04 PM",
          "commitNameOld": "68faa67f1b3b681b40ecdc9002d9fb508e529af4",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 4.76,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,20 @@\n-  void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n-      Block block) throws IOException {\n+  boolean unprotectedRemoveBlock(String path,\n+      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n-    fileNode.removeLastBlock(block);\n+    boolean removed \u003d fileNode.removeLastBlock(block);\n+    if (!removed) {\n+      return false;\n+    }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n     updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n+    return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean unprotectedRemoveBlock(String path,\n      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    boolean removed \u003d fileNode.removeLastBlock(block);\n    if (!removed) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n    updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "boolean"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5008. Make ClientProtocol#abandonBlock() idempotent. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505761 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/13 11:22 AM",
          "commitName": "11c073134afc878619c37c95935d6a3098a21f17",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "17/07/13 5:04 PM",
          "commitNameOld": "68faa67f1b3b681b40ecdc9002d9fb508e529af4",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 4.76,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,20 @@\n-  void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n-      Block block) throws IOException {\n+  boolean unprotectedRemoveBlock(String path,\n+      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n-    fileNode.removeLastBlock(block);\n+    boolean removed \u003d fileNode.removeLastBlock(block);\n+    if (!removed) {\n+      return false;\n+    }\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n     updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n+    return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean unprotectedRemoveBlock(String path,\n      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    boolean removed \u003d fileNode.removeLastBlock(block);\n    if (!removed) {\n      return false;\n    }\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n    updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    },
    "e2a618e1cc3fb99115547af6540932860dc6766e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4523. Fix INodeFile replacement, TestQuota and javac errors from trunk merge.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1450477 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/02/13 2:04 PM",
      "commitName": "e2a618e1cc3fb99115547af6540932860dc6766e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "25/02/13 4:10 PM",
      "commitNameOld": "aa82b03823d809fb70cc3d420570ef20e3368bdf",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.91,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     fileNode.removeLastBlock(block);\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n-    updateCount(iip, 0, 0, -fileNode.getBlockDiskspace(), true);\n+    updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n    updateCount(iip, 0, -fileNode.getBlockDiskspace(), true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4503. Update computeContentSummary(..), spaceConsumedInTree(..) and diskspaceConsumed(..) in INode for snapshot.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1448373 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/02/13 12:02 PM",
      "commitName": "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "14/02/13 3:07 PM",
      "commitNameOld": "d9e2514d21c2ae356ee7fe8d4a857748b5defa4c",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,16 @@\n   void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     fileNode.removeLastBlock(block);\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n-    updateCount(iip, 0,\n-        -fileNode.getPreferredBlockSize()*fileNode.getFileReplication(), true);\n+    updateCount(iip, 0, 0, -fileNode.getBlockDiskspace(), true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n    updateCount(iip, 0, 0, -fileNode.getBlockDiskspace(), true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "2372e394dd99d69d396327d5a5e172953a8b8c6a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4189. Renames the getMutableXxx methods to getXxx4Write and fix a bug that some getExistingPathINodes calls should be getINodesInPath4Write.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1441193 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/01/13 1:13 PM",
      "commitName": "2372e394dd99d69d396327d5a5e172953a8b8c6a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "25/01/13 4:01 PM",
      "commitNameOld": "a3bf2083867db5d848ea14f145d120f02b820af2",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.88,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,17 @@\n   void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     fileNode.removeLastBlock(block);\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    final INodesInPath inodesInPath \u003d rootDir.getExistingPathINodes(path, true);\n-    final INode[] inodes \u003d inodesInPath.getINodes();\n-    updateCount(inodesInPath, inodes.length-1, 0,\n+    final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n+    updateCount(iip, 0,\n         -fileNode.getPreferredBlockSize()*fileNode.getFileReplication(), true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    final INodesInPath iip \u003d rootDir.getINodesInPath4Write(path, true);\n    updateCount(iip, 0,\n        -fileNode.getPreferredBlockSize()*fileNode.getFileReplication(), true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "34413c2000d9262faa37fde88a72939587edc776": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c 1406006 from trunk for HDFS-4151. Change the methods in FSDirectory to pass INodesInPath instead of INode[] as a parameter.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1406014 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/11/12 3:56 PM",
      "commitName": "34413c2000d9262faa37fde88a72939587edc776",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "04/11/12 5:22 PM",
      "commitNameOld": "b3bc2fb76e1aca8e7327d1d1a6e4c8a013c575de",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.94,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,18 @@\n   void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     fileNode.removeLastBlock(block);\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    INode[] pathINodes \u003d getExistingPathINodes(path);\n-    updateCount(pathINodes, pathINodes.length-1, 0,\n+    final INodesInPath inodesInPath \u003d rootDir.getExistingPathINodes(path, true);\n+    final INode[] inodes \u003d inodesInPath.getINodes();\n+    updateCount(inodesInPath, inodes.length-1, 0,\n         -fileNode.getPreferredBlockSize()*fileNode.getFileReplication(), true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    final INodesInPath inodesInPath \u003d rootDir.getExistingPathINodes(path, true);\n    final INode[] inodes \u003d inodesInPath.getINodes();\n    updateCount(inodesInPath, inodes.length-1, 0,\n        -fileNode.getPreferredBlockSize()*fileNode.getFileReplication(), true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "7ee5ce3176a74d217551b5981f809a56c719424b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4151. Change the methods in FSDirectory to pass INodesInPath instead of INode[] as a parameter.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1406006 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/11/12 3:26 PM",
      "commitName": "7ee5ce3176a74d217551b5981f809a56c719424b",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "02/11/12 5:20 PM",
      "commitNameOld": "d174f574bafcfefc635c64a47f258b1ce5d5c84e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.96,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,18 @@\n   void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     fileNode.removeLastBlock(block);\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n-    INode[] pathINodes \u003d getExistingPathINodes(path);\n-    updateCount(pathINodes, pathINodes.length-1, 0,\n+    final INodesInPath inodesInPath \u003d rootDir.getExistingPathINodes(path, true);\n+    final INode[] inodes \u003d inodesInPath.getINodes();\n+    updateCount(inodesInPath, inodes.length-1, 0,\n         -fileNode.getPreferredBlockSize()*fileNode.getBlockReplication(), true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    final INodesInPath inodesInPath \u003d rootDir.getExistingPathINodes(path, true);\n    final INode[] inodes \u003d inodesInPath.getINodes();\n    updateCount(inodesInPath, inodes.length-1, 0,\n        -fileNode.getPreferredBlockSize()*fileNode.getBlockReplication(), true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "9a0651b4b86727910ae29d055aac6a23490b5ed3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4078. Handle replication in snapshots.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1400743 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/10/12 5:11 PM",
      "commitName": "9a0651b4b86727910ae29d055aac6a23490b5ed3",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "21/10/12 2:35 PM",
      "commitNameOld": "820b5495caf032983f6bef2c1cd95af44ed7fa10",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.11,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,17 @@\n   void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     fileNode.removeLastBlock(block);\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     INode[] pathINodes \u003d getExistingPathINodes(path);\n     updateCount(pathINodes, pathINodes.length-1, 0,\n-        -fileNode.getPreferredBlockSize()*fileNode.getBlockReplication(), true);\n+        -fileNode.getPreferredBlockSize()*fileNode.getFileReplication(), true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    INode[] pathINodes \u003d getExistingPathINodes(path);\n    updateCount(pathINodes, pathINodes.length-1, 0,\n        -fileNode.getPreferredBlockSize()*fileNode.getFileReplication(), true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "ad06a087131d69d173d8e03dce5c97650a530f2e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4037. Rename the getReplication() method in BlockCollection to getBlockReplication(). \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1398288 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/10/12 6:48 AM",
      "commitName": "ad06a087131d69d173d8e03dce5c97650a530f2e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "25/07/12 2:52 PM",
      "commitNameOld": "972953bd778081b9e8a0c3778d6df5c5e97368fa",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 81.66,
      "commitsBetweenForRepo": 479,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,17 @@\n   void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n       Block block) throws IOException {\n     // modify file-\u003e block and blocksMap\n     fileNode.removeLastBlock(block);\n     getBlockManager().removeBlockFromMap(block);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n           +path+\" with \"+block\n           +\" block is removed from the file system\");\n     }\n \n     // update space consumed\n     INode[] pathINodes \u003d getExistingPathINodes(path);\n     updateCount(pathINodes, pathINodes.length-1, 0,\n-        -fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n+        -fileNode.getPreferredBlockSize()*fileNode.getBlockReplication(), true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    INode[] pathINodes \u003d getExistingPathINodes(path);\n    updateCount(pathINodes, pathINodes.length-1, 0,\n        -fileNode.getPreferredBlockSize()*fileNode.getBlockReplication(), true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "191db6a9073e8660440c85d2c1a65e2a48b4b45c": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2718. Optimize OP_ADD in edits loading. Contributed by Konstantin Shvachko.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1239760 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/02/12 10:57 AM",
      "commitName": "191db6a9073e8660440c85d2c1a65e2a48b4b45c",
      "commitAuthor": "Konstantin Shvachko",
      "diff": "@@ -0,0 +1,17 @@\n+  void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n+      Block block) throws IOException {\n+    // modify file-\u003e block and blocksMap\n+    fileNode.removeLastBlock(block);\n+    getBlockManager().removeBlockFromMap(block);\n+\n+    if(NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n+          +path+\" with \"+block\n+          +\" block is removed from the file system\");\n+    }\n+\n+    // update space consumed\n+    INode[] pathINodes \u003d getExistingPathINodes(path);\n+    updateCount(pathINodes, pathINodes.length-1, 0,\n+        -fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedRemoveBlock(String path, INodeFileUnderConstruction fileNode, \n      Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    INode[] pathINodes \u003d getExistingPathINodes(path);\n    updateCount(pathINodes, pathINodes.length-1, 0,\n        -fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java"
    },
    "71071b904d0c9aec7b3713d41740f24182e81c36": {
      "type": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-2602. NN should log newly-allocated blocks without losing BlockInfo. Contributed by Aaron T. Myers\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215036 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/12/11 8:18 PM",
      "commitName": "71071b904d0c9aec7b3713d41740f24182e81c36",
      "commitAuthor": "Aaron Myers",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-2602. NN should log newly-allocated blocks without losing BlockInfo. Contributed by Aaron T. Myers\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215036 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "15/12/11 8:18 PM",
          "commitName": "71071b904d0c9aec7b3713d41740f24182e81c36",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "29/10/11 1:11 PM",
          "commitNameOld": "64c019cccc266b9896746d45e314cc4a59ba2e6e",
          "commitAuthorOld": "Konstantin Shvachko",
          "daysBetweenCommits": 47.34,
          "commitsBetweenForRepo": 313,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,17 @@\n-  INode unprotectedAddFile( String path, \n-                            PermissionStatus permissions,\n-                            BlockInfo[] blocks, \n-                            short replication,\n-                            long modificationTime,\n-                            long atime,\n-                            long preferredBlockSize) \n-      throws UnresolvedLinkException {\n-    INode newNode;\n-    long diskspace \u003d UNKNOWN_DISK_SPACE;\n-    assert hasWriteLock();\n-    if (blocks \u003d\u003d null)\n-      newNode \u003d new INodeDirectory(permissions, modificationTime);\n-    else {\n-      newNode \u003d new INodeFile(permissions, blocks.length, replication,\n-                              modificationTime, atime, preferredBlockSize);\n-      diskspace \u003d ((INodeFile)newNode).diskspaceConsumed(blocks);\n-    }\n-    writeLock();\n-    try {\n-      try {\n-        newNode \u003d addNode(path, newNode, diskspace);\n-        if(newNode !\u003d null \u0026\u0026 blocks !\u003d null) {\n-          int nrBlocks \u003d blocks.length;\n-          // Add file-\u003eblock mapping\n-          INodeFile newF \u003d (INodeFile)newNode;\n-          for (int i \u003d 0; i \u003c nrBlocks; i++) {\n-            newF.setBlock(i, getBlockManager().addINode(blocks[i], newF));\n-          }\n-        }\n-      } catch (IOException e) {\n-        return null;\n-      }\n-      return newNode;\n-    } finally {\n-      writeUnlock();\n+  void unprotectedRemoveBlock(String path,\n+      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n+    // modify file-\u003e block and blocksMap\n+    fileNode.removeLastBlock(block);\n+    getBlockManager().removeBlockFromMap(block);\n+\n+    if(NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n+          +path+\" with \"+block\n+          +\" block is removed from the file system\");\n     }\n \n+    // update space consumed\n+    INode[] pathINodes \u003d getExistingPathINodes(path);\n+    updateCount(pathINodes, pathINodes.length - 1, 0,\n+        - fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void unprotectedRemoveBlock(String path,\n      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    INode[] pathINodes \u003d getExistingPathINodes(path);\n    updateCount(pathINodes, pathINodes.length - 1, 0,\n        - fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "unprotectedAddFile",
            "newValue": "unprotectedRemoveBlock"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2602. NN should log newly-allocated blocks without losing BlockInfo. Contributed by Aaron T. Myers\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215036 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "15/12/11 8:18 PM",
          "commitName": "71071b904d0c9aec7b3713d41740f24182e81c36",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "29/10/11 1:11 PM",
          "commitNameOld": "64c019cccc266b9896746d45e314cc4a59ba2e6e",
          "commitAuthorOld": "Konstantin Shvachko",
          "daysBetweenCommits": 47.34,
          "commitsBetweenForRepo": 313,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,17 @@\n-  INode unprotectedAddFile( String path, \n-                            PermissionStatus permissions,\n-                            BlockInfo[] blocks, \n-                            short replication,\n-                            long modificationTime,\n-                            long atime,\n-                            long preferredBlockSize) \n-      throws UnresolvedLinkException {\n-    INode newNode;\n-    long diskspace \u003d UNKNOWN_DISK_SPACE;\n-    assert hasWriteLock();\n-    if (blocks \u003d\u003d null)\n-      newNode \u003d new INodeDirectory(permissions, modificationTime);\n-    else {\n-      newNode \u003d new INodeFile(permissions, blocks.length, replication,\n-                              modificationTime, atime, preferredBlockSize);\n-      diskspace \u003d ((INodeFile)newNode).diskspaceConsumed(blocks);\n-    }\n-    writeLock();\n-    try {\n-      try {\n-        newNode \u003d addNode(path, newNode, diskspace);\n-        if(newNode !\u003d null \u0026\u0026 blocks !\u003d null) {\n-          int nrBlocks \u003d blocks.length;\n-          // Add file-\u003eblock mapping\n-          INodeFile newF \u003d (INodeFile)newNode;\n-          for (int i \u003d 0; i \u003c nrBlocks; i++) {\n-            newF.setBlock(i, getBlockManager().addINode(blocks[i], newF));\n-          }\n-        }\n-      } catch (IOException e) {\n-        return null;\n-      }\n-      return newNode;\n-    } finally {\n-      writeUnlock();\n+  void unprotectedRemoveBlock(String path,\n+      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n+    // modify file-\u003e block and blocksMap\n+    fileNode.removeLastBlock(block);\n+    getBlockManager().removeBlockFromMap(block);\n+\n+    if(NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n+          +path+\" with \"+block\n+          +\" block is removed from the file system\");\n     }\n \n+    // update space consumed\n+    INode[] pathINodes \u003d getExistingPathINodes(path);\n+    updateCount(pathINodes, pathINodes.length - 1, 0,\n+        - fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void unprotectedRemoveBlock(String path,\n      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    INode[] pathINodes \u003d getExistingPathINodes(path);\n    updateCount(pathINodes, pathINodes.length - 1, 0,\n        - fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[path-String, permissions-PermissionStatus, blocks-BlockInfo[], replication-short, modificationTime-long, atime-long, preferredBlockSize-long]",
            "newValue": "[path-String, fileNode-INodeFileUnderConstruction, block-Block]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-2602. NN should log newly-allocated blocks without losing BlockInfo. Contributed by Aaron T. Myers\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215036 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "15/12/11 8:18 PM",
          "commitName": "71071b904d0c9aec7b3713d41740f24182e81c36",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "29/10/11 1:11 PM",
          "commitNameOld": "64c019cccc266b9896746d45e314cc4a59ba2e6e",
          "commitAuthorOld": "Konstantin Shvachko",
          "daysBetweenCommits": 47.34,
          "commitsBetweenForRepo": 313,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,17 @@\n-  INode unprotectedAddFile( String path, \n-                            PermissionStatus permissions,\n-                            BlockInfo[] blocks, \n-                            short replication,\n-                            long modificationTime,\n-                            long atime,\n-                            long preferredBlockSize) \n-      throws UnresolvedLinkException {\n-    INode newNode;\n-    long diskspace \u003d UNKNOWN_DISK_SPACE;\n-    assert hasWriteLock();\n-    if (blocks \u003d\u003d null)\n-      newNode \u003d new INodeDirectory(permissions, modificationTime);\n-    else {\n-      newNode \u003d new INodeFile(permissions, blocks.length, replication,\n-                              modificationTime, atime, preferredBlockSize);\n-      diskspace \u003d ((INodeFile)newNode).diskspaceConsumed(blocks);\n-    }\n-    writeLock();\n-    try {\n-      try {\n-        newNode \u003d addNode(path, newNode, diskspace);\n-        if(newNode !\u003d null \u0026\u0026 blocks !\u003d null) {\n-          int nrBlocks \u003d blocks.length;\n-          // Add file-\u003eblock mapping\n-          INodeFile newF \u003d (INodeFile)newNode;\n-          for (int i \u003d 0; i \u003c nrBlocks; i++) {\n-            newF.setBlock(i, getBlockManager().addINode(blocks[i], newF));\n-          }\n-        }\n-      } catch (IOException e) {\n-        return null;\n-      }\n-      return newNode;\n-    } finally {\n-      writeUnlock();\n+  void unprotectedRemoveBlock(String path,\n+      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n+    // modify file-\u003e block and blocksMap\n+    fileNode.removeLastBlock(block);\n+    getBlockManager().removeBlockFromMap(block);\n+\n+    if(NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n+          +path+\" with \"+block\n+          +\" block is removed from the file system\");\n     }\n \n+    // update space consumed\n+    INode[] pathINodes \u003d getExistingPathINodes(path);\n+    updateCount(pathINodes, pathINodes.length - 1, 0,\n+        - fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void unprotectedRemoveBlock(String path,\n      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    INode[] pathINodes \u003d getExistingPathINodes(path);\n    updateCount(pathINodes, pathINodes.length - 1, 0,\n        - fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "INode",
            "newValue": "void"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-2602. NN should log newly-allocated blocks without losing BlockInfo. Contributed by Aaron T. Myers\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215036 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "15/12/11 8:18 PM",
          "commitName": "71071b904d0c9aec7b3713d41740f24182e81c36",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "29/10/11 1:11 PM",
          "commitNameOld": "64c019cccc266b9896746d45e314cc4a59ba2e6e",
          "commitAuthorOld": "Konstantin Shvachko",
          "daysBetweenCommits": 47.34,
          "commitsBetweenForRepo": 313,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,17 @@\n-  INode unprotectedAddFile( String path, \n-                            PermissionStatus permissions,\n-                            BlockInfo[] blocks, \n-                            short replication,\n-                            long modificationTime,\n-                            long atime,\n-                            long preferredBlockSize) \n-      throws UnresolvedLinkException {\n-    INode newNode;\n-    long diskspace \u003d UNKNOWN_DISK_SPACE;\n-    assert hasWriteLock();\n-    if (blocks \u003d\u003d null)\n-      newNode \u003d new INodeDirectory(permissions, modificationTime);\n-    else {\n-      newNode \u003d new INodeFile(permissions, blocks.length, replication,\n-                              modificationTime, atime, preferredBlockSize);\n-      diskspace \u003d ((INodeFile)newNode).diskspaceConsumed(blocks);\n-    }\n-    writeLock();\n-    try {\n-      try {\n-        newNode \u003d addNode(path, newNode, diskspace);\n-        if(newNode !\u003d null \u0026\u0026 blocks !\u003d null) {\n-          int nrBlocks \u003d blocks.length;\n-          // Add file-\u003eblock mapping\n-          INodeFile newF \u003d (INodeFile)newNode;\n-          for (int i \u003d 0; i \u003c nrBlocks; i++) {\n-            newF.setBlock(i, getBlockManager().addINode(blocks[i], newF));\n-          }\n-        }\n-      } catch (IOException e) {\n-        return null;\n-      }\n-      return newNode;\n-    } finally {\n-      writeUnlock();\n+  void unprotectedRemoveBlock(String path,\n+      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n+    // modify file-\u003e block and blocksMap\n+    fileNode.removeLastBlock(block);\n+    getBlockManager().removeBlockFromMap(block);\n+\n+    if(NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n+          +path+\" with \"+block\n+          +\" block is removed from the file system\");\n     }\n \n+    // update space consumed\n+    INode[] pathINodes \u003d getExistingPathINodes(path);\n+    updateCount(pathINodes, pathINodes.length - 1, 0,\n+        - fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void unprotectedRemoveBlock(String path,\n      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    INode[] pathINodes \u003d getExistingPathINodes(path);\n    updateCount(pathINodes, pathINodes.length - 1, 0,\n        - fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[UnresolvedLinkException]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2602. NN should log newly-allocated blocks without losing BlockInfo. Contributed by Aaron T. Myers\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215036 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "15/12/11 8:18 PM",
          "commitName": "71071b904d0c9aec7b3713d41740f24182e81c36",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "29/10/11 1:11 PM",
          "commitNameOld": "64c019cccc266b9896746d45e314cc4a59ba2e6e",
          "commitAuthorOld": "Konstantin Shvachko",
          "daysBetweenCommits": 47.34,
          "commitsBetweenForRepo": 313,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,17 @@\n-  INode unprotectedAddFile( String path, \n-                            PermissionStatus permissions,\n-                            BlockInfo[] blocks, \n-                            short replication,\n-                            long modificationTime,\n-                            long atime,\n-                            long preferredBlockSize) \n-      throws UnresolvedLinkException {\n-    INode newNode;\n-    long diskspace \u003d UNKNOWN_DISK_SPACE;\n-    assert hasWriteLock();\n-    if (blocks \u003d\u003d null)\n-      newNode \u003d new INodeDirectory(permissions, modificationTime);\n-    else {\n-      newNode \u003d new INodeFile(permissions, blocks.length, replication,\n-                              modificationTime, atime, preferredBlockSize);\n-      diskspace \u003d ((INodeFile)newNode).diskspaceConsumed(blocks);\n-    }\n-    writeLock();\n-    try {\n-      try {\n-        newNode \u003d addNode(path, newNode, diskspace);\n-        if(newNode !\u003d null \u0026\u0026 blocks !\u003d null) {\n-          int nrBlocks \u003d blocks.length;\n-          // Add file-\u003eblock mapping\n-          INodeFile newF \u003d (INodeFile)newNode;\n-          for (int i \u003d 0; i \u003c nrBlocks; i++) {\n-            newF.setBlock(i, getBlockManager().addINode(blocks[i], newF));\n-          }\n-        }\n-      } catch (IOException e) {\n-        return null;\n-      }\n-      return newNode;\n-    } finally {\n-      writeUnlock();\n+  void unprotectedRemoveBlock(String path,\n+      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n+    // modify file-\u003e block and blocksMap\n+    fileNode.removeLastBlock(block);\n+    getBlockManager().removeBlockFromMap(block);\n+\n+    if(NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n+          +path+\" with \"+block\n+          +\" block is removed from the file system\");\n     }\n \n+    // update space consumed\n+    INode[] pathINodes \u003d getExistingPathINodes(path);\n+    updateCount(pathINodes, pathINodes.length - 1, 0,\n+        - fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void unprotectedRemoveBlock(String path,\n      INodeFileUnderConstruction fileNode, Block block) throws IOException {\n    // modify file-\u003e block and blocksMap\n    fileNode.removeLastBlock(block);\n    getBlockManager().removeBlockFromMap(block);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.removeBlock: \"\n          +path+\" with \"+block\n          +\" block is removed from the file system\");\n    }\n\n    // update space consumed\n    INode[] pathINodes \u003d getExistingPathINodes(path);\n    updateCount(pathINodes, pathINodes.length - 1, 0,\n        - fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    },
    "10dc6b09272dbf2022907681e134104e7d418021": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1869. mkdirs should use the supplied permission for all of the created directories.  Contributed by Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189546 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/10/11 5:02 PM",
      "commitName": "10dc6b09272dbf2022907681e134104e7d418021",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "22/09/11 1:25 AM",
      "commitNameOld": "d773bf0fb57bf6fb77dbdd52e1c186833c17361c",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 34.65,
      "commitsBetweenForRepo": 266,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,39 @@\n   INode unprotectedAddFile( String path, \n                             PermissionStatus permissions,\n                             BlockInfo[] blocks, \n                             short replication,\n                             long modificationTime,\n                             long atime,\n                             long preferredBlockSize) \n       throws UnresolvedLinkException {\n     INode newNode;\n     long diskspace \u003d UNKNOWN_DISK_SPACE;\n     assert hasWriteLock();\n     if (blocks \u003d\u003d null)\n       newNode \u003d new INodeDirectory(permissions, modificationTime);\n     else {\n       newNode \u003d new INodeFile(permissions, blocks.length, replication,\n                               modificationTime, atime, preferredBlockSize);\n       diskspace \u003d ((INodeFile)newNode).diskspaceConsumed(blocks);\n     }\n     writeLock();\n     try {\n       try {\n-        newNode \u003d addNode(path, newNode, diskspace, false);\n+        newNode \u003d addNode(path, newNode, diskspace);\n         if(newNode !\u003d null \u0026\u0026 blocks !\u003d null) {\n           int nrBlocks \u003d blocks.length;\n           // Add file-\u003eblock mapping\n           INodeFile newF \u003d (INodeFile)newNode;\n           for (int i \u003d 0; i \u003c nrBlocks; i++) {\n             newF.setBlock(i, getBlockManager().addINode(blocks[i], newF));\n           }\n         }\n       } catch (IOException e) {\n         return null;\n       }\n       return newNode;\n     } finally {\n       writeUnlock();\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  INode unprotectedAddFile( String path, \n                            PermissionStatus permissions,\n                            BlockInfo[] blocks, \n                            short replication,\n                            long modificationTime,\n                            long atime,\n                            long preferredBlockSize) \n      throws UnresolvedLinkException {\n    INode newNode;\n    long diskspace \u003d UNKNOWN_DISK_SPACE;\n    assert hasWriteLock();\n    if (blocks \u003d\u003d null)\n      newNode \u003d new INodeDirectory(permissions, modificationTime);\n    else {\n      newNode \u003d new INodeFile(permissions, blocks.length, replication,\n                              modificationTime, atime, preferredBlockSize);\n      diskspace \u003d ((INodeFile)newNode).diskspaceConsumed(blocks);\n    }\n    writeLock();\n    try {\n      try {\n        newNode \u003d addNode(path, newNode, diskspace);\n        if(newNode !\u003d null \u0026\u0026 blocks !\u003d null) {\n          int nrBlocks \u003d blocks.length;\n          // Add file-\u003eblock mapping\n          INodeFile newF \u003d (INodeFile)newNode;\n          for (int i \u003d 0; i \u003c nrBlocks; i++) {\n            newF.setBlock(i, getBlockManager().addINode(blocks[i], newF));\n          }\n        }\n      } catch (IOException e) {\n        return null;\n      }\n      return newNode;\n    } finally {\n      writeUnlock();\n    }\n\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  INode unprotectedAddFile( String path, \n                            PermissionStatus permissions,\n                            BlockInfo[] blocks, \n                            short replication,\n                            long modificationTime,\n                            long atime,\n                            long preferredBlockSize) \n      throws UnresolvedLinkException {\n    INode newNode;\n    long diskspace \u003d UNKNOWN_DISK_SPACE;\n    assert hasWriteLock();\n    if (blocks \u003d\u003d null)\n      newNode \u003d new INodeDirectory(permissions, modificationTime);\n    else {\n      newNode \u003d new INodeFile(permissions, blocks.length, replication,\n                              modificationTime, atime, preferredBlockSize);\n      diskspace \u003d ((INodeFile)newNode).diskspaceConsumed(blocks);\n    }\n    writeLock();\n    try {\n      try {\n        newNode \u003d addNode(path, newNode, diskspace, false);\n        if(newNode !\u003d null \u0026\u0026 blocks !\u003d null) {\n          int nrBlocks \u003d blocks.length;\n          // Add file-\u003eblock mapping\n          INodeFile newF \u003d (INodeFile)newNode;\n          for (int i \u003d 0; i \u003c nrBlocks; i++) {\n            newF.setBlock(i, getBlockManager().addINode(blocks[i], newF));\n          }\n        }\n      } catch (IOException e) {\n        return null;\n      }\n      return newNode;\n    } finally {\n      writeUnlock();\n    }\n\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  INode unprotectedAddFile( String path, \n                            PermissionStatus permissions,\n                            BlockInfo[] blocks, \n                            short replication,\n                            long modificationTime,\n                            long atime,\n                            long preferredBlockSize) \n      throws UnresolvedLinkException {\n    INode newNode;\n    long diskspace \u003d UNKNOWN_DISK_SPACE;\n    assert hasWriteLock();\n    if (blocks \u003d\u003d null)\n      newNode \u003d new INodeDirectory(permissions, modificationTime);\n    else {\n      newNode \u003d new INodeFile(permissions, blocks.length, replication,\n                              modificationTime, atime, preferredBlockSize);\n      diskspace \u003d ((INodeFile)newNode).diskspaceConsumed(blocks);\n    }\n    writeLock();\n    try {\n      try {\n        newNode \u003d addNode(path, newNode, diskspace, false);\n        if(newNode !\u003d null \u0026\u0026 blocks !\u003d null) {\n          int nrBlocks \u003d blocks.length;\n          // Add file-\u003eblock mapping\n          INodeFile newF \u003d (INodeFile)newNode;\n          for (int i \u003d 0; i \u003c nrBlocks; i++) {\n            newF.setBlock(i, getBlockManager().addINode(blocks[i], newF));\n          }\n        }\n      } catch (IOException e) {\n        return null;\n      }\n      return newNode;\n    } finally {\n      writeUnlock();\n    }\n\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,39 @@\n+  INode unprotectedAddFile( String path, \n+                            PermissionStatus permissions,\n+                            BlockInfo[] blocks, \n+                            short replication,\n+                            long modificationTime,\n+                            long atime,\n+                            long preferredBlockSize) \n+      throws UnresolvedLinkException {\n+    INode newNode;\n+    long diskspace \u003d UNKNOWN_DISK_SPACE;\n+    assert hasWriteLock();\n+    if (blocks \u003d\u003d null)\n+      newNode \u003d new INodeDirectory(permissions, modificationTime);\n+    else {\n+      newNode \u003d new INodeFile(permissions, blocks.length, replication,\n+                              modificationTime, atime, preferredBlockSize);\n+      diskspace \u003d ((INodeFile)newNode).diskspaceConsumed(blocks);\n+    }\n+    writeLock();\n+    try {\n+      try {\n+        newNode \u003d addNode(path, newNode, diskspace, false);\n+        if(newNode !\u003d null \u0026\u0026 blocks !\u003d null) {\n+          int nrBlocks \u003d blocks.length;\n+          // Add file-\u003eblock mapping\n+          INodeFile newF \u003d (INodeFile)newNode;\n+          for (int i \u003d 0; i \u003c nrBlocks; i++) {\n+            newF.setBlock(i, getBlockManager().addINode(blocks[i], newF));\n+          }\n+        }\n+      } catch (IOException e) {\n+        return null;\n+      }\n+      return newNode;\n+    } finally {\n+      writeUnlock();\n+    }\n+\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  INode unprotectedAddFile( String path, \n                            PermissionStatus permissions,\n                            BlockInfo[] blocks, \n                            short replication,\n                            long modificationTime,\n                            long atime,\n                            long preferredBlockSize) \n      throws UnresolvedLinkException {\n    INode newNode;\n    long diskspace \u003d UNKNOWN_DISK_SPACE;\n    assert hasWriteLock();\n    if (blocks \u003d\u003d null)\n      newNode \u003d new INodeDirectory(permissions, modificationTime);\n    else {\n      newNode \u003d new INodeFile(permissions, blocks.length, replication,\n                              modificationTime, atime, preferredBlockSize);\n      diskspace \u003d ((INodeFile)newNode).diskspaceConsumed(blocks);\n    }\n    writeLock();\n    try {\n      try {\n        newNode \u003d addNode(path, newNode, diskspace, false);\n        if(newNode !\u003d null \u0026\u0026 blocks !\u003d null) {\n          int nrBlocks \u003d blocks.length;\n          // Add file-\u003eblock mapping\n          INodeFile newF \u003d (INodeFile)newNode;\n          for (int i \u003d 0; i \u003c nrBlocks; i++) {\n            newF.setBlock(i, getBlockManager().addINode(blocks[i], newF));\n          }\n        }\n      } catch (IOException e) {\n        return null;\n      }\n      return newNode;\n    } finally {\n      writeUnlock();\n    }\n\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java"
    }
  }
}