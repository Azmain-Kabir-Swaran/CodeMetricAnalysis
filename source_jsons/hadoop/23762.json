{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "JobHistory.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java",
  "functionStartLine": 192,
  "functionEndLine": 199,
  "numCommitsSeen": 45,
  "timeTaken": 9910,
  "changeHistory": [
    "7d04a96027ad75877b41b7cd8f67455dd13159d7",
    "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "7d04a96027ad75877b41b7cd8f67455dd13159d7": "Ybodychange",
    "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymultichange(Ymovefromfile,Ybodychange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7d04a96027ad75877b41b7cd8f67455dd13159d7": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3972. Fix locking and exception issues in JobHistory server. (Contributed by Robert Joseph Evans)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1327354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/04/12 6:59 PM",
      "commitName": "7d04a96027ad75877b41b7cd8f67455dd13159d7",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "10/04/12 11:11 AM",
      "commitNameOld": "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 7.32,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,9 @@\n     public void run() {\n       LOG.info(\"History Cleaner started\");\n-      long cutoff \u003d System.currentTimeMillis() - maxAgeMillis;\n       try {\n-        hsManager.clean(cutoff, storage);\n+        hsManager.clean();\n       } catch (IOException e) {\n         LOG.warn(\"Error trying to clean up \", e);\n       }\n       LOG.info(\"History Cleaner complete\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"History Cleaner started\");\n      try {\n        hsManager.clean();\n      } catch (IOException e) {\n        LOG.warn(\"Error trying to clean up \", e);\n      }\n      LOG.info(\"History Cleaner complete\");\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java",
      "extendedDetails": {}
    },
    "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4059. The history server should have a separate pluggable storage/query interface. (Robert Evans via tgraves).\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1311896 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/04/12 11:11 AM",
      "commitName": "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "06/03/12 3:21 PM",
      "commitNameOld": "c3a4de0ec0389064f5468180d1b9024f64b00f40",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 34.78,
      "commitsBetweenForRepo": 243,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,10 @@\n     public void run() {\n       LOG.info(\"History Cleaner started\");\n-      currentTime \u003d System.currentTimeMillis();\n-      boolean halted \u003d false;\n-      //TODO Delete YYYY/MM/DD directories.\n+      long cutoff \u003d System.currentTimeMillis() - maxAgeMillis;\n       try {\n-        List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n-        //Sort in ascending order. Relies on YYYY/MM/DD/Serial\n-        Collections.sort(serialDirList);\n-        for (FileStatus serialDir : serialDirList) {\n-          List\u003cFileStatus\u003e historyFileList \u003d \n-            scanDirectoryForHistoryFiles(serialDir.getPath(), doneDirFc);\n-          for (FileStatus historyFile : historyFileList) {\n-            JobIndexInfo jobIndexInfo \u003d \n-              FileNameIndexUtils.getIndexInfo(historyFile.getPath().getName());\n-            long effectiveTimestamp \u003d \n-              getEffectiveTimestamp(jobIndexInfo.getFinishTime(), historyFile);\n-            if (shouldDelete(effectiveTimestamp)) {\n-              String confFileName \u003d \n-                JobHistoryUtils.getIntermediateConfFileName(jobIndexInfo.getJobId());\n-              MetaInfo metaInfo \u003d new MetaInfo(historyFile.getPath(),\n-                  new Path(historyFile.getPath().getParent(), confFileName), \n-                  null, jobIndexInfo);\n-              delete(metaInfo);\n-            } else {\n-              halted \u003d true;\n-              break;\n-            }\n-          }\n-          if (!halted) {\n-            deleteDir(serialDir.getPath());\n-            removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n-            synchronized (existingDoneSubdirs) {\n-              existingDoneSubdirs.remove(serialDir.getPath());  \n-            }\n-            \n-          } else {\n-            break; //Don\u0027t scan any more directories.\n-    }\n-  }\n+        hsManager.clean(cutoff, storage);\n       } catch (IOException e) {\n-        LOG.warn(\"Error in History cleaner run\", e);\n+        LOG.warn(\"Error trying to clean up \", e);\n       }\n       LOG.info(\"History Cleaner complete\");\n-      LOG.info(\"FilesDeleted: \" + filesDeleted);\n-      LOG.info(\"Directories Deleted: \" + dirsDeleted);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"History Cleaner started\");\n      long cutoff \u003d System.currentTimeMillis() - maxAgeMillis;\n      try {\n        hsManager.clean(cutoff, storage);\n      } catch (IOException e) {\n        LOG.warn(\"Error trying to clean up \", e);\n      }\n      LOG.info(\"History Cleaner complete\");\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void run() {\n      LOG.info(\"History Cleaner started\");\n      currentTime \u003d System.currentTimeMillis();\n      boolean halted \u003d false;\n      //TODO Delete YYYY/MM/DD directories.\n      try {\n        List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n        //Sort in ascending order. Relies on YYYY/MM/DD/Serial\n        Collections.sort(serialDirList);\n        for (FileStatus serialDir : serialDirList) {\n          List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(serialDir.getPath(), doneDirFc);\n          for (FileStatus historyFile : historyFileList) {\n            JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile.getPath().getName());\n            long effectiveTimestamp \u003d getEffectiveTimestamp(jobIndexInfo.getFinishTime(), historyFile);\n            if (shouldDelete(effectiveTimestamp)) {\n              String confFileName \u003d JobHistoryUtils.getIntermediateConfFileName(jobIndexInfo.getJobId());\n              MetaInfo metaInfo \u003d new MetaInfo(historyFile.getPath(), new Path(historyFile.getPath().getParent(), confFileName), null, jobIndexInfo);\n              delete(metaInfo);\n            } else {\n              halted \u003d true;\n              break;\n            }\n          }\n          if (!halted) {\n            deleteDir(serialDir.getPath());\n            removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n            synchronized (existingDoneSubdirs) {\n              existingDoneSubdirs.remove(serialDir.getPath());  \n            }\n            \n          } else {\n            break; //Don\u0027t scan any more directories.\n    }\n  }\n      } catch (IOException e) {\n        LOG.warn(\"Error in History cleaner run\", e);\n      }\n      LOG.info(\"History Cleaner complete\");\n      LOG.info(\"FilesDeleted: \" + filesDeleted);\n      LOG.info(\"Directories Deleted: \" + dirsDeleted);\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,55 +1,41 @@\n     public void run() {\n-      final int MAX_RETRIES \u003d 3;\n-      int remainingRetries \u003d MAX_RETRIES;\n-      // get current flag value and reset it as well\n-      boolean sendProgress \u003d resetProgressFlag();\n-      while (!taskDone.get()) {\n-        try {\n-          boolean taskFound \u003d true; // whether TT knows about this task\n-          // sleep for a bit\n-          try {\n-            Thread.sleep(PROGRESS_INTERVAL);\n-          } \n-          catch (InterruptedException e) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(getTaskID() + \" Progress/ping thread exiting \" +\n-                        \"since it got interrupted\");\n+      LOG.info(\"History Cleaner started\");\n+      currentTime \u003d System.currentTimeMillis();\n+      boolean halted \u003d false;\n+      //TODO Delete YYYY/MM/DD directories.\n+      try {\n+        List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n+        //Sort in ascending order. Relies on YYYY/MM/DD/Serial\n+        Collections.sort(serialDirList);\n+        for (FileStatus serialDir : serialDirList) {\n+          List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(serialDir.getPath(), doneDirFc);\n+          for (FileStatus historyFile : historyFileList) {\n+            JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile.getPath().getName());\n+            long effectiveTimestamp \u003d getEffectiveTimestamp(jobIndexInfo.getFinishTime(), historyFile);\n+            if (shouldDelete(effectiveTimestamp)) {\n+              String confFileName \u003d JobHistoryUtils.getIntermediateConfFileName(jobIndexInfo.getJobId());\n+              MetaInfo metaInfo \u003d new MetaInfo(historyFile.getPath(), new Path(historyFile.getPath().getParent(), confFileName), null, jobIndexInfo);\n+              delete(metaInfo);\n+            } else {\n+              halted \u003d true;\n+              break;\n             }\n-            break;\n           }\n-\n-          if (sendProgress) {\n-            // we need to send progress update\n-            updateCounters();\n-            taskStatus.statusUpdate(taskProgress.get(),\n-                                    taskProgress.toString(), \n-                                    counters);\n-            taskFound \u003d umbilical.statusUpdate(taskId, taskStatus);\n-            taskStatus.clearStatus();\n-          }\n-          else {\n-            // send ping \n-            taskFound \u003d umbilical.ping(taskId);\n-          }\n-\n-          // if Task Tracker is not aware of our task ID (probably because it died and \n-          // came back up), kill ourselves\n-          if (!taskFound) {\n-            LOG.warn(\"Parent died.  Exiting \"+taskId);\n-            System.exit(66);\n-          }\n-\n-          sendProgress \u003d resetProgressFlag(); \n-          remainingRetries \u003d MAX_RETRIES;\n-        } \n-        catch (Throwable t) {\n-          LOG.info(\"Communication exception: \" + StringUtils.stringifyException(t));\n-          remainingRetries -\u003d1;\n-          if (remainingRetries \u003d\u003d 0) {\n-            ReflectionUtils.logThreadInfo(LOG, \"Communication exception\", 0);\n-            LOG.warn(\"Last retry, killing \"+taskId);\n-            System.exit(65);\n-          }\n-        }\n+          if (!halted) {\n+            deleteDir(serialDir.getPath());\n+            removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n+            synchronized (existingDoneSubdirs) {\n+              existingDoneSubdirs.remove(serialDir.getPath());  \n+            }\n+            \n+          } else {\n+            break; //Don\u0027t scan any more directories.\n+    }\n+  }\n+      } catch (IOException e) {\n+        LOG.warn(\"Error in History cleaner run\", e);\n       }\n+      LOG.info(\"History Cleaner complete\");\n+      LOG.info(\"FilesDeleted: \" + filesDeleted);\n+      LOG.info(\"Directories Deleted: \" + dirsDeleted);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void run() {\n      LOG.info(\"History Cleaner started\");\n      currentTime \u003d System.currentTimeMillis();\n      boolean halted \u003d false;\n      //TODO Delete YYYY/MM/DD directories.\n      try {\n        List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n        //Sort in ascending order. Relies on YYYY/MM/DD/Serial\n        Collections.sort(serialDirList);\n        for (FileStatus serialDir : serialDirList) {\n          List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(serialDir.getPath(), doneDirFc);\n          for (FileStatus historyFile : historyFileList) {\n            JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile.getPath().getName());\n            long effectiveTimestamp \u003d getEffectiveTimestamp(jobIndexInfo.getFinishTime(), historyFile);\n            if (shouldDelete(effectiveTimestamp)) {\n              String confFileName \u003d JobHistoryUtils.getIntermediateConfFileName(jobIndexInfo.getJobId());\n              MetaInfo metaInfo \u003d new MetaInfo(historyFile.getPath(), new Path(historyFile.getPath().getParent(), confFileName), null, jobIndexInfo);\n              delete(metaInfo);\n            } else {\n              halted \u003d true;\n              break;\n            }\n          }\n          if (!halted) {\n            deleteDir(serialDir.getPath());\n            removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n            synchronized (existingDoneSubdirs) {\n              existingDoneSubdirs.remove(serialDir.getPath());  \n            }\n            \n          } else {\n            break; //Don\u0027t scan any more directories.\n    }\n  }\n      } catch (IOException e) {\n        LOG.warn(\"Error in History cleaner run\", e);\n      }\n      LOG.info(\"History Cleaner complete\");\n      LOG.info(\"FilesDeleted: \" + filesDeleted);\n      LOG.info(\"Directories Deleted: \" + dirsDeleted);\n    }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java",
          "extendedDetails": {
            "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/Task.java",
            "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java",
            "oldMethodName": "run",
            "newMethodName": "run"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,55 +1,41 @@\n     public void run() {\n-      final int MAX_RETRIES \u003d 3;\n-      int remainingRetries \u003d MAX_RETRIES;\n-      // get current flag value and reset it as well\n-      boolean sendProgress \u003d resetProgressFlag();\n-      while (!taskDone.get()) {\n-        try {\n-          boolean taskFound \u003d true; // whether TT knows about this task\n-          // sleep for a bit\n-          try {\n-            Thread.sleep(PROGRESS_INTERVAL);\n-          } \n-          catch (InterruptedException e) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(getTaskID() + \" Progress/ping thread exiting \" +\n-                        \"since it got interrupted\");\n+      LOG.info(\"History Cleaner started\");\n+      currentTime \u003d System.currentTimeMillis();\n+      boolean halted \u003d false;\n+      //TODO Delete YYYY/MM/DD directories.\n+      try {\n+        List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n+        //Sort in ascending order. Relies on YYYY/MM/DD/Serial\n+        Collections.sort(serialDirList);\n+        for (FileStatus serialDir : serialDirList) {\n+          List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(serialDir.getPath(), doneDirFc);\n+          for (FileStatus historyFile : historyFileList) {\n+            JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile.getPath().getName());\n+            long effectiveTimestamp \u003d getEffectiveTimestamp(jobIndexInfo.getFinishTime(), historyFile);\n+            if (shouldDelete(effectiveTimestamp)) {\n+              String confFileName \u003d JobHistoryUtils.getIntermediateConfFileName(jobIndexInfo.getJobId());\n+              MetaInfo metaInfo \u003d new MetaInfo(historyFile.getPath(), new Path(historyFile.getPath().getParent(), confFileName), null, jobIndexInfo);\n+              delete(metaInfo);\n+            } else {\n+              halted \u003d true;\n+              break;\n             }\n-            break;\n           }\n-\n-          if (sendProgress) {\n-            // we need to send progress update\n-            updateCounters();\n-            taskStatus.statusUpdate(taskProgress.get(),\n-                                    taskProgress.toString(), \n-                                    counters);\n-            taskFound \u003d umbilical.statusUpdate(taskId, taskStatus);\n-            taskStatus.clearStatus();\n-          }\n-          else {\n-            // send ping \n-            taskFound \u003d umbilical.ping(taskId);\n-          }\n-\n-          // if Task Tracker is not aware of our task ID (probably because it died and \n-          // came back up), kill ourselves\n-          if (!taskFound) {\n-            LOG.warn(\"Parent died.  Exiting \"+taskId);\n-            System.exit(66);\n-          }\n-\n-          sendProgress \u003d resetProgressFlag(); \n-          remainingRetries \u003d MAX_RETRIES;\n-        } \n-        catch (Throwable t) {\n-          LOG.info(\"Communication exception: \" + StringUtils.stringifyException(t));\n-          remainingRetries -\u003d1;\n-          if (remainingRetries \u003d\u003d 0) {\n-            ReflectionUtils.logThreadInfo(LOG, \"Communication exception\", 0);\n-            LOG.warn(\"Last retry, killing \"+taskId);\n-            System.exit(65);\n-          }\n-        }\n+          if (!halted) {\n+            deleteDir(serialDir.getPath());\n+            removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n+            synchronized (existingDoneSubdirs) {\n+              existingDoneSubdirs.remove(serialDir.getPath());  \n+            }\n+            \n+          } else {\n+            break; //Don\u0027t scan any more directories.\n+    }\n+  }\n+      } catch (IOException e) {\n+        LOG.warn(\"Error in History cleaner run\", e);\n       }\n+      LOG.info(\"History Cleaner complete\");\n+      LOG.info(\"FilesDeleted: \" + filesDeleted);\n+      LOG.info(\"Directories Deleted: \" + dirsDeleted);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void run() {\n      LOG.info(\"History Cleaner started\");\n      currentTime \u003d System.currentTimeMillis();\n      boolean halted \u003d false;\n      //TODO Delete YYYY/MM/DD directories.\n      try {\n        List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n        //Sort in ascending order. Relies on YYYY/MM/DD/Serial\n        Collections.sort(serialDirList);\n        for (FileStatus serialDir : serialDirList) {\n          List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(serialDir.getPath(), doneDirFc);\n          for (FileStatus historyFile : historyFileList) {\n            JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile.getPath().getName());\n            long effectiveTimestamp \u003d getEffectiveTimestamp(jobIndexInfo.getFinishTime(), historyFile);\n            if (shouldDelete(effectiveTimestamp)) {\n              String confFileName \u003d JobHistoryUtils.getIntermediateConfFileName(jobIndexInfo.getJobId());\n              MetaInfo metaInfo \u003d new MetaInfo(historyFile.getPath(), new Path(historyFile.getPath().getParent(), confFileName), null, jobIndexInfo);\n              delete(metaInfo);\n            } else {\n              halted \u003d true;\n              break;\n            }\n          }\n          if (!halted) {\n            deleteDir(serialDir.getPath());\n            removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n            synchronized (existingDoneSubdirs) {\n              existingDoneSubdirs.remove(serialDir.getPath());  \n            }\n            \n          } else {\n            break; //Don\u0027t scan any more directories.\n    }\n  }\n      } catch (IOException e) {\n        LOG.warn(\"Error in History cleaner run\", e);\n      }\n      LOG.info(\"History Cleaner complete\");\n      LOG.info(\"FilesDeleted: \" + filesDeleted);\n      LOG.info(\"Directories Deleted: \" + dirsDeleted);\n    }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java",
          "extendedDetails": {}
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,55 @@\n+    public void run() {\n+      final int MAX_RETRIES \u003d 3;\n+      int remainingRetries \u003d MAX_RETRIES;\n+      // get current flag value and reset it as well\n+      boolean sendProgress \u003d resetProgressFlag();\n+      while (!taskDone.get()) {\n+        try {\n+          boolean taskFound \u003d true; // whether TT knows about this task\n+          // sleep for a bit\n+          try {\n+            Thread.sleep(PROGRESS_INTERVAL);\n+          } \n+          catch (InterruptedException e) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(getTaskID() + \" Progress/ping thread exiting \" +\n+                        \"since it got interrupted\");\n+            }\n+            break;\n+          }\n+\n+          if (sendProgress) {\n+            // we need to send progress update\n+            updateCounters();\n+            taskStatus.statusUpdate(taskProgress.get(),\n+                                    taskProgress.toString(), \n+                                    counters);\n+            taskFound \u003d umbilical.statusUpdate(taskId, taskStatus);\n+            taskStatus.clearStatus();\n+          }\n+          else {\n+            // send ping \n+            taskFound \u003d umbilical.ping(taskId);\n+          }\n+\n+          // if Task Tracker is not aware of our task ID (probably because it died and \n+          // came back up), kill ourselves\n+          if (!taskFound) {\n+            LOG.warn(\"Parent died.  Exiting \"+taskId);\n+            System.exit(66);\n+          }\n+\n+          sendProgress \u003d resetProgressFlag(); \n+          remainingRetries \u003d MAX_RETRIES;\n+        } \n+        catch (Throwable t) {\n+          LOG.info(\"Communication exception: \" + StringUtils.stringifyException(t));\n+          remainingRetries -\u003d1;\n+          if (remainingRetries \u003d\u003d 0) {\n+            ReflectionUtils.logThreadInfo(LOG, \"Communication exception\", 0);\n+            LOG.warn(\"Last retry, killing \"+taskId);\n+            System.exit(65);\n+          }\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      final int MAX_RETRIES \u003d 3;\n      int remainingRetries \u003d MAX_RETRIES;\n      // get current flag value and reset it as well\n      boolean sendProgress \u003d resetProgressFlag();\n      while (!taskDone.get()) {\n        try {\n          boolean taskFound \u003d true; // whether TT knows about this task\n          // sleep for a bit\n          try {\n            Thread.sleep(PROGRESS_INTERVAL);\n          } \n          catch (InterruptedException e) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(getTaskID() + \" Progress/ping thread exiting \" +\n                        \"since it got interrupted\");\n            }\n            break;\n          }\n\n          if (sendProgress) {\n            // we need to send progress update\n            updateCounters();\n            taskStatus.statusUpdate(taskProgress.get(),\n                                    taskProgress.toString(), \n                                    counters);\n            taskFound \u003d umbilical.statusUpdate(taskId, taskStatus);\n            taskStatus.clearStatus();\n          }\n          else {\n            // send ping \n            taskFound \u003d umbilical.ping(taskId);\n          }\n\n          // if Task Tracker is not aware of our task ID (probably because it died and \n          // came back up), kill ourselves\n          if (!taskFound) {\n            LOG.warn(\"Parent died.  Exiting \"+taskId);\n            System.exit(66);\n          }\n\n          sendProgress \u003d resetProgressFlag(); \n          remainingRetries \u003d MAX_RETRIES;\n        } \n        catch (Throwable t) {\n          LOG.info(\"Communication exception: \" + StringUtils.stringifyException(t));\n          remainingRetries -\u003d1;\n          if (remainingRetries \u003d\u003d 0) {\n            ReflectionUtils.logThreadInfo(LOG, \"Communication exception\", 0);\n            LOG.warn(\"Last retry, killing \"+taskId);\n            System.exit(65);\n          }\n        }\n      }\n    }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/Task.java"
    }
  }
}