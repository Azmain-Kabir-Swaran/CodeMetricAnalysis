{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeManager.java",
  "functionName": "sortLocatedBlock",
  "functionId": "sortLocatedBlock___lb-LocatedBlock(modifiers-final)__targetHost-String__comparator-Comparator__DatanodeInfo__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
  "functionStartLine": 516,
  "functionEndLine": 560,
  "numCommitsSeen": 459,
  "timeTaken": 10356,
  "changeHistory": [
    "29dddb8a14e52681bca8168d29431083c9f32c4a",
    "c892a879ddce3abfd51c8609c81148bf6e4f9daa",
    "d65df0f27395792c6e25f5e03b6ba1765e2ba925",
    "c73e08a6dad46cad14b38a4a586a5cda1622b206",
    "6ef42873a02bfcbff5521869f4d6f66539d1db41",
    "2ffd84273ac490724fe7e7825664bb6d09ef0e99",
    "fef596df038112cbbc86c4dc49314e274fca0190",
    "8a54384a0a85b466284fe5717b1dea0a2f29ec8d",
    "ab934e85947dcf2092050023909dd81ae274ff45",
    "8e73084491c9f317bc8cc3590f93ca67a63687a8",
    "c83c5b868ea34925ecb1597cf1ceb88524ded185",
    "123c563fe6f7a655f11d7414f992d448c5047006",
    "02fcb6b6bae7c3fe2a10b00b2a563e4098ff225e",
    "8590564dc56195cb2caa245e3ee1c06eca3938d3",
    "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7",
    "d543140089690f4ec877d26981f4ad7908b33d1d",
    "415ce38b82fd173790fdbf3760a7846a41a0579d",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "c3f6575ca44e8ad803d0b46991472465b595cdeb"
  ],
  "changeHistoryShort": {
    "29dddb8a14e52681bca8168d29431083c9f32c4a": "Ybodychange",
    "c892a879ddce3abfd51c8609c81148bf6e4f9daa": "Ybodychange",
    "d65df0f27395792c6e25f5e03b6ba1765e2ba925": "Ybodychange",
    "c73e08a6dad46cad14b38a4a586a5cda1622b206": "Ybodychange",
    "6ef42873a02bfcbff5521869f4d6f66539d1db41": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
    "2ffd84273ac490724fe7e7825664bb6d09ef0e99": "Ybodychange",
    "fef596df038112cbbc86c4dc49314e274fca0190": "Ybodychange",
    "8a54384a0a85b466284fe5717b1dea0a2f29ec8d": "Ybodychange",
    "ab934e85947dcf2092050023909dd81ae274ff45": "Ybodychange",
    "8e73084491c9f317bc8cc3590f93ca67a63687a8": "Ymultichange(Yparameterchange,Ybodychange)",
    "c83c5b868ea34925ecb1597cf1ceb88524ded185": "Ymultichange(Yparameterchange,Ybodychange)",
    "123c563fe6f7a655f11d7414f992d448c5047006": "Ybodychange",
    "02fcb6b6bae7c3fe2a10b00b2a563e4098ff225e": "Ybodychange",
    "8590564dc56195cb2caa245e3ee1c06eca3938d3": "Ybodychange",
    "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7": "Ybodychange",
    "d543140089690f4ec877d26981f4ad7908b33d1d": "Ybodychange",
    "415ce38b82fd173790fdbf3760a7846a41a0579d": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "c3f6575ca44e8ad803d0b46991472465b595cdeb": "Yintroduced"
  },
  "changeHistoryDetails": {
    "29dddb8a14e52681bca8168d29431083c9f32c4a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15255. Consider StorageType when DatanodeManager#sortLocatedBlock(). Contributed by Lisheng Sun.\n",
      "commitDate": "12/05/20 7:07 AM",
      "commitName": "29dddb8a14e52681bca8168d29431083c9f32c4a",
      "commitAuthor": "S O\u0027Donnell",
      "commitDateOld": "29/04/20 7:56 AM",
      "commitNameOld": "9ca6298a9ac1ffd5c87222d5b81a561a86913840",
      "commitAuthorOld": "S O\u0027Donnell",
      "daysBetweenCommits": 12.97,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n       Comparator\u003cDatanodeInfo\u003e comparator) {\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     boolean nonDatanodeReader \u003d false;\n     Node client \u003d getDatanodeByHost(targetHost);\n     if (client \u003d\u003d null) {\n       nonDatanodeReader \u003d true;\n       List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e(1);\n       hosts.add(targetHost);\n       List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n       if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n         String rName \u003d resolvedHosts.get(0);\n         if (rName !\u003d null) {\n           client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n             targetHost);\n         }\n       } else {\n         LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n           \"awareness scripts are functional.\");\n       }\n     }\n \n-    DatanodeInfo[] di \u003d lb.getLocations();\n+    DatanodeInfoWithStorage[] di \u003d lb.getLocations();\n     // Move decommissioned/stale datanodes to the bottom\n     Arrays.sort(di, comparator);\n \n     // Sort nodes by network distance only for located blocks\n     int lastActiveIndex \u003d di.length - 1;\n     while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n       --lastActiveIndex;\n     }\n     int activeLen \u003d lastActiveIndex + 1;\n     if(nonDatanodeReader) {\n       networktopology.sortByDistanceUsingNetworkLocation(client,\n           lb.getLocations(), activeLen, createSecondaryNodeSorter());\n     } else {\n       networktopology.sortByDistance(client, lb.getLocations(), activeLen,\n           createSecondaryNodeSorter());\n     }\n     // move PROVIDED storage to the end to prefer local replicas.\n     lb.moveProvidedToEnd(activeLen);\n     // must update cache since we modified locations array\n     lb.updateCachedStorageInfo();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n      Comparator\u003cDatanodeInfo\u003e comparator) {\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    boolean nonDatanodeReader \u003d false;\n    Node client \u003d getDatanodeByHost(targetHost);\n    if (client \u003d\u003d null) {\n      nonDatanodeReader \u003d true;\n      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e(1);\n      hosts.add(targetHost);\n      List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n      if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n        String rName \u003d resolvedHosts.get(0);\n        if (rName !\u003d null) {\n          client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n            targetHost);\n        }\n      } else {\n        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n          \"awareness scripts are functional.\");\n      }\n    }\n\n    DatanodeInfoWithStorage[] di \u003d lb.getLocations();\n    // Move decommissioned/stale datanodes to the bottom\n    Arrays.sort(di, comparator);\n\n    // Sort nodes by network distance only for located blocks\n    int lastActiveIndex \u003d di.length - 1;\n    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n      --lastActiveIndex;\n    }\n    int activeLen \u003d lastActiveIndex + 1;\n    if(nonDatanodeReader) {\n      networktopology.sortByDistanceUsingNetworkLocation(client,\n          lb.getLocations(), activeLen, createSecondaryNodeSorter());\n    } else {\n      networktopology.sortByDistance(client, lb.getLocations(), activeLen,\n          createSecondaryNodeSorter());\n    }\n    // move PROVIDED storage to the end to prefer local replicas.\n    lb.moveProvidedToEnd(activeLen);\n    // must update cache since we modified locations array\n    lb.updateCachedStorageInfo();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "c892a879ddce3abfd51c8609c81148bf6e4f9daa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14882. Consider DataNode load when #getBlockLocation. Contributed by Xiaoqiao He.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n\nReviewed-by: Inigo Goiri \u003cinigoiri@apache.org\u003e\nReviewed-by: Istvan Fajth \u003cpifta@cloudera.com\u003e\n",
      "commitDate": "15/11/19 12:16 PM",
      "commitName": "c892a879ddce3abfd51c8609c81148bf6e4f9daa",
      "commitAuthor": "He Xiaoqiao",
      "commitDateOld": "01/03/19 9:18 AM",
      "commitNameOld": "80b77deb42a3ef94d6bef160bc58d807f2faa104",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 259.12,
      "commitsBetweenForRepo": 1871,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,45 @@\n   private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n       Comparator\u003cDatanodeInfo\u003e comparator) {\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     boolean nonDatanodeReader \u003d false;\n     Node client \u003d getDatanodeByHost(targetHost);\n     if (client \u003d\u003d null) {\n       nonDatanodeReader \u003d true;\n       List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e(1);\n       hosts.add(targetHost);\n       List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n       if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n         String rName \u003d resolvedHosts.get(0);\n         if (rName !\u003d null) {\n           client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n             targetHost);\n         }\n       } else {\n         LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n           \"awareness scripts are functional.\");\n       }\n     }\n \n     DatanodeInfo[] di \u003d lb.getLocations();\n     // Move decommissioned/stale datanodes to the bottom\n     Arrays.sort(di, comparator);\n \n     // Sort nodes by network distance only for located blocks\n     int lastActiveIndex \u003d di.length - 1;\n     while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n       --lastActiveIndex;\n     }\n     int activeLen \u003d lastActiveIndex + 1;\n     if(nonDatanodeReader) {\n       networktopology.sortByDistanceUsingNetworkLocation(client,\n-          lb.getLocations(), activeLen);\n+          lb.getLocations(), activeLen, createSecondaryNodeSorter());\n     } else {\n-      networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n+      networktopology.sortByDistance(client, lb.getLocations(), activeLen,\n+          createSecondaryNodeSorter());\n     }\n     // move PROVIDED storage to the end to prefer local replicas.\n     lb.moveProvidedToEnd(activeLen);\n     // must update cache since we modified locations array\n     lb.updateCachedStorageInfo();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n      Comparator\u003cDatanodeInfo\u003e comparator) {\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    boolean nonDatanodeReader \u003d false;\n    Node client \u003d getDatanodeByHost(targetHost);\n    if (client \u003d\u003d null) {\n      nonDatanodeReader \u003d true;\n      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e(1);\n      hosts.add(targetHost);\n      List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n      if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n        String rName \u003d resolvedHosts.get(0);\n        if (rName !\u003d null) {\n          client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n            targetHost);\n        }\n      } else {\n        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n          \"awareness scripts are functional.\");\n      }\n    }\n\n    DatanodeInfo[] di \u003d lb.getLocations();\n    // Move decommissioned/stale datanodes to the bottom\n    Arrays.sort(di, comparator);\n\n    // Sort nodes by network distance only for located blocks\n    int lastActiveIndex \u003d di.length - 1;\n    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n      --lastActiveIndex;\n    }\n    int activeLen \u003d lastActiveIndex + 1;\n    if(nonDatanodeReader) {\n      networktopology.sortByDistanceUsingNetworkLocation(client,\n          lb.getLocations(), activeLen, createSecondaryNodeSorter());\n    } else {\n      networktopology.sortByDistance(client, lb.getLocations(), activeLen,\n          createSecondaryNodeSorter());\n    }\n    // move PROVIDED storage to the end to prefer local replicas.\n    lb.moveProvidedToEnd(activeLen);\n    // must update cache since we modified locations array\n    lb.updateCachedStorageInfo();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "d65df0f27395792c6e25f5e03b6ba1765e2ba925": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11190. [READ] Namenode support for data stored in external stores.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "d65df0f27395792c6e25f5e03b6ba1765e2ba925",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "17/08/17 3:26 PM",
      "commitNameOld": "b29894889742dda654cd88a7ce72a4e51fccb328",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 120.14,
      "commitsBetweenForRepo": 995,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,44 @@\n   private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n       Comparator\u003cDatanodeInfo\u003e comparator) {\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     boolean nonDatanodeReader \u003d false;\n     Node client \u003d getDatanodeByHost(targetHost);\n     if (client \u003d\u003d null) {\n       nonDatanodeReader \u003d true;\n       List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e(1);\n       hosts.add(targetHost);\n       List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n       if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n         String rName \u003d resolvedHosts.get(0);\n         if (rName !\u003d null) {\n           client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n             targetHost);\n         }\n       } else {\n         LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n           \"awareness scripts are functional.\");\n       }\n     }\n \n     DatanodeInfo[] di \u003d lb.getLocations();\n     // Move decommissioned/stale datanodes to the bottom\n     Arrays.sort(di, comparator);\n \n     // Sort nodes by network distance only for located blocks\n     int lastActiveIndex \u003d di.length - 1;\n     while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n       --lastActiveIndex;\n     }\n     int activeLen \u003d lastActiveIndex + 1;\n     if(nonDatanodeReader) {\n       networktopology.sortByDistanceUsingNetworkLocation(client,\n           lb.getLocations(), activeLen);\n     } else {\n       networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n     }\n+    //move PROVIDED storage to the end to prefer local replicas.\n+    lb.moveProvidedToEnd(activeLen);\n     // must update cache since we modified locations array\n     lb.updateCachedStorageInfo();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n      Comparator\u003cDatanodeInfo\u003e comparator) {\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    boolean nonDatanodeReader \u003d false;\n    Node client \u003d getDatanodeByHost(targetHost);\n    if (client \u003d\u003d null) {\n      nonDatanodeReader \u003d true;\n      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e(1);\n      hosts.add(targetHost);\n      List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n      if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n        String rName \u003d resolvedHosts.get(0);\n        if (rName !\u003d null) {\n          client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n            targetHost);\n        }\n      } else {\n        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n          \"awareness scripts are functional.\");\n      }\n    }\n\n    DatanodeInfo[] di \u003d lb.getLocations();\n    // Move decommissioned/stale datanodes to the bottom\n    Arrays.sort(di, comparator);\n\n    // Sort nodes by network distance only for located blocks\n    int lastActiveIndex \u003d di.length - 1;\n    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n      --lastActiveIndex;\n    }\n    int activeLen \u003d lastActiveIndex + 1;\n    if(nonDatanodeReader) {\n      networktopology.sortByDistanceUsingNetworkLocation(client,\n          lb.getLocations(), activeLen);\n    } else {\n      networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n    }\n    //move PROVIDED storage to the end to prefer local replicas.\n    lb.moveProvidedToEnd(activeLen);\n    // must update cache since we modified locations array\n    lb.updateCachedStorageInfo();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "c73e08a6dad46cad14b38a4a586a5cda1622b206": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10206. Datanodes not sorted properly by distance when the reader isn\u0027t a datanode. (Nandakumar via mingma)\n",
      "commitDate": "07/12/16 8:26 AM",
      "commitName": "c73e08a6dad46cad14b38a4a586a5cda1622b206",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "27/10/16 3:58 PM",
      "commitNameOld": "f3ac1f41b8fa82a0ac87a207d7afa2061d90a9bd",
      "commitAuthorOld": "Erik Krogen",
      "daysBetweenCommits": 40.73,
      "commitsBetweenForRepo": 304,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,42 @@\n   private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n       Comparator\u003cDatanodeInfo\u003e comparator) {\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n+    boolean nonDatanodeReader \u003d false;\n     Node client \u003d getDatanodeByHost(targetHost);\n     if (client \u003d\u003d null) {\n-      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e (1);\n+      nonDatanodeReader \u003d true;\n+      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e(1);\n       hosts.add(targetHost);\n       List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n       if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n         String rName \u003d resolvedHosts.get(0);\n         if (rName !\u003d null) {\n           client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n             targetHost);\n         }\n       } else {\n         LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n           \"awareness scripts are functional.\");\n       }\n     }\n \n     DatanodeInfo[] di \u003d lb.getLocations();\n     // Move decommissioned/stale datanodes to the bottom\n     Arrays.sort(di, comparator);\n \n     // Sort nodes by network distance only for located blocks\n     int lastActiveIndex \u003d di.length - 1;\n     while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n       --lastActiveIndex;\n     }\n     int activeLen \u003d lastActiveIndex + 1;\n-    networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n-\n+    if(nonDatanodeReader) {\n+      networktopology.sortByDistanceUsingNetworkLocation(client,\n+          lb.getLocations(), activeLen);\n+    } else {\n+      networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n+    }\n     // must update cache since we modified locations array\n     lb.updateCachedStorageInfo();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n      Comparator\u003cDatanodeInfo\u003e comparator) {\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    boolean nonDatanodeReader \u003d false;\n    Node client \u003d getDatanodeByHost(targetHost);\n    if (client \u003d\u003d null) {\n      nonDatanodeReader \u003d true;\n      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e(1);\n      hosts.add(targetHost);\n      List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n      if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n        String rName \u003d resolvedHosts.get(0);\n        if (rName !\u003d null) {\n          client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n            targetHost);\n        }\n      } else {\n        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n          \"awareness scripts are functional.\");\n      }\n    }\n\n    DatanodeInfo[] di \u003d lb.getLocations();\n    // Move decommissioned/stale datanodes to the bottom\n    Arrays.sort(di, comparator);\n\n    // Sort nodes by network distance only for located blocks\n    int lastActiveIndex \u003d di.length - 1;\n    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n      --lastActiveIndex;\n    }\n    int activeLen \u003d lastActiveIndex + 1;\n    if(nonDatanodeReader) {\n      networktopology.sortByDistanceUsingNetworkLocation(client,\n          lb.getLocations(), activeLen);\n    } else {\n      networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n    }\n    // must update cache since we modified locations array\n    lb.updateCachedStorageInfo();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "6ef42873a02bfcbff5521869f4d6f66539d1db41": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-9918. Erasure Coding: Sort located striped blocks based on decommissioned states. Contributed by Rakesh R.\n",
      "commitDate": "12/04/16 1:38 PM",
      "commitName": "6ef42873a02bfcbff5521869f4d6f66539d1db41",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-9918. Erasure Coding: Sort located striped blocks based on decommissioned states. Contributed by Rakesh R.\n",
          "commitDate": "12/04/16 1:38 PM",
          "commitName": "6ef42873a02bfcbff5521869f4d6f66539d1db41",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "25/03/16 5:11 PM",
          "commitNameOld": "fde8ac5d8514f5146f438f8d0794116aaef20416",
          "commitAuthorOld": "Lei Xu",
          "daysBetweenCommits": 17.85,
          "commitsBetweenForRepo": 113,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,36 @@\n-  public void sortLocatedBlocks(final String targethost,\n-      final List\u003cLocatedBlock\u003e locatedblocks) {\n-    //sort the blocks\n+  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n+      Comparator\u003cDatanodeInfo\u003e comparator) {\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n-    Node client \u003d getDatanodeByHost(targethost);\n+    Node client \u003d getDatanodeByHost(targetHost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e (1);\n-      hosts.add(targethost);\n+      hosts.add(targetHost);\n       List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n       if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n         String rName \u003d resolvedHosts.get(0);\n         if (rName !\u003d null) {\n           client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n-            targethost);\n+            targetHost);\n         }\n       } else {\n         LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n           \"awareness scripts are functional.\");\n       }\n     }\n-    \n-    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n-        new DFSUtil.DecomStaleComparator(staleInterval) : \n-        DFSUtil.DECOM_COMPARATOR;\n-        \n-    for (LocatedBlock b : locatedblocks) {\n-      DatanodeInfo[] di \u003d b.getLocations();\n-      // Move decommissioned/stale datanodes to the bottom\n-      Arrays.sort(di, comparator);\n-      \n-      int lastActiveIndex \u003d di.length - 1;\n-      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n-          --lastActiveIndex;\n-      }\n-      int activeLen \u003d lastActiveIndex + 1;      \n-      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n-      // must update cache since we modified locations array\n-      b.updateCachedStorageInfo();\n+\n+    DatanodeInfo[] di \u003d lb.getLocations();\n+    // Move decommissioned/stale datanodes to the bottom\n+    Arrays.sort(di, comparator);\n+\n+    // Sort nodes by network distance only for located blocks\n+    int lastActiveIndex \u003d di.length - 1;\n+    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n+      --lastActiveIndex;\n     }\n+    int activeLen \u003d lastActiveIndex + 1;\n+    networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n+\n+    // must update cache since we modified locations array\n+    lb.updateCachedStorageInfo();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n      Comparator\u003cDatanodeInfo\u003e comparator) {\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targetHost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e (1);\n      hosts.add(targetHost);\n      List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n      if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n        String rName \u003d resolvedHosts.get(0);\n        if (rName !\u003d null) {\n          client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n            targetHost);\n        }\n      } else {\n        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n          \"awareness scripts are functional.\");\n      }\n    }\n\n    DatanodeInfo[] di \u003d lb.getLocations();\n    // Move decommissioned/stale datanodes to the bottom\n    Arrays.sort(di, comparator);\n\n    // Sort nodes by network distance only for located blocks\n    int lastActiveIndex \u003d di.length - 1;\n    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n      --lastActiveIndex;\n    }\n    int activeLen \u003d lastActiveIndex + 1;\n    networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n\n    // must update cache since we modified locations array\n    lb.updateCachedStorageInfo();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {
            "oldValue": "sortLocatedBlocks",
            "newValue": "sortLocatedBlock"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9918. Erasure Coding: Sort located striped blocks based on decommissioned states. Contributed by Rakesh R.\n",
          "commitDate": "12/04/16 1:38 PM",
          "commitName": "6ef42873a02bfcbff5521869f4d6f66539d1db41",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "25/03/16 5:11 PM",
          "commitNameOld": "fde8ac5d8514f5146f438f8d0794116aaef20416",
          "commitAuthorOld": "Lei Xu",
          "daysBetweenCommits": 17.85,
          "commitsBetweenForRepo": 113,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,36 @@\n-  public void sortLocatedBlocks(final String targethost,\n-      final List\u003cLocatedBlock\u003e locatedblocks) {\n-    //sort the blocks\n+  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n+      Comparator\u003cDatanodeInfo\u003e comparator) {\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n-    Node client \u003d getDatanodeByHost(targethost);\n+    Node client \u003d getDatanodeByHost(targetHost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e (1);\n-      hosts.add(targethost);\n+      hosts.add(targetHost);\n       List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n       if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n         String rName \u003d resolvedHosts.get(0);\n         if (rName !\u003d null) {\n           client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n-            targethost);\n+            targetHost);\n         }\n       } else {\n         LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n           \"awareness scripts are functional.\");\n       }\n     }\n-    \n-    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n-        new DFSUtil.DecomStaleComparator(staleInterval) : \n-        DFSUtil.DECOM_COMPARATOR;\n-        \n-    for (LocatedBlock b : locatedblocks) {\n-      DatanodeInfo[] di \u003d b.getLocations();\n-      // Move decommissioned/stale datanodes to the bottom\n-      Arrays.sort(di, comparator);\n-      \n-      int lastActiveIndex \u003d di.length - 1;\n-      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n-          --lastActiveIndex;\n-      }\n-      int activeLen \u003d lastActiveIndex + 1;      \n-      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n-      // must update cache since we modified locations array\n-      b.updateCachedStorageInfo();\n+\n+    DatanodeInfo[] di \u003d lb.getLocations();\n+    // Move decommissioned/stale datanodes to the bottom\n+    Arrays.sort(di, comparator);\n+\n+    // Sort nodes by network distance only for located blocks\n+    int lastActiveIndex \u003d di.length - 1;\n+    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n+      --lastActiveIndex;\n     }\n+    int activeLen \u003d lastActiveIndex + 1;\n+    networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n+\n+    // must update cache since we modified locations array\n+    lb.updateCachedStorageInfo();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n      Comparator\u003cDatanodeInfo\u003e comparator) {\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targetHost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e (1);\n      hosts.add(targetHost);\n      List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n      if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n        String rName \u003d resolvedHosts.get(0);\n        if (rName !\u003d null) {\n          client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n            targetHost);\n        }\n      } else {\n        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n          \"awareness scripts are functional.\");\n      }\n    }\n\n    DatanodeInfo[] di \u003d lb.getLocations();\n    // Move decommissioned/stale datanodes to the bottom\n    Arrays.sort(di, comparator);\n\n    // Sort nodes by network distance only for located blocks\n    int lastActiveIndex \u003d di.length - 1;\n    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n      --lastActiveIndex;\n    }\n    int activeLen \u003d lastActiveIndex + 1;\n    networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n\n    // must update cache since we modified locations array\n    lb.updateCachedStorageInfo();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {
            "oldValue": "[targethost-String(modifiers-final), locatedblocks-List\u003cLocatedBlock\u003e(modifiers-final)]",
            "newValue": "[lb-LocatedBlock(modifiers-final), targetHost-String, comparator-Comparator\u003cDatanodeInfo\u003e]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-9918. Erasure Coding: Sort located striped blocks based on decommissioned states. Contributed by Rakesh R.\n",
          "commitDate": "12/04/16 1:38 PM",
          "commitName": "6ef42873a02bfcbff5521869f4d6f66539d1db41",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "25/03/16 5:11 PM",
          "commitNameOld": "fde8ac5d8514f5146f438f8d0794116aaef20416",
          "commitAuthorOld": "Lei Xu",
          "daysBetweenCommits": 17.85,
          "commitsBetweenForRepo": 113,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,36 @@\n-  public void sortLocatedBlocks(final String targethost,\n-      final List\u003cLocatedBlock\u003e locatedblocks) {\n-    //sort the blocks\n+  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n+      Comparator\u003cDatanodeInfo\u003e comparator) {\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n-    Node client \u003d getDatanodeByHost(targethost);\n+    Node client \u003d getDatanodeByHost(targetHost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e (1);\n-      hosts.add(targethost);\n+      hosts.add(targetHost);\n       List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n       if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n         String rName \u003d resolvedHosts.get(0);\n         if (rName !\u003d null) {\n           client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n-            targethost);\n+            targetHost);\n         }\n       } else {\n         LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n           \"awareness scripts are functional.\");\n       }\n     }\n-    \n-    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n-        new DFSUtil.DecomStaleComparator(staleInterval) : \n-        DFSUtil.DECOM_COMPARATOR;\n-        \n-    for (LocatedBlock b : locatedblocks) {\n-      DatanodeInfo[] di \u003d b.getLocations();\n-      // Move decommissioned/stale datanodes to the bottom\n-      Arrays.sort(di, comparator);\n-      \n-      int lastActiveIndex \u003d di.length - 1;\n-      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n-          --lastActiveIndex;\n-      }\n-      int activeLen \u003d lastActiveIndex + 1;      \n-      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n-      // must update cache since we modified locations array\n-      b.updateCachedStorageInfo();\n+\n+    DatanodeInfo[] di \u003d lb.getLocations();\n+    // Move decommissioned/stale datanodes to the bottom\n+    Arrays.sort(di, comparator);\n+\n+    // Sort nodes by network distance only for located blocks\n+    int lastActiveIndex \u003d di.length - 1;\n+    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n+      --lastActiveIndex;\n     }\n+    int activeLen \u003d lastActiveIndex + 1;\n+    networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n+\n+    // must update cache since we modified locations array\n+    lb.updateCachedStorageInfo();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n      Comparator\u003cDatanodeInfo\u003e comparator) {\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targetHost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e (1);\n      hosts.add(targetHost);\n      List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n      if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n        String rName \u003d resolvedHosts.get(0);\n        if (rName !\u003d null) {\n          client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n            targetHost);\n        }\n      } else {\n        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n          \"awareness scripts are functional.\");\n      }\n    }\n\n    DatanodeInfo[] di \u003d lb.getLocations();\n    // Move decommissioned/stale datanodes to the bottom\n    Arrays.sort(di, comparator);\n\n    // Sort nodes by network distance only for located blocks\n    int lastActiveIndex \u003d di.length - 1;\n    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n      --lastActiveIndex;\n    }\n    int activeLen \u003d lastActiveIndex + 1;\n    networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n\n    // must update cache since we modified locations array\n    lb.updateCachedStorageInfo();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9918. Erasure Coding: Sort located striped blocks based on decommissioned states. Contributed by Rakesh R.\n",
          "commitDate": "12/04/16 1:38 PM",
          "commitName": "6ef42873a02bfcbff5521869f4d6f66539d1db41",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "25/03/16 5:11 PM",
          "commitNameOld": "fde8ac5d8514f5146f438f8d0794116aaef20416",
          "commitAuthorOld": "Lei Xu",
          "daysBetweenCommits": 17.85,
          "commitsBetweenForRepo": 113,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,36 @@\n-  public void sortLocatedBlocks(final String targethost,\n-      final List\u003cLocatedBlock\u003e locatedblocks) {\n-    //sort the blocks\n+  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n+      Comparator\u003cDatanodeInfo\u003e comparator) {\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n-    Node client \u003d getDatanodeByHost(targethost);\n+    Node client \u003d getDatanodeByHost(targetHost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e (1);\n-      hosts.add(targethost);\n+      hosts.add(targetHost);\n       List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n       if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n         String rName \u003d resolvedHosts.get(0);\n         if (rName !\u003d null) {\n           client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n-            targethost);\n+            targetHost);\n         }\n       } else {\n         LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n           \"awareness scripts are functional.\");\n       }\n     }\n-    \n-    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n-        new DFSUtil.DecomStaleComparator(staleInterval) : \n-        DFSUtil.DECOM_COMPARATOR;\n-        \n-    for (LocatedBlock b : locatedblocks) {\n-      DatanodeInfo[] di \u003d b.getLocations();\n-      // Move decommissioned/stale datanodes to the bottom\n-      Arrays.sort(di, comparator);\n-      \n-      int lastActiveIndex \u003d di.length - 1;\n-      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n-          --lastActiveIndex;\n-      }\n-      int activeLen \u003d lastActiveIndex + 1;      \n-      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n-      // must update cache since we modified locations array\n-      b.updateCachedStorageInfo();\n+\n+    DatanodeInfo[] di \u003d lb.getLocations();\n+    // Move decommissioned/stale datanodes to the bottom\n+    Arrays.sort(di, comparator);\n+\n+    // Sort nodes by network distance only for located blocks\n+    int lastActiveIndex \u003d di.length - 1;\n+    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n+      --lastActiveIndex;\n     }\n+    int activeLen \u003d lastActiveIndex + 1;\n+    networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n+\n+    // must update cache since we modified locations array\n+    lb.updateCachedStorageInfo();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void sortLocatedBlock(final LocatedBlock lb, String targetHost,\n      Comparator\u003cDatanodeInfo\u003e comparator) {\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targetHost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e (1);\n      hosts.add(targetHost);\n      List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n      if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n        String rName \u003d resolvedHosts.get(0);\n        if (rName !\u003d null) {\n          client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n            targetHost);\n        }\n      } else {\n        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n          \"awareness scripts are functional.\");\n      }\n    }\n\n    DatanodeInfo[] di \u003d lb.getLocations();\n    // Move decommissioned/stale datanodes to the bottom\n    Arrays.sort(di, comparator);\n\n    // Sort nodes by network distance only for located blocks\n    int lastActiveIndex \u003d di.length - 1;\n    while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n      --lastActiveIndex;\n    }\n    int activeLen \u003d lastActiveIndex + 1;\n    networktopology.sortByDistance(client, lb.getLocations(), activeLen);\n\n    // must update cache since we modified locations array\n    lb.updateCachedStorageInfo();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "2ffd84273ac490724fe7e7825664bb6d09ef0e99": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8653. Code cleanup for DatanodeManager, DatanodeDescriptor and DatanodeStorageInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "29/06/15 12:12 PM",
      "commitName": "2ffd84273ac490724fe7e7825664bb6d09ef0e99",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "12/06/15 11:38 AM",
      "commitNameOld": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 17.02,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   public void sortLocatedBlocks(final String targethost,\n       final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n-      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n+      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e (1);\n       hosts.add(targethost);\n       List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n       if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n         String rName \u003d resolvedHosts.get(0);\n         if (rName !\u003d null) {\n           client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n             targethost);\n         }\n       } else {\n         LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n           \"awareness scripts are functional.\");\n       }\n     }\n     \n     Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n         new DFSUtil.DecomStaleComparator(staleInterval) : \n         DFSUtil.DECOM_COMPARATOR;\n         \n     for (LocatedBlock b : locatedblocks) {\n       DatanodeInfo[] di \u003d b.getLocations();\n       // Move decommissioned/stale datanodes to the bottom\n       Arrays.sort(di, comparator);\n       \n       int lastActiveIndex \u003d di.length - 1;\n       while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n           --lastActiveIndex;\n       }\n       int activeLen \u003d lastActiveIndex + 1;      \n       networktopology.sortByDistance(client, b.getLocations(), activeLen);\n       // must update cache since we modified locations array\n       b.updateCachedStorageInfo();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003c\u003e (1);\n      hosts.add(targethost);\n      List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n      if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n        String rName \u003d resolvedHosts.get(0);\n        if (rName !\u003d null) {\n          client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n            targethost);\n        }\n      } else {\n        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n          \"awareness scripts are functional.\");\n      }\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di \u003d b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex \u003d di.length - 1;\n      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen \u003d lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n      // must update cache since we modified locations array\n      b.updateCachedStorageInfo();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "fef596df038112cbbc86c4dc49314e274fca0190": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8055. NullPointerException when topology script is missing. Contributed by Anu Engineer.\n",
      "commitDate": "14/04/15 10:19 AM",
      "commitName": "fef596df038112cbbc86c4dc49314e274fca0190",
      "commitAuthor": "cnauroth",
      "commitDateOld": "20/03/15 12:02 PM",
      "commitNameOld": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 24.93,
      "commitsBetweenForRepo": 215,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,41 @@\n   public void sortLocatedBlocks(final String targethost,\n       final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n-      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n-      if (rName !\u003d null)\n-        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n+      List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n+      if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n+        String rName \u003d resolvedHosts.get(0);\n+        if (rName !\u003d null) {\n+          client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n+            targethost);\n+        }\n+      } else {\n+        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n+          \"awareness scripts are functional.\");\n+      }\n     }\n     \n     Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n         new DFSUtil.DecomStaleComparator(staleInterval) : \n         DFSUtil.DECOM_COMPARATOR;\n         \n     for (LocatedBlock b : locatedblocks) {\n       DatanodeInfo[] di \u003d b.getLocations();\n       // Move decommissioned/stale datanodes to the bottom\n       Arrays.sort(di, comparator);\n       \n       int lastActiveIndex \u003d di.length - 1;\n       while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n           --lastActiveIndex;\n       }\n       int activeLen \u003d lastActiveIndex + 1;      \n       networktopology.sortByDistance(client, b.getLocations(), activeLen);\n       // must update cache since we modified locations array\n       b.updateCachedStorageInfo();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      List\u003cString\u003e resolvedHosts \u003d dnsToSwitchMapping.resolve(hosts);\n      if (resolvedHosts !\u003d null \u0026\u0026 !resolvedHosts.isEmpty()) {\n        String rName \u003d resolvedHosts.get(0);\n        if (rName !\u003d null) {\n          client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n            targethost);\n        }\n      } else {\n        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n          \"awareness scripts are functional.\");\n      }\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di \u003d b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex \u003d di.length - 1;\n      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen \u003d lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n      // must update cache since we modified locations array\n      b.updateCachedStorageInfo();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "8a54384a0a85b466284fe5717b1dea0a2f29ec8d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7761. cleanup unnecssary code logic in LocatedBlock. (yliu)\n",
      "commitDate": "11/02/15 11:08 AM",
      "commitName": "8a54384a0a85b466284fe5717b1dea0a2f29ec8d",
      "commitAuthor": "yliu",
      "commitDateOld": "09/02/15 12:17 PM",
      "commitNameOld": "ab934e85947dcf2092050023909dd81ae274ff45",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 1.95,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public void sortLocatedBlocks(final String targethost,\n       final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n       String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n       if (rName !\u003d null)\n         client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n     }\n     \n     Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n         new DFSUtil.DecomStaleComparator(staleInterval) : \n         DFSUtil.DECOM_COMPARATOR;\n         \n     for (LocatedBlock b : locatedblocks) {\n       DatanodeInfo[] di \u003d b.getLocations();\n       // Move decommissioned/stale datanodes to the bottom\n       Arrays.sort(di, comparator);\n       \n       int lastActiveIndex \u003d di.length - 1;\n       while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n           --lastActiveIndex;\n       }\n       int activeLen \u003d lastActiveIndex + 1;      \n       networktopology.sortByDistance(client, b.getLocations(), activeLen);\n-      // must invalidate cache since we modified locations array\n-      b.invalidateCachedStorageInfo();\n+      // must update cache since we modified locations array\n+      b.updateCachedStorageInfo();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di \u003d b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex \u003d di.length - 1;\n      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen \u003d lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n      // must update cache since we modified locations array\n      b.updateCachedStorageInfo();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "ab934e85947dcf2092050023909dd81ae274ff45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7647. DatanodeManager.sortLocatedBlocks sorts DatanodeInfos but not StorageIDs. (Contributed by Milan Desai)\n",
      "commitDate": "09/02/15 12:17 PM",
      "commitName": "ab934e85947dcf2092050023909dd81ae274ff45",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "08/02/15 11:51 AM",
      "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.02,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,33 @@\n   public void sortLocatedBlocks(final String targethost,\n       final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n       String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n       if (rName !\u003d null)\n         client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n     }\n     \n     Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n         new DFSUtil.DecomStaleComparator(staleInterval) : \n         DFSUtil.DECOM_COMPARATOR;\n         \n     for (LocatedBlock b : locatedblocks) {\n       DatanodeInfo[] di \u003d b.getLocations();\n       // Move decommissioned/stale datanodes to the bottom\n       Arrays.sort(di, comparator);\n       \n       int lastActiveIndex \u003d di.length - 1;\n       while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n           --lastActiveIndex;\n       }\n       int activeLen \u003d lastActiveIndex + 1;      \n       networktopology.sortByDistance(client, b.getLocations(), activeLen);\n+      // must invalidate cache since we modified locations array\n+      b.invalidateCachedStorageInfo();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di \u003d b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex \u003d di.length - 1;\n      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen \u003d lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n      // must invalidate cache since we modified locations array\n      b.invalidateCachedStorageInfo();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "8e73084491c9f317bc8cc3590f93ca67a63687a8": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6840. Clients are always sent to the same datanode when read is off rack. (wang)\n",
      "commitDate": "18/09/14 5:49 PM",
      "commitName": "8e73084491c9f317bc8cc3590f93ca67a63687a8",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6840. Clients are always sent to the same datanode when read is off rack. (wang)\n",
          "commitDate": "18/09/14 5:49 PM",
          "commitName": "8e73084491c9f317bc8cc3590f93ca67a63687a8",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "09/09/14 5:30 PM",
          "commitNameOld": "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8",
          "commitAuthorOld": "Konstantin V Shvachko",
          "daysBetweenCommits": 9.01,
          "commitsBetweenForRepo": 113,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,31 @@\n   public void sortLocatedBlocks(final String targethost,\n-      final List\u003cLocatedBlock\u003e locatedblocks,\n-      boolean randomizeBlockLocationsPerBlock) {\n+      final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n       String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n       if (rName !\u003d null)\n         client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n     }\n     \n     Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n         new DFSUtil.DecomStaleComparator(staleInterval) : \n         DFSUtil.DECOM_COMPARATOR;\n         \n     for (LocatedBlock b : locatedblocks) {\n       DatanodeInfo[] di \u003d b.getLocations();\n       // Move decommissioned/stale datanodes to the bottom\n       Arrays.sort(di, comparator);\n       \n       int lastActiveIndex \u003d di.length - 1;\n       while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n           --lastActiveIndex;\n       }\n       int activeLen \u003d lastActiveIndex + 1;      \n-      networktopology.sortByDistance(client, b.getLocations(), activeLen, b\n-          .getBlock().getBlockId(), randomizeBlockLocationsPerBlock);\n+      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di \u003d b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex \u003d di.length - 1;\n      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen \u003d lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {
            "oldValue": "[targethost-String(modifiers-final), locatedblocks-List\u003cLocatedBlock\u003e(modifiers-final), randomizeBlockLocationsPerBlock-boolean]",
            "newValue": "[targethost-String(modifiers-final), locatedblocks-List\u003cLocatedBlock\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6840. Clients are always sent to the same datanode when read is off rack. (wang)\n",
          "commitDate": "18/09/14 5:49 PM",
          "commitName": "8e73084491c9f317bc8cc3590f93ca67a63687a8",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "09/09/14 5:30 PM",
          "commitNameOld": "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8",
          "commitAuthorOld": "Konstantin V Shvachko",
          "daysBetweenCommits": 9.01,
          "commitsBetweenForRepo": 113,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,31 @@\n   public void sortLocatedBlocks(final String targethost,\n-      final List\u003cLocatedBlock\u003e locatedblocks,\n-      boolean randomizeBlockLocationsPerBlock) {\n+      final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n       String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n       if (rName !\u003d null)\n         client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n     }\n     \n     Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n         new DFSUtil.DecomStaleComparator(staleInterval) : \n         DFSUtil.DECOM_COMPARATOR;\n         \n     for (LocatedBlock b : locatedblocks) {\n       DatanodeInfo[] di \u003d b.getLocations();\n       // Move decommissioned/stale datanodes to the bottom\n       Arrays.sort(di, comparator);\n       \n       int lastActiveIndex \u003d di.length - 1;\n       while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n           --lastActiveIndex;\n       }\n       int activeLen \u003d lastActiveIndex + 1;      \n-      networktopology.sortByDistance(client, b.getLocations(), activeLen, b\n-          .getBlock().getBlockId(), randomizeBlockLocationsPerBlock);\n+      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di \u003d b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex \u003d di.length - 1;\n      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen \u003d lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "c83c5b868ea34925ecb1597cf1ceb88524ded185": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6701. Make seed optional in NetworkTopology#sortByDistance. Contributed by Ashwin Shankar.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612625 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/14 10:47 AM",
      "commitName": "c83c5b868ea34925ecb1597cf1ceb88524ded185",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6701. Make seed optional in NetworkTopology#sortByDistance. Contributed by Ashwin Shankar.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612625 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/14 10:47 AM",
          "commitName": "c83c5b868ea34925ecb1597cf1ceb88524ded185",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "18/07/14 10:58 AM",
          "commitNameOld": "551024915d487957d9e829493ab319c8e31dfa81",
          "commitAuthorOld": "Daryn Sharp",
          "daysBetweenCommits": 3.99,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,32 +1,33 @@\n   public void sortLocatedBlocks(final String targethost,\n-      final List\u003cLocatedBlock\u003e locatedblocks) {\n+      final List\u003cLocatedBlock\u003e locatedblocks,\n+      boolean randomizeBlockLocationsPerBlock) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n       String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n       if (rName !\u003d null)\n         client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n     }\n     \n     Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n         new DFSUtil.DecomStaleComparator(staleInterval) : \n         DFSUtil.DECOM_COMPARATOR;\n         \n     for (LocatedBlock b : locatedblocks) {\n       DatanodeInfo[] di \u003d b.getLocations();\n       // Move decommissioned/stale datanodes to the bottom\n       Arrays.sort(di, comparator);\n       \n       int lastActiveIndex \u003d di.length - 1;\n       while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n           --lastActiveIndex;\n       }\n       int activeLen \u003d lastActiveIndex + 1;      \n-      networktopology.sortByDistance(client, b.getLocations(), activeLen,\n-          b.getBlock().getBlockId());\n+      networktopology.sortByDistance(client, b.getLocations(), activeLen, b\n+          .getBlock().getBlockId(), randomizeBlockLocationsPerBlock);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks,\n      boolean randomizeBlockLocationsPerBlock) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di \u003d b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex \u003d di.length - 1;\n      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen \u003d lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen, b\n          .getBlock().getBlockId(), randomizeBlockLocationsPerBlock);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {
            "oldValue": "[targethost-String(modifiers-final), locatedblocks-List\u003cLocatedBlock\u003e(modifiers-final)]",
            "newValue": "[targethost-String(modifiers-final), locatedblocks-List\u003cLocatedBlock\u003e(modifiers-final), randomizeBlockLocationsPerBlock-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6701. Make seed optional in NetworkTopology#sortByDistance. Contributed by Ashwin Shankar.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612625 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/14 10:47 AM",
          "commitName": "c83c5b868ea34925ecb1597cf1ceb88524ded185",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "18/07/14 10:58 AM",
          "commitNameOld": "551024915d487957d9e829493ab319c8e31dfa81",
          "commitAuthorOld": "Daryn Sharp",
          "daysBetweenCommits": 3.99,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,32 +1,33 @@\n   public void sortLocatedBlocks(final String targethost,\n-      final List\u003cLocatedBlock\u003e locatedblocks) {\n+      final List\u003cLocatedBlock\u003e locatedblocks,\n+      boolean randomizeBlockLocationsPerBlock) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n       String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n       if (rName !\u003d null)\n         client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n     }\n     \n     Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n         new DFSUtil.DecomStaleComparator(staleInterval) : \n         DFSUtil.DECOM_COMPARATOR;\n         \n     for (LocatedBlock b : locatedblocks) {\n       DatanodeInfo[] di \u003d b.getLocations();\n       // Move decommissioned/stale datanodes to the bottom\n       Arrays.sort(di, comparator);\n       \n       int lastActiveIndex \u003d di.length - 1;\n       while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n           --lastActiveIndex;\n       }\n       int activeLen \u003d lastActiveIndex + 1;      \n-      networktopology.sortByDistance(client, b.getLocations(), activeLen,\n-          b.getBlock().getBlockId());\n+      networktopology.sortByDistance(client, b.getLocations(), activeLen, b\n+          .getBlock().getBlockId(), randomizeBlockLocationsPerBlock);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks,\n      boolean randomizeBlockLocationsPerBlock) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di \u003d b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex \u003d di.length - 1;\n      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen \u003d lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen, b\n          .getBlock().getBlockId(), randomizeBlockLocationsPerBlock);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "123c563fe6f7a655f11d7414f992d448c5047006": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6460. Ignore stale and decommissioned nodes in NetworkTopology#sortByDistance. Contributed by Yongjun Zhang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601535 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/06/14 5:39 PM",
      "commitName": "123c563fe6f7a655f11d7414f992d448c5047006",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "03/06/14 11:33 AM",
      "commitNameOld": "02fcb6b6bae7c3fe2a10b00b2a563e4098ff225e",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 6.25,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,32 @@\n   public void sortLocatedBlocks(final String targethost,\n       final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n       String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n       if (rName !\u003d null)\n         client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n     }\n     \n     Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n         new DFSUtil.DecomStaleComparator(staleInterval) : \n         DFSUtil.DECOM_COMPARATOR;\n         \n     for (LocatedBlock b : locatedblocks) {\n-      networktopology.sortByDistance(client, b.getLocations(), b\n-          .getBlock().getBlockId());\n+      DatanodeInfo[] di \u003d b.getLocations();\n       // Move decommissioned/stale datanodes to the bottom\n-      Arrays.sort(b.getLocations(), comparator);\n+      Arrays.sort(di, comparator);\n+      \n+      int lastActiveIndex \u003d di.length - 1;\n+      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n+          --lastActiveIndex;\n+      }\n+      int activeLen \u003d lastActiveIndex + 1;      \n+      networktopology.sortByDistance(client, b.getLocations(), activeLen,\n+          b.getBlock().getBlockId());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di \u003d b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex \u003d di.length - 1;\n      while (lastActiveIndex \u003e 0 \u0026\u0026 isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen \u003d lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen,\n          b.getBlock().getBlockId());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "02fcb6b6bae7c3fe2a10b00b2a563e4098ff225e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6268. Better sorting in NetworkTopology#pseudoSortByDistance when no local node is found. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1599734 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/06/14 11:33 AM",
      "commitName": "02fcb6b6bae7c3fe2a10b00b2a563e4098ff225e",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "19/05/14 10:52 AM",
      "commitNameOld": "35058fc020c19b2bdeaa929e8afeb55703b43837",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 15.03,
      "commitsBetweenForRepo": 83,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,25 @@\n   public void sortLocatedBlocks(final String targethost,\n       final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n       String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n       if (rName !\u003d null)\n         client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n     }\n     \n     Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n         new DFSUtil.DecomStaleComparator(staleInterval) : \n         DFSUtil.DECOM_COMPARATOR;\n         \n     for (LocatedBlock b : locatedblocks) {\n-      networktopology.pseudoSortByDistance(client, b.getLocations());\n+      networktopology.sortByDistance(client, b.getLocations(), b\n+          .getBlock().getBlockId());\n       // Move decommissioned/stale datanodes to the bottom\n       Arrays.sort(b.getLocations(), comparator);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      networktopology.sortByDistance(client, b.getLocations(), b\n          .getBlock().getBlockId());\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(b.getLocations(), comparator);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "8590564dc56195cb2caa245e3ee1c06eca3938d3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4350. Make enabling of stale marking on read and write paths independent. Contributed by Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1441819 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/02/13 2:18 PM",
      "commitName": "8590564dc56195cb2caa245e3ee1c06eca3938d3",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "29/01/13 2:44 PM",
      "commitNameOld": "8acfa66897dc23138f4b0aa41852f50a44407664",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 3.98,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public void sortLocatedBlocks(final String targethost,\n       final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n       String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n       if (rName !\u003d null)\n         client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n     }\n     \n-    Comparator\u003cDatanodeInfo\u003e comparator \u003d checkForStaleDataNodes ? \n+    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n         new DFSUtil.DecomStaleComparator(staleInterval) : \n         DFSUtil.DECOM_COMPARATOR;\n         \n     for (LocatedBlock b : locatedblocks) {\n       networktopology.pseudoSortByDistance(client, b.getLocations());\n       // Move decommissioned/stale datanodes to the bottom\n       Arrays.sort(b.getLocations(), comparator);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      networktopology.pseudoSortByDistance(client, b.getLocations());\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(b.getLocations(), comparator);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3912. Detect and avoid stale datanodes for writes. Contributed by Jing Zhao\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1397211 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/10/12 11:08 AM",
      "commitName": "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "10/10/12 2:15 PM",
      "commitNameOld": "08f35a04c69ea20913bb28b00a1827c77e0e23e3",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 0.87,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   public void sortLocatedBlocks(final String targethost,\n       final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n       String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n       if (rName !\u003d null)\n         client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n     }\n     \n-    Comparator\u003cDatanodeInfo\u003e comparator \u003d checkForStaleNodes ? \n-                    new DFSUtil.DecomStaleComparator(staleInterval) : \n-                    DFSUtil.DECOM_COMPARATOR;\n+    Comparator\u003cDatanodeInfo\u003e comparator \u003d checkForStaleDataNodes ? \n+        new DFSUtil.DecomStaleComparator(staleInterval) : \n+        DFSUtil.DECOM_COMPARATOR;\n+        \n     for (LocatedBlock b : locatedblocks) {\n       networktopology.pseudoSortByDistance(client, b.getLocations());\n       // Move decommissioned/stale datanodes to the bottom\n       Arrays.sort(b.getLocations(), comparator);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d checkForStaleDataNodes ? \n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      networktopology.pseudoSortByDistance(client, b.getLocations());\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(b.getLocations(), comparator);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "d543140089690f4ec877d26981f4ad7908b33d1d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3703. Datanodes are marked stale if heartbeat is not received in configured timeout and are selected as the last location to read from. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1384209 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/09/12 10:46 PM",
      "commitName": "d543140089690f4ec877d26981f4ad7908b33d1d",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "11/09/12 9:10 PM",
      "commitNameOld": "414abe69183a39b38c8f8936785dce3e4774f4ca",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.07,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,23 @@\n   public void sortLocatedBlocks(final String targethost,\n       final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n     // As it is possible for the separation of node manager and datanode, \n     // here we should get node but not datanode only .\n     Node client \u003d getDatanodeByHost(targethost);\n     if (client \u003d\u003d null) {\n       List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n       hosts.add(targethost);\n       String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n       if (rName !\u003d null)\n         client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n     }\n+    \n+    Comparator\u003cDatanodeInfo\u003e comparator \u003d checkForStaleNodes ? \n+                    new DFSUtil.DecomStaleComparator(staleInterval) : \n+                    DFSUtil.DECOM_COMPARATOR;\n     for (LocatedBlock b : locatedblocks) {\n       networktopology.pseudoSortByDistance(client, b.getLocations());\n-      \n-      // Move decommissioned datanodes to the bottom\n-      Arrays.sort(b.getLocations(), DFSUtil.DECOM_COMPARATOR);\n+      // Move decommissioned/stale datanodes to the bottom\n+      Arrays.sort(b.getLocations(), comparator);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator\u003cDatanodeInfo\u003e comparator \u003d checkForStaleNodes ? \n                    new DFSUtil.DecomStaleComparator(staleInterval) : \n                    DFSUtil.DECOM_COMPARATOR;\n    for (LocatedBlock b : locatedblocks) {\n      networktopology.pseudoSortByDistance(client, b.getLocations());\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(b.getLocations(), comparator);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "415ce38b82fd173790fdbf3760a7846a41a0579d": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-8469. Make NetworkTopology class pluggable.  Contributed by Junping Du\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1347867 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/06/12 7:29 PM",
      "commitName": "415ce38b82fd173790fdbf3760a7846a41a0579d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "15/05/12 9:23 AM",
      "commitNameOld": "e9a7648f62c72164decb69390ecff4da65bbca5e",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 23.42,
      "commitsBetweenForRepo": 113,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,20 @@\n   public void sortLocatedBlocks(final String targethost,\n       final List\u003cLocatedBlock\u003e locatedblocks) {\n     //sort the blocks\n-    final DatanodeDescriptor client \u003d getDatanodeByHost(targethost);\n+    // As it is possible for the separation of node manager and datanode, \n+    // here we should get node but not datanode only .\n+    Node client \u003d getDatanodeByHost(targethost);\n+    if (client \u003d\u003d null) {\n+      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n+      hosts.add(targethost);\n+      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n+      if (rName !\u003d null)\n+        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n+    }\n     for (LocatedBlock b : locatedblocks) {\n       networktopology.pseudoSortByDistance(client, b.getLocations());\n       \n       // Move decommissioned datanodes to the bottom\n       Arrays.sort(b.getLocations(), DFSUtil.DECOM_COMPARATOR);\n-    }    \n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client \u003d getDatanodeByHost(targethost);\n    if (client \u003d\u003d null) {\n      List\u003cString\u003e hosts \u003d new ArrayList\u003cString\u003e (1);\n      hosts.add(targethost);\n      String rName \u003d dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName !\u003d null)\n        client \u003d new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    for (LocatedBlock b : locatedblocks) {\n      networktopology.pseudoSortByDistance(client, b.getLocations());\n      \n      // Move decommissioned datanodes to the bottom\n      Arrays.sort(b.getLocations(), DFSUtil.DECOM_COMPARATOR);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    final DatanodeDescriptor client \u003d getDatanodeByHost(targethost);\n    for (LocatedBlock b : locatedblocks) {\n      networktopology.pseudoSortByDistance(client, b.getLocations());\n      \n      // Move decommissioned datanodes to the bottom\n      Arrays.sort(b.getLocations(), DFSUtil.DECOM_COMPARATOR);\n    }    \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    final DatanodeDescriptor client \u003d getDatanodeByHost(targethost);\n    for (LocatedBlock b : locatedblocks) {\n      networktopology.pseudoSortByDistance(client, b.getLocations());\n      \n      // Move decommissioned datanodes to the bottom\n      Arrays.sort(b.getLocations(), DFSUtil.DECOM_COMPARATOR);\n    }    \n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java"
      }
    },
    "c3f6575ca44e8ad803d0b46991472465b595cdeb": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2147. Move cluster network topology to block management and fix some javac warnings.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1148112 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/07/11 5:26 PM",
      "commitName": "c3f6575ca44e8ad803d0b46991472465b595cdeb",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,11 @@\n+  public void sortLocatedBlocks(final String targethost,\n+      final List\u003cLocatedBlock\u003e locatedblocks) {\n+    //sort the blocks\n+    final DatanodeDescriptor client \u003d getDatanodeByHost(targethost);\n+    for (LocatedBlock b : locatedblocks) {\n+      networktopology.pseudoSortByDistance(client, b.getLocations());\n+      \n+      // Move decommissioned datanodes to the bottom\n+      Arrays.sort(b.getLocations(), DFSUtil.DECOM_COMPARATOR);\n+    }    \n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void sortLocatedBlocks(final String targethost,\n      final List\u003cLocatedBlock\u003e locatedblocks) {\n    //sort the blocks\n    final DatanodeDescriptor client \u003d getDatanodeByHost(targethost);\n    for (LocatedBlock b : locatedblocks) {\n      networktopology.pseudoSortByDistance(client, b.getLocations());\n      \n      // Move decommissioned datanodes to the bottom\n      Arrays.sort(b.getLocations(), DFSUtil.DECOM_COMPARATOR);\n    }    \n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java"
    }
  }
}