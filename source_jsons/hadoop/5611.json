{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CacheManager.java",
  "functionName": "removeInternal",
  "functionId": "removeInternal___directive-CacheDirective",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
  "functionStartLine": 675,
  "functionEndLine": 694,
  "numCommitsSeen": 105,
  "timeTaken": 4292,
  "changeHistory": [
    "ca379e1c43fd733a34f3ece6172c96d74c890422",
    "d85c017d0488930d806f267141057fc73e68c728",
    "b9ae3087c0f83bfeeea47ded8e19932b46fd2350",
    "991c453ca3ac141a3f286f74af8401f83c38b230",
    "13edb391d06c479720202eb5ac81f1c71fe64748",
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
    "ce35e0950cef9250ce2ceffb3b8bfcff533c6b92",
    "f79b3e6b17450e9d34c483046b7437b09dd72016"
  ],
  "changeHistoryShort": {
    "ca379e1c43fd733a34f3ece6172c96d74c890422": "Ybodychange",
    "d85c017d0488930d806f267141057fc73e68c728": "Ybodychange",
    "b9ae3087c0f83bfeeea47ded8e19932b46fd2350": "Ymodifierchange",
    "991c453ca3ac141a3f286f74af8401f83c38b230": "Ybodychange",
    "13edb391d06c479720202eb5ac81f1c71fe64748": "Ymultichange(Yparameterchange,Ybodychange)",
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde": "Ymultichange(Yparameterchange,Ybodychange)",
    "ce35e0950cef9250ce2ceffb3b8bfcff533c6b92": "Ymultichange(Yexceptionschange,Ybodychange)",
    "f79b3e6b17450e9d34c483046b7437b09dd72016": "Yintroduced"
  },
  "changeHistoryDetails": {
    "ca379e1c43fd733a34f3ece6172c96d74c890422": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13970. Use MultiMap for CacheManager Directives to simplify the code. Contributed by BELUGA BEHR.\n",
      "commitDate": "13/12/18 9:36 PM",
      "commitName": "ca379e1c43fd733a34f3ece6172c96d74c890422",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "07/09/18 2:59 PM",
      "commitNameOld": "335a8139f5b9004414b2942eeac5a008283a6f75",
      "commitAuthorOld": "Hrishikesh Gadre",
      "daysBetweenCommits": 97.32,
      "commitsBetweenForRepo": 849,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,20 @@\n   private void removeInternal(CacheDirective directive)\n       throws InvalidRequestException {\n     assert namesystem.hasWriteLock();\n     // Remove the corresponding entry in directivesByPath.\n     String path \u003d directive.getPath();\n-    List\u003cCacheDirective\u003e directives \u003d directivesByPath.get(path);\n-    if (directives \u003d\u003d null || !directives.remove(directive)) {\n-      throw new InvalidRequestException(\"Failed to locate entry \" +\n-          directive.getId() + \" by path \" + directive.getPath());\n-    }\n-    if (directives.size() \u003d\u003d 0) {\n-      directivesByPath.remove(path);\n+    if (!directivesByPath.remove(path, directive)) {\n+      throw new InvalidRequestException(\"Failed to locate entry \"\n+          + directive.getId() + \" by path \" + directive.getPath());\n     }\n     // Fix up the stats from removing the pool\n     final CachePool pool \u003d directive.getPool();\n     directive.addBytesNeeded(-directive.getBytesNeeded());\n     directive.addFilesNeeded(-directive.getFilesNeeded());\n \n     directivesById.remove(directive.getId());\n     pool.getDirectiveList().remove(directive);\n     assert directive.getPool() \u003d\u003d null;\n \n     setNeedsRescan();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void removeInternal(CacheDirective directive)\n      throws InvalidRequestException {\n    assert namesystem.hasWriteLock();\n    // Remove the corresponding entry in directivesByPath.\n    String path \u003d directive.getPath();\n    if (!directivesByPath.remove(path, directive)) {\n      throw new InvalidRequestException(\"Failed to locate entry \"\n          + directive.getId() + \" by path \" + directive.getPath());\n    }\n    // Fix up the stats from removing the pool\n    final CachePool pool \u003d directive.getPool();\n    directive.addBytesNeeded(-directive.getBytesNeeded());\n    directive.addFilesNeeded(-directive.getFilesNeeded());\n\n    directivesById.remove(directive.getId());\n    pool.getDirectiveList().remove(directive);\n    assert directive.getPool() \u003d\u003d null;\n\n    setNeedsRescan();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "d85c017d0488930d806f267141057fc73e68c728": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5651. Remove dfs.namenode.caching.enabled and improve CRM locking. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1555002 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/01/14 6:45 PM",
      "commitName": "d85c017d0488930d806f267141057fc73e68c728",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "31/12/13 4:01 PM",
      "commitNameOld": "07e4fb1455abc33584fc666ef745abe256ebd7d1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.11,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,24 @@\n   private void removeInternal(CacheDirective directive)\n       throws InvalidRequestException {\n     assert namesystem.hasWriteLock();\n     // Remove the corresponding entry in directivesByPath.\n     String path \u003d directive.getPath();\n     List\u003cCacheDirective\u003e directives \u003d directivesByPath.get(path);\n     if (directives \u003d\u003d null || !directives.remove(directive)) {\n       throw new InvalidRequestException(\"Failed to locate entry \" +\n           directive.getId() + \" by path \" + directive.getPath());\n     }\n     if (directives.size() \u003d\u003d 0) {\n       directivesByPath.remove(path);\n     }\n     // Fix up the stats from removing the pool\n     final CachePool pool \u003d directive.getPool();\n     directive.addBytesNeeded(-directive.getBytesNeeded());\n     directive.addFilesNeeded(-directive.getFilesNeeded());\n \n     directivesById.remove(directive.getId());\n     pool.getDirectiveList().remove(directive);\n     assert directive.getPool() \u003d\u003d null;\n \n-    if (monitor !\u003d null) {\n-      monitor.setNeedsRescan();\n-    }\n+    setNeedsRescan();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void removeInternal(CacheDirective directive)\n      throws InvalidRequestException {\n    assert namesystem.hasWriteLock();\n    // Remove the corresponding entry in directivesByPath.\n    String path \u003d directive.getPath();\n    List\u003cCacheDirective\u003e directives \u003d directivesByPath.get(path);\n    if (directives \u003d\u003d null || !directives.remove(directive)) {\n      throw new InvalidRequestException(\"Failed to locate entry \" +\n          directive.getId() + \" by path \" + directive.getPath());\n    }\n    if (directives.size() \u003d\u003d 0) {\n      directivesByPath.remove(path);\n    }\n    // Fix up the stats from removing the pool\n    final CachePool pool \u003d directive.getPool();\n    directive.addBytesNeeded(-directive.getBytesNeeded());\n    directive.addFilesNeeded(-directive.getFilesNeeded());\n\n    directivesById.remove(directive.getId());\n    pool.getDirectiveList().remove(directive);\n    assert directive.getPool() \u003d\u003d null;\n\n    setNeedsRescan();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "b9ae3087c0f83bfeeea47ded8e19932b46fd2350": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-5636. Enforce a max TTL per cache pool (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1552841 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/13 3:27 PM",
      "commitName": "b9ae3087c0f83bfeeea47ded8e19932b46fd2350",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "17/12/13 10:47 AM",
      "commitNameOld": "991c453ca3ac141a3f286f74af8401f83c38b230",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 3.19,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n-  public void removeInternal(CacheDirective directive)\n+  private void removeInternal(CacheDirective directive)\n       throws InvalidRequestException {\n     assert namesystem.hasWriteLock();\n     // Remove the corresponding entry in directivesByPath.\n     String path \u003d directive.getPath();\n     List\u003cCacheDirective\u003e directives \u003d directivesByPath.get(path);\n     if (directives \u003d\u003d null || !directives.remove(directive)) {\n       throw new InvalidRequestException(\"Failed to locate entry \" +\n           directive.getId() + \" by path \" + directive.getPath());\n     }\n     if (directives.size() \u003d\u003d 0) {\n       directivesByPath.remove(path);\n     }\n     // Fix up the stats from removing the pool\n     final CachePool pool \u003d directive.getPool();\n     directive.addBytesNeeded(-directive.getBytesNeeded());\n     directive.addFilesNeeded(-directive.getFilesNeeded());\n \n     directivesById.remove(directive.getId());\n     pool.getDirectiveList().remove(directive);\n     assert directive.getPool() \u003d\u003d null;\n \n     if (monitor !\u003d null) {\n       monitor.setNeedsRescan();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void removeInternal(CacheDirective directive)\n      throws InvalidRequestException {\n    assert namesystem.hasWriteLock();\n    // Remove the corresponding entry in directivesByPath.\n    String path \u003d directive.getPath();\n    List\u003cCacheDirective\u003e directives \u003d directivesByPath.get(path);\n    if (directives \u003d\u003d null || !directives.remove(directive)) {\n      throw new InvalidRequestException(\"Failed to locate entry \" +\n          directive.getId() + \" by path \" + directive.getPath());\n    }\n    if (directives.size() \u003d\u003d 0) {\n      directivesByPath.remove(path);\n    }\n    // Fix up the stats from removing the pool\n    final CachePool pool \u003d directive.getPool();\n    directive.addBytesNeeded(-directive.getBytesNeeded());\n    directive.addFilesNeeded(-directive.getFilesNeeded());\n\n    directivesById.remove(directive.getId());\n    pool.getDirectiveList().remove(directive);\n    assert directive.getPool() \u003d\u003d null;\n\n    if (monitor !\u003d null) {\n      monitor.setNeedsRescan();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {
        "oldValue": "[public]",
        "newValue": "[private]"
      }
    },
    "991c453ca3ac141a3f286f74af8401f83c38b230": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 10:47 AM",
      "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "05/12/13 1:09 PM",
      "commitNameOld": "55e5b0653c34a5f4146ce5a97a5b4a88a976d88a",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 11.9,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,26 @@\n   public void removeInternal(CacheDirective directive)\n       throws InvalidRequestException {\n     assert namesystem.hasWriteLock();\n     // Remove the corresponding entry in directivesByPath.\n     String path \u003d directive.getPath();\n     List\u003cCacheDirective\u003e directives \u003d directivesByPath.get(path);\n     if (directives \u003d\u003d null || !directives.remove(directive)) {\n       throw new InvalidRequestException(\"Failed to locate entry \" +\n           directive.getId() + \" by path \" + directive.getPath());\n     }\n     if (directives.size() \u003d\u003d 0) {\n       directivesByPath.remove(path);\n     }\n+    // Fix up the stats from removing the pool\n+    final CachePool pool \u003d directive.getPool();\n+    directive.addBytesNeeded(-directive.getBytesNeeded());\n+    directive.addFilesNeeded(-directive.getFilesNeeded());\n+\n     directivesById.remove(directive.getId());\n-    directive.getPool().getDirectiveList().remove(directive);\n+    pool.getDirectiveList().remove(directive);\n     assert directive.getPool() \u003d\u003d null;\n+\n+    if (monitor !\u003d null) {\n+      monitor.setNeedsRescan();\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeInternal(CacheDirective directive)\n      throws InvalidRequestException {\n    assert namesystem.hasWriteLock();\n    // Remove the corresponding entry in directivesByPath.\n    String path \u003d directive.getPath();\n    List\u003cCacheDirective\u003e directives \u003d directivesByPath.get(path);\n    if (directives \u003d\u003d null || !directives.remove(directive)) {\n      throw new InvalidRequestException(\"Failed to locate entry \" +\n          directive.getId() + \" by path \" + directive.getPath());\n    }\n    if (directives.size() \u003d\u003d 0) {\n      directivesByPath.remove(path);\n    }\n    // Fix up the stats from removing the pool\n    final CachePool pool \u003d directive.getPool();\n    directive.addBytesNeeded(-directive.getBytesNeeded());\n    directive.addFilesNeeded(-directive.getFilesNeeded());\n\n    directivesById.remove(directive.getId());\n    pool.getDirectiveList().remove(directive);\n    assert directive.getPool() \u003d\u003d null;\n\n    if (monitor !\u003d null) {\n      monitor.setNeedsRescan();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "13edb391d06c479720202eb5ac81f1c71fe64748": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 9:55 AM",
      "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 9:55 AM",
          "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "21/11/13 9:12 AM",
          "commitNameOld": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 6.03,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,17 @@\n-  public void removeInternal(CacheDirective existing)\n+  public void removeInternal(CacheDirective directive)\n       throws InvalidRequestException {\n     assert namesystem.hasWriteLock();\n-    // Remove the corresponding entry in entriesByPath.\n-    String path \u003d existing.getPath();\n-    List\u003cCacheDirective\u003e entries \u003d entriesByPath.get(path);\n-    if (entries \u003d\u003d null || !entries.remove(existing)) {\n+    // Remove the corresponding entry in directivesByPath.\n+    String path \u003d directive.getPath();\n+    List\u003cCacheDirective\u003e directives \u003d directivesByPath.get(path);\n+    if (directives \u003d\u003d null || !directives.remove(directive)) {\n       throw new InvalidRequestException(\"Failed to locate entry \" +\n-          existing.getEntryId() + \" by path \" + existing.getPath());\n+          directive.getId() + \" by path \" + directive.getPath());\n     }\n-    if (entries.size() \u003d\u003d 0) {\n-      entriesByPath.remove(path);\n+    if (directives.size() \u003d\u003d 0) {\n+      directivesByPath.remove(path);\n     }\n-    entriesById.remove(existing.getEntryId());\n+    directivesById.remove(directive.getId());\n+    directive.getPool().getDirectiveList().remove(directive);\n+    assert directive.getPool() \u003d\u003d null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void removeInternal(CacheDirective directive)\n      throws InvalidRequestException {\n    assert namesystem.hasWriteLock();\n    // Remove the corresponding entry in directivesByPath.\n    String path \u003d directive.getPath();\n    List\u003cCacheDirective\u003e directives \u003d directivesByPath.get(path);\n    if (directives \u003d\u003d null || !directives.remove(directive)) {\n      throw new InvalidRequestException(\"Failed to locate entry \" +\n          directive.getId() + \" by path \" + directive.getPath());\n    }\n    if (directives.size() \u003d\u003d 0) {\n      directivesByPath.remove(path);\n    }\n    directivesById.remove(directive.getId());\n    directive.getPool().getDirectiveList().remove(directive);\n    assert directive.getPool() \u003d\u003d null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[existing-CacheDirective]",
            "newValue": "[directive-CacheDirective]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 9:55 AM",
          "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "21/11/13 9:12 AM",
          "commitNameOld": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 6.03,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,17 @@\n-  public void removeInternal(CacheDirective existing)\n+  public void removeInternal(CacheDirective directive)\n       throws InvalidRequestException {\n     assert namesystem.hasWriteLock();\n-    // Remove the corresponding entry in entriesByPath.\n-    String path \u003d existing.getPath();\n-    List\u003cCacheDirective\u003e entries \u003d entriesByPath.get(path);\n-    if (entries \u003d\u003d null || !entries.remove(existing)) {\n+    // Remove the corresponding entry in directivesByPath.\n+    String path \u003d directive.getPath();\n+    List\u003cCacheDirective\u003e directives \u003d directivesByPath.get(path);\n+    if (directives \u003d\u003d null || !directives.remove(directive)) {\n       throw new InvalidRequestException(\"Failed to locate entry \" +\n-          existing.getEntryId() + \" by path \" + existing.getPath());\n+          directive.getId() + \" by path \" + directive.getPath());\n     }\n-    if (entries.size() \u003d\u003d 0) {\n-      entriesByPath.remove(path);\n+    if (directives.size() \u003d\u003d 0) {\n+      directivesByPath.remove(path);\n     }\n-    entriesById.remove(existing.getEntryId());\n+    directivesById.remove(directive.getId());\n+    directive.getPool().getDirectiveList().remove(directive);\n+    assert directive.getPool() \u003d\u003d null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void removeInternal(CacheDirective directive)\n      throws InvalidRequestException {\n    assert namesystem.hasWriteLock();\n    // Remove the corresponding entry in directivesByPath.\n    String path \u003d directive.getPath();\n    List\u003cCacheDirective\u003e directives \u003d directivesByPath.get(path);\n    if (directives \u003d\u003d null || !directives.remove(directive)) {\n      throw new InvalidRequestException(\"Failed to locate entry \" +\n          directive.getId() + \" by path \" + directive.getPath());\n    }\n    if (directives.size() \u003d\u003d 0) {\n      directivesByPath.remove(path);\n    }\n    directivesById.remove(directive.getId());\n    directive.getPool().getDirectiveList().remove(directive);\n    assert directive.getPool() \u003d\u003d null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5473. Consistent naming of user-visible caching classes and methods (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544252 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/13 9:12 AM",
      "commitName": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5473. Consistent naming of user-visible caching classes and methods (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544252 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/11/13 9:12 AM",
          "commitName": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "18/11/13 6:01 PM",
          "commitNameOld": "4f15d0af4f3633bfa35f7cb7c1cc15ef545597d0",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 2.63,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,15 @@\n-  public void removeInternal(PathBasedCacheEntry existing)\n+  public void removeInternal(CacheDirective existing)\n       throws InvalidRequestException {\n     assert namesystem.hasWriteLock();\n     // Remove the corresponding entry in entriesByPath.\n     String path \u003d existing.getPath();\n-    List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(path);\n+    List\u003cCacheDirective\u003e entries \u003d entriesByPath.get(path);\n     if (entries \u003d\u003d null || !entries.remove(existing)) {\n       throw new InvalidRequestException(\"Failed to locate entry \" +\n           existing.getEntryId() + \" by path \" + existing.getPath());\n     }\n     if (entries.size() \u003d\u003d 0) {\n       entriesByPath.remove(path);\n     }\n     entriesById.remove(existing.getEntryId());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void removeInternal(CacheDirective existing)\n      throws InvalidRequestException {\n    assert namesystem.hasWriteLock();\n    // Remove the corresponding entry in entriesByPath.\n    String path \u003d existing.getPath();\n    List\u003cCacheDirective\u003e entries \u003d entriesByPath.get(path);\n    if (entries \u003d\u003d null || !entries.remove(existing)) {\n      throw new InvalidRequestException(\"Failed to locate entry \" +\n          existing.getEntryId() + \" by path \" + existing.getPath());\n    }\n    if (entries.size() \u003d\u003d 0) {\n      entriesByPath.remove(path);\n    }\n    entriesById.remove(existing.getEntryId());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[existing-PathBasedCacheEntry]",
            "newValue": "[existing-CacheDirective]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5473. Consistent naming of user-visible caching classes and methods (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544252 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/11/13 9:12 AM",
          "commitName": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "18/11/13 6:01 PM",
          "commitNameOld": "4f15d0af4f3633bfa35f7cb7c1cc15ef545597d0",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 2.63,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,15 @@\n-  public void removeInternal(PathBasedCacheEntry existing)\n+  public void removeInternal(CacheDirective existing)\n       throws InvalidRequestException {\n     assert namesystem.hasWriteLock();\n     // Remove the corresponding entry in entriesByPath.\n     String path \u003d existing.getPath();\n-    List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(path);\n+    List\u003cCacheDirective\u003e entries \u003d entriesByPath.get(path);\n     if (entries \u003d\u003d null || !entries.remove(existing)) {\n       throw new InvalidRequestException(\"Failed to locate entry \" +\n           existing.getEntryId() + \" by path \" + existing.getPath());\n     }\n     if (entries.size() \u003d\u003d 0) {\n       entriesByPath.remove(path);\n     }\n     entriesById.remove(existing.getEntryId());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void removeInternal(CacheDirective existing)\n      throws InvalidRequestException {\n    assert namesystem.hasWriteLock();\n    // Remove the corresponding entry in entriesByPath.\n    String path \u003d existing.getPath();\n    List\u003cCacheDirective\u003e entries \u003d entriesByPath.get(path);\n    if (entries \u003d\u003d null || !entries.remove(existing)) {\n      throw new InvalidRequestException(\"Failed to locate entry \" +\n          existing.getEntryId() + \" by path \" + existing.getPath());\n    }\n    if (entries.size() \u003d\u003d 0) {\n      entriesByPath.remove(path);\n    }\n    entriesById.remove(existing.getEntryId());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "ce35e0950cef9250ce2ceffb3b8bfcff533c6b92": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-5471. CacheAdmin -listPools fails when user lacks permissions to view all pools (Andrew Wang via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1541323 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/11/13 3:52 PM",
      "commitName": "ce35e0950cef9250ce2ceffb3b8bfcff533c6b92",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-5471. CacheAdmin -listPools fails when user lacks permissions to view all pools (Andrew Wang via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1541323 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "12/11/13 3:52 PM",
          "commitName": "ce35e0950cef9250ce2ceffb3b8bfcff533c6b92",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "07/11/13 2:07 PM",
          "commitNameOld": "f79b3e6b17450e9d34c483046b7437b09dd72016",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 5.07,
          "commitsBetweenForRepo": 20,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,15 @@\n   public void removeInternal(PathBasedCacheEntry existing)\n-      throws IOException {\n+      throws InvalidRequestException {\n     assert namesystem.hasWriteLock();\n     // Remove the corresponding entry in entriesByPath.\n     String path \u003d existing.getPath();\n     List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(path);\n     if (entries \u003d\u003d null || !entries.remove(existing)) {\n-      throw new IdNotFoundException(\"removeInternal: failed to locate entry \" +\n+      throw new InvalidRequestException(\"Failed to locate entry \" +\n           existing.getEntryId() + \" by path \" + existing.getPath());\n     }\n     if (entries.size() \u003d\u003d 0) {\n       entriesByPath.remove(path);\n     }\n     entriesById.remove(existing.getEntryId());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void removeInternal(PathBasedCacheEntry existing)\n      throws InvalidRequestException {\n    assert namesystem.hasWriteLock();\n    // Remove the corresponding entry in entriesByPath.\n    String path \u003d existing.getPath();\n    List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(path);\n    if (entries \u003d\u003d null || !entries.remove(existing)) {\n      throw new InvalidRequestException(\"Failed to locate entry \" +\n          existing.getEntryId() + \" by path \" + existing.getPath());\n    }\n    if (entries.size() \u003d\u003d 0) {\n      entriesByPath.remove(path);\n    }\n    entriesById.remove(existing.getEntryId());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[IOException]",
            "newValue": "[InvalidRequestException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5471. CacheAdmin -listPools fails when user lacks permissions to view all pools (Andrew Wang via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1541323 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "12/11/13 3:52 PM",
          "commitName": "ce35e0950cef9250ce2ceffb3b8bfcff533c6b92",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "07/11/13 2:07 PM",
          "commitNameOld": "f79b3e6b17450e9d34c483046b7437b09dd72016",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 5.07,
          "commitsBetweenForRepo": 20,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,15 @@\n   public void removeInternal(PathBasedCacheEntry existing)\n-      throws IOException {\n+      throws InvalidRequestException {\n     assert namesystem.hasWriteLock();\n     // Remove the corresponding entry in entriesByPath.\n     String path \u003d existing.getPath();\n     List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(path);\n     if (entries \u003d\u003d null || !entries.remove(existing)) {\n-      throw new IdNotFoundException(\"removeInternal: failed to locate entry \" +\n+      throw new InvalidRequestException(\"Failed to locate entry \" +\n           existing.getEntryId() + \" by path \" + existing.getPath());\n     }\n     if (entries.size() \u003d\u003d 0) {\n       entriesByPath.remove(path);\n     }\n     entriesById.remove(existing.getEntryId());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void removeInternal(PathBasedCacheEntry existing)\n      throws InvalidRequestException {\n    assert namesystem.hasWriteLock();\n    // Remove the corresponding entry in entriesByPath.\n    String path \u003d existing.getPath();\n    List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(path);\n    if (entries \u003d\u003d null || !entries.remove(existing)) {\n      throw new InvalidRequestException(\"Failed to locate entry \" +\n          existing.getEntryId() + \" by path \" + existing.getPath());\n    }\n    if (entries.size() \u003d\u003d 0) {\n      entriesByPath.remove(path);\n    }\n    entriesById.remove(existing.getEntryId());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "f79b3e6b17450e9d34c483046b7437b09dd72016": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5326. add modifyDirective to cacheAdmin (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539839 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/11/13 2:07 PM",
      "commitName": "f79b3e6b17450e9d34c483046b7437b09dd72016",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,15 @@\n+  public void removeInternal(PathBasedCacheEntry existing)\n+      throws IOException {\n+    assert namesystem.hasWriteLock();\n+    // Remove the corresponding entry in entriesByPath.\n+    String path \u003d existing.getPath();\n+    List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(path);\n+    if (entries \u003d\u003d null || !entries.remove(existing)) {\n+      throw new IdNotFoundException(\"removeInternal: failed to locate entry \" +\n+          existing.getEntryId() + \" by path \" + existing.getPath());\n+    }\n+    if (entries.size() \u003d\u003d 0) {\n+      entriesByPath.remove(path);\n+    }\n+    entriesById.remove(existing.getEntryId());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeInternal(PathBasedCacheEntry existing)\n      throws IOException {\n    assert namesystem.hasWriteLock();\n    // Remove the corresponding entry in entriesByPath.\n    String path \u003d existing.getPath();\n    List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(path);\n    if (entries \u003d\u003d null || !entries.remove(existing)) {\n      throw new IdNotFoundException(\"removeInternal: failed to locate entry \" +\n          existing.getEntryId() + \" by path \" + existing.getPath());\n    }\n    if (entries.size() \u003d\u003d 0) {\n      entriesByPath.remove(path);\n    }\n    entriesById.remove(existing.getEntryId());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java"
    }
  }
}