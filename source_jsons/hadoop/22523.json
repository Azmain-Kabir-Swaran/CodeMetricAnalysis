{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CombineFileInputFormat.java",
  "functionName": "getSplits",
  "functionId": "getSplits___job-JobContext",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
  "functionStartLine": 174,
  "functionEndLine": 248,
  "numCommitsSeen": 16,
  "timeTaken": 4947,
  "changeHistory": [
    "476eca47b52b4ead36a9ea1764dc672d064526c2",
    "381a4c42135916245c8992daa3d03f38e282108d",
    "ec18984252731089ab5af12b3603dcfc3d4f4593",
    "905b17876c44634545a68300ff2f2d73fb86d3b7",
    "16e21dfe92d61deb539c78bc2e43e04d7b474f88",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "476eca47b52b4ead36a9ea1764dc672d064526c2": "Ybodychange",
    "381a4c42135916245c8992daa3d03f38e282108d": "Ybodychange",
    "ec18984252731089ab5af12b3603dcfc3d4f4593": "Ybodychange",
    "905b17876c44634545a68300ff2f2d73fb86d3b7": "Ybodychange",
    "16e21dfe92d61deb539c78bc2e43e04d7b474f88": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "476eca47b52b4ead36a9ea1764dc672d064526c2": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5170. Fixed a wrong log message in CombineFileInputFormat class. Contributed by Sangjin Lee.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1526377 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/09/13 9:30 PM",
      "commitName": "476eca47b52b4ead36a9ea1764dc672d064526c2",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "01/08/13 10:42 AM",
      "commitNameOld": "381a4c42135916245c8992daa3d03f38e282108d",
      "commitAuthorOld": "Siddharth Seth",
      "daysBetweenCommits": 55.45,
      "commitsBetweenForRepo": 302,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,75 @@\n   public List\u003cInputSplit\u003e getSplits(JobContext job) \n     throws IOException {\n     long minSizeNode \u003d 0;\n     long minSizeRack \u003d 0;\n     long maxSize \u003d 0;\n     Configuration conf \u003d job.getConfiguration();\n \n     // the values specified by setxxxSplitSize() takes precedence over the\n     // values that might have been specified in the config\n     if (minSplitSizeNode !\u003d 0) {\n       minSizeNode \u003d minSplitSizeNode;\n     } else {\n       minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n     }\n     if (minSplitSizeRack !\u003d 0) {\n       minSizeRack \u003d minSplitSizeRack;\n     } else {\n       minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n     }\n     if (maxSplitSize !\u003d 0) {\n       maxSize \u003d maxSplitSize;\n     } else {\n       maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n       // If maxSize is not configured, a single split will be generated per\n       // node.\n     }\n     if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n       throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                             \" cannot be larger than maximum split size \" +\n                             maxSize);\n     }\n     if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n-      throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n+      throw new IOException(\"Minimum split size per rack \" + minSizeRack +\n                             \" cannot be larger than maximum split size \" +\n                             maxSize);\n     }\n     if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n-      throw new IOException(\"Minimum split size per node\" + minSizeNode +\n-                            \" cannot be smaller than minimum split \" +\n+      throw new IOException(\"Minimum split size per node \" + minSizeNode +\n+                            \" cannot be larger than minimum split \" +\n                             \"size per rack \" + minSizeRack);\n     }\n \n     // all the files in input set\n     List\u003cFileStatus\u003e stats \u003d listStatus(job);\n     List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n     if (stats.size() \u003d\u003d 0) {\n       return splits;    \n     }\n \n     // In one single iteration, process all the paths in a single pool.\n     // Processing one pool at a time ensures that a split contains paths\n     // from a single pool only.\n     for (MultiPathFilter onepool : pools) {\n       ArrayList\u003cFileStatus\u003e myPaths \u003d new ArrayList\u003cFileStatus\u003e();\n       \n       // pick one input path. If it matches all the filters in a pool,\n       // add it to the output set\n       for (Iterator\u003cFileStatus\u003e iter \u003d stats.iterator(); iter.hasNext();) {\n         FileStatus p \u003d iter.next();\n         if (onepool.accept(p.getPath())) {\n           myPaths.add(p); // add it to my output set\n           iter.remove();\n         }\n       }\n       // create splits for all files in this pool.\n       getMoreSplits(job, myPaths, maxSize, minSizeNode, minSizeRack, splits);\n     }\n \n     // create splits for all files that are not in any pool.\n     getMoreSplits(job, stats, maxSize, minSizeNode, minSizeRack, splits);\n \n     // free up rackToNodes map\n     rackToNodes.clear();\n     return splits;    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cInputSplit\u003e getSplits(JobContext job) \n    throws IOException {\n    long minSizeNode \u003d 0;\n    long minSizeRack \u003d 0;\n    long maxSize \u003d 0;\n    Configuration conf \u003d job.getConfiguration();\n\n    // the values specified by setxxxSplitSize() takes precedence over the\n    // values that might have been specified in the config\n    if (minSplitSizeNode !\u003d 0) {\n      minSizeNode \u003d minSplitSizeNode;\n    } else {\n      minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n    }\n    if (minSplitSizeRack !\u003d 0) {\n      minSizeRack \u003d minSplitSizeRack;\n    } else {\n      minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n    }\n    if (maxSplitSize !\u003d 0) {\n      maxSize \u003d maxSplitSize;\n    } else {\n      maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n      // If maxSize is not configured, a single split will be generated per\n      // node.\n    }\n    if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n      throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n      throw new IOException(\"Minimum split size per rack \" + minSizeRack +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n      throw new IOException(\"Minimum split size per node \" + minSizeNode +\n                            \" cannot be larger than minimum split \" +\n                            \"size per rack \" + minSizeRack);\n    }\n\n    // all the files in input set\n    List\u003cFileStatus\u003e stats \u003d listStatus(job);\n    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    if (stats.size() \u003d\u003d 0) {\n      return splits;    \n    }\n\n    // In one single iteration, process all the paths in a single pool.\n    // Processing one pool at a time ensures that a split contains paths\n    // from a single pool only.\n    for (MultiPathFilter onepool : pools) {\n      ArrayList\u003cFileStatus\u003e myPaths \u003d new ArrayList\u003cFileStatus\u003e();\n      \n      // pick one input path. If it matches all the filters in a pool,\n      // add it to the output set\n      for (Iterator\u003cFileStatus\u003e iter \u003d stats.iterator(); iter.hasNext();) {\n        FileStatus p \u003d iter.next();\n        if (onepool.accept(p.getPath())) {\n          myPaths.add(p); // add it to my output set\n          iter.remove();\n        }\n      }\n      // create splits for all files in this pool.\n      getMoreSplits(job, myPaths, maxSize, minSizeNode, minSizeRack, splits);\n    }\n\n    // create splits for all files that are not in any pool.\n    getMoreSplits(job, stats, maxSize, minSizeNode, minSizeRack, splits);\n\n    // free up rackToNodes map\n    rackToNodes.clear();\n    return splits;    \n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
      "extendedDetails": {}
    },
    "381a4c42135916245c8992daa3d03f38e282108d": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5352. Optimize node local splits generated by CombineFileInputFormat. (sseth)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1509345 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/13 10:42 AM",
      "commitName": "381a4c42135916245c8992daa3d03f38e282108d",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "26/07/13 11:16 AM",
      "commitNameOld": "ec18984252731089ab5af12b3603dcfc3d4f4593",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 5.98,
      "commitsBetweenForRepo": 41,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,75 @@\n   public List\u003cInputSplit\u003e getSplits(JobContext job) \n     throws IOException {\n     long minSizeNode \u003d 0;\n     long minSizeRack \u003d 0;\n     long maxSize \u003d 0;\n     Configuration conf \u003d job.getConfiguration();\n \n     // the values specified by setxxxSplitSize() takes precedence over the\n     // values that might have been specified in the config\n     if (minSplitSizeNode !\u003d 0) {\n       minSizeNode \u003d minSplitSizeNode;\n     } else {\n       minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n     }\n     if (minSplitSizeRack !\u003d 0) {\n       minSizeRack \u003d minSplitSizeRack;\n     } else {\n       minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n     }\n     if (maxSplitSize !\u003d 0) {\n       maxSize \u003d maxSplitSize;\n     } else {\n       maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n+      // If maxSize is not configured, a single split will be generated per\n+      // node.\n     }\n     if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n       throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                             \" cannot be larger than maximum split size \" +\n                             maxSize);\n     }\n     if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n       throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n                             \" cannot be larger than maximum split size \" +\n                             maxSize);\n     }\n     if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n       throw new IOException(\"Minimum split size per node\" + minSizeNode +\n                             \" cannot be smaller than minimum split \" +\n                             \"size per rack \" + minSizeRack);\n     }\n \n     // all the files in input set\n     List\u003cFileStatus\u003e stats \u003d listStatus(job);\n     List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n     if (stats.size() \u003d\u003d 0) {\n       return splits;    \n     }\n \n     // In one single iteration, process all the paths in a single pool.\n     // Processing one pool at a time ensures that a split contains paths\n     // from a single pool only.\n     for (MultiPathFilter onepool : pools) {\n       ArrayList\u003cFileStatus\u003e myPaths \u003d new ArrayList\u003cFileStatus\u003e();\n       \n       // pick one input path. If it matches all the filters in a pool,\n       // add it to the output set\n       for (Iterator\u003cFileStatus\u003e iter \u003d stats.iterator(); iter.hasNext();) {\n         FileStatus p \u003d iter.next();\n         if (onepool.accept(p.getPath())) {\n           myPaths.add(p); // add it to my output set\n           iter.remove();\n         }\n       }\n       // create splits for all files in this pool.\n       getMoreSplits(job, myPaths, maxSize, minSizeNode, minSizeRack, splits);\n     }\n \n     // create splits for all files that are not in any pool.\n     getMoreSplits(job, stats, maxSize, minSizeNode, minSizeRack, splits);\n \n     // free up rackToNodes map\n     rackToNodes.clear();\n     return splits;    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cInputSplit\u003e getSplits(JobContext job) \n    throws IOException {\n    long minSizeNode \u003d 0;\n    long minSizeRack \u003d 0;\n    long maxSize \u003d 0;\n    Configuration conf \u003d job.getConfiguration();\n\n    // the values specified by setxxxSplitSize() takes precedence over the\n    // values that might have been specified in the config\n    if (minSplitSizeNode !\u003d 0) {\n      minSizeNode \u003d minSplitSizeNode;\n    } else {\n      minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n    }\n    if (minSplitSizeRack !\u003d 0) {\n      minSizeRack \u003d minSplitSizeRack;\n    } else {\n      minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n    }\n    if (maxSplitSize !\u003d 0) {\n      maxSize \u003d maxSplitSize;\n    } else {\n      maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n      // If maxSize is not configured, a single split will be generated per\n      // node.\n    }\n    if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n      throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n      throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n      throw new IOException(\"Minimum split size per node\" + minSizeNode +\n                            \" cannot be smaller than minimum split \" +\n                            \"size per rack \" + minSizeRack);\n    }\n\n    // all the files in input set\n    List\u003cFileStatus\u003e stats \u003d listStatus(job);\n    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    if (stats.size() \u003d\u003d 0) {\n      return splits;    \n    }\n\n    // In one single iteration, process all the paths in a single pool.\n    // Processing one pool at a time ensures that a split contains paths\n    // from a single pool only.\n    for (MultiPathFilter onepool : pools) {\n      ArrayList\u003cFileStatus\u003e myPaths \u003d new ArrayList\u003cFileStatus\u003e();\n      \n      // pick one input path. If it matches all the filters in a pool,\n      // add it to the output set\n      for (Iterator\u003cFileStatus\u003e iter \u003d stats.iterator(); iter.hasNext();) {\n        FileStatus p \u003d iter.next();\n        if (onepool.accept(p.getPath())) {\n          myPaths.add(p); // add it to my output set\n          iter.remove();\n        }\n      }\n      // create splits for all files in this pool.\n      getMoreSplits(job, myPaths, maxSize, minSizeNode, minSizeRack, splits);\n    }\n\n    // create splits for all files that are not in any pool.\n    getMoreSplits(job, stats, maxSize, minSizeNode, minSizeRack, splits);\n\n    // free up rackToNodes map\n    rackToNodes.clear();\n    return splits;    \n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
      "extendedDetails": {}
    },
    "ec18984252731089ab5af12b3603dcfc3d4f4593": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-1981. Improve getSplits performance by using listLocatedStatus. Contributed by Hairong Kuang and Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1507385 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/13 11:16 AM",
      "commitName": "ec18984252731089ab5af12b3603dcfc3d4f4593",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "27/02/13 10:49 AM",
      "commitNameOld": "0b9ed2364a0690d62a0d51d636027acb984e3e91",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 148.98,
      "commitsBetweenForRepo": 924,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,73 @@\n   public List\u003cInputSplit\u003e getSplits(JobContext job) \n     throws IOException {\n     long minSizeNode \u003d 0;\n     long minSizeRack \u003d 0;\n     long maxSize \u003d 0;\n     Configuration conf \u003d job.getConfiguration();\n \n     // the values specified by setxxxSplitSize() takes precedence over the\n     // values that might have been specified in the config\n     if (minSplitSizeNode !\u003d 0) {\n       minSizeNode \u003d minSplitSizeNode;\n     } else {\n       minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n     }\n     if (minSplitSizeRack !\u003d 0) {\n       minSizeRack \u003d minSplitSizeRack;\n     } else {\n       minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n     }\n     if (maxSplitSize !\u003d 0) {\n       maxSize \u003d maxSplitSize;\n     } else {\n       maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n     }\n     if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n       throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                             \" cannot be larger than maximum split size \" +\n                             maxSize);\n     }\n     if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n       throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n                             \" cannot be larger than maximum split size \" +\n                             maxSize);\n     }\n     if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n       throw new IOException(\"Minimum split size per node\" + minSizeNode +\n                             \" cannot be smaller than minimum split \" +\n                             \"size per rack \" + minSizeRack);\n     }\n \n     // all the files in input set\n-    Path[] paths \u003d FileUtil.stat2Paths(\n-                     listStatus(job).toArray(new FileStatus[0]));\n+    List\u003cFileStatus\u003e stats \u003d listStatus(job);\n     List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n-    if (paths.length \u003d\u003d 0) {\n+    if (stats.size() \u003d\u003d 0) {\n       return splits;    \n     }\n \n-    // Convert them to Paths first. This is a costly operation and \n-    // we should do it first, otherwise we will incur doing it multiple\n-    // times, one time each for each pool in the next loop.\n-    List\u003cPath\u003e newpaths \u003d new LinkedList\u003cPath\u003e();\n-    for (int i \u003d 0; i \u003c paths.length; i++) {\n-      FileSystem fs \u003d paths[i].getFileSystem(conf);\n-      Path p \u003d fs.makeQualified(paths[i]);\n-      newpaths.add(p);\n-    }\n-\n     // In one single iteration, process all the paths in a single pool.\n     // Processing one pool at a time ensures that a split contains paths\n     // from a single pool only.\n     for (MultiPathFilter onepool : pools) {\n-      ArrayList\u003cPath\u003e myPaths \u003d new ArrayList\u003cPath\u003e();\n+      ArrayList\u003cFileStatus\u003e myPaths \u003d new ArrayList\u003cFileStatus\u003e();\n       \n       // pick one input path. If it matches all the filters in a pool,\n       // add it to the output set\n-      for (Iterator\u003cPath\u003e iter \u003d newpaths.iterator(); iter.hasNext();) {\n-        Path p \u003d iter.next();\n-        if (onepool.accept(p)) {\n+      for (Iterator\u003cFileStatus\u003e iter \u003d stats.iterator(); iter.hasNext();) {\n+        FileStatus p \u003d iter.next();\n+        if (onepool.accept(p.getPath())) {\n           myPaths.add(p); // add it to my output set\n           iter.remove();\n         }\n       }\n       // create splits for all files in this pool.\n-      getMoreSplits(job, myPaths.toArray(new Path[myPaths.size()]), \n-                    maxSize, minSizeNode, minSizeRack, splits);\n+      getMoreSplits(job, myPaths, maxSize, minSizeNode, minSizeRack, splits);\n     }\n \n     // create splits for all files that are not in any pool.\n-    getMoreSplits(job, newpaths.toArray(new Path[newpaths.size()]), \n-                  maxSize, minSizeNode, minSizeRack, splits);\n+    getMoreSplits(job, stats, maxSize, minSizeNode, minSizeRack, splits);\n \n     // free up rackToNodes map\n     rackToNodes.clear();\n     return splits;    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cInputSplit\u003e getSplits(JobContext job) \n    throws IOException {\n    long minSizeNode \u003d 0;\n    long minSizeRack \u003d 0;\n    long maxSize \u003d 0;\n    Configuration conf \u003d job.getConfiguration();\n\n    // the values specified by setxxxSplitSize() takes precedence over the\n    // values that might have been specified in the config\n    if (minSplitSizeNode !\u003d 0) {\n      minSizeNode \u003d minSplitSizeNode;\n    } else {\n      minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n    }\n    if (minSplitSizeRack !\u003d 0) {\n      minSizeRack \u003d minSplitSizeRack;\n    } else {\n      minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n    }\n    if (maxSplitSize !\u003d 0) {\n      maxSize \u003d maxSplitSize;\n    } else {\n      maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n    }\n    if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n      throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n      throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n      throw new IOException(\"Minimum split size per node\" + minSizeNode +\n                            \" cannot be smaller than minimum split \" +\n                            \"size per rack \" + minSizeRack);\n    }\n\n    // all the files in input set\n    List\u003cFileStatus\u003e stats \u003d listStatus(job);\n    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    if (stats.size() \u003d\u003d 0) {\n      return splits;    \n    }\n\n    // In one single iteration, process all the paths in a single pool.\n    // Processing one pool at a time ensures that a split contains paths\n    // from a single pool only.\n    for (MultiPathFilter onepool : pools) {\n      ArrayList\u003cFileStatus\u003e myPaths \u003d new ArrayList\u003cFileStatus\u003e();\n      \n      // pick one input path. If it matches all the filters in a pool,\n      // add it to the output set\n      for (Iterator\u003cFileStatus\u003e iter \u003d stats.iterator(); iter.hasNext();) {\n        FileStatus p \u003d iter.next();\n        if (onepool.accept(p.getPath())) {\n          myPaths.add(p); // add it to my output set\n          iter.remove();\n        }\n      }\n      // create splits for all files in this pool.\n      getMoreSplits(job, myPaths, maxSize, minSizeNode, minSizeRack, splits);\n    }\n\n    // create splits for all files that are not in any pool.\n    getMoreSplits(job, stats, maxSize, minSizeNode, minSizeRack, splits);\n\n    // free up rackToNodes map\n    rackToNodes.clear();\n    return splits;    \n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
      "extendedDetails": {}
    },
    "905b17876c44634545a68300ff2f2d73fb86d3b7": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4723. Fix warnings found by findbugs 2. Contributed by Sandy Ryza\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1409601 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/11/12 4:16 PM",
      "commitName": "905b17876c44634545a68300ff2f2d73fb86d3b7",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "29/10/12 10:41 PM",
      "commitNameOld": "16e21dfe92d61deb539c78bc2e43e04d7b474f88",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 15.77,
      "commitsBetweenForRepo": 98,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,88 +1,87 @@\n   public List\u003cInputSplit\u003e getSplits(JobContext job) \n     throws IOException {\n \n     long minSizeNode \u003d 0;\n     long minSizeRack \u003d 0;\n     long maxSize \u003d 0;\n     Configuration conf \u003d job.getConfiguration();\n \n     // the values specified by setxxxSplitSize() takes precedence over the\n     // values that might have been specified in the config\n     if (minSplitSizeNode !\u003d 0) {\n       minSizeNode \u003d minSplitSizeNode;\n     } else {\n       minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n     }\n     if (minSplitSizeRack !\u003d 0) {\n       minSizeRack \u003d minSplitSizeRack;\n     } else {\n       minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n     }\n     if (maxSplitSize !\u003d 0) {\n       maxSize \u003d maxSplitSize;\n     } else {\n       maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n     }\n     if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n       throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                             \" cannot be larger than maximum split size \" +\n                             maxSize);\n     }\n     if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n       throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n                             \" cannot be larger than maximum split size \" +\n                             maxSize);\n     }\n     if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n       throw new IOException(\"Minimum split size per node\" + minSizeNode +\n                             \" cannot be smaller than minimum split \" +\n                             \"size per rack \" + minSizeRack);\n     }\n \n     // all the files in input set\n     Path[] paths \u003d FileUtil.stat2Paths(\n                      listStatus(job).toArray(new FileStatus[0]));\n     List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n     if (paths.length \u003d\u003d 0) {\n       return splits;    \n     }\n \n     // Convert them to Paths first. This is a costly operation and \n     // we should do it first, otherwise we will incur doing it multiple\n     // times, one time each for each pool in the next loop.\n     List\u003cPath\u003e newpaths \u003d new LinkedList\u003cPath\u003e();\n     for (int i \u003d 0; i \u003c paths.length; i++) {\n       FileSystem fs \u003d paths[i].getFileSystem(conf);\n       Path p \u003d fs.makeQualified(paths[i]);\n       newpaths.add(p);\n     }\n-    paths \u003d null;\n \n     // In one single iteration, process all the paths in a single pool.\n     // Processing one pool at a time ensures that a split contains paths\n     // from a single pool only.\n     for (MultiPathFilter onepool : pools) {\n       ArrayList\u003cPath\u003e myPaths \u003d new ArrayList\u003cPath\u003e();\n       \n       // pick one input path. If it matches all the filters in a pool,\n       // add it to the output set\n       for (Iterator\u003cPath\u003e iter \u003d newpaths.iterator(); iter.hasNext();) {\n         Path p \u003d iter.next();\n         if (onepool.accept(p)) {\n           myPaths.add(p); // add it to my output set\n           iter.remove();\n         }\n       }\n       // create splits for all files in this pool.\n       getMoreSplits(job, myPaths.toArray(new Path[myPaths.size()]), \n                     maxSize, minSizeNode, minSizeRack, splits);\n     }\n \n     // create splits for all files that are not in any pool.\n     getMoreSplits(job, newpaths.toArray(new Path[newpaths.size()]), \n                   maxSize, minSizeNode, minSizeRack, splits);\n \n     // free up rackToNodes map\n     rackToNodes.clear();\n     return splits;    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cInputSplit\u003e getSplits(JobContext job) \n    throws IOException {\n\n    long minSizeNode \u003d 0;\n    long minSizeRack \u003d 0;\n    long maxSize \u003d 0;\n    Configuration conf \u003d job.getConfiguration();\n\n    // the values specified by setxxxSplitSize() takes precedence over the\n    // values that might have been specified in the config\n    if (minSplitSizeNode !\u003d 0) {\n      minSizeNode \u003d minSplitSizeNode;\n    } else {\n      minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n    }\n    if (minSplitSizeRack !\u003d 0) {\n      minSizeRack \u003d minSplitSizeRack;\n    } else {\n      minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n    }\n    if (maxSplitSize !\u003d 0) {\n      maxSize \u003d maxSplitSize;\n    } else {\n      maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n    }\n    if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n      throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n      throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n      throw new IOException(\"Minimum split size per node\" + minSizeNode +\n                            \" cannot be smaller than minimum split \" +\n                            \"size per rack \" + minSizeRack);\n    }\n\n    // all the files in input set\n    Path[] paths \u003d FileUtil.stat2Paths(\n                     listStatus(job).toArray(new FileStatus[0]));\n    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    if (paths.length \u003d\u003d 0) {\n      return splits;    \n    }\n\n    // Convert them to Paths first. This is a costly operation and \n    // we should do it first, otherwise we will incur doing it multiple\n    // times, one time each for each pool in the next loop.\n    List\u003cPath\u003e newpaths \u003d new LinkedList\u003cPath\u003e();\n    for (int i \u003d 0; i \u003c paths.length; i++) {\n      FileSystem fs \u003d paths[i].getFileSystem(conf);\n      Path p \u003d fs.makeQualified(paths[i]);\n      newpaths.add(p);\n    }\n\n    // In one single iteration, process all the paths in a single pool.\n    // Processing one pool at a time ensures that a split contains paths\n    // from a single pool only.\n    for (MultiPathFilter onepool : pools) {\n      ArrayList\u003cPath\u003e myPaths \u003d new ArrayList\u003cPath\u003e();\n      \n      // pick one input path. If it matches all the filters in a pool,\n      // add it to the output set\n      for (Iterator\u003cPath\u003e iter \u003d newpaths.iterator(); iter.hasNext();) {\n        Path p \u003d iter.next();\n        if (onepool.accept(p)) {\n          myPaths.add(p); // add it to my output set\n          iter.remove();\n        }\n      }\n      // create splits for all files in this pool.\n      getMoreSplits(job, myPaths.toArray(new Path[myPaths.size()]), \n                    maxSize, minSizeNode, minSizeRack, splits);\n    }\n\n    // create splits for all files that are not in any pool.\n    getMoreSplits(job, newpaths.toArray(new Path[newpaths.size()]), \n                  maxSize, minSizeNode, minSizeRack, splits);\n\n    // free up rackToNodes map\n    rackToNodes.clear();\n    return splits;    \n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
      "extendedDetails": {}
    },
    "16e21dfe92d61deb539c78bc2e43e04d7b474f88": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-1806. CombineFileInputFormat does not work with paths not on default FS. (Gera Shegalov via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1403614 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/10/12 10:41 PM",
      "commitName": "16e21dfe92d61deb539c78bc2e43e04d7b474f88",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "22/08/12 5:22 PM",
      "commitNameOld": "f909a1d4d70a6ee7ab897f8611481d5e62861a64",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 68.22,
      "commitsBetweenForRepo": 407,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,88 @@\n   public List\u003cInputSplit\u003e getSplits(JobContext job) \n     throws IOException {\n \n     long minSizeNode \u003d 0;\n     long minSizeRack \u003d 0;\n     long maxSize \u003d 0;\n     Configuration conf \u003d job.getConfiguration();\n \n     // the values specified by setxxxSplitSize() takes precedence over the\n     // values that might have been specified in the config\n     if (minSplitSizeNode !\u003d 0) {\n       minSizeNode \u003d minSplitSizeNode;\n     } else {\n       minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n     }\n     if (minSplitSizeRack !\u003d 0) {\n       minSizeRack \u003d minSplitSizeRack;\n     } else {\n       minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n     }\n     if (maxSplitSize !\u003d 0) {\n       maxSize \u003d maxSplitSize;\n     } else {\n       maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n     }\n     if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n       throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                             \" cannot be larger than maximum split size \" +\n                             maxSize);\n     }\n     if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n       throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n                             \" cannot be larger than maximum split size \" +\n                             maxSize);\n     }\n     if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n       throw new IOException(\"Minimum split size per node\" + minSizeNode +\n                             \" cannot be smaller than minimum split \" +\n                             \"size per rack \" + minSizeRack);\n     }\n \n     // all the files in input set\n     Path[] paths \u003d FileUtil.stat2Paths(\n                      listStatus(job).toArray(new FileStatus[0]));\n     List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n     if (paths.length \u003d\u003d 0) {\n       return splits;    \n     }\n \n     // Convert them to Paths first. This is a costly operation and \n     // we should do it first, otherwise we will incur doing it multiple\n     // times, one time each for each pool in the next loop.\n     List\u003cPath\u003e newpaths \u003d new LinkedList\u003cPath\u003e();\n     for (int i \u003d 0; i \u003c paths.length; i++) {\n-      Path p \u003d new Path(paths[i].toUri().getPath());\n+      FileSystem fs \u003d paths[i].getFileSystem(conf);\n+      Path p \u003d fs.makeQualified(paths[i]);\n       newpaths.add(p);\n     }\n     paths \u003d null;\n \n     // In one single iteration, process all the paths in a single pool.\n     // Processing one pool at a time ensures that a split contains paths\n     // from a single pool only.\n     for (MultiPathFilter onepool : pools) {\n       ArrayList\u003cPath\u003e myPaths \u003d new ArrayList\u003cPath\u003e();\n       \n       // pick one input path. If it matches all the filters in a pool,\n       // add it to the output set\n       for (Iterator\u003cPath\u003e iter \u003d newpaths.iterator(); iter.hasNext();) {\n         Path p \u003d iter.next();\n         if (onepool.accept(p)) {\n           myPaths.add(p); // add it to my output set\n           iter.remove();\n         }\n       }\n       // create splits for all files in this pool.\n       getMoreSplits(job, myPaths.toArray(new Path[myPaths.size()]), \n                     maxSize, minSizeNode, minSizeRack, splits);\n     }\n \n     // create splits for all files that are not in any pool.\n     getMoreSplits(job, newpaths.toArray(new Path[newpaths.size()]), \n                   maxSize, minSizeNode, minSizeRack, splits);\n \n     // free up rackToNodes map\n     rackToNodes.clear();\n     return splits;    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cInputSplit\u003e getSplits(JobContext job) \n    throws IOException {\n\n    long minSizeNode \u003d 0;\n    long minSizeRack \u003d 0;\n    long maxSize \u003d 0;\n    Configuration conf \u003d job.getConfiguration();\n\n    // the values specified by setxxxSplitSize() takes precedence over the\n    // values that might have been specified in the config\n    if (minSplitSizeNode !\u003d 0) {\n      minSizeNode \u003d minSplitSizeNode;\n    } else {\n      minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n    }\n    if (minSplitSizeRack !\u003d 0) {\n      minSizeRack \u003d minSplitSizeRack;\n    } else {\n      minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n    }\n    if (maxSplitSize !\u003d 0) {\n      maxSize \u003d maxSplitSize;\n    } else {\n      maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n    }\n    if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n      throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n      throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n      throw new IOException(\"Minimum split size per node\" + minSizeNode +\n                            \" cannot be smaller than minimum split \" +\n                            \"size per rack \" + minSizeRack);\n    }\n\n    // all the files in input set\n    Path[] paths \u003d FileUtil.stat2Paths(\n                     listStatus(job).toArray(new FileStatus[0]));\n    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    if (paths.length \u003d\u003d 0) {\n      return splits;    \n    }\n\n    // Convert them to Paths first. This is a costly operation and \n    // we should do it first, otherwise we will incur doing it multiple\n    // times, one time each for each pool in the next loop.\n    List\u003cPath\u003e newpaths \u003d new LinkedList\u003cPath\u003e();\n    for (int i \u003d 0; i \u003c paths.length; i++) {\n      FileSystem fs \u003d paths[i].getFileSystem(conf);\n      Path p \u003d fs.makeQualified(paths[i]);\n      newpaths.add(p);\n    }\n    paths \u003d null;\n\n    // In one single iteration, process all the paths in a single pool.\n    // Processing one pool at a time ensures that a split contains paths\n    // from a single pool only.\n    for (MultiPathFilter onepool : pools) {\n      ArrayList\u003cPath\u003e myPaths \u003d new ArrayList\u003cPath\u003e();\n      \n      // pick one input path. If it matches all the filters in a pool,\n      // add it to the output set\n      for (Iterator\u003cPath\u003e iter \u003d newpaths.iterator(); iter.hasNext();) {\n        Path p \u003d iter.next();\n        if (onepool.accept(p)) {\n          myPaths.add(p); // add it to my output set\n          iter.remove();\n        }\n      }\n      // create splits for all files in this pool.\n      getMoreSplits(job, myPaths.toArray(new Path[myPaths.size()]), \n                    maxSize, minSizeNode, minSizeRack, splits);\n    }\n\n    // create splits for all files that are not in any pool.\n    getMoreSplits(job, newpaths.toArray(new Path[newpaths.size()]), \n                  maxSize, minSizeNode, minSizeRack, splits);\n\n    // free up rackToNodes map\n    rackToNodes.clear();\n    return splits;    \n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public List\u003cInputSplit\u003e getSplits(JobContext job) \n    throws IOException {\n\n    long minSizeNode \u003d 0;\n    long minSizeRack \u003d 0;\n    long maxSize \u003d 0;\n    Configuration conf \u003d job.getConfiguration();\n\n    // the values specified by setxxxSplitSize() takes precedence over the\n    // values that might have been specified in the config\n    if (minSplitSizeNode !\u003d 0) {\n      minSizeNode \u003d minSplitSizeNode;\n    } else {\n      minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n    }\n    if (minSplitSizeRack !\u003d 0) {\n      minSizeRack \u003d minSplitSizeRack;\n    } else {\n      minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n    }\n    if (maxSplitSize !\u003d 0) {\n      maxSize \u003d maxSplitSize;\n    } else {\n      maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n    }\n    if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n      throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n      throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n      throw new IOException(\"Minimum split size per node\" + minSizeNode +\n                            \" cannot be smaller than minimum split \" +\n                            \"size per rack \" + minSizeRack);\n    }\n\n    // all the files in input set\n    Path[] paths \u003d FileUtil.stat2Paths(\n                     listStatus(job).toArray(new FileStatus[0]));\n    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    if (paths.length \u003d\u003d 0) {\n      return splits;    \n    }\n\n    // Convert them to Paths first. This is a costly operation and \n    // we should do it first, otherwise we will incur doing it multiple\n    // times, one time each for each pool in the next loop.\n    List\u003cPath\u003e newpaths \u003d new LinkedList\u003cPath\u003e();\n    for (int i \u003d 0; i \u003c paths.length; i++) {\n      Path p \u003d new Path(paths[i].toUri().getPath());\n      newpaths.add(p);\n    }\n    paths \u003d null;\n\n    // In one single iteration, process all the paths in a single pool.\n    // Processing one pool at a time ensures that a split contains paths\n    // from a single pool only.\n    for (MultiPathFilter onepool : pools) {\n      ArrayList\u003cPath\u003e myPaths \u003d new ArrayList\u003cPath\u003e();\n      \n      // pick one input path. If it matches all the filters in a pool,\n      // add it to the output set\n      for (Iterator\u003cPath\u003e iter \u003d newpaths.iterator(); iter.hasNext();) {\n        Path p \u003d iter.next();\n        if (onepool.accept(p)) {\n          myPaths.add(p); // add it to my output set\n          iter.remove();\n        }\n      }\n      // create splits for all files in this pool.\n      getMoreSplits(job, myPaths.toArray(new Path[myPaths.size()]), \n                    maxSize, minSizeNode, minSizeRack, splits);\n    }\n\n    // create splits for all files that are not in any pool.\n    getMoreSplits(job, newpaths.toArray(new Path[newpaths.size()]), \n                  maxSize, minSizeNode, minSizeRack, splits);\n\n    // free up rackToNodes map\n    rackToNodes.clear();\n    return splits;    \n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public List\u003cInputSplit\u003e getSplits(JobContext job) \n    throws IOException {\n\n    long minSizeNode \u003d 0;\n    long minSizeRack \u003d 0;\n    long maxSize \u003d 0;\n    Configuration conf \u003d job.getConfiguration();\n\n    // the values specified by setxxxSplitSize() takes precedence over the\n    // values that might have been specified in the config\n    if (minSplitSizeNode !\u003d 0) {\n      minSizeNode \u003d minSplitSizeNode;\n    } else {\n      minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n    }\n    if (minSplitSizeRack !\u003d 0) {\n      minSizeRack \u003d minSplitSizeRack;\n    } else {\n      minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n    }\n    if (maxSplitSize !\u003d 0) {\n      maxSize \u003d maxSplitSize;\n    } else {\n      maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n    }\n    if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n      throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n      throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n      throw new IOException(\"Minimum split size per node\" + minSizeNode +\n                            \" cannot be smaller than minimum split \" +\n                            \"size per rack \" + minSizeRack);\n    }\n\n    // all the files in input set\n    Path[] paths \u003d FileUtil.stat2Paths(\n                     listStatus(job).toArray(new FileStatus[0]));\n    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    if (paths.length \u003d\u003d 0) {\n      return splits;    \n    }\n\n    // Convert them to Paths first. This is a costly operation and \n    // we should do it first, otherwise we will incur doing it multiple\n    // times, one time each for each pool in the next loop.\n    List\u003cPath\u003e newpaths \u003d new LinkedList\u003cPath\u003e();\n    for (int i \u003d 0; i \u003c paths.length; i++) {\n      Path p \u003d new Path(paths[i].toUri().getPath());\n      newpaths.add(p);\n    }\n    paths \u003d null;\n\n    // In one single iteration, process all the paths in a single pool.\n    // Processing one pool at a time ensures that a split contains paths\n    // from a single pool only.\n    for (MultiPathFilter onepool : pools) {\n      ArrayList\u003cPath\u003e myPaths \u003d new ArrayList\u003cPath\u003e();\n      \n      // pick one input path. If it matches all the filters in a pool,\n      // add it to the output set\n      for (Iterator\u003cPath\u003e iter \u003d newpaths.iterator(); iter.hasNext();) {\n        Path p \u003d iter.next();\n        if (onepool.accept(p)) {\n          myPaths.add(p); // add it to my output set\n          iter.remove();\n        }\n      }\n      // create splits for all files in this pool.\n      getMoreSplits(job, myPaths.toArray(new Path[myPaths.size()]), \n                    maxSize, minSizeNode, minSizeRack, splits);\n    }\n\n    // create splits for all files that are not in any pool.\n    getMoreSplits(job, newpaths.toArray(new Path[newpaths.size()]), \n                  maxSize, minSizeNode, minSizeRack, splits);\n\n    // free up rackToNodes map\n    rackToNodes.clear();\n    return splits;    \n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,87 @@\n+  public List\u003cInputSplit\u003e getSplits(JobContext job) \n+    throws IOException {\n+\n+    long minSizeNode \u003d 0;\n+    long minSizeRack \u003d 0;\n+    long maxSize \u003d 0;\n+    Configuration conf \u003d job.getConfiguration();\n+\n+    // the values specified by setxxxSplitSize() takes precedence over the\n+    // values that might have been specified in the config\n+    if (minSplitSizeNode !\u003d 0) {\n+      minSizeNode \u003d minSplitSizeNode;\n+    } else {\n+      minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n+    }\n+    if (minSplitSizeRack !\u003d 0) {\n+      minSizeRack \u003d minSplitSizeRack;\n+    } else {\n+      minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n+    }\n+    if (maxSplitSize !\u003d 0) {\n+      maxSize \u003d maxSplitSize;\n+    } else {\n+      maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n+    }\n+    if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n+      throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n+                            \" cannot be larger than maximum split size \" +\n+                            maxSize);\n+    }\n+    if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n+      throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n+                            \" cannot be larger than maximum split size \" +\n+                            maxSize);\n+    }\n+    if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n+      throw new IOException(\"Minimum split size per node\" + minSizeNode +\n+                            \" cannot be smaller than minimum split \" +\n+                            \"size per rack \" + minSizeRack);\n+    }\n+\n+    // all the files in input set\n+    Path[] paths \u003d FileUtil.stat2Paths(\n+                     listStatus(job).toArray(new FileStatus[0]));\n+    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n+    if (paths.length \u003d\u003d 0) {\n+      return splits;    \n+    }\n+\n+    // Convert them to Paths first. This is a costly operation and \n+    // we should do it first, otherwise we will incur doing it multiple\n+    // times, one time each for each pool in the next loop.\n+    List\u003cPath\u003e newpaths \u003d new LinkedList\u003cPath\u003e();\n+    for (int i \u003d 0; i \u003c paths.length; i++) {\n+      Path p \u003d new Path(paths[i].toUri().getPath());\n+      newpaths.add(p);\n+    }\n+    paths \u003d null;\n+\n+    // In one single iteration, process all the paths in a single pool.\n+    // Processing one pool at a time ensures that a split contains paths\n+    // from a single pool only.\n+    for (MultiPathFilter onepool : pools) {\n+      ArrayList\u003cPath\u003e myPaths \u003d new ArrayList\u003cPath\u003e();\n+      \n+      // pick one input path. If it matches all the filters in a pool,\n+      // add it to the output set\n+      for (Iterator\u003cPath\u003e iter \u003d newpaths.iterator(); iter.hasNext();) {\n+        Path p \u003d iter.next();\n+        if (onepool.accept(p)) {\n+          myPaths.add(p); // add it to my output set\n+          iter.remove();\n+        }\n+      }\n+      // create splits for all files in this pool.\n+      getMoreSplits(job, myPaths.toArray(new Path[myPaths.size()]), \n+                    maxSize, minSizeNode, minSizeRack, splits);\n+    }\n+\n+    // create splits for all files that are not in any pool.\n+    getMoreSplits(job, newpaths.toArray(new Path[newpaths.size()]), \n+                  maxSize, minSizeNode, minSizeRack, splits);\n+\n+    // free up rackToNodes map\n+    rackToNodes.clear();\n+    return splits;    \n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cInputSplit\u003e getSplits(JobContext job) \n    throws IOException {\n\n    long minSizeNode \u003d 0;\n    long minSizeRack \u003d 0;\n    long maxSize \u003d 0;\n    Configuration conf \u003d job.getConfiguration();\n\n    // the values specified by setxxxSplitSize() takes precedence over the\n    // values that might have been specified in the config\n    if (minSplitSizeNode !\u003d 0) {\n      minSizeNode \u003d minSplitSizeNode;\n    } else {\n      minSizeNode \u003d conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\n    }\n    if (minSplitSizeRack !\u003d 0) {\n      minSizeRack \u003d minSplitSizeRack;\n    } else {\n      minSizeRack \u003d conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\n    }\n    if (maxSplitSize !\u003d 0) {\n      maxSize \u003d maxSplitSize;\n    } else {\n      maxSize \u003d conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\n    }\n    if (minSizeNode !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeNode \u003e maxSize) {\n      throw new IOException(\"Minimum split size pernode \" + minSizeNode +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 maxSize !\u003d 0 \u0026\u0026 minSizeRack \u003e maxSize) {\n      throw new IOException(\"Minimum split size per rack\" + minSizeRack +\n                            \" cannot be larger than maximum split size \" +\n                            maxSize);\n    }\n    if (minSizeRack !\u003d 0 \u0026\u0026 minSizeNode \u003e minSizeRack) {\n      throw new IOException(\"Minimum split size per node\" + minSizeNode +\n                            \" cannot be smaller than minimum split \" +\n                            \"size per rack \" + minSizeRack);\n    }\n\n    // all the files in input set\n    Path[] paths \u003d FileUtil.stat2Paths(\n                     listStatus(job).toArray(new FileStatus[0]));\n    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    if (paths.length \u003d\u003d 0) {\n      return splits;    \n    }\n\n    // Convert them to Paths first. This is a costly operation and \n    // we should do it first, otherwise we will incur doing it multiple\n    // times, one time each for each pool in the next loop.\n    List\u003cPath\u003e newpaths \u003d new LinkedList\u003cPath\u003e();\n    for (int i \u003d 0; i \u003c paths.length; i++) {\n      Path p \u003d new Path(paths[i].toUri().getPath());\n      newpaths.add(p);\n    }\n    paths \u003d null;\n\n    // In one single iteration, process all the paths in a single pool.\n    // Processing one pool at a time ensures that a split contains paths\n    // from a single pool only.\n    for (MultiPathFilter onepool : pools) {\n      ArrayList\u003cPath\u003e myPaths \u003d new ArrayList\u003cPath\u003e();\n      \n      // pick one input path. If it matches all the filters in a pool,\n      // add it to the output set\n      for (Iterator\u003cPath\u003e iter \u003d newpaths.iterator(); iter.hasNext();) {\n        Path p \u003d iter.next();\n        if (onepool.accept(p)) {\n          myPaths.add(p); // add it to my output set\n          iter.remove();\n        }\n      }\n      // create splits for all files in this pool.\n      getMoreSplits(job, myPaths.toArray(new Path[myPaths.size()]), \n                    maxSize, minSizeNode, minSizeRack, splits);\n    }\n\n    // create splits for all files that are not in any pool.\n    getMoreSplits(job, newpaths.toArray(new Path[newpaths.size()]), \n                  maxSize, minSizeNode, minSizeRack, splits);\n\n    // free up rackToNodes map\n    rackToNodes.clear();\n    return splits;    \n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java"
    }
  }
}