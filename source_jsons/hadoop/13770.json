{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "metaSave",
  "functionId": "metaSave___out-PrintWriter",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 796,
  "functionEndLine": 890,
  "numCommitsSeen": 507,
  "timeTaken": 23980,
  "changeHistory": [
    "33c62f8f4e94442825fe286c2b18518925d980e6",
    "4d2dce40bbe5242953774e6a2fc0dc9111cf5ed0",
    "4e50dc976a92a9560630c87cfc4e4513916e5735",
    "9a3c2379ef24cdca5153abf4b63fde1131ff8989",
    "a7f085d6bf499edf23e650a4f7211c53a442da0e",
    "919a1d824a0a61145dc7ae59cfba3f34d91f2681",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "c304890c8c7782d835896859f5b7f60b96c306c0",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75",
    "d62b63d297bff12d93de560dd50ddd48743b851d",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
    "31c91706f7d17da006ef2d6c541f8dd092fae077",
    "96f9fc91993b04166f30fdf2dc5145ac91dbf1df",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "ec559db68e1ad306b1ba97283fbda1074fa50eb0",
    "513f17d115564e49124bb744cecf36d16a144ffc",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "969a263188f7015261719fe45fa1505121ebb80e",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "33c62f8f4e94442825fe286c2b18518925d980e6": "Ybodychange",
    "4d2dce40bbe5242953774e6a2fc0dc9111cf5ed0": "Ybodychange",
    "4e50dc976a92a9560630c87cfc4e4513916e5735": "Ybodychange",
    "9a3c2379ef24cdca5153abf4b63fde1131ff8989": "Ybodychange",
    "a7f085d6bf499edf23e650a4f7211c53a442da0e": "Ybodychange",
    "919a1d824a0a61145dc7ae59cfba3f34d91f2681": "Ybodychange",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": "Ybodychange",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ybodychange",
    "c304890c8c7782d835896859f5b7f60b96c306c0": "Ybodychange",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": "Ybodychange",
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75": "Ybodychange",
    "d62b63d297bff12d93de560dd50ddd48743b851d": "Ybodychange",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": "Ybodychange",
    "31c91706f7d17da006ef2d6c541f8dd092fae077": "Ybodychange",
    "96f9fc91993b04166f30fdf2dc5145ac91dbf1df": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "ec559db68e1ad306b1ba97283fbda1074fa50eb0": "Ybodychange",
    "513f17d115564e49124bb744cecf36d16a144ffc": "Ybodychange",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "969a263188f7015261719fe45fa1505121ebb80e": "Ybodychange",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": "Ymultichange(Yfilerename,Ymodifierchange)",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": "Ymultichange(Yfilerename,Ymodifierchange)",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": "Ymultichange(Yfilerename,Ymodifierchange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "33c62f8f4e94442825fe286c2b18518925d980e6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14497. Write lock held by metasave impact following RPC processing. Contributed by He Xiaoqiao.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "30/05/19 1:30 PM",
      "commitName": "33c62f8f4e94442825fe286c2b18518925d980e6",
      "commitAuthor": "He Xiaoqiao",
      "commitDateOld": "03/04/19 11:00 AM",
      "commitNameOld": "be488b6070a124234c77f16193ee925d32ca9a20",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 57.1,
      "commitsBetweenForRepo": 335,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,95 +1,95 @@\n   public void metaSave(PrintWriter out) {\n-    assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n+    assert namesystem.hasReadLock(); // TODO: block manager read lock and NS write lock\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n \n     //\n     // Need to iterate over all queues from neededReplications\n     // except for the QUEUE_WITH_CORRUPT_BLOCKS)\n     //\n     synchronized (neededReconstruction) {\n       out.println(\"Metasave: Blocks waiting for reconstruction: \"\n           + neededReconstruction.getLowRedundancyBlockCount());\n       for (int i \u003d 0; i \u003c neededReconstruction.LEVEL; i++) {\n         if (i !\u003d neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS) {\n           for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.iterator(i);\n                it.hasNext();) {\n             Block block \u003d it.next();\n             dumpBlockMeta(block, out);\n           }\n         }\n       }\n       //\n       // Now prints corrupt blocks separately\n       //\n       out.println(\"Metasave: Blocks currently missing: \" +\n           neededReconstruction.getCorruptBlockSize());\n       for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.\n           iterator(neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS);\n            it.hasNext();) {\n         Block block \u003d it.next();\n         dumpBlockMeta(block, out);\n       }\n     }\n \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n     for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReconstruction\n     pendingReconstruction.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     //Dump corrupt blocks and their storageIDs\n     Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocksSet();\n     out.println(\"Corrupt Blocks:\");\n     for(Block block : corruptBlocks) {\n       Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n           corruptReplicas.getNodes(block);\n       if (corruptNodes \u003d\u003d null) {\n         LOG.warn(\"{} is corrupt but has no associated node.\",\n                  block.getBlockId());\n         continue;\n       }\n       int numNodesToFind \u003d corruptNodes.size();\n       for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n         DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n         if (corruptNodes.contains(node)) {\n           String storageId \u003d storage.getStorageID();\n           DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n           State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n           out.println(\"Block\u003d\" + block.toString()\n               + \"\\tSize\u003d\" + block.getNumBytes()\n               + \"\\tNode\u003d\" + node.getName() + \"\\tStorageID\u003d\" + storageId\n               + \"\\tStorageState\u003d\" + state\n               + \"\\tTotalReplicas\u003d\" + blocksMap.numNodes(block)\n               + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n           numNodesToFind--;\n           if (numNodesToFind \u003d\u003d 0) {\n             break;\n           }\n         }\n       }\n       if (numNodesToFind \u003e 0) {\n         String[] corruptNodesList \u003d new String[corruptNodes.size()];\n         int i \u003d 0;\n         for (DatanodeDescriptor d : corruptNodes) {\n           corruptNodesList[i] \u003d d.getHostName();\n           i++;\n         }\n         out.println(block.getBlockId() + \" corrupt on \" +\n             StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n             \"found in its block locations\");\n       }\n     }\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasReadLock(); // TODO: block manager read lock and NS write lock\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n\n    //\n    // Need to iterate over all queues from neededReplications\n    // except for the QUEUE_WITH_CORRUPT_BLOCKS)\n    //\n    synchronized (neededReconstruction) {\n      out.println(\"Metasave: Blocks waiting for reconstruction: \"\n          + neededReconstruction.getLowRedundancyBlockCount());\n      for (int i \u003d 0; i \u003c neededReconstruction.LEVEL; i++) {\n        if (i !\u003d neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS) {\n          for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.iterator(i);\n               it.hasNext();) {\n            Block block \u003d it.next();\n            dumpBlockMeta(block, out);\n          }\n        }\n      }\n      //\n      // Now prints corrupt blocks separately\n      //\n      out.println(\"Metasave: Blocks currently missing: \" +\n          neededReconstruction.getCorruptBlockSize());\n      for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.\n          iterator(neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS);\n           it.hasNext();) {\n        Block block \u003d it.next();\n        dumpBlockMeta(block, out);\n      }\n    }\n\n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReconstruction\n    pendingReconstruction.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    //Dump corrupt blocks and their storageIDs\n    Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocksSet();\n    out.println(\"Corrupt Blocks:\");\n    for(Block block : corruptBlocks) {\n      Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n          corruptReplicas.getNodes(block);\n      if (corruptNodes \u003d\u003d null) {\n        LOG.warn(\"{} is corrupt but has no associated node.\",\n                 block.getBlockId());\n        continue;\n      }\n      int numNodesToFind \u003d corruptNodes.size();\n      for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n        DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n        if (corruptNodes.contains(node)) {\n          String storageId \u003d storage.getStorageID();\n          DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n          State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n          out.println(\"Block\u003d\" + block.toString()\n              + \"\\tSize\u003d\" + block.getNumBytes()\n              + \"\\tNode\u003d\" + node.getName() + \"\\tStorageID\u003d\" + storageId\n              + \"\\tStorageState\u003d\" + state\n              + \"\\tTotalReplicas\u003d\" + blocksMap.numNodes(block)\n              + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n          numNodesToFind--;\n          if (numNodesToFind \u003d\u003d 0) {\n            break;\n          }\n        }\n      }\n      if (numNodesToFind \u003e 0) {\n        String[] corruptNodesList \u003d new String[corruptNodes.size()];\n        int i \u003d 0;\n        for (DatanodeDescriptor d : corruptNodes) {\n          corruptNodesList[i] \u003d d.getHostName();\n          i++;\n        }\n        out.println(block.getBlockId() + \" corrupt on \" +\n            StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n            \"found in its block locations\");\n      }\n    }\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4d2dce40bbe5242953774e6a2fc0dc9111cf5ed0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12771. Add genstamp and block size to metasave Corrupt blocks list. Contributed by Kuhu Shukla.\n",
      "commitDate": "03/11/17 1:16 PM",
      "commitName": "4d2dce40bbe5242953774e6a2fc0dc9111cf5ed0",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "23/10/17 3:24 PM",
      "commitNameOld": "0f2a69127aa40b7dccdf74ff63e2205a10ae8998",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 10.91,
      "commitsBetweenForRepo": 82,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,95 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n \n     //\n     // Need to iterate over all queues from neededReplications\n     // except for the QUEUE_WITH_CORRUPT_BLOCKS)\n     //\n     synchronized (neededReconstruction) {\n       out.println(\"Metasave: Blocks waiting for reconstruction: \"\n           + neededReconstruction.getLowRedundancyBlockCount());\n       for (int i \u003d 0; i \u003c neededReconstruction.LEVEL; i++) {\n         if (i !\u003d neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS) {\n           for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.iterator(i);\n                it.hasNext();) {\n             Block block \u003d it.next();\n             dumpBlockMeta(block, out);\n           }\n         }\n       }\n       //\n       // Now prints corrupt blocks separately\n       //\n       out.println(\"Metasave: Blocks currently missing: \" +\n           neededReconstruction.getCorruptBlockSize());\n       for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.\n           iterator(neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS);\n            it.hasNext();) {\n         Block block \u003d it.next();\n         dumpBlockMeta(block, out);\n       }\n     }\n \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n     for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReconstruction\n     pendingReconstruction.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     //Dump corrupt blocks and their storageIDs\n     Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocksSet();\n     out.println(\"Corrupt Blocks:\");\n     for(Block block : corruptBlocks) {\n       Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n           corruptReplicas.getNodes(block);\n       if (corruptNodes \u003d\u003d null) {\n         LOG.warn(\"{} is corrupt but has no associated node.\",\n                  block.getBlockId());\n         continue;\n       }\n       int numNodesToFind \u003d corruptNodes.size();\n       for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n         DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n         if (corruptNodes.contains(node)) {\n           String storageId \u003d storage.getStorageID();\n           DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n           State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n-          out.println(\"Block\u003d\" + block.getBlockId() + \"\\tNode\u003d\" + node.getName()\n-              + \"\\tStorageID\u003d\" + storageId + \"\\tStorageState\u003d\" + state\n-              + \"\\tTotalReplicas\u003d\" +\n-              blocksMap.numNodes(block)\n+          out.println(\"Block\u003d\" + block.toString()\n+              + \"\\tSize\u003d\" + block.getNumBytes()\n+              + \"\\tNode\u003d\" + node.getName() + \"\\tStorageID\u003d\" + storageId\n+              + \"\\tStorageState\u003d\" + state\n+              + \"\\tTotalReplicas\u003d\" + blocksMap.numNodes(block)\n               + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n           numNodesToFind--;\n           if (numNodesToFind \u003d\u003d 0) {\n             break;\n           }\n         }\n       }\n       if (numNodesToFind \u003e 0) {\n         String[] corruptNodesList \u003d new String[corruptNodes.size()];\n         int i \u003d 0;\n         for (DatanodeDescriptor d : corruptNodes) {\n           corruptNodesList[i] \u003d d.getHostName();\n           i++;\n         }\n         out.println(block.getBlockId() + \" corrupt on \" +\n             StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n             \"found in its block locations\");\n       }\n     }\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n\n    //\n    // Need to iterate over all queues from neededReplications\n    // except for the QUEUE_WITH_CORRUPT_BLOCKS)\n    //\n    synchronized (neededReconstruction) {\n      out.println(\"Metasave: Blocks waiting for reconstruction: \"\n          + neededReconstruction.getLowRedundancyBlockCount());\n      for (int i \u003d 0; i \u003c neededReconstruction.LEVEL; i++) {\n        if (i !\u003d neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS) {\n          for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.iterator(i);\n               it.hasNext();) {\n            Block block \u003d it.next();\n            dumpBlockMeta(block, out);\n          }\n        }\n      }\n      //\n      // Now prints corrupt blocks separately\n      //\n      out.println(\"Metasave: Blocks currently missing: \" +\n          neededReconstruction.getCorruptBlockSize());\n      for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.\n          iterator(neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS);\n           it.hasNext();) {\n        Block block \u003d it.next();\n        dumpBlockMeta(block, out);\n      }\n    }\n\n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReconstruction\n    pendingReconstruction.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    //Dump corrupt blocks and their storageIDs\n    Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocksSet();\n    out.println(\"Corrupt Blocks:\");\n    for(Block block : corruptBlocks) {\n      Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n          corruptReplicas.getNodes(block);\n      if (corruptNodes \u003d\u003d null) {\n        LOG.warn(\"{} is corrupt but has no associated node.\",\n                 block.getBlockId());\n        continue;\n      }\n      int numNodesToFind \u003d corruptNodes.size();\n      for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n        DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n        if (corruptNodes.contains(node)) {\n          String storageId \u003d storage.getStorageID();\n          DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n          State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n          out.println(\"Block\u003d\" + block.toString()\n              + \"\\tSize\u003d\" + block.getNumBytes()\n              + \"\\tNode\u003d\" + node.getName() + \"\\tStorageID\u003d\" + storageId\n              + \"\\tStorageState\u003d\" + state\n              + \"\\tTotalReplicas\u003d\" + blocksMap.numNodes(block)\n              + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n          numNodesToFind--;\n          if (numNodesToFind \u003d\u003d 0) {\n            break;\n          }\n        }\n      }\n      if (numNodesToFind \u003e 0) {\n        String[] corruptNodesList \u003d new String[corruptNodes.size()];\n        int i \u003d 0;\n        for (DatanodeDescriptor d : corruptNodes) {\n          corruptNodesList[i] \u003d d.getHostName();\n          i++;\n        }\n        out.println(block.getBlockId() + \" corrupt on \" +\n            StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n            \"found in its block locations\");\n      }\n    }\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4e50dc976a92a9560630c87cfc4e4513916e5735": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12218. Addendum. Rename split EC / replicated block metrics in BlockManager.\n",
      "commitDate": "07/09/17 4:57 PM",
      "commitName": "4e50dc976a92a9560630c87cfc4e4513916e5735",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "31/08/17 10:36 PM",
      "commitNameOld": "1fbb662c7092d08a540acff7e92715693412e486",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 6.76,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,94 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n \n     //\n     // Need to iterate over all queues from neededReplications\n     // except for the QUEUE_WITH_CORRUPT_BLOCKS)\n     //\n     synchronized (neededReconstruction) {\n       out.println(\"Metasave: Blocks waiting for reconstruction: \"\n           + neededReconstruction.getLowRedundancyBlockCount());\n       for (int i \u003d 0; i \u003c neededReconstruction.LEVEL; i++) {\n         if (i !\u003d neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS) {\n           for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.iterator(i);\n                it.hasNext();) {\n             Block block \u003d it.next();\n             dumpBlockMeta(block, out);\n           }\n         }\n       }\n       //\n       // Now prints corrupt blocks separately\n       //\n       out.println(\"Metasave: Blocks currently missing: \" +\n           neededReconstruction.getCorruptBlockSize());\n       for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.\n           iterator(neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS);\n            it.hasNext();) {\n         Block block \u003d it.next();\n         dumpBlockMeta(block, out);\n       }\n     }\n \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n     for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReconstruction\n     pendingReconstruction.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     //Dump corrupt blocks and their storageIDs\n-    Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocks();\n+    Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocksSet();\n     out.println(\"Corrupt Blocks:\");\n     for(Block block : corruptBlocks) {\n       Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n           corruptReplicas.getNodes(block);\n       if (corruptNodes \u003d\u003d null) {\n         LOG.warn(\"{} is corrupt but has no associated node.\",\n                  block.getBlockId());\n         continue;\n       }\n       int numNodesToFind \u003d corruptNodes.size();\n       for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n         DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n         if (corruptNodes.contains(node)) {\n           String storageId \u003d storage.getStorageID();\n           DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n           State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n           out.println(\"Block\u003d\" + block.getBlockId() + \"\\tNode\u003d\" + node.getName()\n               + \"\\tStorageID\u003d\" + storageId + \"\\tStorageState\u003d\" + state\n               + \"\\tTotalReplicas\u003d\" +\n               blocksMap.numNodes(block)\n               + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n           numNodesToFind--;\n           if (numNodesToFind \u003d\u003d 0) {\n             break;\n           }\n         }\n       }\n       if (numNodesToFind \u003e 0) {\n         String[] corruptNodesList \u003d new String[corruptNodes.size()];\n         int i \u003d 0;\n         for (DatanodeDescriptor d : corruptNodes) {\n           corruptNodesList[i] \u003d d.getHostName();\n           i++;\n         }\n         out.println(block.getBlockId() + \" corrupt on \" +\n             StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n             \"found in its block locations\");\n       }\n     }\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n\n    //\n    // Need to iterate over all queues from neededReplications\n    // except for the QUEUE_WITH_CORRUPT_BLOCKS)\n    //\n    synchronized (neededReconstruction) {\n      out.println(\"Metasave: Blocks waiting for reconstruction: \"\n          + neededReconstruction.getLowRedundancyBlockCount());\n      for (int i \u003d 0; i \u003c neededReconstruction.LEVEL; i++) {\n        if (i !\u003d neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS) {\n          for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.iterator(i);\n               it.hasNext();) {\n            Block block \u003d it.next();\n            dumpBlockMeta(block, out);\n          }\n        }\n      }\n      //\n      // Now prints corrupt blocks separately\n      //\n      out.println(\"Metasave: Blocks currently missing: \" +\n          neededReconstruction.getCorruptBlockSize());\n      for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.\n          iterator(neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS);\n           it.hasNext();) {\n        Block block \u003d it.next();\n        dumpBlockMeta(block, out);\n      }\n    }\n\n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReconstruction\n    pendingReconstruction.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    //Dump corrupt blocks and their storageIDs\n    Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocksSet();\n    out.println(\"Corrupt Blocks:\");\n    for(Block block : corruptBlocks) {\n      Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n          corruptReplicas.getNodes(block);\n      if (corruptNodes \u003d\u003d null) {\n        LOG.warn(\"{} is corrupt but has no associated node.\",\n                 block.getBlockId());\n        continue;\n      }\n      int numNodesToFind \u003d corruptNodes.size();\n      for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n        DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n        if (corruptNodes.contains(node)) {\n          String storageId \u003d storage.getStorageID();\n          DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n          State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n          out.println(\"Block\u003d\" + block.getBlockId() + \"\\tNode\u003d\" + node.getName()\n              + \"\\tStorageID\u003d\" + storageId + \"\\tStorageState\u003d\" + state\n              + \"\\tTotalReplicas\u003d\" +\n              blocksMap.numNodes(block)\n              + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n          numNodesToFind--;\n          if (numNodesToFind \u003d\u003d 0) {\n            break;\n          }\n        }\n      }\n      if (numNodesToFind \u003e 0) {\n        String[] corruptNodesList \u003d new String[corruptNodes.size()];\n        int i \u003d 0;\n        for (DatanodeDescriptor d : corruptNodes) {\n          corruptNodesList[i] \u003d d.getHostName();\n          i++;\n        }\n        out.println(block.getBlockId() + \" corrupt on \" +\n            StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n            \"found in its block locations\");\n      }\n    }\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "9a3c2379ef24cdca5153abf4b63fde1131ff8989": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12182. BlockManager.metaSave does not distinguish between \"under replicated\" and \"missing\" blocks. Contributed by Wellington Chevreuil.\n",
      "commitDate": "08/08/17 11:44 PM",
      "commitName": "9a3c2379ef24cdca5153abf4b63fde1131ff8989",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "28/07/17 11:24 AM",
      "commitNameOld": "480c8db40c09cd0e25b4d145bc871b70a45d4f50",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 11.51,
      "commitsBetweenForRepo": 131,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,94 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n+\n     //\n-    // Dump contents of neededReconstruction\n+    // Need to iterate over all queues from neededReplications\n+    // except for the QUEUE_WITH_CORRUPT_BLOCKS)\n     //\n     synchronized (neededReconstruction) {\n       out.println(\"Metasave: Blocks waiting for reconstruction: \"\n-          + neededReconstruction.size());\n-      for (Block block : neededReconstruction) {\n+          + neededReconstruction.getLowRedundancyBlockCount());\n+      for (int i \u003d 0; i \u003c neededReconstruction.LEVEL; i++) {\n+        if (i !\u003d neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS) {\n+          for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.iterator(i);\n+               it.hasNext();) {\n+            Block block \u003d it.next();\n+            dumpBlockMeta(block, out);\n+          }\n+        }\n+      }\n+      //\n+      // Now prints corrupt blocks separately\n+      //\n+      out.println(\"Metasave: Blocks currently missing: \" +\n+          neededReconstruction.getCorruptBlockSize());\n+      for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.\n+          iterator(neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS);\n+           it.hasNext();) {\n+        Block block \u003d it.next();\n         dumpBlockMeta(block, out);\n       }\n     }\n-    \n+\n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n     for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReconstruction\n     pendingReconstruction.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     //Dump corrupt blocks and their storageIDs\n     Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocks();\n     out.println(\"Corrupt Blocks:\");\n     for(Block block : corruptBlocks) {\n       Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n           corruptReplicas.getNodes(block);\n       if (corruptNodes \u003d\u003d null) {\n         LOG.warn(\"{} is corrupt but has no associated node.\",\n                  block.getBlockId());\n         continue;\n       }\n       int numNodesToFind \u003d corruptNodes.size();\n       for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n         DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n         if (corruptNodes.contains(node)) {\n           String storageId \u003d storage.getStorageID();\n           DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n           State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n           out.println(\"Block\u003d\" + block.getBlockId() + \"\\tNode\u003d\" + node.getName()\n               + \"\\tStorageID\u003d\" + storageId + \"\\tStorageState\u003d\" + state\n               + \"\\tTotalReplicas\u003d\" +\n               blocksMap.numNodes(block)\n               + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n           numNodesToFind--;\n           if (numNodesToFind \u003d\u003d 0) {\n             break;\n           }\n         }\n       }\n       if (numNodesToFind \u003e 0) {\n         String[] corruptNodesList \u003d new String[corruptNodes.size()];\n         int i \u003d 0;\n         for (DatanodeDescriptor d : corruptNodes) {\n           corruptNodesList[i] \u003d d.getHostName();\n           i++;\n         }\n         out.println(block.getBlockId() + \" corrupt on \" +\n             StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n             \"found in its block locations\");\n       }\n     }\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n\n    //\n    // Need to iterate over all queues from neededReplications\n    // except for the QUEUE_WITH_CORRUPT_BLOCKS)\n    //\n    synchronized (neededReconstruction) {\n      out.println(\"Metasave: Blocks waiting for reconstruction: \"\n          + neededReconstruction.getLowRedundancyBlockCount());\n      for (int i \u003d 0; i \u003c neededReconstruction.LEVEL; i++) {\n        if (i !\u003d neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS) {\n          for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.iterator(i);\n               it.hasNext();) {\n            Block block \u003d it.next();\n            dumpBlockMeta(block, out);\n          }\n        }\n      }\n      //\n      // Now prints corrupt blocks separately\n      //\n      out.println(\"Metasave: Blocks currently missing: \" +\n          neededReconstruction.getCorruptBlockSize());\n      for (Iterator\u003cBlockInfo\u003e it \u003d neededReconstruction.\n          iterator(neededReconstruction.QUEUE_WITH_CORRUPT_BLOCKS);\n           it.hasNext();) {\n        Block block \u003d it.next();\n        dumpBlockMeta(block, out);\n      }\n    }\n\n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReconstruction\n    pendingReconstruction.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    //Dump corrupt blocks and their storageIDs\n    Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocks();\n    out.println(\"Corrupt Blocks:\");\n    for(Block block : corruptBlocks) {\n      Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n          corruptReplicas.getNodes(block);\n      if (corruptNodes \u003d\u003d null) {\n        LOG.warn(\"{} is corrupt but has no associated node.\",\n                 block.getBlockId());\n        continue;\n      }\n      int numNodesToFind \u003d corruptNodes.size();\n      for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n        DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n        if (corruptNodes.contains(node)) {\n          String storageId \u003d storage.getStorageID();\n          DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n          State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n          out.println(\"Block\u003d\" + block.getBlockId() + \"\\tNode\u003d\" + node.getName()\n              + \"\\tStorageID\u003d\" + storageId + \"\\tStorageState\u003d\" + state\n              + \"\\tTotalReplicas\u003d\" +\n              blocksMap.numNodes(block)\n              + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n          numNodesToFind--;\n          if (numNodesToFind \u003d\u003d 0) {\n            break;\n          }\n        }\n      }\n      if (numNodesToFind \u003e 0) {\n        String[] corruptNodesList \u003d new String[corruptNodes.size()];\n        int i \u003d 0;\n        for (DatanodeDescriptor d : corruptNodes) {\n          corruptNodesList[i] \u003d d.getHostName();\n          i++;\n        }\n        out.println(block.getBlockId() + \" corrupt on \" +\n            StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n            \"found in its block locations\");\n      }\n    }\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "a7f085d6bf499edf23e650a4f7211c53a442da0e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11832. Switch leftover logs to slf4j format in BlockManager.java. Contributed by Hui Xu and Chen Liang.\n",
      "commitDate": "29/05/17 1:30 AM",
      "commitName": "a7f085d6bf499edf23e650a4f7211c53a442da0e",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "25/05/17 7:35 AM",
      "commitNameOld": "2e41f8803dd46d1bab16c1b206c71be72ea260a1",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 3.75,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,75 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n     //\n     // Dump contents of neededReconstruction\n     //\n     synchronized (neededReconstruction) {\n       out.println(\"Metasave: Blocks waiting for reconstruction: \"\n           + neededReconstruction.size());\n       for (Block block : neededReconstruction) {\n         dumpBlockMeta(block, out);\n       }\n     }\n     \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n     for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReconstruction\n     pendingReconstruction.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     //Dump corrupt blocks and their storageIDs\n     Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocks();\n     out.println(\"Corrupt Blocks:\");\n     for(Block block : corruptBlocks) {\n       Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n           corruptReplicas.getNodes(block);\n       if (corruptNodes \u003d\u003d null) {\n-        LOG.warn(block.getBlockId() +\n-            \" is corrupt but has no associated node.\");\n+        LOG.warn(\"{} is corrupt but has no associated node.\",\n+                 block.getBlockId());\n         continue;\n       }\n       int numNodesToFind \u003d corruptNodes.size();\n       for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n         DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n         if (corruptNodes.contains(node)) {\n           String storageId \u003d storage.getStorageID();\n           DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n           State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n           out.println(\"Block\u003d\" + block.getBlockId() + \"\\tNode\u003d\" + node.getName()\n               + \"\\tStorageID\u003d\" + storageId + \"\\tStorageState\u003d\" + state\n               + \"\\tTotalReplicas\u003d\" +\n               blocksMap.numNodes(block)\n               + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n           numNodesToFind--;\n           if (numNodesToFind \u003d\u003d 0) {\n             break;\n           }\n         }\n       }\n       if (numNodesToFind \u003e 0) {\n         String[] corruptNodesList \u003d new String[corruptNodes.size()];\n         int i \u003d 0;\n         for (DatanodeDescriptor d : corruptNodes) {\n           corruptNodesList[i] \u003d d.getHostName();\n           i++;\n         }\n         out.println(block.getBlockId() + \" corrupt on \" +\n             StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n             \"found in its block locations\");\n       }\n     }\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n    //\n    // Dump contents of neededReconstruction\n    //\n    synchronized (neededReconstruction) {\n      out.println(\"Metasave: Blocks waiting for reconstruction: \"\n          + neededReconstruction.size());\n      for (Block block : neededReconstruction) {\n        dumpBlockMeta(block, out);\n      }\n    }\n    \n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReconstruction\n    pendingReconstruction.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    //Dump corrupt blocks and their storageIDs\n    Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocks();\n    out.println(\"Corrupt Blocks:\");\n    for(Block block : corruptBlocks) {\n      Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n          corruptReplicas.getNodes(block);\n      if (corruptNodes \u003d\u003d null) {\n        LOG.warn(\"{} is corrupt but has no associated node.\",\n                 block.getBlockId());\n        continue;\n      }\n      int numNodesToFind \u003d corruptNodes.size();\n      for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n        DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n        if (corruptNodes.contains(node)) {\n          String storageId \u003d storage.getStorageID();\n          DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n          State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n          out.println(\"Block\u003d\" + block.getBlockId() + \"\\tNode\u003d\" + node.getName()\n              + \"\\tStorageID\u003d\" + storageId + \"\\tStorageState\u003d\" + state\n              + \"\\tTotalReplicas\u003d\" +\n              blocksMap.numNodes(block)\n              + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n          numNodesToFind--;\n          if (numNodesToFind \u003d\u003d 0) {\n            break;\n          }\n        }\n      }\n      if (numNodesToFind \u003e 0) {\n        String[] corruptNodesList \u003d new String[corruptNodes.size()];\n        int i \u003d 0;\n        for (DatanodeDescriptor d : corruptNodes) {\n          corruptNodesList[i] \u003d d.getHostName();\n          i++;\n        }\n        out.println(block.getBlockId() + \" corrupt on \" +\n            StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n            \"found in its block locations\");\n      }\n    }\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "919a1d824a0a61145dc7ae59cfba3f34d91f2681": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10330. Add Corrupt Blocks Information in Metasave output. Contributed by Kuhu Shukla.\n",
      "commitDate": "27/04/16 6:19 AM",
      "commitName": "919a1d824a0a61145dc7ae59cfba3f34d91f2681",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "25/04/16 10:01 PM",
      "commitNameOld": "5865fe2bf01284993572ea60b3ec3bf8b4492818",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 1.35,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,75 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n     //\n     // Dump contents of neededReconstruction\n     //\n     synchronized (neededReconstruction) {\n       out.println(\"Metasave: Blocks waiting for reconstruction: \"\n           + neededReconstruction.size());\n       for (Block block : neededReconstruction) {\n         dumpBlockMeta(block, out);\n       }\n     }\n     \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n     for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReconstruction\n     pendingReconstruction.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n+    //Dump corrupt blocks and their storageIDs\n+    Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocks();\n+    out.println(\"Corrupt Blocks:\");\n+    for(Block block : corruptBlocks) {\n+      Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n+          corruptReplicas.getNodes(block);\n+      if (corruptNodes \u003d\u003d null) {\n+        LOG.warn(block.getBlockId() +\n+            \" is corrupt but has no associated node.\");\n+        continue;\n+      }\n+      int numNodesToFind \u003d corruptNodes.size();\n+      for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n+        DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n+        if (corruptNodes.contains(node)) {\n+          String storageId \u003d storage.getStorageID();\n+          DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n+          State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n+          out.println(\"Block\u003d\" + block.getBlockId() + \"\\tNode\u003d\" + node.getName()\n+              + \"\\tStorageID\u003d\" + storageId + \"\\tStorageState\u003d\" + state\n+              + \"\\tTotalReplicas\u003d\" +\n+              blocksMap.numNodes(block)\n+              + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n+          numNodesToFind--;\n+          if (numNodesToFind \u003d\u003d 0) {\n+            break;\n+          }\n+        }\n+      }\n+      if (numNodesToFind \u003e 0) {\n+        String[] corruptNodesList \u003d new String[corruptNodes.size()];\n+        int i \u003d 0;\n+        for (DatanodeDescriptor d : corruptNodes) {\n+          corruptNodesList[i] \u003d d.getHostName();\n+          i++;\n+        }\n+        out.println(block.getBlockId() + \" corrupt on \" +\n+            StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n+            \"found in its block locations\");\n+      }\n+    }\n+\n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n    //\n    // Dump contents of neededReconstruction\n    //\n    synchronized (neededReconstruction) {\n      out.println(\"Metasave: Blocks waiting for reconstruction: \"\n          + neededReconstruction.size());\n      for (Block block : neededReconstruction) {\n        dumpBlockMeta(block, out);\n      }\n    }\n    \n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReconstruction\n    pendingReconstruction.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    //Dump corrupt blocks and their storageIDs\n    Set\u003cBlock\u003e corruptBlocks \u003d corruptReplicas.getCorruptBlocks();\n    out.println(\"Corrupt Blocks:\");\n    for(Block block : corruptBlocks) {\n      Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n          corruptReplicas.getNodes(block);\n      if (corruptNodes \u003d\u003d null) {\n        LOG.warn(block.getBlockId() +\n            \" is corrupt but has no associated node.\");\n        continue;\n      }\n      int numNodesToFind \u003d corruptNodes.size();\n      for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n        DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n        if (corruptNodes.contains(node)) {\n          String storageId \u003d storage.getStorageID();\n          DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storageId);\n          State state \u003d (storageInfo \u003d\u003d null) ? null : storageInfo.getState();\n          out.println(\"Block\u003d\" + block.getBlockId() + \"\\tNode\u003d\" + node.getName()\n              + \"\\tStorageID\u003d\" + storageId + \"\\tStorageState\u003d\" + state\n              + \"\\tTotalReplicas\u003d\" +\n              blocksMap.numNodes(block)\n              + \"\\tReason\u003d\" + corruptReplicas.getCorruptReason(block, node));\n          numNodesToFind--;\n          if (numNodesToFind \u003d\u003d 0) {\n            break;\n          }\n        }\n      }\n      if (numNodesToFind \u003e 0) {\n        String[] corruptNodesList \u003d new String[corruptNodes.size()];\n        int i \u003d 0;\n        for (DatanodeDescriptor d : corruptNodes) {\n          corruptNodesList[i] \u003d d.getHostName();\n          i++;\n        }\n        out.println(block.getBlockId() + \" corrupt on \" +\n            StringUtils.join(\",\", corruptNodesList) + \" but not all nodes are\" +\n            \"found in its block locations\");\n      }\n    }\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9869. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-2]. Contributed by Rakesh R.\n",
      "commitDate": "25/04/16 10:01 PM",
      "commitName": "5865fe2bf01284993572ea60b3ec3bf8b4492818",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "17/04/16 6:28 PM",
      "commitNameOld": "67523ffcf491f4f2db5335899c00a174d0caaa9b",
      "commitAuthorOld": "Walter Su",
      "daysBetweenCommits": 8.15,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n     //\n     // Dump contents of neededReconstruction\n     //\n     synchronized (neededReconstruction) {\n       out.println(\"Metasave: Blocks waiting for reconstruction: \"\n           + neededReconstruction.size());\n       for (Block block : neededReconstruction) {\n         dumpBlockMeta(block, out);\n       }\n     }\n     \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n     for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n-    // Dump blocks from pendingReplication\n-    pendingReplications.metaSave(out);\n+    // Dump blocks from pendingReconstruction\n+    pendingReconstruction.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n    //\n    // Dump contents of neededReconstruction\n    //\n    synchronized (neededReconstruction) {\n      out.println(\"Metasave: Blocks waiting for reconstruction: \"\n          + neededReconstruction.size());\n      for (Block block : neededReconstruction) {\n        dumpBlockMeta(block, out);\n      }\n    }\n    \n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReconstruction\n    pendingReconstruction.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n     //\n-    // Dump contents of neededReplication\n+    // Dump contents of neededReconstruction\n     //\n-    synchronized (neededReplications) {\n-      out.println(\"Metasave: Blocks waiting for replication: \" + \n-                  neededReplications.size());\n-      for (Block block : neededReplications) {\n+    synchronized (neededReconstruction) {\n+      out.println(\"Metasave: Blocks waiting for reconstruction: \"\n+          + neededReconstruction.size());\n+      for (Block block : neededReconstruction) {\n         dumpBlockMeta(block, out);\n       }\n     }\n     \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n     for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReplication\n     pendingReplications.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n    //\n    // Dump contents of neededReconstruction\n    //\n    synchronized (neededReconstruction) {\n      out.println(\"Metasave: Blocks waiting for reconstruction: \"\n          + neededReconstruction.size());\n      for (Block block : neededReconstruction) {\n        dumpBlockMeta(block, out);\n      }\n    }\n    \n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "c304890c8c7782d835896859f5b7f60b96c306c0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9542. Move BlockIdManager from FSNamesystem to BlockManager. Contributed by Jing Zhao.\n",
      "commitDate": "21/01/16 11:13 AM",
      "commitName": "c304890c8c7782d835896859f5b7f60b96c306c0",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "06/01/16 9:57 PM",
      "commitNameOld": "34cd7cd76505d01ec251e30837c94ab03319a0c1",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 14.55,
      "commitsBetweenForRepo": 109,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public void metaSave(PrintWriter out) {\n-    assert namesystem.hasWriteLock();\n+    assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         dumpBlockMeta(block, out);\n       }\n     }\n     \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n     for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReplication\n     pendingReplications.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock(); // TODO: block manager read lock and NS write lock\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        dumpBlockMeta(block, out);\n      }\n    }\n    \n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\"\n\nThis reverts commit de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "31/07/15 4:15 PM",
      "commitNameOld": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 5.75,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock();\n-    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003c\u003e();\n-    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003c\u003e();\n+    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         dumpBlockMeta(block, out);\n       }\n     }\n     \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n     for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReplication\n     pendingReplications.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        dumpBlockMeta(block, out);\n      }\n    }\n    \n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\"\n\nThis reverts commit d62b63d297bff12d93de560dd50ddd48743b851d.\n",
      "commitDate": "07/07/15 10:13 AM",
      "commitName": "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "06/07/15 3:54 PM",
      "commitNameOld": "d62b63d297bff12d93de560dd50ddd48743b851d",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.76,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003c\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003c\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n-      for (BlockInfo block : neededReplications) {\n+      for (Block block : neededReplications) {\n         dumpBlockMeta(block, out);\n       }\n     }\n     \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n-    for (BlockInfo block : postponedMisreplicatedBlocks) {\n+    for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReplication\n     pendingReplications.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003c\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003c\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        dumpBlockMeta(block, out);\n      }\n    }\n    \n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "d62b63d297bff12d93de560dd50ddd48743b851d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\n",
      "commitDate": "06/07/15 3:54 PM",
      "commitName": "d62b63d297bff12d93de560dd50ddd48743b851d",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "29/06/15 11:00 AM",
      "commitNameOld": "d3fed8e653ed9e18d3a29a11c4b24a628ac770bb",
      "commitAuthorOld": "Benoy Antony",
      "daysBetweenCommits": 7.2,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003c\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003c\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n-      for (Block block : neededReplications) {\n+      for (BlockInfo block : neededReplications) {\n         dumpBlockMeta(block, out);\n       }\n     }\n     \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n-    for (Block block : postponedMisreplicatedBlocks) {\n+    for (BlockInfo block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReplication\n     pendingReplications.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003c\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003c\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (BlockInfo block : neededReplications) {\n        dumpBlockMeta(block, out);\n      }\n    }\n    \n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (BlockInfo block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\n",
      "commitDate": "26/06/15 10:49 AM",
      "commitName": "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/06/15 2:42 PM",
      "commitNameOld": "afe9ea3c12e1f5a71922400eadb642960bc87ca1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock();\n-    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n-    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003c\u003e();\n+    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003c\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         dumpBlockMeta(block, out);\n       }\n     }\n     \n     // Dump any postponed over-replicated blocks\n     out.println(\"Mis-replicated blocks that have been postponed:\");\n     for (Block block : postponedMisreplicatedBlocks) {\n       dumpBlockMeta(block, out);\n     }\n \n     // Dump blocks from pendingReplication\n     pendingReplications.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003c\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003c\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        dumpBlockMeta(block, out);\n      }\n    }\n    \n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "31c91706f7d17da006ef2d6c541f8dd092fae077": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1972. Fencing mechanism for block invalidations and replications. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1221608 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/11 8:32 PM",
      "commitName": "31c91706f7d17da006ef2d6c541f8dd092fae077",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "20/12/11 7:03 PM",
      "commitNameOld": "36d1c49486587c2dbb193e8538b1d4510c462fa6",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,33 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     datanodeManager.fetchDatanodes(live, dead, false);\n     out.println(\"Live Datanodes: \" + live.size());\n     out.println(\"Dead Datanodes: \" + dead.size());\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n-        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n-                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n-        List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n-          new ArrayList\u003cDatanodeDescriptor\u003e();\n-        \n-        NumberReplicas numReplicas \u003d new NumberReplicas();\n-        // source node returned is not used\n-        chooseSourceDatanode(block, containingNodes,\n-            containingLiveReplicasNodes, numReplicas);\n-        assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n-        int usableReplicas \u003d numReplicas.liveReplicas() +\n-                             numReplicas.decommissionedReplicas();\n-       \n-        if (block instanceof BlockInfo) {\n-          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n-          out.print(fileName + \": \");\n-        }\n-        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n-        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n-                  \" (replicas:\" +\n-                  \" l: \" + numReplicas.liveReplicas() +\n-                  \" d: \" + numReplicas.decommissionedReplicas() +\n-                  \" c: \" + numReplicas.corruptReplicas() +\n-                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n-\n-        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n-                                      corruptReplicas.getNodes(block);\n-        \n-        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n-             jt.hasNext();) {\n-          DatanodeDescriptor node \u003d jt.next();\n-          String state \u003d \"\";\n-          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n-            state \u003d \"(corrupt)\";\n-          } else if (node.isDecommissioned() || \n-              node.isDecommissionInProgress()) {\n-            state \u003d \"(decommissioned)\";\n-          }          \n-          out.print(\" \" + node + state + \" : \");\n-        }\n-        out.println(\"\");\n+        dumpBlockMeta(block, out);\n       }\n     }\n+    \n+    // Dump any postponed over-replicated blocks\n+    out.println(\"Mis-replicated blocks that have been postponed:\");\n+    for (Block block : postponedMisreplicatedBlocks) {\n+      dumpBlockMeta(block, out);\n+    }\n \n     // Dump blocks from pendingReplication\n     pendingReplications.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        dumpBlockMeta(block, out);\n      }\n    }\n    \n    // Dump any postponed over-replicated blocks\n    out.println(\"Mis-replicated blocks that have been postponed:\");\n    for (Block block : postponedMisreplicatedBlocks) {\n      dumpBlockMeta(block, out);\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "96f9fc91993b04166f30fdf2dc5145ac91dbf1df": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2363. Move datanodes size printing from FSNamesystem.metasave(..) to BlockManager.  Contributed by Uma Maheswara Rao G\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1176733 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/11 10:49 PM",
      "commitName": "96f9fc91993b04166f30fdf2dc5145ac91dbf1df",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "24/09/11 8:04 AM",
      "commitNameOld": "5a6f8e38c044c376ac11b5e4e97a7b06f78d4c80",
      "commitAuthorOld": "Konstantin Shvachko",
      "daysBetweenCommits": 3.61,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,67 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock();\n+    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+    datanodeManager.fetchDatanodes(live, dead, false);\n+    out.println(\"Live Datanodes: \" + live.size());\n+    out.println(\"Dead Datanodes: \" + dead.size());\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                           new ArrayList\u003cDatanodeDescriptor\u003e();\n         List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n           new ArrayList\u003cDatanodeDescriptor\u003e();\n         \n         NumberReplicas numReplicas \u003d new NumberReplicas();\n         // source node returned is not used\n         chooseSourceDatanode(block, containingNodes,\n             containingLiveReplicasNodes, numReplicas);\n         assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n         int usableReplicas \u003d numReplicas.liveReplicas() +\n                              numReplicas.decommissionedReplicas();\n        \n         if (block instanceof BlockInfo) {\n           String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n           out.print(fileName + \": \");\n         }\n         // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n         out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                   \" (replicas:\" +\n                   \" l: \" + numReplicas.liveReplicas() +\n                   \" d: \" + numReplicas.decommissionedReplicas() +\n                   \" c: \" + numReplicas.corruptReplicas() +\n                   \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n         Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                       corruptReplicas.getNodes(block);\n         \n         for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n              jt.hasNext();) {\n           DatanodeDescriptor node \u003d jt.next();\n           String state \u003d \"\";\n           if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n             state \u003d \"(corrupt)\";\n           } else if (node.isDecommissioned() || \n               node.isDecommissionInProgress()) {\n             state \u003d \"(decommissioned)\";\n           }          \n           out.print(\" \" + node + state + \" : \");\n         }\n         out.println(\"\");\n       }\n     }\n \n     // Dump blocks from pendingReplication\n     pendingReplications.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    datanodeManager.fetchDatanodes(live, dead, false);\n    out.println(\"Live Datanodes: \" + live.size());\n    out.println(\"Dead Datanodes: \" + dead.size());\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n          new ArrayList\u003cDatanodeDescriptor\u003e();\n        \n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes,\n            containingLiveReplicasNodes, numReplicas);\n        assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock();\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n          new ArrayList\u003cDatanodeDescriptor\u003e();\n        \n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes,\n            containingLiveReplicasNodes, numReplicas);\n        assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "ec559db68e1ad306b1ba97283fbda1074fa50eb0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1480. All replicas of a block can end up on the same rack when some datanodes are decommissioning. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1160897 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/08/11 3:04 PM",
      "commitName": "ec559db68e1ad306b1ba97283fbda1074fa50eb0",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "22/08/11 4:14 PM",
      "commitNameOld": "2892f6d817d74e90ff50073cd3721ed4ec75ba92",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.95,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,62 @@\n   public void metaSave(PrintWriter out) {\n     assert namesystem.hasWriteLock();\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                           new ArrayList\u003cDatanodeDescriptor\u003e();\n+        List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n+          new ArrayList\u003cDatanodeDescriptor\u003e();\n+        \n         NumberReplicas numReplicas \u003d new NumberReplicas();\n         // source node returned is not used\n-        chooseSourceDatanode(block, containingNodes, numReplicas);\n+        chooseSourceDatanode(block, containingNodes,\n+            containingLiveReplicasNodes, numReplicas);\n+        assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n         int usableReplicas \u003d numReplicas.liveReplicas() +\n                              numReplicas.decommissionedReplicas();\n        \n         if (block instanceof BlockInfo) {\n           String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n           out.print(fileName + \": \");\n         }\n         // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n         out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                   \" (replicas:\" +\n                   \" l: \" + numReplicas.liveReplicas() +\n                   \" d: \" + numReplicas.decommissionedReplicas() +\n                   \" c: \" + numReplicas.corruptReplicas() +\n                   \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n         Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                       corruptReplicas.getNodes(block);\n         \n         for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n              jt.hasNext();) {\n           DatanodeDescriptor node \u003d jt.next();\n           String state \u003d \"\";\n           if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n             state \u003d \"(corrupt)\";\n           } else if (node.isDecommissioned() || \n               node.isDecommissionInProgress()) {\n             state \u003d \"(decommissioned)\";\n           }          \n           out.print(\" \" + node + state + \" : \");\n         }\n         out.println(\"\");\n       }\n     }\n \n     // Dump blocks from pendingReplication\n     pendingReplications.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n     invalidateBlocks.dump(out);\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock();\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n          new ArrayList\u003cDatanodeDescriptor\u003e();\n        \n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes,\n            containingLiveReplicasNodes, numReplicas);\n        assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "513f17d115564e49124bb744cecf36d16a144ffc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2273.  Refactor BlockManager.recentInvalidateSets to a new class.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1160475 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/08/11 3:28 PM",
      "commitName": "513f17d115564e49124bb744cecf36d16a144ffc",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "19/08/11 10:36 AM",
      "commitNameOld": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 3.2,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,57 @@\n   public void metaSave(PrintWriter out) {\n+    assert namesystem.hasWriteLock();\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                           new ArrayList\u003cDatanodeDescriptor\u003e();\n         NumberReplicas numReplicas \u003d new NumberReplicas();\n         // source node returned is not used\n         chooseSourceDatanode(block, containingNodes, numReplicas);\n         int usableReplicas \u003d numReplicas.liveReplicas() +\n                              numReplicas.decommissionedReplicas();\n        \n         if (block instanceof BlockInfo) {\n           String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n           out.print(fileName + \": \");\n         }\n         // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n         out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                   \" (replicas:\" +\n                   \" l: \" + numReplicas.liveReplicas() +\n                   \" d: \" + numReplicas.decommissionedReplicas() +\n                   \" c: \" + numReplicas.corruptReplicas() +\n                   \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n         Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                       corruptReplicas.getNodes(block);\n         \n         for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n              jt.hasNext();) {\n           DatanodeDescriptor node \u003d jt.next();\n           String state \u003d \"\";\n           if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n             state \u003d \"(corrupt)\";\n           } else if (node.isDecommissioned() || \n               node.isDecommissionInProgress()) {\n             state \u003d \"(decommissioned)\";\n           }          \n           out.print(\" \" + node + state + \" : \");\n         }\n         out.println(\"\");\n       }\n     }\n \n     // Dump blocks from pendingReplication\n     pendingReplications.metaSave(out);\n \n     // Dump blocks that are waiting to be deleted\n-    dumpRecentInvalidateSets(out);\n+    invalidateBlocks.dump(out);\n \n     // Dump all datanodes\n     getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    assert namesystem.hasWriteLock();\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes, numReplicas);\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    invalidateBlocks.dump(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes, numReplicas);\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    dumpRecentInvalidateSets(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "969a263188f7015261719fe45fa1505121ebb80e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2191.  Move datanodeMap from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151339 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/11 10:46 PM",
      "commitName": "969a263188f7015261719fe45fa1505121ebb80e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "22/07/11 6:01 PM",
      "commitNameOld": "89537b7710b23db7abcd2a77f03818c06a5f5fa7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.2,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,56 @@\n   public void metaSave(PrintWriter out) {\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                           new ArrayList\u003cDatanodeDescriptor\u003e();\n         NumberReplicas numReplicas \u003d new NumberReplicas();\n         // source node returned is not used\n         chooseSourceDatanode(block, containingNodes, numReplicas);\n         int usableReplicas \u003d numReplicas.liveReplicas() +\n                              numReplicas.decommissionedReplicas();\n        \n         if (block instanceof BlockInfo) {\n           String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n           out.print(fileName + \": \");\n         }\n         // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n         out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                   \" (replicas:\" +\n                   \" l: \" + numReplicas.liveReplicas() +\n                   \" d: \" + numReplicas.decommissionedReplicas() +\n                   \" c: \" + numReplicas.corruptReplicas() +\n                   \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n         Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                       corruptReplicas.getNodes(block);\n         \n         for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n              jt.hasNext();) {\n           DatanodeDescriptor node \u003d jt.next();\n           String state \u003d \"\";\n           if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n             state \u003d \"(corrupt)\";\n           } else if (node.isDecommissioned() || \n               node.isDecommissionInProgress()) {\n             state \u003d \"(decommissioned)\";\n           }          \n           out.print(\" \" + node + state + \" : \");\n         }\n         out.println(\"\");\n       }\n     }\n \n-    //\n     // Dump blocks from pendingReplication\n-    //\n     pendingReplications.metaSave(out);\n \n-    //\n     // Dump blocks that are waiting to be deleted\n-    //\n     dumpRecentInvalidateSets(out);\n+\n+    // Dump all datanodes\n+    getDatanodeManager().datanodeDump(out);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void metaSave(PrintWriter out) {\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes, numReplicas);\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    // Dump blocks from pendingReplication\n    pendingReplications.metaSave(out);\n\n    // Dump blocks that are waiting to be deleted\n    dumpRecentInvalidateSets(out);\n\n    // Dump all datanodes\n    getDatanodeManager().datanodeDump(out);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 6:31 PM",
      "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 6:31 PM",
          "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 5:26 PM",
          "commitNameOld": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,57 @@\n-  void metaSave(PrintWriter out) {\n+  public void metaSave(PrintWriter out) {\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                           new ArrayList\u003cDatanodeDescriptor\u003e();\n         NumberReplicas numReplicas \u003d new NumberReplicas();\n         // source node returned is not used\n         chooseSourceDatanode(block, containingNodes, numReplicas);\n         int usableReplicas \u003d numReplicas.liveReplicas() +\n                              numReplicas.decommissionedReplicas();\n        \n         if (block instanceof BlockInfo) {\n           String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n           out.print(fileName + \": \");\n         }\n         // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n         out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                   \" (replicas:\" +\n                   \" l: \" + numReplicas.liveReplicas() +\n                   \" d: \" + numReplicas.decommissionedReplicas() +\n                   \" c: \" + numReplicas.corruptReplicas() +\n                   \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n         Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                       corruptReplicas.getNodes(block);\n         \n         for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n              jt.hasNext();) {\n           DatanodeDescriptor node \u003d jt.next();\n           String state \u003d \"\";\n           if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n             state \u003d \"(corrupt)\";\n           } else if (node.isDecommissioned() || \n               node.isDecommissionInProgress()) {\n             state \u003d \"(decommissioned)\";\n           }          \n           out.print(\" \" + node + state + \" : \");\n         }\n         out.println(\"\");\n       }\n     }\n \n     //\n     // Dump blocks from pendingReplication\n     //\n     pendingReplications.metaSave(out);\n \n     //\n     // Dump blocks that are waiting to be deleted\n     //\n     dumpRecentInvalidateSets(out);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void metaSave(PrintWriter out) {\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes, numReplicas);\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    //\n    // Dump blocks from pendingReplication\n    //\n    pendingReplications.metaSave(out);\n\n    //\n    // Dump blocks that are waiting to be deleted\n    //\n    dumpRecentInvalidateSets(out);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 6:31 PM",
          "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 5:26 PM",
          "commitNameOld": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,57 @@\n-  void metaSave(PrintWriter out) {\n+  public void metaSave(PrintWriter out) {\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                           new ArrayList\u003cDatanodeDescriptor\u003e();\n         NumberReplicas numReplicas \u003d new NumberReplicas();\n         // source node returned is not used\n         chooseSourceDatanode(block, containingNodes, numReplicas);\n         int usableReplicas \u003d numReplicas.liveReplicas() +\n                              numReplicas.decommissionedReplicas();\n        \n         if (block instanceof BlockInfo) {\n           String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n           out.print(fileName + \": \");\n         }\n         // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n         out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                   \" (replicas:\" +\n                   \" l: \" + numReplicas.liveReplicas() +\n                   \" d: \" + numReplicas.decommissionedReplicas() +\n                   \" c: \" + numReplicas.corruptReplicas() +\n                   \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n         Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                       corruptReplicas.getNodes(block);\n         \n         for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n              jt.hasNext();) {\n           DatanodeDescriptor node \u003d jt.next();\n           String state \u003d \"\";\n           if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n             state \u003d \"(corrupt)\";\n           } else if (node.isDecommissioned() || \n               node.isDecommissionInProgress()) {\n             state \u003d \"(decommissioned)\";\n           }          \n           out.print(\" \" + node + state + \" : \");\n         }\n         out.println(\"\");\n       }\n     }\n \n     //\n     // Dump blocks from pendingReplication\n     //\n     pendingReplications.metaSave(out);\n \n     //\n     // Dump blocks that are waiting to be deleted\n     //\n     dumpRecentInvalidateSets(out);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void metaSave(PrintWriter out) {\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes, numReplicas);\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    //\n    // Dump blocks from pendingReplication\n    //\n    pendingReplications.metaSave(out);\n\n    //\n    // Dump blocks that are waiting to be deleted\n    //\n    dumpRecentInvalidateSets(out);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[public]"
          }
        }
      ]
    },
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 5:26 PM",
      "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 5:26 PM",
          "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 4:57 PM",
          "commitNameOld": "d58e3efe9269efe00c309ed0e9726d2f94bcd03a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,57 @@\n-  public void metaSave(PrintWriter out) {\n+  void metaSave(PrintWriter out) {\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                           new ArrayList\u003cDatanodeDescriptor\u003e();\n         NumberReplicas numReplicas \u003d new NumberReplicas();\n         // source node returned is not used\n         chooseSourceDatanode(block, containingNodes, numReplicas);\n         int usableReplicas \u003d numReplicas.liveReplicas() +\n                              numReplicas.decommissionedReplicas();\n        \n         if (block instanceof BlockInfo) {\n           String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n           out.print(fileName + \": \");\n         }\n         // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n         out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                   \" (replicas:\" +\n                   \" l: \" + numReplicas.liveReplicas() +\n                   \" d: \" + numReplicas.decommissionedReplicas() +\n                   \" c: \" + numReplicas.corruptReplicas() +\n                   \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n         Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                       corruptReplicas.getNodes(block);\n         \n         for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n              jt.hasNext();) {\n           DatanodeDescriptor node \u003d jt.next();\n           String state \u003d \"\";\n           if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n             state \u003d \"(corrupt)\";\n           } else if (node.isDecommissioned() || \n               node.isDecommissionInProgress()) {\n             state \u003d \"(decommissioned)\";\n           }          \n           out.print(\" \" + node + state + \" : \");\n         }\n         out.println(\"\");\n       }\n     }\n \n     //\n     // Dump blocks from pendingReplication\n     //\n     pendingReplications.metaSave(out);\n \n     //\n     // Dump blocks that are waiting to be deleted\n     //\n     dumpRecentInvalidateSets(out);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void metaSave(PrintWriter out) {\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes, numReplicas);\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    //\n    // Dump blocks from pendingReplication\n    //\n    pendingReplications.metaSave(out);\n\n    //\n    // Dump blocks that are waiting to be deleted\n    //\n    dumpRecentInvalidateSets(out);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 5:26 PM",
          "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 4:57 PM",
          "commitNameOld": "d58e3efe9269efe00c309ed0e9726d2f94bcd03a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,57 @@\n-  public void metaSave(PrintWriter out) {\n+  void metaSave(PrintWriter out) {\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                           new ArrayList\u003cDatanodeDescriptor\u003e();\n         NumberReplicas numReplicas \u003d new NumberReplicas();\n         // source node returned is not used\n         chooseSourceDatanode(block, containingNodes, numReplicas);\n         int usableReplicas \u003d numReplicas.liveReplicas() +\n                              numReplicas.decommissionedReplicas();\n        \n         if (block instanceof BlockInfo) {\n           String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n           out.print(fileName + \": \");\n         }\n         // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n         out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                   \" (replicas:\" +\n                   \" l: \" + numReplicas.liveReplicas() +\n                   \" d: \" + numReplicas.decommissionedReplicas() +\n                   \" c: \" + numReplicas.corruptReplicas() +\n                   \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n         Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                       corruptReplicas.getNodes(block);\n         \n         for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n              jt.hasNext();) {\n           DatanodeDescriptor node \u003d jt.next();\n           String state \u003d \"\";\n           if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n             state \u003d \"(corrupt)\";\n           } else if (node.isDecommissioned() || \n               node.isDecommissionInProgress()) {\n             state \u003d \"(decommissioned)\";\n           }          \n           out.print(\" \" + node + state + \" : \");\n         }\n         out.println(\"\");\n       }\n     }\n \n     //\n     // Dump blocks from pendingReplication\n     //\n     pendingReplications.metaSave(out);\n \n     //\n     // Dump blocks that are waiting to be deleted\n     //\n     dumpRecentInvalidateSets(out);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void metaSave(PrintWriter out) {\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes, numReplicas);\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    //\n    // Dump blocks from pendingReplication\n    //\n    pendingReplications.metaSave(out);\n\n    //\n    // Dump blocks that are waiting to be deleted\n    //\n    dumpRecentInvalidateSets(out);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        }
      ]
    },
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 4:43 PM",
      "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 4:43 PM",
          "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 9:21 AM",
          "commitNameOld": "1834fb99f516b2f2cd5e0ab1f89d407f98a7237a",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.31,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,57 @@\n-  void metaSave(PrintWriter out) {\n+  public void metaSave(PrintWriter out) {\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                           new ArrayList\u003cDatanodeDescriptor\u003e();\n         NumberReplicas numReplicas \u003d new NumberReplicas();\n         // source node returned is not used\n         chooseSourceDatanode(block, containingNodes, numReplicas);\n         int usableReplicas \u003d numReplicas.liveReplicas() +\n                              numReplicas.decommissionedReplicas();\n        \n         if (block instanceof BlockInfo) {\n           String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n           out.print(fileName + \": \");\n         }\n         // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n         out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                   \" (replicas:\" +\n                   \" l: \" + numReplicas.liveReplicas() +\n                   \" d: \" + numReplicas.decommissionedReplicas() +\n                   \" c: \" + numReplicas.corruptReplicas() +\n                   \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n         Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                       corruptReplicas.getNodes(block);\n         \n         for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n              jt.hasNext();) {\n           DatanodeDescriptor node \u003d jt.next();\n           String state \u003d \"\";\n           if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n             state \u003d \"(corrupt)\";\n           } else if (node.isDecommissioned() || \n               node.isDecommissionInProgress()) {\n             state \u003d \"(decommissioned)\";\n           }          \n           out.print(\" \" + node + state + \" : \");\n         }\n         out.println(\"\");\n       }\n     }\n \n     //\n     // Dump blocks from pendingReplication\n     //\n     pendingReplications.metaSave(out);\n \n     //\n     // Dump blocks that are waiting to be deleted\n     //\n     dumpRecentInvalidateSets(out);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void metaSave(PrintWriter out) {\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes, numReplicas);\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    //\n    // Dump blocks from pendingReplication\n    //\n    pendingReplications.metaSave(out);\n\n    //\n    // Dump blocks that are waiting to be deleted\n    //\n    dumpRecentInvalidateSets(out);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 4:43 PM",
          "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 9:21 AM",
          "commitNameOld": "1834fb99f516b2f2cd5e0ab1f89d407f98a7237a",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.31,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,57 @@\n-  void metaSave(PrintWriter out) {\n+  public void metaSave(PrintWriter out) {\n     //\n     // Dump contents of neededReplication\n     //\n     synchronized (neededReplications) {\n       out.println(\"Metasave: Blocks waiting for replication: \" + \n                   neededReplications.size());\n       for (Block block : neededReplications) {\n         List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                           new ArrayList\u003cDatanodeDescriptor\u003e();\n         NumberReplicas numReplicas \u003d new NumberReplicas();\n         // source node returned is not used\n         chooseSourceDatanode(block, containingNodes, numReplicas);\n         int usableReplicas \u003d numReplicas.liveReplicas() +\n                              numReplicas.decommissionedReplicas();\n        \n         if (block instanceof BlockInfo) {\n           String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n           out.print(fileName + \": \");\n         }\n         // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n         out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                   \" (replicas:\" +\n                   \" l: \" + numReplicas.liveReplicas() +\n                   \" d: \" + numReplicas.decommissionedReplicas() +\n                   \" c: \" + numReplicas.corruptReplicas() +\n                   \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n         Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                       corruptReplicas.getNodes(block);\n         \n         for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n              jt.hasNext();) {\n           DatanodeDescriptor node \u003d jt.next();\n           String state \u003d \"\";\n           if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n             state \u003d \"(corrupt)\";\n           } else if (node.isDecommissioned() || \n               node.isDecommissionInProgress()) {\n             state \u003d \"(decommissioned)\";\n           }          \n           out.print(\" \" + node + state + \" : \");\n         }\n         out.println(\"\");\n       }\n     }\n \n     //\n     // Dump blocks from pendingReplication\n     //\n     pendingReplications.metaSave(out);\n \n     //\n     // Dump blocks that are waiting to be deleted\n     //\n     dumpRecentInvalidateSets(out);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void metaSave(PrintWriter out) {\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes, numReplicas);\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    //\n    // Dump blocks from pendingReplication\n    //\n    pendingReplications.metaSave(out);\n\n    //\n    // Dump blocks that are waiting to be deleted\n    //\n    dumpRecentInvalidateSets(out);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[public]"
          }
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,57 @@\n+  void metaSave(PrintWriter out) {\n+    //\n+    // Dump contents of neededReplication\n+    //\n+    synchronized (neededReplications) {\n+      out.println(\"Metasave: Blocks waiting for replication: \" + \n+                  neededReplications.size());\n+      for (Block block : neededReplications) {\n+        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n+                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n+        NumberReplicas numReplicas \u003d new NumberReplicas();\n+        // source node returned is not used\n+        chooseSourceDatanode(block, containingNodes, numReplicas);\n+        int usableReplicas \u003d numReplicas.liveReplicas() +\n+                             numReplicas.decommissionedReplicas();\n+       \n+        if (block instanceof BlockInfo) {\n+          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n+          out.print(fileName + \": \");\n+        }\n+        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n+        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n+                  \" (replicas:\" +\n+                  \" l: \" + numReplicas.liveReplicas() +\n+                  \" d: \" + numReplicas.decommissionedReplicas() +\n+                  \" c: \" + numReplicas.corruptReplicas() +\n+                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n+\n+        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n+                                      corruptReplicas.getNodes(block);\n+        \n+        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n+             jt.hasNext();) {\n+          DatanodeDescriptor node \u003d jt.next();\n+          String state \u003d \"\";\n+          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n+            state \u003d \"(corrupt)\";\n+          } else if (node.isDecommissioned() || \n+              node.isDecommissionInProgress()) {\n+            state \u003d \"(decommissioned)\";\n+          }          \n+          out.print(\" \" + node + state + \" : \");\n+        }\n+        out.println(\"\");\n+      }\n+    }\n+\n+    //\n+    // Dump blocks from pendingReplication\n+    //\n+    pendingReplications.metaSave(out);\n+\n+    //\n+    // Dump blocks that are waiting to be deleted\n+    //\n+    dumpRecentInvalidateSets(out);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(PrintWriter out) {\n    //\n    // Dump contents of neededReplication\n    //\n    synchronized (neededReplications) {\n      out.println(\"Metasave: Blocks waiting for replication: \" + \n                  neededReplications.size());\n      for (Block block : neededReplications) {\n        List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                          new ArrayList\u003cDatanodeDescriptor\u003e();\n        NumberReplicas numReplicas \u003d new NumberReplicas();\n        // source node returned is not used\n        chooseSourceDatanode(block, containingNodes, numReplicas);\n        int usableReplicas \u003d numReplicas.liveReplicas() +\n                             numReplicas.decommissionedReplicas();\n       \n        if (block instanceof BlockInfo) {\n          String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n          out.print(fileName + \": \");\n        }\n        // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n        out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n                  \" (replicas:\" +\n                  \" l: \" + numReplicas.liveReplicas() +\n                  \" d: \" + numReplicas.decommissionedReplicas() +\n                  \" c: \" + numReplicas.corruptReplicas() +\n                  \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n        Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                      corruptReplicas.getNodes(block);\n        \n        for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n             jt.hasNext();) {\n          DatanodeDescriptor node \u003d jt.next();\n          String state \u003d \"\";\n          if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n            state \u003d \"(corrupt)\";\n          } else if (node.isDecommissioned() || \n              node.isDecommissionInProgress()) {\n            state \u003d \"(decommissioned)\";\n          }          \n          out.print(\" \" + node + state + \" : \");\n        }\n        out.println(\"\");\n      }\n    }\n\n    //\n    // Dump blocks from pendingReplication\n    //\n    pendingReplications.metaSave(out);\n\n    //\n    // Dump blocks that are waiting to be deleted\n    //\n    dumpRecentInvalidateSets(out);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
    }
  }
}