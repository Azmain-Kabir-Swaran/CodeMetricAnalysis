{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RpcProgramNfs3.java",
  "functionName": "read",
  "functionId": "read___xdr-XDR__securityHandler-SecurityHandler__remoteAddress-SocketAddress",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
  "functionStartLine": 705,
  "functionEndLine": 829,
  "numCommitsSeen": 105,
  "timeTaken": 6368,
  "changeHistory": [
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
    "d6602b5f39833611b4afa4581552f6c4c37e23a8",
    "b09a03cd7d26cf96ec26a81ba11f00778241eb3e",
    "f37849188b05a6251584de1aed5e66d5dfa7da4f",
    "5e5e35b1856293503124b77d5d4998a4d8e83082",
    "f6f2a3f1c73266bfedd802eacde60d8b19b81015",
    "d71d40a63d198991077d5babd70be5e9787a53f1",
    "cd9182d8b5f60428f6c91b0eb0b2e61d52a07020",
    "b760f20af122b3e403bf3f5c7fd6320d1e82242f",
    "c9aa74743773c61be938cc1a6ea811ae1404bca2",
    "5f880f79d275c74475836a1932be6f6f2daa1407",
    "2ecab65e3e290a1ee192b39ec70868863853543a",
    "42391d260da400593812396c1ffd45d1a371d3cb",
    "3bfd18c6b0483ae27eff6d53bda934e67dda5464",
    "842aa2bc9432cc137bda0a5aec9c9eef12b000ce",
    "1f7dd7811a1152d0a798524e831586eebbe110b6",
    "5ea533c2bfc72fd3adbfd972d18806fbc397e0f8",
    "05f35518f19d48890770128727289582cca3457b",
    "613979c8fdacf25fd563395ecc399c4de94d3ee7",
    "15632cd76f12c2f7df50d0df4865fbe3d8261597",
    "37f587563a943a827fbff865f5302bac6d202415"
  ],
  "changeHistoryShort": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": "Ybodychange",
    "d6602b5f39833611b4afa4581552f6c4c37e23a8": "Ybodychange",
    "b09a03cd7d26cf96ec26a81ba11f00778241eb3e": "Ybodychange",
    "f37849188b05a6251584de1aed5e66d5dfa7da4f": "Ybodychange",
    "5e5e35b1856293503124b77d5d4998a4d8e83082": "Ybodychange",
    "f6f2a3f1c73266bfedd802eacde60d8b19b81015": "Ybodychange",
    "d71d40a63d198991077d5babd70be5e9787a53f1": "Ybodychange",
    "cd9182d8b5f60428f6c91b0eb0b2e61d52a07020": "Ybodychange",
    "b760f20af122b3e403bf3f5c7fd6320d1e82242f": "Ybodychange",
    "c9aa74743773c61be938cc1a6ea811ae1404bca2": "Ybodychange",
    "5f880f79d275c74475836a1932be6f6f2daa1407": "Ybodychange",
    "2ecab65e3e290a1ee192b39ec70868863853543a": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "42391d260da400593812396c1ffd45d1a371d3cb": "Ybodychange",
    "3bfd18c6b0483ae27eff6d53bda934e67dda5464": "Ybodychange",
    "842aa2bc9432cc137bda0a5aec9c9eef12b000ce": "Ybodychange",
    "1f7dd7811a1152d0a798524e831586eebbe110b6": "Ybodychange",
    "5ea533c2bfc72fd3adbfd972d18806fbc397e0f8": "Ybodychange",
    "05f35518f19d48890770128727289582cca3457b": "Ybodychange",
    "613979c8fdacf25fd563395ecc399c4de94d3ee7": "Ymultichange(Yparameterchange,Ybodychange)",
    "15632cd76f12c2f7df50d0df4865fbe3d8261597": "Ymultichange(Yparameterchange,Ybodychange)",
    "37f587563a943a827fbff865f5302bac6d202415": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10571. Use Log.*(Object, Throwable) overload to log exceptions.\nContributed by Andras Bokor.\n",
      "commitDate": "14/02/18 8:20 AM",
      "commitName": "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "10/10/17 10:38 AM",
      "commitNameOld": "d6602b5f39833611b4afa4581552f6c4c37e23a8",
      "commitAuthorOld": "Jitendra Pandey",
      "daysBetweenCommits": 126.95,
      "commitsBetweenForRepo": 833,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,131 +1,125 @@\n   READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n \n     if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n \n     READ3Request request;\n \n     try {\n       request \u003d READ3Request.deserialize(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     int namenodeId \u003d handle.getNamenodeId();\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"NFS READ fileHandle: \" + handle.dumpFileHandle()+ \" offset: \"\n-          + offset + \" count: \" + count + \" client: \" + remoteAddress);\n+      LOG.debug(\"NFS READ fileHandle: {} offset: {} count: {} client: {}\",\n+          handle.dumpFileHandle(), offset, count, remoteAddress);\n     }\n-\n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName, namenodeId);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Get error accessing file, fileId: \" + handle.getFileId(), e);\n-        }\n+        LOG.debug(\"Get error accessing file, fileId: {}\",\n+            handle.getFileId(), e);\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Can\u0027t get path for fileId: \" + handle.getFileId());\n-        }\n+        LOG.debug(\"Can\u0027t get path for fileId: {}\", handle.getFileId());\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(),\n           securityHandler.getAuxGids(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003e\u003d attrs.getSize();\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n-      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n-          + \". Read may not get most recent data.\");\n+      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d{}. \" +\n+          \"Read may not get most recent data.\", ret);\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle), namenodeId);\n \n         if (fis \u003d\u003d null) {\n             return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n         }\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n           metrics.incrBytesRead(readCount);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage().equals(\"Stream closed\")) {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle), namenodeId);\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n-        LOG.info(\"Partial read. Asked offset: \" + offset + \" count: \" + count\n-            + \" and read back: \" + readCount + \" file size: \"\n-            + attrs.getSize());\n+        LOG.info(\"Partial read. Asked offset: {} count: {} and read back: {} \" +\n+                \"file size: {}\", offset, count, readCount, attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n-      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n-          + \" count: \" + count, e);\n+      LOG.warn(\"Read error. Offset: {} count: {}\", offset, count, e);\n       int status \u003d mapErrorStatus(e);\n       return new READ3Response(status);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    READ3Request request;\n\n    try {\n      request \u003d READ3Request.deserialize(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    int namenodeId \u003d handle.getNamenodeId();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileHandle: {} offset: {} count: {} client: {}\",\n          handle.dumpFileHandle(), offset, count, remoteAddress);\n    }\n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName, namenodeId);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        LOG.debug(\"Get error accessing file, fileId: {}\",\n            handle.getFileId(), e);\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        LOG.debug(\"Can\u0027t get path for fileId: {}\", handle.getFileId());\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(),\n          securityHandler.getAuxGids(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003e\u003d attrs.getSize();\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n\n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d{}. \" +\n          \"Read may not get most recent data.\", ret);\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle), namenodeId);\n\n        if (fis \u003d\u003d null) {\n            return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n        }\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n          metrics.incrBytesRead(readCount);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage().equals(\"Stream closed\")) {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle), namenodeId);\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partial read. Asked offset: {} count: {} and read back: {} \" +\n                \"file size: {}\", offset, count, readCount, attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error. Offset: {} count: {}\", offset, count, e);\n      int status \u003d mapErrorStatus(e);\n      return new READ3Response(status);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "d6602b5f39833611b4afa4581552f6c4c37e23a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11575. Supporting HDFS NFS gateway with Federated HDFS. Contributed by Mukul Kumar Singh.\n",
      "commitDate": "10/10/17 10:38 AM",
      "commitName": "d6602b5f39833611b4afa4581552f6c4c37e23a8",
      "commitAuthor": "Jitendra Pandey",
      "commitDateOld": "14/08/17 9:57 PM",
      "commitNameOld": "645a8f2a4d09acb5a21820f52ee78784d9e4cc8a",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 56.53,
      "commitsBetweenForRepo": 520,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,130 +1,131 @@\n   READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n \n     if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n \n-    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n-    if (dfsClient \u003d\u003d null) {\n-      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n-      return response;\n-    }\n-\n     READ3Request request;\n \n     try {\n       request \u003d READ3Request.deserialize(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n+    int namenodeId \u003d handle.getNamenodeId();\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n-          + \" count: \" + count + \" client: \" + remoteAddress);\n+      LOG.debug(\"NFS READ fileHandle: \" + handle.dumpFileHandle()+ \" offset: \"\n+          + offset + \" count: \" + count + \" client: \" + remoteAddress);\n+    }\n+\n+    DFSClient dfsClient \u003d clientCache.getDfsClient(userName, namenodeId);\n+    if (dfsClient \u003d\u003d null) {\n+      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n+      return response;\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId: \" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId: \" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(),\n           securityHandler.getAuxGids(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003e\u003d attrs.getSize();\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n-            Nfs3Utils.getFileIdPath(handle));\n+            Nfs3Utils.getFileIdPath(handle), namenodeId);\n \n         if (fis \u003d\u003d null) {\n             return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n         }\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n           metrics.incrBytesRead(readCount);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage().equals(\"Stream closed\")) {\n             clientCache.invalidateDfsInputStream(userName,\n-                Nfs3Utils.getFileIdPath(handle));\n+                Nfs3Utils.getFileIdPath(handle), namenodeId);\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partial read. Asked offset: \" + offset + \" count: \" + count\n             + \" and read back: \" + readCount + \" file size: \"\n             + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       int status \u003d mapErrorStatus(e);\n       return new READ3Response(status);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    READ3Request request;\n\n    try {\n      request \u003d READ3Request.deserialize(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    int namenodeId \u003d handle.getNamenodeId();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileHandle: \" + handle.dumpFileHandle()+ \" offset: \"\n          + offset + \" count: \" + count + \" client: \" + remoteAddress);\n    }\n\n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName, namenodeId);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId: \" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId: \" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(),\n          securityHandler.getAuxGids(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003e\u003d attrs.getSize();\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n\n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle), namenodeId);\n\n        if (fis \u003d\u003d null) {\n            return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n        }\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n          metrics.incrBytesRead(readCount);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage().equals(\"Stream closed\")) {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle), namenodeId);\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partial read. Asked offset: \" + offset + \" count: \" + count\n            + \" and read back: \" + readCount + \" file size: \"\n            + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      int status \u003d mapErrorStatus(e);\n      return new READ3Response(status);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "b09a03cd7d26cf96ec26a81ba11f00778241eb3e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10862. Typos in 4 log messages. Contributed by Mehran Hassani.\n",
      "commitDate": "16/09/16 12:08 AM",
      "commitName": "b09a03cd7d26cf96ec26a81ba11f00778241eb3e",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "06/12/15 9:43 AM",
      "commitNameOld": "65f395226ba6cc3750a268a308e288b916f8df1e",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 284.56,
      "commitsBetweenForRepo": 1935,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,130 +1,130 @@\n   READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n \n     if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n \n     READ3Request request;\n \n     try {\n       request \u003d READ3Request.deserialize(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count + \" client: \" + remoteAddress);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId: \" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId: \" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(),\n           securityHandler.getAuxGids(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003e\u003d attrs.getSize();\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         if (fis \u003d\u003d null) {\n             return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n         }\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n           metrics.incrBytesRead(readCount);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage().equals(\"Stream closed\")) {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n-        LOG.info(\"Partical read. Asked offset: \" + offset + \" count: \" + count\n+        LOG.info(\"Partial read. Asked offset: \" + offset + \" count: \" + count\n             + \" and read back: \" + readCount + \" file size: \"\n             + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       int status \u003d mapErrorStatus(e);\n       return new READ3Response(status);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    READ3Request request;\n\n    try {\n      request \u003d READ3Request.deserialize(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count + \" client: \" + remoteAddress);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId: \" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId: \" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(),\n          securityHandler.getAuxGids(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003e\u003d attrs.getSize();\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n\n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        if (fis \u003d\u003d null) {\n            return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n        }\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n          metrics.incrBytesRead(readCount);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage().equals(\"Stream closed\")) {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partial read. Asked offset: \" + offset + \" count: \" + count\n            + \" and read back: \" + readCount + \" file size: \"\n            + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      int status \u003d mapErrorStatus(e);\n      return new READ3Response(status);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "f37849188b05a6251584de1aed5e66d5dfa7da4f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7423. various typos and message formatting fixes in nfs daemon and doc. (Charles Lamb via yliu)\n",
      "commitDate": "28/01/15 12:56 PM",
      "commitName": "f37849188b05a6251584de1aed5e66d5dfa7da4f",
      "commitAuthor": "yliu",
      "commitDateOld": "19/01/15 5:29 PM",
      "commitNameOld": "5e5e35b1856293503124b77d5d4998a4d8e83082",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 8.81,
      "commitsBetweenForRepo": 76,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,129 +1,130 @@\n   READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n \n     if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n \n     READ3Request request;\n \n     try {\n       request \u003d READ3Request.deserialize(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n-          + \" count: \" + count + \" client:\" + remoteAddress);\n+          + \" count: \" + count + \" client: \" + remoteAddress);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n+          LOG.debug(\"Get error accessing file, fileId: \" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n+          LOG.debug(\"Can\u0027t get path for fileId: \" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(),\n           securityHandler.getAuxGids(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003e\u003d attrs.getSize();\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         if (fis \u003d\u003d null) {\n             return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n         }\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n           metrics.incrBytesRead(readCount);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage().equals(\"Stream closed\")) {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n-        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n-            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n+        LOG.info(\"Partical read. Asked offset: \" + offset + \" count: \" + count\n+            + \" and read back: \" + readCount + \" file size: \"\n+            + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       int status \u003d mapErrorStatus(e);\n       return new READ3Response(status);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    READ3Request request;\n\n    try {\n      request \u003d READ3Request.deserialize(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count + \" client: \" + remoteAddress);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId: \" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId: \" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(),\n          securityHandler.getAuxGids(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003e\u003d attrs.getSize();\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n\n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        if (fis \u003d\u003d null) {\n            return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n        }\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n          metrics.incrBytesRead(readCount);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage().equals(\"Stream closed\")) {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset: \" + offset + \" count: \" + count\n            + \" and read back: \" + readCount + \" file size: \"\n            + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      int status \u003d mapErrorStatus(e);\n      return new READ3Response(status);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "5e5e35b1856293503124b77d5d4998a4d8e83082": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7640. print NFS Client in the NFS log. Contributed by Brandon Li.\n",
      "commitDate": "19/01/15 5:29 PM",
      "commitName": "5e5e35b1856293503124b77d5d4998a4d8e83082",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "11/12/14 3:40 PM",
      "commitNameOld": "f6f2a3f1c73266bfedd802eacde60d8b19b81015",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 39.08,
      "commitsBetweenForRepo": 202,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,129 +1,129 @@\n   READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n \n     if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n \n     READ3Request request;\n \n     try {\n       request \u003d READ3Request.deserialize(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n-          + \" count: \" + count);\n+          + \" count: \" + count + \" client:\" + remoteAddress);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(),\n           securityHandler.getAuxGids(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003e\u003d attrs.getSize();\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         if (fis \u003d\u003d null) {\n             return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n         }\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n           metrics.incrBytesRead(readCount);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage().equals(\"Stream closed\")) {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       int status \u003d mapErrorStatus(e);\n       return new READ3Response(status);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    READ3Request request;\n\n    try {\n      request \u003d READ3Request.deserialize(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count + \" client:\" + remoteAddress);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(),\n          securityHandler.getAuxGids(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003e\u003d attrs.getSize();\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n\n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        if (fis \u003d\u003d null) {\n            return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n        }\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n          metrics.incrBytesRead(readCount);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage().equals(\"Stream closed\")) {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      int status \u003d mapErrorStatus(e);\n      return new READ3Response(status);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "f6f2a3f1c73266bfedd802eacde60d8b19b81015": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7449. Add metrics to NFS gateway. Contributed by Brandon Li\n",
      "commitDate": "11/12/14 3:40 PM",
      "commitName": "f6f2a3f1c73266bfedd802eacde60d8b19b81015",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "09/12/14 8:42 PM",
      "commitNameOld": "195f31a8ef6b15e1962ab945b2f83af98e0058c6",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.79,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,128 +1,129 @@\n   READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n \n     if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n \n     READ3Request request;\n \n     try {\n       request \u003d READ3Request.deserialize(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(),\n           securityHandler.getAuxGids(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003e\u003d attrs.getSize();\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         if (fis \u003d\u003d null) {\n             return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n         }\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n+          metrics.incrBytesRead(readCount);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage().equals(\"Stream closed\")) {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       int status \u003d mapErrorStatus(e);\n       return new READ3Response(status);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    READ3Request request;\n\n    try {\n      request \u003d READ3Request.deserialize(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(),\n          securityHandler.getAuxGids(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003e\u003d attrs.getSize();\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n\n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        if (fis \u003d\u003d null) {\n            return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n        }\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n          metrics.incrBytesRead(readCount);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage().equals(\"Stream closed\")) {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      int status \u003d mapErrorStatus(e);\n      return new READ3Response(status);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "d71d40a63d198991077d5babd70be5e9787a53f1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7180. NFSv3 gateway frequently gets stuck due to GC. Contributed by Brandon Li\n",
      "commitDate": "22/10/14 9:27 PM",
      "commitName": "d71d40a63d198991077d5babd70be5e9787a53f1",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "21/10/14 11:49 AM",
      "commitNameOld": "4e134a02a4b6f30704b99dfb166dc361daf426ea",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 1.4,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,128 +1,128 @@\n   READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n \n     if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n \n-    READ3Request request \u003d null;\n+    READ3Request request;\n \n     try {\n       request \u003d READ3Request.deserialize(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(),\n           securityHandler.getAuxGids(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n-        eof \u003d offset \u003c attrs.getSize() ? false : true;\n+        eof \u003d offset \u003e\u003d attrs.getSize();\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         if (fis \u003d\u003d null) {\n             return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n         }\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n-          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n+          if (e.getMessage().equals(\"Stream closed\")) {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n-      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n+      eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       int status \u003d mapErrorStatus(e);\n       return new READ3Response(status);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    READ3Request request;\n\n    try {\n      request \u003d READ3Request.deserialize(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(),\n          securityHandler.getAuxGids(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003e\u003d attrs.getSize();\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n\n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        if (fis \u003d\u003d null) {\n            return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n        }\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage().equals(\"Stream closed\")) {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003e\u003d attrs.getSize();\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      int status \u003d mapErrorStatus(e);\n      return new READ3Response(status);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "cd9182d8b5f60428f6c91b0eb0b2e61d52a07020": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6892. Add XDR packaging method for each NFS request. Contributed by Brandon Li\n",
      "commitDate": "27/08/14 11:06 AM",
      "commitName": "cd9182d8b5f60428f6c91b0eb0b2e61d52a07020",
      "commitAuthor": "brandonli",
      "commitDateOld": "21/08/14 10:53 AM",
      "commitNameOld": "7b28f363b1b3f12cecc92d0bba8eb3021b67b48e",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 6.01,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,128 +1,128 @@\n   READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n \n     if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n \n     READ3Request request \u003d null;\n \n     try {\n-      request \u003d new READ3Request(xdr);\n+      request \u003d READ3Request.deserialize(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(),\n           securityHandler.getAuxGids(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         if (fis \u003d\u003d null) {\n             return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n         }\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       int status \u003d mapErrorStatus(e);\n       return new READ3Response(status);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    READ3Request request \u003d null;\n\n    try {\n      request \u003d READ3Request.deserialize(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(),\n          securityHandler.getAuxGids(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n\n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        if (fis \u003d\u003d null) {\n            return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n        }\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      int status \u003d mapErrorStatus(e);\n      return new READ3Response(status);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "b760f20af122b3e403bf3f5c7fd6320d1e82242f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6582. Missing null check in RpcProgramNfs3#read(XDR, SecurityHandler). Contributed by Abhiraj Butala\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1617366 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/08/14 2:34 PM",
      "commitName": "b760f20af122b3e403bf3f5c7fd6320d1e82242f",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "04/08/14 10:40 AM",
      "commitNameOld": "c9aa74743773c61be938cc1a6ea811ae1404bca2",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 7.16,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,124 +1,128 @@\n   READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n \n     if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(),\n           securityHandler.getAuxGids(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n+        if (fis \u003d\u003d null) {\n+            return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n+        }\n+\n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       int status \u003d mapErrorStatus(e);\n       return new READ3Response(status);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(),\n          securityHandler.getAuxGids(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n\n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        if (fis \u003d\u003d null) {\n            return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n        }\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      int status \u003d mapErrorStatus(e);\n      return new READ3Response(status);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "c9aa74743773c61be938cc1a6ea811ae1404bca2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6451. NFS should not return NFS3ERR_IO for AccessControlException. Contributed by Abhiraj Butala\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/08/14 10:40 AM",
      "commitName": "c9aa74743773c61be938cc1a6ea811ae1404bca2",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "23/07/14 2:22 PM",
      "commitNameOld": "2a5f1029a5221c42ab61b22f99d79251ed069ca4",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 11.85,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,123 +1,124 @@\n   READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n-    \n+\n     if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n-    \n+\n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n-    \n+\n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(),\n           securityHandler.getAuxGids(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n-    \n+\n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n-      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n+      int status \u003d mapErrorStatus(e);\n+      return new READ3Response(status);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n\n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n\n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(),\n          securityHandler.getAuxGids(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n\n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      int status \u003d mapErrorStatus(e);\n      return new READ3Response(status);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "5f880f79d275c74475836a1932be6f6f2daa1407": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10701. NFS should not validate the access premission only based on the user\u0027s primary group. Contributed by Harsh J.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1606042 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/06/14 5:00 AM",
      "commitName": "5f880f79d275c74475836a1932be6f6f2daa1407",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "19/06/14 12:39 PM",
      "commitNameOld": "9ff3836a367737d6dfcb12f50c8bd2f1b2233e37",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 7.68,
      "commitsBetweenForRepo": 56,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,122 +1,123 @@\n   READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n     \n     if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n-          securityHandler.getUid(), securityHandler.getGid(), attrs);\n+          securityHandler.getUid(), securityHandler.getGid(),\n+          securityHandler.getAuxGids(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n    \n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(),\n          securityHandler.getAuxGids(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "2ecab65e3e290a1ee192b39ec70868863853543a": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-6439. NFS should not reject NFS requests to the NULL procedure whether port monitoring is enabled or not. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603622 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/06/14 1:45 PM",
      "commitName": "2ecab65e3e290a1ee192b39ec70868863853543a",
      "commitAuthor": "Brandon Li",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6439. NFS should not reject NFS requests to the NULL procedure whether port monitoring is enabled or not. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603622 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/06/14 1:45 PM",
          "commitName": "2ecab65e3e290a1ee192b39ec70868863853543a",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "30/05/14 4:53 PM",
          "commitNameOld": "42391d260da400593812396c1ffd45d1a371d3cb",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 18.87,
          "commitsBetweenForRepo": 106,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,122 +1,122 @@\n-  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n-      InetAddress client) {\n+  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n+      SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n     \n-    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n+    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n    \n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
          "extendedDetails": {
            "oldValue": "[xdr-XDR, securityHandler-SecurityHandler, client-InetAddress]",
            "newValue": "[xdr-XDR, securityHandler-SecurityHandler, remoteAddress-SocketAddress]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-6439. NFS should not reject NFS requests to the NULL procedure whether port monitoring is enabled or not. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603622 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/06/14 1:45 PM",
          "commitName": "2ecab65e3e290a1ee192b39ec70868863853543a",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "30/05/14 4:53 PM",
          "commitNameOld": "42391d260da400593812396c1ffd45d1a371d3cb",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 18.87,
          "commitsBetweenForRepo": 106,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,122 +1,122 @@\n-  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n-      InetAddress client) {\n+  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n+      SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n     \n-    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n+    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n    \n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6439. NFS should not reject NFS requests to the NULL procedure whether port monitoring is enabled or not. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603622 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/06/14 1:45 PM",
          "commitName": "2ecab65e3e290a1ee192b39ec70868863853543a",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "30/05/14 4:53 PM",
          "commitNameOld": "42391d260da400593812396c1ffd45d1a371d3cb",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 18.87,
          "commitsBetweenForRepo": 106,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,122 +1,122 @@\n-  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n-      InetAddress client) {\n+  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n+      SocketAddress remoteAddress) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n     \n-    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n+    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n           NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      SocketAddress remoteAddress) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n    \n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
          "extendedDetails": {}
        }
      ]
    },
    "42391d260da400593812396c1ffd45d1a371d3cb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6056. Clean up NFS config settings. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1598782 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/05/14 4:53 PM",
      "commitName": "42391d260da400593812396c1ffd45d1a371d3cb",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "29/05/14 2:16 PM",
      "commitNameOld": "c55947343e0a629f71d69b49ba87997573fe48a2",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 1.11,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,122 +1,122 @@\n   public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       InetAddress client) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n     \n     if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n-      int rtmax \u003d config.getInt(Nfs3Constant.MAX_READ_TRANSFER_SIZE_KEY,\n-              Nfs3Constant.MAX_READ_TRANSFER_SIZE_DEFAULT);\n+      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n+          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,\n          NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "3bfd18c6b0483ae27eff6d53bda934e67dda5464": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6050. NFS does not handle exceptions correctly in a few places. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1581055 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/03/14 1:49 PM",
      "commitName": "3bfd18c6b0483ae27eff6d53bda934e67dda5464",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "13/03/14 2:03 PM",
      "commitNameOld": "842aa2bc9432cc137bda0a5aec9c9eef12b000ce",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 10.99,
      "commitsBetweenForRepo": 79,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,124 +1,122 @@\n   public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       InetAddress client) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n     \n     if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n-        attrs \u003d Nfs3Utils.getFileAttr(\n-                  dfsClient,\n-                  Nfs3Utils.getFileIdPath(handle),\n-                  iug);\n+        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n+            Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n+          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int rtmax \u003d config.getInt(Nfs3Constant.MAX_READ_TRANSFER_SIZE_KEY,\n               Nfs3Constant.MAX_READ_TRANSFER_SIZE_DEFAULT);\n       int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(dfsClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId(), e);\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(Nfs3Constant.MAX_READ_TRANSFER_SIZE_KEY,\n              Nfs3Constant.MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "842aa2bc9432cc137bda0a5aec9c9eef12b000ce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6080. Improve NFS gateway performance by making rtmax and wtmax configurable. Contributed by Abin Shahab\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1577319 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/03/14 2:03 PM",
      "commitName": "842aa2bc9432cc137bda0a5aec9c9eef12b000ce",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "05/03/14 2:20 PM",
      "commitNameOld": "6adf7a0ecb7946434b9312aa01505f58eaefd21f",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 7.95,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,122 +1,124 @@\n   public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       InetAddress client) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n     \n     if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(\n                   dfsClient,\n                   Nfs3Utils.getFileIdPath(handle),\n                   iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n-      int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n+      int rtmax \u003d config.getInt(Nfs3Constant.MAX_READ_TRANSFER_SIZE_KEY,\n+              Nfs3Constant.MAX_READ_TRANSFER_SIZE_DEFAULT);\n+      int buffSize \u003d Math.min(rtmax, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(\n                  dfsClient,\n                  Nfs3Utils.getFileIdPath(handle),\n                  iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int rtmax \u003d config.getInt(Nfs3Constant.MAX_READ_TRANSFER_SIZE_KEY,\n              Nfs3Constant.MAX_READ_TRANSFER_SIZE_DEFAULT);\n      int buffSize \u003d Math.min(rtmax, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "1f7dd7811a1152d0a798524e831586eebbe110b6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5804. HDFS NFS Gateway fails to mount and proxy when using Kerberos. Contributed by Abin Shahab.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563323 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/01/14 3:00 PM",
      "commitName": "1f7dd7811a1152d0a798524e831586eebbe110b6",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/11/13 3:41 PM",
      "commitNameOld": "5ea533c2bfc72fd3adbfd972d18806fbc397e0f8",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 64.97,
      "commitsBetweenForRepo": 336,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,120 +1,122 @@\n   public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       InetAddress client) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n     \n     if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n-        attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n-            Nfs3Utils.getFileIdPath(handle), iug);\n+        attrs \u003d Nfs3Utils.getFileAttr(\n+                  dfsClient,\n+                  Nfs3Utils.getFileIdPath(handle),\n+                  iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     // In case there is buffered data for the same file, flush it. This can be\n     // optimized later by reading from the cache.\n     int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n     if (ret !\u003d Nfs3Status.NFS3_OK) {\n       LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n           + \". Read may not get most recent data.\");\n     }\n \n     try {\n       int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(\n                  dfsClient,\n                  Nfs3Utils.getFileIdPath(handle),\n                  iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "5ea533c2bfc72fd3adbfd972d18806fbc397e0f8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5563. NFS gateway should commit the buffered data when read request comes after write to the same file. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546233 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 3:41 PM",
      "commitName": "5ea533c2bfc72fd3adbfd972d18806fbc397e0f8",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "09/11/13 8:07 PM",
      "commitNameOld": "ec9ec0084eccdd45a8c3e37ef8121fb8bd44ecd0",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 17.82,
      "commitsBetweenForRepo": 119,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,112 +1,120 @@\n   public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       InetAddress client) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     final String userName \u003d securityHandler.getUser();\n     \n     if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n     DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n+    // In case there is buffered data for the same file, flush it. This can be\n+    // optimized later by reading from the cache.\n+    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n+    if (ret !\u003d Nfs3Status.NFS3_OK) {\n+      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n+          + \". Read may not get most recent data.\");\n+    }\n+\n     try {\n       int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       int readCount \u003d 0;\n       /**\n        * Retry exactly once because the DFSInputStream can be stale.\n        */\n       for (int i \u003d 0; i \u003c 1; ++i) {\n         FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n             Nfs3Utils.getFileIdPath(handle));\n \n         try {\n           readCount \u003d fis.read(offset, readbuffer, 0, count);\n         } catch (IOException e) {\n           // TODO: A cleaner way is to throw a new type of exception\n           // which requires incompatible changes.\n           if (e.getMessage() \u003d\u003d \"Stream closed\") {\n             clientCache.invalidateDfsInputStream(userName,\n                 Nfs3Utils.getFileIdPath(handle));\n             continue;\n           } else {\n             throw e;\n           }\n         }\n       }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    // In case there is buffered data for the same file, flush it. This can be\n    // optimized later by reading from the cache.\n    int ret \u003d writeManager.commitBeforeRead(dfsClient, handle, offset + count);\n    if (ret !\u003d Nfs3Status.NFS3_OK) {\n      LOG.warn(\"commitBeforeRead didn\u0027t succeed with ret\u003d\" + ret\n          + \". Read may not get most recent data.\");\n    }\n\n    try {\n      int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "05f35518f19d48890770128727289582cca3457b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5171. NFS should create input stream for a file and try to share it with multiple read requests. Contributed by Haohui Mai\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1535586 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/10/13 4:40 PM",
      "commitName": "05f35518f19d48890770128727289582cca3457b",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "15/10/13 2:23 PM",
      "commitNameOld": "a9befa6f0a8a27b49b1e6483e749661f493f06cf",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 9.1,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,112 @@\n   public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n       InetAddress client) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n+    final String userName \u003d securityHandler.getUser();\n     \n     if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n-    DFSClient dfsClient \u003d clientCache.get(securityHandler.getUser());\n+    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n           securityHandler.getUid(), securityHandler.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     try {\n       int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n-      DFSInputStream is \u003d dfsClient.open(Nfs3Utils.getFileIdPath(handle));\n-      FSDataInputStream fis \u003d new FSDataInputStream(is);\n-      \n-      int readCount \u003d fis.read(offset, readbuffer, 0, count);\n-      fis.close();\n+      int readCount \u003d 0;\n+      /**\n+       * Retry exactly once because the DFSInputStream can be stale.\n+       */\n+      for (int i \u003d 0; i \u003c 1; ++i) {\n+        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n+            Nfs3Utils.getFileIdPath(handle));\n+\n+        try {\n+          readCount \u003d fis.read(offset, readbuffer, 0, count);\n+        } catch (IOException e) {\n+          // TODO: A cleaner way is to throw a new type of exception\n+          // which requires incompatible changes.\n+          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n+            clientCache.invalidateDfsInputStream(userName,\n+                Nfs3Utils.getFileIdPath(handle));\n+            continue;\n+          } else {\n+            throw e;\n+          }\n+        }\n+      }\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    final String userName \u003d securityHandler.getUser();\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.getDfsClient(userName);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    try {\n      int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      int readCount \u003d 0;\n      /**\n       * Retry exactly once because the DFSInputStream can be stale.\n       */\n      for (int i \u003d 0; i \u003c 1; ++i) {\n        FSDataInputStream fis \u003d clientCache.getDfsInputStream(userName,\n            Nfs3Utils.getFileIdPath(handle));\n\n        try {\n          readCount \u003d fis.read(offset, readbuffer, 0, count);\n        } catch (IOException e) {\n          // TODO: A cleaner way is to throw a new type of exception\n          // which requires incompatible changes.\n          if (e.getMessage() \u003d\u003d \"Stream closed\") {\n            clientCache.invalidateDfsInputStream(userName,\n                Nfs3Utils.getFileIdPath(handle));\n            continue;\n          } else {\n            throw e;\n          }\n        }\n      }\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
      "extendedDetails": {}
    },
    "613979c8fdacf25fd563395ecc399c4de94d3ee7": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5085. Refactor o.a.h.nfs to support different types of authentications. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1521601 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/09/13 12:29 PM",
      "commitName": "613979c8fdacf25fd563395ecc399c4de94d3ee7",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5085. Refactor o.a.h.nfs to support different types of authentications. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1521601 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "10/09/13 12:29 PM",
          "commitName": "613979c8fdacf25fd563395ecc399c4de94d3ee7",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "31/08/13 2:12 PM",
          "commitNameOld": "cbca1668317f3f2d295eea53d7bd020bda4a810f",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 9.93,
          "commitsBetweenForRepo": 38,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,95 +1,95 @@\n-  public READ3Response read(XDR xdr, RpcAuthSys authSys, InetAddress client) {\n+  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n+      InetAddress client) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     \n     if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n-    String uname \u003d authSysCheck(authSys);\n-    DFSClient dfsClient \u003d clientCache.get(uname);\n+    DFSClient dfsClient \u003d clientCache.get(securityHandler.getUser());\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n-      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(authSys.getUid(),\n-          authSys.getGid(), attrs);\n+      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n+          securityHandler.getUid(), securityHandler.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     try {\n       int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       DFSInputStream is \u003d dfsClient.open(Nfs3Utils.getFileIdPath(handle));\n       FSDataInputStream fis \u003d new FSDataInputStream(is);\n       \n       int readCount \u003d fis.read(offset, readbuffer, 0, count);\n       fis.close();\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.get(securityHandler.getUser());\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    \n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    try {\n      int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      DFSInputStream is \u003d dfsClient.open(Nfs3Utils.getFileIdPath(handle));\n      FSDataInputStream fis \u003d new FSDataInputStream(is);\n      \n      int readCount \u003d fis.read(offset, readbuffer, 0, count);\n      fis.close();\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
          "extendedDetails": {
            "oldValue": "[xdr-XDR, authSys-RpcAuthSys, client-InetAddress]",
            "newValue": "[xdr-XDR, securityHandler-SecurityHandler, client-InetAddress]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5085. Refactor o.a.h.nfs to support different types of authentications. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1521601 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "10/09/13 12:29 PM",
          "commitName": "613979c8fdacf25fd563395ecc399c4de94d3ee7",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "31/08/13 2:12 PM",
          "commitNameOld": "cbca1668317f3f2d295eea53d7bd020bda4a810f",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 9.93,
          "commitsBetweenForRepo": 38,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,95 +1,95 @@\n-  public READ3Response read(XDR xdr, RpcAuthSys authSys, InetAddress client) {\n+  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n+      InetAddress client) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n     \n     if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n       response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n       return response;\n     }\n     \n-    String uname \u003d authSysCheck(authSys);\n-    DFSClient dfsClient \u003d clientCache.get(uname);\n+    DFSClient dfsClient \u003d clientCache.get(securityHandler.getUser());\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n-      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(authSys.getUid(),\n-          authSys.getGid(), attrs);\n+      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n+          securityHandler.getUid(), securityHandler.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     try {\n       int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       DFSInputStream is \u003d dfsClient.open(Nfs3Utils.getFileIdPath(handle));\n       FSDataInputStream fis \u003d new FSDataInputStream(is);\n       \n       int readCount \u003d fis.read(offset, readbuffer, 0, count);\n       fis.close();\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public READ3Response read(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient \u003d clientCache.get(securityHandler.getUser());\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    \n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(\n          securityHandler.getUid(), securityHandler.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    try {\n      int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      DFSInputStream is \u003d dfsClient.open(Nfs3Utils.getFileIdPath(handle));\n      FSDataInputStream fis \u003d new FSDataInputStream(is);\n      \n      int readCount \u003d fis.read(offset, readbuffer, 0, count);\n      fis.close();\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
          "extendedDetails": {}
        }
      ]
    },
    "15632cd76f12c2f7df50d0df4865fbe3d8261597": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4947 Add NFS server export table to control export by hostname or IP range. Contributed by Jing Zhao\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1517040 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/08/13 2:14 PM",
      "commitName": "15632cd76f12c2f7df50d0df4865fbe3d8261597",
      "commitAuthor": "Brandon Li",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4947 Add NFS server export table to control export by hostname or IP range. Contributed by Jing Zhao\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1517040 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/08/13 2:14 PM",
          "commitName": "15632cd76f12c2f7df50d0df4865fbe3d8261597",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "19/08/13 2:54 PM",
          "commitNameOld": "c9b89de0eacf15f21faa3a7ba30d4773f571c9a4",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 3.97,
          "commitsBetweenForRepo": 30,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,89 +1,95 @@\n-  public READ3Response read(XDR xdr, RpcAuthSys authSys) {\n+  public READ3Response read(XDR xdr, RpcAuthSys authSys, InetAddress client) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n+    \n+    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n+      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n+      return response;\n+    }\n+    \n     String uname \u003d authSysCheck(authSys);\n     DFSClient dfsClient \u003d clientCache.get(uname);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(authSys.getUid(),\n           authSys.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     try {\n       int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       DFSInputStream is \u003d dfsClient.open(Nfs3Utils.getFileIdPath(handle));\n       FSDataInputStream fis \u003d new FSDataInputStream(is);\n       \n       int readCount \u003d fis.read(offset, readbuffer, 0, count);\n       fis.close();\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public READ3Response read(XDR xdr, RpcAuthSys authSys, InetAddress client) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    String uname \u003d authSysCheck(authSys);\n    DFSClient dfsClient \u003d clientCache.get(uname);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    \n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(authSys.getUid(),\n          authSys.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    try {\n      int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      DFSInputStream is \u003d dfsClient.open(Nfs3Utils.getFileIdPath(handle));\n      FSDataInputStream fis \u003d new FSDataInputStream(is);\n      \n      int readCount \u003d fis.read(offset, readbuffer, 0, count);\n      fis.close();\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
          "extendedDetails": {
            "oldValue": "[xdr-XDR, authSys-RpcAuthSys]",
            "newValue": "[xdr-XDR, authSys-RpcAuthSys, client-InetAddress]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4947 Add NFS server export table to control export by hostname or IP range. Contributed by Jing Zhao\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1517040 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/08/13 2:14 PM",
          "commitName": "15632cd76f12c2f7df50d0df4865fbe3d8261597",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "19/08/13 2:54 PM",
          "commitNameOld": "c9b89de0eacf15f21faa3a7ba30d4773f571c9a4",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 3.97,
          "commitsBetweenForRepo": 30,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,89 +1,95 @@\n-  public READ3Response read(XDR xdr, RpcAuthSys authSys) {\n+  public READ3Response read(XDR xdr, RpcAuthSys authSys, InetAddress client) {\n     READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n+    \n+    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n+      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n+      return response;\n+    }\n+    \n     String uname \u003d authSysCheck(authSys);\n     DFSClient dfsClient \u003d clientCache.get(uname);\n     if (dfsClient \u003d\u003d null) {\n       response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n       return response;\n     }\n     \n     READ3Request request \u003d null;\n \n     try {\n       request \u003d new READ3Request(xdr);\n     } catch (IOException e) {\n       LOG.error(\"Invalid READ request\");\n       return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n     }\n \n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n \n     \n     FileHandle handle \u003d request.getHandle();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n           + \" count: \" + count);\n     }\n \n     Nfs3FileAttributes attrs;\n     boolean eof;\n     if (count \u003d\u003d 0) {\n       // Only do access check.\n       try {\n         // Don\u0027t read from cache. Client may not have read permission.\n         attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n             Nfs3Utils.getFileIdPath(handle), iug);\n       } catch (IOException e) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_IO);\n       }\n       if (attrs \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n         }\n         return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n       }\n       int access \u003d Nfs3Utils.getAccessRightsForUserGroup(authSys.getUid(),\n           authSys.getGid(), attrs);\n       if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n         eof \u003d offset \u003c attrs.getSize() ? false : true;\n         return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n             ByteBuffer.wrap(new byte[0]));\n       } else {\n         return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n       }\n     }\n     \n     try {\n       int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n       byte[] readbuffer \u003d new byte[buffSize];\n \n       DFSInputStream is \u003d dfsClient.open(Nfs3Utils.getFileIdPath(handle));\n       FSDataInputStream fis \u003d new FSDataInputStream(is);\n       \n       int readCount \u003d fis.read(offset, readbuffer, 0, count);\n       fis.close();\n \n       attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n           iug);\n       if (readCount \u003c count) {\n         LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n             + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n       }\n       // HDFS returns -1 for read beyond file size.\n       if (readCount \u003c 0) {\n         readCount \u003d 0;\n       }\n       eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n       return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n           ByteBuffer.wrap(readbuffer));\n \n     } catch (IOException e) {\n       LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n           + \" count: \" + count, e);\n       return new READ3Response(Nfs3Status.NFS3ERR_IO);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public READ3Response read(XDR xdr, RpcAuthSys authSys, InetAddress client) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    String uname \u003d authSysCheck(authSys);\n    DFSClient dfsClient \u003d clientCache.get(uname);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    \n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(authSys.getUid(),\n          authSys.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    try {\n      int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      DFSInputStream is \u003d dfsClient.open(Nfs3Utils.getFileIdPath(handle));\n      FSDataInputStream fis \u003d new FSDataInputStream(is);\n      \n      int readCount \u003d fis.read(offset, readbuffer, 0, count);\n      fis.close();\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java",
          "extendedDetails": {}
        }
      ]
    },
    "37f587563a943a827fbff865f5302bac6d202415": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4762 Provide HDFS based NFSv3 and Mountd implementation. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1499029 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/07/13 10:31 AM",
      "commitName": "37f587563a943a827fbff865f5302bac6d202415",
      "commitAuthor": "Brandon Li",
      "diff": "@@ -0,0 +1,89 @@\n+  public READ3Response read(XDR xdr, RpcAuthSys authSys) {\n+    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n+    String uname \u003d authSysCheck(authSys);\n+    DFSClient dfsClient \u003d clientCache.get(uname);\n+    if (dfsClient \u003d\u003d null) {\n+      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n+      return response;\n+    }\n+    \n+    READ3Request request \u003d null;\n+\n+    try {\n+      request \u003d new READ3Request(xdr);\n+    } catch (IOException e) {\n+      LOG.error(\"Invalid READ request\");\n+      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n+    }\n+\n+    long offset \u003d request.getOffset();\n+    int count \u003d request.getCount();\n+\n+    \n+    FileHandle handle \u003d request.getHandle();\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n+          + \" count: \" + count);\n+    }\n+\n+    Nfs3FileAttributes attrs;\n+    boolean eof;\n+    if (count \u003d\u003d 0) {\n+      // Only do access check.\n+      try {\n+        // Don\u0027t read from cache. Client may not have read permission.\n+        attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n+            Nfs3Utils.getFileIdPath(handle), iug);\n+      } catch (IOException e) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n+        }\n+        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n+      }\n+      if (attrs \u003d\u003d null) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n+        }\n+        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n+      }\n+      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(authSys.getUid(),\n+          authSys.getGid(), attrs);\n+      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n+        eof \u003d offset \u003c attrs.getSize() ? false : true;\n+        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n+            ByteBuffer.wrap(new byte[0]));\n+      } else {\n+        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n+      }\n+    }\n+    \n+    try {\n+      int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n+      byte[] readbuffer \u003d new byte[buffSize];\n+\n+      DFSInputStream is \u003d dfsClient.open(Nfs3Utils.getFileIdPath(handle));\n+      FSDataInputStream fis \u003d new FSDataInputStream(is);\n+      \n+      int readCount \u003d fis.read(offset, readbuffer, 0, count);\n+      fis.close();\n+\n+      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n+          iug);\n+      if (readCount \u003c count) {\n+        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n+            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n+      }\n+      // HDFS returns -1 for read beyond file size.\n+      if (readCount \u003c 0) {\n+        readCount \u003d 0;\n+      }\n+      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n+      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n+          ByteBuffer.wrap(readbuffer));\n+\n+    } catch (IOException e) {\n+      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n+          + \" count: \" + count, e);\n+      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public READ3Response read(XDR xdr, RpcAuthSys authSys) {\n    READ3Response response \u003d new READ3Response(Nfs3Status.NFS3_OK);\n    String uname \u003d authSysCheck(authSys);\n    DFSClient dfsClient \u003d clientCache.get(uname);\n    if (dfsClient \u003d\u003d null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    READ3Request request \u003d null;\n\n    try {\n      request \u003d new READ3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid READ request\");\n      return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n\n    \n    FileHandle handle \u003d request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS READ fileId: \" + handle.getFileId() + \" offset: \" + offset\n          + \" count: \" + count);\n    }\n\n    Nfs3FileAttributes attrs;\n    boolean eof;\n    if (count \u003d\u003d 0) {\n      // Only do access check.\n      try {\n        // Don\u0027t read from cache. Client may not have read permission.\n        attrs \u003d Nfs3Utils.getFileAttr(superUserClient,\n            Nfs3Utils.getFileIdPath(handle), iug);\n      } catch (IOException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Get error accessing file, fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_IO);\n      }\n      if (attrs \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Can\u0027t get path for fileId:\" + handle.getFileId());\n        }\n        return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\n      }\n      int access \u003d Nfs3Utils.getAccessRightsForUserGroup(authSys.getUid(),\n          authSys.getGid(), attrs);\n      if ((access \u0026 Nfs3Constant.ACCESS3_READ) !\u003d 0) {\n        eof \u003d offset \u003c attrs.getSize() ? false : true;\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof,\n            ByteBuffer.wrap(new byte[0]));\n      } else {\n        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\n      }\n    }\n    \n    try {\n      int buffSize \u003d Math.min(MAX_READ_TRANSFER_SIZE, count);\n      byte[] readbuffer \u003d new byte[buffSize];\n\n      DFSInputStream is \u003d dfsClient.open(Nfs3Utils.getFileIdPath(handle));\n      FSDataInputStream fis \u003d new FSDataInputStream(is);\n      \n      int readCount \u003d fis.read(offset, readbuffer, 0, count);\n      fis.close();\n\n      attrs \u003d Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle),\n          iug);\n      if (readCount \u003c count) {\n        LOG.info(\"Partical read. Asked offset:\" + offset + \" count:\" + count\n            + \" and read back:\" + readCount + \"file size:\" + attrs.getSize());\n      }\n      // HDFS returns -1 for read beyond file size.\n      if (readCount \u003c 0) {\n        readCount \u003d 0;\n      }\n      eof \u003d (offset + readCount) \u003c attrs.getSize() ? false : true;\n      return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof,\n          ByteBuffer.wrap(readbuffer));\n\n    } catch (IOException e) {\n      LOG.warn(\"Read error: \" + e.getClass() + \" offset: \" + offset\n          + \" count: \" + count, e);\n      return new READ3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java"
    }
  }
}