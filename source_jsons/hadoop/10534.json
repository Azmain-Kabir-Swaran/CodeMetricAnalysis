{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BPServiceActor.java",
  "functionName": "sendHeartBeat",
  "functionId": "sendHeartBeat___requestBlockReportLease-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
  "functionStartLine": 516,
  "functionEndLine": 560,
  "numCommitsSeen": 234,
  "timeTaken": 11976,
  "changeHistory": [
    "6e04b00df1bf4f0a45571c9fc4361e4e8a05f7ee",
    "2acc50b826fa8b00f2b09d9546c4b3215b89d46d",
    "00eceed233d6e80d5c7137bf5b5286746ec4d5fb",
    "695a402fcad20c711c5d845e0664c43fd6b06286",
    "0f2d1ddc2c41c8db800c58cabb150e71804fe23a",
    "e7c8da614c37e36fb8081234f4c639d6054f6082",
    "b57368b6f893cb27d77fc9425e116f1312f4790f",
    "2a0082c51da7cbe2770eddb5f72cd7f8d72fa5f6",
    "d8736eb9ca351b82854601ea3b1fbc3c9fab44e4",
    "12b5b06c063d93e6c683c9b6fac9a96912f59e59",
    "9729b244de50322c2cc889c97c2ffb2b4675cf77",
    "9673baa7e8b43fa6300080f72ebce0189ea775e5",
    "0ebab3a88a5f172a1180f4e88a91cf6194b273ca",
    "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
    "4551da302d94cffea0313eac79479ab6f9b7cb34",
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
    "df3e1a31582653f1c4e187ae7466aee6cb27e4d4",
    "b3f28dbb3d1ab6b2f686efdd7bdb064426177f21",
    "8dbb5237684bb9de78430b5cef27be40c78a8474",
    "1e346aa829519f8a2aa830e76d9856f914861805",
    "39ce694d05c6d8c428bd87bc1b9c95f94dfdf6fd",
    "1f92266516c882e43fa453b876dd8ca09893c477",
    "0864ef19089f703232107d8aa26c4a7571ff132e",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "6e04b00df1bf4f0a45571c9fc4361e4e8a05f7ee": "Ybodychange",
    "2acc50b826fa8b00f2b09d9546c4b3215b89d46d": "Ybodychange",
    "00eceed233d6e80d5c7137bf5b5286746ec4d5fb": "Ybodychange",
    "695a402fcad20c711c5d845e0664c43fd6b06286": "Ybodychange",
    "0f2d1ddc2c41c8db800c58cabb150e71804fe23a": "Ybodychange",
    "e7c8da614c37e36fb8081234f4c639d6054f6082": "Ybodychange",
    "b57368b6f893cb27d77fc9425e116f1312f4790f": "Ybodychange",
    "2a0082c51da7cbe2770eddb5f72cd7f8d72fa5f6": "Ybodychange",
    "d8736eb9ca351b82854601ea3b1fbc3c9fab44e4": "Ybodychange",
    "12b5b06c063d93e6c683c9b6fac9a96912f59e59": "Ymultichange(Yparameterchange,Ybodychange)",
    "9729b244de50322c2cc889c97c2ffb2b4675cf77": "Ybodychange",
    "9673baa7e8b43fa6300080f72ebce0189ea775e5": "Ybodychange",
    "0ebab3a88a5f172a1180f4e88a91cf6194b273ca": "Ybodychange",
    "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48": "Ybodychange",
    "4551da302d94cffea0313eac79479ab6f9b7cb34": "Ybodychange",
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0": "Ybodychange",
    "df3e1a31582653f1c4e187ae7466aee6cb27e4d4": "Ybodychange",
    "b3f28dbb3d1ab6b2f686efdd7bdb064426177f21": "Ybodychange",
    "8dbb5237684bb9de78430b5cef27be40c78a8474": "Yreturntypechange",
    "1e346aa829519f8a2aa830e76d9856f914861805": "Ymultichange(Ymovefromfile,Ybodychange)",
    "39ce694d05c6d8c428bd87bc1b9c95f94dfdf6fd": "Ymovefromfile",
    "1f92266516c882e43fa453b876dd8ca09893c477": "Ybodychange",
    "0864ef19089f703232107d8aa26c4a7571ff132e": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6e04b00df1bf4f0a45571c9fc4361e4e8a05f7ee": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12288. Fix DataNode\u0027s xceiver count calculation. Contributed by Lisheng Sun.\n",
      "commitDate": "23/05/20 9:58 AM",
      "commitName": "6e04b00df1bf4f0a45571c9fc4361e4e8a05f7ee",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "25/03/20 11:30 AM",
      "commitNameOld": "cdcb77a2c5ca99502d2ac2fbf803f22463eb1343",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 58.94,
      "commitsBetweenForRepo": 203,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n       throws IOException {\n     scheduler.scheduleNextHeartbeat();\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n     \n     final long now \u003d monotonicNow();\n     scheduler.updateLastHeartbeatTime(now);\n     VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n         .getVolumeFailureSummary();\n     int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n         volumeFailureSummary.getFailedStorageLocations().length : 0;\n     final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n     final SlowPeerReports slowPeers \u003d\n         outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n             SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n             SlowPeerReports.EMPTY_REPORT;\n     final SlowDiskReports slowDisks \u003d\n         outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n             SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n             SlowDiskReports.EMPTY_REPORT;\n \n     HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n-        dn.getXceiverCount(),\n+        dn.getActiveTransferThreadCount(),\n         numFailedVolumes,\n         volumeFailureSummary,\n         requestBlockReportLease,\n         slowPeers,\n         slowDisks);\n \n     if (outliersReportDue) {\n       // If the report was due and successfully sent, schedule the next one.\n       scheduler.scheduleNextOutlierReport();\n     }\n \n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n      throws IOException {\n    scheduler.scheduleNextHeartbeat();\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    final long now \u003d monotonicNow();\n    scheduler.updateLastHeartbeatTime(now);\n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n    final SlowPeerReports slowPeers \u003d\n        outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n            SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n            SlowPeerReports.EMPTY_REPORT;\n    final SlowDiskReports slowDisks \u003d\n        outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n            SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n            SlowDiskReports.EMPTY_REPORT;\n\n    HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getActiveTransferThreadCount(),\n        numFailedVolumes,\n        volumeFailureSummary,\n        requestBlockReportLease,\n        slowPeers,\n        slowDisks);\n\n    if (outliersReportDue) {\n      // If the report was due and successfully sent, schedule the next one.\n      scheduler.scheduleNextOutlierReport();\n    }\n\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "2acc50b826fa8b00f2b09d9546c4b3215b89d46d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13165: [SPS]: Collects successfully moved block details via IBR. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "2acc50b826fa8b00f2b09d9546c4b3215b89d46d",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "00eceed233d6e80d5c7137bf5b5286746ec4d5fb",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,45 @@\n   HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n       throws IOException {\n     scheduler.scheduleNextHeartbeat();\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n     \n     final long now \u003d monotonicNow();\n     scheduler.updateLastHeartbeatTime(now);\n     VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n         .getVolumeFailureSummary();\n     int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n         volumeFailureSummary.getFailedStorageLocations().length : 0;\n     final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n     final SlowPeerReports slowPeers \u003d\n         outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n             SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n             SlowPeerReports.EMPTY_REPORT;\n     final SlowDiskReports slowDisks \u003d\n         outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n             SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n             SlowDiskReports.EMPTY_REPORT;\n \n-    // Get the blocks storage move attempt finished blocks\n-    List\u003cBlock\u003e results \u003d dn.getStoragePolicySatisfyWorker()\n-        .getBlocksMovementsStatusHandler().getMoveAttemptFinishedBlocks();\n-    BlocksStorageMoveAttemptFinished storageMoveAttemptFinishedBlks \u003d\n-        getStorageMoveAttemptFinishedBlocks(results);\n-\n     HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         numFailedVolumes,\n         volumeFailureSummary,\n         requestBlockReportLease,\n         slowPeers,\n-        slowDisks,\n-        storageMoveAttemptFinishedBlks);\n+        slowDisks);\n \n     if (outliersReportDue) {\n       // If the report was due and successfully sent, schedule the next one.\n       scheduler.scheduleNextOutlierReport();\n     }\n \n-    // Remove the blocks movement results after successfully transferring\n-    // to namenode.\n-    dn.getStoragePolicySatisfyWorker().getBlocksMovementsStatusHandler()\n-        .remove(results);\n-\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n      throws IOException {\n    scheduler.scheduleNextHeartbeat();\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    final long now \u003d monotonicNow();\n    scheduler.updateLastHeartbeatTime(now);\n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n    final SlowPeerReports slowPeers \u003d\n        outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n            SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n            SlowPeerReports.EMPTY_REPORT;\n    final SlowDiskReports slowDisks \u003d\n        outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n            SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n            SlowDiskReports.EMPTY_REPORT;\n\n    HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary,\n        requestBlockReportLease,\n        slowPeers,\n        slowDisks);\n\n    if (outliersReportDue) {\n      // If the report was due and successfully sent, schedule the next one.\n      scheduler.scheduleNextOutlierReport();\n    }\n\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "00eceed233d6e80d5c7137bf5b5286746ec4d5fb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12570: [SPS]: Refactor Co-ordinator datanode logic to track the block storage movements. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "00eceed233d6e80d5c7137bf5b5286746ec4d5fb",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:05 AM",
      "commitNameOld": "695a402fcad20c711c5d845e0664c43fd6b06286",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,57 @@\n   HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n       throws IOException {\n     scheduler.scheduleNextHeartbeat();\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n     \n     final long now \u003d monotonicNow();\n     scheduler.updateLastHeartbeatTime(now);\n     VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n         .getVolumeFailureSummary();\n     int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n         volumeFailureSummary.getFailedStorageLocations().length : 0;\n     final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n     final SlowPeerReports slowPeers \u003d\n         outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n             SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n             SlowPeerReports.EMPTY_REPORT;\n     final SlowDiskReports slowDisks \u003d\n         outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n             SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n             SlowDiskReports.EMPTY_REPORT;\n \n-    BlocksStorageMovementResult[] blksMovementResults \u003d\n-        getBlocksMovementResults();\n+    // Get the blocks storage move attempt finished blocks\n+    List\u003cBlock\u003e results \u003d dn.getStoragePolicySatisfyWorker()\n+        .getBlocksMovementsStatusHandler().getMoveAttemptFinishedBlocks();\n+    BlocksStorageMoveAttemptFinished storageMoveAttemptFinishedBlks \u003d\n+        getStorageMoveAttemptFinishedBlocks(results);\n \n     HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         numFailedVolumes,\n         volumeFailureSummary,\n         requestBlockReportLease,\n         slowPeers,\n         slowDisks,\n-        blksMovementResults);\n+        storageMoveAttemptFinishedBlks);\n \n     if (outliersReportDue) {\n       // If the report was due and successfully sent, schedule the next one.\n       scheduler.scheduleNextOutlierReport();\n     }\n \n     // Remove the blocks movement results after successfully transferring\n     // to namenode.\n     dn.getStoragePolicySatisfyWorker().getBlocksMovementsStatusHandler()\n-        .remove(blksMovementResults);\n+        .remove(results);\n \n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n      throws IOException {\n    scheduler.scheduleNextHeartbeat();\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    final long now \u003d monotonicNow();\n    scheduler.updateLastHeartbeatTime(now);\n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n    final SlowPeerReports slowPeers \u003d\n        outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n            SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n            SlowPeerReports.EMPTY_REPORT;\n    final SlowDiskReports slowDisks \u003d\n        outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n            SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n            SlowDiskReports.EMPTY_REPORT;\n\n    // Get the blocks storage move attempt finished blocks\n    List\u003cBlock\u003e results \u003d dn.getStoragePolicySatisfyWorker()\n        .getBlocksMovementsStatusHandler().getMoveAttemptFinishedBlocks();\n    BlocksStorageMoveAttemptFinished storageMoveAttemptFinishedBlks \u003d\n        getStorageMoveAttemptFinishedBlocks(results);\n\n    HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary,\n        requestBlockReportLease,\n        slowPeers,\n        slowDisks,\n        storageMoveAttemptFinishedBlks);\n\n    if (outliersReportDue) {\n      // If the report was due and successfully sent, schedule the next one.\n      scheduler.scheduleNextOutlierReport();\n    }\n\n    // Remove the blocks movement results after successfully transferring\n    // to namenode.\n    dn.getStoragePolicySatisfyWorker().getBlocksMovementsStatusHandler()\n        .remove(results);\n\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "695a402fcad20c711c5d845e0664c43fd6b06286": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11334: [SPS]: NN switch and rescheduling movements can lead to have more than one coordinator for same file blocks. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:05 AM",
      "commitName": "695a402fcad20c711c5d845e0664c43fd6b06286",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:05 AM",
      "commitNameOld": "0f2d1ddc2c41c8db800c58cabb150e71804fe23a",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n       throws IOException {\n     scheduler.scheduleNextHeartbeat();\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n     \n     final long now \u003d monotonicNow();\n     scheduler.updateLastHeartbeatTime(now);\n     VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n         .getVolumeFailureSummary();\n     int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n         volumeFailureSummary.getFailedStorageLocations().length : 0;\n     final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n     final SlowPeerReports slowPeers \u003d\n         outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n             SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n             SlowPeerReports.EMPTY_REPORT;\n     final SlowDiskReports slowDisks \u003d\n         outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n             SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n             SlowDiskReports.EMPTY_REPORT;\n \n     BlocksStorageMovementResult[] blksMovementResults \u003d\n         getBlocksMovementResults();\n \n     HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         numFailedVolumes,\n         volumeFailureSummary,\n         requestBlockReportLease,\n         slowPeers,\n         slowDisks,\n         blksMovementResults);\n \n     if (outliersReportDue) {\n       // If the report was due and successfully sent, schedule the next one.\n       scheduler.scheduleNextOutlierReport();\n     }\n \n     // Remove the blocks movement results after successfully transferring\n     // to namenode.\n-    dn.getStoragePolicySatisfyWorker().getBlocksMovementsCompletionHandler()\n+    dn.getStoragePolicySatisfyWorker().getBlocksMovementsStatusHandler()\n         .remove(blksMovementResults);\n \n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n      throws IOException {\n    scheduler.scheduleNextHeartbeat();\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    final long now \u003d monotonicNow();\n    scheduler.updateLastHeartbeatTime(now);\n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n    final SlowPeerReports slowPeers \u003d\n        outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n            SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n            SlowPeerReports.EMPTY_REPORT;\n    final SlowDiskReports slowDisks \u003d\n        outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n            SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n            SlowDiskReports.EMPTY_REPORT;\n\n    BlocksStorageMovementResult[] blksMovementResults \u003d\n        getBlocksMovementResults();\n\n    HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary,\n        requestBlockReportLease,\n        slowPeers,\n        slowDisks,\n        blksMovementResults);\n\n    if (outliersReportDue) {\n      // If the report was due and successfully sent, schedule the next one.\n      scheduler.scheduleNextOutlierReport();\n    }\n\n    // Remove the blocks movement results after successfully transferring\n    // to namenode.\n    dn.getStoragePolicySatisfyWorker().getBlocksMovementsStatusHandler()\n        .remove(blksMovementResults);\n\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "0f2d1ddc2c41c8db800c58cabb150e71804fe23a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10954. [SPS]: Provide mechanism to send blocks movement result back to NN from coordinator DN. Contributed by Rakesh R\n",
      "commitDate": "12/08/18 3:05 AM",
      "commitName": "0f2d1ddc2c41c8db800c58cabb150e71804fe23a",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "12/06/17 6:45 PM",
      "commitNameOld": "bec79ca2495abdc347d64628151c90f5ce777046",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 425.35,
      "commitsBetweenForRepo": 3610,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,54 @@\n   HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n       throws IOException {\n     scheduler.scheduleNextHeartbeat();\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n     \n     final long now \u003d monotonicNow();\n     scheduler.updateLastHeartbeatTime(now);\n     VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n         .getVolumeFailureSummary();\n     int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n         volumeFailureSummary.getFailedStorageLocations().length : 0;\n     final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n     final SlowPeerReports slowPeers \u003d\n         outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n             SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n             SlowPeerReports.EMPTY_REPORT;\n     final SlowDiskReports slowDisks \u003d\n         outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n             SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n             SlowDiskReports.EMPTY_REPORT;\n+\n+    BlocksStorageMovementResult[] blksMovementResults \u003d\n+        getBlocksMovementResults();\n+\n     HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         numFailedVolumes,\n         volumeFailureSummary,\n         requestBlockReportLease,\n         slowPeers,\n-        slowDisks);\n+        slowDisks,\n+        blksMovementResults);\n \n     if (outliersReportDue) {\n       // If the report was due and successfully sent, schedule the next one.\n       scheduler.scheduleNextOutlierReport();\n     }\n+\n+    // Remove the blocks movement results after successfully transferring\n+    // to namenode.\n+    dn.getStoragePolicySatisfyWorker().getBlocksMovementsCompletionHandler()\n+        .remove(blksMovementResults);\n+\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n      throws IOException {\n    scheduler.scheduleNextHeartbeat();\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    final long now \u003d monotonicNow();\n    scheduler.updateLastHeartbeatTime(now);\n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n    final SlowPeerReports slowPeers \u003d\n        outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n            SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n            SlowPeerReports.EMPTY_REPORT;\n    final SlowDiskReports slowDisks \u003d\n        outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n            SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n            SlowDiskReports.EMPTY_REPORT;\n\n    BlocksStorageMovementResult[] blksMovementResults \u003d\n        getBlocksMovementResults();\n\n    HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary,\n        requestBlockReportLease,\n        slowPeers,\n        slowDisks,\n        blksMovementResults);\n\n    if (outliersReportDue) {\n      // If the report was due and successfully sent, schedule the next one.\n      scheduler.scheduleNextOutlierReport();\n    }\n\n    // Remove the blocks movement results after successfully transferring\n    // to namenode.\n    dn.getStoragePolicySatisfyWorker().getBlocksMovementsCompletionHandler()\n        .remove(blksMovementResults);\n\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "e7c8da614c37e36fb8081234f4c639d6054f6082": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11545. Propagate DataNode\u0027s slow disks info to the NameNode via Heartbeats. Contributed by Hanisha Koneru.\n",
      "commitDate": "20/03/17 9:54 PM",
      "commitName": "e7c8da614c37e36fb8081234f4c639d6054f6082",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "02/03/17 12:45 PM",
      "commitNameOld": "b3ec531f400dd0a6506dc71233d38ae57b764a43",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 18.34,
      "commitsBetweenForRepo": 110,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,43 @@\n   HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n       throws IOException {\n     scheduler.scheduleNextHeartbeat();\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n     \n     final long now \u003d monotonicNow();\n     scheduler.updateLastHeartbeatTime(now);\n     VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n         .getVolumeFailureSummary();\n     int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n         volumeFailureSummary.getFailedStorageLocations().length : 0;\n-    final boolean slowPeersReportDue \u003d scheduler.isSlowPeersReportDue(now);\n+    final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n     final SlowPeerReports slowPeers \u003d\n-        slowPeersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n+        outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n             SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n             SlowPeerReports.EMPTY_REPORT;\n+    final SlowDiskReports slowDisks \u003d\n+        outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n+            SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n+            SlowDiskReports.EMPTY_REPORT;\n     HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         numFailedVolumes,\n         volumeFailureSummary,\n         requestBlockReportLease,\n-        slowPeers);\n+        slowPeers,\n+        slowDisks);\n \n-    if (slowPeersReportDue) {\n+    if (outliersReportDue) {\n       // If the report was due and successfully sent, schedule the next one.\n-      scheduler.scheduleNextSlowPeerReport();\n+      scheduler.scheduleNextOutlierReport();\n     }\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n      throws IOException {\n    scheduler.scheduleNextHeartbeat();\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    final long now \u003d monotonicNow();\n    scheduler.updateLastHeartbeatTime(now);\n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    final boolean outliersReportDue \u003d scheduler.isOutliersReportDue(now);\n    final SlowPeerReports slowPeers \u003d\n        outliersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n            SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n            SlowPeerReports.EMPTY_REPORT;\n    final SlowDiskReports slowDisks \u003d\n        outliersReportDue \u0026\u0026 dn.getDiskMetrics() !\u003d null ?\n            SlowDiskReports.create(dn.getDiskMetrics().getDiskOutliersStats()) :\n            SlowDiskReports.EMPTY_REPORT;\n    HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary,\n        requestBlockReportLease,\n        slowPeers,\n        slowDisks);\n\n    if (outliersReportDue) {\n      // If the report was due and successfully sent, schedule the next one.\n      scheduler.scheduleNextOutlierReport();\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "b57368b6f893cb27d77fc9425e116f1312f4790f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11194. Maintain aggregated peer performance metrics on NameNode.\n",
      "commitDate": "24/01/17 4:58 PM",
      "commitName": "b57368b6f893cb27d77fc9425e116f1312f4790f",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "16/12/16 9:46 AM",
      "commitNameOld": "a95639068c99ebcaefe8b6c4268449d12a6577d6",
      "commitAuthorOld": "Anu Engineer",
      "daysBetweenCommits": 39.3,
      "commitsBetweenForRepo": 176,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,38 @@\n   HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n       throws IOException {\n     scheduler.scheduleNextHeartbeat();\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n     \n-    scheduler.updateLastHeartbeatTime(monotonicNow());\n+    final long now \u003d monotonicNow();\n+    scheduler.updateLastHeartbeatTime(now);\n     VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n         .getVolumeFailureSummary();\n     int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n         volumeFailureSummary.getFailedStorageLocations().length : 0;\n-    return bpNamenode.sendHeartbeat(bpRegistration,\n+    final boolean slowPeersReportDue \u003d scheduler.isSlowPeersReportDue(now);\n+    final SlowPeerReports slowPeers \u003d\n+        slowPeersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n+            SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n+            SlowPeerReports.EMPTY_REPORT;\n+    HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         numFailedVolumes,\n         volumeFailureSummary,\n-        requestBlockReportLease);\n+        requestBlockReportLease,\n+        slowPeers);\n+\n+    if (slowPeersReportDue) {\n+      // If the report was due and successfully sent, schedule the next one.\n+      scheduler.scheduleNextSlowPeerReport();\n+    }\n+    return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n      throws IOException {\n    scheduler.scheduleNextHeartbeat();\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    final long now \u003d monotonicNow();\n    scheduler.updateLastHeartbeatTime(now);\n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    final boolean slowPeersReportDue \u003d scheduler.isSlowPeersReportDue(now);\n    final SlowPeerReports slowPeers \u003d\n        slowPeersReportDue \u0026\u0026 dn.getPeerMetrics() !\u003d null ?\n            SlowPeerReports.create(dn.getPeerMetrics().getOutliers()) :\n            SlowPeerReports.EMPTY_REPORT;\n    HeartbeatResponse response \u003d bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary,\n        requestBlockReportLease,\n        slowPeers);\n\n    if (slowPeersReportDue) {\n      // If the report was due and successfully sent, schedule the next one.\n      scheduler.scheduleNextSlowPeerReport();\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "2a0082c51da7cbe2770eddb5f72cd7f8d72fa5f6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10440. Improve DataNode web UI (Contributed by Weiwei Yang)\n",
      "commitDate": "28/06/16 4:19 AM",
      "commitName": "2a0082c51da7cbe2770eddb5f72cd7f8d72fa5f6",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "04/04/16 6:49 PM",
      "commitNameOld": "818d6b799eead13a17a0214172df60a269b046fb",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 84.4,
      "commitsBetweenForRepo": 582,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,25 @@\n   HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n       throws IOException {\n     scheduler.scheduleNextHeartbeat();\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n     \n+    scheduler.updateLastHeartbeatTime(monotonicNow());\n     VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n         .getVolumeFailureSummary();\n     int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n         volumeFailureSummary.getFailedStorageLocations().length : 0;\n     return bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         numFailedVolumes,\n         volumeFailureSummary,\n         requestBlockReportLease);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n      throws IOException {\n    scheduler.scheduleNextHeartbeat();\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    scheduler.updateLastHeartbeatTime(monotonicNow());\n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary,\n        requestBlockReportLease);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "d8736eb9ca351b82854601ea3b1fbc3c9fab44e4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9305. Delayed heartbeat processing causes storm of subsequent heartbeats. (Contributed by Arpit Agarwal)\n",
      "commitDate": "26/10/15 3:54 PM",
      "commitName": "d8736eb9ca351b82854601ea3b1fbc3c9fab44e4",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "01/09/15 5:58 PM",
      "commitNameOld": "5652131d2ea68c408dd3cd8bee31723642a8cdde",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 54.91,
      "commitsBetweenForRepo": 418,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n       throws IOException {\n+    scheduler.scheduleNextHeartbeat();\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n     \n     VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n         .getVolumeFailureSummary();\n     int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n         volumeFailureSummary.getFailedStorageLocations().length : 0;\n     return bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         numFailedVolumes,\n         volumeFailureSummary,\n         requestBlockReportLease);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n      throws IOException {\n    scheduler.scheduleNextHeartbeat();\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary,\n        requestBlockReportLease);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "12b5b06c063d93e6c683c9b6fac9a96912f59e59": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7923. The DataNodes should rate-limit their full block reports by asking the NN on heartbeat messages (cmccabe)\n",
      "commitDate": "12/06/15 11:17 AM",
      "commitName": "12b5b06c063d93e6c683c9b6fac9a96912f59e59",
      "commitAuthor": "Colin Patrick Mccabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7923. The DataNodes should rate-limit their full block reports by asking the NN on heartbeat messages (cmccabe)\n",
          "commitDate": "12/06/15 11:17 AM",
          "commitName": "12b5b06c063d93e6c683c9b6fac9a96912f59e59",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "19/05/15 10:50 AM",
          "commitNameOld": "470c87dbc6c24dd3b370f1ad9e7ab1f6dabd2080",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 24.02,
          "commitsBetweenForRepo": 173,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,23 @@\n-  HeartbeatResponse sendHeartBeat() throws IOException {\n+  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n+      throws IOException {\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n     \n     VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n         .getVolumeFailureSummary();\n     int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n         volumeFailureSummary.getFailedStorageLocations().length : 0;\n     return bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         numFailedVolumes,\n-        volumeFailureSummary);\n+        volumeFailureSummary,\n+        requestBlockReportLease);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n      throws IOException {\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary,\n        requestBlockReportLease);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[requestBlockReportLease-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7923. The DataNodes should rate-limit their full block reports by asking the NN on heartbeat messages (cmccabe)\n",
          "commitDate": "12/06/15 11:17 AM",
          "commitName": "12b5b06c063d93e6c683c9b6fac9a96912f59e59",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "19/05/15 10:50 AM",
          "commitNameOld": "470c87dbc6c24dd3b370f1ad9e7ab1f6dabd2080",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 24.02,
          "commitsBetweenForRepo": 173,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,23 @@\n-  HeartbeatResponse sendHeartBeat() throws IOException {\n+  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n+      throws IOException {\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n     \n     VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n         .getVolumeFailureSummary();\n     int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n         volumeFailureSummary.getFailedStorageLocations().length : 0;\n     return bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         numFailedVolumes,\n-        volumeFailureSummary);\n+        volumeFailureSummary,\n+        requestBlockReportLease);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n      throws IOException {\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary,\n        requestBlockReportLease);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
          "extendedDetails": {}
        }
      ]
    },
    "9729b244de50322c2cc889c97c2ffb2b4675cf77": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7604. Track and display failed DataNode storage locations in NameNode. Contributed by Chris Nauroth.\n",
      "commitDate": "16/02/15 2:43 PM",
      "commitName": "9729b244de50322c2cc889c97c2ffb2b4675cf77",
      "commitAuthor": "cnauroth",
      "commitDateOld": "12/02/15 7:15 AM",
      "commitNameOld": "38262779bbf38a427bad6d044e91165567f1d206",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 4.31,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,21 @@\n   HeartbeatResponse sendHeartBeat() throws IOException {\n     StorageReport[] reports \u003d\n         dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat with \" + reports.length +\n                 \" storage reports from service actor: \" + this);\n     }\n-\n+    \n+    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n+        .getVolumeFailureSummary();\n+    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n+        volumeFailureSummary.getFailedStorageLocations().length : 0;\n     return bpNamenode.sendHeartbeat(bpRegistration,\n         reports,\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n-        dn.getFSDataset().getNumFailedVolumes());\n+        numFailedVolumes,\n+        volumeFailureSummary);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n    \n    VolumeFailureSummary volumeFailureSummary \u003d dn.getFSDataset()\n        .getVolumeFailureSummary();\n    int numFailedVolumes \u003d volumeFailureSummary !\u003d null ?\n        volumeFailureSummary.getFailedStorageLocations().length : 0;\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        numFailedVolumes,\n        volumeFailureSummary);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "9673baa7e8b43fa6300080f72ebce0189ea775e5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5320. Add datanode caching metrics. Contributed by Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1540796 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/11/13 10:30 AM",
      "commitName": "9673baa7e8b43fa6300080f72ebce0189ea775e5",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "21/10/13 12:29 PM",
      "commitNameOld": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 20.96,
      "commitsBetweenForRepo": 85,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   HeartbeatResponse sendHeartBeat() throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat from service actor: \" + this);\n     }\n     // reports number of failed volumes\n     StorageReport[] report \u003d { new StorageReport(bpRegistration.getStorageID(),\n         false,\n         dn.getFSDataset().getCapacity(),\n         dn.getFSDataset().getDfsUsed(),\n         dn.getFSDataset().getRemaining(),\n         dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n     return bpNamenode.sendHeartbeat(bpRegistration, report,\n-        dn.getFSDataset().getDnCacheCapacity(),\n-        dn.getFSDataset().getDnCacheUsed(),\n+        dn.getFSDataset().getCacheCapacity(),\n+        dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         dn.getFSDataset().getNumFailedVolumes());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat from service actor: \" + this);\n    }\n    // reports number of failed volumes\n    StorageReport[] report \u003d { new StorageReport(bpRegistration.getStorageID(),\n        false,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n    return bpNamenode.sendHeartbeat(bpRegistration, report,\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        dn.getFSDataset().getNumFailedVolumes());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "0ebab3a88a5f172a1180f4e88a91cf6194b273ca": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5377. Heartbeats from Datandode should include one storage report per storage directory\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1534464 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/10/13 7:22 PM",
      "commitName": "0ebab3a88a5f172a1180f4e88a91cf6194b273ca",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "27/09/13 9:05 AM",
      "commitNameOld": "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 24.43,
      "commitsBetweenForRepo": 174,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,14 @@\n   HeartbeatResponse sendHeartBeat() throws IOException {\n+    StorageReport[] reports \u003d\n+        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Sending heartbeat from service actor: \" + this);\n+      LOG.debug(\"Sending heartbeat with \" + reports.length +\n+                \" storage reports from service actor: \" + this);\n     }\n-    // reports number of failed volumes\n-    StorageReport[] report \u003d { new StorageReport(bpRegistration.getDatanodeUuid(),\n-        false,\n-        dn.getFSDataset().getCapacity(),\n-        dn.getFSDataset().getDfsUsed(),\n-        dn.getFSDataset().getRemaining(),\n-        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n-    return bpNamenode.sendHeartbeat(bpRegistration, report,\n+\n+    return bpNamenode.sendHeartbeat(bpRegistration,\n+        reports,\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         dn.getFSDataset().getNumFailedVolumes());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    StorageReport[] reports \u003d\n        dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat with \" + reports.length +\n                \" storage reports from service actor: \" + this);\n    }\n\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        reports,\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        dn.getFSDataset().getNumFailedVolumes());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5378. In CacheReport, don\u0027t send genstamp and length on the wire (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1534334 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/10/13 12:29 PM",
      "commitName": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "16/09/13 11:41 AM",
      "commitNameOld": "85c203602993a946fb5f41eadf1cf1484a0ce686",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 35.03,
      "commitsBetweenForRepo": 245,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,18 @@\n   HeartbeatResponse sendHeartBeat() throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat from service actor: \" + this);\n     }\n     // reports number of failed volumes\n     StorageReport[] report \u003d { new StorageReport(bpRegistration.getStorageID(),\n         false,\n         dn.getFSDataset().getCapacity(),\n         dn.getFSDataset().getDfsUsed(),\n         dn.getFSDataset().getRemaining(),\n         dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n-    CacheReport[] cacheReport \u003d { new CacheReport(\n-        dn.getFSDataset().getCacheCapacity(),\n-        dn.getFSDataset().getCacheUsed()) };\n-    return bpNamenode.sendHeartbeat(bpRegistration, report, cacheReport,\n+    return bpNamenode.sendHeartbeat(bpRegistration, report,\n+        dn.getFSDataset().getDnCacheCapacity(),\n+        dn.getFSDataset().getDnCacheUsed(),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         dn.getFSDataset().getNumFailedVolumes());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat from service actor: \" + this);\n    }\n    // reports number of failed volumes\n    StorageReport[] report \u003d { new StorageReport(bpRegistration.getStorageID(),\n        false,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n    return bpNamenode.sendHeartbeat(bpRegistration, report,\n        dn.getFSDataset().getDnCacheCapacity(),\n        dn.getFSDataset().getDnCacheUsed(),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        dn.getFSDataset().getNumFailedVolumes());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "4551da302d94cffea0313eac79479ab6f9b7cb34": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5233. Use Datanode UUID to identify Datanodes.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1525407 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/09/13 11:03 AM",
      "commitName": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "25/08/13 8:18 PM",
      "commitNameOld": "73d14311bc847a29c2b8ec30bbfbaf59cd3cb713",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 27.61,
      "commitsBetweenForRepo": 131,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   HeartbeatResponse sendHeartBeat() throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat from service actor: \" + this);\n     }\n     // reports number of failed volumes\n-    StorageReport[] report \u003d { new StorageReport(bpRegistration.getStorageID(),\n+    StorageReport[] report \u003d { new StorageReport(bpRegistration.getDatanodeUuid(),\n         false,\n         dn.getFSDataset().getCapacity(),\n         dn.getFSDataset().getDfsUsed(),\n         dn.getFSDataset().getRemaining(),\n         dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n     return bpNamenode.sendHeartbeat(bpRegistration, report,\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         dn.getFSDataset().getNumFailedVolumes());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat from service actor: \" + this);\n    }\n    // reports number of failed volumes\n    StorageReport[] report \u003d { new StorageReport(bpRegistration.getDatanodeUuid(),\n        false,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n    return bpNamenode.sendHeartbeat(bpRegistration, report,\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        dn.getFSDataset().getNumFailedVolumes());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5141. Add cache status information to datanode heartbeat. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1519101 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/08/13 3:15 PM",
      "commitName": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "23/08/13 8:41 PM",
      "commitNameOld": "b992219fa13ccee2b417d91222fd0c3e8c3ffe11",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 6.77,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,19 @@\n   HeartbeatResponse sendHeartBeat() throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Sending heartbeat from service actor: \" + this);\n     }\n     // reports number of failed volumes\n     StorageReport[] report \u003d { new StorageReport(bpRegistration.getStorageID(),\n         false,\n         dn.getFSDataset().getCapacity(),\n         dn.getFSDataset().getDfsUsed(),\n         dn.getFSDataset().getRemaining(),\n         dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n-    return bpNamenode.sendHeartbeat(bpRegistration, report,\n+    CacheReport[] cacheReport \u003d { new CacheReport(\n+        dn.getFSDataset().getCacheCapacity(),\n+        dn.getFSDataset().getCacheUsed()) };\n+    return bpNamenode.sendHeartbeat(bpRegistration, report, cacheReport,\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         dn.getFSDataset().getNumFailedVolumes());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat from service actor: \" + this);\n    }\n    // reports number of failed volumes\n    StorageReport[] report \u003d { new StorageReport(bpRegistration.getStorageID(),\n        false,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n    CacheReport[] cacheReport \u003d { new CacheReport(\n        dn.getFSDataset().getCacheCapacity(),\n        dn.getFSDataset().getCacheUsed()) };\n    return bpNamenode.sendHeartbeat(bpRegistration, report, cacheReport,\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        dn.getFSDataset().getNumFailedVolumes());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "df3e1a31582653f1c4e187ae7466aee6cb27e4d4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3240. Drop log level of \"heartbeat: ...\" in BPServiceActor to DEBUG. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1311577 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/04/12 10:12 PM",
      "commitName": "df3e1a31582653f1c4e187ae7466aee6cb27e4d4",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "31/03/12 8:41 PM",
      "commitNameOld": "0663dbaac0a19719ddf9cd4290ba893bfca69da2",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 9.06,
      "commitsBetweenForRepo": 94,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,16 @@\n   HeartbeatResponse sendHeartBeat() throws IOException {\n-    LOG.info(\"heartbeat: \" + this);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Sending heartbeat from service actor: \" + this);\n+    }\n     // reports number of failed volumes\n     StorageReport[] report \u003d { new StorageReport(bpRegistration.getStorageID(),\n         false,\n         dn.getFSDataset().getCapacity(),\n         dn.getFSDataset().getDfsUsed(),\n         dn.getFSDataset().getRemaining(),\n         dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n     return bpNamenode.sendHeartbeat(bpRegistration, report,\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(),\n         dn.getFSDataset().getNumFailedVolumes());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Sending heartbeat from service actor: \" + this);\n    }\n    // reports number of failed volumes\n    StorageReport[] report \u003d { new StorageReport(bpRegistration.getStorageID(),\n        false,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId())) };\n    return bpNamenode.sendHeartbeat(bpRegistration, report,\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(),\n        dn.getFSDataset().getNumFailedVolumes());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "b3f28dbb3d1ab6b2f686efdd7bdb064426177f21": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2626. BPOfferService.verifyAndSetNamespaceInfo needs to be synchronized. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1210340 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/12/11 10:36 PM",
      "commitName": "b3f28dbb3d1ab6b2f686efdd7bdb064426177f21",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "01/12/11 12:03 AM",
      "commitNameOld": "8dbb5237684bb9de78430b5cef27be40c78a8474",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 3.94,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,10 @@\n   HeartbeatResponse sendHeartBeat() throws IOException {\n     LOG.info(\"heartbeat: \" + this);\n-    // TODO: saw an NPE here - maybe if the two BPOS register at\n-    // same time, this one won\u0027t block on the other one?\n     return bpNamenode.sendHeartbeat(bpRegistration,\n         dn.getFSDataset().getCapacity(),\n         dn.getFSDataset().getDfsUsed(),\n         dn.getFSDataset().getRemaining(),\n         dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId()),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(), dn.getFSDataset().getNumFailedVolumes());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    LOG.info(\"heartbeat: \" + this);\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId()),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(), dn.getFSDataset().getNumFailedVolumes());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "8dbb5237684bb9de78430b5cef27be40c78a8474": {
      "type": "Yreturntypechange",
      "commitMessage": "HDFS-2616. Change DatanodeProtocol#sendHeartbeat() to return HeartbeatResponse. (suresh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1208987 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/12/11 12:03 AM",
      "commitName": "8dbb5237684bb9de78430b5cef27be40c78a8474",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "30/11/11 5:10 PM",
      "commitNameOld": "1e346aa829519f8a2aa830e76d9856f914861805",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.29,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,12 @@\n-  DatanodeCommand [] sendHeartBeat() throws IOException {\n+  HeartbeatResponse sendHeartBeat() throws IOException {\n     LOG.info(\"heartbeat: \" + this);\n     // TODO: saw an NPE here - maybe if the two BPOS register at\n     // same time, this one won\u0027t block on the other one?\n     return bpNamenode.sendHeartbeat(bpRegistration,\n         dn.getFSDataset().getCapacity(),\n         dn.getFSDataset().getDfsUsed(),\n         dn.getFSDataset().getRemaining(),\n         dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId()),\n         dn.getXmitsInProgress(),\n         dn.getXceiverCount(), dn.getFSDataset().getNumFailedVolumes());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HeartbeatResponse sendHeartBeat() throws IOException {\n    LOG.info(\"heartbeat: \" + this);\n    // TODO: saw an NPE here - maybe if the two BPOS register at\n    // same time, this one won\u0027t block on the other one?\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId()),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(), dn.getFSDataset().getNumFailedVolumes());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {
        "oldValue": "DatanodeCommand[]",
        "newValue": "HeartbeatResponse"
      }
    },
    "1e346aa829519f8a2aa830e76d9856f914861805": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-1971. Send block report from datanode to both active and standby namenodes. (sanjay, todd via suresh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1208925 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/11/11 5:10 PM",
      "commitName": "1e346aa829519f8a2aa830e76d9856f914861805",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-1971. Send block report from datanode to both active and standby namenodes. (sanjay, todd via suresh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1208925 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/11/11 5:10 PM",
          "commitName": "1e346aa829519f8a2aa830e76d9856f914861805",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "30/11/11 1:46 PM",
          "commitNameOld": "f87a4b40bc99e76602a75906df31747cfdbff78a",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,12 @@\n   DatanodeCommand [] sendHeartBeat() throws IOException {\n+    LOG.info(\"heartbeat: \" + this);\n+    // TODO: saw an NPE here - maybe if the two BPOS register at\n+    // same time, this one won\u0027t block on the other one?\n     return bpNamenode.sendHeartbeat(bpRegistration,\n-        dn.data.getCapacity(),\n-        dn.data.getDfsUsed(),\n-        dn.data.getRemaining(),\n-        dn.data.getBlockPoolUsed(getBlockPoolId()),\n-        dn.xmitsInProgress.get(),\n-        dn.getXceiverCount(), dn.data.getNumFailedVolumes());\n+        dn.getFSDataset().getCapacity(),\n+        dn.getFSDataset().getDfsUsed(),\n+        dn.getFSDataset().getRemaining(),\n+        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId()),\n+        dn.getXmitsInProgress(),\n+        dn.getXceiverCount(), dn.getFSDataset().getNumFailedVolumes());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeCommand [] sendHeartBeat() throws IOException {\n    LOG.info(\"heartbeat: \" + this);\n    // TODO: saw an NPE here - maybe if the two BPOS register at\n    // same time, this one won\u0027t block on the other one?\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId()),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(), dn.getFSDataset().getNumFailedVolumes());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
            "oldMethodName": "sendHeartBeat",
            "newMethodName": "sendHeartBeat"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-1971. Send block report from datanode to both active and standby namenodes. (sanjay, todd via suresh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1208925 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/11/11 5:10 PM",
          "commitName": "1e346aa829519f8a2aa830e76d9856f914861805",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "30/11/11 1:46 PM",
          "commitNameOld": "f87a4b40bc99e76602a75906df31747cfdbff78a",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,12 @@\n   DatanodeCommand [] sendHeartBeat() throws IOException {\n+    LOG.info(\"heartbeat: \" + this);\n+    // TODO: saw an NPE here - maybe if the two BPOS register at\n+    // same time, this one won\u0027t block on the other one?\n     return bpNamenode.sendHeartbeat(bpRegistration,\n-        dn.data.getCapacity(),\n-        dn.data.getDfsUsed(),\n-        dn.data.getRemaining(),\n-        dn.data.getBlockPoolUsed(getBlockPoolId()),\n-        dn.xmitsInProgress.get(),\n-        dn.getXceiverCount(), dn.data.getNumFailedVolumes());\n+        dn.getFSDataset().getCapacity(),\n+        dn.getFSDataset().getDfsUsed(),\n+        dn.getFSDataset().getRemaining(),\n+        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId()),\n+        dn.getXmitsInProgress(),\n+        dn.getXceiverCount(), dn.getFSDataset().getNumFailedVolumes());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeCommand [] sendHeartBeat() throws IOException {\n    LOG.info(\"heartbeat: \" + this);\n    // TODO: saw an NPE here - maybe if the two BPOS register at\n    // same time, this one won\u0027t block on the other one?\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        dn.getFSDataset().getCapacity(),\n        dn.getFSDataset().getDfsUsed(),\n        dn.getFSDataset().getRemaining(),\n        dn.getFSDataset().getBlockPoolUsed(bpos.getBlockPoolId()),\n        dn.getXmitsInProgress(),\n        dn.getXceiverCount(), dn.getFSDataset().getNumFailedVolumes());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
          "extendedDetails": {}
        }
      ]
    },
    "39ce694d05c6d8c428bd87bc1b9c95f94dfdf6fd": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-2566. Move BPOfferService to be a non-inner class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1204659 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/11 11:27 AM",
      "commitName": "39ce694d05c6d8c428bd87bc1b9c95f94dfdf6fd",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "21/11/11 11:03 AM",
      "commitNameOld": "68173af69d2fbda3292404c90a5077483e14d6f4",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,9 @@\n-    DatanodeCommand [] sendHeartBeat() throws IOException {\n-      return bpNamenode.sendHeartbeat(bpRegistration,\n-          dn.data.getCapacity(),\n-          dn.data.getDfsUsed(),\n-          dn.data.getRemaining(),\n-          dn.data.getBlockPoolUsed(getBlockPoolId()),\n-          dn.xmitsInProgress.get(),\n-          dn.getXceiverCount(), dn.data.getNumFailedVolumes());\n-    }\n\\ No newline at end of file\n+  DatanodeCommand [] sendHeartBeat() throws IOException {\n+    return bpNamenode.sendHeartbeat(bpRegistration,\n+        dn.data.getCapacity(),\n+        dn.data.getDfsUsed(),\n+        dn.data.getRemaining(),\n+        dn.data.getBlockPoolUsed(getBlockPoolId()),\n+        dn.xmitsInProgress.get(),\n+        dn.getXceiverCount(), dn.data.getNumFailedVolumes());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand [] sendHeartBeat() throws IOException {\n    return bpNamenode.sendHeartbeat(bpRegistration,\n        dn.data.getCapacity(),\n        dn.data.getDfsUsed(),\n        dn.data.getRemaining(),\n        dn.data.getBlockPoolUsed(getBlockPoolId()),\n        dn.xmitsInProgress.get(),\n        dn.getXceiverCount(), dn.data.getNumFailedVolumes());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
        "oldMethodName": "sendHeartBeat",
        "newMethodName": "sendHeartBeat"
      }
    },
    "1f92266516c882e43fa453b876dd8ca09893c477": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2563. Some cleanup in BPOfferService. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203943 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 5:31 PM",
      "commitName": "1f92266516c882e43fa453b876dd8ca09893c477",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "18/11/11 1:04 AM",
      "commitNameOld": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.69,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,9 @@\n     DatanodeCommand [] sendHeartBeat() throws IOException {\n       return bpNamenode.sendHeartbeat(bpRegistration,\n           dn.data.getCapacity(),\n           dn.data.getDfsUsed(),\n           dn.data.getRemaining(),\n-          dn.data.getBlockPoolUsed(blockPoolId),\n+          dn.data.getBlockPoolUsed(getBlockPoolId()),\n           dn.xmitsInProgress.get(),\n           dn.getXceiverCount(), dn.data.getNumFailedVolumes());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    DatanodeCommand [] sendHeartBeat() throws IOException {\n      return bpNamenode.sendHeartbeat(bpRegistration,\n          dn.data.getCapacity(),\n          dn.data.getDfsUsed(),\n          dn.data.getRemaining(),\n          dn.data.getBlockPoolUsed(getBlockPoolId()),\n          dn.xmitsInProgress.get(),\n          dn.getXceiverCount(), dn.data.getNumFailedVolumes());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "0864ef19089f703232107d8aa26c4a7571ff132e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2560. Refactor BPOfferService to be a static inner class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203444 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/11/11 4:45 PM",
      "commitName": "0864ef19089f703232107d8aa26c4a7571ff132e",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "31/10/11 10:17 PM",
      "commitNameOld": "1c940637b14eee777a65d153d0d712a1aea3866c",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 16.81,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,9 @@\n     DatanodeCommand [] sendHeartBeat() throws IOException {\n       return bpNamenode.sendHeartbeat(bpRegistration,\n-          data.getCapacity(),\n-          data.getDfsUsed(),\n-          data.getRemaining(),\n-          data.getBlockPoolUsed(blockPoolId),\n-          xmitsInProgress.get(),\n-          getXceiverCount(), data.getNumFailedVolumes());\n+          dn.data.getCapacity(),\n+          dn.data.getDfsUsed(),\n+          dn.data.getRemaining(),\n+          dn.data.getBlockPoolUsed(blockPoolId),\n+          dn.xmitsInProgress.get(),\n+          dn.getXceiverCount(), dn.data.getNumFailedVolumes());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    DatanodeCommand [] sendHeartBeat() throws IOException {\n      return bpNamenode.sendHeartbeat(bpRegistration,\n          dn.data.getCapacity(),\n          dn.data.getDfsUsed(),\n          dn.data.getRemaining(),\n          dn.data.getBlockPoolUsed(blockPoolId),\n          dn.xmitsInProgress.get(),\n          dn.getXceiverCount(), dn.data.getNumFailedVolumes());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    DatanodeCommand [] sendHeartBeat() throws IOException {\n      return bpNamenode.sendHeartbeat(bpRegistration,\n          data.getCapacity(),\n          data.getDfsUsed(),\n          data.getRemaining(),\n          data.getBlockPoolUsed(blockPoolId),\n          xmitsInProgress.get(),\n          getXceiverCount(), data.getNumFailedVolumes());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    DatanodeCommand [] sendHeartBeat() throws IOException {\n      return bpNamenode.sendHeartbeat(bpRegistration,\n          data.getCapacity(),\n          data.getDfsUsed(),\n          data.getRemaining(),\n          data.getBlockPoolUsed(blockPoolId),\n          xmitsInProgress.get(),\n          getXceiverCount(), data.getNumFailedVolumes());\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,9 @@\n+    DatanodeCommand [] sendHeartBeat() throws IOException {\n+      return bpNamenode.sendHeartbeat(bpRegistration,\n+          data.getCapacity(),\n+          data.getDfsUsed(),\n+          data.getRemaining(),\n+          data.getBlockPoolUsed(blockPoolId),\n+          xmitsInProgress.get(),\n+          getXceiverCount(), data.getNumFailedVolumes());\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    DatanodeCommand [] sendHeartBeat() throws IOException {\n      return bpNamenode.sendHeartbeat(bpRegistration,\n          data.getCapacity(),\n          data.getDfsUsed(),\n          data.getRemaining(),\n          data.getBlockPoolUsed(blockPoolId),\n          xmitsInProgress.get(),\n          getXceiverCount(), data.getNumFailedVolumes());\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
    }
  }
}