{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "readWithStrategy",
  "functionId": "readWithStrategy___strategy-ReaderStrategy",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 861,
  "functionEndLine": 929,
  "numCommitsSeen": 166,
  "timeTaken": 12199,
  "changeHistory": [
    "d10f77e3c91225f86ed9c0f0e6a9adf2e1434674",
    "b3119b9ab60a19d624db476c4e1c53410870c7a6",
    "08bb6c49a5aec32b7d9f29238560f947420405d6",
    "960940e0e08f7839775f2d8a352b444d104d36b4",
    "6eba79232f36b36e0196163adc8fe4219a6b6bf9",
    "793447f79924c97c2b562d5e41fa85adf19673fe",
    "be34e85e682880f46eee0310bf00ecc7d39cd5bd",
    "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b",
    "8808779db351fe444388d4acb3094766b5980718",
    "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "bff5999d07e9416a22846c849487e509ede55040",
    "a97a1e73177974cff8afafad6ca43a96563f3c61",
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
    "0126cf16b73843da2e504b6a03fee8bd93a404d5",
    "21d225af4dd0189509fa98c3499319672096a1b6",
    "befb254e61a34352d146be79656d656044432dd1",
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
    "6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
    "b7cd8c0f865e88e40eee75fd2690b1fdc4155071",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "2c5dd549e31aa5d3377ff2619ede8e92b8dc5d0f",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "d10f77e3c91225f86ed9c0f0e6a9adf2e1434674": "Ybodychange",
    "b3119b9ab60a19d624db476c4e1c53410870c7a6": "Ybodychange",
    "08bb6c49a5aec32b7d9f29238560f947420405d6": "Ybodychange",
    "960940e0e08f7839775f2d8a352b444d104d36b4": "Ybodychange",
    "6eba79232f36b36e0196163adc8fe4219a6b6bf9": "Ybodychange",
    "793447f79924c97c2b562d5e41fa85adf19673fe": "Ymultichange(Yparameterchange,Ybodychange)",
    "be34e85e682880f46eee0310bf00ecc7d39cd5bd": "Ybodychange",
    "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b": "Ybodychange",
    "8808779db351fe444388d4acb3094766b5980718": "Ybodychange",
    "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "bff5999d07e9416a22846c849487e509ede55040": "Ymodifierchange",
    "a97a1e73177974cff8afafad6ca43a96563f3c61": "Ybodychange",
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f": "Ymultichange(Ymodifierchange,Ybodychange)",
    "0126cf16b73843da2e504b6a03fee8bd93a404d5": "Ybodychange",
    "21d225af4dd0189509fa98c3499319672096a1b6": "Ybodychange",
    "befb254e61a34352d146be79656d656044432dd1": "Ybodychange",
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
    "6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc": "Ybodychange",
    "b7cd8c0f865e88e40eee75fd2690b1fdc4155071": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "2c5dd549e31aa5d3377ff2619ede8e92b8dc5d0f": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d10f77e3c91225f86ed9c0f0e6a9adf2e1434674": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15119. Allow expiration of cached locations in DFSInputStream.\nContributed by Ahmed Hussein.\n",
      "commitDate": "24/01/20 7:15 AM",
      "commitName": "d10f77e3c91225f86ed9c0f0e6a9adf2e1434674",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "12/12/19 2:23 AM",
      "commitNameOld": "0e28cd8f63615dddded2f1183f27efb5c2aaf6aa",
      "commitAuthorOld": "He Xiaoqiao",
      "daysBetweenCommits": 43.2,
      "commitsBetweenForRepo": 139,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,69 @@\n   protected synchronized int readWithStrategy(ReaderStrategy strategy)\n       throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n \n     int len \u003d strategy.getTargetLength();\n     CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n-          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n+          // currentNode needs to be updated if the blockLocations timestamp has\n+          // expired.\n+          if (pos \u003e blockEnd || currentNode \u003d\u003d null\n+              || updateBlockLocationsStamp()) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n           int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           updateReadStatistics(readStatistics, result, blockReader);\n           dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n               result);\n           if (readStatistics.getBlockType() \u003d\u003d BlockType.STRIPED) {\n             dfsClient.updateFileSystemECReadStats(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;\n         } catch (IOException e) {\n           checkInterrupted(e);\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) {\n             addToLocalDeadNodes(currentNode);\n             dfsClient.addNodeToDeadNodeDetector(this, currentNode);\n           }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occurred.\n           reportCheckSumFailure(corruptedBlocks,\n               getCurrentBlockLocationsLength(), false);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy)\n      throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n\n    int len \u003d strategy.getTargetLength();\n    CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          // currentNode needs to be updated if the blockLocations timestamp has\n          // expired.\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null\n              || updateBlockLocationsStamp()) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          updateReadStatistics(readStatistics, result, blockReader);\n          dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n              result);\n          if (readStatistics.getBlockType() \u003d\u003d BlockType.STRIPED) {\n            dfsClient.updateFileSystemECReadStats(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          checkInterrupted(e);\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) {\n            addToLocalDeadNodes(currentNode);\n            dfsClient.addNodeToDeadNodeDetector(this, currentNode);\n          }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occurred.\n          reportCheckSumFailure(corruptedBlocks,\n              getCurrentBlockLocationsLength(), false);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "b3119b9ab60a19d624db476c4e1c53410870c7a6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14648. Implement DeadNodeDetector basic model. Contributed by Lisheng Sun.\n",
      "commitDate": "15/11/19 7:32 PM",
      "commitName": "b3119b9ab60a19d624db476c4e1c53410870c7a6",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "06/11/19 5:58 AM",
      "commitNameOld": "c36014165c212b26d75268ee3659aa2cadcff349",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 9.57,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,66 @@\n   protected synchronized int readWithStrategy(ReaderStrategy strategy)\n       throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n \n     int len \u003d strategy.getTargetLength();\n     CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n           int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           updateReadStatistics(readStatistics, result, blockReader);\n           dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n               result);\n           if (readStatistics.getBlockType() \u003d\u003d BlockType.STRIPED) {\n             dfsClient.updateFileSystemECReadStats(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;\n         } catch (IOException e) {\n           checkInterrupted(e);\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) {\n-            addToDeadNodes(currentNode);\n+            addToLocalDeadNodes(currentNode);\n+            dfsClient.addNodeToDeadNodeDetector(this, currentNode);\n           }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occurred.\n           reportCheckSumFailure(corruptedBlocks,\n               getCurrentBlockLocationsLength(), false);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy)\n      throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n\n    int len \u003d strategy.getTargetLength();\n    CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          updateReadStatistics(readStatistics, result, blockReader);\n          dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n              result);\n          if (readStatistics.getBlockType() \u003d\u003d BlockType.STRIPED) {\n            dfsClient.updateFileSystemECReadStats(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          checkInterrupted(e);\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) {\n            addToLocalDeadNodes(currentNode);\n            dfsClient.addNodeToDeadNodeDetector(this, currentNode);\n          }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occurred.\n          reportCheckSumFailure(corruptedBlocks,\n              getCurrentBlockLocationsLength(), false);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "08bb6c49a5aec32b7d9f29238560f947420405d6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13926. ThreadLocal aggregations for FileSystem.Statistics are incorrect with striped reads.\nContributed by Xiao Chen, Hrishikesh Gadre.\n\nSigned-off-by: Xiao Chen \u003cxiao@apache.org\u003e\n",
      "commitDate": "08/10/18 8:31 PM",
      "commitName": "08bb6c49a5aec32b7d9f29238560f947420405d6",
      "commitAuthor": "Hrishikesh Gadre",
      "commitDateOld": "02/07/18 3:11 AM",
      "commitNameOld": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 98.72,
      "commitsBetweenForRepo": 804,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,65 @@\n   protected synchronized int readWithStrategy(ReaderStrategy strategy)\n       throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n \n     int len \u003d strategy.getTargetLength();\n     CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n           int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n+          updateReadStatistics(readStatistics, result, blockReader);\n+          dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n+              result);\n+          if (readStatistics.getBlockType() \u003d\u003d BlockType.STRIPED) {\n+            dfsClient.updateFileSystemECReadStats(result);\n+          }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;\n         } catch (IOException e) {\n           checkInterrupted(e);\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) {\n             addToDeadNodes(currentNode);\n           }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occurred.\n           reportCheckSumFailure(corruptedBlocks,\n               getCurrentBlockLocationsLength(), false);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy)\n      throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n\n    int len \u003d strategy.getTargetLength();\n    CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          updateReadStatistics(readStatistics, result, blockReader);\n          dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n              result);\n          if (readStatistics.getBlockType() \u003d\u003d BlockType.STRIPED) {\n            dfsClient.updateFileSystemECReadStats(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          checkInterrupted(e);\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) {\n            addToDeadNodes(currentNode);\n          }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occurred.\n          reportCheckSumFailure(corruptedBlocks,\n              getCurrentBlockLocationsLength(), false);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "960940e0e08f7839775f2d8a352b444d104d36b4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13539. DFSStripedInputStream NPE when reportCheckSumFailure.\n",
      "commitDate": "14/05/18 9:28 AM",
      "commitName": "960940e0e08f7839775f2d8a352b444d104d36b4",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "09/11/17 10:16 AM",
      "commitNameOld": "bf6a660232b01642b07697a289c773ea5b97217c",
      "commitAuthorOld": "John Zhuge",
      "daysBetweenCommits": 185.93,
      "commitsBetweenForRepo": 1786,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,59 @@\n   protected synchronized int readWithStrategy(ReaderStrategy strategy)\n       throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n \n     int len \u003d strategy.getTargetLength();\n     CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n           int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;\n         } catch (IOException e) {\n           checkInterrupted(e);\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) {\n             addToDeadNodes(currentNode);\n           }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occurred.\n           reportCheckSumFailure(corruptedBlocks,\n-              currentLocatedBlock.getLocations().length, false);\n+              getCurrentBlockLocationsLength(), false);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy)\n      throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n\n    int len \u003d strategy.getTargetLength();\n    CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          checkInterrupted(e);\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) {\n            addToDeadNodes(currentNode);\n          }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occurred.\n          reportCheckSumFailure(corruptedBlocks,\n              getCurrentBlockLocationsLength(), false);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "6eba79232f36b36e0196163adc8fe4219a6b6bf9": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14271. Correct spelling of \u0027occurred\u0027 and variants. Contributed by Yeliang Cang\n",
      "commitDate": "03/04/17 8:13 PM",
      "commitName": "6eba79232f36b36e0196163adc8fe4219a6b6bf9",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "10/02/17 10:27 AM",
      "commitNameOld": "07a5184f74fdeffc42cdaec42ad4378c0e41c541",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 52.37,
      "commitsBetweenForRepo": 311,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,59 @@\n   protected synchronized int readWithStrategy(ReaderStrategy strategy)\n       throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n \n     int len \u003d strategy.getTargetLength();\n     CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n           int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;\n         } catch (IOException e) {\n           checkInterrupted(e);\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) {\n             addToDeadNodes(currentNode);\n           }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n-          // was successful or ChecksumException occured.\n+          // was successful or ChecksumException occurred.\n           reportCheckSumFailure(corruptedBlocks,\n               currentLocatedBlock.getLocations().length, false);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy)\n      throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n\n    int len \u003d strategy.getTargetLength();\n    CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          checkInterrupted(e);\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) {\n            addToDeadNodes(currentNode);\n          }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occurred.\n          reportCheckSumFailure(corruptedBlocks,\n              currentLocatedBlock.getLocations().length, false);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "793447f79924c97c2b562d5e41fa85adf19673fe": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8905. Refactor DFSInputStream#ReaderStrategy. Contributed by Kai Zheng and Sammi Chen\n",
      "commitDate": "24/08/16 6:57 AM",
      "commitName": "793447f79924c97c2b562d5e41fa85adf19673fe",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8905. Refactor DFSInputStream#ReaderStrategy. Contributed by Kai Zheng and Sammi Chen\n",
          "commitDate": "24/08/16 6:57 AM",
          "commitName": "793447f79924c97c2b562d5e41fa85adf19673fe",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "08/06/16 10:52 PM",
          "commitNameOld": "8ea9bbce2614e8eb499af73589f021ed1789e78f",
          "commitAuthorOld": "Masatake Iwasaki",
          "daysBetweenCommits": 76.34,
          "commitsBetweenForRepo": 637,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,56 +1,57 @@\n-  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off,\n-      int len) throws IOException {\n+  protected synchronized int readWithStrategy(ReaderStrategy strategy)\n+      throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n \n+    int len \u003d strategy.getTargetLength();\n     CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n-          int result \u003d readBuffer(strategy, off, realLen, corruptedBlocks);\n+          int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;\n         } catch (IOException e) {\n           checkInterrupted(e);\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlocks,\n               currentLocatedBlock.getLocations().length, false);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy)\n      throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n\n    int len \u003d strategy.getTargetLength();\n    CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          checkInterrupted(e);\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlocks,\n              currentLocatedBlock.getLocations().length, false);\n        }\n      }\n    }\n    return -1;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[strategy-ReaderStrategy, off-int, len-int]",
            "newValue": "[strategy-ReaderStrategy]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8905. Refactor DFSInputStream#ReaderStrategy. Contributed by Kai Zheng and Sammi Chen\n",
          "commitDate": "24/08/16 6:57 AM",
          "commitName": "793447f79924c97c2b562d5e41fa85adf19673fe",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "08/06/16 10:52 PM",
          "commitNameOld": "8ea9bbce2614e8eb499af73589f021ed1789e78f",
          "commitAuthorOld": "Masatake Iwasaki",
          "daysBetweenCommits": 76.34,
          "commitsBetweenForRepo": 637,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,56 +1,57 @@\n-  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off,\n-      int len) throws IOException {\n+  protected synchronized int readWithStrategy(ReaderStrategy strategy)\n+      throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n \n+    int len \u003d strategy.getTargetLength();\n     CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n-          int result \u003d readBuffer(strategy, off, realLen, corruptedBlocks);\n+          int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;\n         } catch (IOException e) {\n           checkInterrupted(e);\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlocks,\n               currentLocatedBlock.getLocations().length, false);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy)\n      throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n\n    int len \u003d strategy.getTargetLength();\n    CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, realLen, corruptedBlocks);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          checkInterrupted(e);\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlocks,\n              currentLocatedBlock.getLocations().length, false);\n        }\n      }\n    }\n    return -1;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "be34e85e682880f46eee0310bf00ecc7d39cd5bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10468. HDFS read ends up ignoring an interrupt. Contributed by Jing Zhao\n",
      "commitDate": "07/06/16 10:48 AM",
      "commitName": "be34e85e682880f46eee0310bf00ecc7d39cd5bd",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "25/04/16 12:01 PM",
      "commitNameOld": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 42.95,
      "commitsBetweenForRepo": 291,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,56 @@\n   protected synchronized int readWithStrategy(ReaderStrategy strategy, int off,\n       int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n \n     CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n           int result \u003d readBuffer(strategy, off, realLen, corruptedBlocks);\n \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;\n         } catch (IOException e) {\n+          checkInterrupted(e);\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlocks,\n               currentLocatedBlock.getLocations().length, false);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off,\n      int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n\n    CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlocks);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          checkInterrupted(e);\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlocks,\n              currentLocatedBlock.getLocations().length, false);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9579. Provide bytes-read-by-network-distance metrics at FileSystem.Statistics level (Ming Ma via sjlee)\n",
      "commitDate": "19/03/16 2:02 PM",
      "commitName": "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "25/02/16 9:55 AM",
      "commitNameOld": "8808779db351fe444388d4acb3094766b5980718",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 23.13,
      "commitsBetweenForRepo": 135,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,55 @@\n   protected synchronized int readWithStrategy(ReaderStrategy strategy, int off,\n       int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n \n     CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n           int result \u003d readBuffer(strategy, off, realLen, corruptedBlocks);\n \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n-          if (dfsClient.stats !\u003d null) {\n-            dfsClient.stats.incrementBytesRead(result);\n-          }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;\n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlocks,\n               currentLocatedBlock.getLocations().length, false);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off,\n      int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n\n    CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlocks);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlocks,\n              currentLocatedBlock.getLocations().length, false);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "8808779db351fe444388d4acb3094766b5980718": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9734. Refactoring of checksum failure report related codes. Contributed by Kai Zheng.\n\nChange-Id: Ie69a77e3498a360959f8e213c51fb2b17c28b64a\n",
      "commitDate": "25/02/16 9:55 AM",
      "commitName": "8808779db351fe444388d4acb3094766b5980718",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "22/01/16 9:46 AM",
      "commitNameOld": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 34.01,
      "commitsBetweenForRepo": 234,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,58 @@\n   protected synchronized int readWithStrategy(ReaderStrategy strategy, int off,\n       int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n-    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \u003d new HashMap\u003c\u003e();\n+\n+    CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n-          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n+          int result \u003d readBuffer(strategy, off, realLen, corruptedBlocks);\n \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;\n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n-          reportCheckSumFailure(corruptedBlockMap,\n+          reportCheckSumFailure(corruptedBlocks,\n               currentLocatedBlock.getLocations().length, false);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off,\n      int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n\n    CorruptedBlocks corruptedBlocks \u003d new CorruptedBlocks();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlocks);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlocks,\n              currentLocatedBlock.getLocations().length, false);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9646. ErasureCodingWorker may fail when recovering data blocks with length less than the first internal block. Contributed by Jing Zhao.\n",
      "commitDate": "22/01/16 9:46 AM",
      "commitName": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "20/01/16 11:26 AM",
      "commitNameOld": "7905788db94d560e6668af0d4bed22b326961aaf",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.93,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,57 @@\n-  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n+  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off,\n+      int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \u003d new HashMap\u003c\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n           int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;\n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap,\n-              currentLocatedBlock.getLocations().length);\n+              currentLocatedBlock.getLocations().length, false);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off,\n      int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \u003d new HashMap\u003c\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap,\n              currentLocatedBlock.getLocations().length, false);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,56 @@\n   protected synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n-    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n-      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n+    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \u003d new HashMap\u003c\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n           int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n-          \n+\n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n-          throw ce;            \n+          throw ce;\n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n-          reportCheckSumFailure(corruptedBlockMap, \n+          reportCheckSumFailure(corruptedBlockMap,\n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \u003d new HashMap\u003c\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n\n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;\n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap,\n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "bff5999d07e9416a22846c849487e509ede55040": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
      "commitDate": "02/07/15 3:41 AM",
      "commitName": "bff5999d07e9416a22846c849487e509ede55040",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "04/06/15 10:51 AM",
      "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 27.7,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n-  private synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n+  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n           int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldValue": "[private, synchronized]",
        "newValue": "[protected, synchronized]"
      }
    },
    "a97a1e73177974cff8afafad6ca43a96563f3c61": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7494. Checking of closed in DFSInputStream#pread() should be protected by synchronization (Ted Yu via Colin P. McCabe)\n",
      "commitDate": "16/12/14 11:07 AM",
      "commitName": "a97a1e73177974cff8afafad6ca43a96563f3c61",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "02/12/14 8:57 PM",
      "commitNameOld": "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
      "commitAuthorOld": "stack",
      "daysBetweenCommits": 13.59,
      "commitsBetweenForRepo": 103,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   private synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n-    if (closed) {\n+    if (closed.get()) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           synchronized(infoLock) {\n             if (locatedBlocks.isLastBlockComplete()) {\n               realLen \u003d (int) Math.min(realLen,\n                   locatedBlocks.getFileLength() - pos);\n             }\n           }\n           int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed.get()) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-6735. A minor optimization to avoid pread() be blocked by read() inside the same DFSInputStream (Lars Hofhansl via stack)\n",
      "commitDate": "02/12/14 8:57 PM",
      "commitName": "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
      "commitAuthor": "stack",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-6735. A minor optimization to avoid pread() be blocked by read() inside the same DFSInputStream (Lars Hofhansl via stack)\n",
          "commitDate": "02/12/14 8:57 PM",
          "commitName": "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
          "commitAuthor": "stack",
          "commitDateOld": "05/11/14 9:00 PM",
          "commitNameOld": "80d7d183cd4052d6e6d412ff6588d26471c85d6d",
          "commitAuthorOld": "Milan Desai",
          "daysBetweenCommits": 27.0,
          "commitsBetweenForRepo": 189,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,55 +1,57 @@\n-  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n+  private synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n-          if (locatedBlocks.isLastBlockComplete()) {\n-            realLen \u003d (int) Math.min(realLen,\n-                locatedBlocks.getFileLength() - pos);\n+          synchronized(infoLock) {\n+            if (locatedBlocks.isLastBlockComplete()) {\n+              realLen \u003d (int) Math.min(realLen,\n+                  locatedBlocks.getFileLength() - pos);\n+            }\n           }\n           int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[private, synchronized]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6735. A minor optimization to avoid pread() be blocked by read() inside the same DFSInputStream (Lars Hofhansl via stack)\n",
          "commitDate": "02/12/14 8:57 PM",
          "commitName": "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
          "commitAuthor": "stack",
          "commitDateOld": "05/11/14 9:00 PM",
          "commitNameOld": "80d7d183cd4052d6e6d412ff6588d26471c85d6d",
          "commitAuthorOld": "Milan Desai",
          "daysBetweenCommits": 27.0,
          "commitsBetweenForRepo": 189,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,55 +1,57 @@\n-  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n+  private synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n-          if (locatedBlocks.isLastBlockComplete()) {\n-            realLen \u003d (int) Math.min(realLen,\n-                locatedBlocks.getFileLength() - pos);\n+          synchronized(infoLock) {\n+            if (locatedBlocks.isLastBlockComplete()) {\n+              realLen \u003d (int) Math.min(realLen,\n+                  locatedBlocks.getFileLength() - pos);\n+            }\n           }\n           int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          synchronized(infoLock) {\n            if (locatedBlocks.isLastBlockComplete()) {\n              realLen \u003d (int) Math.min(realLen,\n                  locatedBlocks.getFileLength() - pos);\n            }\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "0126cf16b73843da2e504b6a03fee8bd93a404d5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7263. Snapshot read can reveal future bytes for appended files. Contributed by Tao Luo.",
      "commitDate": "29/10/14 8:21 PM",
      "commitName": "0126cf16b73843da2e504b6a03fee8bd93a404d5",
      "commitAuthor": "Tao Luo",
      "commitDateOld": "27/10/14 9:38 AM",
      "commitNameOld": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 2.45,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,55 @@\n   private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           if (locatedBlocks.isLastBlockComplete()) {\n-            realLen \u003d (int) Math.min(realLen, locatedBlocks.getFileLength());\n+            realLen \u003d (int) Math.min(realLen,\n+                locatedBlocks.getFileLength() - pos);\n           }\n           int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          if (locatedBlocks.isLastBlockComplete()) {\n            realLen \u003d (int) Math.min(realLen,\n                locatedBlocks.getFileLength() - pos);\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "21d225af4dd0189509fa98c3499319672096a1b6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6162. Format strings should use platform independent line separator. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582181 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/03/14 9:25 PM",
      "commitName": "21d225af4dd0189509fa98c3499319672096a1b6",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "24/03/14 4:32 PM",
      "commitNameOld": "c2ef7e239eb0e81cf8a3e971378e9e696202de67",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.2,
      "commitsBetweenForRepo": 28,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           if (locatedBlocks.isLastBlockComplete()) {\n             realLen \u003d (int) Math.min(realLen, locatedBlocks.getFileLength());\n           }\n           int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n-          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n+          if (dfsClient.stats !\u003d null) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          if (locatedBlocks.isLastBlockComplete()) {\n            realLen \u003d (int) Math.min(realLen, locatedBlocks.getFileLength());\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "befb254e61a34352d146be79656d656044432dd1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5343. When cat command is issued on snapshot files, getting unexpected result. Contributed by Sathish.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1561325 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/14 7:50 AM",
      "commitName": "befb254e61a34352d146be79656d656044432dd1",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "15/01/14 11:23 AM",
      "commitNameOld": "c3b236ce5d2ca0905d03d63efe74b9f2c8e27436",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 9.85,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,54 @@\n   private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n+          if (locatedBlocks.isLastBlockComplete()) {\n+            realLen \u003d (int) Math.min(realLen, locatedBlocks.getFileLength());\n+          }\n           int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          if (locatedBlocks.isLastBlockComplete()) {\n            realLen \u003d (int) Math.min(realLen, locatedBlocks.getFileLength());\n          }\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-2834. Add a ByteBuffer-based read API to DFSInputStream. Contributed by Henry Robinson.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/03/12 10:30 AM",
      "commitName": "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-2834. Add a ByteBuffer-based read API to DFSInputStream. Contributed by Henry Robinson.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303474 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/03/12 10:30 AM",
          "commitName": "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "15/03/12 1:26 PM",
          "commitNameOld": "6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 5.88,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,51 @@\n-  public synchronized int read(byte buf[], int off, int len) throws IOException {\n+  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n-          int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n+          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "read",
            "newValue": "readWithStrategy"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2834. Add a ByteBuffer-based read API to DFSInputStream. Contributed by Henry Robinson.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303474 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/03/12 10:30 AM",
          "commitName": "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "15/03/12 1:26 PM",
          "commitNameOld": "6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 5.88,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,51 @@\n-  public synchronized int read(byte buf[], int off, int len) throws IOException {\n+  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n-          int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n+          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[buf-byte[], off-int, len-int]",
            "newValue": "[strategy-ReaderStrategy, off-int, len-int]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2834. Add a ByteBuffer-based read API to DFSInputStream. Contributed by Henry Robinson.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303474 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/03/12 10:30 AM",
          "commitName": "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "15/03/12 1:26 PM",
          "commitNameOld": "6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 5.88,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,51 @@\n-  public synchronized int read(byte buf[], int off, int len) throws IOException {\n+  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n-          int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n+          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2834. Add a ByteBuffer-based read API to DFSInputStream. Contributed by Henry Robinson.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303474 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/03/12 10:30 AM",
          "commitName": "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "15/03/12 1:26 PM",
          "commitNameOld": "6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 5.88,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,51 @@\n-  public synchronized int read(byte buf[], int off, int len) throws IOException {\n+  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           // currentNode can be left as null if previous read had a checksum\n           // error on the same block. See HDFS-3067\n           if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n-          int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n+          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          int result \u003d readBuffer(strategy, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3067. NPE in DFSInputStream.readBuffer if read is repeated on corrupted block. Contributed by Henry Robinson.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301182 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/03/12 1:26 PM",
      "commitName": "6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "21/11/11 6:57 PM",
      "commitNameOld": "2ab10e29d9cca5018064be46a40e3c74423615a8",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 114.73,
      "commitsBetweenForRepo": 799,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,51 @@\n   public synchronized int read(byte buf[], int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n-          if (pos \u003e blockEnd) {\n+          // currentNode can be left as null if previous read had a checksum\n+          // error on the same block. See HDFS-3067\n+          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte buf[], int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          // currentNode can be left as null if previous read had a checksum\n          // error on the same block. See HDFS-3067\n          if (pos \u003e blockEnd || currentNode \u003d\u003d null) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "b7cd8c0f865e88e40eee75fd2690b1fdc4155071": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2564. Cleanup unnecessary exceptions thrown and unnecessary casts. Contributed by Hari Mankude\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203950 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 6:34 PM",
      "commitName": "b7cd8c0f865e88e40eee75fd2690b1fdc4155071",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "02/11/11 11:54 PM",
      "commitNameOld": "40fe96546fcd68696076db67053f30d38a39a0d5",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 15.82,
      "commitsBetweenForRepo": 55,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n   public synchronized int read(byte buf[], int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           if (pos \u003e blockEnd) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n-          int realLen \u003d (int) Math.min((long) len, (blockEnd - pos + 1L));\n+          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n           int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n             DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte buf[], int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          if (pos \u003e blockEnd) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min(len, (blockEnd - pos + 1L));\n          int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized int read(byte buf[], int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          if (pos \u003e blockEnd) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min((long) len, (blockEnd - pos + 1L));\n          int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized int read(byte buf[], int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          if (pos \u003e blockEnd) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min((long) len, (blockEnd - pos + 1L));\n          int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "2c5dd549e31aa5d3377ff2619ede8e92b8dc5d0f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1977. Stop using StringUtils.stringifyException(). Contributed by Bharath Mundlapudi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1145834 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/07/11 6:11 PM",
      "commitName": "2c5dd549e31aa5d3377ff2619ede8e92b8dc5d0f",
      "commitAuthor": "Jitendra Nath Pandey",
      "commitDateOld": "08/07/11 1:13 PM",
      "commitNameOld": "6ec9b178c664796b44eab6c41f7216577d380f7c",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.21,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n   public synchronized int read(byte buf[], int off, int len) throws IOException {\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     failures \u003d 0;\n     if (pos \u003c getFileLength()) {\n       int retries \u003d 2;\n       while (retries \u003e 0) {\n         try {\n           if (pos \u003e blockEnd) {\n             currentNode \u003d blockSeekTo(pos);\n           }\n           int realLen \u003d (int) Math.min((long) len, (blockEnd - pos + 1L));\n           int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n           \n           if (result \u003e\u003d 0) {\n             pos +\u003d result;\n           } else {\n             // got a EOS from reader though we expect more data on it.\n             throw new IOException(\"Unexpected EOS from the reader\");\n           }\n           if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n             dfsClient.stats.incrementBytesRead(result);\n           }\n           return result;\n         } catch (ChecksumException ce) {\n           throw ce;            \n         } catch (IOException e) {\n           if (retries \u003d\u003d 1) {\n-            DFSClient.LOG.warn(\"DFS Read: \" + StringUtils.stringifyException(e));\n+            DFSClient.LOG.warn(\"DFS Read\", e);\n           }\n           blockEnd \u003d -1;\n           if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n           if (--retries \u003d\u003d 0) {\n             throw e;\n           }\n         } finally {\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occured.\n           reportCheckSumFailure(corruptedBlockMap, \n               currentLocatedBlock.getLocations().length);\n         }\n       }\n     }\n     return -1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte buf[], int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          if (pos \u003e blockEnd) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min((long) len, (blockEnd - pos + 1L));\n          int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read\", e);\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,49 @@\n+  public synchronized int read(byte buf[], int off, int len) throws IOException {\n+    dfsClient.checkOpen();\n+    if (closed) {\n+      throw new IOException(\"Stream closed\");\n+    }\n+    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n+      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n+    failures \u003d 0;\n+    if (pos \u003c getFileLength()) {\n+      int retries \u003d 2;\n+      while (retries \u003e 0) {\n+        try {\n+          if (pos \u003e blockEnd) {\n+            currentNode \u003d blockSeekTo(pos);\n+          }\n+          int realLen \u003d (int) Math.min((long) len, (blockEnd - pos + 1L));\n+          int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n+          \n+          if (result \u003e\u003d 0) {\n+            pos +\u003d result;\n+          } else {\n+            // got a EOS from reader though we expect more data on it.\n+            throw new IOException(\"Unexpected EOS from the reader\");\n+          }\n+          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n+            dfsClient.stats.incrementBytesRead(result);\n+          }\n+          return result;\n+        } catch (ChecksumException ce) {\n+          throw ce;            \n+        } catch (IOException e) {\n+          if (retries \u003d\u003d 1) {\n+            DFSClient.LOG.warn(\"DFS Read: \" + StringUtils.stringifyException(e));\n+          }\n+          blockEnd \u003d -1;\n+          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n+          if (--retries \u003d\u003d 0) {\n+            throw e;\n+          }\n+        } finally {\n+          // Check if need to report block replicas corruption either read\n+          // was successful or ChecksumException occured.\n+          reportCheckSumFailure(corruptedBlockMap, \n+              currentLocatedBlock.getLocations().length);\n+        }\n+      }\n+    }\n+    return -1;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte buf[], int off, int len) throws IOException {\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    failures \u003d 0;\n    if (pos \u003c getFileLength()) {\n      int retries \u003d 2;\n      while (retries \u003e 0) {\n        try {\n          if (pos \u003e blockEnd) {\n            currentNode \u003d blockSeekTo(pos);\n          }\n          int realLen \u003d (int) Math.min((long) len, (blockEnd - pos + 1L));\n          int result \u003d readBuffer(buf, off, realLen, corruptedBlockMap);\n          \n          if (result \u003e\u003d 0) {\n            pos +\u003d result;\n          } else {\n            // got a EOS from reader though we expect more data on it.\n            throw new IOException(\"Unexpected EOS from the reader\");\n          }\n          if (dfsClient.stats !\u003d null \u0026\u0026 result !\u003d -1) {\n            dfsClient.stats.incrementBytesRead(result);\n          }\n          return result;\n        } catch (ChecksumException ce) {\n          throw ce;            \n        } catch (IOException e) {\n          if (retries \u003d\u003d 1) {\n            DFSClient.LOG.warn(\"DFS Read: \" + StringUtils.stringifyException(e));\n          }\n          blockEnd \u003d -1;\n          if (currentNode !\u003d null) { addToDeadNodes(currentNode); }\n          if (--retries \u003d\u003d 0) {\n            throw e;\n          }\n        } finally {\n          // Check if need to report block replicas corruption either read\n          // was successful or ChecksumException occured.\n          reportCheckSumFailure(corruptedBlockMap, \n              currentLocatedBlock.getLocations().length);\n        }\n      }\n    }\n    return -1;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}