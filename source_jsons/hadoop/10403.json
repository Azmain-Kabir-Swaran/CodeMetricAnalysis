{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataXceiver.java",
  "functionName": "blockChecksum",
  "functionId": "blockChecksum___block-ExtendedBlock__blockToken-Token__BlockTokenIdentifier____blockChecksumOptions-BlockChecksumOptions",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
  "functionStartLine": 988,
  "functionEndLine": 1027,
  "numCommitsSeen": 234,
  "timeTaken": 10820,
  "changeHistory": [
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
    "38c4c14472996562eb3d610649246770c2888c6b",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
    "4da8490b512a33a255ed27309860859388d7c168",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e",
    "2d4f3e567e4bb8068c028de12df118a4f3fa6343",
    "86cad007d7d6366b293bb9a073814889081c8662",
    "3671a5e16fbddbe5a0516289ce98e1305e02291c",
    "3d9ad8e3b60dd21db45466f4736abe6b1812b522",
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21",
    "c46de830da98959f40dd41c95bdebecdfb9ea730",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "662b1887af4e39f3eadd7dda4953c7f2529b43bc",
    "905a127850d5e0cba85c2e075f989fa0f5cf129a",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf": "Ymultichange(Yparameterchange,Ybodychange)",
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": "Ybodychange",
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6": "Ymultichange(Ybodychange,Yparametermetachange)",
    "38c4c14472996562eb3d610649246770c2888c6b": "Ybodychange",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": "Ybodychange",
    "4da8490b512a33a255ed27309860859388d7c168": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e": "Ybodychange",
    "2d4f3e567e4bb8068c028de12df118a4f3fa6343": "Ybodychange",
    "86cad007d7d6366b293bb9a073814889081c8662": "Ybodychange",
    "3671a5e16fbddbe5a0516289ce98e1305e02291c": "Ybodychange",
    "3d9ad8e3b60dd21db45466f4736abe6b1812b522": "Ybodychange",
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21": "Ybodychange",
    "c46de830da98959f40dd41c95bdebecdfb9ea730": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ybodychange",
    "662b1887af4e39f3eadd7dda4953c7f2529b43bc": "Ybodychange",
    "905a127850d5e0cba85c2e075f989fa0f5cf129a": "Ybodychange",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": "Ybodychange",
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Yparametermetachange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13056. Expose file-level composite CRCs in HDFS which are comparable across different instances/layouts. Contributed by Dennis Huo.\n",
      "commitDate": "10/04/18 9:31 PM",
      "commitName": "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
      "commitAuthor": "Xiao Chen",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13056. Expose file-level composite CRCs in HDFS which are comparable across different instances/layouts. Contributed by Dennis Huo.\n",
          "commitDate": "10/04/18 9:31 PM",
          "commitName": "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
          "commitAuthor": "Xiao Chen",
          "commitDateOld": "14/02/18 8:20 AM",
          "commitNameOld": "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 55.51,
          "commitsBetweenForRepo": 494,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,40 @@\n   public void blockChecksum(ExtendedBlock block,\n-                            Token\u003cBlockTokenIdentifier\u003e blockToken)\n+      Token\u003cBlockTokenIdentifier\u003e blockToken,\n+      BlockChecksumOptions blockChecksumOptions)\n       throws IOException {\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken, Op.BLOCK_CHECKSUM,\n         BlockTokenIdentifier.AccessMode.READ);\n-    BlockChecksumComputer maker \u003d\n-        new ReplicatedBlockChecksumComputer(datanode, block);\n+    BlockChecksumComputer maker \u003d new ReplicatedBlockChecksumComputer(\n+        datanode, block, blockChecksumOptions);\n \n     try {\n       maker.compute();\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n           .setStatus(SUCCESS)\n           .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()\n               .setBytesPerCrc(maker.getBytesPerCRC())\n               .setCrcPerBlock(maker.getCrcPerBlock())\n-              .setMd5(ByteString.copyFrom(maker.getOutBytes()))\n-              .setCrcType(PBHelperClient.convert(maker.getCrcType())))\n+              .setBlockChecksum(ByteString.copyFrom(maker.getOutBytes()))\n+              .setCrcType(PBHelperClient.convert(maker.getCrcType()))\n+              .setBlockChecksumOptions(\n+                  PBHelperClient.convert(blockChecksumOptions)))\n           .build()\n           .writeDelimitedTo(out);\n       out.flush();\n     } catch (IOException ioe) {\n       LOG.info(\"blockChecksum {} received exception {}\",\n           block, ioe.toString());\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockChecksum(ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      BlockChecksumOptions blockChecksumOptions)\n      throws IOException {\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken, Op.BLOCK_CHECKSUM,\n        BlockTokenIdentifier.AccessMode.READ);\n    BlockChecksumComputer maker \u003d new ReplicatedBlockChecksumComputer(\n        datanode, block, blockChecksumOptions);\n\n    try {\n      maker.compute();\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n          .setStatus(SUCCESS)\n          .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()\n              .setBytesPerCrc(maker.getBytesPerCRC())\n              .setCrcPerBlock(maker.getCrcPerBlock())\n              .setBlockChecksum(ByteString.copyFrom(maker.getOutBytes()))\n              .setCrcType(PBHelperClient.convert(maker.getCrcType()))\n              .setBlockChecksumOptions(\n                  PBHelperClient.convert(blockChecksumOptions)))\n          .build()\n          .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum {} received exception {}\",\n          block, ioe.toString());\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {
            "oldValue": "[block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e]",
            "newValue": "[block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, blockChecksumOptions-BlockChecksumOptions]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13056. Expose file-level composite CRCs in HDFS which are comparable across different instances/layouts. Contributed by Dennis Huo.\n",
          "commitDate": "10/04/18 9:31 PM",
          "commitName": "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
          "commitAuthor": "Xiao Chen",
          "commitDateOld": "14/02/18 8:20 AM",
          "commitNameOld": "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 55.51,
          "commitsBetweenForRepo": 494,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,40 @@\n   public void blockChecksum(ExtendedBlock block,\n-                            Token\u003cBlockTokenIdentifier\u003e blockToken)\n+      Token\u003cBlockTokenIdentifier\u003e blockToken,\n+      BlockChecksumOptions blockChecksumOptions)\n       throws IOException {\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken, Op.BLOCK_CHECKSUM,\n         BlockTokenIdentifier.AccessMode.READ);\n-    BlockChecksumComputer maker \u003d\n-        new ReplicatedBlockChecksumComputer(datanode, block);\n+    BlockChecksumComputer maker \u003d new ReplicatedBlockChecksumComputer(\n+        datanode, block, blockChecksumOptions);\n \n     try {\n       maker.compute();\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n           .setStatus(SUCCESS)\n           .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()\n               .setBytesPerCrc(maker.getBytesPerCRC())\n               .setCrcPerBlock(maker.getCrcPerBlock())\n-              .setMd5(ByteString.copyFrom(maker.getOutBytes()))\n-              .setCrcType(PBHelperClient.convert(maker.getCrcType())))\n+              .setBlockChecksum(ByteString.copyFrom(maker.getOutBytes()))\n+              .setCrcType(PBHelperClient.convert(maker.getCrcType()))\n+              .setBlockChecksumOptions(\n+                  PBHelperClient.convert(blockChecksumOptions)))\n           .build()\n           .writeDelimitedTo(out);\n       out.flush();\n     } catch (IOException ioe) {\n       LOG.info(\"blockChecksum {} received exception {}\",\n           block, ioe.toString());\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockChecksum(ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      BlockChecksumOptions blockChecksumOptions)\n      throws IOException {\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken, Op.BLOCK_CHECKSUM,\n        BlockTokenIdentifier.AccessMode.READ);\n    BlockChecksumComputer maker \u003d new ReplicatedBlockChecksumComputer(\n        datanode, block, blockChecksumOptions);\n\n    try {\n      maker.compute();\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n          .setStatus(SUCCESS)\n          .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()\n              .setBytesPerCrc(maker.getBytesPerCRC())\n              .setCrcPerBlock(maker.getCrcPerBlock())\n              .setBlockChecksum(ByteString.copyFrom(maker.getOutBytes()))\n              .setCrcType(PBHelperClient.convert(maker.getCrcType()))\n              .setBlockChecksumOptions(\n                  PBHelperClient.convert(blockChecksumOptions)))\n          .build()\n          .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum {} received exception {}\",\n          block, ioe.toString());\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {}
        }
      ]
    },
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10571. Use Log.*(Object, Throwable) overload to log exceptions.\nContributed by Andras Bokor.\n",
      "commitDate": "14/02/18 8:20 AM",
      "commitName": "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "01/11/17 1:41 AM",
      "commitNameOld": "56b88b06705441f6f171eec7fb2fa77946ca204b",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 105.32,
      "commitsBetweenForRepo": 696,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,37 @@\n   public void blockChecksum(ExtendedBlock block,\n                             Token\u003cBlockTokenIdentifier\u003e blockToken)\n       throws IOException {\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken, Op.BLOCK_CHECKSUM,\n         BlockTokenIdentifier.AccessMode.READ);\n     BlockChecksumComputer maker \u003d\n         new ReplicatedBlockChecksumComputer(datanode, block);\n \n     try {\n       maker.compute();\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n           .setStatus(SUCCESS)\n           .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()\n               .setBytesPerCrc(maker.getBytesPerCRC())\n               .setCrcPerBlock(maker.getCrcPerBlock())\n               .setMd5(ByteString.copyFrom(maker.getOutBytes()))\n               .setCrcType(PBHelperClient.convert(maker.getCrcType())))\n           .build()\n           .writeDelimitedTo(out);\n       out.flush();\n     } catch (IOException ioe) {\n-      LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n+      LOG.info(\"blockChecksum {} received exception {}\",\n+          block, ioe.toString());\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(ExtendedBlock block,\n                            Token\u003cBlockTokenIdentifier\u003e blockToken)\n      throws IOException {\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken, Op.BLOCK_CHECKSUM,\n        BlockTokenIdentifier.AccessMode.READ);\n    BlockChecksumComputer maker \u003d\n        new ReplicatedBlockChecksumComputer(datanode, block);\n\n    try {\n      maker.compute();\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n          .setStatus(SUCCESS)\n          .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()\n              .setBytesPerCrc(maker.getBytesPerCRC())\n              .setCrcPerBlock(maker.getCrcPerBlock())\n              .setMd5(ByteString.copyFrom(maker.getOutBytes()))\n              .setCrcType(PBHelperClient.convert(maker.getCrcType())))\n          .build()\n          .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum {} received exception {}\",\n          block, ioe.toString());\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6": {
      "type": "Ymultichange(Ybodychange,Yparametermetachange)",
      "commitMessage": "HDFS-9733. Refactor DFSClient#getFileChecksum and DataXceiver#blockChecksum. Contributed by Kai Zheng\n",
      "commitDate": "29/02/16 9:52 PM",
      "commitName": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
      "commitAuthor": "Uma Maheswara Rao G",
      "subchanges": [
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9733. Refactor DFSClient#getFileChecksum and DataXceiver#blockChecksum. Contributed by Kai Zheng\n",
          "commitDate": "29/02/16 9:52 PM",
          "commitName": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "26/02/16 3:32 PM",
          "commitNameOld": "d1d4e16690cc85f7f22fbead9cf596260819b561",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 3.26,
          "commitsBetweenForRepo": 21,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,37 @@\n-  public void blockChecksum(final ExtendedBlock block,\n-      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+  public void blockChecksum(ExtendedBlock block,\n+                            Token\u003cBlockTokenIdentifier\u003e blockToken)\n+      throws IOException {\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n-    // client side now can specify a range of the block for checksum\n-    long requestLength \u003d block.getNumBytes();\n-    Preconditions.checkArgument(requestLength \u003e\u003d 0);\n-    long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n-    boolean partialBlk \u003d requestLength \u003c visibleLength;\n \n-    final LengthInputStream metadataIn \u003d datanode.data\n-        .getMetaDataInputStream(block);\n-    \n-    final DataInputStream checksumIn \u003d new DataInputStream(\n-        new BufferedInputStream(metadataIn, ioFileBufferSize));\n+    BlockChecksumComputer maker \u003d\n+        new ReplicatedBlockChecksumComputer(datanode, block);\n+\n     try {\n-      //read metadata file\n-      final BlockMetadataHeader header \u003d BlockMetadataHeader\n-          .readHeader(checksumIn);\n-      final DataChecksum checksum \u003d header.getChecksum();\n-      final int csize \u003d checksum.getChecksumSize();\n-      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n-      final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n-        (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n-\n-      final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n-          calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n-            : MD5Hash.digest(checksumIn);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n-            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n-      }\n+      maker.compute();\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n-        .setStatus(SUCCESS)\n-        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n-          .setBytesPerCrc(bytesPerCRC)\n-          .setCrcPerBlock(crcPerBlock)\n-          .setMd5(ByteString.copyFrom(md5.getDigest()))\n-          .setCrcType(PBHelperClient.convert(checksum.getChecksumType())))\n-        .build()\n-        .writeDelimitedTo(out);\n+          .setStatus(SUCCESS)\n+          .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()\n+              .setBytesPerCrc(maker.getBytesPerCRC())\n+              .setCrcPerBlock(maker.getCrcPerBlock())\n+              .setMd5(ByteString.copyFrom(maker.getOutBytes()))\n+              .setCrcType(PBHelperClient.convert(maker.getCrcType())))\n+          .build()\n+          .writeDelimitedTo(out);\n       out.flush();\n     } catch (IOException ioe) {\n       LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n-      IOUtils.closeStream(checksumIn);\n-      IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockChecksum(ExtendedBlock block,\n                            Token\u003cBlockTokenIdentifier\u003e blockToken)\n      throws IOException {\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n\n    BlockChecksumComputer maker \u003d\n        new ReplicatedBlockChecksumComputer(datanode, block);\n\n    try {\n      maker.compute();\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n          .setStatus(SUCCESS)\n          .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()\n              .setBytesPerCrc(maker.getBytesPerCRC())\n              .setCrcPerBlock(maker.getCrcPerBlock())\n              .setMd5(ByteString.copyFrom(maker.getOutBytes()))\n              .setCrcType(PBHelperClient.convert(maker.getCrcType())))\n          .build()\n          .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HDFS-9733. Refactor DFSClient#getFileChecksum and DataXceiver#blockChecksum. Contributed by Kai Zheng\n",
          "commitDate": "29/02/16 9:52 PM",
          "commitName": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "26/02/16 3:32 PM",
          "commitNameOld": "d1d4e16690cc85f7f22fbead9cf596260819b561",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 3.26,
          "commitsBetweenForRepo": 21,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,37 @@\n-  public void blockChecksum(final ExtendedBlock block,\n-      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+  public void blockChecksum(ExtendedBlock block,\n+                            Token\u003cBlockTokenIdentifier\u003e blockToken)\n+      throws IOException {\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n-    // client side now can specify a range of the block for checksum\n-    long requestLength \u003d block.getNumBytes();\n-    Preconditions.checkArgument(requestLength \u003e\u003d 0);\n-    long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n-    boolean partialBlk \u003d requestLength \u003c visibleLength;\n \n-    final LengthInputStream metadataIn \u003d datanode.data\n-        .getMetaDataInputStream(block);\n-    \n-    final DataInputStream checksumIn \u003d new DataInputStream(\n-        new BufferedInputStream(metadataIn, ioFileBufferSize));\n+    BlockChecksumComputer maker \u003d\n+        new ReplicatedBlockChecksumComputer(datanode, block);\n+\n     try {\n-      //read metadata file\n-      final BlockMetadataHeader header \u003d BlockMetadataHeader\n-          .readHeader(checksumIn);\n-      final DataChecksum checksum \u003d header.getChecksum();\n-      final int csize \u003d checksum.getChecksumSize();\n-      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n-      final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n-        (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n-\n-      final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n-          calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n-            : MD5Hash.digest(checksumIn);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n-            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n-      }\n+      maker.compute();\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n-        .setStatus(SUCCESS)\n-        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n-          .setBytesPerCrc(bytesPerCRC)\n-          .setCrcPerBlock(crcPerBlock)\n-          .setMd5(ByteString.copyFrom(md5.getDigest()))\n-          .setCrcType(PBHelperClient.convert(checksum.getChecksumType())))\n-        .build()\n-        .writeDelimitedTo(out);\n+          .setStatus(SUCCESS)\n+          .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()\n+              .setBytesPerCrc(maker.getBytesPerCRC())\n+              .setCrcPerBlock(maker.getCrcPerBlock())\n+              .setMd5(ByteString.copyFrom(maker.getOutBytes()))\n+              .setCrcType(PBHelperClient.convert(maker.getCrcType())))\n+          .build()\n+          .writeDelimitedTo(out);\n       out.flush();\n     } catch (IOException ioe) {\n       LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n-      IOUtils.closeStream(checksumIn);\n-      IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockChecksum(ExtendedBlock block,\n                            Token\u003cBlockTokenIdentifier\u003e blockToken)\n      throws IOException {\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n\n    BlockChecksumComputer maker \u003d\n        new ReplicatedBlockChecksumComputer(datanode, block);\n\n    try {\n      maker.compute();\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n          .setStatus(SUCCESS)\n          .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()\n              .setBytesPerCrc(maker.getBytesPerCRC())\n              .setCrcPerBlock(maker.getCrcPerBlock())\n              .setMd5(ByteString.copyFrom(maker.getOutBytes()))\n              .setCrcType(PBHelperClient.convert(maker.getCrcType())))\n          .build()\n          .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {
            "oldValue": "[block-ExtendedBlock(modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]",
            "newValue": "[block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e]"
          }
        }
      ]
    },
    "38c4c14472996562eb3d610649246770c2888c6b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9574. Reduce client failures during datanode restart. Contributed by Kihwal Lee.\n",
      "commitDate": "08/01/16 9:13 AM",
      "commitName": "38c4c14472996562eb3d610649246770c2888c6b",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "04/01/16 2:32 PM",
      "commitNameOld": "778146eaae5b1e17928a1f26fb1e46536a6ee510",
      "commitAuthorOld": "Uma Mahesh",
      "daysBetweenCommits": 3.78,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,60 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+    updateCurrentThreadName(\"Getting checksum for block \" + block);\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n     // client side now can specify a range of the block for checksum\n     long requestLength \u003d block.getNumBytes();\n     Preconditions.checkArgument(requestLength \u003e\u003d 0);\n     long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n     boolean partialBlk \u003d requestLength \u003c visibleLength;\n \n-    updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final LengthInputStream metadataIn \u003d datanode.data\n         .getMetaDataInputStream(block);\n     \n     final DataInputStream checksumIn \u003d new DataInputStream(\n         new BufferedInputStream(metadataIn, ioFileBufferSize));\n-    updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader\n           .readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum();\n       final int csize \u003d checksum.getChecksumSize();\n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n         (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n \n       final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n           calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n             : MD5Hash.digest(checksumIn);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           .setCrcType(PBHelperClient.convert(checksum.getChecksumType())))\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } catch (IOException ioe) {\n       LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n    // client side now can specify a range of the block for checksum\n    long requestLength \u003d block.getNumBytes();\n    Preconditions.checkArgument(requestLength \u003e\u003d 0);\n    long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n    boolean partialBlk \u003d requestLength \u003c visibleLength;\n\n    final LengthInputStream metadataIn \u003d datanode.data\n        .getMetaDataInputStream(block);\n    \n    final DataInputStream checksumIn \u003d new DataInputStream(\n        new BufferedInputStream(metadataIn, ioFileBufferSize));\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader\n          .readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum();\n      final int csize \u003d checksum.getChecksumSize();\n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n        (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n\n      final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n          calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n            : MD5Hash.digest(checksumIn);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelperClient.convert(checksum.getChecksumType())))\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8934. Move ShortCircuitShm to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "22/08/15 1:31 PM",
      "commitName": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/08/15 11:28 AM",
      "commitNameOld": "3aac4758b007a56e3d66998d457b2156effca528",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.09,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n     // client side now can specify a range of the block for checksum\n     long requestLength \u003d block.getNumBytes();\n     Preconditions.checkArgument(requestLength \u003e\u003d 0);\n     long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n     boolean partialBlk \u003d requestLength \u003c visibleLength;\n \n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final LengthInputStream metadataIn \u003d datanode.data\n         .getMetaDataInputStream(block);\n     \n     final DataInputStream checksumIn \u003d new DataInputStream(\n         new BufferedInputStream(metadataIn, ioFileBufferSize));\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader\n           .readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum();\n       final int csize \u003d checksum.getChecksumSize();\n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n         (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n \n       final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n           calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n             : MD5Hash.digest(checksumIn);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n-          .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n+          .setCrcType(PBHelperClient.convert(checksum.getChecksumType())))\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } catch (IOException ioe) {\n       LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n    // client side now can specify a range of the block for checksum\n    long requestLength \u003d block.getNumBytes();\n    Preconditions.checkArgument(requestLength \u003e\u003d 0);\n    long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n    boolean partialBlk \u003d requestLength \u003c visibleLength;\n\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d datanode.data\n        .getMetaDataInputStream(block);\n    \n    final DataInputStream checksumIn \u003d new DataInputStream(\n        new BufferedInputStream(metadataIn, ioFileBufferSize));\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader\n          .readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum();\n      final int csize \u003d checksum.getChecksumSize();\n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n        (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n\n      final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n          calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n            : MD5Hash.digest(checksumIn);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelperClient.convert(checksum.getChecksumType())))\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "4da8490b512a33a255ed27309860859388d7c168": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
      "commitDate": "05/05/15 3:41 PM",
      "commitName": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.23,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n     // client side now can specify a range of the block for checksum\n     long requestLength \u003d block.getNumBytes();\n     Preconditions.checkArgument(requestLength \u003e\u003d 0);\n     long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n     boolean partialBlk \u003d requestLength \u003c visibleLength;\n \n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final LengthInputStream metadataIn \u003d datanode.data\n         .getMetaDataInputStream(block);\n     \n     final DataInputStream checksumIn \u003d new DataInputStream(\n-        new BufferedInputStream(metadataIn, HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n+        new BufferedInputStream(metadataIn, ioFileBufferSize));\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader\n           .readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum();\n       final int csize \u003d checksum.getChecksumSize();\n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n         (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n \n       final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n           calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n             : MD5Hash.digest(checksumIn);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } catch (IOException ioe) {\n       LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n    // client side now can specify a range of the block for checksum\n    long requestLength \u003d block.getNumBytes();\n    Preconditions.checkArgument(requestLength \u003e\u003d 0);\n    long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n    boolean partialBlk \u003d requestLength \u003c visibleLength;\n\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d datanode.data\n        .getMetaDataInputStream(block);\n    \n    final DataInputStream checksumIn \u003d new DataInputStream(\n        new BufferedInputStream(metadataIn, ioFileBufferSize));\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader\n          .readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum();\n      final int csize \u003d checksum.getChecksumSize();\n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n        (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n\n      final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n          calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n            : MD5Hash.digest(checksumIn);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "23/04/15 7:00 PM",
      "commitNameOld": "a0e0a63209b5eb17dca5cc503be36aa52defeabd",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 8.63,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n     // client side now can specify a range of the block for checksum\n     long requestLength \u003d block.getNumBytes();\n     Preconditions.checkArgument(requestLength \u003e\u003d 0);\n     long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n     boolean partialBlk \u003d requestLength \u003c visibleLength;\n \n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final LengthInputStream metadataIn \u003d datanode.data\n         .getMetaDataInputStream(block);\n     \n     final DataInputStream checksumIn \u003d new DataInputStream(\n-        new BufferedInputStream(metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n+        new BufferedInputStream(metadataIn, HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader\n           .readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum();\n       final int csize \u003d checksum.getChecksumSize();\n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n         (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n \n       final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n           calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n             : MD5Hash.digest(checksumIn);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } catch (IOException ioe) {\n       LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n    // client side now can specify a range of the block for checksum\n    long requestLength \u003d block.getNumBytes();\n    Preconditions.checkArgument(requestLength \u003e\u003d 0);\n    long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n    boolean partialBlk \u003d requestLength \u003c visibleLength;\n\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d datanode.data\n        .getMetaDataInputStream(block);\n    \n    final DataInputStream checksumIn \u003d new DataInputStream(\n        new BufferedInputStream(metadataIn, HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader\n          .readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum();\n      final int csize \u003d checksum.getChecksumSize();\n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n        (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n\n      final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n          calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n            : MD5Hash.digest(checksumIn);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8103. Move BlockTokenSecretManager.AccessMode into BlockTokenIdentifier. Contributed by Haohui Mai.\n",
      "commitDate": "10/04/15 4:36 PM",
      "commitName": "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "20/03/15 12:02 PM",
      "commitNameOld": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 21.19,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n-        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n+        Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n     // client side now can specify a range of the block for checksum\n     long requestLength \u003d block.getNumBytes();\n     Preconditions.checkArgument(requestLength \u003e\u003d 0);\n     long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n     boolean partialBlk \u003d requestLength \u003c visibleLength;\n \n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final LengthInputStream metadataIn \u003d datanode.data\n         .getMetaDataInputStream(block);\n     \n     final DataInputStream checksumIn \u003d new DataInputStream(\n         new BufferedInputStream(metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader\n           .readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum();\n       final int csize \u003d checksum.getChecksumSize();\n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n         (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n \n       final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n           calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n             : MD5Hash.digest(checksumIn);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } catch (IOException ioe) {\n       LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenIdentifier.AccessMode.READ);\n    // client side now can specify a range of the block for checksum\n    long requestLength \u003d block.getNumBytes();\n    Preconditions.checkArgument(requestLength \u003e\u003d 0);\n    long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n    boolean partialBlk \u003d requestLength \u003c visibleLength;\n\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d datanode.data\n        .getMetaDataInputStream(block);\n    \n    final DataInputStream checksumIn \u003d new DataInputStream(\n        new BufferedInputStream(metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader\n          .readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum();\n      final int csize \u003d checksum.getChecksumSize();\n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n        (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n\n      final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n          calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n            : MD5Hash.digest(checksumIn);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "2d4f3e567e4bb8068c028de12df118a4f3fa6343": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7331. Add Datanode network counts to datanode jmx page. Contributed by Charles Lamb.\n",
      "commitDate": "21/11/14 4:36 PM",
      "commitName": "2d4f3e567e4bb8068c028de12df118a4f3fa6343",
      "commitAuthor": "Aaron T. Myers",
      "commitDateOld": "08/11/14 10:24 PM",
      "commitNameOld": "9ba8d8c7eb65eeb6fe673f04e493d9eedd95a822",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 12.76,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     // client side now can specify a range of the block for checksum\n     long requestLength \u003d block.getNumBytes();\n     Preconditions.checkArgument(requestLength \u003e\u003d 0);\n     long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n     boolean partialBlk \u003d requestLength \u003c visibleLength;\n \n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final LengthInputStream metadataIn \u003d datanode.data\n         .getMetaDataInputStream(block);\n     \n     final DataInputStream checksumIn \u003d new DataInputStream(\n         new BufferedInputStream(metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader\n           .readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum();\n       final int csize \u003d checksum.getChecksumSize();\n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n         (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n \n       final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n           calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n             : MD5Hash.digest(checksumIn);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } catch (IOException ioe) {\n       LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n-      datanode.metrics.incrDatanodeNetworkErrors();\n+      incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    // client side now can specify a range of the block for checksum\n    long requestLength \u003d block.getNumBytes();\n    Preconditions.checkArgument(requestLength \u003e\u003d 0);\n    long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n    boolean partialBlk \u003d requestLength \u003c visibleLength;\n\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d datanode.data\n        .getMetaDataInputStream(block);\n    \n    final DataInputStream checksumIn \u003d new DataInputStream(\n        new BufferedInputStream(metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader\n          .readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum();\n      final int csize \u003d checksum.getChecksumSize();\n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n        (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n\n      final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n          calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n            : MD5Hash.digest(checksumIn);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "86cad007d7d6366b293bb9a073814889081c8662": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7222. Expose DataNode network errors as a metric. (Charles Lamb via wang)\n",
      "commitDate": "23/10/14 12:53 PM",
      "commitName": "86cad007d7d6366b293bb9a073814889081c8662",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/08/14 9:47 PM",
      "commitNameOld": "a317bd7b02c37bd57743bfad59593ec12f53f4ed",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 56.63,
      "commitsBetweenForRepo": 574,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,61 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     // client side now can specify a range of the block for checksum\n     long requestLength \u003d block.getNumBytes();\n     Preconditions.checkArgument(requestLength \u003e\u003d 0);\n     long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n     boolean partialBlk \u003d requestLength \u003c visibleLength;\n \n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final LengthInputStream metadataIn \u003d datanode.data\n         .getMetaDataInputStream(block);\n     \n     final DataInputStream checksumIn \u003d new DataInputStream(\n         new BufferedInputStream(metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader\n           .readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum();\n       final int csize \u003d checksum.getChecksumSize();\n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n         (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n \n       final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n           calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n             : MD5Hash.digest(checksumIn);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n+    } catch (IOException ioe) {\n+      LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n+      datanode.metrics.incrDatanodeNetworkErrors();\n+      throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    // client side now can specify a range of the block for checksum\n    long requestLength \u003d block.getNumBytes();\n    Preconditions.checkArgument(requestLength \u003e\u003d 0);\n    long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n    boolean partialBlk \u003d requestLength \u003c visibleLength;\n\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d datanode.data\n        .getMetaDataInputStream(block);\n    \n    final DataInputStream checksumIn \u003d new DataInputStream(\n        new BufferedInputStream(metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader\n          .readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum();\n      final int csize \u003d checksum.getChecksumSize();\n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n        (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n\n      final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n          calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n            : MD5Hash.digest(checksumIn);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } catch (IOException ioe) {\n      LOG.info(\"blockChecksum \" + block + \" received exception \" + ioe);\n      datanode.metrics.incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "3671a5e16fbddbe5a0516289ce98e1305e02291c": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5899. Support incremental data copy in DistCp. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596931 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/05/14 11:17 AM",
      "commitName": "3671a5e16fbddbe5a0516289ce98e1305e02291c",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "23/04/14 1:13 PM",
      "commitNameOld": "876fd8ab7913a259ff9f69c16cc2d9af46ad3f9b",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 28.92,
      "commitsBetweenForRepo": 168,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,57 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n-    updateCurrentThreadName(\"Reading metadata for block \" + block);\n-    final LengthInputStream metadataIn \u003d \n-      datanode.data.getMetaDataInputStream(block);\n-    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n-        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n+    // client side now can specify a range of the block for checksum\n+    long requestLength \u003d block.getNumBytes();\n+    Preconditions.checkArgument(requestLength \u003e\u003d 0);\n+    long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n+    boolean partialBlk \u003d requestLength \u003c visibleLength;\n \n+    updateCurrentThreadName(\"Reading metadata for block \" + block);\n+    final LengthInputStream metadataIn \u003d datanode.data\n+        .getMetaDataInputStream(block);\n+    \n+    final DataInputStream checksumIn \u003d new DataInputStream(\n+        new BufferedInputStream(metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n-      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n-      final DataChecksum checksum \u003d header.getChecksum(); \n+      final BlockMetadataHeader header \u003d BlockMetadataHeader\n+          .readHeader(checksumIn);\n+      final DataChecksum checksum \u003d header.getChecksum();\n+      final int csize \u003d checksum.getChecksumSize();\n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n-      final long crcPerBlock \u003d checksum.getChecksumSize() \u003e 0 \n-              ? (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize()\n-              : 0;\n-      \n-      //compute block checksum\n-      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n+      final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n+        (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n \n+      final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n+          calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n+            : MD5Hash.digest(checksumIn);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n-          .setCrcType(PBHelper.convert(checksum.getChecksumType()))\n-          )\n+          .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    // client side now can specify a range of the block for checksum\n    long requestLength \u003d block.getNumBytes();\n    Preconditions.checkArgument(requestLength \u003e\u003d 0);\n    long visibleLength \u003d datanode.data.getReplicaVisibleLength(block);\n    boolean partialBlk \u003d requestLength \u003c visibleLength;\n\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d datanode.data\n        .getMetaDataInputStream(block);\n    \n    final DataInputStream checksumIn \u003d new DataInputStream(\n        new BufferedInputStream(metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader\n          .readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum();\n      final int csize \u003d checksum.getChecksumSize();\n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d csize \u003c\u003d 0 ? 0 : \n        (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;\n\n      final MD5Hash md5 \u003d partialBlk \u0026\u0026 crcPerBlock \u003e 0 ? \n          calcPartialBlockChecksum(block, requestLength, checksum, checksumIn)\n            : MD5Hash.digest(checksumIn);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType())))\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "3d9ad8e3b60dd21db45466f4736abe6b1812b522": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5843. DFSClient.getFileChecksum() throws IOException if checksum is disabled. Contributed by Laurent Goujon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1562927 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/01/14 11:15 AM",
      "commitName": "3d9ad8e3b60dd21db45466f4736abe6b1812b522",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "09/12/13 9:38 AM",
      "commitNameOld": "43c33491fcfa5155455efd8161cf7a11b3630b2d",
      "commitAuthorOld": "",
      "daysBetweenCommits": 52.07,
      "commitsBetweenForRepo": 263,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,51 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final LengthInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n         metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n-      final long crcPerBlock \u003d (metadataIn.getLength()\n-          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n+      final long crcPerBlock \u003d checksum.getChecksumSize() \u003e 0 \n+              ? (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize()\n+              : 0;\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           .setCrcType(PBHelper.convert(checksum.getChecksumType()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d checksum.getChecksumSize() \u003e 0 \n              ? (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize()\n              : 0;\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4363. Combine PBHelper and HdfsProtoUtil and remove redundant methods. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431088 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:20 PM",
      "commitName": "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "08/01/13 6:39 PM",
      "commitNameOld": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.78,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,50 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final LengthInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n         metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d (metadataIn.getLength()\n           - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n-          .setCrcType(HdfsProtoUtil.toProto(checksum.getChecksumType()))\n+          .setCrcType(PBHelper.convert(checksum.getChecksumType()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "c46de830da98959f40dd41c95bdebecdfb9ea730": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3177. Update DFSClient and DataXceiver to handle different checkum types in file checksum computation.  Contributed by Kihwal Lee\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1376928 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/12 7:15 AM",
      "commitName": "c46de830da98959f40dd41c95bdebecdfb9ea730",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "14/08/12 1:59 PM",
      "commitNameOld": "f98d8eb291be364102b5c3011ce72e8f43eab389",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 9.72,
      "commitsBetweenForRepo": 76,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,50 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final LengthInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n         metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d (metadataIn.getLength()\n           - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n+          .setCrcType(HdfsProtoUtil.toProto(checksum.getChecksumType()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(HdfsProtoUtil.toProto(checksum.getChecksumType()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "15/07/12 7:58 PM",
      "commitNameOld": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 22.57,
      "commitsBetweenForRepo": 106,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n-        NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n+        getOutputStream());\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final LengthInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n         metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d (metadataIn.getLength()\n           - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "662b1887af4e39f3eadd7dda4953c7f2529b43bc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3088. Move FSDatasetInterface inner classes to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301661 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/03/12 10:32 AM",
      "commitName": "662b1887af4e39f3eadd7dda4953c7f2529b43bc",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "19/11/11 8:27 PM",
      "commitNameOld": "8e8bb50afd823a26d9a7ed1311ad050ef059fb5d",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 117.55,
      "commitsBetweenForRepo": 835,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n-    final MetaDataInputStream metadataIn \u003d \n+    final LengthInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n         metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d (metadataIn.getLength()\n           - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "905a127850d5e0cba85c2e075f989fa0f5cf129a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2562. Refactor DN configuration variables out of DataNode class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203543 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 1:04 AM",
      "commitName": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "31/10/11 10:17 PM",
      "commitNameOld": "1c940637b14eee777a65d153d0d712a1aea3866c",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 17.16,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n-        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n+        NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final MetaDataInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n         metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d (metadataIn.getLength()\n           - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final MetaDataInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1620. Rename HdfsConstants -\u003e HdfsServerConstants, FSConstants -\u003e HdfsConstants. (Harsh J Chouraria via atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165096 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/11 12:30 PM",
      "commitName": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 10.8,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final MetaDataInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n-        metadataIn, FSConstants.IO_FILE_BUFFER_SIZE));\n+        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d (metadataIn.getLength()\n           - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final MetaDataInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final MetaDataInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, FSConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final MetaDataInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, FSConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
      }
    },
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2241. Remove implementing FSConstants interface to just get the constants from the interface. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1156420 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/08/11 5:46 PM",
      "commitName": "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "12/07/11 6:11 PM",
      "commitNameOld": "2c5dd549e31aa5d3377ff2619ede8e92b8dc5d0f",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 28.98,
      "commitsBetweenForRepo": 108,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n   public void blockChecksum(final ExtendedBlock block,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final MetaDataInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n-        metadataIn, BUFFER_SIZE));\n+        metadataIn, FSConstants.IO_FILE_BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d (metadataIn.getLength()\n           - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final MetaDataInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, FSConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Yparametermetachange)",
      "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/06/11 4:57 PM",
      "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/06/11 4:57 PM",
          "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/06/11 10:12 AM",
          "commitNameOld": "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.28,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,49 @@\n-  protected void opBlockChecksum(DataInputStream in, ExtendedBlock block,\n-      Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+  public void blockChecksum(final ExtendedBlock block,\n+      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final MetaDataInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n         metadataIn, BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d (metadataIn.getLength()\n           - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final MetaDataInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {
            "oldValue": "opBlockChecksum",
            "newValue": "blockChecksum"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/06/11 4:57 PM",
          "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/06/11 10:12 AM",
          "commitNameOld": "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.28,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,49 @@\n-  protected void opBlockChecksum(DataInputStream in, ExtendedBlock block,\n-      Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+  public void blockChecksum(final ExtendedBlock block,\n+      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final MetaDataInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n         metadataIn, BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d (metadataIn.getLength()\n           - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final MetaDataInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {
            "oldValue": "[in-DataInputStream, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e]",
            "newValue": "[block-ExtendedBlock(modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/06/11 4:57 PM",
          "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/06/11 10:12 AM",
          "commitNameOld": "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.28,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,49 @@\n-  protected void opBlockChecksum(DataInputStream in, ExtendedBlock block,\n-      Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+  public void blockChecksum(final ExtendedBlock block,\n+      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final MetaDataInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n         metadataIn, BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d (metadataIn.getLength()\n           - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final MetaDataInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {
            "oldValue": "[protected]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/06/11 4:57 PM",
          "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/06/11 10:12 AM",
          "commitNameOld": "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.28,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,49 @@\n-  protected void opBlockChecksum(DataInputStream in, ExtendedBlock block,\n-      Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+  public void blockChecksum(final ExtendedBlock block,\n+      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n     checkAccess(out, true, block, blockToken,\n         Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n     updateCurrentThreadName(\"Reading metadata for block \" + block);\n     final MetaDataInputStream metadataIn \u003d \n       datanode.data.getMetaDataInputStream(block);\n     final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n         metadataIn, BUFFER_SIZE));\n \n     updateCurrentThreadName(\"Getting checksum for block \" + block);\n     try {\n       //read metadata file\n       final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       final DataChecksum checksum \u003d header.getChecksum(); \n       final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n       final long crcPerBlock \u003d (metadataIn.getLength()\n           - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n       \n       //compute block checksum\n       final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n             + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n       }\n \n       //write reply\n       BlockOpResponseProto.newBuilder()\n         .setStatus(SUCCESS)\n         .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n           .setBytesPerCrc(bytesPerCRC)\n           .setCrcPerBlock(crcPerBlock)\n           .setMd5(ByteString.copyFrom(md5.getDigest()))\n           )\n         .build()\n         .writeDelimitedTo(out);\n       out.flush();\n     } finally {\n       IOUtils.closeStream(out);\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(metadataIn);\n     }\n \n     //update metrics\n     datanode.metrics.addBlockChecksumOp(elapsed());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockChecksum(final ExtendedBlock block,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final MetaDataInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {
            "oldValue": "[in-DataInputStream, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e]",
            "newValue": "[block-ExtendedBlock(modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]"
          }
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,49 @@\n+  protected void opBlockChecksum(DataInputStream in, ExtendedBlock block,\n+      Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+    final DataOutputStream out \u003d new DataOutputStream(\n+        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n+    checkAccess(out, true, block, blockToken,\n+        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n+    updateCurrentThreadName(\"Reading metadata for block \" + block);\n+    final MetaDataInputStream metadataIn \u003d \n+      datanode.data.getMetaDataInputStream(block);\n+    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n+        metadataIn, BUFFER_SIZE));\n+\n+    updateCurrentThreadName(\"Getting checksum for block \" + block);\n+    try {\n+      //read metadata file\n+      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n+      final DataChecksum checksum \u003d header.getChecksum(); \n+      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n+      final long crcPerBlock \u003d (metadataIn.getLength()\n+          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n+      \n+      //compute block checksum\n+      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n+\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n+            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n+      }\n+\n+      //write reply\n+      BlockOpResponseProto.newBuilder()\n+        .setStatus(SUCCESS)\n+        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n+          .setBytesPerCrc(bytesPerCRC)\n+          .setCrcPerBlock(crcPerBlock)\n+          .setMd5(ByteString.copyFrom(md5.getDigest()))\n+          )\n+        .build()\n+        .writeDelimitedTo(out);\n+      out.flush();\n+    } finally {\n+      IOUtils.closeStream(out);\n+      IOUtils.closeStream(checksumIn);\n+      IOUtils.closeStream(metadataIn);\n+    }\n+\n+    //update metrics\n+    datanode.metrics.addBlockChecksumOp(elapsed());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void opBlockChecksum(DataInputStream in, ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final MetaDataInputStream metadataIn \u003d \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn \u003d new DataInputStream(new BufferedInputStream(\n        metadataIn, BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum \u003d header.getChecksum(); \n      final int bytesPerCRC \u003d checksum.getBytesPerChecksum();\n      final long crcPerBlock \u003d (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 \u003d MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block\u003d\" + block + \", bytesPerCRC\u003d\" + bytesPerCRC\n            + \", crcPerBlock\u003d\" + crcPerBlock + \", md5\u003d\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
    }
  }
}