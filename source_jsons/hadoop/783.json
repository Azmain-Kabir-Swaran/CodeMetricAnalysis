{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StripedDataStreamer.java",
  "functionName": "setupPipelineInternal",
  "functionId": "setupPipelineInternal___nodes-DatanodeInfo[]__nodeStorageTypes-StorageType[]__nodeStorageIDs-String[]",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripedDataStreamer.java",
  "functionStartLine": 125,
  "functionEndLine": 182,
  "numCommitsSeen": 14,
  "timeTaken": 1615,
  "changeHistory": [
    "db6252b6c3959220c6f985f940e2e731f99d8e30",
    "a3954ccab148bddc290cb96528e63ff19799bcc9",
    "627da6f7178e18aa41996969c408b6f344e297d1"
  ],
  "changeHistoryShort": {
    "db6252b6c3959220c6f985f940e2e731f99d8e30": "Ybodychange",
    "a3954ccab148bddc290cb96528e63ff19799bcc9": "Ymultichange(Yparameterchange,Ybodychange)",
    "627da6f7178e18aa41996969c408b6f344e297d1": "Ybodychange"
  },
  "changeHistoryDetails": {
    "db6252b6c3959220c6f985f940e2e731f99d8e30": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15210. EC : File write hanged when DN is shutdown by admin command. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "28/04/20 10:28 PM",
      "commitName": "db6252b6c3959220c6f985f940e2e731f99d8e30",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "05/05/17 12:01 PM",
      "commitNameOld": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 1089.44,
      "commitsBetweenForRepo": 7877,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,58 @@\n   protected void setupPipelineInternal(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, String[] nodeStorageIDs)\n       throws IOException {\n     boolean success \u003d false;\n     while (!success \u0026\u0026 !streamerClosed() \u0026\u0026 dfsClient.clientRunning) {\n       if (!handleRestartingDatanode()) {\n         return;\n       }\n       if (!handleBadDatanode()) {\n         // for striped streamer if it is datanode error then close the stream\n         // and return. no need to replace datanode\n         return;\n       }\n \n       // get a new generation stamp and an access token\n       final LocatedBlock lb \u003d coordinator.getNewBlocks().take(index);\n       long newGS \u003d lb.getBlock().getGenerationStamp();\n       setAccessToken(lb.getBlockToken());\n \n       // set up the pipeline again with the remaining nodes. when a striped\n       // data streamer comes here, it must be in external error state.\n-      assert getErrorState().hasExternalError();\n+      assert getErrorState().hasExternalError()\n+          || getErrorState().doWaitForRestart();\n       success \u003d createBlockOutputStream(nodes, nodeStorageTypes,\n           nodeStorageIDs, newGS, true);\n \n       failPacket4Testing();\n       getErrorState().checkRestartingNodeDeadline(nodes);\n \n       // notify coordinator the result of createBlockOutputStream\n       synchronized (coordinator) {\n         if (!streamerClosed()) {\n           coordinator.updateStreamer(this, success);\n           coordinator.notify();\n         } else {\n           success \u003d false;\n         }\n       }\n \n       if (success) {\n         // wait for results of other streamers\n         success \u003d coordinator.takeStreamerUpdateResult(index);\n         if (success) {\n           // if all succeeded, update its block using the new GS\n           updateBlockGS(newGS);\n         } else {\n           // otherwise close the block stream and restart the recovery process\n           closeStream();\n         }\n       } else {\n         // if fail, close the stream. The internal error state and last\n         // exception have already been set in createBlockOutputStream\n         // TODO: wait for restarting DataNodes during RollingUpgrade\n         closeStream();\n         setStreamerAsClosed();\n       }\n     } // while\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void setupPipelineInternal(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, String[] nodeStorageIDs)\n      throws IOException {\n    boolean success \u003d false;\n    while (!success \u0026\u0026 !streamerClosed() \u0026\u0026 dfsClient.clientRunning) {\n      if (!handleRestartingDatanode()) {\n        return;\n      }\n      if (!handleBadDatanode()) {\n        // for striped streamer if it is datanode error then close the stream\n        // and return. no need to replace datanode\n        return;\n      }\n\n      // get a new generation stamp and an access token\n      final LocatedBlock lb \u003d coordinator.getNewBlocks().take(index);\n      long newGS \u003d lb.getBlock().getGenerationStamp();\n      setAccessToken(lb.getBlockToken());\n\n      // set up the pipeline again with the remaining nodes. when a striped\n      // data streamer comes here, it must be in external error state.\n      assert getErrorState().hasExternalError()\n          || getErrorState().doWaitForRestart();\n      success \u003d createBlockOutputStream(nodes, nodeStorageTypes,\n          nodeStorageIDs, newGS, true);\n\n      failPacket4Testing();\n      getErrorState().checkRestartingNodeDeadline(nodes);\n\n      // notify coordinator the result of createBlockOutputStream\n      synchronized (coordinator) {\n        if (!streamerClosed()) {\n          coordinator.updateStreamer(this, success);\n          coordinator.notify();\n        } else {\n          success \u003d false;\n        }\n      }\n\n      if (success) {\n        // wait for results of other streamers\n        success \u003d coordinator.takeStreamerUpdateResult(index);\n        if (success) {\n          // if all succeeded, update its block using the new GS\n          updateBlockGS(newGS);\n        } else {\n          // otherwise close the block stream and restart the recovery process\n          closeStream();\n        }\n      } else {\n        // if fail, close the stream. The internal error state and last\n        // exception have already been set in createBlockOutputStream\n        // TODO: wait for restarting DataNodes during RollingUpgrade\n        closeStream();\n        setStreamerAsClosed();\n      }\n    } // while\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripedDataStreamer.java",
      "extendedDetails": {}
    },
    "a3954ccab148bddc290cb96528e63ff19799bcc9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
      "commitDate": "05/05/17 12:01 PM",
      "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthor": "Chris Douglas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "15/02/17 10:44 AM",
          "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 79.01,
          "commitsBetweenForRepo": 465,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,55 +1,57 @@\n   protected void setupPipelineInternal(DatanodeInfo[] nodes,\n-      StorageType[] nodeStorageTypes) throws IOException {\n+      StorageType[] nodeStorageTypes, String[] nodeStorageIDs)\n+      throws IOException {\n     boolean success \u003d false;\n     while (!success \u0026\u0026 !streamerClosed() \u0026\u0026 dfsClient.clientRunning) {\n       if (!handleRestartingDatanode()) {\n         return;\n       }\n       if (!handleBadDatanode()) {\n         // for striped streamer if it is datanode error then close the stream\n         // and return. no need to replace datanode\n         return;\n       }\n \n       // get a new generation stamp and an access token\n       final LocatedBlock lb \u003d coordinator.getNewBlocks().take(index);\n       long newGS \u003d lb.getBlock().getGenerationStamp();\n       setAccessToken(lb.getBlockToken());\n \n       // set up the pipeline again with the remaining nodes. when a striped\n       // data streamer comes here, it must be in external error state.\n       assert getErrorState().hasExternalError();\n-      success \u003d createBlockOutputStream(nodes, nodeStorageTypes, newGS, true);\n+      success \u003d createBlockOutputStream(nodes, nodeStorageTypes,\n+          nodeStorageIDs, newGS, true);\n \n       failPacket4Testing();\n       getErrorState().checkRestartingNodeDeadline(nodes);\n \n       // notify coordinator the result of createBlockOutputStream\n       synchronized (coordinator) {\n         if (!streamerClosed()) {\n           coordinator.updateStreamer(this, success);\n           coordinator.notify();\n         } else {\n           success \u003d false;\n         }\n       }\n \n       if (success) {\n         // wait for results of other streamers\n         success \u003d coordinator.takeStreamerUpdateResult(index);\n         if (success) {\n           // if all succeeded, update its block using the new GS\n           updateBlockGS(newGS);\n         } else {\n           // otherwise close the block stream and restart the recovery process\n           closeStream();\n         }\n       } else {\n         // if fail, close the stream. The internal error state and last\n         // exception have already been set in createBlockOutputStream\n         // TODO: wait for restarting DataNodes during RollingUpgrade\n         closeStream();\n         setStreamerAsClosed();\n       }\n     } // while\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void setupPipelineInternal(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, String[] nodeStorageIDs)\n      throws IOException {\n    boolean success \u003d false;\n    while (!success \u0026\u0026 !streamerClosed() \u0026\u0026 dfsClient.clientRunning) {\n      if (!handleRestartingDatanode()) {\n        return;\n      }\n      if (!handleBadDatanode()) {\n        // for striped streamer if it is datanode error then close the stream\n        // and return. no need to replace datanode\n        return;\n      }\n\n      // get a new generation stamp and an access token\n      final LocatedBlock lb \u003d coordinator.getNewBlocks().take(index);\n      long newGS \u003d lb.getBlock().getGenerationStamp();\n      setAccessToken(lb.getBlockToken());\n\n      // set up the pipeline again with the remaining nodes. when a striped\n      // data streamer comes here, it must be in external error state.\n      assert getErrorState().hasExternalError();\n      success \u003d createBlockOutputStream(nodes, nodeStorageTypes,\n          nodeStorageIDs, newGS, true);\n\n      failPacket4Testing();\n      getErrorState().checkRestartingNodeDeadline(nodes);\n\n      // notify coordinator the result of createBlockOutputStream\n      synchronized (coordinator) {\n        if (!streamerClosed()) {\n          coordinator.updateStreamer(this, success);\n          coordinator.notify();\n        } else {\n          success \u003d false;\n        }\n      }\n\n      if (success) {\n        // wait for results of other streamers\n        success \u003d coordinator.takeStreamerUpdateResult(index);\n        if (success) {\n          // if all succeeded, update its block using the new GS\n          updateBlockGS(newGS);\n        } else {\n          // otherwise close the block stream and restart the recovery process\n          closeStream();\n        }\n      } else {\n        // if fail, close the stream. The internal error state and last\n        // exception have already been set in createBlockOutputStream\n        // TODO: wait for restarting DataNodes during RollingUpgrade\n        closeStream();\n        setStreamerAsClosed();\n      }\n    } // while\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripedDataStreamer.java",
          "extendedDetails": {
            "oldValue": "[nodes-DatanodeInfo[], nodeStorageTypes-StorageType[]]",
            "newValue": "[nodes-DatanodeInfo[], nodeStorageTypes-StorageType[], nodeStorageIDs-String[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "15/02/17 10:44 AM",
          "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 79.01,
          "commitsBetweenForRepo": 465,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,55 +1,57 @@\n   protected void setupPipelineInternal(DatanodeInfo[] nodes,\n-      StorageType[] nodeStorageTypes) throws IOException {\n+      StorageType[] nodeStorageTypes, String[] nodeStorageIDs)\n+      throws IOException {\n     boolean success \u003d false;\n     while (!success \u0026\u0026 !streamerClosed() \u0026\u0026 dfsClient.clientRunning) {\n       if (!handleRestartingDatanode()) {\n         return;\n       }\n       if (!handleBadDatanode()) {\n         // for striped streamer if it is datanode error then close the stream\n         // and return. no need to replace datanode\n         return;\n       }\n \n       // get a new generation stamp and an access token\n       final LocatedBlock lb \u003d coordinator.getNewBlocks().take(index);\n       long newGS \u003d lb.getBlock().getGenerationStamp();\n       setAccessToken(lb.getBlockToken());\n \n       // set up the pipeline again with the remaining nodes. when a striped\n       // data streamer comes here, it must be in external error state.\n       assert getErrorState().hasExternalError();\n-      success \u003d createBlockOutputStream(nodes, nodeStorageTypes, newGS, true);\n+      success \u003d createBlockOutputStream(nodes, nodeStorageTypes,\n+          nodeStorageIDs, newGS, true);\n \n       failPacket4Testing();\n       getErrorState().checkRestartingNodeDeadline(nodes);\n \n       // notify coordinator the result of createBlockOutputStream\n       synchronized (coordinator) {\n         if (!streamerClosed()) {\n           coordinator.updateStreamer(this, success);\n           coordinator.notify();\n         } else {\n           success \u003d false;\n         }\n       }\n \n       if (success) {\n         // wait for results of other streamers\n         success \u003d coordinator.takeStreamerUpdateResult(index);\n         if (success) {\n           // if all succeeded, update its block using the new GS\n           updateBlockGS(newGS);\n         } else {\n           // otherwise close the block stream and restart the recovery process\n           closeStream();\n         }\n       } else {\n         // if fail, close the stream. The internal error state and last\n         // exception have already been set in createBlockOutputStream\n         // TODO: wait for restarting DataNodes during RollingUpgrade\n         closeStream();\n         setStreamerAsClosed();\n       }\n     } // while\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void setupPipelineInternal(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, String[] nodeStorageIDs)\n      throws IOException {\n    boolean success \u003d false;\n    while (!success \u0026\u0026 !streamerClosed() \u0026\u0026 dfsClient.clientRunning) {\n      if (!handleRestartingDatanode()) {\n        return;\n      }\n      if (!handleBadDatanode()) {\n        // for striped streamer if it is datanode error then close the stream\n        // and return. no need to replace datanode\n        return;\n      }\n\n      // get a new generation stamp and an access token\n      final LocatedBlock lb \u003d coordinator.getNewBlocks().take(index);\n      long newGS \u003d lb.getBlock().getGenerationStamp();\n      setAccessToken(lb.getBlockToken());\n\n      // set up the pipeline again with the remaining nodes. when a striped\n      // data streamer comes here, it must be in external error state.\n      assert getErrorState().hasExternalError();\n      success \u003d createBlockOutputStream(nodes, nodeStorageTypes,\n          nodeStorageIDs, newGS, true);\n\n      failPacket4Testing();\n      getErrorState().checkRestartingNodeDeadline(nodes);\n\n      // notify coordinator the result of createBlockOutputStream\n      synchronized (coordinator) {\n        if (!streamerClosed()) {\n          coordinator.updateStreamer(this, success);\n          coordinator.notify();\n        } else {\n          success \u003d false;\n        }\n      }\n\n      if (success) {\n        // wait for results of other streamers\n        success \u003d coordinator.takeStreamerUpdateResult(index);\n        if (success) {\n          // if all succeeded, update its block using the new GS\n          updateBlockGS(newGS);\n        } else {\n          // otherwise close the block stream and restart the recovery process\n          closeStream();\n        }\n      } else {\n        // if fail, close the stream. The internal error state and last\n        // exception have already been set in createBlockOutputStream\n        // TODO: wait for restarting DataNodes during RollingUpgrade\n        closeStream();\n        setStreamerAsClosed();\n      }\n    } // while\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripedDataStreamer.java",
          "extendedDetails": {}
        }
      ]
    },
    "627da6f7178e18aa41996969c408b6f344e297d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8498. Blocks can be committed with wrong size. Contributed by Jing Zhao.\n",
      "commitDate": "15/02/17 10:44 AM",
      "commitName": "627da6f7178e18aa41996969c408b6f344e297d1",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "17/05/16 3:57 PM",
      "commitNameOld": "16c07cc68a3e0a06f57b7f4c7207cc8e5dce211f",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 273.82,
      "commitsBetweenForRepo": 1886,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   protected void setupPipelineInternal(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes) throws IOException {\n     boolean success \u003d false;\n     while (!success \u0026\u0026 !streamerClosed() \u0026\u0026 dfsClient.clientRunning) {\n       if (!handleRestartingDatanode()) {\n         return;\n       }\n       if (!handleBadDatanode()) {\n         // for striped streamer if it is datanode error then close the stream\n         // and return. no need to replace datanode\n         return;\n       }\n \n       // get a new generation stamp and an access token\n       final LocatedBlock lb \u003d coordinator.getNewBlocks().take(index);\n       long newGS \u003d lb.getBlock().getGenerationStamp();\n       setAccessToken(lb.getBlockToken());\n \n       // set up the pipeline again with the remaining nodes. when a striped\n       // data streamer comes here, it must be in external error state.\n       assert getErrorState().hasExternalError();\n       success \u003d createBlockOutputStream(nodes, nodeStorageTypes, newGS, true);\n \n       failPacket4Testing();\n       getErrorState().checkRestartingNodeDeadline(nodes);\n \n       // notify coordinator the result of createBlockOutputStream\n       synchronized (coordinator) {\n         if (!streamerClosed()) {\n           coordinator.updateStreamer(this, success);\n           coordinator.notify();\n         } else {\n           success \u003d false;\n         }\n       }\n \n       if (success) {\n         // wait for results of other streamers\n         success \u003d coordinator.takeStreamerUpdateResult(index);\n         if (success) {\n           // if all succeeded, update its block using the new GS\n-          block \u003d newBlock(block, newGS);\n+          updateBlockGS(newGS);\n         } else {\n           // otherwise close the block stream and restart the recovery process\n           closeStream();\n         }\n       } else {\n         // if fail, close the stream. The internal error state and last\n         // exception have already been set in createBlockOutputStream\n         // TODO: wait for restarting DataNodes during RollingUpgrade\n         closeStream();\n         setStreamerAsClosed();\n       }\n     } // while\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void setupPipelineInternal(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes) throws IOException {\n    boolean success \u003d false;\n    while (!success \u0026\u0026 !streamerClosed() \u0026\u0026 dfsClient.clientRunning) {\n      if (!handleRestartingDatanode()) {\n        return;\n      }\n      if (!handleBadDatanode()) {\n        // for striped streamer if it is datanode error then close the stream\n        // and return. no need to replace datanode\n        return;\n      }\n\n      // get a new generation stamp and an access token\n      final LocatedBlock lb \u003d coordinator.getNewBlocks().take(index);\n      long newGS \u003d lb.getBlock().getGenerationStamp();\n      setAccessToken(lb.getBlockToken());\n\n      // set up the pipeline again with the remaining nodes. when a striped\n      // data streamer comes here, it must be in external error state.\n      assert getErrorState().hasExternalError();\n      success \u003d createBlockOutputStream(nodes, nodeStorageTypes, newGS, true);\n\n      failPacket4Testing();\n      getErrorState().checkRestartingNodeDeadline(nodes);\n\n      // notify coordinator the result of createBlockOutputStream\n      synchronized (coordinator) {\n        if (!streamerClosed()) {\n          coordinator.updateStreamer(this, success);\n          coordinator.notify();\n        } else {\n          success \u003d false;\n        }\n      }\n\n      if (success) {\n        // wait for results of other streamers\n        success \u003d coordinator.takeStreamerUpdateResult(index);\n        if (success) {\n          // if all succeeded, update its block using the new GS\n          updateBlockGS(newGS);\n        } else {\n          // otherwise close the block stream and restart the recovery process\n          closeStream();\n        }\n      } else {\n        // if fail, close the stream. The internal error state and last\n        // exception have already been set in createBlockOutputStream\n        // TODO: wait for restarting DataNodes during RollingUpgrade\n        closeStream();\n        setStreamerAsClosed();\n      }\n    } // while\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripedDataStreamer.java",
      "extendedDetails": {}
    }
  }
}