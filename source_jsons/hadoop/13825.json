{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "validateReconstructionWork",
  "functionId": "validateReconstructionWork___rw-BlockReconstructionWork",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 2195,
  "functionEndLine": 2254,
  "numCommitsSeen": 792,
  "timeTaken": 15767,
  "changeHistory": [
    "a98352ced18e51003b443e1a652d19ec00b2f2d2",
    "c99a12167ff9566012ef32104a3964887d62c899",
    "de4894936a5b581572f35fa5b8979d9f23da0891",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "743a99f2dbc9a27e19f92ff3551937d90dba2e89",
    "e54cc2931262bf49682a8323da9811976218c03b",
    "972782d9568e0849484c027f27c1638ba50ec56e",
    "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
    "3a9571308e99cc374681bbc451a517d41a150aa0",
    "8a91109d16394310f2568717f103e6fff7cbddb0",
    "5ba2b98d0fe29603e136fc43a14f853e820cf7e2",
    "e27c2ae8bafc94f18eb38f5d839dcef5652d424e",
    "f62237bc2f02afe11ce185e13aa51a60b5960037"
  ],
  "changeHistoryShort": {
    "a98352ced18e51003b443e1a652d19ec00b2f2d2": "Ybodychange",
    "c99a12167ff9566012ef32104a3964887d62c899": "Ybodychange",
    "de4894936a5b581572f35fa5b8979d9f23da0891": "Ybodychange",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": "Ybodychange",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": "Ybodychange",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ybodychange",
    "743a99f2dbc9a27e19f92ff3551937d90dba2e89": "Ybodychange",
    "e54cc2931262bf49682a8323da9811976218c03b": "Ybodychange",
    "972782d9568e0849484c027f27c1638ba50ec56e": "Ybodychange",
    "a0fb2eff9b71e2e2c0e53262773b34bed82585d4": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
    "3a9571308e99cc374681bbc451a517d41a150aa0": "Ybodychange",
    "8a91109d16394310f2568717f103e6fff7cbddb0": "Ybodychange",
    "5ba2b98d0fe29603e136fc43a14f853e820cf7e2": "Ybodychange",
    "e27c2ae8bafc94f18eb38f5d839dcef5652d424e": "Ybodychange",
    "f62237bc2f02afe11ce185e13aa51a60b5960037": "Ybodychange"
  },
  "changeHistoryDetails": {
    "a98352ced18e51003b443e1a652d19ec00b2f2d2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15086. Block scheduled counter never get decremet if the block got deleted before replication. Contributed by hemanthboyina.\n",
      "commitDate": "13/02/20 3:27 AM",
      "commitName": "a98352ced18e51003b443e1a652d19ec00b2f2d2",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "06/11/19 9:56 AM",
      "commitNameOld": "dd900259c421d6edd0b89a535a1fe08ada91735f",
      "commitAuthorOld": "Chen Liang",
      "daysBetweenCommits": 98.73,
      "commitsBetweenForRepo": 341,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,60 @@\n   private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final short requiredRedundancy \u003d\n         getExpectedLiveRedundancyNum(block, numReplicas);\n     final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     BlockPlacementStatus placementStatus \u003d getBlockPlacementStatus(block);\n     if ((numReplicas.liveReplicas() \u003e\u003d requiredRedundancy) \u0026\u0026\n         (!placementStatus.isPlacementPolicySatisfied())) {\n       BlockPlacementStatus newPlacementStatus \u003d\n           getBlockPlacementStatus(block, targets);\n       if (!newPlacementStatus.isPlacementPolicySatisfied() \u0026\u0026\n           (newPlacementStatus.getAdditionalReplicasRequired() \u003e\u003d\n               placementStatus.getAdditionalReplicasRequired())) {\n         // If the new targets do not meet the placement policy, or at least\n         // reduce the number of replicas needed, then no use continuing.\n         return false;\n       }\n       // mark that the reconstruction work is to replicate internal block to a\n       // new rack.\n       rw.setNotEnoughRack();\n     }\n \n     // Add block to the datanode\u0027s task list\n     rw.addTaskToDatanode(numReplicas);\n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // reconstructions that fail after an appropriate amount of time.\n-    pendingReconstruction.increment(block,\n-        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n+    pendingReconstruction.increment(block, targets);\n     blockLog.debug(\"BLOCK* block {} is moved from neededReconstruction to \"\n         + \"pendingReconstruction\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReconstruction\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredRedundancy) {\n       neededReconstruction.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final short requiredRedundancy \u003d\n        getExpectedLiveRedundancyNum(block, numReplicas);\n    final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    BlockPlacementStatus placementStatus \u003d getBlockPlacementStatus(block);\n    if ((numReplicas.liveReplicas() \u003e\u003d requiredRedundancy) \u0026\u0026\n        (!placementStatus.isPlacementPolicySatisfied())) {\n      BlockPlacementStatus newPlacementStatus \u003d\n          getBlockPlacementStatus(block, targets);\n      if (!newPlacementStatus.isPlacementPolicySatisfied() \u0026\u0026\n          (newPlacementStatus.getAdditionalReplicasRequired() \u003e\u003d\n              placementStatus.getAdditionalReplicasRequired())) {\n        // If the new targets do not meet the placement policy, or at least\n        // reduce the number of replicas needed, then no use continuing.\n        return false;\n      }\n      // mark that the reconstruction work is to replicate internal block to a\n      // new rack.\n      rw.setNotEnoughRack();\n    }\n\n    // Add block to the datanode\u0027s task list\n    rw.addTaskToDatanode(numReplicas);\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // reconstructions that fail after an appropriate amount of time.\n    pendingReconstruction.increment(block, targets);\n    blockLog.debug(\"BLOCK* block {} is moved from neededReconstruction to \"\n        + \"pendingReconstruction\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReconstruction\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredRedundancy) {\n      neededReconstruction.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "c99a12167ff9566012ef32104a3964887d62c899": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14637. Namenode may not replicate blocks to meet the policy after enabling upgradeDomain. Contributed by Stephen O\u0027Donnell.\n\nReviewed-by: Ayush Saxena \u003cayushsaxena@apache.org\u003e\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "03/10/19 10:13 PM",
      "commitName": "c99a12167ff9566012ef32104a3964887d62c899",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "28/09/19 9:14 AM",
      "commitNameOld": "c4c8d5fd0e3c17ccdcf18ece8e005f510328b060",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 5.54,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,61 @@\n   private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final short requiredRedundancy \u003d\n         getExpectedLiveRedundancyNum(block, numReplicas);\n     final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n+    BlockPlacementStatus placementStatus \u003d getBlockPlacementStatus(block);\n     if ((numReplicas.liveReplicas() \u003e\u003d requiredRedundancy) \u0026\u0026\n-        (!isPlacementPolicySatisfied(block)) ) {\n-      if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n-        // No use continuing, unless a new rack in this case\n+        (!placementStatus.isPlacementPolicySatisfied())) {\n+      BlockPlacementStatus newPlacementStatus \u003d\n+          getBlockPlacementStatus(block, targets);\n+      if (!newPlacementStatus.isPlacementPolicySatisfied() \u0026\u0026\n+          (newPlacementStatus.getAdditionalReplicasRequired() \u003e\u003d\n+              placementStatus.getAdditionalReplicasRequired())) {\n+        // If the new targets do not meet the placement policy, or at least\n+        // reduce the number of replicas needed, then no use continuing.\n         return false;\n       }\n       // mark that the reconstruction work is to replicate internal block to a\n       // new rack.\n       rw.setNotEnoughRack();\n     }\n \n     // Add block to the datanode\u0027s task list\n     rw.addTaskToDatanode(numReplicas);\n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // reconstructions that fail after an appropriate amount of time.\n     pendingReconstruction.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReconstruction to \"\n         + \"pendingReconstruction\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReconstruction\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredRedundancy) {\n       neededReconstruction.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final short requiredRedundancy \u003d\n        getExpectedLiveRedundancyNum(block, numReplicas);\n    final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    BlockPlacementStatus placementStatus \u003d getBlockPlacementStatus(block);\n    if ((numReplicas.liveReplicas() \u003e\u003d requiredRedundancy) \u0026\u0026\n        (!placementStatus.isPlacementPolicySatisfied())) {\n      BlockPlacementStatus newPlacementStatus \u003d\n          getBlockPlacementStatus(block, targets);\n      if (!newPlacementStatus.isPlacementPolicySatisfied() \u0026\u0026\n          (newPlacementStatus.getAdditionalReplicasRequired() \u003e\u003d\n              placementStatus.getAdditionalReplicasRequired())) {\n        // If the new targets do not meet the placement policy, or at least\n        // reduce the number of replicas needed, then no use continuing.\n        return false;\n      }\n      // mark that the reconstruction work is to replicate internal block to a\n      // new rack.\n      rw.setNotEnoughRack();\n    }\n\n    // Add block to the datanode\u0027s task list\n    rw.addTaskToDatanode(numReplicas);\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // reconstructions that fail after an appropriate amount of time.\n    pendingReconstruction.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReconstruction to \"\n        + \"pendingReconstruction\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReconstruction\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredRedundancy) {\n      neededReconstruction.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "de4894936a5b581572f35fa5b8979d9f23da0891": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11368. Erasure Coding: Deprecate replication-related config keys. Contributed by Rakesh R.\n",
      "commitDate": "23/11/16 4:42 PM",
      "commitName": "de4894936a5b581572f35fa5b8979d9f23da0891",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "11/11/16 2:31 PM",
      "commitNameOld": "4484b48498b2ab2a40a404c487c7a4e875df10dc",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 12.09,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final short requiredRedundancy \u003d\n         getExpectedLiveRedundancyNum(block, numReplicas);\n     final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n-      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n+      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ((numReplicas.liveReplicas() \u003e\u003d requiredRedundancy) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n         // No use continuing, unless a new rack in this case\n         return false;\n       }\n       // mark that the reconstruction work is to replicate internal block to a\n       // new rack.\n       rw.setNotEnoughRack();\n     }\n \n     // Add block to the datanode\u0027s task list\n     rw.addTaskToDatanode(numReplicas);\n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // reconstructions that fail after an appropriate amount of time.\n     pendingReconstruction.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n-    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n-        + \"pendingReplications\", block);\n+    blockLog.debug(\"BLOCK* block {} is moved from neededReconstruction to \"\n+        + \"pendingReconstruction\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReconstruction\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredRedundancy) {\n       neededReconstruction.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final short requiredRedundancy \u003d\n        getExpectedLiveRedundancyNum(block, numReplicas);\n    final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ((numReplicas.liveReplicas() \u003e\u003d requiredRedundancy) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n        // No use continuing, unless a new rack in this case\n        return false;\n      }\n      // mark that the reconstruction work is to replicate internal block to a\n      // new rack.\n      rw.setNotEnoughRack();\n    }\n\n    // Add block to the datanode\u0027s task list\n    rw.addTaskToDatanode(numReplicas);\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // reconstructions that fail after an appropriate amount of time.\n    pendingReconstruction.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReconstruction to \"\n        + \"pendingReconstruction\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReconstruction\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredRedundancy) {\n      neededReconstruction.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "14/10/16 6:13 PM",
      "commitNameOld": "391ce535a739dc92cb90017d759217265a4fd969",
      "commitAuthorOld": "Vinitha Reddy Gankidi",
      "daysBetweenCommits": 2.98,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n-    final short requiredRedundancy \u003d getExpectedRedundancyNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n+    final short requiredRedundancy \u003d\n+        getExpectedLiveRedundancyNum(block, numReplicas);\n     final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n-    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n-        requiredRedundancy)) {\n+    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ((numReplicas.liveReplicas() \u003e\u003d requiredRedundancy) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n         // No use continuing, unless a new rack in this case\n         return false;\n       }\n       // mark that the reconstruction work is to replicate internal block to a\n       // new rack.\n       rw.setNotEnoughRack();\n     }\n \n     // Add block to the datanode\u0027s task list\n     rw.addTaskToDatanode(numReplicas);\n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // reconstructions that fail after an appropriate amount of time.\n     pendingReconstruction.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReconstruction\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredRedundancy) {\n       neededReconstruction.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final short requiredRedundancy \u003d\n        getExpectedLiveRedundancyNum(block, numReplicas);\n    final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ((numReplicas.liveReplicas() \u003e\u003d requiredRedundancy) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n        // No use continuing, unless a new rack in this case\n        return false;\n      }\n      // mark that the reconstruction work is to replicate internal block to a\n      // new rack.\n      rw.setNotEnoughRack();\n    }\n\n    // Add block to the datanode\u0027s task list\n    rw.addTaskToDatanode(numReplicas);\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // reconstructions that fail after an appropriate amount of time.\n    pendingReconstruction.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReconstruction\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredRedundancy) {\n      neededReconstruction.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10236. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-3]. Contributed by Rakesh R.\n",
      "commitDate": "26/05/16 4:50 PM",
      "commitName": "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "28/04/16 10:44 AM",
      "commitNameOld": "6243eabb48390fffada2418ade5adf9e0766afbe",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 28.25,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n-    final short requiredReplication \u003d getExpectedReplicaNum(block);\n+    final short requiredRedundancy \u003d getExpectedRedundancyNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n-        requiredReplication)) {\n+        requiredRedundancy)) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n-    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n+    if ((numReplicas.liveReplicas() \u003e\u003d requiredRedundancy) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n         // No use continuing, unless a new rack in this case\n         return false;\n       }\n       // mark that the reconstruction work is to replicate internal block to a\n       // new rack.\n       rw.setNotEnoughRack();\n     }\n \n     // Add block to the datanode\u0027s task list\n     rw.addTaskToDatanode(numReplicas);\n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // reconstructions that fail after an appropriate amount of time.\n     pendingReconstruction.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReconstruction\n-    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n+    if(numEffectiveReplicas + targets.length \u003e\u003d requiredRedundancy) {\n       neededReconstruction.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredRedundancy \u003d getExpectedRedundancyNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredRedundancy)) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ((numReplicas.liveReplicas() \u003e\u003d requiredRedundancy) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n        // No use continuing, unless a new rack in this case\n        return false;\n      }\n      // mark that the reconstruction work is to replicate internal block to a\n      // new rack.\n      rw.setNotEnoughRack();\n    }\n\n    // Add block to the datanode\u0027s task list\n    rw.addTaskToDatanode(numReplicas);\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // reconstructions that fail after an appropriate amount of time.\n    pendingReconstruction.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReconstruction\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredRedundancy) {\n      neededReconstruction.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9869. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-2]. Contributed by Rakesh R.\n",
      "commitDate": "25/04/16 10:01 PM",
      "commitName": "5865fe2bf01284993572ea60b3ec3bf8b4492818",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "17/04/16 6:28 PM",
      "commitNameOld": "67523ffcf491f4f2db5335899c00a174d0caaa9b",
      "commitAuthorOld": "Walter Su",
      "daysBetweenCommits": 8.15,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n-    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n+    final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n         // No use continuing, unless a new rack in this case\n         return false;\n       }\n       // mark that the reconstruction work is to replicate internal block to a\n       // new rack.\n       rw.setNotEnoughRack();\n     }\n \n     // Add block to the datanode\u0027s task list\n     rw.addTaskToDatanode(numReplicas);\n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n-    // replications that fail after an appropriate amount of time.\n-    pendingReplications.increment(block,\n+    // reconstructions that fail after an appropriate amount of time.\n+    pendingReconstruction.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReconstruction\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReconstruction.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n        // No use continuing, unless a new rack in this case\n        return false;\n      }\n      // mark that the reconstruction work is to replicate internal block to a\n      // new rack.\n      rw.setNotEnoughRack();\n    }\n\n    // Add block to the datanode\u0027s task list\n    rw.addTaskToDatanode(numReplicas);\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // reconstructions that fail after an appropriate amount of time.\n    pendingReconstruction.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReconstruction\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReconstruction.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n-      neededReplications.remove(block, priority);\n+      neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n-      neededReplications.remove(block, priority);\n+      neededReconstruction.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n         // No use continuing, unless a new rack in this case\n         return false;\n       }\n       // mark that the reconstruction work is to replicate internal block to a\n       // new rack.\n       rw.setNotEnoughRack();\n     }\n \n     // Add block to the datanode\u0027s task list\n     rw.addTaskToDatanode(numReplicas);\n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n-    // remove from neededReplications\n+    // remove from neededReconstruction\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n-      neededReplications.remove(block, priority);\n+      neededReconstruction.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReconstruction.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n        // No use continuing, unless a new rack in this case\n        return false;\n      }\n      // mark that the reconstruction work is to replicate internal block to a\n      // new rack.\n      rw.setNotEnoughRack();\n    }\n\n    // Add block to the datanode\u0027s task list\n    rw.addTaskToDatanode(numReplicas);\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReconstruction\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReconstruction.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "743a99f2dbc9a27e19f92ff3551937d90dba2e89": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8786. Erasure coding: use simple replication for internal blocks on decommissioning datanodes. Contributed by Rakesh R.\n",
      "commitDate": "08/03/16 10:24 AM",
      "commitName": "743a99f2dbc9a27e19f92ff3551937d90dba2e89",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "07/03/16 12:19 PM",
      "commitNameOld": "724d2299cd2516d90c030f6e20d814cceb439228",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.92,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,55 @@\n   private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n         // No use continuing, unless a new rack in this case\n         return false;\n       }\n+      // mark that the reconstruction work is to replicate internal block to a\n+      // new rack.\n+      rw.setNotEnoughRack();\n     }\n \n     // Add block to the datanode\u0027s task list\n-    rw.addTaskToDatanode();\n+    rw.addTaskToDatanode(numReplicas);\n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReplications\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReplications.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n        // No use continuing, unless a new rack in this case\n        return false;\n      }\n      // mark that the reconstruction work is to replicate internal block to a\n      // new rack.\n      rw.setNotEnoughRack();\n    }\n\n    // Add block to the datanode\u0027s task list\n    rw.addTaskToDatanode(numReplicas);\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReplications\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReplications.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "e54cc2931262bf49682a8323da9811976218c03b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9818. Correctly handle EC reconstruction work caused by not enough racks. Contributed by Jing Zhao.\n",
      "commitDate": "19/02/16 7:02 PM",
      "commitName": "e54cc2931262bf49682a8323da9811976218c03b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/02/16 11:07 AM",
      "commitNameOld": "972782d9568e0849484c027f27c1638ba50ec56e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 7.33,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,52 @@\n   private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n-      if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n-          targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n-        //No use continuing, unless a new rack in this case\n+      if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n+        // No use continuing, unless a new rack in this case\n         return false;\n       }\n     }\n \n-    // Add block to the to be reconstructed list\n-    if (block.isStriped()) {\n-      assert rw instanceof ErasureCodingWork;\n-      assert rw.getTargets().length \u003e 0;\n-      assert pendingNum \u003d\u003d 0 : \"Should wait the previous reconstruction\"\n-          + \" to finish\";\n-      final ErasureCodingPolicy ecPolicy \u003d\n-          ((BlockInfoStriped) block).getErasureCodingPolicy();\n-      assert ecPolicy !\u003d null;\n-\n-      rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n-          new ExtendedBlock(getBlockPoolId(), block),\n-          rw.getSrcNodes(), rw.getTargets(),\n-          ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n-    } else {\n-      rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n-    }\n-\n+    // Add block to the datanode\u0027s task list\n+    rw.addTaskToDatanode();\n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReplications\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReplications.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (!isInNewRack(rw.getSrcNodes(), targets[0].getDatanodeDescriptor())) {\n        // No use continuing, unless a new rack in this case\n        return false;\n      }\n    }\n\n    // Add block to the datanode\u0027s task list\n    rw.addTaskToDatanode();\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReplications\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReplications.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "972782d9568e0849484c027f27c1638ba50ec56e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9754. Avoid unnecessary getBlockCollection calls in BlockManager. Contributed by Jing Zhao.\n",
      "commitDate": "12/02/16 11:07 AM",
      "commitName": "972782d9568e0849484c027f27c1638ba50ec56e",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "10/02/16 9:24 PM",
      "commitNameOld": "19adb2bc641999b83e25ff0e107ba8c6edbad399",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.57,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,69 @@\n   private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n-    // block should belong to a file\n-    BlockCollection bc \u003d getBlockCollection(block);\n-    // abandoned block or block reopened for append\n-    if (bc \u003d\u003d null\n-        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n+    // skip abandoned block or block reopened for append\n+    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n           targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n         //No use continuing, unless a new rack in this case\n         return false;\n       }\n     }\n \n     // Add block to the to be reconstructed list\n     if (block.isStriped()) {\n       assert rw instanceof ErasureCodingWork;\n       assert rw.getTargets().length \u003e 0;\n       assert pendingNum \u003d\u003d 0 : \"Should wait the previous reconstruction\"\n           + \" to finish\";\n-      String src \u003d getBlockCollection(block).getName();\n-      ErasureCodingPolicy ecPolicy \u003d null;\n-      try {\n-        ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n-      } catch (IOException e) {\n-        blockLog\n-            .warn(\"Failed to get EC policy for the file {} \", src);\n-      }\n-      if (ecPolicy \u003d\u003d null) {\n-        blockLog.warn(\"No erasure coding policy found for the file {}. \"\n-            + \"So cannot proceed for reconstruction\", src);\n-        // TODO: we may have to revisit later for what we can do better to\n-        // handle this case.\n-        return false;\n-      }\n+      final ErasureCodingPolicy ecPolicy \u003d\n+          ((BlockInfoStriped) block).getErasureCodingPolicy();\n+      assert ecPolicy !\u003d null;\n+\n       rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n-          new ExtendedBlock(namesystem.getBlockPoolId(), block),\n+          new ExtendedBlock(getBlockPoolId(), block),\n           rw.getSrcNodes(), rw.getTargets(),\n           ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n     } else {\n       rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n     }\n \n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReplications\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReplications.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n          targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n        //No use continuing, unless a new rack in this case\n        return false;\n      }\n    }\n\n    // Add block to the to be reconstructed list\n    if (block.isStriped()) {\n      assert rw instanceof ErasureCodingWork;\n      assert rw.getTargets().length \u003e 0;\n      assert pendingNum \u003d\u003d 0 : \"Should wait the previous reconstruction\"\n          + \" to finish\";\n      final ErasureCodingPolicy ecPolicy \u003d\n          ((BlockInfoStriped) block).getErasureCodingPolicy();\n      assert ecPolicy !\u003d null;\n\n      rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n          new ExtendedBlock(getBlockPoolId(), block),\n          rw.getSrcNodes(), rw.getTargets(),\n          ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n    } else {\n      rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n    }\n\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReplications\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReplications.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "a0fb2eff9b71e2e2c0e53262773b34bed82585d4": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
      "commitDate": "09/02/16 2:43 PM",
      "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
          "commitDate": "09/02/16 2:43 PM",
          "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "02/02/16 11:23 AM",
          "commitNameOld": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 7.14,
          "commitsBetweenForRepo": 51,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,82 +1,83 @@\n-  private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n+  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n           targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n         //No use continuing, unless a new rack in this case\n         return false;\n       }\n     }\n \n-    // Add block to the to be recovered list\n+    // Add block to the to be reconstructed list\n     if (block.isStriped()) {\n       assert rw instanceof ErasureCodingWork;\n       assert rw.getTargets().length \u003e 0;\n-      assert pendingNum \u003d\u003d 0: \"Should wait the previous recovery to finish\";\n+      assert pendingNum \u003d\u003d 0 : \"Should wait the previous reconstruction\"\n+          + \" to finish\";\n       String src \u003d getBlockCollection(block).getName();\n       ErasureCodingPolicy ecPolicy \u003d null;\n       try {\n         ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n       } catch (IOException e) {\n         blockLog\n             .warn(\"Failed to get EC policy for the file {} \", src);\n       }\n       if (ecPolicy \u003d\u003d null) {\n         blockLog.warn(\"No erasure coding policy found for the file {}. \"\n-            + \"So cannot proceed for recovery\", src);\n+            + \"So cannot proceed for reconstruction\", src);\n         // TODO: we may have to revisit later for what we can do better to\n         // handle this case.\n         return false;\n       }\n       rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n           new ExtendedBlock(namesystem.getBlockPoolId(), block),\n           rw.getSrcNodes(), rw.getTargets(),\n           ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n     } else {\n       rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n     }\n \n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReplications\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReplications.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n          targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n        //No use continuing, unless a new rack in this case\n        return false;\n      }\n    }\n\n    // Add block to the to be reconstructed list\n    if (block.isStriped()) {\n      assert rw instanceof ErasureCodingWork;\n      assert rw.getTargets().length \u003e 0;\n      assert pendingNum \u003d\u003d 0 : \"Should wait the previous reconstruction\"\n          + \" to finish\";\n      String src \u003d getBlockCollection(block).getName();\n      ErasureCodingPolicy ecPolicy \u003d null;\n      try {\n        ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n      } catch (IOException e) {\n        blockLog\n            .warn(\"Failed to get EC policy for the file {} \", src);\n      }\n      if (ecPolicy \u003d\u003d null) {\n        blockLog.warn(\"No erasure coding policy found for the file {}. \"\n            + \"So cannot proceed for reconstruction\", src);\n        // TODO: we may have to revisit later for what we can do better to\n        // handle this case.\n        return false;\n      }\n      rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n          new ExtendedBlock(namesystem.getBlockPoolId(), block),\n          rw.getSrcNodes(), rw.getTargets(),\n          ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n    } else {\n      rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n    }\n\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReplications\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReplications.remove(block, priority);\n    }\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "validateRecoveryWork",
            "newValue": "validateReconstructionWork"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
          "commitDate": "09/02/16 2:43 PM",
          "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "02/02/16 11:23 AM",
          "commitNameOld": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 7.14,
          "commitsBetweenForRepo": 51,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,82 +1,83 @@\n-  private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n+  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n           targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n         //No use continuing, unless a new rack in this case\n         return false;\n       }\n     }\n \n-    // Add block to the to be recovered list\n+    // Add block to the to be reconstructed list\n     if (block.isStriped()) {\n       assert rw instanceof ErasureCodingWork;\n       assert rw.getTargets().length \u003e 0;\n-      assert pendingNum \u003d\u003d 0: \"Should wait the previous recovery to finish\";\n+      assert pendingNum \u003d\u003d 0 : \"Should wait the previous reconstruction\"\n+          + \" to finish\";\n       String src \u003d getBlockCollection(block).getName();\n       ErasureCodingPolicy ecPolicy \u003d null;\n       try {\n         ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n       } catch (IOException e) {\n         blockLog\n             .warn(\"Failed to get EC policy for the file {} \", src);\n       }\n       if (ecPolicy \u003d\u003d null) {\n         blockLog.warn(\"No erasure coding policy found for the file {}. \"\n-            + \"So cannot proceed for recovery\", src);\n+            + \"So cannot proceed for reconstruction\", src);\n         // TODO: we may have to revisit later for what we can do better to\n         // handle this case.\n         return false;\n       }\n       rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n           new ExtendedBlock(namesystem.getBlockPoolId(), block),\n           rw.getSrcNodes(), rw.getTargets(),\n           ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n     } else {\n       rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n     }\n \n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReplications\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReplications.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n          targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n        //No use continuing, unless a new rack in this case\n        return false;\n      }\n    }\n\n    // Add block to the to be reconstructed list\n    if (block.isStriped()) {\n      assert rw instanceof ErasureCodingWork;\n      assert rw.getTargets().length \u003e 0;\n      assert pendingNum \u003d\u003d 0 : \"Should wait the previous reconstruction\"\n          + \" to finish\";\n      String src \u003d getBlockCollection(block).getName();\n      ErasureCodingPolicy ecPolicy \u003d null;\n      try {\n        ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n      } catch (IOException e) {\n        blockLog\n            .warn(\"Failed to get EC policy for the file {} \", src);\n      }\n      if (ecPolicy \u003d\u003d null) {\n        blockLog.warn(\"No erasure coding policy found for the file {}. \"\n            + \"So cannot proceed for reconstruction\", src);\n        // TODO: we may have to revisit later for what we can do better to\n        // handle this case.\n        return false;\n      }\n      rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n          new ExtendedBlock(namesystem.getBlockPoolId(), block),\n          rw.getSrcNodes(), rw.getTargets(),\n          ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n    } else {\n      rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n    }\n\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReplications\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReplications.remove(block, priority);\n    }\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[rw-BlockRecoveryWork]",
            "newValue": "[rw-BlockReconstructionWork]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
          "commitDate": "09/02/16 2:43 PM",
          "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "02/02/16 11:23 AM",
          "commitNameOld": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 7.14,
          "commitsBetweenForRepo": 51,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,82 +1,83 @@\n-  private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n+  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n           targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n         //No use continuing, unless a new rack in this case\n         return false;\n       }\n     }\n \n-    // Add block to the to be recovered list\n+    // Add block to the to be reconstructed list\n     if (block.isStriped()) {\n       assert rw instanceof ErasureCodingWork;\n       assert rw.getTargets().length \u003e 0;\n-      assert pendingNum \u003d\u003d 0: \"Should wait the previous recovery to finish\";\n+      assert pendingNum \u003d\u003d 0 : \"Should wait the previous reconstruction\"\n+          + \" to finish\";\n       String src \u003d getBlockCollection(block).getName();\n       ErasureCodingPolicy ecPolicy \u003d null;\n       try {\n         ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n       } catch (IOException e) {\n         blockLog\n             .warn(\"Failed to get EC policy for the file {} \", src);\n       }\n       if (ecPolicy \u003d\u003d null) {\n         blockLog.warn(\"No erasure coding policy found for the file {}. \"\n-            + \"So cannot proceed for recovery\", src);\n+            + \"So cannot proceed for reconstruction\", src);\n         // TODO: we may have to revisit later for what we can do better to\n         // handle this case.\n         return false;\n       }\n       rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n           new ExtendedBlock(namesystem.getBlockPoolId(), block),\n           rw.getSrcNodes(), rw.getTargets(),\n           ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n     } else {\n       rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n     }\n \n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReplications\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReplications.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean validateReconstructionWork(BlockReconstructionWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n          targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n        //No use continuing, unless a new rack in this case\n        return false;\n      }\n    }\n\n    // Add block to the to be reconstructed list\n    if (block.isStriped()) {\n      assert rw instanceof ErasureCodingWork;\n      assert rw.getTargets().length \u003e 0;\n      assert pendingNum \u003d\u003d 0 : \"Should wait the previous reconstruction\"\n          + \" to finish\";\n      String src \u003d getBlockCollection(block).getName();\n      ErasureCodingPolicy ecPolicy \u003d null;\n      try {\n        ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n      } catch (IOException e) {\n        blockLog\n            .warn(\"Failed to get EC policy for the file {} \", src);\n      }\n      if (ecPolicy \u003d\u003d null) {\n        blockLog.warn(\"No erasure coding policy found for the file {}. \"\n            + \"So cannot proceed for reconstruction\", src);\n        // TODO: we may have to revisit later for what we can do better to\n        // handle this case.\n        return false;\n      }\n      rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n          new ExtendedBlock(namesystem.getBlockPoolId(), block),\n          rw.getSrcNodes(), rw.getTargets(),\n          ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n    } else {\n      rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n    }\n\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReplications\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReplications.remove(block, priority);\n    }\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "3a9571308e99cc374681bbc451a517d41a150aa0": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9677. Rename generationStampV1/generationStampV2 to legacyGenerationStamp/generationStamp. Contributed by Mingliang Liu.\"\n\nThis reverts commit 8a91109d16394310f2568717f103e6fff7cbddb0.\n",
      "commitDate": "27/01/16 4:31 PM",
      "commitName": "3a9571308e99cc374681bbc451a517d41a150aa0",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/01/16 3:48 PM",
      "commitNameOld": "8a91109d16394310f2568717f103e6fff7cbddb0",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,82 @@\n   private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n           targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n         //No use continuing, unless a new rack in this case\n         return false;\n       }\n     }\n \n     // Add block to the to be recovered list\n     if (block.isStriped()) {\n       assert rw instanceof ErasureCodingWork;\n       assert rw.getTargets().length \u003e 0;\n       assert pendingNum \u003d\u003d 0: \"Should wait the previous recovery to finish\";\n       String src \u003d getBlockCollection(block).getName();\n       ErasureCodingPolicy ecPolicy \u003d null;\n       try {\n         ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n       } catch (IOException e) {\n         blockLog\n             .warn(\"Failed to get EC policy for the file {} \", src);\n       }\n       if (ecPolicy \u003d\u003d null) {\n         blockLog.warn(\"No erasure coding policy found for the file {}. \"\n             + \"So cannot proceed for recovery\", src);\n         // TODO: we may have to revisit later for what we can do better to\n         // handle this case.\n         return false;\n       }\n       rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n-          new ExtendedBlock(getBlockPoolId(), block),\n+          new ExtendedBlock(namesystem.getBlockPoolId(), block),\n           rw.getSrcNodes(), rw.getTargets(),\n           ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n     } else {\n       rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n     }\n \n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReplications\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReplications.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n          targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n        //No use continuing, unless a new rack in this case\n        return false;\n      }\n    }\n\n    // Add block to the to be recovered list\n    if (block.isStriped()) {\n      assert rw instanceof ErasureCodingWork;\n      assert rw.getTargets().length \u003e 0;\n      assert pendingNum \u003d\u003d 0: \"Should wait the previous recovery to finish\";\n      String src \u003d getBlockCollection(block).getName();\n      ErasureCodingPolicy ecPolicy \u003d null;\n      try {\n        ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n      } catch (IOException e) {\n        blockLog\n            .warn(\"Failed to get EC policy for the file {} \", src);\n      }\n      if (ecPolicy \u003d\u003d null) {\n        blockLog.warn(\"No erasure coding policy found for the file {}. \"\n            + \"So cannot proceed for recovery\", src);\n        // TODO: we may have to revisit later for what we can do better to\n        // handle this case.\n        return false;\n      }\n      rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n          new ExtendedBlock(namesystem.getBlockPoolId(), block),\n          rw.getSrcNodes(), rw.getTargets(),\n          ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n    } else {\n      rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n    }\n\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReplications\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReplications.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8a91109d16394310f2568717f103e6fff7cbddb0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9677. Rename generationStampV1/generationStampV2 to legacyGenerationStamp/generationStamp. Contributed by Mingliang Liu.\n",
      "commitDate": "27/01/16 3:48 PM",
      "commitName": "8a91109d16394310f2568717f103e6fff7cbddb0",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "25/01/16 6:32 PM",
      "commitNameOld": "bd909ed9f2d853f614f04a50e2230a7932732776",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 1.89,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,82 @@\n   private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n           targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n         //No use continuing, unless a new rack in this case\n         return false;\n       }\n     }\n \n     // Add block to the to be recovered list\n     if (block.isStriped()) {\n       assert rw instanceof ErasureCodingWork;\n       assert rw.getTargets().length \u003e 0;\n       assert pendingNum \u003d\u003d 0: \"Should wait the previous recovery to finish\";\n       String src \u003d getBlockCollection(block).getName();\n       ErasureCodingPolicy ecPolicy \u003d null;\n       try {\n         ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n       } catch (IOException e) {\n         blockLog\n             .warn(\"Failed to get EC policy for the file {} \", src);\n       }\n       if (ecPolicy \u003d\u003d null) {\n         blockLog.warn(\"No erasure coding policy found for the file {}. \"\n             + \"So cannot proceed for recovery\", src);\n         // TODO: we may have to revisit later for what we can do better to\n         // handle this case.\n         return false;\n       }\n       rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n-          new ExtendedBlock(namesystem.getBlockPoolId(), block),\n+          new ExtendedBlock(getBlockPoolId(), block),\n           rw.getSrcNodes(), rw.getTargets(),\n           ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n     } else {\n       rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n     }\n \n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReplications\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReplications.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n          targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n        //No use continuing, unless a new rack in this case\n        return false;\n      }\n    }\n\n    // Add block to the to be recovered list\n    if (block.isStriped()) {\n      assert rw instanceof ErasureCodingWork;\n      assert rw.getTargets().length \u003e 0;\n      assert pendingNum \u003d\u003d 0: \"Should wait the previous recovery to finish\";\n      String src \u003d getBlockCollection(block).getName();\n      ErasureCodingPolicy ecPolicy \u003d null;\n      try {\n        ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n      } catch (IOException e) {\n        blockLog\n            .warn(\"Failed to get EC policy for the file {} \", src);\n      }\n      if (ecPolicy \u003d\u003d null) {\n        blockLog.warn(\"No erasure coding policy found for the file {}. \"\n            + \"So cannot proceed for recovery\", src);\n        // TODO: we may have to revisit later for what we can do better to\n        // handle this case.\n        return false;\n      }\n      rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n          new ExtendedBlock(getBlockPoolId(), block),\n          rw.getSrcNodes(), rw.getTargets(),\n          ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n    } else {\n      rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n    }\n\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReplications\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReplications.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "5ba2b98d0fe29603e136fc43a14f853e820cf7e2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9275. Wait previous ErasureCodingWork to finish before schedule another one. (Walter Su via yliu)\n",
      "commitDate": "02/11/15 5:14 PM",
      "commitName": "5ba2b98d0fe29603e136fc43a14f853e820cf7e2",
      "commitAuthor": "yliu",
      "commitDateOld": "27/10/15 11:37 AM",
      "commitNameOld": "fe93577faf49ceb2ee47a7762a61625313ea773b",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 6.28,
      "commitsBetweenForRepo": 66,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,81 +1,82 @@\n   private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!isPlacementPolicySatisfied(block)) ) {\n       if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n           targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n         //No use continuing, unless a new rack in this case\n         return false;\n       }\n     }\n \n     // Add block to the to be recovered list\n     if (block.isStriped()) {\n       assert rw instanceof ErasureCodingWork;\n       assert rw.getTargets().length \u003e 0;\n+      assert pendingNum \u003d\u003d 0: \"Should wait the previous recovery to finish\";\n       String src \u003d getBlockCollection(block).getName();\n       ErasureCodingPolicy ecPolicy \u003d null;\n       try {\n         ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n       } catch (IOException e) {\n         blockLog\n             .warn(\"Failed to get EC policy for the file {} \", src);\n       }\n       if (ecPolicy \u003d\u003d null) {\n         blockLog.warn(\"No erasure coding policy found for the file {}. \"\n             + \"So cannot proceed for recovery\", src);\n         // TODO: we may have to revisit later for what we can do better to\n         // handle this case.\n         return false;\n       }\n       rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n           new ExtendedBlock(namesystem.getBlockPoolId(), block),\n           rw.getSrcNodes(), rw.getTargets(),\n           ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n     } else {\n       rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n     }\n \n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReplications\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReplications.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n          targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n        //No use continuing, unless a new rack in this case\n        return false;\n      }\n    }\n\n    // Add block to the to be recovered list\n    if (block.isStriped()) {\n      assert rw instanceof ErasureCodingWork;\n      assert rw.getTargets().length \u003e 0;\n      assert pendingNum \u003d\u003d 0: \"Should wait the previous recovery to finish\";\n      String src \u003d getBlockCollection(block).getName();\n      ErasureCodingPolicy ecPolicy \u003d null;\n      try {\n        ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n      } catch (IOException e) {\n        blockLog\n            .warn(\"Failed to get EC policy for the file {} \", src);\n      }\n      if (ecPolicy \u003d\u003d null) {\n        blockLog.warn(\"No erasure coding policy found for the file {}. \"\n            + \"So cannot proceed for recovery\", src);\n        // TODO: we may have to revisit later for what we can do better to\n        // handle this case.\n        return false;\n      }\n      rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n          new ExtendedBlock(namesystem.getBlockPoolId(), block),\n          rw.getSrcNodes(), rw.getTargets(),\n          ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n    } else {\n      rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n    }\n\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReplications\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReplications.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "e27c2ae8bafc94f18eb38f5d839dcef5652d424e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8647. Abstract BlockManager\u0027s rack policy into BlockPlacementPolicy. (Brahma Reddy Battula via mingma)\n",
      "commitDate": "21/10/15 8:06 AM",
      "commitName": "e27c2ae8bafc94f18eb38f5d839dcef5652d424e",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "15/10/15 3:07 AM",
      "commitNameOld": "5411dc559d5f73e4153e76fdff94a26869c17a37",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 6.21,
      "commitsBetweenForRepo": 41,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,81 +1,81 @@\n   private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n-        (!blockHasEnoughRacks(block, requiredReplication)) ) {\n+        (!isPlacementPolicySatisfied(block)) ) {\n       if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n           targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n         //No use continuing, unless a new rack in this case\n         return false;\n       }\n     }\n \n     // Add block to the to be recovered list\n     if (block.isStriped()) {\n       assert rw instanceof ErasureCodingWork;\n       assert rw.getTargets().length \u003e 0;\n       String src \u003d getBlockCollection(block).getName();\n       ErasureCodingPolicy ecPolicy \u003d null;\n       try {\n         ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n       } catch (IOException e) {\n         blockLog\n             .warn(\"Failed to get EC policy for the file {} \", src);\n       }\n       if (ecPolicy \u003d\u003d null) {\n         blockLog.warn(\"No erasure coding policy found for the file {}. \"\n             + \"So cannot proceed for recovery\", src);\n         // TODO: we may have to revisit later for what we can do better to\n         // handle this case.\n         return false;\n       }\n       rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n           new ExtendedBlock(namesystem.getBlockPoolId(), block),\n           rw.getSrcNodes(), rw.getTargets(),\n           ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n     } else {\n       rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n     }\n \n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReplications\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReplications.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!isPlacementPolicySatisfied(block)) ) {\n      if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n          targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n        //No use continuing, unless a new rack in this case\n        return false;\n      }\n    }\n\n    // Add block to the to be recovered list\n    if (block.isStriped()) {\n      assert rw instanceof ErasureCodingWork;\n      assert rw.getTargets().length \u003e 0;\n      String src \u003d getBlockCollection(block).getName();\n      ErasureCodingPolicy ecPolicy \u003d null;\n      try {\n        ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n      } catch (IOException e) {\n        blockLog\n            .warn(\"Failed to get EC policy for the file {} \", src);\n      }\n      if (ecPolicy \u003d\u003d null) {\n        blockLog.warn(\"No erasure coding policy found for the file {}. \"\n            + \"So cannot proceed for recovery\", src);\n        // TODO: we may have to revisit later for what we can do better to\n        // handle this case.\n        return false;\n      }\n      rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n          new ExtendedBlock(namesystem.getBlockPoolId(), block),\n          rw.getSrcNodes(), rw.getTargets(),\n          ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n    } else {\n      rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n    }\n\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReplications\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReplications.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "f62237bc2f02afe11ce185e13aa51a60b5960037": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8833. Erasure coding: store EC schema and cell size in INodeFile and eliminate notion of EC zones.\n",
      "commitDate": "09/09/15 11:07 PM",
      "commitName": "f62237bc2f02afe11ce185e13aa51a60b5960037",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "01/09/15 2:48 PM",
      "commitNameOld": "53358fe680a11c1b66a7f60733d11c1f4efe0232",
      "commitAuthorOld": "",
      "daysBetweenCommits": 8.35,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,81 @@\n   private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n     BlockInfo block \u003d rw.getBlock();\n     int priority \u003d rw.getPriority();\n     // Recheck since global lock was released\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       return false;\n     }\n \n     // do not schedule more if enough replicas is already pending\n     final short requiredReplication \u003d getExpectedReplicaNum(block);\n     NumberReplicas numReplicas \u003d countNodes(block);\n     final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       rw.resetTargets();\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return false;\n     }\n \n     DatanodeStorageInfo[] targets \u003d rw.getTargets();\n     if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n         (!blockHasEnoughRacks(block, requiredReplication)) ) {\n       if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n           targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n         //No use continuing, unless a new rack in this case\n         return false;\n       }\n     }\n \n     // Add block to the to be recovered list\n     if (block.isStriped()) {\n       assert rw instanceof ErasureCodingWork;\n       assert rw.getTargets().length \u003e 0;\n       String src \u003d getBlockCollection(block).getName();\n-      ErasureCodingZone ecZone \u003d null;\n+      ErasureCodingPolicy ecPolicy \u003d null;\n       try {\n-        ecZone \u003d namesystem.getErasureCodingZoneForPath(src);\n+        ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n       } catch (IOException e) {\n         blockLog\n-            .warn(\"Failed to get the EC zone for the file {} \", src);\n+            .warn(\"Failed to get EC policy for the file {} \", src);\n       }\n-      if (ecZone \u003d\u003d null) {\n+      if (ecPolicy \u003d\u003d null) {\n         blockLog.warn(\"No erasure coding policy found for the file {}. \"\n             + \"So cannot proceed for recovery\", src);\n         // TODO: we may have to revisit later for what we can do better to\n         // handle this case.\n         return false;\n       }\n       rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n           new ExtendedBlock(namesystem.getBlockPoolId(), block),\n           rw.getSrcNodes(), rw.getTargets(),\n-          ((ErasureCodingWork) rw).getLiveBlockIndicies(),\n-          ecZone.getErasureCodingPolicy());\n+          ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n     } else {\n       rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n     }\n \n     DatanodeStorageInfo.incrementBlocksScheduled(targets);\n \n     // Move the block-replication into a \"pending\" state.\n     // The reason we use \u0027pending\u0027 is so we can retry\n     // replications that fail after an appropriate amount of time.\n     pendingReplications.increment(block,\n         DatanodeStorageInfo.toDatanodeDescriptors(targets));\n     blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n         + \"pendingReplications\", block);\n \n     int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n     // remove from neededReplications\n     if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n       neededReplications.remove(block, priority);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean validateRecoveryWork(BlockRecoveryWork rw) {\n    BlockInfo block \u003d rw.getBlock();\n    int priority \u003d rw.getPriority();\n    // Recheck since global lock was released\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      return false;\n    }\n\n    // do not schedule more if enough replicas is already pending\n    final short requiredReplication \u003d getExpectedReplicaNum(block);\n    NumberReplicas numReplicas \u003d countNodes(block);\n    final int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      rw.resetTargets();\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return false;\n    }\n\n    DatanodeStorageInfo[] targets \u003d rw.getTargets();\n    if ( (numReplicas.liveReplicas() \u003e\u003d requiredReplication) \u0026\u0026\n        (!blockHasEnoughRacks(block, requiredReplication)) ) {\n      if (rw.getSrcNodes()[0].getNetworkLocation().equals(\n          targets[0].getDatanodeDescriptor().getNetworkLocation())) {\n        //No use continuing, unless a new rack in this case\n        return false;\n      }\n    }\n\n    // Add block to the to be recovered list\n    if (block.isStriped()) {\n      assert rw instanceof ErasureCodingWork;\n      assert rw.getTargets().length \u003e 0;\n      String src \u003d getBlockCollection(block).getName();\n      ErasureCodingPolicy ecPolicy \u003d null;\n      try {\n        ecPolicy \u003d namesystem.getErasureCodingPolicyForPath(src);\n      } catch (IOException e) {\n        blockLog\n            .warn(\"Failed to get EC policy for the file {} \", src);\n      }\n      if (ecPolicy \u003d\u003d null) {\n        blockLog.warn(\"No erasure coding policy found for the file {}. \"\n            + \"So cannot proceed for recovery\", src);\n        // TODO: we may have to revisit later for what we can do better to\n        // handle this case.\n        return false;\n      }\n      rw.getTargets()[0].getDatanodeDescriptor().addBlockToBeErasureCoded(\n          new ExtendedBlock(namesystem.getBlockPoolId(), block),\n          rw.getSrcNodes(), rw.getTargets(),\n          ((ErasureCodingWork) rw).getLiveBlockIndicies(), ecPolicy);\n    } else {\n      rw.getSrcNodes()[0].addBlockToBeReplicated(block, targets);\n    }\n\n    DatanodeStorageInfo.incrementBlocksScheduled(targets);\n\n    // Move the block-replication into a \"pending\" state.\n    // The reason we use \u0027pending\u0027 is so we can retry\n    // replications that fail after an appropriate amount of time.\n    pendingReplications.increment(block,\n        DatanodeStorageInfo.toDatanodeDescriptors(targets));\n    blockLog.debug(\"BLOCK* block {} is moved from neededReplications to \"\n        + \"pendingReplications\", block);\n\n    int numEffectiveReplicas \u003d numReplicas.liveReplicas() + pendingNum;\n    // remove from neededReplications\n    if(numEffectiveReplicas + targets.length \u003e\u003d requiredReplication) {\n      neededReplications.remove(block, priority);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    }
  }
}