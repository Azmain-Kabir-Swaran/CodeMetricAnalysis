{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStorage.java",
  "functionName": "removeDuplicateEntries",
  "functionId": "removeDuplicateEntries___all-ArrayList__LinkArgs____duplicates-ArrayList__LinkArgs__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
  "functionStartLine": 1204,
  "functionEndLine": 1297,
  "numCommitsSeen": 75,
  "timeTaken": 1983,
  "changeHistory": [
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a",
    "8fa265a290792ff42635ff9b42416c634f88bdf3"
  ],
  "changeHistoryShort": {
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a": "Ybodychange",
    "8fa265a290792ff42635ff9b42416c634f88bdf3": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12997. Move logging to slf4j in BlockPoolSliceStorage and Storage. Contributed by Ajay Kumar.\n",
      "commitDate": "01/02/18 10:45 AM",
      "commitName": "b3ae11d59790bb08b81848e9f944db7d3afbbd8a",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "8239e3afb31d3c4485817d4b8b8b195b554acbe7",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 47.7,
      "commitsBetweenForRepo": 240,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,94 @@\n   private static void removeDuplicateEntries(ArrayList\u003cLinkArgs\u003e all,\n                                              ArrayList\u003cLinkArgs\u003e duplicates) {\n     // Maps blockId -\u003e metadata file with highest genstamp\n     TreeMap\u003cLong, List\u003cLinkArgs\u003e\u003e highestGenstamps \u003d\n         new TreeMap\u003cLong, List\u003cLinkArgs\u003e\u003e();\n     for (LinkArgs duplicate : duplicates) {\n       if (!Block.isMetaFilename(duplicate.src.getName())) {\n         continue;\n       }\n       long blockId \u003d Block.getBlockId(duplicate.src.getName());\n       List\u003cLinkArgs\u003e prevHighest \u003d highestGenstamps.get(blockId);\n       if (prevHighest \u003d\u003d null) {\n         List\u003cLinkArgs\u003e highest \u003d new LinkedList\u003cLinkArgs\u003e();\n         highest.add(duplicate);\n         highestGenstamps.put(blockId, highest);\n         continue;\n       }\n       long prevGenstamp \u003d\n           Block.getGenerationStamp(prevHighest.get(0).src.getName());\n       long genstamp \u003d Block.getGenerationStamp(duplicate.src.getName());\n       if (genstamp \u003c prevGenstamp) {\n         continue;\n       }\n       if (genstamp \u003e prevGenstamp) {\n         prevHighest.clear();\n       }\n       prevHighest.add(duplicate);\n     }\n \n     // Remove data / metadata entries that don\u0027t have the highest genstamp\n     // from the duplicates list.\n     for (Iterator\u003cLinkArgs\u003e iter \u003d duplicates.iterator(); iter.hasNext(); ) {\n       LinkArgs duplicate \u003d iter.next();\n       long blockId \u003d Block.getBlockId(duplicate.src.getName());\n       List\u003cLinkArgs\u003e highest \u003d highestGenstamps.get(blockId);\n       if (highest !\u003d null) {\n         boolean found \u003d false;\n         for (LinkArgs high : highest) {\n           if (high.src.getParent().equals(duplicate.src.getParent())) {\n             found \u003d true;\n             break;\n           }\n         }\n         if (!found) {\n-          LOG.warn(\"Unexpectedly low genstamp on \" +\n-                   duplicate.src.getAbsolutePath() + \".\");\n+          LOG.warn(\"Unexpectedly low genstamp on {}.\",\n+              duplicate.src.getAbsolutePath());\n           iter.remove();\n         }\n       }\n     }\n \n     // Find the longest block files\n     // We let the \"last guy win\" here, since we\u0027re only interested in\n     // preserving one block file / metadata file pair.\n     TreeMap\u003cLong, LinkArgs\u003e longestBlockFiles \u003d new TreeMap\u003cLong, LinkArgs\u003e();\n     for (LinkArgs duplicate : duplicates) {\n       if (Block.isMetaFilename(duplicate.src.getName())) {\n         continue;\n       }\n       long blockId \u003d Block.getBlockId(duplicate.src.getName());\n       LinkArgs prevLongest \u003d longestBlockFiles.get(blockId);\n       if (prevLongest \u003d\u003d null) {\n         longestBlockFiles.put(blockId, duplicate);\n         continue;\n       }\n       long blockLength \u003d duplicate.src.length();\n       long prevBlockLength \u003d prevLongest.src.length();\n       if (blockLength \u003c prevBlockLength) {\n-        LOG.warn(\"Unexpectedly short length on \" +\n-            duplicate.src.getAbsolutePath() + \".\");\n+        LOG.warn(\"Unexpectedly short length on {}.\",\n+            duplicate.src.getAbsolutePath());\n         continue;\n       }\n       if (blockLength \u003e prevBlockLength) {\n-        LOG.warn(\"Unexpectedly short length on \" +\n-            prevLongest.src.getAbsolutePath() + \".\");\n+        LOG.warn(\"Unexpectedly short length on {}.\",\n+            prevLongest.src.getAbsolutePath());\n       }\n       longestBlockFiles.put(blockId, duplicate);\n     }\n \n     // Remove data / metadata entries that aren\u0027t the longest, or weren\u0027t\n     // arbitrarily selected by us.\n     for (Iterator\u003cLinkArgs\u003e iter \u003d all.iterator(); iter.hasNext(); ) {\n       LinkArgs args \u003d iter.next();\n       long blockId \u003d Block.getBlockId(args.src.getName());\n       LinkArgs bestDuplicate \u003d longestBlockFiles.get(blockId);\n       if (bestDuplicate \u003d\u003d null) {\n         continue; // file has no duplicates\n       }\n       if (!bestDuplicate.src.getParent().equals(args.src.getParent())) {\n-        LOG.warn(\"Discarding \" + args.src.getAbsolutePath() + \".\");\n+        LOG.warn(\"Discarding {}.\", args.src.getAbsolutePath());\n         iter.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void removeDuplicateEntries(ArrayList\u003cLinkArgs\u003e all,\n                                             ArrayList\u003cLinkArgs\u003e duplicates) {\n    // Maps blockId -\u003e metadata file with highest genstamp\n    TreeMap\u003cLong, List\u003cLinkArgs\u003e\u003e highestGenstamps \u003d\n        new TreeMap\u003cLong, List\u003cLinkArgs\u003e\u003e();\n    for (LinkArgs duplicate : duplicates) {\n      if (!Block.isMetaFilename(duplicate.src.getName())) {\n        continue;\n      }\n      long blockId \u003d Block.getBlockId(duplicate.src.getName());\n      List\u003cLinkArgs\u003e prevHighest \u003d highestGenstamps.get(blockId);\n      if (prevHighest \u003d\u003d null) {\n        List\u003cLinkArgs\u003e highest \u003d new LinkedList\u003cLinkArgs\u003e();\n        highest.add(duplicate);\n        highestGenstamps.put(blockId, highest);\n        continue;\n      }\n      long prevGenstamp \u003d\n          Block.getGenerationStamp(prevHighest.get(0).src.getName());\n      long genstamp \u003d Block.getGenerationStamp(duplicate.src.getName());\n      if (genstamp \u003c prevGenstamp) {\n        continue;\n      }\n      if (genstamp \u003e prevGenstamp) {\n        prevHighest.clear();\n      }\n      prevHighest.add(duplicate);\n    }\n\n    // Remove data / metadata entries that don\u0027t have the highest genstamp\n    // from the duplicates list.\n    for (Iterator\u003cLinkArgs\u003e iter \u003d duplicates.iterator(); iter.hasNext(); ) {\n      LinkArgs duplicate \u003d iter.next();\n      long blockId \u003d Block.getBlockId(duplicate.src.getName());\n      List\u003cLinkArgs\u003e highest \u003d highestGenstamps.get(blockId);\n      if (highest !\u003d null) {\n        boolean found \u003d false;\n        for (LinkArgs high : highest) {\n          if (high.src.getParent().equals(duplicate.src.getParent())) {\n            found \u003d true;\n            break;\n          }\n        }\n        if (!found) {\n          LOG.warn(\"Unexpectedly low genstamp on {}.\",\n              duplicate.src.getAbsolutePath());\n          iter.remove();\n        }\n      }\n    }\n\n    // Find the longest block files\n    // We let the \"last guy win\" here, since we\u0027re only interested in\n    // preserving one block file / metadata file pair.\n    TreeMap\u003cLong, LinkArgs\u003e longestBlockFiles \u003d new TreeMap\u003cLong, LinkArgs\u003e();\n    for (LinkArgs duplicate : duplicates) {\n      if (Block.isMetaFilename(duplicate.src.getName())) {\n        continue;\n      }\n      long blockId \u003d Block.getBlockId(duplicate.src.getName());\n      LinkArgs prevLongest \u003d longestBlockFiles.get(blockId);\n      if (prevLongest \u003d\u003d null) {\n        longestBlockFiles.put(blockId, duplicate);\n        continue;\n      }\n      long blockLength \u003d duplicate.src.length();\n      long prevBlockLength \u003d prevLongest.src.length();\n      if (blockLength \u003c prevBlockLength) {\n        LOG.warn(\"Unexpectedly short length on {}.\",\n            duplicate.src.getAbsolutePath());\n        continue;\n      }\n      if (blockLength \u003e prevBlockLength) {\n        LOG.warn(\"Unexpectedly short length on {}.\",\n            prevLongest.src.getAbsolutePath());\n      }\n      longestBlockFiles.put(blockId, duplicate);\n    }\n\n    // Remove data / metadata entries that aren\u0027t the longest, or weren\u0027t\n    // arbitrarily selected by us.\n    for (Iterator\u003cLinkArgs\u003e iter \u003d all.iterator(); iter.hasNext(); ) {\n      LinkArgs args \u003d iter.next();\n      long blockId \u003d Block.getBlockId(args.src.getName());\n      LinkArgs bestDuplicate \u003d longestBlockFiles.get(blockId);\n      if (bestDuplicate \u003d\u003d null) {\n        continue; // file has no duplicates\n      }\n      if (!bestDuplicate.src.getParent().equals(args.src.getParent())) {\n        LOG.warn(\"Discarding {}.\", args.src.getAbsolutePath());\n        iter.remove();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {}
    },
    "8fa265a290792ff42635ff9b42416c634f88bdf3": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7443. Datanode upgrade to BLOCKID_BASED_LAYOUT fails if duplicate block files are present in the same volume (cmccabe)\n",
      "commitDate": "19/12/14 1:18 PM",
      "commitName": "8fa265a290792ff42635ff9b42416c634f88bdf3",
      "commitAuthor": "Colin Patrick Mccabe",
      "diff": "@@ -0,0 +1,94 @@\n+  private static void removeDuplicateEntries(ArrayList\u003cLinkArgs\u003e all,\n+                                             ArrayList\u003cLinkArgs\u003e duplicates) {\n+    // Maps blockId -\u003e metadata file with highest genstamp\n+    TreeMap\u003cLong, List\u003cLinkArgs\u003e\u003e highestGenstamps \u003d\n+        new TreeMap\u003cLong, List\u003cLinkArgs\u003e\u003e();\n+    for (LinkArgs duplicate : duplicates) {\n+      if (!Block.isMetaFilename(duplicate.src.getName())) {\n+        continue;\n+      }\n+      long blockId \u003d Block.getBlockId(duplicate.src.getName());\n+      List\u003cLinkArgs\u003e prevHighest \u003d highestGenstamps.get(blockId);\n+      if (prevHighest \u003d\u003d null) {\n+        List\u003cLinkArgs\u003e highest \u003d new LinkedList\u003cLinkArgs\u003e();\n+        highest.add(duplicate);\n+        highestGenstamps.put(blockId, highest);\n+        continue;\n+      }\n+      long prevGenstamp \u003d\n+          Block.getGenerationStamp(prevHighest.get(0).src.getName());\n+      long genstamp \u003d Block.getGenerationStamp(duplicate.src.getName());\n+      if (genstamp \u003c prevGenstamp) {\n+        continue;\n+      }\n+      if (genstamp \u003e prevGenstamp) {\n+        prevHighest.clear();\n+      }\n+      prevHighest.add(duplicate);\n+    }\n+\n+    // Remove data / metadata entries that don\u0027t have the highest genstamp\n+    // from the duplicates list.\n+    for (Iterator\u003cLinkArgs\u003e iter \u003d duplicates.iterator(); iter.hasNext(); ) {\n+      LinkArgs duplicate \u003d iter.next();\n+      long blockId \u003d Block.getBlockId(duplicate.src.getName());\n+      List\u003cLinkArgs\u003e highest \u003d highestGenstamps.get(blockId);\n+      if (highest !\u003d null) {\n+        boolean found \u003d false;\n+        for (LinkArgs high : highest) {\n+          if (high.src.getParent().equals(duplicate.src.getParent())) {\n+            found \u003d true;\n+            break;\n+          }\n+        }\n+        if (!found) {\n+          LOG.warn(\"Unexpectedly low genstamp on \" +\n+                   duplicate.src.getAbsolutePath() + \".\");\n+          iter.remove();\n+        }\n+      }\n+    }\n+\n+    // Find the longest block files\n+    // We let the \"last guy win\" here, since we\u0027re only interested in\n+    // preserving one block file / metadata file pair.\n+    TreeMap\u003cLong, LinkArgs\u003e longestBlockFiles \u003d new TreeMap\u003cLong, LinkArgs\u003e();\n+    for (LinkArgs duplicate : duplicates) {\n+      if (Block.isMetaFilename(duplicate.src.getName())) {\n+        continue;\n+      }\n+      long blockId \u003d Block.getBlockId(duplicate.src.getName());\n+      LinkArgs prevLongest \u003d longestBlockFiles.get(blockId);\n+      if (prevLongest \u003d\u003d null) {\n+        longestBlockFiles.put(blockId, duplicate);\n+        continue;\n+      }\n+      long blockLength \u003d duplicate.src.length();\n+      long prevBlockLength \u003d prevLongest.src.length();\n+      if (blockLength \u003c prevBlockLength) {\n+        LOG.warn(\"Unexpectedly short length on \" +\n+            duplicate.src.getAbsolutePath() + \".\");\n+        continue;\n+      }\n+      if (blockLength \u003e prevBlockLength) {\n+        LOG.warn(\"Unexpectedly short length on \" +\n+            prevLongest.src.getAbsolutePath() + \".\");\n+      }\n+      longestBlockFiles.put(blockId, duplicate);\n+    }\n+\n+    // Remove data / metadata entries that aren\u0027t the longest, or weren\u0027t\n+    // arbitrarily selected by us.\n+    for (Iterator\u003cLinkArgs\u003e iter \u003d all.iterator(); iter.hasNext(); ) {\n+      LinkArgs args \u003d iter.next();\n+      long blockId \u003d Block.getBlockId(args.src.getName());\n+      LinkArgs bestDuplicate \u003d longestBlockFiles.get(blockId);\n+      if (bestDuplicate \u003d\u003d null) {\n+        continue; // file has no duplicates\n+      }\n+      if (!bestDuplicate.src.getParent().equals(args.src.getParent())) {\n+        LOG.warn(\"Discarding \" + args.src.getAbsolutePath() + \".\");\n+        iter.remove();\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static void removeDuplicateEntries(ArrayList\u003cLinkArgs\u003e all,\n                                             ArrayList\u003cLinkArgs\u003e duplicates) {\n    // Maps blockId -\u003e metadata file with highest genstamp\n    TreeMap\u003cLong, List\u003cLinkArgs\u003e\u003e highestGenstamps \u003d\n        new TreeMap\u003cLong, List\u003cLinkArgs\u003e\u003e();\n    for (LinkArgs duplicate : duplicates) {\n      if (!Block.isMetaFilename(duplicate.src.getName())) {\n        continue;\n      }\n      long blockId \u003d Block.getBlockId(duplicate.src.getName());\n      List\u003cLinkArgs\u003e prevHighest \u003d highestGenstamps.get(blockId);\n      if (prevHighest \u003d\u003d null) {\n        List\u003cLinkArgs\u003e highest \u003d new LinkedList\u003cLinkArgs\u003e();\n        highest.add(duplicate);\n        highestGenstamps.put(blockId, highest);\n        continue;\n      }\n      long prevGenstamp \u003d\n          Block.getGenerationStamp(prevHighest.get(0).src.getName());\n      long genstamp \u003d Block.getGenerationStamp(duplicate.src.getName());\n      if (genstamp \u003c prevGenstamp) {\n        continue;\n      }\n      if (genstamp \u003e prevGenstamp) {\n        prevHighest.clear();\n      }\n      prevHighest.add(duplicate);\n    }\n\n    // Remove data / metadata entries that don\u0027t have the highest genstamp\n    // from the duplicates list.\n    for (Iterator\u003cLinkArgs\u003e iter \u003d duplicates.iterator(); iter.hasNext(); ) {\n      LinkArgs duplicate \u003d iter.next();\n      long blockId \u003d Block.getBlockId(duplicate.src.getName());\n      List\u003cLinkArgs\u003e highest \u003d highestGenstamps.get(blockId);\n      if (highest !\u003d null) {\n        boolean found \u003d false;\n        for (LinkArgs high : highest) {\n          if (high.src.getParent().equals(duplicate.src.getParent())) {\n            found \u003d true;\n            break;\n          }\n        }\n        if (!found) {\n          LOG.warn(\"Unexpectedly low genstamp on \" +\n                   duplicate.src.getAbsolutePath() + \".\");\n          iter.remove();\n        }\n      }\n    }\n\n    // Find the longest block files\n    // We let the \"last guy win\" here, since we\u0027re only interested in\n    // preserving one block file / metadata file pair.\n    TreeMap\u003cLong, LinkArgs\u003e longestBlockFiles \u003d new TreeMap\u003cLong, LinkArgs\u003e();\n    for (LinkArgs duplicate : duplicates) {\n      if (Block.isMetaFilename(duplicate.src.getName())) {\n        continue;\n      }\n      long blockId \u003d Block.getBlockId(duplicate.src.getName());\n      LinkArgs prevLongest \u003d longestBlockFiles.get(blockId);\n      if (prevLongest \u003d\u003d null) {\n        longestBlockFiles.put(blockId, duplicate);\n        continue;\n      }\n      long blockLength \u003d duplicate.src.length();\n      long prevBlockLength \u003d prevLongest.src.length();\n      if (blockLength \u003c prevBlockLength) {\n        LOG.warn(\"Unexpectedly short length on \" +\n            duplicate.src.getAbsolutePath() + \".\");\n        continue;\n      }\n      if (blockLength \u003e prevBlockLength) {\n        LOG.warn(\"Unexpectedly short length on \" +\n            prevLongest.src.getAbsolutePath() + \".\");\n      }\n      longestBlockFiles.put(blockId, duplicate);\n    }\n\n    // Remove data / metadata entries that aren\u0027t the longest, or weren\u0027t\n    // arbitrarily selected by us.\n    for (Iterator\u003cLinkArgs\u003e iter \u003d all.iterator(); iter.hasNext(); ) {\n      LinkArgs args \u003d iter.next();\n      long blockId \u003d Block.getBlockId(args.src.getName());\n      LinkArgs bestDuplicate \u003d longestBlockFiles.get(blockId);\n      if (bestDuplicate \u003d\u003d null) {\n        continue; // file has no duplicates\n      }\n      if (!bestDuplicate.src.getParent().equals(args.src.getParent())) {\n        LOG.warn(\"Discarding \" + args.src.getAbsolutePath() + \".\");\n        iter.remove();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java"
    }
  }
}