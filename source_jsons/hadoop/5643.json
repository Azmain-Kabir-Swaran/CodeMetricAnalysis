{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSTreeTraverser.java",
  "functionName": "traverseDirInt",
  "functionId": "traverseDirInt___startId-long(modifiers-final)__curr-INode__startAfters-List__byte[]____traverseInfo-TraverseInfo(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSTreeTraverser.java",
  "functionStartLine": 126,
  "functionEndLine": 215,
  "numCommitsSeen": 3,
  "timeTaken": 876,
  "changeHistory": [
    "f89594f0b80e8efffdcb887daa4a18a2b0a228b3"
  ],
  "changeHistoryShort": {
    "f89594f0b80e8efffdcb887daa4a18a2b0a228b3": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f89594f0b80e8efffdcb887daa4a18a2b0a228b3": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13328. Abstract ReencryptionHandler recursive logic in separate class. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "10/04/18 11:05 AM",
      "commitName": "f89594f0b80e8efffdcb887daa4a18a2b0a228b3",
      "commitAuthor": "Rakesh Radhakrishnan",
      "diff": "@@ -0,0 +1,90 @@\n+  protected INode traverseDirInt(final long startId, INode curr,\n+      List\u003cbyte[]\u003e startAfters, final TraverseInfo traverseInfo)\n+      throws IOException, InterruptedException {\n+    assert dir.hasReadLock();\n+    assert dir.getFSNamesystem().hasReadLock();\n+    long lockStartTime \u003d timer.monotonicNow();\n+    Preconditions.checkNotNull(curr, \"Current inode can\u0027t be null\");\n+    checkINodeReady(startId);\n+    final INodeDirectory parent \u003d curr.isDirectory() ? curr.asDirectory()\n+        : curr.getParent();\n+    ReadOnlyList\u003cINode\u003e children \u003d parent\n+        .getChildrenList(Snapshot.CURRENT_STATE_ID);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Traversing directory {}\", parent.getFullPathName());\n+    }\n+\n+    final byte[] startAfter \u003d startAfters.get(startAfters.size() - 1);\n+    boolean lockReleased \u003d false;\n+    for (int i \u003d INodeDirectory.nextChild(children, startAfter); i \u003c children\n+        .size(); ++i) {\n+      final INode inode \u003d children.get(i);\n+      if (!processFileInode(inode, traverseInfo)) {\n+        // inode wasn\u0027t processes. Recurse down if it\u0027s a dir,\n+        // skip otherwise.\n+        if (!inode.isDirectory()) {\n+          continue;\n+        }\n+\n+        if (!canTraverseDir(inode)) {\n+          continue;\n+        }\n+        // add 1 level to the depth-first search.\n+        curr \u003d inode;\n+        if (!startAfters.isEmpty()) {\n+          startAfters.remove(startAfters.size() - 1);\n+          startAfters.add(curr.getLocalNameBytes());\n+        }\n+        startAfters.add(HdfsFileStatus.EMPTY_NAME);\n+        return lockReleased ? null : curr;\n+      }\n+      if (shouldSubmitCurrentBatch()) {\n+        final byte[] currentStartAfter \u003d inode.getLocalNameBytes();\n+        final String parentPath \u003d parent.getFullPathName();\n+        lockReleased \u003d true;\n+        readUnlock();\n+        submitCurrentBatch(startId);\n+        try {\n+          throttle();\n+          checkPauseForTesting();\n+        } finally {\n+          readLock();\n+          lockStartTime \u003d timer.monotonicNow();\n+        }\n+        checkINodeReady(startId);\n+\n+        // Things could have changed when the lock was released.\n+        // Re-resolve the parent inode.\n+        FSPermissionChecker pc \u003d dir.getPermissionChecker();\n+        INode newParent \u003d dir\n+            .resolvePath(pc, parentPath, FSDirectory.DirOp.READ)\n+            .getLastINode();\n+        if (newParent \u003d\u003d null || !newParent.equals(parent)) {\n+          // parent dir is deleted or recreated. We\u0027re done.\n+          return null;\n+        }\n+        children \u003d parent.getChildrenList(Snapshot.CURRENT_STATE_ID);\n+        // -1 to counter the ++ on the for loop\n+        i \u003d INodeDirectory.nextChild(children, currentStartAfter) - 1;\n+      }\n+      if ((timer.monotonicNow()\n+          - lockStartTime) \u003e readLockReportingThresholdMs) {\n+        readUnlock();\n+        try {\n+          throttle();\n+        } finally {\n+          readLock();\n+          lockStartTime \u003d timer.monotonicNow();\n+        }\n+      }\n+    }\n+    // Successfully finished this dir, adjust pointers to 1 level up, and\n+    // startAfter this dir.\n+    startAfters.remove(startAfters.size() - 1);\n+    if (!startAfters.isEmpty()) {\n+      startAfters.remove(startAfters.size() - 1);\n+      startAfters.add(curr.getLocalNameBytes());\n+    }\n+    curr \u003d curr.getParent();\n+    return lockReleased ? null : curr;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected INode traverseDirInt(final long startId, INode curr,\n      List\u003cbyte[]\u003e startAfters, final TraverseInfo traverseInfo)\n      throws IOException, InterruptedException {\n    assert dir.hasReadLock();\n    assert dir.getFSNamesystem().hasReadLock();\n    long lockStartTime \u003d timer.monotonicNow();\n    Preconditions.checkNotNull(curr, \"Current inode can\u0027t be null\");\n    checkINodeReady(startId);\n    final INodeDirectory parent \u003d curr.isDirectory() ? curr.asDirectory()\n        : curr.getParent();\n    ReadOnlyList\u003cINode\u003e children \u003d parent\n        .getChildrenList(Snapshot.CURRENT_STATE_ID);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Traversing directory {}\", parent.getFullPathName());\n    }\n\n    final byte[] startAfter \u003d startAfters.get(startAfters.size() - 1);\n    boolean lockReleased \u003d false;\n    for (int i \u003d INodeDirectory.nextChild(children, startAfter); i \u003c children\n        .size(); ++i) {\n      final INode inode \u003d children.get(i);\n      if (!processFileInode(inode, traverseInfo)) {\n        // inode wasn\u0027t processes. Recurse down if it\u0027s a dir,\n        // skip otherwise.\n        if (!inode.isDirectory()) {\n          continue;\n        }\n\n        if (!canTraverseDir(inode)) {\n          continue;\n        }\n        // add 1 level to the depth-first search.\n        curr \u003d inode;\n        if (!startAfters.isEmpty()) {\n          startAfters.remove(startAfters.size() - 1);\n          startAfters.add(curr.getLocalNameBytes());\n        }\n        startAfters.add(HdfsFileStatus.EMPTY_NAME);\n        return lockReleased ? null : curr;\n      }\n      if (shouldSubmitCurrentBatch()) {\n        final byte[] currentStartAfter \u003d inode.getLocalNameBytes();\n        final String parentPath \u003d parent.getFullPathName();\n        lockReleased \u003d true;\n        readUnlock();\n        submitCurrentBatch(startId);\n        try {\n          throttle();\n          checkPauseForTesting();\n        } finally {\n          readLock();\n          lockStartTime \u003d timer.monotonicNow();\n        }\n        checkINodeReady(startId);\n\n        // Things could have changed when the lock was released.\n        // Re-resolve the parent inode.\n        FSPermissionChecker pc \u003d dir.getPermissionChecker();\n        INode newParent \u003d dir\n            .resolvePath(pc, parentPath, FSDirectory.DirOp.READ)\n            .getLastINode();\n        if (newParent \u003d\u003d null || !newParent.equals(parent)) {\n          // parent dir is deleted or recreated. We\u0027re done.\n          return null;\n        }\n        children \u003d parent.getChildrenList(Snapshot.CURRENT_STATE_ID);\n        // -1 to counter the ++ on the for loop\n        i \u003d INodeDirectory.nextChild(children, currentStartAfter) - 1;\n      }\n      if ((timer.monotonicNow()\n          - lockStartTime) \u003e readLockReportingThresholdMs) {\n        readUnlock();\n        try {\n          throttle();\n        } finally {\n          readLock();\n          lockStartTime \u003d timer.monotonicNow();\n        }\n      }\n    }\n    // Successfully finished this dir, adjust pointers to 1 level up, and\n    // startAfter this dir.\n    startAfters.remove(startAfters.size() - 1);\n    if (!startAfters.isEmpty()) {\n      startAfters.remove(startAfters.size() - 1);\n      startAfters.add(curr.getLocalNameBytes());\n    }\n    curr \u003d curr.getParent();\n    return lockReleased ? null : curr;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSTreeTraverser.java"
    }
  }
}