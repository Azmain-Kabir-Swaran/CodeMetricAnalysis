{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataNode.java",
  "functionName": "checkDiskError",
  "functionId": "checkDiskError",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
  "functionStartLine": 3443,
  "functionEndLine": 3460,
  "numCommitsSeen": 358,
  "timeTaken": 2765,
  "changeHistory": [
    "a3f945fb8466d461d42ce60f0bc12c96fbb2db23",
    "dfcb331ba3516264398121c9f23af3a79c0509cc"
  ],
  "changeHistoryShort": {
    "a3f945fb8466d461d42ce60f0bc12c96fbb2db23": "Ybodychange",
    "dfcb331ba3516264398121c9f23af3a79c0509cc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a3f945fb8466d461d42ce60f0bc12c96fbb2db23": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-17035. fixed typos (timeout, interruped) (#2007)\n\nCo-authored-by: Sungpeo Kook \u003celixir.kook@kakaocorp.com\u003e",
      "commitDate": "12/05/20 8:50 AM",
      "commitName": "a3f945fb8466d461d42ce60f0bc12c96fbb2db23",
      "commitAuthor": "Elixir Kook",
      "commitDateOld": "09/02/20 10:02 AM",
      "commitNameOld": "6191d4b4a0919863fda78e549ab6c60022e3ebc2",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 92.91,
      "commitsBetweenForRepo": 311,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public void checkDiskError() throws IOException {\n     Set\u003cFsVolumeSpi\u003e unhealthyVolumes;\n     try {\n       unhealthyVolumes \u003d volumeChecker.checkAllVolumes(data);\n       lastDiskErrorCheck \u003d Time.monotonicNow();\n     } catch (InterruptedException e) {\n-      LOG.error(\"Interruped while running disk check\", e);\n+      LOG.error(\"Interrupted while running disk check\", e);\n       throw new IOException(\"Interrupted while running disk check\", e);\n     }\n \n     if (unhealthyVolumes.size() \u003e 0) {\n       LOG.warn(\"checkDiskError got {} failed volumes - {}\",\n           unhealthyVolumes.size(), unhealthyVolumes);\n       handleVolumeFailures(unhealthyVolumes);\n     } else {\n       LOG.debug(\"checkDiskError encountered no failures\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void checkDiskError() throws IOException {\n    Set\u003cFsVolumeSpi\u003e unhealthyVolumes;\n    try {\n      unhealthyVolumes \u003d volumeChecker.checkAllVolumes(data);\n      lastDiskErrorCheck \u003d Time.monotonicNow();\n    } catch (InterruptedException e) {\n      LOG.error(\"Interrupted while running disk check\", e);\n      throw new IOException(\"Interrupted while running disk check\", e);\n    }\n\n    if (unhealthyVolumes.size() \u003e 0) {\n      LOG.warn(\"checkDiskError got {} failed volumes - {}\",\n          unhealthyVolumes.size(), unhealthyVolumes);\n      handleVolumeFailures(unhealthyVolumes);\n    } else {\n      LOG.debug(\"checkDiskError encountered no failures\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "dfcb331ba3516264398121c9f23af3a79c0509cc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13076: [SPS]: Addendum. Resolve conflicts after rebasing branch to trunk. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "dfcb331ba3516264398121c9f23af3a79c0509cc",
      "commitAuthor": "Rakesh Radhakrishnan",
      "diff": "@@ -0,0 +1,18 @@\n+  public void checkDiskError() throws IOException {\n+    Set\u003cFsVolumeSpi\u003e unhealthyVolumes;\n+    try {\n+      unhealthyVolumes \u003d volumeChecker.checkAllVolumes(data);\n+      lastDiskErrorCheck \u003d Time.monotonicNow();\n+    } catch (InterruptedException e) {\n+      LOG.error(\"Interruped while running disk check\", e);\n+      throw new IOException(\"Interrupted while running disk check\", e);\n+    }\n+\n+    if (unhealthyVolumes.size() \u003e 0) {\n+      LOG.warn(\"checkDiskError got {} failed volumes - {}\",\n+          unhealthyVolumes.size(), unhealthyVolumes);\n+      handleVolumeFailures(unhealthyVolumes);\n+    } else {\n+      LOG.debug(\"checkDiskError encountered no failures\");\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void checkDiskError() throws IOException {\n    Set\u003cFsVolumeSpi\u003e unhealthyVolumes;\n    try {\n      unhealthyVolumes \u003d volumeChecker.checkAllVolumes(data);\n      lastDiskErrorCheck \u003d Time.monotonicNow();\n    } catch (InterruptedException e) {\n      LOG.error(\"Interruped while running disk check\", e);\n      throw new IOException(\"Interrupted while running disk check\", e);\n    }\n\n    if (unhealthyVolumes.size() \u003e 0) {\n      LOG.warn(\"checkDiskError got {} failed volumes - {}\",\n          unhealthyVolumes.size(), unhealthyVolumes);\n      handleVolumeFailures(unhealthyVolumes);\n    } else {\n      LOG.debug(\"checkDiskError encountered no failures\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
    }
  }
}