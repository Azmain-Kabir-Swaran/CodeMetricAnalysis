{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PacketReceiver.java",
  "functionName": "reslicePacket",
  "functionId": "reslicePacket___headerLen-int__checksumsLen-int__dataLen-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/PacketReceiver.java",
  "functionStartLine": 218,
  "functionEndLine": 250,
  "numCommitsSeen": 11,
  "timeTaken": 1275,
  "changeHistory": [
    "826ae1c26d31f87d88efc920b271bec7eec2e17a",
    "ded304e6a6e74742f6f4a35989f762dcefa234cb",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50"
  ],
  "changeHistoryShort": {
    "826ae1c26d31f87d88efc920b271bec7eec2e17a": "Yfilerename",
    "ded304e6a6e74742f6f4a35989f762dcefa234cb": "Ybodychange",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": "Yintroduced"
  },
  "changeHistoryDetails": {
    "826ae1c26d31f87d88efc920b271bec7eec2e17a": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8990. Move RemoteBlockReader to hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "31/08/15 1:54 PM",
      "commitName": "826ae1c26d31f87d88efc920b271bec7eec2e17a",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "31/08/15 11:48 AM",
      "commitNameOld": "caa04de149030691b7bc952b534c6128db217ed2",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.09,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void reslicePacket(\n      int headerLen, int checksumsLen, int dataLen) {\n    // Packet structure (refer to doRead() for details):\n    //   PLEN    HLEN      HEADER     CHECKSUMS  DATA\n    //   32-bit  16-bit   \u003cprotobuf\u003e  \u003cvariable length\u003e\n    //   |--- lenThroughHeader ----|\n    //   |----------- lenThroughChecksums   ----|\n    //   |------------------- lenThroughData    ------| \n    int lenThroughHeader \u003d PacketHeader.PKT_LENGTHS_LEN + headerLen;\n    int lenThroughChecksums \u003d lenThroughHeader + checksumsLen;\n    int lenThroughData \u003d lenThroughChecksums + dataLen;\n\n    assert dataLen \u003e\u003d 0 : \"invalid datalen: \" + dataLen;\n    assert curPacketBuf.position() \u003d\u003d lenThroughHeader;\n    assert curPacketBuf.limit() \u003d\u003d lenThroughData :\n      \"headerLen\u003d \" + headerLen + \" clen\u003d\" + checksumsLen + \" dlen\u003d\" + dataLen +\n      \" rem\u003d\" + curPacketBuf.remaining();\n\n    // Slice the checksums.\n    curPacketBuf.position(lenThroughHeader);\n    curPacketBuf.limit(lenThroughChecksums);\n    curChecksumSlice \u003d curPacketBuf.slice();\n\n    // Slice the data.\n    curPacketBuf.position(lenThroughChecksums);\n    curPacketBuf.limit(lenThroughData);\n    curDataSlice \u003d curPacketBuf.slice();\n    \n    // Reset buffer to point to the entirety of the packet (including\n    // length prefixes)\n    curPacketBuf.position(0);\n    curPacketBuf.limit(lenThroughData);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/PacketReceiver.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/PacketReceiver.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/PacketReceiver.java"
      }
    },
    "ded304e6a6e74742f6f4a35989f762dcefa234cb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4049. Fix hflush performance regression due to nagling delays. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1398591 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/10/12 5:55 PM",
      "commitName": "ded304e6a6e74742f6f4a35989f762dcefa234cb",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "09/08/12 2:31 PM",
      "commitNameOld": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 67.14,
      "commitsBetweenForRepo": 411,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,33 @@\n   private void reslicePacket(\n       int headerLen, int checksumsLen, int dataLen) {\n+    // Packet structure (refer to doRead() for details):\n+    //   PLEN    HLEN      HEADER     CHECKSUMS  DATA\n+    //   32-bit  16-bit   \u003cprotobuf\u003e  \u003cvariable length\u003e\n+    //   |--- lenThroughHeader ----|\n+    //   |----------- lenThroughChecksums   ----|\n+    //   |------------------- lenThroughData    ------| \n+    int lenThroughHeader \u003d PacketHeader.PKT_LENGTHS_LEN + headerLen;\n+    int lenThroughChecksums \u003d lenThroughHeader + checksumsLen;\n+    int lenThroughData \u003d lenThroughChecksums + dataLen;\n+\n     assert dataLen \u003e\u003d 0 : \"invalid datalen: \" + dataLen;\n-    \n-    assert curPacketBuf.position() \u003d\u003d headerLen;\n-    assert checksumsLen + dataLen \u003d\u003d curPacketBuf.remaining() :\n+    assert curPacketBuf.position() \u003d\u003d lenThroughHeader;\n+    assert curPacketBuf.limit() \u003d\u003d lenThroughData :\n       \"headerLen\u003d \" + headerLen + \" clen\u003d\" + checksumsLen + \" dlen\u003d\" + dataLen +\n       \" rem\u003d\" + curPacketBuf.remaining();\n-    \n-    curPacketBuf.position(headerLen);\n-    curPacketBuf.limit(headerLen + checksumsLen);\n+\n+    // Slice the checksums.\n+    curPacketBuf.position(lenThroughHeader);\n+    curPacketBuf.limit(lenThroughChecksums);\n     curChecksumSlice \u003d curPacketBuf.slice();\n \n-    curPacketBuf.position(headerLen + checksumsLen);\n-    curPacketBuf.limit(headerLen + checksumsLen + dataLen);\n+    // Slice the data.\n+    curPacketBuf.position(lenThroughChecksums);\n+    curPacketBuf.limit(lenThroughData);\n     curDataSlice \u003d curPacketBuf.slice();\n     \n+    // Reset buffer to point to the entirety of the packet (including\n+    // length prefixes)\n     curPacketBuf.position(0);\n-    curPacketBuf.limit(headerLen + checksumsLen + dataLen);\n+    curPacketBuf.limit(lenThroughData);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void reslicePacket(\n      int headerLen, int checksumsLen, int dataLen) {\n    // Packet structure (refer to doRead() for details):\n    //   PLEN    HLEN      HEADER     CHECKSUMS  DATA\n    //   32-bit  16-bit   \u003cprotobuf\u003e  \u003cvariable length\u003e\n    //   |--- lenThroughHeader ----|\n    //   |----------- lenThroughChecksums   ----|\n    //   |------------------- lenThroughData    ------| \n    int lenThroughHeader \u003d PacketHeader.PKT_LENGTHS_LEN + headerLen;\n    int lenThroughChecksums \u003d lenThroughHeader + checksumsLen;\n    int lenThroughData \u003d lenThroughChecksums + dataLen;\n\n    assert dataLen \u003e\u003d 0 : \"invalid datalen: \" + dataLen;\n    assert curPacketBuf.position() \u003d\u003d lenThroughHeader;\n    assert curPacketBuf.limit() \u003d\u003d lenThroughData :\n      \"headerLen\u003d \" + headerLen + \" clen\u003d\" + checksumsLen + \" dlen\u003d\" + dataLen +\n      \" rem\u003d\" + curPacketBuf.remaining();\n\n    // Slice the checksums.\n    curPacketBuf.position(lenThroughHeader);\n    curPacketBuf.limit(lenThroughChecksums);\n    curChecksumSlice \u003d curPacketBuf.slice();\n\n    // Slice the data.\n    curPacketBuf.position(lenThroughChecksums);\n    curPacketBuf.limit(lenThroughData);\n    curDataSlice \u003d curPacketBuf.slice();\n    \n    // Reset buffer to point to the entirety of the packet (including\n    // length prefixes)\n    curPacketBuf.position(0);\n    curPacketBuf.limit(lenThroughData);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/PacketReceiver.java",
      "extendedDetails": {}
    },
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3721. hsync support broke wire compatibility. Contributed by Todd Lipcon and Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1371495 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/12 2:31 PM",
      "commitName": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
      "commitAuthor": "Aaron Myers",
      "diff": "@@ -0,0 +1,20 @@\n+  private void reslicePacket(\n+      int headerLen, int checksumsLen, int dataLen) {\n+    assert dataLen \u003e\u003d 0 : \"invalid datalen: \" + dataLen;\n+    \n+    assert curPacketBuf.position() \u003d\u003d headerLen;\n+    assert checksumsLen + dataLen \u003d\u003d curPacketBuf.remaining() :\n+      \"headerLen\u003d \" + headerLen + \" clen\u003d\" + checksumsLen + \" dlen\u003d\" + dataLen +\n+      \" rem\u003d\" + curPacketBuf.remaining();\n+    \n+    curPacketBuf.position(headerLen);\n+    curPacketBuf.limit(headerLen + checksumsLen);\n+    curChecksumSlice \u003d curPacketBuf.slice();\n+\n+    curPacketBuf.position(headerLen + checksumsLen);\n+    curPacketBuf.limit(headerLen + checksumsLen + dataLen);\n+    curDataSlice \u003d curPacketBuf.slice();\n+    \n+    curPacketBuf.position(0);\n+    curPacketBuf.limit(headerLen + checksumsLen + dataLen);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void reslicePacket(\n      int headerLen, int checksumsLen, int dataLen) {\n    assert dataLen \u003e\u003d 0 : \"invalid datalen: \" + dataLen;\n    \n    assert curPacketBuf.position() \u003d\u003d headerLen;\n    assert checksumsLen + dataLen \u003d\u003d curPacketBuf.remaining() :\n      \"headerLen\u003d \" + headerLen + \" clen\u003d\" + checksumsLen + \" dlen\u003d\" + dataLen +\n      \" rem\u003d\" + curPacketBuf.remaining();\n    \n    curPacketBuf.position(headerLen);\n    curPacketBuf.limit(headerLen + checksumsLen);\n    curChecksumSlice \u003d curPacketBuf.slice();\n\n    curPacketBuf.position(headerLen + checksumsLen);\n    curPacketBuf.limit(headerLen + checksumsLen + dataLen);\n    curDataSlice \u003d curPacketBuf.slice();\n    \n    curPacketBuf.position(0);\n    curPacketBuf.limit(headerLen + checksumsLen + dataLen);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/PacketReceiver.java"
    }
  }
}