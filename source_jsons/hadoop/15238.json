{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSAdmin.java",
  "functionName": "refreshSuperUserGroupsConfiguration",
  "functionId": "refreshSuperUserGroupsConfiguration",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
  "functionStartLine": 1745,
  "functionEndLine": 1792,
  "numCommitsSeen": 118,
  "timeTaken": 7705,
  "changeHistory": [
    "01bd6ab18fa48f4c7cac1497905b52e547962599",
    "6f8003dc7bc9e8be7b0512c514d370c303faf003",
    "e8ca6480050e38d2fe4859baf4f9a8d22e7f9b85",
    "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
    "c69dfdd5e14af490790dff8227b11962ec816577",
    "4efd3699a6f271a21b1024d17c277930c83436da",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "01bd6ab18fa48f4c7cac1497905b52e547962599": "Ybodychange",
    "6f8003dc7bc9e8be7b0512c514d370c303faf003": "Ybodychange",
    "e8ca6480050e38d2fe4859baf4f9a8d22e7f9b85": "Ybodychange",
    "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd": "Ybodychange",
    "c69dfdd5e14af490790dff8227b11962ec816577": "Ybodychange",
    "4efd3699a6f271a21b1024d17c277930c83436da": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "01bd6ab18fa48f4c7cac1497905b52e547962599": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12935. Get ambiguous result for DFSAdmin command in HA mode when only one namenode is up. Contributed by Jianfei Jiang.\n",
      "commitDate": "07/02/18 9:40 AM",
      "commitName": "01bd6ab18fa48f4c7cac1497905b52e547962599",
      "commitAuthor": "Brahma Reddy Battula",
      "commitDateOld": "05/01/18 10:31 PM",
      "commitNameOld": "bf5c94899537011465350d5d999fad9ffaeb605d",
      "commitAuthorOld": "Yiqun Lin",
      "daysBetweenCommits": 32.46,
      "commitsBetweenForRepo": 204,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,48 @@\n   public int refreshSuperUserGroupsConfiguration() throws IOException {\n     // Get the current configuration\n     Configuration conf \u003d getConf();\n \n     // for security authorization\n     // server principal for this call \n     // should be NAMENODE\u0027s one.\n     conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n         conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n \n     DistributedFileSystem dfs \u003d getDFS();\n     URI dfsUri \u003d dfs.getUri();\n     boolean isHaEnabled \u003d HAUtilClient.isLogicalUri(conf, dfsUri);\n \n     if (isHaEnabled) {\n       // Run refreshSuperUserGroupsConfiguration for all NNs if HA is enabled\n       String nsId \u003d dfsUri.getHost();\n       List\u003cProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e\u003e proxies \u003d\n           HAUtil.getProxiesForAllNameNodesInNameservice(conf, nsId,\n               RefreshUserMappingsProtocol.class);\n+      List\u003cIOException\u003e exceptions \u003d new ArrayList\u003c\u003e();\n       for (ProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e proxy : proxies) {\n-        proxy.getProxy().refreshSuperUserGroupsConfiguration();\n-        System.out.println(\"Refresh super user groups configuration \" +\n-            \"successful for \" + proxy.getAddress());\n+        try{\n+          proxy.getProxy().refreshSuperUserGroupsConfiguration();\n+          System.out.println(\"Refresh super user groups configuration \" +\n+              \"successful for \" + proxy.getAddress());\n+        }catch (IOException ioe){\n+          System.out.println(\"Refresh super user groups configuration \" +\n+              \"failed for \" + proxy.getAddress());\n+          exceptions.add(ioe);\n+        }\n+      }\n+      if(!exceptions.isEmpty()){\n+        throw MultipleIOException.createIOException(exceptions);\n       }\n     } else {\n       // Create the client\n       RefreshUserMappingsProtocol refreshProtocol \u003d\n           NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n               RefreshUserMappingsProtocol.class).getProxy();\n \n       // Refresh the user-to-groups mappings\n       refreshProtocol.refreshSuperUserGroupsConfiguration();\n       System.out.println(\"Refresh super user groups configuration successful\");\n     }\n \n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int refreshSuperUserGroupsConfiguration() throws IOException {\n    // Get the current configuration\n    Configuration conf \u003d getConf();\n\n    // for security authorization\n    // server principal for this call \n    // should be NAMENODE\u0027s one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n\n    DistributedFileSystem dfs \u003d getDFS();\n    URI dfsUri \u003d dfs.getUri();\n    boolean isHaEnabled \u003d HAUtilClient.isLogicalUri(conf, dfsUri);\n\n    if (isHaEnabled) {\n      // Run refreshSuperUserGroupsConfiguration for all NNs if HA is enabled\n      String nsId \u003d dfsUri.getHost();\n      List\u003cProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e\u003e proxies \u003d\n          HAUtil.getProxiesForAllNameNodesInNameservice(conf, nsId,\n              RefreshUserMappingsProtocol.class);\n      List\u003cIOException\u003e exceptions \u003d new ArrayList\u003c\u003e();\n      for (ProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e proxy : proxies) {\n        try{\n          proxy.getProxy().refreshSuperUserGroupsConfiguration();\n          System.out.println(\"Refresh super user groups configuration \" +\n              \"successful for \" + proxy.getAddress());\n        }catch (IOException ioe){\n          System.out.println(\"Refresh super user groups configuration \" +\n              \"failed for \" + proxy.getAddress());\n          exceptions.add(ioe);\n        }\n      }\n      if(!exceptions.isEmpty()){\n        throw MultipleIOException.createIOException(exceptions);\n      }\n    } else {\n      // Create the client\n      RefreshUserMappingsProtocol refreshProtocol \u003d\n          NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n              RefreshUserMappingsProtocol.class).getProxy();\n\n      // Refresh the user-to-groups mappings\n      refreshProtocol.refreshSuperUserGroupsConfiguration();\n      System.out.println(\"Refresh super user groups configuration successful\");\n    }\n\n    return 0;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "6f8003dc7bc9e8be7b0512c514d370c303faf003": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8185. Separate client related routines in HAUtil into a new class. Contributed by Haohui Mai.\n",
      "commitDate": "21/04/15 9:59 PM",
      "commitName": "6f8003dc7bc9e8be7b0512c514d370c303faf003",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "25/03/15 10:38 AM",
      "commitNameOld": "5e21e4ca377f68e030f8f3436cd93fd7a74dc5e0",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 27.47,
      "commitsBetweenForRepo": 221,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   public int refreshSuperUserGroupsConfiguration() throws IOException {\n     // Get the current configuration\n     Configuration conf \u003d getConf();\n \n     // for security authorization\n     // server principal for this call \n     // should be NAMENODE\u0027s one.\n     conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n         conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n \n     DistributedFileSystem dfs \u003d getDFS();\n     URI dfsUri \u003d dfs.getUri();\n-    boolean isHaEnabled \u003d HAUtil.isLogicalUri(conf, dfsUri);\n+    boolean isHaEnabled \u003d HAUtilClient.isLogicalUri(conf, dfsUri);\n \n     if (isHaEnabled) {\n       // Run refreshSuperUserGroupsConfiguration for all NNs if HA is enabled\n       String nsId \u003d dfsUri.getHost();\n       List\u003cProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e\u003e proxies \u003d\n           HAUtil.getProxiesForAllNameNodesInNameservice(conf, nsId,\n               RefreshUserMappingsProtocol.class);\n       for (ProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e proxy : proxies) {\n         proxy.getProxy().refreshSuperUserGroupsConfiguration();\n         System.out.println(\"Refresh super user groups configuration \" +\n             \"successful for \" + proxy.getAddress());\n       }\n     } else {\n       // Create the client\n       RefreshUserMappingsProtocol refreshProtocol \u003d\n           NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n               RefreshUserMappingsProtocol.class).getProxy();\n \n       // Refresh the user-to-groups mappings\n       refreshProtocol.refreshSuperUserGroupsConfiguration();\n       System.out.println(\"Refresh super user groups configuration successful\");\n     }\n \n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int refreshSuperUserGroupsConfiguration() throws IOException {\n    // Get the current configuration\n    Configuration conf \u003d getConf();\n\n    // for security authorization\n    // server principal for this call \n    // should be NAMENODE\u0027s one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n\n    DistributedFileSystem dfs \u003d getDFS();\n    URI dfsUri \u003d dfs.getUri();\n    boolean isHaEnabled \u003d HAUtilClient.isLogicalUri(conf, dfsUri);\n\n    if (isHaEnabled) {\n      // Run refreshSuperUserGroupsConfiguration for all NNs if HA is enabled\n      String nsId \u003d dfsUri.getHost();\n      List\u003cProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e\u003e proxies \u003d\n          HAUtil.getProxiesForAllNameNodesInNameservice(conf, nsId,\n              RefreshUserMappingsProtocol.class);\n      for (ProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e proxy : proxies) {\n        proxy.getProxy().refreshSuperUserGroupsConfiguration();\n        System.out.println(\"Refresh super user groups configuration \" +\n            \"successful for \" + proxy.getAddress());\n      }\n    } else {\n      // Create the client\n      RefreshUserMappingsProtocol refreshProtocol \u003d\n          NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n              RefreshUserMappingsProtocol.class).getProxy();\n\n      // Refresh the user-to-groups mappings\n      refreshProtocol.refreshSuperUserGroupsConfiguration();\n      System.out.println(\"Refresh super user groups configuration successful\");\n    }\n\n    return 0;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "e8ca6480050e38d2fe4859baf4f9a8d22e7f9b85": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6507. Improve DFSAdmin to support HA cluster better. (Contributd by Zesheng Wu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1604692 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/06/14 10:16 PM",
      "commitName": "e8ca6480050e38d2fe4859baf4f9a8d22e7f9b85",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "11/06/14 6:27 PM",
      "commitNameOld": "34e9173c00f7e1ae55dec365850849c793cde8e3",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 11.16,
      "commitsBetweenForRepo": 76,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,38 @@\n   public int refreshSuperUserGroupsConfiguration() throws IOException {\n     // Get the current configuration\n     Configuration conf \u003d getConf();\n \n     // for security authorization\n     // server principal for this call \n     // should be NAMENODE\u0027s one.\n     conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n         conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n \n-    // Create the client\n-    RefreshUserMappingsProtocol refreshProtocol \u003d\n-      NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n-          RefreshUserMappingsProtocol.class).getProxy();\n+    DistributedFileSystem dfs \u003d getDFS();\n+    URI dfsUri \u003d dfs.getUri();\n+    boolean isHaEnabled \u003d HAUtil.isLogicalUri(conf, dfsUri);\n \n-    // Refresh the user-to-groups mappings\n-    refreshProtocol.refreshSuperUserGroupsConfiguration();\n+    if (isHaEnabled) {\n+      // Run refreshSuperUserGroupsConfiguration for all NNs if HA is enabled\n+      String nsId \u003d dfsUri.getHost();\n+      List\u003cProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e\u003e proxies \u003d\n+          HAUtil.getProxiesForAllNameNodesInNameservice(conf, nsId,\n+              RefreshUserMappingsProtocol.class);\n+      for (ProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e proxy : proxies) {\n+        proxy.getProxy().refreshSuperUserGroupsConfiguration();\n+        System.out.println(\"Refresh super user groups configuration \" +\n+            \"successful for \" + proxy.getAddress());\n+      }\n+    } else {\n+      // Create the client\n+      RefreshUserMappingsProtocol refreshProtocol \u003d\n+          NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n+              RefreshUserMappingsProtocol.class).getProxy();\n+\n+      // Refresh the user-to-groups mappings\n+      refreshProtocol.refreshSuperUserGroupsConfiguration();\n+      System.out.println(\"Refresh super user groups configuration successful\");\n+    }\n \n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int refreshSuperUserGroupsConfiguration() throws IOException {\n    // Get the current configuration\n    Configuration conf \u003d getConf();\n\n    // for security authorization\n    // server principal for this call \n    // should be NAMENODE\u0027s one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n\n    DistributedFileSystem dfs \u003d getDFS();\n    URI dfsUri \u003d dfs.getUri();\n    boolean isHaEnabled \u003d HAUtil.isLogicalUri(conf, dfsUri);\n\n    if (isHaEnabled) {\n      // Run refreshSuperUserGroupsConfiguration for all NNs if HA is enabled\n      String nsId \u003d dfsUri.getHost();\n      List\u003cProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e\u003e proxies \u003d\n          HAUtil.getProxiesForAllNameNodesInNameservice(conf, nsId,\n              RefreshUserMappingsProtocol.class);\n      for (ProxyAndInfo\u003cRefreshUserMappingsProtocol\u003e proxy : proxies) {\n        proxy.getProxy().refreshSuperUserGroupsConfiguration();\n        System.out.println(\"Refresh super user groups configuration \" +\n            \"successful for \" + proxy.getAddress());\n      }\n    } else {\n      // Create the client\n      RefreshUserMappingsProtocol refreshProtocol \u003d\n          NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n              RefreshUserMappingsProtocol.class).getProxy();\n\n      // Refresh the user-to-groups mappings\n      refreshProtocol.refreshSuperUserGroupsConfiguration();\n      System.out.println(\"Refresh super user groups configuration successful\");\n    }\n\n    return 0;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/04/14 11:55 AM",
      "commitName": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "03/04/14 1:18 PM",
      "commitNameOld": "620809b9a063bd6ea84d582166eed3fb957dcd0a",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.94,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   public int refreshSuperUserGroupsConfiguration() throws IOException {\n     // Get the current configuration\n     Configuration conf \u003d getConf();\n \n     // for security authorization\n     // server principal for this call \n     // should be NAMENODE\u0027s one.\n     conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n-        conf.get(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\"));\n+        conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n \n     // Create the client\n     RefreshUserMappingsProtocol refreshProtocol \u003d\n       NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n           RefreshUserMappingsProtocol.class).getProxy();\n \n     // Refresh the user-to-groups mappings\n     refreshProtocol.refreshSuperUserGroupsConfiguration();\n \n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int refreshSuperUserGroupsConfiguration() throws IOException {\n    // Get the current configuration\n    Configuration conf \u003d getConf();\n\n    // for security authorization\n    // server principal for this call \n    // should be NAMENODE\u0027s one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\n\n    // Create the client\n    RefreshUserMappingsProtocol refreshProtocol \u003d\n      NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n          RefreshUserMappingsProtocol.class).getProxy();\n\n    // Refresh the user-to-groups mappings\n    refreshProtocol.refreshSuperUserGroupsConfiguration();\n\n    return 0;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "c69dfdd5e14af490790dff8227b11962ec816577": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2958. Sweep for remaining proxy construction which doesn\u0027t go through failover path.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1294811 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/02/12 12:09 PM",
      "commitName": "c69dfdd5e14af490790dff8227b11962ec816577",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "07/02/12 7:55 PM",
      "commitNameOld": "11db1b855fa08a5ef7cdf2d40bd18ee33eba92ee",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 20.68,
      "commitsBetweenForRepo": 151,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   public int refreshSuperUserGroupsConfiguration() throws IOException {\n     // Get the current configuration\n     Configuration conf \u003d getConf();\n \n     // for security authorization\n     // server principal for this call \n     // should be NAMENODE\u0027s one.\n     conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n         conf.get(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\"));\n \n     // Create the client\n-    RefreshUserMappingsProtocolClientSideTranslatorPB refreshProtocol \u003d \n-        new RefreshUserMappingsProtocolClientSideTranslatorPB(\n-        NameNode.getAddress(conf), getUGI(), conf);\n+    RefreshUserMappingsProtocol refreshProtocol \u003d\n+      NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n+          RefreshUserMappingsProtocol.class).getProxy();\n \n     // Refresh the user-to-groups mappings\n     refreshProtocol.refreshSuperUserGroupsConfiguration();\n \n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int refreshSuperUserGroupsConfiguration() throws IOException {\n    // Get the current configuration\n    Configuration conf \u003d getConf();\n\n    // for security authorization\n    // server principal for this call \n    // should be NAMENODE\u0027s one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\"));\n\n    // Create the client\n    RefreshUserMappingsProtocol refreshProtocol \u003d\n      NameNodeProxies.createProxy(conf, FileSystem.getDefaultUri(conf),\n          RefreshUserMappingsProtocol.class).getProxy();\n\n    // Refresh the user-to-groups mappings\n    refreshProtocol.refreshSuperUserGroupsConfiguration();\n\n    return 0;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "4efd3699a6f271a21b1024d17c277930c83436da": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2697. Move RefreshAuthPolicy, RefreshUserMappings, GetUserMappings protocol to protocol buffers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227887 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/01/12 3:03 PM",
      "commitName": "4efd3699a6f271a21b1024d17c277930c83436da",
      "commitAuthor": "Jitendra Nath Pandey",
      "commitDateOld": "24/10/11 12:08 PM",
      "commitNameOld": "0920056f0467bcf055628bc23d91c602aac7da49",
      "commitAuthorOld": "Sanjay Radia",
      "daysBetweenCommits": 73.16,
      "commitsBetweenForRepo": 440,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,20 @@\n   public int refreshSuperUserGroupsConfiguration() throws IOException {\n     // Get the current configuration\n     Configuration conf \u003d getConf();\n \n     // for security authorization\n     // server principal for this call \n     // should be NAMENODE\u0027s one.\n     conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n         conf.get(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\"));\n \n     // Create the client\n-    RefreshUserMappingsProtocol refreshProtocol \u003d \n-      (RefreshUserMappingsProtocol) \n-      RPC.getProxy(RefreshUserMappingsProtocol.class, \n-          RefreshUserMappingsProtocol.versionID, \n-          NameNode.getAddress(conf), getUGI(), conf,\n-          NetUtils.getSocketFactory(conf, \n-              RefreshUserMappingsProtocol.class));\n+    RefreshUserMappingsProtocolClientSideTranslatorPB refreshProtocol \u003d \n+        new RefreshUserMappingsProtocolClientSideTranslatorPB(\n+        NameNode.getAddress(conf), getUGI(), conf);\n \n     // Refresh the user-to-groups mappings\n     refreshProtocol.refreshSuperUserGroupsConfiguration();\n \n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int refreshSuperUserGroupsConfiguration() throws IOException {\n    // Get the current configuration\n    Configuration conf \u003d getConf();\n\n    // for security authorization\n    // server principal for this call \n    // should be NAMENODE\u0027s one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\"));\n\n    // Create the client\n    RefreshUserMappingsProtocolClientSideTranslatorPB refreshProtocol \u003d \n        new RefreshUserMappingsProtocolClientSideTranslatorPB(\n        NameNode.getAddress(conf), getUGI(), conf);\n\n    // Refresh the user-to-groups mappings\n    refreshProtocol.refreshSuperUserGroupsConfiguration();\n\n    return 0;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public int refreshSuperUserGroupsConfiguration() throws IOException {\n    // Get the current configuration\n    Configuration conf \u003d getConf();\n\n    // for security authorization\n    // server principal for this call \n    // should be NAMENODE\u0027s one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\"));\n\n    // Create the client\n    RefreshUserMappingsProtocol refreshProtocol \u003d \n      (RefreshUserMappingsProtocol) \n      RPC.getProxy(RefreshUserMappingsProtocol.class, \n          RefreshUserMappingsProtocol.versionID, \n          NameNode.getAddress(conf), getUGI(), conf,\n          NetUtils.getSocketFactory(conf, \n              RefreshUserMappingsProtocol.class));\n\n    // Refresh the user-to-groups mappings\n    refreshProtocol.refreshSuperUserGroupsConfiguration();\n\n    return 0;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public int refreshSuperUserGroupsConfiguration() throws IOException {\n    // Get the current configuration\n    Configuration conf \u003d getConf();\n\n    // for security authorization\n    // server principal for this call \n    // should be NAMENODE\u0027s one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\"));\n\n    // Create the client\n    RefreshUserMappingsProtocol refreshProtocol \u003d \n      (RefreshUserMappingsProtocol) \n      RPC.getProxy(RefreshUserMappingsProtocol.class, \n          RefreshUserMappingsProtocol.versionID, \n          NameNode.getAddress(conf), getUGI(), conf,\n          NetUtils.getSocketFactory(conf, \n              RefreshUserMappingsProtocol.class));\n\n    // Refresh the user-to-groups mappings\n    refreshProtocol.refreshSuperUserGroupsConfiguration();\n\n    return 0;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,24 @@\n+  public int refreshSuperUserGroupsConfiguration() throws IOException {\n+    // Get the current configuration\n+    Configuration conf \u003d getConf();\n+\n+    // for security authorization\n+    // server principal for this call \n+    // should be NAMENODE\u0027s one.\n+    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n+        conf.get(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\"));\n+\n+    // Create the client\n+    RefreshUserMappingsProtocol refreshProtocol \u003d \n+      (RefreshUserMappingsProtocol) \n+      RPC.getProxy(RefreshUserMappingsProtocol.class, \n+          RefreshUserMappingsProtocol.versionID, \n+          NameNode.getAddress(conf), getUGI(), conf,\n+          NetUtils.getSocketFactory(conf, \n+              RefreshUserMappingsProtocol.class));\n+\n+    // Refresh the user-to-groups mappings\n+    refreshProtocol.refreshSuperUserGroupsConfiguration();\n+\n+    return 0;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public int refreshSuperUserGroupsConfiguration() throws IOException {\n    // Get the current configuration\n    Configuration conf \u003d getConf();\n\n    // for security authorization\n    // server principal for this call \n    // should be NAMENODE\u0027s one.\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, \n        conf.get(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\"));\n\n    // Create the client\n    RefreshUserMappingsProtocol refreshProtocol \u003d \n      (RefreshUserMappingsProtocol) \n      RPC.getProxy(RefreshUserMappingsProtocol.class, \n          RefreshUserMappingsProtocol.versionID, \n          NameNode.getAddress(conf), getUGI(), conf,\n          NetUtils.getSocketFactory(conf, \n              RefreshUserMappingsProtocol.class));\n\n    // Refresh the user-to-groups mappings\n    refreshProtocol.refreshSuperUserGroupsConfiguration();\n\n    return 0;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java"
    }
  }
}