{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FlowScanner.java",
  "functionName": "nextInternal",
  "functionId": "nextInternal___cells-List__Cell____scannerContext-ScannerContext",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-2/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
  "functionStartLine": 185,
  "functionEndLine": 245,
  "numCommitsSeen": 4,
  "timeTaken": 778,
  "changeHistory": [
    "55ba49dd071b66e72c47a1c41e88b9a5feddf53b"
  ],
  "changeHistoryShort": {
    "55ba49dd071b66e72c47a1c41e88b9a5feddf53b": "Yintroduced"
  },
  "changeHistoryDetails": {
    "55ba49dd071b66e72c47a1c41e88b9a5feddf53b": {
      "type": "Yintroduced",
      "commitMessage": "YARN-7346. Add a profile to allow optional compilation for ATSv2 with HBase-2.0. Contributed by Haibo Chen and Rohith.\n",
      "commitDate": "05/03/18 10:25 PM",
      "commitName": "55ba49dd071b66e72c47a1c41e88b9a5feddf53b",
      "commitAuthor": "Rohith Sharma K S",
      "diff": "@@ -0,0 +1,61 @@\n+  private boolean nextInternal(List\u003cCell\u003e cells, ScannerContext scannerContext)\n+      throws IOException {\n+    Cell cell \u003d null;\n+    startNext();\n+    // Loop through all the cells in this row\n+    // For min/max/metrics we do need to scan the entire set of cells to get the\n+    // right one\n+    // But with flush/compaction, the number of cells being scanned will go down\n+    // cells are grouped per column qualifier then sorted by cell timestamp\n+    // (latest to oldest) per column qualifier\n+    // So all cells in one qualifier come one after the other before we see the\n+    // next column qualifier\n+    ByteArrayComparator comp \u003d new ByteArrayComparator();\n+    byte[] previousColumnQualifier \u003d Separator.EMPTY_BYTES;\n+    AggregationOperation currentAggOp \u003d null;\n+    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n+    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n+    int addedCnt \u003d 0;\n+    long currentTimestamp \u003d System.currentTimeMillis();\n+    ValueConverter converter \u003d null;\n+    int limit \u003d batchSize;\n+\n+    while (limit \u003c\u003d 0 || addedCnt \u003c limit) {\n+      cell \u003d peekAtNextCell(scannerContext);\n+      if (cell \u003d\u003d null) {\n+        break;\n+      }\n+      byte[] currentColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n+      if (previousColumnQualifier \u003d\u003d null) {\n+        // first time in loop\n+        previousColumnQualifier \u003d currentColumnQualifier;\n+      }\n+\n+      converter \u003d getValueConverter(currentColumnQualifier);\n+      if (comp.compare(previousColumnQualifier, currentColumnQualifier) !\u003d 0) {\n+        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n+            converter, currentTimestamp);\n+        resetState(currentColumnCells, alreadySeenAggDim);\n+        previousColumnQualifier \u003d currentColumnQualifier;\n+        currentAggOp \u003d getCurrentAggOp(cell);\n+        converter \u003d getValueConverter(currentColumnQualifier);\n+      }\n+      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n+          converter, scannerContext);\n+      nextCell(scannerContext);\n+    }\n+    if ((!currentColumnCells.isEmpty()) \u0026\u0026 ((limit \u003c\u003d 0 || addedCnt \u003c limit))) {\n+      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp, converter,\n+          currentTimestamp);\n+      if (LOG.isDebugEnabled()) {\n+        if (addedCnt \u003e 0) {\n+          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n+              + \" rowKey\u003d\"\n+              + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));\n+        } else {\n+          LOG.debug(\"emitted no cells for \" + this.action);\n+        }\n+      }\n+    }\n+    return hasMore();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, ScannerContext scannerContext)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] previousColumnQualifier \u003d Separator.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    long currentTimestamp \u003d System.currentTimeMillis();\n    ValueConverter converter \u003d null;\n    int limit \u003d batchSize;\n\n    while (limit \u003c\u003d 0 || addedCnt \u003c limit) {\n      cell \u003d peekAtNextCell(scannerContext);\n      if (cell \u003d\u003d null) {\n        break;\n      }\n      byte[] currentColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (previousColumnQualifier \u003d\u003d null) {\n        // first time in loop\n        previousColumnQualifier \u003d currentColumnQualifier;\n      }\n\n      converter \u003d getValueConverter(currentColumnQualifier);\n      if (comp.compare(previousColumnQualifier, currentColumnQualifier) !\u003d 0) {\n        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n            converter, currentTimestamp);\n        resetState(currentColumnCells, alreadySeenAggDim);\n        previousColumnQualifier \u003d currentColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(currentColumnQualifier);\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          converter, scannerContext);\n      nextCell(scannerContext);\n    }\n    if ((!currentColumnCells.isEmpty()) \u0026\u0026 ((limit \u003c\u003d 0 || addedCnt \u003c limit))) {\n      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp, converter,\n          currentTimestamp);\n      if (LOG.isDebugEnabled()) {\n        if (addedCnt \u003e 0) {\n          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n              + \" rowKey\u003d\"\n              + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));\n        } else {\n          LOG.debug(\"emitted no cells for \" + this.action);\n        }\n      }\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-2/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java"
    }
  }
}