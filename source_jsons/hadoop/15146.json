{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSImageLoader.java",
  "functionName": "loadINodeSection",
  "functionId": "loadINodeSection___in-InputStream",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java",
  "functionStartLine": 234,
  "functionEndLine": 251,
  "numCommitsSeen": 22,
  "timeTaken": 1339,
  "changeHistory": [
    "1f5b42ac4881b734c799bfb527884c0d117929bd",
    "54c1daa580e1cceee541be86fc2b694fa1be26df"
  ],
  "changeHistoryShort": {
    "1f5b42ac4881b734c799bfb527884c0d117929bd": "Ymultichange(Yreturntypechange,Ybodychange)",
    "54c1daa580e1cceee541be86fc2b694fa1be26df": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1f5b42ac4881b734c799bfb527884c0d117929bd": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-7158. Reduce the memory usage of WebImageViewer. Contributed by Haohui Mai.\n",
      "commitDate": "01/10/14 10:53 AM",
      "commitName": "1f5b42ac4881b734c799bfb527884c0d117929bd",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-7158. Reduce the memory usage of WebImageViewer. Contributed by Haohui Mai.\n",
          "commitDate": "01/10/14 10:53 AM",
          "commitName": "1f5b42ac4881b734c799bfb527884c0d117929bd",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "29/09/14 10:27 PM",
          "commitNameOld": "bb84f1fccb18c6c7373851e05d2451d55e908242",
          "commitAuthorOld": "arp",
          "daysBetweenCommits": 1.52,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,18 @@\n-  private static void loadINodeSection(InputStream in) throws IOException {\n+  private static byte[][] loadINodeSection(InputStream in)\n+          throws IOException {\n     FsImageProto.INodeSection s \u003d FsImageProto.INodeSection\n         .parseDelimitedFrom(in);\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Found \" + s.getNumInodes() + \" inodes in inode section\");\n-    }\n+    LOG.info(\"Loading \" + s.getNumInodes() + \" inodes.\");\n+    final byte[][] inodes \u003d new byte[(int) s.getNumInodes()][];\n+\n     for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n-      FsImageProto.INodeSection.INode p \u003d FsImageProto.INodeSection.INode\n-          .parseDelimitedFrom(in);\n-      inodes.put(p.getId(), p);\n-      if (LOG.isTraceEnabled()) {\n-        LOG.trace(\"Loaded inode id \" + p.getId() + \" type \" + p.getType()\n-            + \" name \u0027\" + p.getName().toStringUtf8() + \"\u0027\");\n-      }\n+      int size \u003d CodedInputStream.readRawVarint32(in.read(), in);\n+      byte[] bytes \u003d new byte[size];\n+      IOUtils.readFully(in, bytes, 0, size);\n+      inodes[i] \u003d bytes;\n     }\n+    LOG.debug(\"Sorting inodes\");\n+    Arrays.sort(inodes, INODE_BYTES_COMPARATOR);\n+    LOG.debug(\"Finished sorting inodes\");\n+    return inodes;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static byte[][] loadINodeSection(InputStream in)\n          throws IOException {\n    FsImageProto.INodeSection s \u003d FsImageProto.INodeSection\n        .parseDelimitedFrom(in);\n    LOG.info(\"Loading \" + s.getNumInodes() + \" inodes.\");\n    final byte[][] inodes \u003d new byte[(int) s.getNumInodes()][];\n\n    for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n      int size \u003d CodedInputStream.readRawVarint32(in.read(), in);\n      byte[] bytes \u003d new byte[size];\n      IOUtils.readFully(in, bytes, 0, size);\n      inodes[i] \u003d bytes;\n    }\n    LOG.debug(\"Sorting inodes\");\n    Arrays.sort(inodes, INODE_BYTES_COMPARATOR);\n    LOG.debug(\"Finished sorting inodes\");\n    return inodes;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "byte[][]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7158. Reduce the memory usage of WebImageViewer. Contributed by Haohui Mai.\n",
          "commitDate": "01/10/14 10:53 AM",
          "commitName": "1f5b42ac4881b734c799bfb527884c0d117929bd",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "29/09/14 10:27 PM",
          "commitNameOld": "bb84f1fccb18c6c7373851e05d2451d55e908242",
          "commitAuthorOld": "arp",
          "daysBetweenCommits": 1.52,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,18 @@\n-  private static void loadINodeSection(InputStream in) throws IOException {\n+  private static byte[][] loadINodeSection(InputStream in)\n+          throws IOException {\n     FsImageProto.INodeSection s \u003d FsImageProto.INodeSection\n         .parseDelimitedFrom(in);\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Found \" + s.getNumInodes() + \" inodes in inode section\");\n-    }\n+    LOG.info(\"Loading \" + s.getNumInodes() + \" inodes.\");\n+    final byte[][] inodes \u003d new byte[(int) s.getNumInodes()][];\n+\n     for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n-      FsImageProto.INodeSection.INode p \u003d FsImageProto.INodeSection.INode\n-          .parseDelimitedFrom(in);\n-      inodes.put(p.getId(), p);\n-      if (LOG.isTraceEnabled()) {\n-        LOG.trace(\"Loaded inode id \" + p.getId() + \" type \" + p.getType()\n-            + \" name \u0027\" + p.getName().toStringUtf8() + \"\u0027\");\n-      }\n+      int size \u003d CodedInputStream.readRawVarint32(in.read(), in);\n+      byte[] bytes \u003d new byte[size];\n+      IOUtils.readFully(in, bytes, 0, size);\n+      inodes[i] \u003d bytes;\n     }\n+    LOG.debug(\"Sorting inodes\");\n+    Arrays.sort(inodes, INODE_BYTES_COMPARATOR);\n+    LOG.debug(\"Finished sorting inodes\");\n+    return inodes;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static byte[][] loadINodeSection(InputStream in)\n          throws IOException {\n    FsImageProto.INodeSection s \u003d FsImageProto.INodeSection\n        .parseDelimitedFrom(in);\n    LOG.info(\"Loading \" + s.getNumInodes() + \" inodes.\");\n    final byte[][] inodes \u003d new byte[(int) s.getNumInodes()][];\n\n    for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n      int size \u003d CodedInputStream.readRawVarint32(in.read(), in);\n      byte[] bytes \u003d new byte[size];\n      IOUtils.readFully(in, bytes, 0, size);\n      inodes[i] \u003d bytes;\n    }\n    LOG.debug(\"Sorting inodes\");\n    Arrays.sort(inodes, INODE_BYTES_COMPARATOR);\n    LOG.debug(\"Finished sorting inodes\");\n    return inodes;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java",
          "extendedDetails": {}
        }
      ]
    },
    "54c1daa580e1cceee541be86fc2b694fa1be26df": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5978. Create a tool to take fsimage and expose read-only WebHDFS API. Contributed by Akira Ajisaka.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582433 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/03/14 11:15 AM",
      "commitName": "54c1daa580e1cceee541be86fc2b694fa1be26df",
      "commitAuthor": "Haohui Mai",
      "diff": "@@ -0,0 +1,16 @@\n+  private static void loadINodeSection(InputStream in) throws IOException {\n+    FsImageProto.INodeSection s \u003d FsImageProto.INodeSection\n+        .parseDelimitedFrom(in);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Found \" + s.getNumInodes() + \" inodes in inode section\");\n+    }\n+    for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n+      FsImageProto.INodeSection.INode p \u003d FsImageProto.INodeSection.INode\n+          .parseDelimitedFrom(in);\n+      inodes.put(p.getId(), p);\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(\"Loaded inode id \" + p.getId() + \" type \" + p.getType()\n+            + \" name \u0027\" + p.getName().toStringUtf8() + \"\u0027\");\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static void loadINodeSection(InputStream in) throws IOException {\n    FsImageProto.INodeSection s \u003d FsImageProto.INodeSection\n        .parseDelimitedFrom(in);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Found \" + s.getNumInodes() + \" inodes in inode section\");\n    }\n    for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n      FsImageProto.INodeSection.INode p \u003d FsImageProto.INodeSection.INode\n          .parseDelimitedFrom(in);\n      inodes.put(p.getId(), p);\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"Loaded inode id \" + p.getId() + \" type \" + p.getType()\n            + \" name \u0027\" + p.getName().toStringUtf8() + \"\u0027\");\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java"
    }
  }
}