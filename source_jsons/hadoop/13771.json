{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "dumpBlockMeta",
  "functionId": "dumpBlockMeta___block-Block__out-PrintWriter",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 896,
  "functionEndLine": 955,
  "numCommitsSeen": 912,
  "timeTaken": 20630,
  "changeHistory": [
    "02009c3bb762393540cdf92cfd9c840807272903",
    "1bea785020a538115b3e08f41ff88167033d2775",
    "4ca3a6b21a3a25acf16d026c699154047b1f686b",
    "0f2a69127aa40b7dccdf74ff63e2205a10ae8998",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "70d6f201260086a3f12beaa317fede2a99639fef",
    "4cbbfa2220e884e91bf18ad1cc2f3b11f895f8c9",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75",
    "d62b63d297bff12d93de560dd50ddd48743b851d",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "2f341414dd4a052bee3907ff4a6db283a15f9d53",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9",
    "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15",
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
    "8854c210bdf8aba763b2a0f0327729f315a066d5",
    "2c15726999c45a53bf8ae1ce6b9d6fdcc5adfc67",
    "7e8e983620f3ae3462d115972707c72b7d9cbabd",
    "f0f9a3631fe4950f5cf548f192226836925d0f05"
  ],
  "changeHistoryShort": {
    "02009c3bb762393540cdf92cfd9c840807272903": "Ybodychange",
    "1bea785020a538115b3e08f41ff88167033d2775": "Ybodychange",
    "4ca3a6b21a3a25acf16d026c699154047b1f686b": "Ybodychange",
    "0f2a69127aa40b7dccdf74ff63e2205a10ae8998": "Ybodychange",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ybodychange",
    "70d6f201260086a3f12beaa317fede2a99639fef": "Ybodychange",
    "4cbbfa2220e884e91bf18ad1cc2f3b11f895f8c9": "Ybodychange",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": "Ybodychange",
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75": "Ymultichange(Yparameterchange,Ybodychange)",
    "d62b63d297bff12d93de560dd50ddd48743b851d": "Ymultichange(Yparameterchange,Ybodychange)",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b": "Ybodychange",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "2f341414dd4a052bee3907ff4a6db283a15f9d53": "Ybodychange",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": "Ybodychange",
    "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15": "Ybodychange",
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a": "Ybodychange",
    "8854c210bdf8aba763b2a0f0327729f315a066d5": "Ybodychange",
    "2c15726999c45a53bf8ae1ce6b9d6fdcc5adfc67": "Ybodychange",
    "7e8e983620f3ae3462d115972707c72b7d9cbabd": "Ybodychange",
    "f0f9a3631fe4950f5cf548f192226836925d0f05": "Ybodychange"
  },
  "changeHistoryDetails": {
    "02009c3bb762393540cdf92cfd9c840807272903": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14768. EC : Busy DN replica should be consider in live replica check. Contributed by guojh.\n",
      "commitDate": "01/11/19 9:45 AM",
      "commitName": "02009c3bb762393540cdf92cfd9c840807272903",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "31/10/19 11:19 AM",
      "commitNameOld": "9d25ae7669eed1a047578b574f42bd121b445a3c",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 0.93,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     BlockInfo blockInfo \u003d getStoredBlock(block);\n     if (blockInfo \u003d\u003d null) {\n       out.println(\"Block \"+ block + \" is Null\");\n       return;\n     }\n     // source node returned is not used\n     chooseSourceDatanodes(blockInfo, containingNodes,\n-        containingLiveReplicasNodes, numReplicas,\n+        containingLiveReplicasNodes, numReplicas, new ArrayList\u003cByte\u003e(),\n         new ArrayList\u003cByte\u003e(), LowRedundancyBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n \n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") +\n               \" (replicas:\" +\n               \" live: \" + numReplicas.liveReplicas() +\n               \" decommissioning and decommissioned: \" +\n         numReplicas.decommissionedAndDecommissioning() +\n               \" corrupt: \" + numReplicas.corruptReplicas() +\n               \" in excess: \" + numReplicas.excessReplicas() +\n               \" maintenance mode: \" + numReplicas.maintenanceReplicas() + \") \");\n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       } else if (node.isMaintenance() || node.isInMaintenance()){\n         state \u003d \"(maintenance)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    BlockInfo blockInfo \u003d getStoredBlock(block);\n    if (blockInfo \u003d\u003d null) {\n      out.println(\"Block \"+ block + \" is Null\");\n      return;\n    }\n    // source node returned is not used\n    chooseSourceDatanodes(blockInfo, containingNodes,\n        containingLiveReplicasNodes, numReplicas, new ArrayList\u003cByte\u003e(),\n        new ArrayList\u003cByte\u003e(), LowRedundancyBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n\n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") +\n              \" (replicas:\" +\n              \" live: \" + numReplicas.liveReplicas() +\n              \" decommissioning and decommissioned: \" +\n        numReplicas.decommissionedAndDecommissioning() +\n              \" corrupt: \" + numReplicas.corruptReplicas() +\n              \" in excess: \" + numReplicas.excessReplicas() +\n              \" maintenance mode: \" + numReplicas.maintenanceReplicas() + \") \");\n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      } else if (node.isMaintenance() || node.isInMaintenance()){\n        state \u003d \"(maintenance)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "1bea785020a538115b3e08f41ff88167033d2775": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14081. hdfs dfsadmin -metasave metasave_test results NPE. Contributed by Shweta Yakkali.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "20/02/19 2:28 PM",
      "commitName": "1bea785020a538115b3e08f41ff88167033d2775",
      "commitAuthor": "Shweta Yakkali",
      "commitDateOld": "11/01/19 10:54 AM",
      "commitNameOld": "fb8932a727f757b2e9c1c61a18145878d0eb77bd",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 40.15,
      "commitsBetweenForRepo": 297,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,60 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n+    BlockInfo blockInfo \u003d getStoredBlock(block);\n+    if (blockInfo \u003d\u003d null) {\n+      out.println(\"Block \"+ block + \" is Null\");\n+      return;\n+    }\n     // source node returned is not used\n-    chooseSourceDatanodes(getStoredBlock(block), containingNodes,\n+    chooseSourceDatanodes(blockInfo, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         new ArrayList\u003cByte\u003e(), LowRedundancyBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n-    \n+\n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") +\n               \" (replicas:\" +\n               \" live: \" + numReplicas.liveReplicas() +\n               \" decommissioning and decommissioned: \" +\n         numReplicas.decommissionedAndDecommissioning() +\n               \" corrupt: \" + numReplicas.corruptReplicas() +\n               \" in excess: \" + numReplicas.excessReplicas() +\n               \" maintenance mode: \" + numReplicas.maintenanceReplicas() + \") \");\n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       } else if (node.isMaintenance() || node.isInMaintenance()){\n         state \u003d \"(maintenance)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    BlockInfo blockInfo \u003d getStoredBlock(block);\n    if (blockInfo \u003d\u003d null) {\n      out.println(\"Block \"+ block + \" is Null\");\n      return;\n    }\n    // source node returned is not used\n    chooseSourceDatanodes(blockInfo, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        new ArrayList\u003cByte\u003e(), LowRedundancyBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n\n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") +\n              \" (replicas:\" +\n              \" live: \" + numReplicas.liveReplicas() +\n              \" decommissioning and decommissioned: \" +\n        numReplicas.decommissionedAndDecommissioning() +\n              \" corrupt: \" + numReplicas.corruptReplicas() +\n              \" in excess: \" + numReplicas.excessReplicas() +\n              \" maintenance mode: \" + numReplicas.maintenanceReplicas() + \") \");\n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      } else if (node.isMaintenance() || node.isInMaintenance()){\n        state \u003d \"(maintenance)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4ca3a6b21a3a25acf16d026c699154047b1f686b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14108. Performance improvement in BlockManager Data Structures. Contributed by Beluga Behr.\n",
      "commitDate": "28/11/18 11:25 AM",
      "commitName": "4ca3a6b21a3a25acf16d026c699154047b1f686b",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "05/11/18 9:38 PM",
      "commitNameOld": "ffc9c50e074aeca804674c6e1e6b0f1eb629e230",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 22.57,
      "commitsBetweenForRepo": 159,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanodes(getStoredBlock(block), containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n-        new LinkedList\u003cByte\u003e(), LowRedundancyBlocks.LEVEL);\n+        new ArrayList\u003cByte\u003e(), LowRedundancyBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n     \n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") +\n               \" (replicas:\" +\n               \" live: \" + numReplicas.liveReplicas() +\n               \" decommissioning and decommissioned: \" +\n         numReplicas.decommissionedAndDecommissioning() +\n               \" corrupt: \" + numReplicas.corruptReplicas() +\n               \" in excess: \" + numReplicas.excessReplicas() +\n               \" maintenance mode: \" + numReplicas.maintenanceReplicas() + \") \");\n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       } else if (node.isMaintenance() || node.isInMaintenance()){\n         state \u003d \"(maintenance)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanodes(getStoredBlock(block), containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        new ArrayList\u003cByte\u003e(), LowRedundancyBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") +\n              \" (replicas:\" +\n              \" live: \" + numReplicas.liveReplicas() +\n              \" decommissioning and decommissioned: \" +\n        numReplicas.decommissionedAndDecommissioning() +\n              \" corrupt: \" + numReplicas.corruptReplicas() +\n              \" in excess: \" + numReplicas.excessReplicas() +\n              \" maintenance mode: \" + numReplicas.maintenanceReplicas() + \") \");\n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      } else if (node.isMaintenance() || node.isInMaintenance()){\n        state \u003d \"(maintenance)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "0f2a69127aa40b7dccdf74ff63e2205a10ae8998": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12249. dfsadmin -metaSave to output maintenance mode blocks. Contributed by Wellington Chevreuil.\n",
      "commitDate": "23/10/17 3:24 PM",
      "commitName": "0f2a69127aa40b7dccdf74ff63e2205a10ae8998",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "19/10/17 6:17 AM",
      "commitNameOld": "4ab0c8f96a41c573cc1f1e71c18871d243f952b9",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 4.38,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,55 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanodes(getStoredBlock(block), containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         new LinkedList\u003cByte\u003e(), LowRedundancyBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n     \n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") +\n               \" (replicas:\" +\n-              \" l: \" + numReplicas.liveReplicas() +\n-              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n-              \" c: \" + numReplicas.corruptReplicas() +\n-              \" e: \" + numReplicas.excessReplicas() + \") \");\n+              \" live: \" + numReplicas.liveReplicas() +\n+              \" decommissioning and decommissioned: \" +\n+        numReplicas.decommissionedAndDecommissioning() +\n+              \" corrupt: \" + numReplicas.corruptReplicas() +\n+              \" in excess: \" + numReplicas.excessReplicas() +\n+              \" maintenance mode: \" + numReplicas.maintenanceReplicas() + \") \");\n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n+      } else if (node.isMaintenance() || node.isInMaintenance()){\n+        state \u003d \"(maintenance)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanodes(getStoredBlock(block), containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        new LinkedList\u003cByte\u003e(), LowRedundancyBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") +\n              \" (replicas:\" +\n              \" live: \" + numReplicas.liveReplicas() +\n              \" decommissioning and decommissioned: \" +\n        numReplicas.decommissionedAndDecommissioning() +\n              \" corrupt: \" + numReplicas.corruptReplicas() +\n              \" in excess: \" + numReplicas.excessReplicas() +\n              \" maintenance mode: \" + numReplicas.maintenanceReplicas() + \") \");\n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      } else if (node.isMaintenance() || node.isInMaintenance()){\n        state \u003d \"(maintenance)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanodes(getStoredBlock(block), containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n-        new LinkedList\u003cByte\u003e(), UnderReplicatedBlocks.LEVEL);\n+        new LinkedList\u003cByte\u003e(), LowRedundancyBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n     \n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanodes(getStoredBlock(block), containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        new LinkedList\u003cByte\u003e(), LowRedundancyBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "70d6f201260086a3f12beaa317fede2a99639fef": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9575. Use byte array for internal block indices in a striped block.  Contributed by jing9\n",
      "commitDate": "21/12/15 10:47 PM",
      "commitName": "70d6f201260086a3f12beaa317fede2a99639fef",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "16/12/15 6:16 PM",
      "commitNameOld": "f741476146574550a1a208d58ef8be76639e5ddc",
      "commitAuthorOld": "Uma Mahesh",
      "daysBetweenCommits": 5.19,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanodes(getStoredBlock(block), containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n-        new LinkedList\u003cShort\u003e(), UnderReplicatedBlocks.LEVEL);\n+        new LinkedList\u003cByte\u003e(), UnderReplicatedBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n     \n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanodes(getStoredBlock(block), containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        new LinkedList\u003cByte\u003e(), UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4cbbfa2220e884e91bf18ad1cc2f3b11f895f8c9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8248. Store INodeId instead of the INodeFile object in BlockInfoContiguous. Contributed by Haohui Mai.\n",
      "commitDate": "26/08/15 6:14 PM",
      "commitName": "4cbbfa2220e884e91bf18ad1cc2f3b11f895f8c9",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/08/15 11:31 AM",
      "commitNameOld": "b5ce87f84d9de0a5347ab38c0567a5a70d1fbfd7",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 2.28,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n     \n     if (block instanceof BlockInfo) {\n-      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n+      BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d getBlockCollection((BlockInfo)block);\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\"\n\nThis reverts commit de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "31/07/15 4:15 PM",
      "commitNameOld": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 5.75,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n-      new ArrayList\u003c\u003e();\n-\n+      new ArrayList\u003cDatanodeStorageInfo\u003e();\n+    \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n     \n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n-    for (DatanodeStorageInfo storage : getStorages(block)) {\n+    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "Revert \"HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\"\n\nThis reverts commit d62b63d297bff12d93de560dd50ddd48743b851d.\n",
      "commitDate": "07/07/15 10:13 AM",
      "commitName": "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "Revert \"HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\"\n\nThis reverts commit d62b63d297bff12d93de560dd50ddd48743b851d.\n",
          "commitDate": "07/07/15 10:13 AM",
          "commitName": "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "06/07/15 3:54 PM",
          "commitNameOld": "d62b63d297bff12d93de560dd50ddd48743b851d",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.76,
          "commitsBetweenForRepo": 8,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,51 @@\n-  private void dumpBlockMeta(BlockInfo block, PrintWriter out) {\n-    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n-    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d new ArrayList\u003c\u003e();\n+  private void dumpBlockMeta(Block block, PrintWriter out) {\n+    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n+                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n+    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n+      new ArrayList\u003c\u003e();\n \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     \n-    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which\n-    // are not included in the numReplicas.liveReplicas() count\n+    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n+    // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n-\n-    BlockCollection bc \u003d block.getBlockCollection();\n-    String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n-    out.print(fileName + \": \");\n-\n+    \n+    if (block instanceof BlockInfo) {\n+      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n+      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n+      out.print(fileName + \": \");\n+    }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n-    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n-        corruptReplicas.getNodes(block);\n+    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n+                                  corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003c\u003e();\n\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[block-BlockInfo, out-PrintWriter]",
            "newValue": "[block-Block, out-PrintWriter]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "Revert \"HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\"\n\nThis reverts commit d62b63d297bff12d93de560dd50ddd48743b851d.\n",
          "commitDate": "07/07/15 10:13 AM",
          "commitName": "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "06/07/15 3:54 PM",
          "commitNameOld": "d62b63d297bff12d93de560dd50ddd48743b851d",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.76,
          "commitsBetweenForRepo": 8,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,51 @@\n-  private void dumpBlockMeta(BlockInfo block, PrintWriter out) {\n-    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n-    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d new ArrayList\u003c\u003e();\n+  private void dumpBlockMeta(Block block, PrintWriter out) {\n+    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n+                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n+    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n+      new ArrayList\u003c\u003e();\n \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     \n-    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which\n-    // are not included in the numReplicas.liveReplicas() count\n+    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n+    // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n-\n-    BlockCollection bc \u003d block.getBlockCollection();\n-    String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n-    out.print(fileName + \": \");\n-\n+    \n+    if (block instanceof BlockInfo) {\n+      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n+      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n+      out.print(fileName + \": \");\n+    }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n-    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n-        corruptReplicas.getNodes(block);\n+    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n+                                  corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003c\u003e();\n\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "d62b63d297bff12d93de560dd50ddd48743b851d": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\n",
      "commitDate": "06/07/15 3:54 PM",
      "commitName": "d62b63d297bff12d93de560dd50ddd48743b851d",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\n",
          "commitDate": "06/07/15 3:54 PM",
          "commitName": "d62b63d297bff12d93de560dd50ddd48743b851d",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "29/06/15 11:00 AM",
          "commitNameOld": "d3fed8e653ed9e18d3a29a11c4b24a628ac770bb",
          "commitAuthorOld": "Benoy Antony",
          "daysBetweenCommits": 7.2,
          "commitsBetweenForRepo": 50,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,48 @@\n-  private void dumpBlockMeta(Block block, PrintWriter out) {\n-    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n-                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n-    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n-      new ArrayList\u003c\u003e();\n+  private void dumpBlockMeta(BlockInfo block, PrintWriter out) {\n+    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n+    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d new ArrayList\u003c\u003e();\n \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     \n-    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n-    // not included in the numReplicas.liveReplicas() count\n+    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which\n+    // are not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n-    \n-    if (block instanceof BlockInfo) {\n-      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n-      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n-      out.print(fileName + \": \");\n-    }\n+\n+    BlockCollection bc \u003d block.getBlockCollection();\n+    String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n+    out.print(fileName + \": \");\n+\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n-    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n-                                  corruptReplicas.getNodes(block);\n+    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n+        corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void dumpBlockMeta(BlockInfo block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d new ArrayList\u003c\u003e();\n\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which\n    // are not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n\n    BlockCollection bc \u003d block.getBlockCollection();\n    String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n    out.print(fileName + \": \");\n\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n        corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[block-Block, out-PrintWriter]",
            "newValue": "[block-BlockInfo, out-PrintWriter]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\n",
          "commitDate": "06/07/15 3:54 PM",
          "commitName": "d62b63d297bff12d93de560dd50ddd48743b851d",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "29/06/15 11:00 AM",
          "commitNameOld": "d3fed8e653ed9e18d3a29a11c4b24a628ac770bb",
          "commitAuthorOld": "Benoy Antony",
          "daysBetweenCommits": 7.2,
          "commitsBetweenForRepo": 50,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,48 @@\n-  private void dumpBlockMeta(Block block, PrintWriter out) {\n-    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n-                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n-    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n-      new ArrayList\u003c\u003e();\n+  private void dumpBlockMeta(BlockInfo block, PrintWriter out) {\n+    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n+    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d new ArrayList\u003c\u003e();\n \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     \n-    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n-    // not included in the numReplicas.liveReplicas() count\n+    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which\n+    // are not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n-    \n-    if (block instanceof BlockInfo) {\n-      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n-      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n-      out.print(fileName + \": \");\n-    }\n+\n+    BlockCollection bc \u003d block.getBlockCollection();\n+    String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n+    out.print(fileName + \": \");\n+\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n-    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n-                                  corruptReplicas.getNodes(block);\n+    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n+        corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void dumpBlockMeta(BlockInfo block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d new ArrayList\u003c\u003e();\n\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which\n    // are not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n\n    BlockCollection bc \u003d block.getBlockCollection();\n    String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n    out.print(fileName + \": \");\n\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d\n        corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\n",
      "commitDate": "26/06/15 10:49 AM",
      "commitName": "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/06/15 2:42 PM",
      "commitNameOld": "afe9ea3c12e1f5a71922400eadb642960bc87ca1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n-      new ArrayList\u003cDatanodeStorageInfo\u003e();\n-    \n+      new ArrayList\u003c\u003e();\n+\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n     \n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n-    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n+    for (DatanodeStorageInfo storage : getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003c\u003e();\n\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "19/05/15 11:05 AM",
      "commitNameOld": "8860e352c394372e4eb3ebdf82ea899567f34e4e",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 8.19,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n     \n-    if (block instanceof BlockInfoContiguous) {\n-      BlockCollection bc \u003d ((BlockInfoContiguous) block).getBlockCollection();\n+    if (block instanceof BlockInfo) {\n+      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7933. fsck should also report decommissioning replicas. Contributed by Xiaoyu Yao.\n",
      "commitDate": "11/04/15 1:23 PM",
      "commitName": "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
      "commitAuthor": "cnauroth",
      "commitDateOld": "10/04/15 4:36 PM",
      "commitNameOld": "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.87,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n-                         numReplicas.decommissionedReplicas();\n+                         numReplicas.decommissionedAndDecommissioning();\n     \n     if (block instanceof BlockInfoContiguous) {\n       BlockCollection bc \u003d ((BlockInfoContiguous) block).getBlockCollection();\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n-              \" d: \" + numReplicas.decommissionedReplicas() +\n+              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedAndDecommissioning();\n    \n    if (block instanceof BlockInfoContiguous) {\n      BlockCollection bc \u003d ((BlockInfoContiguous) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedAndDecommissioning() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "04/02/15 11:31 AM",
      "commitNameOld": "9175105eeaecf0a1d60b57989b73ce45cee4689b",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.01,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     \n     // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n     // not included in the numReplicas.liveReplicas() count\n     assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedReplicas();\n     \n-    if (block instanceof BlockInfo) {\n-      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n+    if (block instanceof BlockInfoContiguous) {\n+      BlockCollection bc \u003d ((BlockInfoContiguous) block).getBlockCollection();\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedReplicas() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedReplicas();\n    \n    if (block instanceof BlockInfoContiguous) {\n      BlockCollection bc \u003d ((BlockInfoContiguous) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedReplicas() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "2f341414dd4a052bee3907ff4a6db283a15f9d53": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5318. Support read-only and read-write paths to shared replicas. (Contributed by Eric Sirianni)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1569951 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/02/14 2:59 PM",
      "commitName": "2f341414dd4a052bee3907ff4a6db283a15f9d53",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "31/01/14 1:00 PM",
      "commitNameOld": "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 19.08,
      "commitsBetweenForRepo": 153,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,51 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n-    assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n+    \n+    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n+    // not included in the numReplicas.liveReplicas() count\n+    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedReplicas() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    \n    // containingLiveReplicasNodes can include READ_ONLY_SHARED replicas which are \n    // not included in the numReplicas.liveReplicas() count\n    assert containingLiveReplicasNodes.size() \u003e\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedReplicas();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedReplicas() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4990. Change BlockPlacementPolicy to choose storages instead of datanodes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1524444 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/13 8:12 AM",
      "commitName": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "16/09/13 7:59 PM",
      "commitNameOld": "0398943572e052bef2c21466871b833950625d82",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.51,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,48 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n-    List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n-      new ArrayList\u003cDatanodeDescriptor\u003e();\n+    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n+      new ArrayList\u003cDatanodeStorageInfo\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedReplicas() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeStorageInfo\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeStorageInfo\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedReplicas();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedReplicas() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5134. Move blockContentsStale, heartbeatedSinceFailover and firstBlockReport from DatanodeDescriptor to DatanodeStorageInfo; and fix a synchronization problem in DatanodeStorageInfo.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1520938 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/09/13 3:53 PM",
      "commitName": "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "03/09/13 7:03 AM",
      "commitNameOld": "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.37,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,48 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeDescriptor\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedReplicas() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n-      if (node.areBlockContentsStale()) {\n+      if (storage.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeDescriptor\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedReplicas();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedReplicas() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (storage.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5009. Include storage information in the LocatedBlock.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1519691 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/09/13 7:03 AM",
      "commitName": "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "27/08/13 11:30 PM",
      "commitNameOld": "5d9d702607913685eab0d8ad077040ddc82bf085",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.31,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,48 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeDescriptor\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n       BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n       String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedReplicas() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n-    for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n-         jt.hasNext();) {\n-      DatanodeDescriptor node \u003d jt.next();\n+    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n+      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (node.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeDescriptor\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedReplicas();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedReplicas() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (node.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8854c210bdf8aba763b2a0f0327729f315a066d5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4867. metaSave NPEs when there are invalid blocks in repl queue. Contributed by Plamen Jeliazkov and Ravi Prakash.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1490433 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/06/13 1:44 PM",
      "commitName": "8854c210bdf8aba763b2a0f0327729f315a066d5",
      "commitAuthor": "Konstantin Shvachko",
      "commitDateOld": "04/06/13 2:43 PM",
      "commitNameOld": "c4382e7565447277e716c22dd20053113e0732cb",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 1.96,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,49 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeDescriptor\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         UnderReplicatedBlocks.LEVEL);\n     assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n-      String fileName \u003d ((BlockInfo)block).getBlockCollection().getName();\n+      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n+      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedReplicas() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n          jt.hasNext();) {\n       DatanodeDescriptor node \u003d jt.next();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (node.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeDescriptor\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedReplicas();\n    \n    if (block instanceof BlockInfo) {\n      BlockCollection bc \u003d ((BlockInfo) block).getBlockCollection();\n      String fileName \u003d (bc \u003d\u003d null) ? \"[orphaned]\" : bc.getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedReplicas() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n         jt.hasNext();) {\n      DatanodeDescriptor node \u003d jt.next();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (node.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "2c15726999c45a53bf8ae1ce6b9d6fdcc5adfc67": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4270. Introduce soft and hard limits for max replication so that replications of the highest priority are allowed to choose a source datanode that has reached its soft limit but not the hard limit.  Contributed by Derek Dagit\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1428739 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/01/13 12:09 AM",
      "commitName": "2c15726999c45a53bf8ae1ce6b9d6fdcc5adfc67",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "09/11/12 10:07 AM",
      "commitNameOld": "db71de2e11cfa56a254ef4c92fea5ef4f8c19100",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 55.59,
      "commitsBetweenForRepo": 211,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,48 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeDescriptor\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n-        containingLiveReplicasNodes, numReplicas);\n+        containingLiveReplicasNodes, numReplicas,\n+        UnderReplicatedBlocks.LEVEL);\n     assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n       String fileName \u003d ((BlockInfo)block).getBlockCollection().getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedReplicas() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n          jt.hasNext();) {\n       DatanodeDescriptor node \u003d jt.next();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (node.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeDescriptor\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas,\n        UnderReplicatedBlocks.LEVEL);\n    assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedReplicas();\n    \n    if (block instanceof BlockInfo) {\n      String fileName \u003d ((BlockInfo)block).getBlockCollection().getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedReplicas() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n         jt.hasNext();) {\n      DatanodeDescriptor node \u003d jt.next();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (node.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "7e8e983620f3ae3462d115972707c72b7d9cbabd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3369. Rename {get|set|add}INode(..) methods in BlockManager and BlocksMap to {get|set|add}BlockCollection(..).  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1336909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/05/12 2:41 PM",
      "commitName": "7e8e983620f3ae3462d115972707c72b7d9cbabd",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "10/05/12 2:59 AM",
      "commitNameOld": "f1ff05bf47a7dfb670bc63e4e6e58d74f6b5b4a7",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.49,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,47 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeDescriptor\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas);\n     assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n-      String fileName \u003d ((BlockInfo)block).getINode().getName();\n+      String fileName \u003d ((BlockInfo)block).getBlockCollection().getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedReplicas() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n          jt.hasNext();) {\n       DatanodeDescriptor node \u003d jt.next();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (node.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeDescriptor\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas);\n    assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedReplicas();\n    \n    if (block instanceof BlockInfo) {\n      String fileName \u003d ((BlockInfo)block).getBlockCollection().getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedReplicas() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n         jt.hasNext();) {\n      DatanodeDescriptor node \u003d jt.next();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (node.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "f0f9a3631fe4950f5cf548f192226836925d0f05": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3363. Define BlockCollection and MutableBlockCollection interfaces so that INodeFile and INodeFileUnderConstruction do not have to be used in block management.  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1335304 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/05/12 5:06 PM",
      "commitName": "f0f9a3631fe4950f5cf548f192226836925d0f05",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/05/12 4:02 PM",
      "commitNameOld": "8620a99d1eea163b7505cde0a57e849b1b2a3a6f",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.04,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,47 @@\n   private void dumpBlockMeta(Block block, PrintWriter out) {\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                       new ArrayList\u003cDatanodeDescriptor\u003e();\n     List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n       new ArrayList\u003cDatanodeDescriptor\u003e();\n     \n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     // source node returned is not used\n     chooseSourceDatanode(block, containingNodes,\n         containingLiveReplicasNodes, numReplicas);\n     assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n     int usableReplicas \u003d numReplicas.liveReplicas() +\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n-      String fileName \u003d ((BlockInfo)block).getINode().getFullPathName();\n+      String fileName \u003d ((BlockInfo)block).getINode().getName();\n       out.print(fileName + \": \");\n     }\n     // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n     out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n               \" (replicas:\" +\n               \" l: \" + numReplicas.liveReplicas() +\n               \" d: \" + numReplicas.decommissionedReplicas() +\n               \" c: \" + numReplicas.corruptReplicas() +\n               \" e: \" + numReplicas.excessReplicas() + \") \"); \n \n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                   corruptReplicas.getNodes(block);\n     \n     for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n          jt.hasNext();) {\n       DatanodeDescriptor node \u003d jt.next();\n       String state \u003d \"\";\n       if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n         state \u003d \"(corrupt)\";\n       } else if (node.isDecommissioned() || \n           node.isDecommissionInProgress()) {\n         state \u003d \"(decommissioned)\";\n       }\n       \n       if (node.areBlockContentsStale()) {\n         state +\u003d \" (block deletions maybe out of date)\";\n       }\n       out.print(\" \" + node + state + \" : \");\n     }\n     out.println(\"\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpBlockMeta(Block block, PrintWriter out) {\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d\n                                      new ArrayList\u003cDatanodeDescriptor\u003e();\n    List\u003cDatanodeDescriptor\u003e containingLiveReplicasNodes \u003d\n      new ArrayList\u003cDatanodeDescriptor\u003e();\n    \n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    // source node returned is not used\n    chooseSourceDatanode(block, containingNodes,\n        containingLiveReplicasNodes, numReplicas);\n    assert containingLiveReplicasNodes.size() \u003d\u003d numReplicas.liveReplicas();\n    int usableReplicas \u003d numReplicas.liveReplicas() +\n                         numReplicas.decommissionedReplicas();\n    \n    if (block instanceof BlockInfo) {\n      String fileName \u003d ((BlockInfo)block).getINode().getName();\n      out.print(fileName + \": \");\n    }\n    // l: \u003d\u003d live:, d: \u003d\u003d decommissioned c: \u003d\u003d corrupt e: \u003d\u003d excess\n    out.print(block + ((usableReplicas \u003e 0)? \"\" : \" MISSING\") + \n              \" (replicas:\" +\n              \" l: \" + numReplicas.liveReplicas() +\n              \" d: \" + numReplicas.decommissionedReplicas() +\n              \" c: \" + numReplicas.corruptReplicas() +\n              \" e: \" + numReplicas.excessReplicas() + \") \"); \n\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d \n                                  corruptReplicas.getNodes(block);\n    \n    for (Iterator\u003cDatanodeDescriptor\u003e jt \u003d blocksMap.nodeIterator(block);\n         jt.hasNext();) {\n      DatanodeDescriptor node \u003d jt.next();\n      String state \u003d \"\";\n      if (corruptNodes !\u003d null \u0026\u0026 corruptNodes.contains(node)) {\n        state \u003d \"(corrupt)\";\n      } else if (node.isDecommissioned() || \n          node.isDecommissionInProgress()) {\n        state \u003d \"(decommissioned)\";\n      }\n      \n      if (node.areBlockContentsStale()) {\n        state +\u003d \" (block deletions maybe out of date)\";\n      }\n      out.print(\" \" + node + state + \" : \");\n    }\n    out.println(\"\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    }
  }
}