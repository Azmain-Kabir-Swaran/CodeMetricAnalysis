{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LoadJob.java",
  "functionName": "buildSplits",
  "functionId": "buildSplits___inputDir-FilePool",
  "sourceFilePath": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
  "functionStartLine": 600,
  "functionEndLine": 662,
  "numCommitsSeen": 11,
  "timeTaken": 4787,
  "changeHistory": [
    "dcf84707ab50662add112bd6b01c0bfd63374853",
    "231e39462dbfe60f66710e0425dbf16069382dbe",
    "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "dcf84707ab50662add112bd6b01c0bfd63374853": "Yfilerename",
    "231e39462dbfe60f66710e0425dbf16069382dbe": "Ybodychange",
    "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "dcf84707ab50662add112bd6b01c0bfd63374853": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3543. Mavenize Gridmix. (tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1339629 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/05/12 8:06 AM",
      "commitName": "dcf84707ab50662add112bd6b01c0bfd63374853",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "17/05/12 7:20 AM",
      "commitNameOld": "e1f09365ca0bee093f849fcf2e546dd6e2c0a965",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void buildSplits(FilePool inputDir) throws IOException {\n    long mapInputBytesTotal \u003d 0L;\n    long mapOutputBytesTotal \u003d 0L;\n    long mapOutputRecordsTotal \u003d 0L;\n    final JobStory jobdesc \u003d getJobDesc();\n    if (null \u003d\u003d jobdesc) {\n      return;\n    }\n    final int maps \u003d jobdesc.getNumberMaps();\n    final int reds \u003d jobdesc.getNumberReduces();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      mapInputBytesTotal +\u003d info.getInputBytes();\n      mapOutputBytesTotal +\u003d info.getOutputBytes();\n      mapOutputRecordsTotal +\u003d info.getOutputRecords();\n    }\n    final double[] reduceRecordRatio \u003d new double[reds];\n    final double[] reduceByteRatio \u003d new double[reds];\n    for (int i \u003d 0; i \u003c reds; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.REDUCE, i);\n      reduceByteRatio[i] \u003d info.getInputBytes() / (1.0 * mapOutputBytesTotal);\n      reduceRecordRatio[i] \u003d\n        info.getInputRecords() / (1.0 * mapOutputRecordsTotal);\n    }\n    final InputStriper striper \u003d new InputStriper(inputDir, mapInputBytesTotal);\n    final List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final int nSpec \u003d reds / maps + ((reds % maps) \u003e i ? 1 : 0);\n      final long[] specBytes \u003d new long[nSpec];\n      final long[] specRecords \u003d new long[nSpec];\n      final ResourceUsageMetrics[] metrics \u003d new ResourceUsageMetrics[nSpec];\n      for (int j \u003d 0; j \u003c nSpec; ++j) {\n        final TaskInfo info \u003d\n          jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);\n        specBytes[j] \u003d info.getOutputBytes();\n        specRecords[j] \u003d info.getOutputRecords();\n        metrics[j] \u003d info.getResourceUsageMetrics();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d %d %d %d %d\", id(), i,\n                    i + j * maps, info.getOutputRecords(), \n                    info.getOutputBytes(), \n                    info.getResourceUsageMetrics().getCumulativeCpuUsage(),\n                    info.getResourceUsageMetrics().getPhysicalMemoryUsage(),\n                    info.getResourceUsageMetrics().getVirtualMemoryUsage(),\n                    info.getResourceUsageMetrics().getHeapUsage()));\n        }\n      }\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      long possiblyCompressedInputBytes \u003d info.getInputBytes();\n      Configuration conf \u003d job.getConfiguration();\n      long uncompressedInputBytes \u003d\n          CompressionEmulationUtil.getUncompressedInputBytes(\n          possiblyCompressedInputBytes, conf);\n      splits.add(\n        new LoadSplit(striper.splitFor(inputDir, uncompressedInputBytes, 3), \n                      maps, i, uncompressedInputBytes, info.getInputRecords(),\n                      info.getOutputBytes(), info.getOutputRecords(),\n                      reduceByteRatio, reduceRecordRatio, specBytes, \n                      specRecords, info.getResourceUsageMetrics(),\n                      metrics));\n    }\n    pushDescription(id(), splits);\n  }",
      "path": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
        "newPath": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
      }
    },
    "231e39462dbfe60f66710e0425dbf16069382dbe": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2722. [Gridmix] Gridmix simulated job\u0027s map\u0027s hdfsBytesRead counter is wrong when compressed input is used.(ravigummadi)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1297052 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/03/12 5:44 AM",
      "commitName": "231e39462dbfe60f66710e0425dbf16069382dbe",
      "commitAuthor": "Ravi Gummadi",
      "commitDateOld": "21/10/11 11:14 PM",
      "commitNameOld": "5795fcfd9904431ec075fdce7ab8559ff50eccd2",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 135.31,
      "commitsBetweenForRepo": 978,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,63 @@\n   void buildSplits(FilePool inputDir) throws IOException {\n     long mapInputBytesTotal \u003d 0L;\n     long mapOutputBytesTotal \u003d 0L;\n     long mapOutputRecordsTotal \u003d 0L;\n     final JobStory jobdesc \u003d getJobDesc();\n     if (null \u003d\u003d jobdesc) {\n       return;\n     }\n     final int maps \u003d jobdesc.getNumberMaps();\n     final int reds \u003d jobdesc.getNumberReduces();\n     for (int i \u003d 0; i \u003c maps; ++i) {\n       final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n       mapInputBytesTotal +\u003d info.getInputBytes();\n       mapOutputBytesTotal +\u003d info.getOutputBytes();\n       mapOutputRecordsTotal +\u003d info.getOutputRecords();\n     }\n     final double[] reduceRecordRatio \u003d new double[reds];\n     final double[] reduceByteRatio \u003d new double[reds];\n     for (int i \u003d 0; i \u003c reds; ++i) {\n       final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.REDUCE, i);\n       reduceByteRatio[i] \u003d info.getInputBytes() / (1.0 * mapOutputBytesTotal);\n       reduceRecordRatio[i] \u003d\n         info.getInputRecords() / (1.0 * mapOutputRecordsTotal);\n     }\n     final InputStriper striper \u003d new InputStriper(inputDir, mapInputBytesTotal);\n     final List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n     for (int i \u003d 0; i \u003c maps; ++i) {\n       final int nSpec \u003d reds / maps + ((reds % maps) \u003e i ? 1 : 0);\n       final long[] specBytes \u003d new long[nSpec];\n       final long[] specRecords \u003d new long[nSpec];\n       final ResourceUsageMetrics[] metrics \u003d new ResourceUsageMetrics[nSpec];\n       for (int j \u003d 0; j \u003c nSpec; ++j) {\n         final TaskInfo info \u003d\n           jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);\n         specBytes[j] \u003d info.getOutputBytes();\n         specRecords[j] \u003d info.getOutputRecords();\n         metrics[j] \u003d info.getResourceUsageMetrics();\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d %d %d %d %d\", id(), i,\n                     i + j * maps, info.getOutputRecords(), \n                     info.getOutputBytes(), \n                     info.getResourceUsageMetrics().getCumulativeCpuUsage(),\n                     info.getResourceUsageMetrics().getPhysicalMemoryUsage(),\n                     info.getResourceUsageMetrics().getVirtualMemoryUsage(),\n                     info.getResourceUsageMetrics().getHeapUsage()));\n         }\n       }\n       final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n+      long possiblyCompressedInputBytes \u003d info.getInputBytes();\n+      Configuration conf \u003d job.getConfiguration();\n+      long uncompressedInputBytes \u003d\n+          CompressionEmulationUtil.getUncompressedInputBytes(\n+          possiblyCompressedInputBytes, conf);\n       splits.add(\n-        new LoadSplit(striper.splitFor(inputDir, info.getInputBytes(), 3), \n-                      maps, i, info.getInputBytes(), info.getInputRecords(),\n+        new LoadSplit(striper.splitFor(inputDir, uncompressedInputBytes, 3), \n+                      maps, i, uncompressedInputBytes, info.getInputRecords(),\n                       info.getOutputBytes(), info.getOutputRecords(),\n                       reduceByteRatio, reduceRecordRatio, specBytes, \n                       specRecords, info.getResourceUsageMetrics(),\n                       metrics));\n     }\n     pushDescription(id(), splits);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void buildSplits(FilePool inputDir) throws IOException {\n    long mapInputBytesTotal \u003d 0L;\n    long mapOutputBytesTotal \u003d 0L;\n    long mapOutputRecordsTotal \u003d 0L;\n    final JobStory jobdesc \u003d getJobDesc();\n    if (null \u003d\u003d jobdesc) {\n      return;\n    }\n    final int maps \u003d jobdesc.getNumberMaps();\n    final int reds \u003d jobdesc.getNumberReduces();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      mapInputBytesTotal +\u003d info.getInputBytes();\n      mapOutputBytesTotal +\u003d info.getOutputBytes();\n      mapOutputRecordsTotal +\u003d info.getOutputRecords();\n    }\n    final double[] reduceRecordRatio \u003d new double[reds];\n    final double[] reduceByteRatio \u003d new double[reds];\n    for (int i \u003d 0; i \u003c reds; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.REDUCE, i);\n      reduceByteRatio[i] \u003d info.getInputBytes() / (1.0 * mapOutputBytesTotal);\n      reduceRecordRatio[i] \u003d\n        info.getInputRecords() / (1.0 * mapOutputRecordsTotal);\n    }\n    final InputStriper striper \u003d new InputStriper(inputDir, mapInputBytesTotal);\n    final List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final int nSpec \u003d reds / maps + ((reds % maps) \u003e i ? 1 : 0);\n      final long[] specBytes \u003d new long[nSpec];\n      final long[] specRecords \u003d new long[nSpec];\n      final ResourceUsageMetrics[] metrics \u003d new ResourceUsageMetrics[nSpec];\n      for (int j \u003d 0; j \u003c nSpec; ++j) {\n        final TaskInfo info \u003d\n          jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);\n        specBytes[j] \u003d info.getOutputBytes();\n        specRecords[j] \u003d info.getOutputRecords();\n        metrics[j] \u003d info.getResourceUsageMetrics();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d %d %d %d %d\", id(), i,\n                    i + j * maps, info.getOutputRecords(), \n                    info.getOutputBytes(), \n                    info.getResourceUsageMetrics().getCumulativeCpuUsage(),\n                    info.getResourceUsageMetrics().getPhysicalMemoryUsage(),\n                    info.getResourceUsageMetrics().getVirtualMemoryUsage(),\n                    info.getResourceUsageMetrics().getHeapUsage()));\n        }\n      }\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      long possiblyCompressedInputBytes \u003d info.getInputBytes();\n      Configuration conf \u003d job.getConfiguration();\n      long uncompressedInputBytes \u003d\n          CompressionEmulationUtil.getUncompressedInputBytes(\n          possiblyCompressedInputBytes, conf);\n      splits.add(\n        new LoadSplit(striper.splitFor(inputDir, uncompressedInputBytes, 3), \n                      maps, i, uncompressedInputBytes, info.getInputRecords(),\n                      info.getOutputBytes(), info.getOutputRecords(),\n                      reduceByteRatio, reduceRecordRatio, specBytes, \n                      specRecords, info.getResourceUsageMetrics(),\n                      metrics));\n    }\n    pushDescription(id(), splits);\n  }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {}
    },
    "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3008. Improvements to cumulative CPU emulation for short running tasks in Gridmix. (amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179933 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/10/11 9:59 PM",
      "commitName": "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 43.2,
      "commitsBetweenForRepo": 284,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,58 @@\n   void buildSplits(FilePool inputDir) throws IOException {\n     long mapInputBytesTotal \u003d 0L;\n     long mapOutputBytesTotal \u003d 0L;\n     long mapOutputRecordsTotal \u003d 0L;\n     final JobStory jobdesc \u003d getJobDesc();\n     if (null \u003d\u003d jobdesc) {\n       return;\n     }\n     final int maps \u003d jobdesc.getNumberMaps();\n     final int reds \u003d jobdesc.getNumberReduces();\n     for (int i \u003d 0; i \u003c maps; ++i) {\n       final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n       mapInputBytesTotal +\u003d info.getInputBytes();\n       mapOutputBytesTotal +\u003d info.getOutputBytes();\n       mapOutputRecordsTotal +\u003d info.getOutputRecords();\n     }\n     final double[] reduceRecordRatio \u003d new double[reds];\n     final double[] reduceByteRatio \u003d new double[reds];\n     for (int i \u003d 0; i \u003c reds; ++i) {\n       final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.REDUCE, i);\n       reduceByteRatio[i] \u003d info.getInputBytes() / (1.0 * mapOutputBytesTotal);\n       reduceRecordRatio[i] \u003d\n         info.getInputRecords() / (1.0 * mapOutputRecordsTotal);\n     }\n     final InputStriper striper \u003d new InputStriper(inputDir, mapInputBytesTotal);\n     final List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n     for (int i \u003d 0; i \u003c maps; ++i) {\n       final int nSpec \u003d reds / maps + ((reds % maps) \u003e i ? 1 : 0);\n       final long[] specBytes \u003d new long[nSpec];\n       final long[] specRecords \u003d new long[nSpec];\n       final ResourceUsageMetrics[] metrics \u003d new ResourceUsageMetrics[nSpec];\n       for (int j \u003d 0; j \u003c nSpec; ++j) {\n         final TaskInfo info \u003d\n           jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);\n         specBytes[j] \u003d info.getOutputBytes();\n         specRecords[j] \u003d info.getOutputRecords();\n         metrics[j] \u003d info.getResourceUsageMetrics();\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d\", id(), i,\n+          LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d %d %d %d %d\", id(), i,\n                     i + j * maps, info.getOutputRecords(), \n-                    info.getOutputBytes()));\n+                    info.getOutputBytes(), \n+                    info.getResourceUsageMetrics().getCumulativeCpuUsage(),\n+                    info.getResourceUsageMetrics().getPhysicalMemoryUsage(),\n+                    info.getResourceUsageMetrics().getVirtualMemoryUsage(),\n+                    info.getResourceUsageMetrics().getHeapUsage()));\n         }\n       }\n       final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n       splits.add(\n         new LoadSplit(striper.splitFor(inputDir, info.getInputBytes(), 3), \n                       maps, i, info.getInputBytes(), info.getInputRecords(),\n                       info.getOutputBytes(), info.getOutputRecords(),\n                       reduceByteRatio, reduceRecordRatio, specBytes, \n                       specRecords, info.getResourceUsageMetrics(),\n                       metrics));\n     }\n     pushDescription(id(), splits);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void buildSplits(FilePool inputDir) throws IOException {\n    long mapInputBytesTotal \u003d 0L;\n    long mapOutputBytesTotal \u003d 0L;\n    long mapOutputRecordsTotal \u003d 0L;\n    final JobStory jobdesc \u003d getJobDesc();\n    if (null \u003d\u003d jobdesc) {\n      return;\n    }\n    final int maps \u003d jobdesc.getNumberMaps();\n    final int reds \u003d jobdesc.getNumberReduces();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      mapInputBytesTotal +\u003d info.getInputBytes();\n      mapOutputBytesTotal +\u003d info.getOutputBytes();\n      mapOutputRecordsTotal +\u003d info.getOutputRecords();\n    }\n    final double[] reduceRecordRatio \u003d new double[reds];\n    final double[] reduceByteRatio \u003d new double[reds];\n    for (int i \u003d 0; i \u003c reds; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.REDUCE, i);\n      reduceByteRatio[i] \u003d info.getInputBytes() / (1.0 * mapOutputBytesTotal);\n      reduceRecordRatio[i] \u003d\n        info.getInputRecords() / (1.0 * mapOutputRecordsTotal);\n    }\n    final InputStriper striper \u003d new InputStriper(inputDir, mapInputBytesTotal);\n    final List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final int nSpec \u003d reds / maps + ((reds % maps) \u003e i ? 1 : 0);\n      final long[] specBytes \u003d new long[nSpec];\n      final long[] specRecords \u003d new long[nSpec];\n      final ResourceUsageMetrics[] metrics \u003d new ResourceUsageMetrics[nSpec];\n      for (int j \u003d 0; j \u003c nSpec; ++j) {\n        final TaskInfo info \u003d\n          jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);\n        specBytes[j] \u003d info.getOutputBytes();\n        specRecords[j] \u003d info.getOutputRecords();\n        metrics[j] \u003d info.getResourceUsageMetrics();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d %d %d %d %d\", id(), i,\n                    i + j * maps, info.getOutputRecords(), \n                    info.getOutputBytes(), \n                    info.getResourceUsageMetrics().getCumulativeCpuUsage(),\n                    info.getResourceUsageMetrics().getPhysicalMemoryUsage(),\n                    info.getResourceUsageMetrics().getVirtualMemoryUsage(),\n                    info.getResourceUsageMetrics().getHeapUsage()));\n        }\n      }\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      splits.add(\n        new LoadSplit(striper.splitFor(inputDir, info.getInputBytes(), 3), \n                      maps, i, info.getInputBytes(), info.getInputRecords(),\n                      info.getOutputBytes(), info.getOutputRecords(),\n                      reduceByteRatio, reduceRecordRatio, specBytes, \n                      specRecords, info.getResourceUsageMetrics(),\n                      metrics));\n    }\n    pushDescription(id(), splits);\n  }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void buildSplits(FilePool inputDir) throws IOException {\n    long mapInputBytesTotal \u003d 0L;\n    long mapOutputBytesTotal \u003d 0L;\n    long mapOutputRecordsTotal \u003d 0L;\n    final JobStory jobdesc \u003d getJobDesc();\n    if (null \u003d\u003d jobdesc) {\n      return;\n    }\n    final int maps \u003d jobdesc.getNumberMaps();\n    final int reds \u003d jobdesc.getNumberReduces();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      mapInputBytesTotal +\u003d info.getInputBytes();\n      mapOutputBytesTotal +\u003d info.getOutputBytes();\n      mapOutputRecordsTotal +\u003d info.getOutputRecords();\n    }\n    final double[] reduceRecordRatio \u003d new double[reds];\n    final double[] reduceByteRatio \u003d new double[reds];\n    for (int i \u003d 0; i \u003c reds; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.REDUCE, i);\n      reduceByteRatio[i] \u003d info.getInputBytes() / (1.0 * mapOutputBytesTotal);\n      reduceRecordRatio[i] \u003d\n        info.getInputRecords() / (1.0 * mapOutputRecordsTotal);\n    }\n    final InputStriper striper \u003d new InputStriper(inputDir, mapInputBytesTotal);\n    final List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final int nSpec \u003d reds / maps + ((reds % maps) \u003e i ? 1 : 0);\n      final long[] specBytes \u003d new long[nSpec];\n      final long[] specRecords \u003d new long[nSpec];\n      final ResourceUsageMetrics[] metrics \u003d new ResourceUsageMetrics[nSpec];\n      for (int j \u003d 0; j \u003c nSpec; ++j) {\n        final TaskInfo info \u003d\n          jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);\n        specBytes[j] \u003d info.getOutputBytes();\n        specRecords[j] \u003d info.getOutputRecords();\n        metrics[j] \u003d info.getResourceUsageMetrics();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d\", id(), i,\n                    i + j * maps, info.getOutputRecords(), \n                    info.getOutputBytes()));\n        }\n      }\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      splits.add(\n        new LoadSplit(striper.splitFor(inputDir, info.getInputBytes(), 3), \n                      maps, i, info.getInputBytes(), info.getInputRecords(),\n                      info.getOutputBytes(), info.getOutputRecords(),\n                      reduceByteRatio, reduceRecordRatio, specBytes, \n                      specRecords, info.getResourceUsageMetrics(),\n                      metrics));\n    }\n    pushDescription(id(), splits);\n  }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
        "newPath": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void buildSplits(FilePool inputDir) throws IOException {\n    long mapInputBytesTotal \u003d 0L;\n    long mapOutputBytesTotal \u003d 0L;\n    long mapOutputRecordsTotal \u003d 0L;\n    final JobStory jobdesc \u003d getJobDesc();\n    if (null \u003d\u003d jobdesc) {\n      return;\n    }\n    final int maps \u003d jobdesc.getNumberMaps();\n    final int reds \u003d jobdesc.getNumberReduces();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      mapInputBytesTotal +\u003d info.getInputBytes();\n      mapOutputBytesTotal +\u003d info.getOutputBytes();\n      mapOutputRecordsTotal +\u003d info.getOutputRecords();\n    }\n    final double[] reduceRecordRatio \u003d new double[reds];\n    final double[] reduceByteRatio \u003d new double[reds];\n    for (int i \u003d 0; i \u003c reds; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.REDUCE, i);\n      reduceByteRatio[i] \u003d info.getInputBytes() / (1.0 * mapOutputBytesTotal);\n      reduceRecordRatio[i] \u003d\n        info.getInputRecords() / (1.0 * mapOutputRecordsTotal);\n    }\n    final InputStriper striper \u003d new InputStriper(inputDir, mapInputBytesTotal);\n    final List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final int nSpec \u003d reds / maps + ((reds % maps) \u003e i ? 1 : 0);\n      final long[] specBytes \u003d new long[nSpec];\n      final long[] specRecords \u003d new long[nSpec];\n      final ResourceUsageMetrics[] metrics \u003d new ResourceUsageMetrics[nSpec];\n      for (int j \u003d 0; j \u003c nSpec; ++j) {\n        final TaskInfo info \u003d\n          jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);\n        specBytes[j] \u003d info.getOutputBytes();\n        specRecords[j] \u003d info.getOutputRecords();\n        metrics[j] \u003d info.getResourceUsageMetrics();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d\", id(), i,\n                    i + j * maps, info.getOutputRecords(), \n                    info.getOutputBytes()));\n        }\n      }\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      splits.add(\n        new LoadSplit(striper.splitFor(inputDir, info.getInputBytes(), 3), \n                      maps, i, info.getInputBytes(), info.getInputRecords(),\n                      info.getOutputBytes(), info.getOutputRecords(),\n                      reduceByteRatio, reduceRecordRatio, specBytes, \n                      specRecords, info.getResourceUsageMetrics(),\n                      metrics));\n    }\n    pushDescription(id(), splits);\n  }",
      "path": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
        "newPath": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
      }
    },
    "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2106. [Gridmix] Cumulative CPU usage emulation in Gridmix. (amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1135396 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/06/11 12:44 AM",
      "commitName": "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 1.41,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,54 @@\n   void buildSplits(FilePool inputDir) throws IOException {\n     long mapInputBytesTotal \u003d 0L;\n     long mapOutputBytesTotal \u003d 0L;\n     long mapOutputRecordsTotal \u003d 0L;\n     final JobStory jobdesc \u003d getJobDesc();\n     if (null \u003d\u003d jobdesc) {\n       return;\n     }\n     final int maps \u003d jobdesc.getNumberMaps();\n     final int reds \u003d jobdesc.getNumberReduces();\n     for (int i \u003d 0; i \u003c maps; ++i) {\n       final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n       mapInputBytesTotal +\u003d info.getInputBytes();\n       mapOutputBytesTotal +\u003d info.getOutputBytes();\n       mapOutputRecordsTotal +\u003d info.getOutputRecords();\n     }\n     final double[] reduceRecordRatio \u003d new double[reds];\n     final double[] reduceByteRatio \u003d new double[reds];\n     for (int i \u003d 0; i \u003c reds; ++i) {\n       final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.REDUCE, i);\n       reduceByteRatio[i] \u003d info.getInputBytes() / (1.0 * mapOutputBytesTotal);\n       reduceRecordRatio[i] \u003d\n         info.getInputRecords() / (1.0 * mapOutputRecordsTotal);\n     }\n     final InputStriper striper \u003d new InputStriper(inputDir, mapInputBytesTotal);\n     final List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n     for (int i \u003d 0; i \u003c maps; ++i) {\n       final int nSpec \u003d reds / maps + ((reds % maps) \u003e i ? 1 : 0);\n       final long[] specBytes \u003d new long[nSpec];\n       final long[] specRecords \u003d new long[nSpec];\n+      final ResourceUsageMetrics[] metrics \u003d new ResourceUsageMetrics[nSpec];\n       for (int j \u003d 0; j \u003c nSpec; ++j) {\n         final TaskInfo info \u003d\n           jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);\n         specBytes[j] \u003d info.getOutputBytes();\n         specRecords[j] \u003d info.getOutputRecords();\n+        metrics[j] \u003d info.getResourceUsageMetrics();\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d\", id(), i,\n                     i + j * maps, info.getOutputRecords(), \n                     info.getOutputBytes()));\n         }\n       }\n       final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n       splits.add(\n         new LoadSplit(striper.splitFor(inputDir, info.getInputBytes(), 3), \n                       maps, i, info.getInputBytes(), info.getInputRecords(),\n                       info.getOutputBytes(), info.getOutputRecords(),\n                       reduceByteRatio, reduceRecordRatio, specBytes, \n-                      specRecords));\n+                      specRecords, info.getResourceUsageMetrics(),\n+                      metrics));\n     }\n     pushDescription(id(), splits);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void buildSplits(FilePool inputDir) throws IOException {\n    long mapInputBytesTotal \u003d 0L;\n    long mapOutputBytesTotal \u003d 0L;\n    long mapOutputRecordsTotal \u003d 0L;\n    final JobStory jobdesc \u003d getJobDesc();\n    if (null \u003d\u003d jobdesc) {\n      return;\n    }\n    final int maps \u003d jobdesc.getNumberMaps();\n    final int reds \u003d jobdesc.getNumberReduces();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      mapInputBytesTotal +\u003d info.getInputBytes();\n      mapOutputBytesTotal +\u003d info.getOutputBytes();\n      mapOutputRecordsTotal +\u003d info.getOutputRecords();\n    }\n    final double[] reduceRecordRatio \u003d new double[reds];\n    final double[] reduceByteRatio \u003d new double[reds];\n    for (int i \u003d 0; i \u003c reds; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.REDUCE, i);\n      reduceByteRatio[i] \u003d info.getInputBytes() / (1.0 * mapOutputBytesTotal);\n      reduceRecordRatio[i] \u003d\n        info.getInputRecords() / (1.0 * mapOutputRecordsTotal);\n    }\n    final InputStriper striper \u003d new InputStriper(inputDir, mapInputBytesTotal);\n    final List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final int nSpec \u003d reds / maps + ((reds % maps) \u003e i ? 1 : 0);\n      final long[] specBytes \u003d new long[nSpec];\n      final long[] specRecords \u003d new long[nSpec];\n      final ResourceUsageMetrics[] metrics \u003d new ResourceUsageMetrics[nSpec];\n      for (int j \u003d 0; j \u003c nSpec; ++j) {\n        final TaskInfo info \u003d\n          jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);\n        specBytes[j] \u003d info.getOutputBytes();\n        specRecords[j] \u003d info.getOutputRecords();\n        metrics[j] \u003d info.getResourceUsageMetrics();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d\", id(), i,\n                    i + j * maps, info.getOutputRecords(), \n                    info.getOutputBytes()));\n        }\n      }\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      splits.add(\n        new LoadSplit(striper.splitFor(inputDir, info.getInputBytes(), 3), \n                      maps, i, info.getInputBytes(), info.getInputRecords(),\n                      info.getOutputBytes(), info.getOutputRecords(),\n                      reduceByteRatio, reduceRecordRatio, specBytes, \n                      specRecords, info.getResourceUsageMetrics(),\n                      metrics));\n    }\n    pushDescription(id(), splits);\n  }",
      "path": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,51 @@\n+  void buildSplits(FilePool inputDir) throws IOException {\n+    long mapInputBytesTotal \u003d 0L;\n+    long mapOutputBytesTotal \u003d 0L;\n+    long mapOutputRecordsTotal \u003d 0L;\n+    final JobStory jobdesc \u003d getJobDesc();\n+    if (null \u003d\u003d jobdesc) {\n+      return;\n+    }\n+    final int maps \u003d jobdesc.getNumberMaps();\n+    final int reds \u003d jobdesc.getNumberReduces();\n+    for (int i \u003d 0; i \u003c maps; ++i) {\n+      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n+      mapInputBytesTotal +\u003d info.getInputBytes();\n+      mapOutputBytesTotal +\u003d info.getOutputBytes();\n+      mapOutputRecordsTotal +\u003d info.getOutputRecords();\n+    }\n+    final double[] reduceRecordRatio \u003d new double[reds];\n+    final double[] reduceByteRatio \u003d new double[reds];\n+    for (int i \u003d 0; i \u003c reds; ++i) {\n+      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.REDUCE, i);\n+      reduceByteRatio[i] \u003d info.getInputBytes() / (1.0 * mapOutputBytesTotal);\n+      reduceRecordRatio[i] \u003d\n+        info.getInputRecords() / (1.0 * mapOutputRecordsTotal);\n+    }\n+    final InputStriper striper \u003d new InputStriper(inputDir, mapInputBytesTotal);\n+    final List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n+    for (int i \u003d 0; i \u003c maps; ++i) {\n+      final int nSpec \u003d reds / maps + ((reds % maps) \u003e i ? 1 : 0);\n+      final long[] specBytes \u003d new long[nSpec];\n+      final long[] specRecords \u003d new long[nSpec];\n+      for (int j \u003d 0; j \u003c nSpec; ++j) {\n+        final TaskInfo info \u003d\n+          jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);\n+        specBytes[j] \u003d info.getOutputBytes();\n+        specRecords[j] \u003d info.getOutputRecords();\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d\", id(), i,\n+                    i + j * maps, info.getOutputRecords(), \n+                    info.getOutputBytes()));\n+        }\n+      }\n+      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n+      splits.add(\n+        new LoadSplit(striper.splitFor(inputDir, info.getInputBytes(), 3), \n+                      maps, i, info.getInputBytes(), info.getInputRecords(),\n+                      info.getOutputBytes(), info.getOutputRecords(),\n+                      reduceByteRatio, reduceRecordRatio, specBytes, \n+                      specRecords));\n+    }\n+    pushDescription(id(), splits);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void buildSplits(FilePool inputDir) throws IOException {\n    long mapInputBytesTotal \u003d 0L;\n    long mapOutputBytesTotal \u003d 0L;\n    long mapOutputRecordsTotal \u003d 0L;\n    final JobStory jobdesc \u003d getJobDesc();\n    if (null \u003d\u003d jobdesc) {\n      return;\n    }\n    final int maps \u003d jobdesc.getNumberMaps();\n    final int reds \u003d jobdesc.getNumberReduces();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      mapInputBytesTotal +\u003d info.getInputBytes();\n      mapOutputBytesTotal +\u003d info.getOutputBytes();\n      mapOutputRecordsTotal +\u003d info.getOutputRecords();\n    }\n    final double[] reduceRecordRatio \u003d new double[reds];\n    final double[] reduceByteRatio \u003d new double[reds];\n    for (int i \u003d 0; i \u003c reds; ++i) {\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.REDUCE, i);\n      reduceByteRatio[i] \u003d info.getInputBytes() / (1.0 * mapOutputBytesTotal);\n      reduceRecordRatio[i] \u003d\n        info.getInputRecords() / (1.0 * mapOutputRecordsTotal);\n    }\n    final InputStriper striper \u003d new InputStriper(inputDir, mapInputBytesTotal);\n    final List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e();\n    for (int i \u003d 0; i \u003c maps; ++i) {\n      final int nSpec \u003d reds / maps + ((reds % maps) \u003e i ? 1 : 0);\n      final long[] specBytes \u003d new long[nSpec];\n      final long[] specRecords \u003d new long[nSpec];\n      for (int j \u003d 0; j \u003c nSpec; ++j) {\n        final TaskInfo info \u003d\n          jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);\n        specBytes[j] \u003d info.getOutputBytes();\n        specRecords[j] \u003d info.getOutputRecords();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"SPEC(%d) %d -\u003e %d %d %d\", id(), i,\n                    i + j * maps, info.getOutputRecords(), \n                    info.getOutputBytes()));\n        }\n      }\n      final TaskInfo info \u003d jobdesc.getTaskInfo(TaskType.MAP, i);\n      splits.add(\n        new LoadSplit(striper.splitFor(inputDir, info.getInputBytes(), 3), \n                      maps, i, info.getInputBytes(), info.getInputRecords(),\n                      info.getOutputBytes(), info.getOutputRecords(),\n                      reduceByteRatio, reduceRecordRatio, specBytes, \n                      specRecords));\n    }\n    pushDescription(id(), splits);\n  }",
      "path": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
    }
  }
}