{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "EditLogFileOutputStream.java",
  "functionName": "close",
  "functionId": "close",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
  "functionStartLine": 143,
  "functionEndLine": 172,
  "numCommitsSeen": 32,
  "timeTaken": 5706,
  "changeHistory": [
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
    "18312804e9c86c0ea6a259e288994fea6fa366ef",
    "6d4a0915676b8185a4727a10fcfeb40aa24cacc5",
    "19dd66a3f616cd8a4527f2adeef911a7d4b3f349",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
    "44320eed1732ea59bd9ec83009eb10e0e6f13023",
    "438c32aaf9fb0c63f55044cf5ef1b2e0adcf7fea",
    "1ba3ddbe6d8f617fd2fa4150e8a7be097dd83b4a",
    "7bd41f031f3852ac44b911a71d1d57ea3629b134",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": "Ybodychange",
    "18312804e9c86c0ea6a259e288994fea6fa366ef": "Ybodychange",
    "6d4a0915676b8185a4727a10fcfeb40aa24cacc5": "Ybodychange",
    "19dd66a3f616cd8a4527f2adeef911a7d4b3f349": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": "Ybodychange",
    "44320eed1732ea59bd9ec83009eb10e0e6f13023": "Ybodychange",
    "438c32aaf9fb0c63f55044cf5ef1b2e0adcf7fea": "Ybodychange",
    "1ba3ddbe6d8f617fd2fa4150e8a7be097dd83b4a": "Ybodychange",
    "7bd41f031f3852ac44b911a71d1d57ea3629b134": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13695. Move logging to slf4j in HDFS package. Contributed by Ian Pickering.\n",
      "commitDate": "06/09/18 2:48 PM",
      "commitName": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "05/11/14 10:14 AM",
      "commitNameOld": "18312804e9c86c0ea6a259e288994fea6fa366ef",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1401.15,
      "commitsBetweenForRepo": 10593,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n   public void close() throws IOException {\n     if (fp \u003d\u003d null) {\n       throw new IOException(\"Trying to use aborted output stream\");\n     }\n \n     try {\n       // close should have been called after all pending transactions\n       // have been flushed \u0026 synced.\n       // if already closed, just skip\n       if (doubleBuf !\u003d null) {\n         doubleBuf.close();\n         doubleBuf \u003d null;\n       }\n       \n       // remove any preallocated padding bytes from the transaction log.\n       if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n         fc.truncate(fc.position());\n         fc.close();\n         fc \u003d null;\n       }\n       fp.close();\n       fp \u003d null;\n     } finally {\n-      IOUtils.cleanup(LOG, fc, fp);\n+      IOUtils.cleanupWithLogger(LOG, fc, fp);\n       doubleBuf \u003d null;\n       fc \u003d null;\n       fp \u003d null;\n     }\n     fp \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (fp \u003d\u003d null) {\n      throw new IOException(\"Trying to use aborted output stream\");\n    }\n\n    try {\n      // close should have been called after all pending transactions\n      // have been flushed \u0026 synced.\n      // if already closed, just skip\n      if (doubleBuf !\u003d null) {\n        doubleBuf.close();\n        doubleBuf \u003d null;\n      }\n      \n      // remove any preallocated padding bytes from the transaction log.\n      if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n        fc.truncate(fc.position());\n        fc.close();\n        fc \u003d null;\n      }\n      fp.close();\n      fp \u003d null;\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, fc, fp);\n      doubleBuf \u003d null;\n      fc \u003d null;\n      fp \u003d null;\n    }\n    fp \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
      "extendedDetails": {}
    },
    "18312804e9c86c0ea6a259e288994fea6fa366ef": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7357. FSNamesystem.checkFileProgress should log file path. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "05/11/14 10:14 AM",
      "commitName": "18312804e9c86c0ea6a259e288994fea6fa366ef",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/03/14 4:32 PM",
      "commitNameOld": "c2ef7e239eb0e81cf8a3e971378e9e696202de67",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 225.78,
      "commitsBetweenForRepo": 1835,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n   public void close() throws IOException {\n     if (fp \u003d\u003d null) {\n       throw new IOException(\"Trying to use aborted output stream\");\n     }\n \n     try {\n       // close should have been called after all pending transactions\n       // have been flushed \u0026 synced.\n       // if already closed, just skip\n       if (doubleBuf !\u003d null) {\n         doubleBuf.close();\n         doubleBuf \u003d null;\n       }\n       \n       // remove any preallocated padding bytes from the transaction log.\n       if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n         fc.truncate(fc.position());\n         fc.close();\n         fc \u003d null;\n       }\n       fp.close();\n       fp \u003d null;\n     } finally {\n-      IOUtils.cleanup(FSNamesystem.LOG, fc, fp);\n+      IOUtils.cleanup(LOG, fc, fp);\n       doubleBuf \u003d null;\n       fc \u003d null;\n       fp \u003d null;\n     }\n     fp \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (fp \u003d\u003d null) {\n      throw new IOException(\"Trying to use aborted output stream\");\n    }\n\n    try {\n      // close should have been called after all pending transactions\n      // have been flushed \u0026 synced.\n      // if already closed, just skip\n      if (doubleBuf !\u003d null) {\n        doubleBuf.close();\n        doubleBuf \u003d null;\n      }\n      \n      // remove any preallocated padding bytes from the transaction log.\n      if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n        fc.truncate(fc.position());\n        fc.close();\n        fc \u003d null;\n      }\n      fp.close();\n      fp \u003d null;\n    } finally {\n      IOUtils.cleanup(LOG, fc, fp);\n      doubleBuf \u003d null;\n      fc \u003d null;\n      fp \u003d null;\n    }\n    fp \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
      "extendedDetails": {}
    },
    "6d4a0915676b8185a4727a10fcfeb40aa24cacc5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4034. Remove redundant null checks. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430585 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 2:38 PM",
      "commitName": "6d4a0915676b8185a4727a10fcfeb40aa24cacc5",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "10/09/12 11:51 AM",
      "commitNameOld": "ca4582222e89114e4c61d38fbf973a66d2867abf",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 120.16,
      "commitsBetweenForRepo": 579,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,30 @@\n   public void close() throws IOException {\n     if (fp \u003d\u003d null) {\n       throw new IOException(\"Trying to use aborted output stream\");\n     }\n \n     try {\n       // close should have been called after all pending transactions\n       // have been flushed \u0026 synced.\n       // if already closed, just skip\n       if (doubleBuf !\u003d null) {\n         doubleBuf.close();\n         doubleBuf \u003d null;\n       }\n       \n       // remove any preallocated padding bytes from the transaction log.\n       if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n         fc.truncate(fc.position());\n         fc.close();\n         fc \u003d null;\n       }\n-      if (fp !\u003d null) {\n-        fp.close();\n-        fp \u003d null;\n-      }\n+      fp.close();\n+      fp \u003d null;\n     } finally {\n       IOUtils.cleanup(FSNamesystem.LOG, fc, fp);\n       doubleBuf \u003d null;\n       fc \u003d null;\n       fp \u003d null;\n     }\n     fp \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (fp \u003d\u003d null) {\n      throw new IOException(\"Trying to use aborted output stream\");\n    }\n\n    try {\n      // close should have been called after all pending transactions\n      // have been flushed \u0026 synced.\n      // if already closed, just skip\n      if (doubleBuf !\u003d null) {\n        doubleBuf.close();\n        doubleBuf \u003d null;\n      }\n      \n      // remove any preallocated padding bytes from the transaction log.\n      if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n        fc.truncate(fc.position());\n        fc.close();\n        fc \u003d null;\n      }\n      fp.close();\n      fp \u003d null;\n    } finally {\n      IOUtils.cleanup(FSNamesystem.LOG, fc, fp);\n      doubleBuf \u003d null;\n      fc \u003d null;\n      fp \u003d null;\n    }\n    fp \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
      "extendedDetails": {}
    },
    "19dd66a3f616cd8a4527f2adeef911a7d4b3f349": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3510.  Editlog pre-allocation is performed prior to writing edits to avoid partial edits case disk out of space. Contributed by Collin McCabe.\n        \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1355189 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/12 4:00 PM",
      "commitName": "19dd66a3f616cd8a4527f2adeef911a7d4b3f349",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "12/06/12 7:42 PM",
      "commitNameOld": "07b85844311b8b01e2edd2ceca8438b30d2872cd",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 15.85,
      "commitsBetweenForRepo": 58,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,32 @@\n   public void close() throws IOException {\n     if (fp \u003d\u003d null) {\n       throw new IOException(\"Trying to use aborted output stream\");\n     }\n \n     try {\n       // close should have been called after all pending transactions\n       // have been flushed \u0026 synced.\n       // if already closed, just skip\n       if (doubleBuf !\u003d null) {\n         doubleBuf.close();\n         doubleBuf \u003d null;\n       }\n       \n-      // remove the last INVALID marker from transaction log.\n+      // remove any preallocated padding bytes from the transaction log.\n       if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n         fc.truncate(fc.position());\n         fc.close();\n         fc \u003d null;\n       }\n       if (fp !\u003d null) {\n         fp.close();\n         fp \u003d null;\n       }\n     } finally {\n       IOUtils.cleanup(FSNamesystem.LOG, fc, fp);\n       doubleBuf \u003d null;\n       fc \u003d null;\n       fp \u003d null;\n     }\n     fp \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (fp \u003d\u003d null) {\n      throw new IOException(\"Trying to use aborted output stream\");\n    }\n\n    try {\n      // close should have been called after all pending transactions\n      // have been flushed \u0026 synced.\n      // if already closed, just skip\n      if (doubleBuf !\u003d null) {\n        doubleBuf.close();\n        doubleBuf \u003d null;\n      }\n      \n      // remove any preallocated padding bytes from the transaction log.\n      if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n        fc.truncate(fc.position());\n        fc.close();\n        fc \u003d null;\n      }\n      if (fp !\u003d null) {\n        fp.close();\n        fp \u003d null;\n      }\n    } finally {\n      IOUtils.cleanup(FSNamesystem.LOG, fc, fp);\n      doubleBuf \u003d null;\n      fc \u003d null;\n      fp \u003d null;\n    }\n    fp \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void close() throws IOException {\n    if (fp \u003d\u003d null) {\n      throw new IOException(\"Trying to use aborted output stream\");\n    }\n\n    try {\n      // close should have been called after all pending transactions\n      // have been flushed \u0026 synced.\n      // if already closed, just skip\n      if (doubleBuf !\u003d null) {\n        doubleBuf.close();\n        doubleBuf \u003d null;\n      }\n      \n      // remove the last INVALID marker from transaction log.\n      if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n        fc.truncate(fc.position());\n        fc.close();\n        fc \u003d null;\n      }\n      if (fp !\u003d null) {\n        fp.close();\n        fp \u003d null;\n      }\n    } finally {\n      IOUtils.cleanup(FSNamesystem.LOG, fc, fp);\n      doubleBuf \u003d null;\n      fc \u003d null;\n      fp \u003d null;\n    }\n    fp \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void close() throws IOException {\n    if (fp \u003d\u003d null) {\n      throw new IOException(\"Trying to use aborted output stream\");\n    }\n\n    try {\n      // close should have been called after all pending transactions\n      // have been flushed \u0026 synced.\n      // if already closed, just skip\n      if (doubleBuf !\u003d null) {\n        doubleBuf.close();\n        doubleBuf \u003d null;\n      }\n      \n      // remove the last INVALID marker from transaction log.\n      if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n        fc.truncate(fc.position());\n        fc.close();\n        fc \u003d null;\n      }\n      if (fp !\u003d null) {\n        fp.close();\n        fp \u003d null;\n      }\n    } finally {\n      IOUtils.cleanup(FSNamesystem.LOG, fc, fp);\n      doubleBuf \u003d null;\n      fc \u003d null;\n      fp \u003d null;\n    }\n    fp \u003d null;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java"
      }
    },
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/07/11 9:28 AM",
      "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "27/07/11 10:46 PM",
      "commitNameOld": "44320eed1732ea59bd9ec83009eb10e0e6f13023",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.45,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,32 @@\n   public void close() throws IOException {\n+    if (fp \u003d\u003d null) {\n+      throw new IOException(\"Trying to use aborted output stream\");\n+    }\n+\n     try {\n       // close should have been called after all pending transactions\n       // have been flushed \u0026 synced.\n       // if already closed, just skip\n       if (doubleBuf !\u003d null) {\n         doubleBuf.close();\n         doubleBuf \u003d null;\n       }\n       \n       // remove the last INVALID marker from transaction log.\n       if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n         fc.truncate(fc.position());\n         fc.close();\n         fc \u003d null;\n       }\n       if (fp !\u003d null) {\n         fp.close();\n         fp \u003d null;\n       }\n     } finally {\n       IOUtils.cleanup(FSNamesystem.LOG, fc, fp);\n       doubleBuf \u003d null;\n       fc \u003d null;\n       fp \u003d null;\n     }\n+    fp \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (fp \u003d\u003d null) {\n      throw new IOException(\"Trying to use aborted output stream\");\n    }\n\n    try {\n      // close should have been called after all pending transactions\n      // have been flushed \u0026 synced.\n      // if already closed, just skip\n      if (doubleBuf !\u003d null) {\n        doubleBuf.close();\n        doubleBuf \u003d null;\n      }\n      \n      // remove the last INVALID marker from transaction log.\n      if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n        fc.truncate(fc.position());\n        fc.close();\n        fc \u003d null;\n      }\n      if (fp !\u003d null) {\n        fp.close();\n        fp \u003d null;\n      }\n    } finally {\n      IOUtils.cleanup(FSNamesystem.LOG, fc, fp);\n      doubleBuf \u003d null;\n      fc \u003d null;\n      fp \u003d null;\n    }\n    fp \u003d null;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
      "extendedDetails": {}
    },
    "44320eed1732ea59bd9ec83009eb10e0e6f13023": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2212. Refactor double-buffering code out of EditLogOutputStreams. Contributed by Todd Lipcon\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151736 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/07/11 10:46 PM",
      "commitName": "44320eed1732ea59bd9ec83009eb10e0e6f13023",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "26/07/11 1:46 PM",
      "commitNameOld": "438c32aaf9fb0c63f55044cf5ef1b2e0adcf7fea",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 1.37,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,27 @@\n   public void close() throws IOException {\n     try {\n       // close should have been called after all pending transactions\n       // have been flushed \u0026 synced.\n       // if already closed, just skip\n-      if(bufCurrent !\u003d null)\n-      {\n-        int bufSize \u003d bufCurrent.size();\n-        if (bufSize !\u003d 0) {\n-          throw new IOException(\"FSEditStream has \" + bufSize\n-              + \" bytes still to be flushed and cannot \" + \"be closed.\");\n-        }\n-        bufCurrent.close();\n-        bufCurrent \u003d null;\n-        writer \u003d null;\n+      if (doubleBuf !\u003d null) {\n+        doubleBuf.close();\n+        doubleBuf \u003d null;\n       }\n-  \n-      if(bufReady !\u003d null) {\n-        bufReady.close();\n-        bufReady \u003d null;\n-      }\n-  \n+      \n       // remove the last INVALID marker from transaction log.\n       if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n         fc.truncate(fc.position());\n         fc.close();\n         fc \u003d null;\n       }\n       if (fp !\u003d null) {\n         fp.close();\n         fp \u003d null;\n       }\n     } finally {\n-      IOUtils.cleanup(FSNamesystem.LOG, bufCurrent, bufReady, fc, fp);\n-      bufCurrent \u003d bufReady \u003d null;\n-      writer \u003d null;\n+      IOUtils.cleanup(FSNamesystem.LOG, fc, fp);\n+      doubleBuf \u003d null;\n       fc \u003d null;\n       fp \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    try {\n      // close should have been called after all pending transactions\n      // have been flushed \u0026 synced.\n      // if already closed, just skip\n      if (doubleBuf !\u003d null) {\n        doubleBuf.close();\n        doubleBuf \u003d null;\n      }\n      \n      // remove the last INVALID marker from transaction log.\n      if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n        fc.truncate(fc.position());\n        fc.close();\n        fc \u003d null;\n      }\n      if (fp !\u003d null) {\n        fp.close();\n        fp \u003d null;\n      }\n    } finally {\n      IOUtils.cleanup(FSNamesystem.LOG, fc, fp);\n      doubleBuf \u003d null;\n      fc \u003d null;\n      fp \u003d null;\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
      "extendedDetails": {}
    },
    "438c32aaf9fb0c63f55044cf5ef1b2e0adcf7fea": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2149. Move EditLogOp serialization formats into FsEditLogOp implementations. Contributed by Ivan Kelly.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151238 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/11 1:46 PM",
      "commitName": "438c32aaf9fb0c63f55044cf5ef1b2e0adcf7fea",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "11/07/11 7:18 PM",
      "commitNameOld": "1ba3ddbe6d8f617fd2fa4150e8a7be097dd83b4a",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 14.77,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,40 @@\n   public void close() throws IOException {\n     try {\n       // close should have been called after all pending transactions\n       // have been flushed \u0026 synced.\n       // if already closed, just skip\n       if(bufCurrent !\u003d null)\n       {\n         int bufSize \u003d bufCurrent.size();\n         if (bufSize !\u003d 0) {\n           throw new IOException(\"FSEditStream has \" + bufSize\n               + \" bytes still to be flushed and cannot \" + \"be closed.\");\n         }\n         bufCurrent.close();\n         bufCurrent \u003d null;\n+        writer \u003d null;\n       }\n   \n       if(bufReady !\u003d null) {\n         bufReady.close();\n         bufReady \u003d null;\n       }\n   \n       // remove the last INVALID marker from transaction log.\n       if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n         fc.truncate(fc.position());\n         fc.close();\n         fc \u003d null;\n       }\n       if (fp !\u003d null) {\n         fp.close();\n         fp \u003d null;\n       }\n     } finally {\n       IOUtils.cleanup(FSNamesystem.LOG, bufCurrent, bufReady, fc, fp);\n       bufCurrent \u003d bufReady \u003d null;\n+      writer \u003d null;\n       fc \u003d null;\n       fp \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    try {\n      // close should have been called after all pending transactions\n      // have been flushed \u0026 synced.\n      // if already closed, just skip\n      if(bufCurrent !\u003d null)\n      {\n        int bufSize \u003d bufCurrent.size();\n        if (bufSize !\u003d 0) {\n          throw new IOException(\"FSEditStream has \" + bufSize\n              + \" bytes still to be flushed and cannot \" + \"be closed.\");\n        }\n        bufCurrent.close();\n        bufCurrent \u003d null;\n        writer \u003d null;\n      }\n  \n      if(bufReady !\u003d null) {\n        bufReady.close();\n        bufReady \u003d null;\n      }\n  \n      // remove the last INVALID marker from transaction log.\n      if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n        fc.truncate(fc.position());\n        fc.close();\n        fc \u003d null;\n      }\n      if (fp !\u003d null) {\n        fp.close();\n        fp \u003d null;\n      }\n    } finally {\n      IOUtils.cleanup(FSNamesystem.LOG, bufCurrent, bufReady, fc, fp);\n      bufCurrent \u003d bufReady \u003d null;\n      writer \u003d null;\n      fc \u003d null;\n      fp \u003d null;\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
      "extendedDetails": {}
    },
    "1ba3ddbe6d8f617fd2fa4150e8a7be097dd83b4a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2132. Potential resource leak in EditLogFileOutputStream.close. (atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1145428 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/07/11 7:18 PM",
      "commitName": "1ba3ddbe6d8f617fd2fa4150e8a7be097dd83b4a",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "30/06/11 4:21 PM",
      "commitNameOld": "7bd41f031f3852ac44b911a71d1d57ea3629b134",
      "commitAuthorOld": "Matthew Foley",
      "daysBetweenCommits": 11.12,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,38 @@\n   public void close() throws IOException {\n-    // close should have been called after all pending transactions\n-    // have been flushed \u0026 synced.\n-    // if already closed, just skip\n-    if(bufCurrent !\u003d null)\n-    {\n-      int bufSize \u003d bufCurrent.size();\n-      if (bufSize !\u003d 0) {\n-        throw new IOException(\"FSEditStream has \" + bufSize\n-            + \" bytes still to be flushed and cannot \" + \"be closed.\");\n+    try {\n+      // close should have been called after all pending transactions\n+      // have been flushed \u0026 synced.\n+      // if already closed, just skip\n+      if(bufCurrent !\u003d null)\n+      {\n+        int bufSize \u003d bufCurrent.size();\n+        if (bufSize !\u003d 0) {\n+          throw new IOException(\"FSEditStream has \" + bufSize\n+              + \" bytes still to be flushed and cannot \" + \"be closed.\");\n+        }\n+        bufCurrent.close();\n+        bufCurrent \u003d null;\n       }\n-      bufCurrent.close();\n-      bufCurrent \u003d null;\n-    }\n-\n-    if(bufReady !\u003d null) {\n-      bufReady.close();\n-      bufReady \u003d null;\n-    }\n-\n-    // remove the last INVALID marker from transaction log.\n-    if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n-      fc.truncate(fc.position());\n-      fc.close();\n-    }\n-    if (fp !\u003d null) {\n-      fp.close();\n+  \n+      if(bufReady !\u003d null) {\n+        bufReady.close();\n+        bufReady \u003d null;\n+      }\n+  \n+      // remove the last INVALID marker from transaction log.\n+      if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n+        fc.truncate(fc.position());\n+        fc.close();\n+        fc \u003d null;\n+      }\n+      if (fp !\u003d null) {\n+        fp.close();\n+        fp \u003d null;\n+      }\n+    } finally {\n+      IOUtils.cleanup(FSNamesystem.LOG, bufCurrent, bufReady, fc, fp);\n+      bufCurrent \u003d bufReady \u003d null;\n+      fc \u003d null;\n+      fp \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    try {\n      // close should have been called after all pending transactions\n      // have been flushed \u0026 synced.\n      // if already closed, just skip\n      if(bufCurrent !\u003d null)\n      {\n        int bufSize \u003d bufCurrent.size();\n        if (bufSize !\u003d 0) {\n          throw new IOException(\"FSEditStream has \" + bufSize\n              + \" bytes still to be flushed and cannot \" + \"be closed.\");\n        }\n        bufCurrent.close();\n        bufCurrent \u003d null;\n      }\n  \n      if(bufReady !\u003d null) {\n        bufReady.close();\n        bufReady \u003d null;\n      }\n  \n      // remove the last INVALID marker from transaction log.\n      if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n        fc.truncate(fc.position());\n        fc.close();\n        fc \u003d null;\n      }\n      if (fp !\u003d null) {\n        fp.close();\n        fp \u003d null;\n      }\n    } finally {\n      IOUtils.cleanup(FSNamesystem.LOG, bufCurrent, bufReady, fc, fp);\n      bufCurrent \u003d bufReady \u003d null;\n      fc \u003d null;\n      fp \u003d null;\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
      "extendedDetails": {}
    },
    "7bd41f031f3852ac44b911a71d1d57ea3629b134": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2011. Removal and restoration of storage directories on checkpointing failure doesn\u0027t work properly. Contributed by Ravi Prakash.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1141748 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/06/11 4:21 PM",
      "commitName": "7bd41f031f3852ac44b911a71d1d57ea3629b134",
      "commitAuthor": "Matthew Foley",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 18.06,
      "commitsBetweenForRepo": 62,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,29 @@\n   public void close() throws IOException {\n     // close should have been called after all pending transactions\n     // have been flushed \u0026 synced.\n-    int bufSize \u003d bufCurrent.size();\n-    if (bufSize !\u003d 0) {\n-      throw new IOException(\"FSEditStream has \" + bufSize\n-          + \" bytes still to be flushed and cannot \" + \"be closed.\");\n+    // if already closed, just skip\n+    if(bufCurrent !\u003d null)\n+    {\n+      int bufSize \u003d bufCurrent.size();\n+      if (bufSize !\u003d 0) {\n+        throw new IOException(\"FSEditStream has \" + bufSize\n+            + \" bytes still to be flushed and cannot \" + \"be closed.\");\n+      }\n+      bufCurrent.close();\n+      bufCurrent \u003d null;\n     }\n-    bufCurrent.close();\n-    bufReady.close();\n+\n+    if(bufReady !\u003d null) {\n+      bufReady.close();\n+      bufReady \u003d null;\n+    }\n \n     // remove the last INVALID marker from transaction log.\n-    fc.truncate(fc.position());\n-    fp.close();\n-\n-    bufCurrent \u003d bufReady \u003d null;\n+    if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n+      fc.truncate(fc.position());\n+      fc.close();\n+    }\n+    if (fp !\u003d null) {\n+      fp.close();\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    // close should have been called after all pending transactions\n    // have been flushed \u0026 synced.\n    // if already closed, just skip\n    if(bufCurrent !\u003d null)\n    {\n      int bufSize \u003d bufCurrent.size();\n      if (bufSize !\u003d 0) {\n        throw new IOException(\"FSEditStream has \" + bufSize\n            + \" bytes still to be flushed and cannot \" + \"be closed.\");\n      }\n      bufCurrent.close();\n      bufCurrent \u003d null;\n    }\n\n    if(bufReady !\u003d null) {\n      bufReady.close();\n      bufReady \u003d null;\n    }\n\n    // remove the last INVALID marker from transaction log.\n    if (fc !\u003d null \u0026\u0026 fc.isOpen()) {\n      fc.truncate(fc.position());\n      fc.close();\n    }\n    if (fp !\u003d null) {\n      fp.close();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,17 @@\n+  public void close() throws IOException {\n+    // close should have been called after all pending transactions\n+    // have been flushed \u0026 synced.\n+    int bufSize \u003d bufCurrent.size();\n+    if (bufSize !\u003d 0) {\n+      throw new IOException(\"FSEditStream has \" + bufSize\n+          + \" bytes still to be flushed and cannot \" + \"be closed.\");\n+    }\n+    bufCurrent.close();\n+    bufReady.close();\n+\n+    // remove the last INVALID marker from transaction log.\n+    fc.truncate(fc.position());\n+    fp.close();\n+\n+    bufCurrent \u003d bufReady \u003d null;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    // close should have been called after all pending transactions\n    // have been flushed \u0026 synced.\n    int bufSize \u003d bufCurrent.size();\n    if (bufSize !\u003d 0) {\n      throw new IOException(\"FSEditStream has \" + bufSize\n          + \" bytes still to be flushed and cannot \" + \"be closed.\");\n    }\n    bufCurrent.close();\n    bufReady.close();\n\n    // remove the last INVALID marker from transaction log.\n    fc.truncate(fc.position());\n    fp.close();\n\n    bufCurrent \u003d bufReady \u003d null;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java"
    }
  }
}