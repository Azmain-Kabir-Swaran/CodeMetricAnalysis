{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSImageLoader.java",
  "functionName": "loadINodeDirectorySection",
  "functionId": "loadINodeDirectorySection___in-InputStream__refIdList-List__Long__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java",
  "functionStartLine": 186,
  "functionEndLine": 213,
  "numCommitsSeen": 29,
  "timeTaken": 1821,
  "changeHistory": [
    "1f5b42ac4881b734c799bfb527884c0d117929bd",
    "54c1daa580e1cceee541be86fc2b694fa1be26df"
  ],
  "changeHistoryShort": {
    "1f5b42ac4881b734c799bfb527884c0d117929bd": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "54c1daa580e1cceee541be86fc2b694fa1be26df": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1f5b42ac4881b734c799bfb527884c0d117929bd": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-7158. Reduce the memory usage of WebImageViewer. Contributed by Haohui Mai.\n",
      "commitDate": "01/10/14 10:53 AM",
      "commitName": "1f5b42ac4881b734c799bfb527884c0d117929bd",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7158. Reduce the memory usage of WebImageViewer. Contributed by Haohui Mai.\n",
          "commitDate": "01/10/14 10:53 AM",
          "commitName": "1f5b42ac4881b734c799bfb527884c0d117929bd",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "29/09/14 10:27 PM",
          "commitNameOld": "bb84f1fccb18c6c7373851e05d2451d55e908242",
          "commitAuthorOld": "arp",
          "daysBetweenCommits": 1.52,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,28 @@\n-  private static void loadINodeDirectorySection(InputStream in)\n+  private static Map\u003cLong, long[]\u003e loadINodeDirectorySection\n+          (InputStream in, List\u003cLong\u003e refIdList)\n       throws IOException {\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Loading directory section\");\n-    }\n+    LOG.info(\"Loading inode directory section\");\n+    Map\u003cLong, long[]\u003e dirs \u003d Maps.newHashMap();\n+    long counter \u003d 0;\n     while (true) {\n       FsImageProto.INodeDirectorySection.DirEntry e \u003d\n           FsImageProto.INodeDirectorySection.DirEntry.parseDelimitedFrom(in);\n       // note that in is a LimitedInputStream\n       if (e \u003d\u003d null) {\n         break;\n       }\n+      ++counter;\n+\n       long[] l \u003d new long[e.getChildrenCount() + e.getRefChildrenCount()];\n       for (int i \u003d 0; i \u003c e.getChildrenCount(); ++i) {\n         l[i] \u003d e.getChildren(i);\n       }\n       for (int i \u003d e.getChildrenCount(); i \u003c l.length; i++) {\n         int refId \u003d e.getRefChildren(i - e.getChildrenCount());\n-        l[i] \u003d refList.get(refId).getReferredId();\n+        l[i] \u003d refIdList.get(refId);\n       }\n-      dirmap.put(e.getParent(), l);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Loaded directory (parent \" + e.getParent()\n-            + \") with \" + e.getChildrenCount() + \" children and \"\n-            + e.getRefChildrenCount() + \" reference children\");\n-      }\n+      dirs.put(e.getParent(), l);\n     }\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Loaded \" + dirmap.size() + \" directories\");\n-    }\n+    LOG.info(\"Loaded \" + counter + \" directories\");\n+    return dirs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static Map\u003cLong, long[]\u003e loadINodeDirectorySection\n          (InputStream in, List\u003cLong\u003e refIdList)\n      throws IOException {\n    LOG.info(\"Loading inode directory section\");\n    Map\u003cLong, long[]\u003e dirs \u003d Maps.newHashMap();\n    long counter \u003d 0;\n    while (true) {\n      FsImageProto.INodeDirectorySection.DirEntry e \u003d\n          FsImageProto.INodeDirectorySection.DirEntry.parseDelimitedFrom(in);\n      // note that in is a LimitedInputStream\n      if (e \u003d\u003d null) {\n        break;\n      }\n      ++counter;\n\n      long[] l \u003d new long[e.getChildrenCount() + e.getRefChildrenCount()];\n      for (int i \u003d 0; i \u003c e.getChildrenCount(); ++i) {\n        l[i] \u003d e.getChildren(i);\n      }\n      for (int i \u003d e.getChildrenCount(); i \u003c l.length; i++) {\n        int refId \u003d e.getRefChildren(i - e.getChildrenCount());\n        l[i] \u003d refIdList.get(refId);\n      }\n      dirs.put(e.getParent(), l);\n    }\n    LOG.info(\"Loaded \" + counter + \" directories\");\n    return dirs;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java",
          "extendedDetails": {
            "oldValue": "[in-InputStream]",
            "newValue": "[in-InputStream, refIdList-List\u003cLong\u003e]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-7158. Reduce the memory usage of WebImageViewer. Contributed by Haohui Mai.\n",
          "commitDate": "01/10/14 10:53 AM",
          "commitName": "1f5b42ac4881b734c799bfb527884c0d117929bd",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "29/09/14 10:27 PM",
          "commitNameOld": "bb84f1fccb18c6c7373851e05d2451d55e908242",
          "commitAuthorOld": "arp",
          "daysBetweenCommits": 1.52,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,28 @@\n-  private static void loadINodeDirectorySection(InputStream in)\n+  private static Map\u003cLong, long[]\u003e loadINodeDirectorySection\n+          (InputStream in, List\u003cLong\u003e refIdList)\n       throws IOException {\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Loading directory section\");\n-    }\n+    LOG.info(\"Loading inode directory section\");\n+    Map\u003cLong, long[]\u003e dirs \u003d Maps.newHashMap();\n+    long counter \u003d 0;\n     while (true) {\n       FsImageProto.INodeDirectorySection.DirEntry e \u003d\n           FsImageProto.INodeDirectorySection.DirEntry.parseDelimitedFrom(in);\n       // note that in is a LimitedInputStream\n       if (e \u003d\u003d null) {\n         break;\n       }\n+      ++counter;\n+\n       long[] l \u003d new long[e.getChildrenCount() + e.getRefChildrenCount()];\n       for (int i \u003d 0; i \u003c e.getChildrenCount(); ++i) {\n         l[i] \u003d e.getChildren(i);\n       }\n       for (int i \u003d e.getChildrenCount(); i \u003c l.length; i++) {\n         int refId \u003d e.getRefChildren(i - e.getChildrenCount());\n-        l[i] \u003d refList.get(refId).getReferredId();\n+        l[i] \u003d refIdList.get(refId);\n       }\n-      dirmap.put(e.getParent(), l);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Loaded directory (parent \" + e.getParent()\n-            + \") with \" + e.getChildrenCount() + \" children and \"\n-            + e.getRefChildrenCount() + \" reference children\");\n-      }\n+      dirs.put(e.getParent(), l);\n     }\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Loaded \" + dirmap.size() + \" directories\");\n-    }\n+    LOG.info(\"Loaded \" + counter + \" directories\");\n+    return dirs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static Map\u003cLong, long[]\u003e loadINodeDirectorySection\n          (InputStream in, List\u003cLong\u003e refIdList)\n      throws IOException {\n    LOG.info(\"Loading inode directory section\");\n    Map\u003cLong, long[]\u003e dirs \u003d Maps.newHashMap();\n    long counter \u003d 0;\n    while (true) {\n      FsImageProto.INodeDirectorySection.DirEntry e \u003d\n          FsImageProto.INodeDirectorySection.DirEntry.parseDelimitedFrom(in);\n      // note that in is a LimitedInputStream\n      if (e \u003d\u003d null) {\n        break;\n      }\n      ++counter;\n\n      long[] l \u003d new long[e.getChildrenCount() + e.getRefChildrenCount()];\n      for (int i \u003d 0; i \u003c e.getChildrenCount(); ++i) {\n        l[i] \u003d e.getChildren(i);\n      }\n      for (int i \u003d e.getChildrenCount(); i \u003c l.length; i++) {\n        int refId \u003d e.getRefChildren(i - e.getChildrenCount());\n        l[i] \u003d refIdList.get(refId);\n      }\n      dirs.put(e.getParent(), l);\n    }\n    LOG.info(\"Loaded \" + counter + \" directories\");\n    return dirs;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "Map\u003cLong,long[]\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7158. Reduce the memory usage of WebImageViewer. Contributed by Haohui Mai.\n",
          "commitDate": "01/10/14 10:53 AM",
          "commitName": "1f5b42ac4881b734c799bfb527884c0d117929bd",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "29/09/14 10:27 PM",
          "commitNameOld": "bb84f1fccb18c6c7373851e05d2451d55e908242",
          "commitAuthorOld": "arp",
          "daysBetweenCommits": 1.52,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,28 @@\n-  private static void loadINodeDirectorySection(InputStream in)\n+  private static Map\u003cLong, long[]\u003e loadINodeDirectorySection\n+          (InputStream in, List\u003cLong\u003e refIdList)\n       throws IOException {\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Loading directory section\");\n-    }\n+    LOG.info(\"Loading inode directory section\");\n+    Map\u003cLong, long[]\u003e dirs \u003d Maps.newHashMap();\n+    long counter \u003d 0;\n     while (true) {\n       FsImageProto.INodeDirectorySection.DirEntry e \u003d\n           FsImageProto.INodeDirectorySection.DirEntry.parseDelimitedFrom(in);\n       // note that in is a LimitedInputStream\n       if (e \u003d\u003d null) {\n         break;\n       }\n+      ++counter;\n+\n       long[] l \u003d new long[e.getChildrenCount() + e.getRefChildrenCount()];\n       for (int i \u003d 0; i \u003c e.getChildrenCount(); ++i) {\n         l[i] \u003d e.getChildren(i);\n       }\n       for (int i \u003d e.getChildrenCount(); i \u003c l.length; i++) {\n         int refId \u003d e.getRefChildren(i - e.getChildrenCount());\n-        l[i] \u003d refList.get(refId).getReferredId();\n+        l[i] \u003d refIdList.get(refId);\n       }\n-      dirmap.put(e.getParent(), l);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Loaded directory (parent \" + e.getParent()\n-            + \") with \" + e.getChildrenCount() + \" children and \"\n-            + e.getRefChildrenCount() + \" reference children\");\n-      }\n+      dirs.put(e.getParent(), l);\n     }\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Loaded \" + dirmap.size() + \" directories\");\n-    }\n+    LOG.info(\"Loaded \" + counter + \" directories\");\n+    return dirs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static Map\u003cLong, long[]\u003e loadINodeDirectorySection\n          (InputStream in, List\u003cLong\u003e refIdList)\n      throws IOException {\n    LOG.info(\"Loading inode directory section\");\n    Map\u003cLong, long[]\u003e dirs \u003d Maps.newHashMap();\n    long counter \u003d 0;\n    while (true) {\n      FsImageProto.INodeDirectorySection.DirEntry e \u003d\n          FsImageProto.INodeDirectorySection.DirEntry.parseDelimitedFrom(in);\n      // note that in is a LimitedInputStream\n      if (e \u003d\u003d null) {\n        break;\n      }\n      ++counter;\n\n      long[] l \u003d new long[e.getChildrenCount() + e.getRefChildrenCount()];\n      for (int i \u003d 0; i \u003c e.getChildrenCount(); ++i) {\n        l[i] \u003d e.getChildren(i);\n      }\n      for (int i \u003d e.getChildrenCount(); i \u003c l.length; i++) {\n        int refId \u003d e.getRefChildren(i - e.getChildrenCount());\n        l[i] \u003d refIdList.get(refId);\n      }\n      dirs.put(e.getParent(), l);\n    }\n    LOG.info(\"Loaded \" + counter + \" directories\");\n    return dirs;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java",
          "extendedDetails": {}
        }
      ]
    },
    "54c1daa580e1cceee541be86fc2b694fa1be26df": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5978. Create a tool to take fsimage and expose read-only WebHDFS API. Contributed by Akira Ajisaka.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582433 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/03/14 11:15 AM",
      "commitName": "54c1daa580e1cceee541be86fc2b694fa1be26df",
      "commitAuthor": "Haohui Mai",
      "diff": "@@ -0,0 +1,31 @@\n+  private static void loadINodeDirectorySection(InputStream in)\n+      throws IOException {\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Loading directory section\");\n+    }\n+    while (true) {\n+      FsImageProto.INodeDirectorySection.DirEntry e \u003d\n+          FsImageProto.INodeDirectorySection.DirEntry.parseDelimitedFrom(in);\n+      // note that in is a LimitedInputStream\n+      if (e \u003d\u003d null) {\n+        break;\n+      }\n+      long[] l \u003d new long[e.getChildrenCount() + e.getRefChildrenCount()];\n+      for (int i \u003d 0; i \u003c e.getChildrenCount(); ++i) {\n+        l[i] \u003d e.getChildren(i);\n+      }\n+      for (int i \u003d e.getChildrenCount(); i \u003c l.length; i++) {\n+        int refId \u003d e.getRefChildren(i - e.getChildrenCount());\n+        l[i] \u003d refList.get(refId).getReferredId();\n+      }\n+      dirmap.put(e.getParent(), l);\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Loaded directory (parent \" + e.getParent()\n+            + \") with \" + e.getChildrenCount() + \" children and \"\n+            + e.getRefChildrenCount() + \" reference children\");\n+      }\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Loaded \" + dirmap.size() + \" directories\");\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static void loadINodeDirectorySection(InputStream in)\n      throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Loading directory section\");\n    }\n    while (true) {\n      FsImageProto.INodeDirectorySection.DirEntry e \u003d\n          FsImageProto.INodeDirectorySection.DirEntry.parseDelimitedFrom(in);\n      // note that in is a LimitedInputStream\n      if (e \u003d\u003d null) {\n        break;\n      }\n      long[] l \u003d new long[e.getChildrenCount() + e.getRefChildrenCount()];\n      for (int i \u003d 0; i \u003c e.getChildrenCount(); ++i) {\n        l[i] \u003d e.getChildren(i);\n      }\n      for (int i \u003d e.getChildrenCount(); i \u003c l.length; i++) {\n        int refId \u003d e.getRefChildren(i - e.getChildrenCount());\n        l[i] \u003d refList.get(refId).getReferredId();\n      }\n      dirmap.put(e.getParent(), l);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Loaded directory (parent \" + e.getParent()\n            + \") with \" + e.getChildrenCount() + \" children and \"\n            + e.getRefChildrenCount() + \" reference children\");\n      }\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Loaded \" + dirmap.size() + \" directories\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java"
    }
  }
}