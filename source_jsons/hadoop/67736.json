{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DynamicRecordReader.java",
  "functionName": "nextKeyValue",
  "functionId": "nextKeyValue",
  "sourceFilePath": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java",
  "functionStartLine": 100,
  "functionEndLine": 133,
  "numCommitsSeen": 4,
  "timeTaken": 789,
  "changeHistory": [
    "2868ca0328d908056745223fb38d9a90fd2811ba",
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67"
  ],
  "changeHistoryShort": {
    "2868ca0328d908056745223fb38d9a90fd2811ba": "Ybodychange",
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67": "Yintroduced"
  },
  "changeHistoryDetails": {
    "2868ca0328d908056745223fb38d9a90fd2811ba": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6451. DistCp has incorrect chunkFilePath for multiple jobs when strategy is dynamic. Contributed by Kuhu Shukla.\n",
      "commitDate": "30/10/15 12:56 PM",
      "commitName": "2868ca0328d908056745223fb38d9a90fd2811ba",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "03/03/15 1:06 AM",
      "commitNameOld": "9ae7f9eb7baeb244e1b95aabc93ad8124870b9a9",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 241.45,
      "commitsBetweenForRepo": 2078,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   public boolean nextKeyValue()\n       throws IOException, InterruptedException {\n \n     if (chunk \u003d\u003d null) {\n       if (LOG.isDebugEnabled())\n         LOG.debug(taskId + \": RecordReader is null. No records to be read.\");\n       return false;\n     }\n \n     if (chunk.getReader().nextKeyValue()) {\n       ++numRecordsProcessedByThisMap;\n       return true;\n     }\n \n     if (LOG.isDebugEnabled())\n       LOG.debug(taskId + \": Current chunk exhausted. \" +\n                          \" Attempting to pick up new one.\");\n \n     chunk.release();\n     timeOfLastChunkDirScan \u003d System.currentTimeMillis();\n     isChunkDirAlreadyScanned \u003d false;\n     \n-    chunk \u003d DynamicInputChunk.acquire(taskAttemptContext);\n+    chunk \u003d chunkContext.acquire(taskAttemptContext);\n \n     if (chunk \u003d\u003d null) return false;\n \n     if (chunk.getReader().nextKeyValue()) {\n       ++numRecordsProcessedByThisMap;\n       return true;\n     }\n     else {\n       return false;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean nextKeyValue()\n      throws IOException, InterruptedException {\n\n    if (chunk \u003d\u003d null) {\n      if (LOG.isDebugEnabled())\n        LOG.debug(taskId + \": RecordReader is null. No records to be read.\");\n      return false;\n    }\n\n    if (chunk.getReader().nextKeyValue()) {\n      ++numRecordsProcessedByThisMap;\n      return true;\n    }\n\n    if (LOG.isDebugEnabled())\n      LOG.debug(taskId + \": Current chunk exhausted. \" +\n                         \" Attempting to pick up new one.\");\n\n    chunk.release();\n    timeOfLastChunkDirScan \u003d System.currentTimeMillis();\n    isChunkDirAlreadyScanned \u003d false;\n    \n    chunk \u003d chunkContext.acquire(taskAttemptContext);\n\n    if (chunk \u003d\u003d null) return false;\n\n    if (chunk.getReader().nextKeyValue()) {\n      ++numRecordsProcessedByThisMap;\n      return true;\n    }\n    else {\n      return false;\n    }\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java",
      "extendedDetails": {}
    },
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1236045 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/12 10:36 PM",
      "commitName": "d06948002fb0cabf72cc0d46bf2fa67d45370f67",
      "commitAuthor": "Mahadev Konar",
      "diff": "@@ -0,0 +1,34 @@\n+  public boolean nextKeyValue()\n+      throws IOException, InterruptedException {\n+\n+    if (chunk \u003d\u003d null) {\n+      if (LOG.isDebugEnabled())\n+        LOG.debug(taskId + \": RecordReader is null. No records to be read.\");\n+      return false;\n+    }\n+\n+    if (chunk.getReader().nextKeyValue()) {\n+      ++numRecordsProcessedByThisMap;\n+      return true;\n+    }\n+\n+    if (LOG.isDebugEnabled())\n+      LOG.debug(taskId + \": Current chunk exhausted. \" +\n+                         \" Attempting to pick up new one.\");\n+\n+    chunk.release();\n+    timeOfLastChunkDirScan \u003d System.currentTimeMillis();\n+    isChunkDirAlreadyScanned \u003d false;\n+    \n+    chunk \u003d DynamicInputChunk.acquire(taskAttemptContext);\n+\n+    if (chunk \u003d\u003d null) return false;\n+\n+    if (chunk.getReader().nextKeyValue()) {\n+      ++numRecordsProcessedByThisMap;\n+      return true;\n+    }\n+    else {\n+      return false;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean nextKeyValue()\n      throws IOException, InterruptedException {\n\n    if (chunk \u003d\u003d null) {\n      if (LOG.isDebugEnabled())\n        LOG.debug(taskId + \": RecordReader is null. No records to be read.\");\n      return false;\n    }\n\n    if (chunk.getReader().nextKeyValue()) {\n      ++numRecordsProcessedByThisMap;\n      return true;\n    }\n\n    if (LOG.isDebugEnabled())\n      LOG.debug(taskId + \": Current chunk exhausted. \" +\n                         \" Attempting to pick up new one.\");\n\n    chunk.release();\n    timeOfLastChunkDirScan \u003d System.currentTimeMillis();\n    isChunkDirAlreadyScanned \u003d false;\n    \n    chunk \u003d DynamicInputChunk.acquire(taskAttemptContext);\n\n    if (chunk \u003d\u003d null) return false;\n\n    if (chunk.getReader().nextKeyValue()) {\n      ++numRecordsProcessedByThisMap;\n      return true;\n    }\n    else {\n      return false;\n    }\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java"
    }
  }
}