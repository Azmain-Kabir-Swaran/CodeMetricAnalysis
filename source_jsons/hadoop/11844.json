{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsVolumeImpl.java",
  "functionName": "copyReplicaWithNewBlockIdAndGS",
  "functionId": "copyReplicaWithNewBlockIdAndGS___replicaInfo-ReplicaInfo__bpid-String__newBlkId-long__newGS-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java",
  "functionStartLine": 1308,
  "functionEndLine": 1319,
  "numCommitsSeen": 204,
  "timeTaken": 8488,
  "changeHistory": [
    "86c9862bec0248d671e657aa56094a2919b8ac14",
    "96d307e1e320eafb470faf7bd47af3341c399d55",
    "c992bcf9c136d3df686655a80e636bb7bb0664da",
    "4da8490b512a33a255ed27309860859388d7c168",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f"
  ],
  "changeHistoryShort": {
    "86c9862bec0248d671e657aa56094a2919b8ac14": "Ymultichange(Ymovefromfile,Ybodychange,Yparameterchange)",
    "96d307e1e320eafb470faf7bd47af3341c399d55": "Ybodychange",
    "c992bcf9c136d3df686655a80e636bb7bb0664da": "Ybodychange",
    "4da8490b512a33a255ed27309860859388d7c168": "Ybodychange",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": "Ybodychange",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "86c9862bec0248d671e657aa56094a2919b8ac14": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "13/09/16 12:54 PM",
      "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
      "commitAuthor": "Lei Xu",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
          "commitDate": "13/09/16 12:54 PM",
          "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "13/09/16 12:42 PM",
          "commitNameOld": "1c0d18f32289837b8981aed80e7bdcd360adfb85",
          "commitAuthorOld": "Anu Engineer",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,13 +1,12 @@\n   private File[] copyReplicaWithNewBlockIdAndGS(\n-      ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n+      ReplicaInfo replicaInfo, String bpid, long newBlkId, long newGS)\n       throws IOException {\n     String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n     FsVolumeImpl v \u003d (FsVolumeImpl) replicaInfo.getVolume();\n     final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n     final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n     final File dstBlockFile \u003d new File(destDir, blockFileName);\n     final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n-    return copyBlockFiles(replicaInfo.getMetaFile(),\n-        replicaInfo.getBlockFile(),\n-        dstMetaFile, dstBlockFile, true, smallBufferSize, conf);\n+    return FsDatasetImpl.copyBlockFiles(replicaInfo, dstMetaFile,\n+        dstBlockFile, true, DFSUtilClient.getSmallBufferSize(conf), conf);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private File[] copyReplicaWithNewBlockIdAndGS(\n      ReplicaInfo replicaInfo, String bpid, long newBlkId, long newGS)\n      throws IOException {\n    String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n    FsVolumeImpl v \u003d (FsVolumeImpl) replicaInfo.getVolume();\n    final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n    final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n    final File dstBlockFile \u003d new File(destDir, blockFileName);\n    final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n    return FsDatasetImpl.copyBlockFiles(replicaInfo, dstMetaFile,\n        dstBlockFile, true, DFSUtilClient.getSmallBufferSize(conf), conf);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java",
            "oldMethodName": "copyReplicaWithNewBlockIdAndGS",
            "newMethodName": "copyReplicaWithNewBlockIdAndGS"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
          "commitDate": "13/09/16 12:54 PM",
          "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "13/09/16 12:42 PM",
          "commitNameOld": "1c0d18f32289837b8981aed80e7bdcd360adfb85",
          "commitAuthorOld": "Anu Engineer",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,13 +1,12 @@\n   private File[] copyReplicaWithNewBlockIdAndGS(\n-      ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n+      ReplicaInfo replicaInfo, String bpid, long newBlkId, long newGS)\n       throws IOException {\n     String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n     FsVolumeImpl v \u003d (FsVolumeImpl) replicaInfo.getVolume();\n     final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n     final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n     final File dstBlockFile \u003d new File(destDir, blockFileName);\n     final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n-    return copyBlockFiles(replicaInfo.getMetaFile(),\n-        replicaInfo.getBlockFile(),\n-        dstMetaFile, dstBlockFile, true, smallBufferSize, conf);\n+    return FsDatasetImpl.copyBlockFiles(replicaInfo, dstMetaFile,\n+        dstBlockFile, true, DFSUtilClient.getSmallBufferSize(conf), conf);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private File[] copyReplicaWithNewBlockIdAndGS(\n      ReplicaInfo replicaInfo, String bpid, long newBlkId, long newGS)\n      throws IOException {\n    String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n    FsVolumeImpl v \u003d (FsVolumeImpl) replicaInfo.getVolume();\n    final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n    final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n    final File dstBlockFile \u003d new File(destDir, blockFileName);\n    final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n    return FsDatasetImpl.copyBlockFiles(replicaInfo, dstMetaFile,\n        dstBlockFile, true, DFSUtilClient.getSmallBufferSize(conf), conf);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
          "commitDate": "13/09/16 12:54 PM",
          "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "13/09/16 12:42 PM",
          "commitNameOld": "1c0d18f32289837b8981aed80e7bdcd360adfb85",
          "commitAuthorOld": "Anu Engineer",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,13 +1,12 @@\n   private File[] copyReplicaWithNewBlockIdAndGS(\n-      ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n+      ReplicaInfo replicaInfo, String bpid, long newBlkId, long newGS)\n       throws IOException {\n     String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n     FsVolumeImpl v \u003d (FsVolumeImpl) replicaInfo.getVolume();\n     final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n     final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n     final File dstBlockFile \u003d new File(destDir, blockFileName);\n     final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n-    return copyBlockFiles(replicaInfo.getMetaFile(),\n-        replicaInfo.getBlockFile(),\n-        dstMetaFile, dstBlockFile, true, smallBufferSize, conf);\n+    return FsDatasetImpl.copyBlockFiles(replicaInfo, dstMetaFile,\n+        dstBlockFile, true, DFSUtilClient.getSmallBufferSize(conf), conf);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private File[] copyReplicaWithNewBlockIdAndGS(\n      ReplicaInfo replicaInfo, String bpid, long newBlkId, long newGS)\n      throws IOException {\n    String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n    FsVolumeImpl v \u003d (FsVolumeImpl) replicaInfo.getVolume();\n    final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n    final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n    final File dstBlockFile \u003d new File(destDir, blockFileName);\n    final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n    return FsDatasetImpl.copyBlockFiles(replicaInfo, dstMetaFile,\n        dstBlockFile, true, DFSUtilClient.getSmallBufferSize(conf), conf);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java",
          "extendedDetails": {
            "oldValue": "[replicaInfo-ReplicaUnderRecovery, bpid-String, newBlkId-long, newGS-long]",
            "newValue": "[replicaInfo-ReplicaInfo, bpid-String, newBlkId-long, newGS-long]"
          }
        }
      ]
    },
    "96d307e1e320eafb470faf7bd47af3341c399d55": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9516. Truncate file fails with data dirs on multiple disks. Contributed by Plamen Jeliazkov.\n",
      "commitDate": "15/12/15 12:32 AM",
      "commitName": "96d307e1e320eafb470faf7bd47af3341c399d55",
      "commitAuthor": "Plamen Jeliazkov",
      "commitDateOld": "14/12/15 11:25 AM",
      "commitNameOld": "de522d2cd46be13806d13aa5f373b310e0ad6693",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 0.55,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,13 @@\n   private File[] copyReplicaWithNewBlockIdAndGS(\n       ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n       throws IOException {\n     String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n-    try (FsVolumeReference ref \u003d volumes.getNextVolume(\n-        replicaInfo.getVolume().getStorageType(), replicaInfo.getNumBytes())) {\n-      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n-      final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n-      final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n-      final File dstBlockFile \u003d new File(destDir, blockFileName);\n-      final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n-      return copyBlockFiles(replicaInfo.getMetaFile(),\n-          replicaInfo.getBlockFile(),\n-          dstMetaFile, dstBlockFile, true, smallBufferSize, conf);\n-    }\n+    FsVolumeImpl v \u003d (FsVolumeImpl) replicaInfo.getVolume();\n+    final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n+    final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n+    final File dstBlockFile \u003d new File(destDir, blockFileName);\n+    final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n+    return copyBlockFiles(replicaInfo.getMetaFile(),\n+        replicaInfo.getBlockFile(),\n+        dstMetaFile, dstBlockFile, true, smallBufferSize, conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private File[] copyReplicaWithNewBlockIdAndGS(\n      ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n      throws IOException {\n    String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n    FsVolumeImpl v \u003d (FsVolumeImpl) replicaInfo.getVolume();\n    final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n    final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n    final File dstBlockFile \u003d new File(destDir, blockFileName);\n    final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n    return copyBlockFiles(replicaInfo.getMetaFile(),\n        replicaInfo.getBlockFile(),\n        dstMetaFile, dstBlockFile, true, smallBufferSize, conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "c992bcf9c136d3df686655a80e636bb7bb0664da": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8951. Move the shortcircuit package to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/08/15 2:02 PM",
      "commitName": "c992bcf9c136d3df686655a80e636bb7bb0664da",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "17/08/15 5:40 PM",
      "commitNameOld": "eee4d716b48074825e1afcd9c74038a393ddeb69",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 8.85,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   private File[] copyReplicaWithNewBlockIdAndGS(\n       ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n       throws IOException {\n     String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n     try (FsVolumeReference ref \u003d volumes.getNextVolume(\n         replicaInfo.getVolume().getStorageType(), replicaInfo.getNumBytes())) {\n       FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n       final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n       final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n       final File dstBlockFile \u003d new File(destDir, blockFileName);\n       final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n       return copyBlockFiles(replicaInfo.getMetaFile(),\n           replicaInfo.getBlockFile(),\n-          dstMetaFile, dstBlockFile, true, smallBufferSize);\n+          dstMetaFile, dstBlockFile, true, smallBufferSize, conf);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private File[] copyReplicaWithNewBlockIdAndGS(\n      ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n      throws IOException {\n    String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n    try (FsVolumeReference ref \u003d volumes.getNextVolume(\n        replicaInfo.getVolume().getStorageType(), replicaInfo.getNumBytes())) {\n      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n      final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n      final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n      final File dstBlockFile \u003d new File(destDir, blockFileName);\n      final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n      return copyBlockFiles(replicaInfo.getMetaFile(),\n          replicaInfo.getBlockFile(),\n          dstMetaFile, dstBlockFile, true, smallBufferSize, conf);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "4da8490b512a33a255ed27309860859388d7c168": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
      "commitDate": "05/05/15 3:41 PM",
      "commitName": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "05/05/15 11:08 AM",
      "commitNameOld": "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.19,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   private File[] copyReplicaWithNewBlockIdAndGS(\n       ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n       throws IOException {\n     String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n     try (FsVolumeReference ref \u003d volumes.getNextVolume(\n         replicaInfo.getVolume().getStorageType(), replicaInfo.getNumBytes())) {\n       FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n       final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n       final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n       final File dstBlockFile \u003d new File(destDir, blockFileName);\n       final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n       return copyBlockFiles(replicaInfo.getMetaFile(),\n           replicaInfo.getBlockFile(),\n-          dstMetaFile, dstBlockFile, true);\n+          dstMetaFile, dstBlockFile, true, smallBufferSize);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private File[] copyReplicaWithNewBlockIdAndGS(\n      ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n      throws IOException {\n    String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n    try (FsVolumeReference ref \u003d volumes.getNextVolume(\n        replicaInfo.getVolume().getStorageType(), replicaInfo.getNumBytes())) {\n      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n      final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n      final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n      final File dstBlockFile \u003d new File(destDir, blockFileName);\n      final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n      return copyBlockFiles(replicaInfo.getMetaFile(),\n          replicaInfo.getBlockFile(),\n          dstMetaFile, dstBlockFile, true, smallBufferSize);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)\n",
      "commitDate": "20/01/15 7:05 PM",
      "commitName": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "13/01/15 12:24 AM",
      "commitNameOld": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 7.78,
      "commitsBetweenForRepo": 49,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,16 @@\n   private File[] copyReplicaWithNewBlockIdAndGS(\n       ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n       throws IOException {\n     String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n-    FsVolumeImpl v \u003d volumes.getNextVolume(\n-        replicaInfo.getVolume().getStorageType(), replicaInfo.getNumBytes());\n-    final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n-    final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n-    final File dstBlockFile \u003d new File(destDir, blockFileName);\n-    final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n-    return copyBlockFiles(replicaInfo.getMetaFile(), replicaInfo.getBlockFile(),\n-        dstMetaFile, dstBlockFile, true);\n+    try (FsVolumeReference ref \u003d volumes.getNextVolume(\n+        replicaInfo.getVolume().getStorageType(), replicaInfo.getNumBytes())) {\n+      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n+      final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n+      final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n+      final File dstBlockFile \u003d new File(destDir, blockFileName);\n+      final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n+      return copyBlockFiles(replicaInfo.getMetaFile(),\n+          replicaInfo.getBlockFile(),\n+          dstMetaFile, dstBlockFile, true);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private File[] copyReplicaWithNewBlockIdAndGS(\n      ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n      throws IOException {\n    String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n    try (FsVolumeReference ref \u003d volumes.getNextVolume(\n        replicaInfo.getVolume().getStorageType(), replicaInfo.getNumBytes())) {\n      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n      final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n      final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n      final File dstBlockFile \u003d new File(destDir, blockFileName);\n      final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n      return copyBlockFiles(replicaInfo.getMetaFile(),\n          replicaInfo.getBlockFile(),\n          dstMetaFile, dstBlockFile, true);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7056. Snapshot support for truncate. Contributed by Konstantin Shvachko and Plamen Jeliazkov.",
      "commitDate": "13/01/15 12:24 AM",
      "commitName": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthor": "Konstantin V Shvachko",
      "diff": "@@ -0,0 +1,13 @@\n+  private File[] copyReplicaWithNewBlockIdAndGS(\n+      ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n+      throws IOException {\n+    String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n+    FsVolumeImpl v \u003d volumes.getNextVolume(\n+        replicaInfo.getVolume().getStorageType(), replicaInfo.getNumBytes());\n+    final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n+    final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n+    final File dstBlockFile \u003d new File(destDir, blockFileName);\n+    final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n+    return copyBlockFiles(replicaInfo.getMetaFile(), replicaInfo.getBlockFile(),\n+        dstMetaFile, dstBlockFile, true);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private File[] copyReplicaWithNewBlockIdAndGS(\n      ReplicaUnderRecovery replicaInfo, String bpid, long newBlkId, long newGS)\n      throws IOException {\n    String blockFileName \u003d Block.BLOCK_FILE_PREFIX + newBlkId;\n    FsVolumeImpl v \u003d volumes.getNextVolume(\n        replicaInfo.getVolume().getStorageType(), replicaInfo.getNumBytes());\n    final File tmpDir \u003d v.getBlockPoolSlice(bpid).getTmpDir();\n    final File destDir \u003d DatanodeUtil.idToBlockDir(tmpDir, newBlkId);\n    final File dstBlockFile \u003d new File(destDir, blockFileName);\n    final File dstMetaFile \u003d FsDatasetUtil.getMetaFile(dstBlockFile, newGS);\n    return copyBlockFiles(replicaInfo.getMetaFile(), replicaInfo.getBlockFile(),\n        dstMetaFile, dstBlockFile, true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java"
    }
  }
}