{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirDeleteOp.java",
  "functionName": "delete",
  "functionId": "delete___fsd-FSDirectory__iip-INodesInPath__collectedBlocks-BlocksMapUpdateInfo__removedINodes-List__INode____removedUCFiles-List__Long____mtime-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
  "functionStartLine": 46,
  "functionEndLine": 74,
  "numCommitsSeen": 737,
  "timeTaken": 11431,
  "changeHistory": [
    "743c2e9071f4a73e0196ad4ca005b767758642b9",
    "3565c9af17ab05bf9e7f68b71b6c6850df772bb9",
    "44f48ee96ee6b2a3909911c37bfddb0c963d5ffc",
    "745d04be59accf80feda0ad38efcc74ba362f2ca",
    "b2c85db86c9a62b0a03ee87547265077f664970a",
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
    "4a3161182905afaf450a60d02528161ed1f97471",
    "76a621ffd2d66bf012a554f4400091a92a5b473e",
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d",
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
    "0689363343a281a6f7f6f395227668bddc2663eb",
    "4da6de1ca3a1aaca6c80b16318340cfdcd3cea07",
    "8c7a7e619699386f9e6991842558d78aa0c8053d"
  ],
  "changeHistoryShort": {
    "743c2e9071f4a73e0196ad4ca005b767758642b9": "Ybodychange",
    "3565c9af17ab05bf9e7f68b71b6c6850df772bb9": "Ybodychange",
    "44f48ee96ee6b2a3909911c37bfddb0c963d5ffc": "Ybodychange",
    "745d04be59accf80feda0ad38efcc74ba362f2ca": "Ybodychange",
    "b2c85db86c9a62b0a03ee87547265077f664970a": "Ybodychange",
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63": "Ymultichange(Yparameterchange,Ybodychange)",
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": "Ymultichange(Yparameterchange,Ybodychange)",
    "4a3161182905afaf450a60d02528161ed1f97471": "Ybodychange",
    "76a621ffd2d66bf012a554f4400091a92a5b473e": "Ybodychange",
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d": "Ybodychange",
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "0689363343a281a6f7f6f395227668bddc2663eb": "Ybodychange",
    "4da6de1ca3a1aaca6c80b16318340cfdcd3cea07": "Ybodychange",
    "8c7a7e619699386f9e6991842558d78aa0c8053d": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "743c2e9071f4a73e0196ad4ca005b767758642b9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15316. Deletion failure should not remove directory from snapshottables. Contributed by hemanthboyina\n",
      "commitDate": "13/05/20 2:31 AM",
      "commitName": "743c2e9071f4a73e0196ad4ca005b767758642b9",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "15/11/19 12:35 PM",
      "commitNameOld": "67f2c491fe3cd400605fb6082fd3504bc5e97037",
      "commitAuthorOld": "Hui Fei",
      "daysBetweenCommits": 179.54,
      "commitsBetweenForRepo": 613,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   static long delete(FSDirectory fsd, INodesInPath iip,\n       BlocksMapUpdateInfo collectedBlocks, List\u003cINode\u003e removedINodes,\n       List\u003cLong\u003e removedUCFiles, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     long filesRemoved \u003d -1;\n     FSNamesystem fsn \u003d fsd.getFSNamesystem();\n     fsd.writeLock();\n     try {\n       if (deleteAllowed(iip)) {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n         FSDirSnapshotOp.checkSnapshot(fsd, iip, snapshottableDirs);\n         ReclaimContext context \u003d new ReclaimContext(\n             fsd.getBlockStoragePolicySuite(), collectedBlocks, removedINodes,\n             removedUCFiles);\n         if (unprotectedDelete(fsd, iip, context, mtime)) {\n           filesRemoved \u003d context.quotaDelta().getNsDelta();\n+          fsn.removeSnapshottableDirs(snapshottableDirs);\n         }\n         fsd.updateReplicationFactor(context.collectedBlocks()\n                                         .toUpdateReplicationInfo());\n-        fsn.removeSnapshottableDirs(snapshottableDirs);\n         fsd.updateCount(iip, context.quotaDelta(), false);\n       }\n     } finally {\n       fsd.writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static long delete(FSDirectory fsd, INodesInPath iip,\n      BlocksMapUpdateInfo collectedBlocks, List\u003cINode\u003e removedINodes,\n      List\u003cLong\u003e removedUCFiles, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    long filesRemoved \u003d -1;\n    FSNamesystem fsn \u003d fsd.getFSNamesystem();\n    fsd.writeLock();\n    try {\n      if (deleteAllowed(iip)) {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n        FSDirSnapshotOp.checkSnapshot(fsd, iip, snapshottableDirs);\n        ReclaimContext context \u003d new ReclaimContext(\n            fsd.getBlockStoragePolicySuite(), collectedBlocks, removedINodes,\n            removedUCFiles);\n        if (unprotectedDelete(fsd, iip, context, mtime)) {\n          filesRemoved \u003d context.quotaDelta().getNsDelta();\n          fsn.removeSnapshottableDirs(snapshottableDirs);\n        }\n        fsd.updateReplicationFactor(context.collectedBlocks()\n                                        .toUpdateReplicationInfo());\n        fsd.updateCount(iip, context.quotaDelta(), false);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return filesRemoved;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "3565c9af17ab05bf9e7f68b71b6c6850df772bb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10979. Pass IIP for FSDirDeleteOp methods. Contributed by Daryn Sharp.\n",
      "commitDate": "07/10/16 12:15 PM",
      "commitName": "3565c9af17ab05bf9e7f68b71b6c6850df772bb9",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "04/10/16 1:05 PM",
      "commitNameOld": "44f48ee96ee6b2a3909911c37bfddb0c963d5ffc",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 2.97,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   static long delete(FSDirectory fsd, INodesInPath iip,\n       BlocksMapUpdateInfo collectedBlocks, List\u003cINode\u003e removedINodes,\n       List\u003cLong\u003e removedUCFiles, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     long filesRemoved \u003d -1;\n     FSNamesystem fsn \u003d fsd.getFSNamesystem();\n     fsd.writeLock();\n     try {\n-      if (deleteAllowed(iip, iip.getPath()) ) {\n+      if (deleteAllowed(iip)) {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n         FSDirSnapshotOp.checkSnapshot(fsd, iip, snapshottableDirs);\n         ReclaimContext context \u003d new ReclaimContext(\n             fsd.getBlockStoragePolicySuite(), collectedBlocks, removedINodes,\n             removedUCFiles);\n         if (unprotectedDelete(fsd, iip, context, mtime)) {\n           filesRemoved \u003d context.quotaDelta().getNsDelta();\n         }\n         fsd.updateReplicationFactor(context.collectedBlocks()\n                                         .toUpdateReplicationInfo());\n         fsn.removeSnapshottableDirs(snapshottableDirs);\n         fsd.updateCount(iip, context.quotaDelta(), false);\n       }\n     } finally {\n       fsd.writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static long delete(FSDirectory fsd, INodesInPath iip,\n      BlocksMapUpdateInfo collectedBlocks, List\u003cINode\u003e removedINodes,\n      List\u003cLong\u003e removedUCFiles, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    long filesRemoved \u003d -1;\n    FSNamesystem fsn \u003d fsd.getFSNamesystem();\n    fsd.writeLock();\n    try {\n      if (deleteAllowed(iip)) {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n        FSDirSnapshotOp.checkSnapshot(fsd, iip, snapshottableDirs);\n        ReclaimContext context \u003d new ReclaimContext(\n            fsd.getBlockStoragePolicySuite(), collectedBlocks, removedINodes,\n            removedUCFiles);\n        if (unprotectedDelete(fsd, iip, context, mtime)) {\n          filesRemoved \u003d context.quotaDelta().getNsDelta();\n        }\n        fsd.updateReplicationFactor(context.collectedBlocks()\n                                        .toUpdateReplicationInfo());\n        fsn.removeSnapshottableDirs(snapshottableDirs);\n        fsd.updateCount(iip, context.quotaDelta(), false);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return filesRemoved;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "44f48ee96ee6b2a3909911c37bfddb0c963d5ffc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10956. Remove rename/delete performance penalty when not using snapshots. Contributed by Daryn Sharp.\n",
      "commitDate": "04/10/16 1:05 PM",
      "commitName": "44f48ee96ee6b2a3909911c37bfddb0c963d5ffc",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "17/08/16 1:53 PM",
      "commitNameOld": "869393643de23dcb010cc33091c8eb398de0fd6c",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 47.97,
      "commitsBetweenForRepo": 285,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   static long delete(FSDirectory fsd, INodesInPath iip,\n       BlocksMapUpdateInfo collectedBlocks, List\u003cINode\u003e removedINodes,\n       List\u003cLong\u003e removedUCFiles, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     long filesRemoved \u003d -1;\n     FSNamesystem fsn \u003d fsd.getFSNamesystem();\n     fsd.writeLock();\n     try {\n       if (deleteAllowed(iip, iip.getPath()) ) {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n-        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n+        FSDirSnapshotOp.checkSnapshot(fsd, iip, snapshottableDirs);\n         ReclaimContext context \u003d new ReclaimContext(\n             fsd.getBlockStoragePolicySuite(), collectedBlocks, removedINodes,\n             removedUCFiles);\n         if (unprotectedDelete(fsd, iip, context, mtime)) {\n           filesRemoved \u003d context.quotaDelta().getNsDelta();\n         }\n         fsd.updateReplicationFactor(context.collectedBlocks()\n                                         .toUpdateReplicationInfo());\n         fsn.removeSnapshottableDirs(snapshottableDirs);\n         fsd.updateCount(iip, context.quotaDelta(), false);\n       }\n     } finally {\n       fsd.writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static long delete(FSDirectory fsd, INodesInPath iip,\n      BlocksMapUpdateInfo collectedBlocks, List\u003cINode\u003e removedINodes,\n      List\u003cLong\u003e removedUCFiles, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    long filesRemoved \u003d -1;\n    FSNamesystem fsn \u003d fsd.getFSNamesystem();\n    fsd.writeLock();\n    try {\n      if (deleteAllowed(iip, iip.getPath()) ) {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n        FSDirSnapshotOp.checkSnapshot(fsd, iip, snapshottableDirs);\n        ReclaimContext context \u003d new ReclaimContext(\n            fsd.getBlockStoragePolicySuite(), collectedBlocks, removedINodes,\n            removedUCFiles);\n        if (unprotectedDelete(fsd, iip, context, mtime)) {\n          filesRemoved \u003d context.quotaDelta().getNsDelta();\n        }\n        fsd.updateReplicationFactor(context.collectedBlocks()\n                                        .toUpdateReplicationInfo());\n        fsn.removeSnapshottableDirs(snapshottableDirs);\n        fsd.updateCount(iip, context.quotaDelta(), false);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return filesRemoved;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "745d04be59accf80feda0ad38efcc74ba362f2ca": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8823. Move replication factor into individual blocks. Contributed by Haohui Mai.\n",
      "commitDate": "22/08/15 12:09 AM",
      "commitName": "745d04be59accf80feda0ad38efcc74ba362f2ca",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "13/05/15 9:50 PM",
      "commitNameOld": "b2c85db86c9a62b0a03ee87547265077f664970a",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 100.1,
      "commitsBetweenForRepo": 639,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,29 @@\n   static long delete(FSDirectory fsd, INodesInPath iip,\n       BlocksMapUpdateInfo collectedBlocks, List\u003cINode\u003e removedINodes,\n       List\u003cLong\u003e removedUCFiles, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     long filesRemoved \u003d -1;\n+    FSNamesystem fsn \u003d fsd.getFSNamesystem();\n     fsd.writeLock();\n     try {\n       if (deleteAllowed(iip, iip.getPath()) ) {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n         FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n         ReclaimContext context \u003d new ReclaimContext(\n             fsd.getBlockStoragePolicySuite(), collectedBlocks, removedINodes,\n             removedUCFiles);\n         if (unprotectedDelete(fsd, iip, context, mtime)) {\n           filesRemoved \u003d context.quotaDelta().getNsDelta();\n         }\n-        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n+        fsd.updateReplicationFactor(context.collectedBlocks()\n+                                        .toUpdateReplicationInfo());\n+        fsn.removeSnapshottableDirs(snapshottableDirs);\n         fsd.updateCount(iip, context.quotaDelta(), false);\n       }\n     } finally {\n       fsd.writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static long delete(FSDirectory fsd, INodesInPath iip,\n      BlocksMapUpdateInfo collectedBlocks, List\u003cINode\u003e removedINodes,\n      List\u003cLong\u003e removedUCFiles, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    long filesRemoved \u003d -1;\n    FSNamesystem fsn \u003d fsd.getFSNamesystem();\n    fsd.writeLock();\n    try {\n      if (deleteAllowed(iip, iip.getPath()) ) {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        ReclaimContext context \u003d new ReclaimContext(\n            fsd.getBlockStoragePolicySuite(), collectedBlocks, removedINodes,\n            removedUCFiles);\n        if (unprotectedDelete(fsd, iip, context, mtime)) {\n          filesRemoved \u003d context.quotaDelta().getNsDelta();\n        }\n        fsd.updateReplicationFactor(context.collectedBlocks()\n                                        .toUpdateReplicationInfo());\n        fsn.removeSnapshottableDirs(snapshottableDirs);\n        fsd.updateCount(iip, context.quotaDelta(), false);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return filesRemoved;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "b2c85db86c9a62b0a03ee87547265077f664970a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7728. Avoid updating quota usage while loading edits. Contributed by Jing Zhao.\n",
      "commitDate": "13/05/15 9:50 PM",
      "commitName": "b2c85db86c9a62b0a03ee87547265077f664970a",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "09/05/15 10:51 PM",
      "commitNameOld": "4536399d47f6c061e149e2504600804a0f1e093d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.96,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,26 @@\n-  static long delete(\n-      FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes, List\u003cLong\u003e removedUCFiles,\n-      long mtime) throws IOException {\n+  static long delete(FSDirectory fsd, INodesInPath iip,\n+      BlocksMapUpdateInfo collectedBlocks, List\u003cINode\u003e removedINodes,\n+      List\u003cLong\u003e removedUCFiles, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n-    final long filesRemoved;\n+    long filesRemoved \u003d -1;\n     fsd.writeLock();\n     try {\n-      if (!deleteAllowed(iip, iip.getPath()) ) {\n-        filesRemoved \u003d -1;\n-      } else {\n+      if (deleteAllowed(iip, iip.getPath()) ) {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n         FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n-                                         removedINodes, removedUCFiles, mtime);\n+        ReclaimContext context \u003d new ReclaimContext(\n+            fsd.getBlockStoragePolicySuite(), collectedBlocks, removedINodes,\n+            removedUCFiles);\n+        if (unprotectedDelete(fsd, iip, context, mtime)) {\n+          filesRemoved \u003d context.quotaDelta().getNsDelta();\n+        }\n         fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n+        fsd.updateCount(iip, context.quotaDelta(), false);\n       }\n     } finally {\n       fsd.writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static long delete(FSDirectory fsd, INodesInPath iip,\n      BlocksMapUpdateInfo collectedBlocks, List\u003cINode\u003e removedINodes,\n      List\u003cLong\u003e removedUCFiles, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    long filesRemoved \u003d -1;\n    fsd.writeLock();\n    try {\n      if (deleteAllowed(iip, iip.getPath()) ) {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        ReclaimContext context \u003d new ReclaimContext(\n            fsd.getBlockStoragePolicySuite(), collectedBlocks, removedINodes,\n            removedUCFiles);\n        if (unprotectedDelete(fsd, iip, context, mtime)) {\n          filesRemoved \u003d context.quotaDelta().getNsDelta();\n        }\n        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n        fsd.updateCount(iip, context.quotaDelta(), false);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return filesRemoved;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6757. Simplify lease manager with INodeID. Contributed by Haohui Mai.\n",
      "commitDate": "08/05/15 11:04 PM",
      "commitName": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6757. Simplify lease manager with INodeID. Contributed by Haohui Mai.\n",
          "commitDate": "08/05/15 11:04 PM",
          "commitName": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "28/04/15 6:05 PM",
          "commitNameOld": "c79e7f7d997596e0c38ae4cddff2bd0910581c16",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 10.21,
          "commitsBetweenForRepo": 163,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,24 @@\n   static long delete(\n       FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n+      List\u003cINode\u003e removedINodes, List\u003cLong\u003e removedUCFiles,\n+      long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     final long filesRemoved;\n     fsd.writeLock();\n     try {\n       if (!deleteAllowed(iip, iip.getPath()) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n         FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n-                                         removedINodes, mtime);\n+                                         removedINodes, removedUCFiles, mtime);\n         fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       fsd.writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static long delete(\n      FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, List\u003cLong\u003e removedUCFiles,\n      long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    final long filesRemoved;\n    fsd.writeLock();\n    try {\n      if (!deleteAllowed(iip, iip.getPath()) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n                                         removedINodes, removedUCFiles, mtime);\n        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, iip-INodesInPath, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, mtime-long]",
            "newValue": "[fsd-FSDirectory, iip-INodesInPath, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, removedUCFiles-List\u003cLong\u003e, mtime-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6757. Simplify lease manager with INodeID. Contributed by Haohui Mai.\n",
          "commitDate": "08/05/15 11:04 PM",
          "commitName": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "28/04/15 6:05 PM",
          "commitNameOld": "c79e7f7d997596e0c38ae4cddff2bd0910581c16",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 10.21,
          "commitsBetweenForRepo": 163,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,24 @@\n   static long delete(\n       FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n+      List\u003cINode\u003e removedINodes, List\u003cLong\u003e removedUCFiles,\n+      long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     final long filesRemoved;\n     fsd.writeLock();\n     try {\n       if (!deleteAllowed(iip, iip.getPath()) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n         FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n-                                         removedINodes, mtime);\n+                                         removedINodes, removedUCFiles, mtime);\n         fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       fsd.writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static long delete(\n      FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, List\u003cLong\u003e removedUCFiles,\n      long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    final long filesRemoved;\n    fsd.writeLock();\n    try {\n      if (!deleteAllowed(iip, iip.getPath()) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n                                         removedINodes, removedUCFiles, mtime);\n        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "17/01/15 12:56 PM",
      "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,23 @@\n-  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n+  static long delete(\n+      FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n+      List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     final long filesRemoved;\n-    writeLock();\n+    fsd.writeLock();\n     try {\n       if (!deleteAllowed(iip, iip.getPath()) ) {\n         filesRemoved \u003d -1;\n       } else {\n-        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n+        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n         FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n-            removedINodes, mtime);\n-        namesystem.removeSnapshottableDirs(snapshottableDirs);\n+        filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n+                                         removedINodes, mtime);\n+        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n-      writeUnlock();\n+      fsd.writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static long delete(\n      FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    final long filesRemoved;\n    fsd.writeLock();\n    try {\n      if (!deleteAllowed(iip, iip.getPath()) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n                                         removedINodes, mtime);\n        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
            "oldMethodName": "delete",
            "newMethodName": "delete"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,23 @@\n-  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n+  static long delete(\n+      FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n+      List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     final long filesRemoved;\n-    writeLock();\n+    fsd.writeLock();\n     try {\n       if (!deleteAllowed(iip, iip.getPath()) ) {\n         filesRemoved \u003d -1;\n       } else {\n-        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n+        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n         FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n-            removedINodes, mtime);\n-        namesystem.removeSnapshottableDirs(snapshottableDirs);\n+        filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n+                                         removedINodes, mtime);\n+        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n-      writeUnlock();\n+      fsd.writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static long delete(\n      FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    final long filesRemoved;\n    fsd.writeLock();\n    try {\n      if (!deleteAllowed(iip, iip.getPath()) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n                                         removedINodes, mtime);\n        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,23 @@\n-  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n+  static long delete(\n+      FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n+      List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     final long filesRemoved;\n-    writeLock();\n+    fsd.writeLock();\n     try {\n       if (!deleteAllowed(iip, iip.getPath()) ) {\n         filesRemoved \u003d -1;\n       } else {\n-        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n+        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n         FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n-            removedINodes, mtime);\n-        namesystem.removeSnapshottableDirs(snapshottableDirs);\n+        filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n+                                         removedINodes, mtime);\n+        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n-      writeUnlock();\n+      fsd.writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static long delete(\n      FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    final long filesRemoved;\n    fsd.writeLock();\n    try {\n      if (!deleteAllowed(iip, iip.getPath()) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n                                         removedINodes, mtime);\n        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,23 @@\n-  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n+  static long delete(\n+      FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n+      List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     final long filesRemoved;\n-    writeLock();\n+    fsd.writeLock();\n     try {\n       if (!deleteAllowed(iip, iip.getPath()) ) {\n         filesRemoved \u003d -1;\n       } else {\n-        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n+        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n         FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n-            removedINodes, mtime);\n-        namesystem.removeSnapshottableDirs(snapshottableDirs);\n+        filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n+                                         removedINodes, mtime);\n+        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n-      writeUnlock();\n+      fsd.writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static long delete(\n      FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    final long filesRemoved;\n    fsd.writeLock();\n    try {\n      if (!deleteAllowed(iip, iip.getPath()) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003c\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(fsd, iip, collectedBlocks,\n                                         removedINodes, mtime);\n        fsd.getFSNamesystem().removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      fsd.writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "[iip-INodesInPath, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, mtime-long]",
            "newValue": "[fsd-FSDirectory, iip-INodesInPath, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, mtime-long]"
          }
        }
      ]
    },
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
      "commitDate": "12/12/14 3:13 PM",
      "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
          "commitDate": "12/12/14 3:13 PM",
          "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "09/12/14 11:37 AM",
          "commitNameOld": "5776a41da08af653206bb94d7c76c9c4dcce059a",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 3.15,
          "commitsBetweenForRepo": 33,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,22 @@\n-  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n+  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n               List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     final long filesRemoved;\n     writeLock();\n     try {\n-      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n-          normalizePath(src), false);\n-      if (!deleteAllowed(inodesInPath, src) ) {\n+      if (!deleteAllowed(iip, iip.getPath()) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n-        FSDirSnapshotOp.checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n+        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n+        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n             removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    final long filesRemoved;\n    writeLock();\n    try {\n      if (!deleteAllowed(iip, iip.getPath()) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[src-String, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, mtime-long]",
            "newValue": "[iip-INodesInPath, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, mtime-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
          "commitDate": "12/12/14 3:13 PM",
          "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "09/12/14 11:37 AM",
          "commitNameOld": "5776a41da08af653206bb94d7c76c9c4dcce059a",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 3.15,
          "commitsBetweenForRepo": 33,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,22 @@\n-  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n+  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n               List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     final long filesRemoved;\n     writeLock();\n     try {\n-      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n-          normalizePath(src), false);\n-      if (!deleteAllowed(inodesInPath, src) ) {\n+      if (!deleteAllowed(iip, iip.getPath()) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n-        FSDirSnapshotOp.checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n+        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n+        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n             removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    final long filesRemoved;\n    writeLock();\n    try {\n      if (!deleteAllowed(iip, iip.getPath()) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    },
    "4a3161182905afaf450a60d02528161ed1f97471": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7440. Consolidate snapshot related operations in a single class. Contributed by Haohui Mai.\n",
      "commitDate": "25/11/14 9:04 PM",
      "commitName": "4a3161182905afaf450a60d02528161ed1f97471",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/11/14 3:42 PM",
      "commitNameOld": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.22,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n               List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n-        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n+        FSDirSnapshotOp.checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n        FSDirSnapshotOp.checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "76a621ffd2d66bf012a554f4400091a92a5b473e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6609. Use DirectorySnapshottableFeature to represent a snapshottable directory. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1608631 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/07/14 5:08 PM",
      "commitName": "76a621ffd2d66bf012a554f4400091a92a5b473e",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "23/06/14 11:56 PM",
      "commitNameOld": "08986fdbed5a15bcdc57d142922911759b97e9d1",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 13.72,
      "commitsBetweenForRepo": 88,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,24 @@\n   long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n               List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n-        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n-            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n+        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6480. Move waitForReady() from FSDirectory to FSNamesystem. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603705 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/06/14 9:13 PM",
      "commitName": "a4e0ff5e052abad498595ee198b49c5310c9ec0d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "18/06/14 12:57 PM",
      "commitNameOld": "4cf94aaf809c77b3b7dc925faa39a72d53e4246e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.35,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,25 @@\n   long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n               List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n-    waitForReady();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-6315. Decouple recording edit logs from FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601960 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/06/14 10:22 AM",
      "commitName": "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6315. Decouple recording edit logs from FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601960 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/06/14 10:22 AM",
          "commitName": "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/05/14 6:57 AM",
          "commitNameOld": "ac23a55547716df29b3e25c98a113399e184d9d1",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 21.14,
          "commitsBetweenForRepo": 105,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,26 @@\n-  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n+  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n+              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n-    long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n-            removedINodes, now);\n+            removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n-    if (filesRemoved \u003c 0) {\n-      return false;\n-    }\n-    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n-    incrDeletedFileCount(filesRemoved);\n-    // Blocks/INodes will be handled later by the caller of this method\n-    getFSNamesystem().removePathAndBlocks(src, null, null);\n-    return true;\n+    return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[src-String, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, logRetryCache-boolean]",
            "newValue": "[src-String, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, mtime-long]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-6315. Decouple recording edit logs from FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601960 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/06/14 10:22 AM",
          "commitName": "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/05/14 6:57 AM",
          "commitNameOld": "ac23a55547716df29b3e25c98a113399e184d9d1",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 21.14,
          "commitsBetweenForRepo": 105,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,26 @@\n-  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n+  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n+              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n-    long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n-            removedINodes, now);\n+            removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n-    if (filesRemoved \u003c 0) {\n-      return false;\n-    }\n-    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n-    incrDeletedFileCount(filesRemoved);\n-    // Blocks/INodes will be handled later by the caller of this method\n-    getFSNamesystem().removePathAndBlocks(src, null, null);\n-    return true;\n+    return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "boolean",
            "newValue": "long"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6315. Decouple recording edit logs from FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601960 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/06/14 10:22 AM",
          "commitName": "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/05/14 6:57 AM",
          "commitNameOld": "ac23a55547716df29b3e25c98a113399e184d9d1",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 21.14,
          "commitsBetweenForRepo": 105,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,26 @@\n-  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n+  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n+              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n-    long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n-            removedINodes, now);\n+            removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n-    if (filesRemoved \u003c 0) {\n-      return false;\n-    }\n-    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n-    incrDeletedFileCount(filesRemoved);\n-    // Blocks/INodes will be handled later by the caller of this method\n-    getFSNamesystem().removePathAndBlocks(src, null, null);\n-    return true;\n+    return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    },
    "0689363343a281a6f7f6f395227668bddc2663eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6304. Consolidate the logic of path resolution in FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1591411 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/04/14 10:44 AM",
      "commitName": "0689363343a281a6f7f6f395227668bddc2663eb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/04/14 7:05 PM",
      "commitNameOld": "10a037cccb00c9f791da394bf2dc05985fb80612",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 5.65,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n       List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n     long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n-      final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n+      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, now);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     if (filesRemoved \u003c 0) {\n       return false;\n     }\n     fsImage.getEditLog().logDelete(src, now, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n     // Blocks/INodes will be handled later by the caller of this method\n     getFSNamesystem().removePathAndBlocks(src, null, null);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    long now \u003d now();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, now);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    if (filesRemoved \u003c 0) {\n      return false;\n    }\n    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n    // Blocks/INodes will be handled later by the caller of this method\n    getFSNamesystem().removePathAndBlocks(src, null, null);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "4da6de1ca3a1aaca6c80b16318340cfdcd3cea07": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5982. Need to update snapshot manager when applying editlog for deleting a snapshottable directory. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1570395 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/02/14 4:15 PM",
      "commitName": "4da6de1ca3a1aaca6c80b16318340cfdcd3cea07",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/02/14 2:54 PM",
      "commitNameOld": "fc14360b0340a33c0e1eb34967d4dcd772533418",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 8.06,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,34 @@\n   boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n       List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n     long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n-        // Before removing the node, first check if the targetNode is for a\n-        // snapshottable dir with snapshots, or its descendants have\n-        // snapshottable dir with snapshots\n-        final INode targetNode \u003d inodesInPath.getLastINode();\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n-        checkSnapshot(targetNode, snapshottableDirs);\n+        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, now);\n-        if (snapshottableDirs.size() \u003e 0) {\n-          // There are some snapshottable directories without snapshots to be\n-          // deleted. Need to update the SnapshotManager.\n-          namesystem.removeSnapshottableDirs(snapshottableDirs);\n-        }\n+        namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     if (filesRemoved \u003c 0) {\n       return false;\n     }\n     fsImage.getEditLog().logDelete(src, now, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n     // Blocks/INodes will be handled later by the caller of this method\n     getFSNamesystem().removePathAndBlocks(src, null, null);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    long now \u003d now();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, now);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    if (filesRemoved \u003c 0) {\n      return false;\n    }\n    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n    // Blocks/INodes will be handled later by the caller of this method\n    getFSNamesystem().removePathAndBlocks(src, null, null);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "8c7a7e619699386f9e6991842558d78aa0c8053d": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/07/13 12:51 AM",
      "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/07/13 12:51 AM",
          "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "22/07/13 11:22 AM",
          "commitNameOld": "11c073134afc878619c37c95935d6a3098a21f17",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 7.56,
          "commitsBetweenForRepo": 56,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,42 @@\n   boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes) throws IOException {\n+      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n     long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         // Before removing the node, first check if the targetNode is for a\n         // snapshottable dir with snapshots, or its descendants have\n         // snapshottable dir with snapshots\n         final INode targetNode \u003d inodesInPath.getLastINode();\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(targetNode, snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, now);\n         if (snapshottableDirs.size() \u003e 0) {\n           // There are some snapshottable directories without snapshots to be\n           // deleted. Need to update the SnapshotManager.\n           namesystem.removeSnapshottableDirs(snapshottableDirs);\n         }\n       }\n     } finally {\n       writeUnlock();\n     }\n     if (filesRemoved \u003c 0) {\n       return false;\n     }\n-    fsImage.getEditLog().logDelete(src, now);\n+    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n     // Blocks/INodes will be handled later by the caller of this method\n     getFSNamesystem().removePathAndBlocks(src, null, null);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    long now \u003d now();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        // Before removing the node, first check if the targetNode is for a\n        // snapshottable dir with snapshots, or its descendants have\n        // snapshottable dir with snapshots\n        final INode targetNode \u003d inodesInPath.getLastINode();\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(targetNode, snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, now);\n        if (snapshottableDirs.size() \u003e 0) {\n          // There are some snapshottable directories without snapshots to be\n          // deleted. Need to update the SnapshotManager.\n          namesystem.removeSnapshottableDirs(snapshottableDirs);\n        }\n      }\n    } finally {\n      writeUnlock();\n    }\n    if (filesRemoved \u003c 0) {\n      return false;\n    }\n    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n    // Blocks/INodes will be handled later by the caller of this method\n    getFSNamesystem().removePathAndBlocks(src, null, null);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[src-String, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e]",
            "newValue": "[src-String, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/07/13 12:51 AM",
          "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "22/07/13 11:22 AM",
          "commitNameOld": "11c073134afc878619c37c95935d6a3098a21f17",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 7.56,
          "commitsBetweenForRepo": 56,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,42 @@\n   boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes) throws IOException {\n+      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n     long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         // Before removing the node, first check if the targetNode is for a\n         // snapshottable dir with snapshots, or its descendants have\n         // snapshottable dir with snapshots\n         final INode targetNode \u003d inodesInPath.getLastINode();\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(targetNode, snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, now);\n         if (snapshottableDirs.size() \u003e 0) {\n           // There are some snapshottable directories without snapshots to be\n           // deleted. Need to update the SnapshotManager.\n           namesystem.removeSnapshottableDirs(snapshottableDirs);\n         }\n       }\n     } finally {\n       writeUnlock();\n     }\n     if (filesRemoved \u003c 0) {\n       return false;\n     }\n-    fsImage.getEditLog().logDelete(src, now);\n+    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n     // Blocks/INodes will be handled later by the caller of this method\n     getFSNamesystem().removePathAndBlocks(src, null, null);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    long now \u003d now();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        // Before removing the node, first check if the targetNode is for a\n        // snapshottable dir with snapshots, or its descendants have\n        // snapshottable dir with snapshots\n        final INode targetNode \u003d inodesInPath.getLastINode();\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(targetNode, snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, now);\n        if (snapshottableDirs.size() \u003e 0) {\n          // There are some snapshottable directories without snapshots to be\n          // deleted. Need to update the SnapshotManager.\n          namesystem.removeSnapshottableDirs(snapshottableDirs);\n        }\n      }\n    } finally {\n      writeUnlock();\n    }\n    if (filesRemoved \u003c 0) {\n      return false;\n    }\n    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n    // Blocks/INodes will be handled later by the caller of this method\n    getFSNamesystem().removePathAndBlocks(src, null, null);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}