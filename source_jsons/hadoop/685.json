{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "createSocketForPipeline",
  "functionId": "createSocketForPipeline___first-DatanodeInfo(modifiers-final)__length-int(modifiers-final)__client-DFSClient(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 245,
  "functionEndLine": 263,
  "numCommitsSeen": 156,
  "timeTaken": 9655,
  "changeHistory": [
    "372d1302c63c6f49f99be5766c5da9647ebd9ca6",
    "49949a4bb03aa81cbb9115e91ab1c61cc6dc8a62",
    "aa09880ab85f3c35c12373976e7b03f3140b65c8",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "7947e5b53b9ac9524b535b0384c1c355b74723ff",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
    "2ad7397c49844b5c12e122779c8760f51fa3a998",
    "f98d8eb291be364102b5c3011ce72e8f43eab389",
    "4f15b9dfed02845b07539f074ccee3074647dffd",
    "be7dd8333a7e56e732171db0781786987de03195",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "372d1302c63c6f49f99be5766c5da9647ebd9ca6": "Ybodychange",
    "49949a4bb03aa81cbb9115e91ab1c61cc6dc8a62": "Ybodychange",
    "aa09880ab85f3c35c12373976e7b03f3140b65c8": "Ybodychange",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "7947e5b53b9ac9524b535b0384c1c355b74723ff": "Ybodychange",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": "Ymovefromfile",
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6": "Ybodychange",
    "2ad7397c49844b5c12e122779c8760f51fa3a998": "Ybodychange",
    "f98d8eb291be364102b5c3011ce72e8f43eab389": "Ybodychange",
    "4f15b9dfed02845b07539f074ccee3074647dffd": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "372d1302c63c6f49f99be5766c5da9647ebd9ca6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9700. DFSClient and DFSOutputStream should set TCP_NODELAY on sockets for DataTransferProtocol (Gary Helmling via iwasakims)\n",
      "commitDate": "12/02/16 10:31 AM",
      "commitName": "372d1302c63c6f49f99be5766c5da9647ebd9ca6",
      "commitAuthor": "Masatake Iwasaki",
      "commitDateOld": "08/02/16 10:16 AM",
      "commitNameOld": "193d27de0a5d23a61cabd41162ebc3292d8526d1",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 4.01,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,19 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     final DfsClientConf conf \u003d client.getConf();\n     final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n     LOG.debug(\"Connecting to datanode {}\", dnAddr);\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(),\n         conf.getSocketTimeout());\n+    sock.setTcpNoDelay(conf.getDataTransferTcpNoDelay());\n     sock.setSoTimeout(timeout);\n     sock.setKeepAlive(true);\n     if (conf.getSocketSendBufferSize() \u003e 0) {\n       sock.setSendBufferSize(conf.getSocketSendBufferSize());\n     }\n     LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final DfsClientConf conf \u003d client.getConf();\n    final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n    LOG.debug(\"Connecting to datanode {}\", dnAddr);\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(),\n        conf.getSocketTimeout());\n    sock.setTcpNoDelay(conf.getDataTransferTcpNoDelay());\n    sock.setSoTimeout(timeout);\n    sock.setKeepAlive(true);\n    if (conf.getSocketSendBufferSize() \u003e 0) {\n      sock.setSendBufferSize(conf.getSocketSendBufferSize());\n    }\n    LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "49949a4bb03aa81cbb9115e91ab1c61cc6dc8a62": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8894. Set SO_KEEPALIVE on DN server sockets. Contributed by Kanaka Kumar Avvaru.\n",
      "commitDate": "15/12/15 2:38 PM",
      "commitName": "49949a4bb03aa81cbb9115e91ab1c61cc6dc8a62",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "03/11/15 1:34 PM",
      "commitNameOld": "dac0463a4e20dfb3a802355919fc22b8e017a4e1",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 42.04,
      "commitsBetweenForRepo": 261,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,18 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     final DfsClientConf conf \u003d client.getConf();\n     final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n     LOG.debug(\"Connecting to datanode {}\", dnAddr);\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(),\n         conf.getSocketTimeout());\n     sock.setSoTimeout(timeout);\n+    sock.setKeepAlive(true);\n     if (conf.getSocketSendBufferSize() \u003e 0) {\n       sock.setSendBufferSize(conf.getSocketSendBufferSize());\n     }\n     LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final DfsClientConf conf \u003d client.getConf();\n    final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n    LOG.debug(\"Connecting to datanode {}\", dnAddr);\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(),\n        conf.getSocketTimeout());\n    sock.setSoTimeout(timeout);\n    sock.setKeepAlive(true);\n    if (conf.getSocketSendBufferSize() \u003e 0) {\n      sock.setSendBufferSize(conf.getSocketSendBufferSize());\n    }\n    LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "aa09880ab85f3c35c12373976e7b03f3140b65c8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9259. Make SO_SNDBUF size configurable at DFSClient side for hdfs write scenario. (Mingliang Liu via mingma)\n",
      "commitDate": "27/10/15 9:28 AM",
      "commitName": "aa09880ab85f3c35c12373976e7b03f3140b65c8",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "06/10/15 10:56 AM",
      "commitNameOld": "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 20.94,
      "commitsBetweenForRepo": 183,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,17 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     final DfsClientConf conf \u003d client.getConf();\n     final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n     LOG.debug(\"Connecting to datanode {}\", dnAddr);\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(),\n         conf.getSocketTimeout());\n     sock.setSoTimeout(timeout);\n-    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n+    if (conf.getSocketSendBufferSize() \u003e 0) {\n+      sock.setSendBufferSize(conf.getSocketSendBufferSize());\n+    }\n     LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final DfsClientConf conf \u003d client.getConf();\n    final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n    LOG.debug(\"Connecting to datanode {}\", dnAddr);\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(),\n        conf.getSocketTimeout());\n    sock.setSoTimeout(timeout);\n    if (conf.getSocketSendBufferSize() \u003e 0) {\n      sock.setSendBufferSize(conf.getSocketSendBufferSize());\n    }\n    LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,14 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     final DfsClientConf conf \u003d client.getConf();\n     final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Connecting to datanode \" + dnAddr);\n-    }\n+    LOG.debug(\"Connecting to datanode {}\", dnAddr);\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\n     sock.setSoTimeout(timeout);\n     sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n-    }\n+    LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final DfsClientConf conf \u003d client.getConf();\n    final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n    LOG.debug(\"Connecting to datanode {}\", dnAddr);\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,18 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     final DfsClientConf conf \u003d client.getConf();\n     final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n-    LOG.debug(\"Connecting to datanode {}\", dnAddr);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Connecting to datanode \" + dnAddr);\n+    }\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\n     sock.setSoTimeout(timeout);\n     sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n-    LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n+    }\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final DfsClientConf conf \u003d client.getConf();\n    final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Connecting to datanode \" + dnAddr);\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 11:29 AM",
      "commitNameOld": "4c9497cbf02ecc82532a4e79e18912d8e0eb4731",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 1.26,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,14 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     final DfsClientConf conf \u003d client.getConf();\n     final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Connecting to datanode \" + dnAddr);\n-    }\n+    LOG.debug(\"Connecting to datanode {}\", dnAddr);\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\n     sock.setSoTimeout(timeout);\n     sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n-    }\n+    LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final DfsClientConf conf \u003d client.getConf();\n    final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n    LOG.debug(\"Connecting to datanode {}\", dnAddr);\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final DfsClientConf conf \u003d client.getConf();\n    final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Connecting to datanode \" + dnAddr);\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
      }
    },
    "7947e5b53b9ac9524b535b0384c1c355b74723ff": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8283. DataStreamer cleanup and some minor improvement. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "29/04/15 10:41 AM",
      "commitName": "7947e5b53b9ac9524b535b0384c1c355b74723ff",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/04/15 12:21 AM",
      "commitNameOld": "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 5.43,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     final DfsClientConf conf \u003d client.getConf();\n     final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n-    if (DFSClient.LOG.isDebugEnabled()) {\n-      DFSClient.LOG.debug(\"Connecting to datanode \" + dnAddr);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Connecting to datanode \" + dnAddr);\n     }\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\n     sock.setSoTimeout(timeout);\n     sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n-    if(DFSClient.LOG.isDebugEnabled()) {\n-      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n     }\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final DfsClientConf conf \u003d client.getConf();\n    final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Connecting to datanode \" + dnAddr);\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "07/04/15 1:59 PM",
      "commitNameOld": "571a1ce9d037d99e7c9042bcb77ae7a2c4daf6d3",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 3.03,
      "commitsBetweenForRepo": 48,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n-    final String dnAddr \u003d first.getXferAddr(\n-        client.getConf().connectToDnViaHostname);\n+    final DfsClientConf conf \u003d client.getConf();\n+    final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Connecting to datanode \" + dnAddr);\n     }\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n-    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), client.getConf().socketTimeout);\n+    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\n     sock.setSoTimeout(timeout);\n     sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n     if(DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n     }\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final DfsClientConf conf \u003d client.getConf();\n    final String dnAddr \u003d first.getXferAddr(conf.isConnectToDnViaHostname());\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + dnAddr);\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "24/03/15 11:06 AM",
      "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/03/15 10:49 AM",
      "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final String dnAddr \u003d first.getXferAddr(\n        client.getConf().connectToDnViaHostname);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + dnAddr);\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), client.getConf().socketTimeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
        "oldMethodName": "createSocketForPipeline",
        "newMethodName": "createSocketForPipeline"
      }
    },
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4914. Use DFSClient.Conf instead of Configuration.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1494854 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/06/13 9:43 PM",
      "commitName": "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "18/06/13 2:05 PM",
      "commitNameOld": "1c309f763be3dd2e3d7d1616d2c960ff80cf9b03",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 1.32,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,18 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n-    final String dnAddr \u003d first.getXferAddr(client.connectToDnViaHostname());\n+    final String dnAddr \u003d first.getXferAddr(\n+        client.getConf().connectToDnViaHostname);\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Connecting to datanode \" + dnAddr);\n     }\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), client.getConf().socketTimeout);\n     sock.setSoTimeout(timeout);\n     sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n     if(DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n     }\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final String dnAddr \u003d first.getXferAddr(\n        client.getConf().connectToDnViaHostname);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + dnAddr);\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), client.getConf().socketTimeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "2ad7397c49844b5c12e122779c8760f51fa3a998": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4807. createSocketForPipeline() should not include timeout extension on connect. Contributed by Cristina Abad.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1485861 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/05/13 2:35 PM",
      "commitName": "2ad7397c49844b5c12e122779c8760f51fa3a998",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "21/05/13 6:42 AM",
      "commitNameOld": "98a692fd6361365db4afb9523a5d83ee32774112",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 2.33,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,17 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     final String dnAddr \u003d first.getXferAddr(client.connectToDnViaHostname());\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Connecting to datanode \" + dnAddr);\n     }\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n-    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), timeout);\n+    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), client.getConf().socketTimeout);\n     sock.setSoTimeout(timeout);\n     sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n     if(DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n     }\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final String dnAddr \u003d first.getXferAddr(client.connectToDnViaHostname());\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + dnAddr);\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), client.getConf().socketTimeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "f98d8eb291be364102b5c3011ce72e8f43eab389": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3150. Add option for clients to contact DNs via hostname. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1373094 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/08/12 1:59 PM",
      "commitName": "f98d8eb291be364102b5c3011ce72e8f43eab389",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "09/08/12 2:31 PM",
      "commitNameOld": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 4.98,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,17 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n-    if(DFSClient.LOG.isDebugEnabled()) {\n-      DFSClient.LOG.debug(\"Connecting to datanode \" + first);\n+    final String dnAddr \u003d first.getXferAddr(client.connectToDnViaHostname());\n+    if (DFSClient.LOG.isDebugEnabled()) {\n+      DFSClient.LOG.debug(\"Connecting to datanode \" + dnAddr);\n     }\n-    final InetSocketAddress isa \u003d\n-      NetUtils.createSocketAddr(first.getXferAddr());\n+    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), timeout);\n     sock.setSoTimeout(timeout);\n     sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n     if(DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n     }\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    final String dnAddr \u003d first.getXferAddr(client.connectToDnViaHostname());\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + dnAddr);\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(dnAddr);\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), timeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "4f15b9dfed02845b07539f074ccee3074647dffd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3148. The client should be able to use multiple local interfaces for data transfer. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308617 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/04/12 4:20 PM",
      "commitName": "4f15b9dfed02845b07539f074ccee3074647dffd",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "01/04/12 3:12 PM",
      "commitNameOld": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.05,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,17 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     if(DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Connecting to datanode \" + first);\n     }\n     final InetSocketAddress isa \u003d\n       NetUtils.createSocketAddr(first.getXferAddr());\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n-    NetUtils.connect(sock, isa, timeout);\n+    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), timeout);\n     sock.setSoTimeout(timeout);\n     sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n     if(DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n     }\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + first);\n    }\n    final InetSocketAddress isa \u003d\n      NetUtils.createSocketAddr(first.getXferAddr());\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), timeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "01/03/12 3:51 PM",
      "commitNameOld": "b2f67b47044a5cbb0c3aaac83299afba541aa771",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 30.93,
      "commitsBetweenForRepo": 182,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,17 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     if(DFSClient.LOG.isDebugEnabled()) {\n-      DFSClient.LOG.debug(\"Connecting to datanode \" + first.getName());\n+      DFSClient.LOG.debug(\"Connecting to datanode \" + first);\n     }\n-    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(first.getName());\n+    final InetSocketAddress isa \u003d\n+      NetUtils.createSocketAddr(first.getXferAddr());\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, timeout);\n     sock.setSoTimeout(timeout);\n     sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n     if(DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n     }\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + first);\n    }\n    final InetSocketAddress isa \u003d\n      NetUtils.createSocketAddr(first.getXferAddr());\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, timeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1620. Rename HdfsConstants -\u003e HdfsServerConstants, FSConstants -\u003e HdfsConstants. (Harsh J Chouraria via atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165096 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/11 12:30 PM",
      "commitName": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 10.8,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     if(DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Connecting to datanode \" + first.getName());\n     }\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(first.getName());\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, timeout);\n     sock.setSoTimeout(timeout);\n-    sock.setSendBufferSize(FSConstants.DEFAULT_DATA_SOCKET_SIZE);\n+    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n     if(DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n     }\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + first.getName());\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(first.getName());\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, timeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(HdfsConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + first.getName());\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(first.getName());\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, timeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(FSConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + first.getName());\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(first.getName());\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, timeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(FSConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2241. Remove implementing FSConstants interface to just get the constants from the interface. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1156420 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/08/11 5:46 PM",
      "commitName": "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "12/07/11 6:11 PM",
      "commitNameOld": "2c5dd549e31aa5d3377ff2619ede8e92b8dc5d0f",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 28.98,
      "commitsBetweenForRepo": 108,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   static Socket createSocketForPipeline(final DatanodeInfo first,\n       final int length, final DFSClient client) throws IOException {\n     if(DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Connecting to datanode \" + first.getName());\n     }\n     final InetSocketAddress isa \u003d NetUtils.createSocketAddr(first.getName());\n     final Socket sock \u003d client.socketFactory.createSocket();\n     final int timeout \u003d client.getDatanodeReadTimeout(length);\n     NetUtils.connect(sock, isa, timeout);\n     sock.setSoTimeout(timeout);\n-    sock.setSendBufferSize(DFSClient.DEFAULT_DATA_SOCKET_SIZE);\n+    sock.setSendBufferSize(FSConstants.DEFAULT_DATA_SOCKET_SIZE);\n     if(DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n     }\n     return sock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + first.getName());\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(first.getName());\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, timeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(FSConstants.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,16 @@\n+  static Socket createSocketForPipeline(final DatanodeInfo first,\n+      final int length, final DFSClient client) throws IOException {\n+    if(DFSClient.LOG.isDebugEnabled()) {\n+      DFSClient.LOG.debug(\"Connecting to datanode \" + first.getName());\n+    }\n+    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(first.getName());\n+    final Socket sock \u003d client.socketFactory.createSocket();\n+    final int timeout \u003d client.getDatanodeReadTimeout(length);\n+    NetUtils.connect(sock, isa, timeout);\n+    sock.setSoTimeout(timeout);\n+    sock.setSendBufferSize(DFSClient.DEFAULT_DATA_SOCKET_SIZE);\n+    if(DFSClient.LOG.isDebugEnabled()) {\n+      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n+    }\n+    return sock;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  static Socket createSocketForPipeline(final DatanodeInfo first,\n      final int length, final DFSClient client) throws IOException {\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Connecting to datanode \" + first.getName());\n    }\n    final InetSocketAddress isa \u003d NetUtils.createSocketAddr(first.getName());\n    final Socket sock \u003d client.socketFactory.createSocket();\n    final int timeout \u003d client.getDatanodeReadTimeout(length);\n    NetUtils.connect(sock, isa, timeout);\n    sock.setSoTimeout(timeout);\n    sock.setSendBufferSize(DFSClient.DEFAULT_DATA_SOCKET_SIZE);\n    if(DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"Send buf size \" + sock.getSendBufferSize());\n    }\n    return sock;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}