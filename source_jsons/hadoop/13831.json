{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "chooseSourceDatanodes",
  "functionId": "chooseSourceDatanodes___block-BlockInfo__containingNodes-List__DatanodeDescriptor____nodesContainingLiveReplicas-List__DatanodeStorageInfo____numReplicas-NumberReplicas__liveBlockIndices-List__Byte____liveBusyBlockIndices-List__Byte____priority-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 2395,
  "functionEndLine": 2503,
  "numCommitsSeen": 1246,
  "timeTaken": 16660,
  "changeHistory": [
    "429da635ec70f9abe5ab71e24c9f2eec0aa36e18",
    "02009c3bb762393540cdf92cfd9c840807272903",
    "9d25ae7669eed1a047578b574f42bd121b445a3c",
    "c4c8d5fd0e3c17ccdcf18ece8e005f510328b060",
    "0d5d0b914ac959ce2c41f483ac5b74f58053cd00",
    "ce58c05f1d89a72c787f3571f78a9464d0ab3933",
    "d1c303a49763029fffa5164295034af8e81e74a0",
    "c89b29bd421152f0e7e16936f18d9e852895c37a",
    "07b98e7830c2214340cb7f434df674057e89df94",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "6979cbfc1f4c28440816b56f5624765872b0be49",
    "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
    "70d6f201260086a3f12beaa317fede2a99639fef",
    "5411dc559d5f73e4153e76fdff94a26869c17a37",
    "73b86a5046fe3262dde7b05be46b18575e35fd5f"
  ],
  "changeHistoryShort": {
    "429da635ec70f9abe5ab71e24c9f2eec0aa36e18": "Ybodychange",
    "02009c3bb762393540cdf92cfd9c840807272903": "Ymultichange(Yparameterchange,Ybodychange)",
    "9d25ae7669eed1a047578b574f42bd121b445a3c": "Ybodychange",
    "c4c8d5fd0e3c17ccdcf18ece8e005f510328b060": "Ybodychange",
    "0d5d0b914ac959ce2c41f483ac5b74f58053cd00": "Ybodychange",
    "ce58c05f1d89a72c787f3571f78a9464d0ab3933": "Ybodychange",
    "d1c303a49763029fffa5164295034af8e81e74a0": "Ybodychange",
    "c89b29bd421152f0e7e16936f18d9e852895c37a": "Ybodychange",
    "07b98e7830c2214340cb7f434df674057e89df94": "Ybodychange",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ybodychange",
    "6979cbfc1f4c28440816b56f5624765872b0be49": "Ybodychange",
    "47b92f2b6f2dafc129a41b247f35e77c8e47ffba": "Ybodychange",
    "70d6f201260086a3f12beaa317fede2a99639fef": "Ymultichange(Yparameterchange,Ybodychange)",
    "5411dc559d5f73e4153e76fdff94a26869c17a37": "Ybodychange",
    "73b86a5046fe3262dde7b05be46b18575e35fd5f": "Ybodychange"
  },
  "changeHistoryDetails": {
    "429da635ec70f9abe5ab71e24c9f2eec0aa36e18": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15186. Erasure Coding: Decommission may generate the parity block\u0027s content with all 0 in some case. Contributed by Yao Guangdong.\n",
      "commitDate": "27/02/20 11:01 AM",
      "commitName": "429da635ec70f9abe5ab71e24c9f2eec0aa36e18",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "26/02/20 12:52 PM",
      "commitNameOld": "209630472a3216c9f13bcffa62b553f9fb7675ca",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 0.92,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,107 +1,109 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas, List\u003cByte\u003e liveBlockIndices,\n       List\u003cByte\u003e liveBusyBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n     DatanodeDescriptor decommissionedSrc \u003d null;\n \n     BitSet liveBitSet \u003d null;\n     BitSet decommissioningBitSet \u003d null;\n     if (isStriped) {\n       int blockNum \u003d ((BlockInfoStriped) block).getTotalBlockNum();\n       liveBitSet \u003d new BitSet(blockNum);\n       decommissioningBitSet \u003d new BitSet(blockNum);\n     }\n \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n               storage.getStorageType(), storage.getState());\n         }\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n       // Never use maintenance node not suitable for read\n       // or unknown state replicas.\n       if (state \u003d\u003d null\n           || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n         continue;\n       }\n \n       // Save the live decommissioned replica in case we need it. Such replicas\n       // are normally not used for replication, but if nothing else is\n       // available, one can be selected as a source.\n       if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n         if (decommissionedSrc \u003d\u003d null ||\n             ThreadLocalRandom.current().nextBoolean()) {\n           decommissionedSrc \u003d node;\n         }\n         continue;\n       }\n \n       // for EC here need to make sure the numReplicas replicates state correct\n       // because in the scheduleReconstruction it need the numReplicas to check\n       // whether need to reconstruct the ec internal block\n       byte blockIndex \u003d -1;\n       if (isStriped) {\n         blockIndex \u003d ((BlockInfoStriped) block)\n             .getStorageBlockIndex(storage);\n         countLiveAndDecommissioningReplicas(numReplicas, state,\n             liveBitSet, decommissioningBitSet, blockIndex);\n       }\n \n       if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n           \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n-        if (isStriped \u0026\u0026 state \u003d\u003d StoredReplicaState.LIVE) {\n+        if (isStriped \u0026\u0026 (state \u003d\u003d StoredReplicaState.LIVE\n+            || state \u003d\u003d StoredReplicaState.DECOMMISSIONING)) {\n           liveBusyBlockIndices.add(blockIndex);\n         }\n         continue; // already reached replication limit\n       }\n \n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n-        if (isStriped \u0026\u0026 state \u003d\u003d StoredReplicaState.LIVE) {\n+        if (isStriped \u0026\u0026 (state \u003d\u003d StoredReplicaState.LIVE\n+            || state \u003d\u003d StoredReplicaState.DECOMMISSIONING)) {\n           liveBusyBlockIndices.add(blockIndex);\n         }\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           liveBlockIndices.add(blockIndex);\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n \n     // Pick a live decommissioned replica, if nothing else is available.\n     if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n         srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n       srcNodes.add(decommissionedSrc);\n     }\n \n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas, List\u003cByte\u003e liveBlockIndices,\n      List\u003cByte\u003e liveBusyBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n    DatanodeDescriptor decommissionedSrc \u003d null;\n\n    BitSet liveBitSet \u003d null;\n    BitSet decommissioningBitSet \u003d null;\n    if (isStriped) {\n      int blockNum \u003d ((BlockInfoStriped) block).getTotalBlockNum();\n      liveBitSet \u003d new BitSet(blockNum);\n      decommissioningBitSet \u003d new BitSet(blockNum);\n    }\n\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n              storage.getStorageType(), storage.getState());\n        }\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // Never use maintenance node not suitable for read\n      // or unknown state replicas.\n      if (state \u003d\u003d null\n          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n        continue;\n      }\n\n      // Save the live decommissioned replica in case we need it. Such replicas\n      // are normally not used for replication, but if nothing else is\n      // available, one can be selected as a source.\n      if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        if (decommissionedSrc \u003d\u003d null ||\n            ThreadLocalRandom.current().nextBoolean()) {\n          decommissionedSrc \u003d node;\n        }\n        continue;\n      }\n\n      // for EC here need to make sure the numReplicas replicates state correct\n      // because in the scheduleReconstruction it need the numReplicas to check\n      // whether need to reconstruct the ec internal block\n      byte blockIndex \u003d -1;\n      if (isStriped) {\n        blockIndex \u003d ((BlockInfoStriped) block)\n            .getStorageBlockIndex(storage);\n        countLiveAndDecommissioningReplicas(numReplicas, state,\n            liveBitSet, decommissioningBitSet, blockIndex);\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        if (isStriped \u0026\u0026 (state \u003d\u003d StoredReplicaState.LIVE\n            || state \u003d\u003d StoredReplicaState.DECOMMISSIONING)) {\n          liveBusyBlockIndices.add(blockIndex);\n        }\n        continue; // already reached replication limit\n      }\n\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        if (isStriped \u0026\u0026 (state \u003d\u003d StoredReplicaState.LIVE\n            || state \u003d\u003d StoredReplicaState.DECOMMISSIONING)) {\n          liveBusyBlockIndices.add(blockIndex);\n        }\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add(blockIndex);\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n\n    // Pick a live decommissioned replica, if nothing else is available.\n    if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n        srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n      srcNodes.add(decommissionedSrc);\n    }\n\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "02009c3bb762393540cdf92cfd9c840807272903": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-14768. EC : Busy DN replica should be consider in live replica check. Contributed by guojh.\n",
      "commitDate": "01/11/19 9:45 AM",
      "commitName": "02009c3bb762393540cdf92cfd9c840807272903",
      "commitAuthor": "Surendra Singh Lilhore",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-14768. EC : Busy DN replica should be consider in live replica check. Contributed by guojh.\n",
          "commitDate": "01/11/19 9:45 AM",
          "commitName": "02009c3bb762393540cdf92cfd9c840807272903",
          "commitAuthor": "Surendra Singh Lilhore",
          "commitDateOld": "31/10/19 11:19 AM",
          "commitNameOld": "9d25ae7669eed1a047578b574f42bd121b445a3c",
          "commitAuthorOld": "Ayush Saxena",
          "daysBetweenCommits": 0.93,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,101 +1,107 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n-      NumberReplicas numReplicas,\n-      List\u003cByte\u003e liveBlockIndices, int priority) {\n+      NumberReplicas numReplicas, List\u003cByte\u003e liveBlockIndices,\n+      List\u003cByte\u003e liveBusyBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n     DatanodeDescriptor decommissionedSrc \u003d null;\n \n     BitSet liveBitSet \u003d null;\n     BitSet decommissioningBitSet \u003d null;\n     if (isStriped) {\n       int blockNum \u003d ((BlockInfoStriped) block).getTotalBlockNum();\n       liveBitSet \u003d new BitSet(blockNum);\n       decommissioningBitSet \u003d new BitSet(blockNum);\n     }\n \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n               storage.getStorageType(), storage.getState());\n         }\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n       // Never use maintenance node not suitable for read\n       // or unknown state replicas.\n       if (state \u003d\u003d null\n           || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n         continue;\n       }\n \n       // Save the live decommissioned replica in case we need it. Such replicas\n       // are normally not used for replication, but if nothing else is\n       // available, one can be selected as a source.\n       if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n         if (decommissionedSrc \u003d\u003d null ||\n             ThreadLocalRandom.current().nextBoolean()) {\n           decommissionedSrc \u003d node;\n         }\n         continue;\n       }\n \n-      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n-          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n-          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n-        continue; // already reached replication limit\n-      }\n-\n       // for EC here need to make sure the numReplicas replicates state correct\n       // because in the scheduleReconstruction it need the numReplicas to check\n       // whether need to reconstruct the ec internal block\n       byte blockIndex \u003d -1;\n       if (isStriped) {\n         blockIndex \u003d ((BlockInfoStriped) block)\n             .getStorageBlockIndex(storage);\n         countLiveAndDecommissioningReplicas(numReplicas, state,\n             liveBitSet, decommissioningBitSet, blockIndex);\n       }\n \n+      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n+          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n+          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n+        if (isStriped \u0026\u0026 state \u003d\u003d StoredReplicaState.LIVE) {\n+          liveBusyBlockIndices.add(blockIndex);\n+        }\n+        continue; // already reached replication limit\n+      }\n+\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n+        if (isStriped \u0026\u0026 state \u003d\u003d StoredReplicaState.LIVE) {\n+          liveBusyBlockIndices.add(blockIndex);\n+        }\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           liveBlockIndices.add(blockIndex);\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n \n     // Pick a live decommissioned replica, if nothing else is available.\n     if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n         srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n       srcNodes.add(decommissionedSrc);\n     }\n \n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas, List\u003cByte\u003e liveBlockIndices,\n      List\u003cByte\u003e liveBusyBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n    DatanodeDescriptor decommissionedSrc \u003d null;\n\n    BitSet liveBitSet \u003d null;\n    BitSet decommissioningBitSet \u003d null;\n    if (isStriped) {\n      int blockNum \u003d ((BlockInfoStriped) block).getTotalBlockNum();\n      liveBitSet \u003d new BitSet(blockNum);\n      decommissioningBitSet \u003d new BitSet(blockNum);\n    }\n\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n              storage.getStorageType(), storage.getState());\n        }\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // Never use maintenance node not suitable for read\n      // or unknown state replicas.\n      if (state \u003d\u003d null\n          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n        continue;\n      }\n\n      // Save the live decommissioned replica in case we need it. Such replicas\n      // are normally not used for replication, but if nothing else is\n      // available, one can be selected as a source.\n      if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        if (decommissionedSrc \u003d\u003d null ||\n            ThreadLocalRandom.current().nextBoolean()) {\n          decommissionedSrc \u003d node;\n        }\n        continue;\n      }\n\n      // for EC here need to make sure the numReplicas replicates state correct\n      // because in the scheduleReconstruction it need the numReplicas to check\n      // whether need to reconstruct the ec internal block\n      byte blockIndex \u003d -1;\n      if (isStriped) {\n        blockIndex \u003d ((BlockInfoStriped) block)\n            .getStorageBlockIndex(storage);\n        countLiveAndDecommissioningReplicas(numReplicas, state,\n            liveBitSet, decommissioningBitSet, blockIndex);\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        if (isStriped \u0026\u0026 state \u003d\u003d StoredReplicaState.LIVE) {\n          liveBusyBlockIndices.add(blockIndex);\n        }\n        continue; // already reached replication limit\n      }\n\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        if (isStriped \u0026\u0026 state \u003d\u003d StoredReplicaState.LIVE) {\n          liveBusyBlockIndices.add(blockIndex);\n        }\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add(blockIndex);\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n\n    // Pick a live decommissioned replica, if nothing else is available.\n    if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n        srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n      srcNodes.add(decommissionedSrc);\n    }\n\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[block-BlockInfo, containingNodes-List\u003cDatanodeDescriptor\u003e, nodesContainingLiveReplicas-List\u003cDatanodeStorageInfo\u003e, numReplicas-NumberReplicas, liveBlockIndices-List\u003cByte\u003e, priority-int]",
            "newValue": "[block-BlockInfo, containingNodes-List\u003cDatanodeDescriptor\u003e, nodesContainingLiveReplicas-List\u003cDatanodeStorageInfo\u003e, numReplicas-NumberReplicas, liveBlockIndices-List\u003cByte\u003e, liveBusyBlockIndices-List\u003cByte\u003e, priority-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-14768. EC : Busy DN replica should be consider in live replica check. Contributed by guojh.\n",
          "commitDate": "01/11/19 9:45 AM",
          "commitName": "02009c3bb762393540cdf92cfd9c840807272903",
          "commitAuthor": "Surendra Singh Lilhore",
          "commitDateOld": "31/10/19 11:19 AM",
          "commitNameOld": "9d25ae7669eed1a047578b574f42bd121b445a3c",
          "commitAuthorOld": "Ayush Saxena",
          "daysBetweenCommits": 0.93,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,101 +1,107 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n-      NumberReplicas numReplicas,\n-      List\u003cByte\u003e liveBlockIndices, int priority) {\n+      NumberReplicas numReplicas, List\u003cByte\u003e liveBlockIndices,\n+      List\u003cByte\u003e liveBusyBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n     DatanodeDescriptor decommissionedSrc \u003d null;\n \n     BitSet liveBitSet \u003d null;\n     BitSet decommissioningBitSet \u003d null;\n     if (isStriped) {\n       int blockNum \u003d ((BlockInfoStriped) block).getTotalBlockNum();\n       liveBitSet \u003d new BitSet(blockNum);\n       decommissioningBitSet \u003d new BitSet(blockNum);\n     }\n \n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n               storage.getStorageType(), storage.getState());\n         }\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n       // Never use maintenance node not suitable for read\n       // or unknown state replicas.\n       if (state \u003d\u003d null\n           || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n         continue;\n       }\n \n       // Save the live decommissioned replica in case we need it. Such replicas\n       // are normally not used for replication, but if nothing else is\n       // available, one can be selected as a source.\n       if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n         if (decommissionedSrc \u003d\u003d null ||\n             ThreadLocalRandom.current().nextBoolean()) {\n           decommissionedSrc \u003d node;\n         }\n         continue;\n       }\n \n-      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n-          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n-          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n-        continue; // already reached replication limit\n-      }\n-\n       // for EC here need to make sure the numReplicas replicates state correct\n       // because in the scheduleReconstruction it need the numReplicas to check\n       // whether need to reconstruct the ec internal block\n       byte blockIndex \u003d -1;\n       if (isStriped) {\n         blockIndex \u003d ((BlockInfoStriped) block)\n             .getStorageBlockIndex(storage);\n         countLiveAndDecommissioningReplicas(numReplicas, state,\n             liveBitSet, decommissioningBitSet, blockIndex);\n       }\n \n+      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n+          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n+          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n+        if (isStriped \u0026\u0026 state \u003d\u003d StoredReplicaState.LIVE) {\n+          liveBusyBlockIndices.add(blockIndex);\n+        }\n+        continue; // already reached replication limit\n+      }\n+\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n+        if (isStriped \u0026\u0026 state \u003d\u003d StoredReplicaState.LIVE) {\n+          liveBusyBlockIndices.add(blockIndex);\n+        }\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           liveBlockIndices.add(blockIndex);\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n \n     // Pick a live decommissioned replica, if nothing else is available.\n     if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n         srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n       srcNodes.add(decommissionedSrc);\n     }\n \n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas, List\u003cByte\u003e liveBlockIndices,\n      List\u003cByte\u003e liveBusyBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n    DatanodeDescriptor decommissionedSrc \u003d null;\n\n    BitSet liveBitSet \u003d null;\n    BitSet decommissioningBitSet \u003d null;\n    if (isStriped) {\n      int blockNum \u003d ((BlockInfoStriped) block).getTotalBlockNum();\n      liveBitSet \u003d new BitSet(blockNum);\n      decommissioningBitSet \u003d new BitSet(blockNum);\n    }\n\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n              storage.getStorageType(), storage.getState());\n        }\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // Never use maintenance node not suitable for read\n      // or unknown state replicas.\n      if (state \u003d\u003d null\n          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n        continue;\n      }\n\n      // Save the live decommissioned replica in case we need it. Such replicas\n      // are normally not used for replication, but if nothing else is\n      // available, one can be selected as a source.\n      if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        if (decommissionedSrc \u003d\u003d null ||\n            ThreadLocalRandom.current().nextBoolean()) {\n          decommissionedSrc \u003d node;\n        }\n        continue;\n      }\n\n      // for EC here need to make sure the numReplicas replicates state correct\n      // because in the scheduleReconstruction it need the numReplicas to check\n      // whether need to reconstruct the ec internal block\n      byte blockIndex \u003d -1;\n      if (isStriped) {\n        blockIndex \u003d ((BlockInfoStriped) block)\n            .getStorageBlockIndex(storage);\n        countLiveAndDecommissioningReplicas(numReplicas, state,\n            liveBitSet, decommissioningBitSet, blockIndex);\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        if (isStriped \u0026\u0026 state \u003d\u003d StoredReplicaState.LIVE) {\n          liveBusyBlockIndices.add(blockIndex);\n        }\n        continue; // already reached replication limit\n      }\n\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        if (isStriped \u0026\u0026 state \u003d\u003d StoredReplicaState.LIVE) {\n          liveBusyBlockIndices.add(blockIndex);\n        }\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add(blockIndex);\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n\n    // Pick a live decommissioned replica, if nothing else is available.\n    if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n        srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n      srcNodes.add(decommissionedSrc);\n    }\n\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "9d25ae7669eed1a047578b574f42bd121b445a3c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14920. Erasure Coding: Decommission may hang If one or more datanodes are out of service during decommission. Contributed by Fei Hui.\n",
      "commitDate": "31/10/19 11:19 AM",
      "commitName": "9d25ae7669eed1a047578b574f42bd121b445a3c",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "03/10/19 10:13 PM",
      "commitNameOld": "c99a12167ff9566012ef32104a3964887d62c899",
      "commitAuthorOld": "Stephen O\u0027Donnell",
      "daysBetweenCommits": 27.55,
      "commitsBetweenForRepo": 128,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,101 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n     DatanodeDescriptor decommissionedSrc \u003d null;\n \n-    BitSet bitSet \u003d isStriped ?\n-        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n+    BitSet liveBitSet \u003d null;\n+    BitSet decommissioningBitSet \u003d null;\n+    if (isStriped) {\n+      int blockNum \u003d ((BlockInfoStriped) block).getTotalBlockNum();\n+      liveBitSet \u003d new BitSet(blockNum);\n+      decommissioningBitSet \u003d new BitSet(blockNum);\n+    }\n+\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n               storage.getStorageType(), storage.getState());\n         }\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n       // Never use maintenance node not suitable for read\n       // or unknown state replicas.\n       if (state \u003d\u003d null\n           || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n         continue;\n       }\n \n       // Save the live decommissioned replica in case we need it. Such replicas\n       // are normally not used for replication, but if nothing else is\n       // available, one can be selected as a source.\n       if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n         if (decommissionedSrc \u003d\u003d null ||\n             ThreadLocalRandom.current().nextBoolean()) {\n           decommissionedSrc \u003d node;\n         }\n         continue;\n       }\n \n       if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n           \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n         continue; // already reached replication limit\n       }\n \n       // for EC here need to make sure the numReplicas replicates state correct\n       // because in the scheduleReconstruction it need the numReplicas to check\n       // whether need to reconstruct the ec internal block\n       byte blockIndex \u003d -1;\n       if (isStriped) {\n         blockIndex \u003d ((BlockInfoStriped) block)\n             .getStorageBlockIndex(storage);\n-        if (state \u003d\u003d StoredReplicaState.LIVE) {\n-          if (!bitSet.get(blockIndex)) {\n-            bitSet.set(blockIndex);\n-          } else {\n-            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n-            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n-          }\n-        }\n+        countLiveAndDecommissioningReplicas(numReplicas, state,\n+            liveBitSet, decommissioningBitSet, blockIndex);\n       }\n \n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           liveBlockIndices.add(blockIndex);\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n \n     // Pick a live decommissioned replica, if nothing else is available.\n     if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n         srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n       srcNodes.add(decommissionedSrc);\n     }\n \n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n    DatanodeDescriptor decommissionedSrc \u003d null;\n\n    BitSet liveBitSet \u003d null;\n    BitSet decommissioningBitSet \u003d null;\n    if (isStriped) {\n      int blockNum \u003d ((BlockInfoStriped) block).getTotalBlockNum();\n      liveBitSet \u003d new BitSet(blockNum);\n      decommissioningBitSet \u003d new BitSet(blockNum);\n    }\n\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n              storage.getStorageType(), storage.getState());\n        }\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // Never use maintenance node not suitable for read\n      // or unknown state replicas.\n      if (state \u003d\u003d null\n          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n        continue;\n      }\n\n      // Save the live decommissioned replica in case we need it. Such replicas\n      // are normally not used for replication, but if nothing else is\n      // available, one can be selected as a source.\n      if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        if (decommissionedSrc \u003d\u003d null ||\n            ThreadLocalRandom.current().nextBoolean()) {\n          decommissionedSrc \u003d node;\n        }\n        continue;\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        continue; // already reached replication limit\n      }\n\n      // for EC here need to make sure the numReplicas replicates state correct\n      // because in the scheduleReconstruction it need the numReplicas to check\n      // whether need to reconstruct the ec internal block\n      byte blockIndex \u003d -1;\n      if (isStriped) {\n        blockIndex \u003d ((BlockInfoStriped) block)\n            .getStorageBlockIndex(storage);\n        countLiveAndDecommissioningReplicas(numReplicas, state,\n            liveBitSet, decommissioningBitSet, blockIndex);\n      }\n\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add(blockIndex);\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n\n    // Pick a live decommissioned replica, if nothing else is available.\n    if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n        srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n      srcNodes.add(decommissionedSrc);\n    }\n\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "c4c8d5fd0e3c17ccdcf18ece8e005f510328b060": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14849. Erasure Coding: the internal block is replicated many times when datanode is decommissioning. Contributed by HuangTao.\n",
      "commitDate": "28/09/19 9:14 AM",
      "commitName": "c4c8d5fd0e3c17ccdcf18ece8e005f510328b060",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "28/09/19 9:02 AM",
      "commitNameOld": "0d5d0b914ac959ce2c41f483ac5b74f58053cd00",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,99 +1,101 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n     DatanodeDescriptor decommissionedSrc \u003d null;\n \n     BitSet bitSet \u003d isStriped ?\n         new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n               storage.getStorageType(), storage.getState());\n         }\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n       // Never use maintenance node not suitable for read\n       // or unknown state replicas.\n       if (state \u003d\u003d null\n           || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n         continue;\n       }\n \n       // Save the live decommissioned replica in case we need it. Such replicas\n       // are normally not used for replication, but if nothing else is\n       // available, one can be selected as a source.\n       if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n         if (decommissionedSrc \u003d\u003d null ||\n             ThreadLocalRandom.current().nextBoolean()) {\n           decommissionedSrc \u003d node;\n         }\n         continue;\n       }\n \n       if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n           \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n         continue; // already reached replication limit\n       }\n \n       // for EC here need to make sure the numReplicas replicates state correct\n       // because in the scheduleReconstruction it need the numReplicas to check\n       // whether need to reconstruct the ec internal block\n       byte blockIndex \u003d -1;\n       if (isStriped) {\n         blockIndex \u003d ((BlockInfoStriped) block)\n             .getStorageBlockIndex(storage);\n-        if (!bitSet.get(blockIndex)) {\n-          bitSet.set(blockIndex);\n-        } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n-          numReplicas.subtract(StoredReplicaState.LIVE, 1);\n-          numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n+        if (state \u003d\u003d StoredReplicaState.LIVE) {\n+          if (!bitSet.get(blockIndex)) {\n+            bitSet.set(blockIndex);\n+          } else {\n+            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n+            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n+          }\n         }\n       }\n \n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           liveBlockIndices.add(blockIndex);\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n \n     // Pick a live decommissioned replica, if nothing else is available.\n     if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n         srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n       srcNodes.add(decommissionedSrc);\n     }\n \n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n    DatanodeDescriptor decommissionedSrc \u003d null;\n\n    BitSet bitSet \u003d isStriped ?\n        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n              storage.getStorageType(), storage.getState());\n        }\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // Never use maintenance node not suitable for read\n      // or unknown state replicas.\n      if (state \u003d\u003d null\n          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n        continue;\n      }\n\n      // Save the live decommissioned replica in case we need it. Such replicas\n      // are normally not used for replication, but if nothing else is\n      // available, one can be selected as a source.\n      if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        if (decommissionedSrc \u003d\u003d null ||\n            ThreadLocalRandom.current().nextBoolean()) {\n          decommissionedSrc \u003d node;\n        }\n        continue;\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        continue; // already reached replication limit\n      }\n\n      // for EC here need to make sure the numReplicas replicates state correct\n      // because in the scheduleReconstruction it need the numReplicas to check\n      // whether need to reconstruct the ec internal block\n      byte blockIndex \u003d -1;\n      if (isStriped) {\n        blockIndex \u003d ((BlockInfoStriped) block)\n            .getStorageBlockIndex(storage);\n        if (state \u003d\u003d StoredReplicaState.LIVE) {\n          if (!bitSet.get(blockIndex)) {\n            bitSet.set(blockIndex);\n          } else {\n            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n          }\n        }\n      }\n\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add(blockIndex);\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n\n    // Pick a live decommissioned replica, if nothing else is available.\n    if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n        srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n      srcNodes.add(decommissionedSrc);\n    }\n\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "0d5d0b914ac959ce2c41f483ac5b74f58053cd00": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-14849. Erasure Coding: the internal block is replicated many times when datanode is decommissioning. Contributed by HuangTao.\"\n\nThis reverts commit ce58c05f1d89a72c787f3571f78a9464d0ab3933.\n",
      "commitDate": "28/09/19 9:02 AM",
      "commitName": "0d5d0b914ac959ce2c41f483ac5b74f58053cd00",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "27/09/19 6:28 AM",
      "commitNameOld": "ce58c05f1d89a72c787f3571f78a9464d0ab3933",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 1.11,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,99 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n     DatanodeDescriptor decommissionedSrc \u003d null;\n \n     BitSet bitSet \u003d isStriped ?\n         new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n               storage.getStorageType(), storage.getState());\n         }\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n       // Never use maintenance node not suitable for read\n       // or unknown state replicas.\n       if (state \u003d\u003d null\n           || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n         continue;\n       }\n \n       // Save the live decommissioned replica in case we need it. Such replicas\n       // are normally not used for replication, but if nothing else is\n       // available, one can be selected as a source.\n       if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n         if (decommissionedSrc \u003d\u003d null ||\n             ThreadLocalRandom.current().nextBoolean()) {\n           decommissionedSrc \u003d node;\n         }\n         continue;\n       }\n \n       if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n           \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n         continue; // already reached replication limit\n       }\n \n       // for EC here need to make sure the numReplicas replicates state correct\n       // because in the scheduleReconstruction it need the numReplicas to check\n       // whether need to reconstruct the ec internal block\n       byte blockIndex \u003d -1;\n       if (isStriped) {\n         blockIndex \u003d ((BlockInfoStriped) block)\n             .getStorageBlockIndex(storage);\n-        if (state \u003d\u003d StoredReplicaState.LIVE) {\n-          if (!bitSet.get(blockIndex)) {\n-            bitSet.set(blockIndex);\n-          } else {\n-            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n-            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n-          }\n+        if (!bitSet.get(blockIndex)) {\n+          bitSet.set(blockIndex);\n+        } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n+          numReplicas.subtract(StoredReplicaState.LIVE, 1);\n+          numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n         }\n       }\n \n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           liveBlockIndices.add(blockIndex);\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n \n     // Pick a live decommissioned replica, if nothing else is available.\n     if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n         srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n       srcNodes.add(decommissionedSrc);\n     }\n \n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n    DatanodeDescriptor decommissionedSrc \u003d null;\n\n    BitSet bitSet \u003d isStriped ?\n        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n              storage.getStorageType(), storage.getState());\n        }\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // Never use maintenance node not suitable for read\n      // or unknown state replicas.\n      if (state \u003d\u003d null\n          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n        continue;\n      }\n\n      // Save the live decommissioned replica in case we need it. Such replicas\n      // are normally not used for replication, but if nothing else is\n      // available, one can be selected as a source.\n      if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        if (decommissionedSrc \u003d\u003d null ||\n            ThreadLocalRandom.current().nextBoolean()) {\n          decommissionedSrc \u003d node;\n        }\n        continue;\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        continue; // already reached replication limit\n      }\n\n      // for EC here need to make sure the numReplicas replicates state correct\n      // because in the scheduleReconstruction it need the numReplicas to check\n      // whether need to reconstruct the ec internal block\n      byte blockIndex \u003d -1;\n      if (isStriped) {\n        blockIndex \u003d ((BlockInfoStriped) block)\n            .getStorageBlockIndex(storage);\n        if (!bitSet.get(blockIndex)) {\n          bitSet.set(blockIndex);\n        } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n          numReplicas.subtract(StoredReplicaState.LIVE, 1);\n          numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n        }\n      }\n\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add(blockIndex);\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n\n    // Pick a live decommissioned replica, if nothing else is available.\n    if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n        srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n      srcNodes.add(decommissionedSrc);\n    }\n\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "ce58c05f1d89a72c787f3571f78a9464d0ab3933": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14849. Erasure Coding: the internal block is replicated many times when datanode is decommissioning. Contributed by HuangTao.\n",
      "commitDate": "27/09/19 6:28 AM",
      "commitName": "ce58c05f1d89a72c787f3571f78a9464d0ab3933",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "24/09/19 1:01 PM",
      "commitNameOld": "66400c1cbb2b4b2f08f7db965c8b7237072bdcc4",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 2.73,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,99 +1,101 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n     DatanodeDescriptor decommissionedSrc \u003d null;\n \n     BitSet bitSet \u003d isStriped ?\n         new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n               storage.getStorageType(), storage.getState());\n         }\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n       // Never use maintenance node not suitable for read\n       // or unknown state replicas.\n       if (state \u003d\u003d null\n           || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n         continue;\n       }\n \n       // Save the live decommissioned replica in case we need it. Such replicas\n       // are normally not used for replication, but if nothing else is\n       // available, one can be selected as a source.\n       if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n         if (decommissionedSrc \u003d\u003d null ||\n             ThreadLocalRandom.current().nextBoolean()) {\n           decommissionedSrc \u003d node;\n         }\n         continue;\n       }\n \n       if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n           \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n         continue; // already reached replication limit\n       }\n \n       // for EC here need to make sure the numReplicas replicates state correct\n       // because in the scheduleReconstruction it need the numReplicas to check\n       // whether need to reconstruct the ec internal block\n       byte blockIndex \u003d -1;\n       if (isStriped) {\n         blockIndex \u003d ((BlockInfoStriped) block)\n             .getStorageBlockIndex(storage);\n-        if (!bitSet.get(blockIndex)) {\n-          bitSet.set(blockIndex);\n-        } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n-          numReplicas.subtract(StoredReplicaState.LIVE, 1);\n-          numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n+        if (state \u003d\u003d StoredReplicaState.LIVE) {\n+          if (!bitSet.get(blockIndex)) {\n+            bitSet.set(blockIndex);\n+          } else {\n+            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n+            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n+          }\n         }\n       }\n \n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           liveBlockIndices.add(blockIndex);\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n \n     // Pick a live decommissioned replica, if nothing else is available.\n     if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n         srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n       srcNodes.add(decommissionedSrc);\n     }\n \n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n    DatanodeDescriptor decommissionedSrc \u003d null;\n\n    BitSet bitSet \u003d isStriped ?\n        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n              storage.getStorageType(), storage.getState());\n        }\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // Never use maintenance node not suitable for read\n      // or unknown state replicas.\n      if (state \u003d\u003d null\n          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n        continue;\n      }\n\n      // Save the live decommissioned replica in case we need it. Such replicas\n      // are normally not used for replication, but if nothing else is\n      // available, one can be selected as a source.\n      if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        if (decommissionedSrc \u003d\u003d null ||\n            ThreadLocalRandom.current().nextBoolean()) {\n          decommissionedSrc \u003d node;\n        }\n        continue;\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        continue; // already reached replication limit\n      }\n\n      // for EC here need to make sure the numReplicas replicates state correct\n      // because in the scheduleReconstruction it need the numReplicas to check\n      // whether need to reconstruct the ec internal block\n      byte blockIndex \u003d -1;\n      if (isStriped) {\n        blockIndex \u003d ((BlockInfoStriped) block)\n            .getStorageBlockIndex(storage);\n        if (state \u003d\u003d StoredReplicaState.LIVE) {\n          if (!bitSet.get(blockIndex)) {\n            bitSet.set(blockIndex);\n          } else {\n            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n          }\n        }\n      }\n\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add(blockIndex);\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n\n    // Pick a live decommissioned replica, if nothing else is available.\n    if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n        srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n      srcNodes.add(decommissionedSrc);\n    }\n\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "d1c303a49763029fffa5164295034af8e81e74a0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14699. Erasure Coding: Storage not considered in live replica when replication streams hard limit reached to threshold. Contributed by Zhao Yi Ming.\n",
      "commitDate": "12/09/19 6:41 AM",
      "commitName": "d1c303a49763029fffa5164295034af8e81e74a0",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "29/07/19 2:31 PM",
      "commitNameOld": "8053085388fec17a40856f36821c142be32aa364",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 44.67,
      "commitsBetweenForRepo": 426,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,99 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n     DatanodeDescriptor decommissionedSrc \u003d null;\n \n     BitSet bitSet \u003d isStriped ?\n         new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n               storage.getStorageType(), storage.getState());\n         }\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n       // Never use maintenance node not suitable for read\n       // or unknown state replicas.\n       if (state \u003d\u003d null\n           || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n         continue;\n       }\n \n       // Save the live decommissioned replica in case we need it. Such replicas\n       // are normally not used for replication, but if nothing else is\n       // available, one can be selected as a source.\n       if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n         if (decommissionedSrc \u003d\u003d null ||\n             ThreadLocalRandom.current().nextBoolean()) {\n           decommissionedSrc \u003d node;\n         }\n         continue;\n       }\n \n       if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n           \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n         continue; // already reached replication limit\n       }\n+\n+      // for EC here need to make sure the numReplicas replicates state correct\n+      // because in the scheduleReconstruction it need the numReplicas to check\n+      // whether need to reconstruct the ec internal block\n+      byte blockIndex \u003d -1;\n+      if (isStriped) {\n+        blockIndex \u003d ((BlockInfoStriped) block)\n+            .getStorageBlockIndex(storage);\n+        if (!bitSet.get(blockIndex)) {\n+          bitSet.set(blockIndex);\n+        } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n+          numReplicas.subtract(StoredReplicaState.LIVE, 1);\n+          numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n+        }\n+      }\n+\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n-          byte blockIndex \u003d ((BlockInfoStriped) block).\n-              getStorageBlockIndex(storage);\n           liveBlockIndices.add(blockIndex);\n-          if (!bitSet.get(blockIndex)) {\n-            bitSet.set(blockIndex);\n-          } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n-            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n-            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n-          }\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n \n     // Pick a live decommissioned replica, if nothing else is available.\n     if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n         srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n       srcNodes.add(decommissionedSrc);\n     }\n \n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n    DatanodeDescriptor decommissionedSrc \u003d null;\n\n    BitSet bitSet \u003d isStriped ?\n        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n              storage.getStorageType(), storage.getState());\n        }\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // Never use maintenance node not suitable for read\n      // or unknown state replicas.\n      if (state \u003d\u003d null\n          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n        continue;\n      }\n\n      // Save the live decommissioned replica in case we need it. Such replicas\n      // are normally not used for replication, but if nothing else is\n      // available, one can be selected as a source.\n      if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        if (decommissionedSrc \u003d\u003d null ||\n            ThreadLocalRandom.current().nextBoolean()) {\n          decommissionedSrc \u003d node;\n        }\n        continue;\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        continue; // already reached replication limit\n      }\n\n      // for EC here need to make sure the numReplicas replicates state correct\n      // because in the scheduleReconstruction it need the numReplicas to check\n      // whether need to reconstruct the ec internal block\n      byte blockIndex \u003d -1;\n      if (isStriped) {\n        blockIndex \u003d ((BlockInfoStriped) block)\n            .getStorageBlockIndex(storage);\n        if (!bitSet.get(blockIndex)) {\n          bitSet.set(blockIndex);\n        } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n          numReplicas.subtract(StoredReplicaState.LIVE, 1);\n          numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n        }\n      }\n\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add(blockIndex);\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n\n    // Pick a live decommissioned replica, if nothing else is available.\n    if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n        srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n      srcNodes.add(decommissionedSrc);\n    }\n\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "c89b29bd421152f0e7e16936f18d9e852895c37a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12893. [READ] Support replication of Provided blocks with non-default topologies.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "c89b29bd421152f0e7e16936f18d9e852895c37a",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "fb996a32a98a25c0fe34a8ebb28563b53cd6e20e",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,91 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n     DatanodeDescriptor decommissionedSrc \u003d null;\n \n     BitSet bitSet \u003d isStriped ?\n         new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n-      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n+      final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n+        if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n+          storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n+              storage.getStorageType(), storage.getState());\n+        }\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n       // Never use maintenance node not suitable for read\n       // or unknown state replicas.\n       if (state \u003d\u003d null\n           || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n         continue;\n       }\n \n       // Save the live decommissioned replica in case we need it. Such replicas\n       // are normally not used for replication, but if nothing else is\n       // available, one can be selected as a source.\n       if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n         if (decommissionedSrc \u003d\u003d null ||\n             ThreadLocalRandom.current().nextBoolean()) {\n           decommissionedSrc \u003d node;\n         }\n         continue;\n       }\n \n       if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n           \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n         continue; // already reached replication limit\n       }\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           byte blockIndex \u003d ((BlockInfoStriped) block).\n               getStorageBlockIndex(storage);\n           liveBlockIndices.add(blockIndex);\n           if (!bitSet.get(blockIndex)) {\n             bitSet.set(blockIndex);\n           } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n             numReplicas.subtract(StoredReplicaState.LIVE, 1);\n             numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n           }\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n \n     // Pick a live decommissioned replica, if nothing else is available.\n     if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n         srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n       srcNodes.add(decommissionedSrc);\n     }\n \n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n    DatanodeDescriptor decommissionedSrc \u003d null;\n\n    BitSet bitSet \u003d isStriped ?\n        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d getDatanodeDescriptorFromStorage(storage);\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          storage \u003d new DatanodeStorageInfo(node, storage.getStorageID(),\n              storage.getStorageType(), storage.getState());\n        }\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // Never use maintenance node not suitable for read\n      // or unknown state replicas.\n      if (state \u003d\u003d null\n          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n        continue;\n      }\n\n      // Save the live decommissioned replica in case we need it. Such replicas\n      // are normally not used for replication, but if nothing else is\n      // available, one can be selected as a source.\n      if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        if (decommissionedSrc \u003d\u003d null ||\n            ThreadLocalRandom.current().nextBoolean()) {\n          decommissionedSrc \u003d node;\n        }\n        continue;\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        continue; // already reached replication limit\n      }\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          byte blockIndex \u003d ((BlockInfoStriped) block).\n              getStorageBlockIndex(storage);\n          liveBlockIndices.add(blockIndex);\n          if (!bitSet.get(blockIndex)) {\n            bitSet.set(blockIndex);\n          } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n          }\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n\n    // Pick a live decommissioned replica, if nothing else is available.\n    if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n        srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n      srcNodes.add(decommissionedSrc);\n    }\n\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "07b98e7830c2214340cb7f434df674057e89df94": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11609. Some blocks can be permanently lost if nodes are decommissioned while dead. Contributed by Kihwal Lee.\n",
      "commitDate": "01/05/17 12:19 PM",
      "commitName": "07b98e7830c2214340cb7f434df674057e89df94",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "25/04/17 11:57 PM",
      "commitNameOld": "2f73396b5901fd5fe29f6cd76fc1b3134b854b37",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 5.52,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,87 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n+    DatanodeDescriptor decommissionedSrc \u003d null;\n \n     BitSet bitSet \u003d isStriped ?\n         new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n-      // never use already decommissioned nodes, maintenance node not\n-      // suitable for read or unknown state replicas.\n-      if (state \u003d\u003d null || state \u003d\u003d StoredReplicaState.DECOMMISSIONED\n+      // Never use maintenance node not suitable for read\n+      // or unknown state replicas.\n+      if (state \u003d\u003d null\n           || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n         continue;\n       }\n \n+      // Save the live decommissioned replica in case we need it. Such replicas\n+      // are normally not used for replication, but if nothing else is\n+      // available, one can be selected as a source.\n+      if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n+        if (decommissionedSrc \u003d\u003d null ||\n+            ThreadLocalRandom.current().nextBoolean()) {\n+          decommissionedSrc \u003d node;\n+        }\n+        continue;\n+      }\n+\n       if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n           \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n         continue; // already reached replication limit\n       }\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           byte blockIndex \u003d ((BlockInfoStriped) block).\n               getStorageBlockIndex(storage);\n           liveBlockIndices.add(blockIndex);\n           if (!bitSet.get(blockIndex)) {\n             bitSet.set(blockIndex);\n           } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n             numReplicas.subtract(StoredReplicaState.LIVE, 1);\n             numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n           }\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n+\n+    // Pick a live decommissioned replica, if nothing else is available.\n+    if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n+        srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n+      srcNodes.add(decommissionedSrc);\n+    }\n+\n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n    DatanodeDescriptor decommissionedSrc \u003d null;\n\n    BitSet bitSet \u003d isStriped ?\n        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // Never use maintenance node not suitable for read\n      // or unknown state replicas.\n      if (state \u003d\u003d null\n          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n        continue;\n      }\n\n      // Save the live decommissioned replica in case we need it. Such replicas\n      // are normally not used for replication, but if nothing else is\n      // available, one can be selected as a source.\n      if (state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        if (decommissionedSrc \u003d\u003d null ||\n            ThreadLocalRandom.current().nextBoolean()) {\n          decommissionedSrc \u003d node;\n        }\n        continue;\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        continue; // already reached replication limit\n      }\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          byte blockIndex \u003d ((BlockInfoStriped) block).\n              getStorageBlockIndex(storage);\n          liveBlockIndices.add(blockIndex);\n          if (!bitSet.get(blockIndex)) {\n            bitSet.set(blockIndex);\n          } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n          }\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n\n    // Pick a live decommissioned replica, if nothing else is available.\n    if (!isStriped \u0026\u0026 nodesContainingLiveReplicas.isEmpty() \u0026\u0026\n        srcNodes.isEmpty() \u0026\u0026 decommissionedSrc !\u003d null) {\n      srcNodes.add(decommissionedSrc);\n    }\n\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "14/10/16 6:13 PM",
      "commitNameOld": "391ce535a739dc92cb90017d759217265a4fd969",
      "commitAuthorOld": "Vinitha Reddy Gankidi",
      "daysBetweenCommits": 2.98,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,68 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n \n     BitSet bitSet \u003d isStriped ?\n         new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n-      // never use already decommissioned nodes or unknown state replicas\n-      if (state \u003d\u003d null || state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n+      // never use already decommissioned nodes, maintenance node not\n+      // suitable for read or unknown state replicas.\n+      if (state \u003d\u003d null || state \u003d\u003d StoredReplicaState.DECOMMISSIONED\n+          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n         continue;\n       }\n \n       if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n-          \u0026\u0026 !node.isDecommissionInProgress() \n+          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n         continue; // already reached replication limit\n       }\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           byte blockIndex \u003d ((BlockInfoStriped) block).\n               getStorageBlockIndex(storage);\n           liveBlockIndices.add(blockIndex);\n           if (!bitSet.get(blockIndex)) {\n             bitSet.set(blockIndex);\n           } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n             numReplicas.subtract(StoredReplicaState.LIVE, 1);\n             numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n           }\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n\n    BitSet bitSet \u003d isStriped ?\n        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // never use already decommissioned nodes, maintenance node not\n      // suitable for read or unknown state replicas.\n      if (state \u003d\u003d null || state \u003d\u003d StoredReplicaState.DECOMMISSIONED\n          || state \u003d\u003d StoredReplicaState.MAINTENANCE_NOT_FOR_READ) {\n        continue;\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 (!node.isDecommissionInProgress() \u0026\u0026 !node.isEnteringMaintenance())\n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        continue; // already reached replication limit\n      }\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          byte blockIndex \u003d ((BlockInfoStriped) block).\n              getStorageBlockIndex(storage);\n          liveBlockIndices.add(blockIndex);\n          if (!bitSet.get(blockIndex)) {\n            bitSet.set(blockIndex);\n          } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n          }\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,66 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n \n     BitSet bitSet \u003d isStriped ?\n         new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n       // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n       // never use already decommissioned nodes or unknown state replicas\n       if (state \u003d\u003d null || state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n         continue;\n       }\n \n-      if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n+      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n           \u0026\u0026 !node.isDecommissionInProgress() \n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n         continue; // already reached replication limit\n       }\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n         continue;\n       }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           byte blockIndex \u003d ((BlockInfoStriped) block).\n               getStorageBlockIndex(storage);\n           liveBlockIndices.add(blockIndex);\n           if (!bitSet.get(blockIndex)) {\n             bitSet.set(blockIndex);\n           } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n             numReplicas.subtract(StoredReplicaState.LIVE, 1);\n             numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n           }\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n\n    BitSet bitSet \u003d isStriped ?\n        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // never use already decommissioned nodes or unknown state replicas\n      if (state \u003d\u003d null || state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        continue;\n      }\n\n      if (priority !\u003d LowRedundancyBlocks.QUEUE_HIGHEST_PRIORITY\n          \u0026\u0026 !node.isDecommissionInProgress() \n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        continue; // already reached replication limit\n      }\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          byte blockIndex \u003d ((BlockInfoStriped) block).\n              getStorageBlockIndex(storage);\n          liveBlockIndices.add(blockIndex);\n          if (!bitSet.get(blockIndex)) {\n            bitSet.set(blockIndex);\n          } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n          }\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "6979cbfc1f4c28440816b56f5624765872b0be49": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9838. Refactor the excessReplicateMap to a class.\n",
      "commitDate": "24/02/16 7:42 PM",
      "commitName": "6979cbfc1f4c28440816b56f5624765872b0be49",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "24/02/16 3:13 PM",
      "commitNameOld": "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.19,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,66 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n \n     BitSet bitSet \u003d isStriped ?\n         new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n           storage, corruptReplicas.getNodes(block), false);\n       if (state \u003d\u003d StoredReplicaState.LIVE) {\n         nodesContainingLiveReplicas.add(storage);\n       }\n       containingNodes.add(node);\n \n-      // do not select corrupted replica as src. also do not select the block\n-      // that is already in excess map\n+      // do not select the replica if it is corrupt or excess\n       if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n           state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n       }\n \n+      // never use already decommissioned nodes or unknown state replicas\n+      if (state \u003d\u003d null || state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n+        continue;\n+      }\n+\n       if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n           \u0026\u0026 !node.isDecommissionInProgress() \n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n         continue; // already reached replication limit\n       }\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n         continue;\n       }\n-      // never use already decommissioned nodes\n-      if (node.isDecommissioned()) {\n-        continue;\n-      }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           byte blockIndex \u003d ((BlockInfoStriped) block).\n               getStorageBlockIndex(storage);\n           liveBlockIndices.add(blockIndex);\n           if (!bitSet.get(blockIndex)) {\n             bitSet.set(blockIndex);\n           } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n             numReplicas.subtract(StoredReplicaState.LIVE, 1);\n             numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n           }\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n\n    BitSet bitSet \u003d isStriped ?\n        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select the replica if it is corrupt or excess\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      // never use already decommissioned nodes or unknown state replicas\n      if (state \u003d\u003d null || state \u003d\u003d StoredReplicaState.DECOMMISSIONED) {\n        continue;\n      }\n\n      if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n          \u0026\u0026 !node.isDecommissionInProgress() \n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        continue; // already reached replication limit\n      }\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          byte blockIndex \u003d ((BlockInfoStriped) block).\n              getStorageBlockIndex(storage);\n          liveBlockIndices.add(blockIndex);\n          if (!bitSet.get(blockIndex)) {\n            bitSet.set(blockIndex);\n          } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n          }\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "47b92f2b6f2dafc129a41b247f35e77c8e47ffba": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9837. BlockManager#countNodes should be able to detect duplicated internal blocks. Contributed by Jing Zhao.\n",
      "commitDate": "24/02/16 3:13 PM",
      "commitName": "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "20/02/16 11:19 PM",
      "commitNameOld": "d5abd293a890a8a1da48a166a291ae1c5644ad57",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 3.66,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,66 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n-    int live \u003d 0;\n-    int readonly \u003d 0;\n-    int decommissioned \u003d 0;\n-    int decommissioning \u003d 0;\n-    int corrupt \u003d 0;\n-    int excess \u003d 0;\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n \n-    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(block);\n+    BitSet bitSet \u003d isStriped ?\n+        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n-      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d\n-        excessReplicateMap.get(node.getDatanodeUuid());\n-      int countableReplica \u003d storage.getState() \u003d\u003d State.NORMAL ? 1 : 0;\n-      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node)))\n-        corrupt +\u003d countableReplica;\n-      else if (node.isDecommissionInProgress()) {\n-        decommissioning +\u003d countableReplica;\n-      } else if (node.isDecommissioned()) {\n-        decommissioned +\u003d countableReplica;\n-      } else if (excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block)) {\n-        excess +\u003d countableReplica;\n-      } else {\n+      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n+          storage, corruptReplicas.getNodes(block), false);\n+      if (state \u003d\u003d StoredReplicaState.LIVE) {\n         nodesContainingLiveReplicas.add(storage);\n-        live +\u003d countableReplica;\n-      }\n-      if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n-        readonly++;\n       }\n       containingNodes.add(node);\n-      // Check if this replica is corrupt\n-      // If so, do not select the node as src node\n-      if ((nodesCorrupt !\u003d null) \u0026\u0026 nodesCorrupt.contains(node))\n+\n+      // do not select corrupted replica as src. also do not select the block\n+      // that is already in excess map\n+      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n+          state \u003d\u003d StoredReplicaState.EXCESS) {\n         continue;\n+      }\n+\n       if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n           \u0026\u0026 !node.isDecommissionInProgress() \n-          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams)\n-      {\n+          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n         continue; // already reached replication limit\n       }\n-      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit)\n-      {\n+      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n         continue;\n       }\n-      // the block must not be scheduled for removal on srcNode\n-      if(excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block))\n-        continue;\n       // never use already decommissioned nodes\n-      if(node.isDecommissioned())\n+      if (node.isDecommissioned()) {\n         continue;\n+      }\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n-          liveBlockIndices.add(((BlockInfoStriped) block).\n-              getStorageBlockIndex(storage));\n+          byte blockIndex \u003d ((BlockInfoStriped) block).\n+              getStorageBlockIndex(storage);\n+          liveBlockIndices.add(blockIndex);\n+          if (!bitSet.get(blockIndex)) {\n+            bitSet.set(blockIndex);\n+          } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n+            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n+            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n+          }\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n-      if (!isStriped \u0026\u0026 ThreadLocalRandom.current().nextBoolean()) {\n+      if (ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n-    if(numReplicas !\u003d null)\n-      numReplicas.set(live, readonly, decommissioned, decommissioning, corrupt,\n-          excess, 0);\n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n\n    BitSet bitSet \u003d isStriped ?\n        new BitSet(((BlockInfoStriped) block).getTotalBlockNum()) : null;\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      final StoredReplicaState state \u003d checkReplicaOnStorage(numReplicas, block,\n          storage, corruptReplicas.getNodes(block), false);\n      if (state \u003d\u003d StoredReplicaState.LIVE) {\n        nodesContainingLiveReplicas.add(storage);\n      }\n      containingNodes.add(node);\n\n      // do not select corrupted replica as src. also do not select the block\n      // that is already in excess map\n      if (state \u003d\u003d StoredReplicaState.CORRUPT ||\n          state \u003d\u003d StoredReplicaState.EXCESS) {\n        continue;\n      }\n\n      if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n          \u0026\u0026 !node.isDecommissionInProgress() \n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams) {\n        continue; // already reached replication limit\n      }\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit) {\n        continue;\n      }\n      // never use already decommissioned nodes\n      if (node.isDecommissioned()) {\n        continue;\n      }\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          byte blockIndex \u003d ((BlockInfoStriped) block).\n              getStorageBlockIndex(storage);\n          liveBlockIndices.add(blockIndex);\n          if (!bitSet.get(blockIndex)) {\n            bitSet.set(blockIndex);\n          } else if (state \u003d\u003d StoredReplicaState.LIVE) {\n            numReplicas.subtract(StoredReplicaState.LIVE, 1);\n            numReplicas.add(StoredReplicaState.REDUNDANT, 1);\n          }\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "70d6f201260086a3f12beaa317fede2a99639fef": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9575. Use byte array for internal block indices in a striped block.  Contributed by jing9\n",
      "commitDate": "21/12/15 10:47 PM",
      "commitName": "70d6f201260086a3f12beaa317fede2a99639fef",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9575. Use byte array for internal block indices in a striped block.  Contributed by jing9\n",
          "commitDate": "21/12/15 10:47 PM",
          "commitName": "70d6f201260086a3f12beaa317fede2a99639fef",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "16/12/15 6:16 PM",
          "commitNameOld": "f741476146574550a1a208d58ef8be76639e5ddc",
          "commitAuthorOld": "Uma Mahesh",
          "daysBetweenCommits": 5.19,
          "commitsBetweenForRepo": 38,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,80 +1,80 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n-      List\u003cShort\u003e liveBlockIndices, int priority) {\n+      List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     int live \u003d 0;\n     int readonly \u003d 0;\n     int decommissioned \u003d 0;\n     int decommissioning \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n \n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(block);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d\n         excessReplicateMap.get(node.getDatanodeUuid());\n       int countableReplica \u003d storage.getState() \u003d\u003d State.NORMAL ? 1 : 0;\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node)))\n         corrupt +\u003d countableReplica;\n       else if (node.isDecommissionInProgress()) {\n         decommissioning +\u003d countableReplica;\n       } else if (node.isDecommissioned()) {\n         decommissioned +\u003d countableReplica;\n       } else if (excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block)) {\n         excess +\u003d countableReplica;\n       } else {\n         nodesContainingLiveReplicas.add(storage);\n         live +\u003d countableReplica;\n       }\n       if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n         readonly++;\n       }\n       containingNodes.add(node);\n       // Check if this replica is corrupt\n       // If so, do not select the node as src node\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 nodesCorrupt.contains(node))\n         continue;\n       if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n           \u0026\u0026 !node.isDecommissionInProgress() \n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams)\n       {\n         continue; // already reached replication limit\n       }\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit)\n       {\n         continue;\n       }\n       // the block must not be scheduled for removal on srcNode\n       if(excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block))\n         continue;\n       // never use already decommissioned nodes\n       if(node.isDecommissioned())\n         continue;\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n-          liveBlockIndices.add((short) ((BlockInfoStriped) block).\n+          liveBlockIndices.add(((BlockInfoStriped) block).\n               getStorageBlockIndex(storage));\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (!isStriped \u0026\u0026 ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n     if(numReplicas !\u003d null)\n       numReplicas.set(live, readonly, decommissioned, decommissioning, corrupt,\n           excess, 0);\n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    int live \u003d 0;\n    int readonly \u003d 0;\n    int decommissioned \u003d 0;\n    int decommissioning \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(block);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d\n        excessReplicateMap.get(node.getDatanodeUuid());\n      int countableReplica \u003d storage.getState() \u003d\u003d State.NORMAL ? 1 : 0;\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node)))\n        corrupt +\u003d countableReplica;\n      else if (node.isDecommissionInProgress()) {\n        decommissioning +\u003d countableReplica;\n      } else if (node.isDecommissioned()) {\n        decommissioned +\u003d countableReplica;\n      } else if (excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block)) {\n        excess +\u003d countableReplica;\n      } else {\n        nodesContainingLiveReplicas.add(storage);\n        live +\u003d countableReplica;\n      }\n      if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n        readonly++;\n      }\n      containingNodes.add(node);\n      // Check if this replica is corrupt\n      // If so, do not select the node as src node\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 nodesCorrupt.contains(node))\n        continue;\n      if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n          \u0026\u0026 !node.isDecommissionInProgress() \n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams)\n      {\n        continue; // already reached replication limit\n      }\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit)\n      {\n        continue;\n      }\n      // the block must not be scheduled for removal on srcNode\n      if(excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block))\n        continue;\n      // never use already decommissioned nodes\n      if(node.isDecommissioned())\n        continue;\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add(((BlockInfoStriped) block).\n              getStorageBlockIndex(storage));\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (!isStriped \u0026\u0026 ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n    if(numReplicas !\u003d null)\n      numReplicas.set(live, readonly, decommissioned, decommissioning, corrupt,\n          excess, 0);\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[block-BlockInfo, containingNodes-List\u003cDatanodeDescriptor\u003e, nodesContainingLiveReplicas-List\u003cDatanodeStorageInfo\u003e, numReplicas-NumberReplicas, liveBlockIndices-List\u003cShort\u003e, priority-int]",
            "newValue": "[block-BlockInfo, containingNodes-List\u003cDatanodeDescriptor\u003e, nodesContainingLiveReplicas-List\u003cDatanodeStorageInfo\u003e, numReplicas-NumberReplicas, liveBlockIndices-List\u003cByte\u003e, priority-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9575. Use byte array for internal block indices in a striped block.  Contributed by jing9\n",
          "commitDate": "21/12/15 10:47 PM",
          "commitName": "70d6f201260086a3f12beaa317fede2a99639fef",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "16/12/15 6:16 PM",
          "commitNameOld": "f741476146574550a1a208d58ef8be76639e5ddc",
          "commitAuthorOld": "Uma Mahesh",
          "daysBetweenCommits": 5.19,
          "commitsBetweenForRepo": 38,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,80 +1,80 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n-      List\u003cShort\u003e liveBlockIndices, int priority) {\n+      List\u003cByte\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     int live \u003d 0;\n     int readonly \u003d 0;\n     int decommissioned \u003d 0;\n     int decommissioning \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n \n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(block);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d\n         excessReplicateMap.get(node.getDatanodeUuid());\n       int countableReplica \u003d storage.getState() \u003d\u003d State.NORMAL ? 1 : 0;\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node)))\n         corrupt +\u003d countableReplica;\n       else if (node.isDecommissionInProgress()) {\n         decommissioning +\u003d countableReplica;\n       } else if (node.isDecommissioned()) {\n         decommissioned +\u003d countableReplica;\n       } else if (excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block)) {\n         excess +\u003d countableReplica;\n       } else {\n         nodesContainingLiveReplicas.add(storage);\n         live +\u003d countableReplica;\n       }\n       if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n         readonly++;\n       }\n       containingNodes.add(node);\n       // Check if this replica is corrupt\n       // If so, do not select the node as src node\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 nodesCorrupt.contains(node))\n         continue;\n       if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n           \u0026\u0026 !node.isDecommissionInProgress() \n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams)\n       {\n         continue; // already reached replication limit\n       }\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit)\n       {\n         continue;\n       }\n       // the block must not be scheduled for removal on srcNode\n       if(excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block))\n         continue;\n       // never use already decommissioned nodes\n       if(node.isDecommissioned())\n         continue;\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n-          liveBlockIndices.add((short) ((BlockInfoStriped) block).\n+          liveBlockIndices.add(((BlockInfoStriped) block).\n               getStorageBlockIndex(storage));\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (!isStriped \u0026\u0026 ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n     if(numReplicas !\u003d null)\n       numReplicas.set(live, readonly, decommissioned, decommissioning, corrupt,\n           excess, 0);\n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cByte\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    int live \u003d 0;\n    int readonly \u003d 0;\n    int decommissioned \u003d 0;\n    int decommissioning \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(block);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d\n        excessReplicateMap.get(node.getDatanodeUuid());\n      int countableReplica \u003d storage.getState() \u003d\u003d State.NORMAL ? 1 : 0;\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node)))\n        corrupt +\u003d countableReplica;\n      else if (node.isDecommissionInProgress()) {\n        decommissioning +\u003d countableReplica;\n      } else if (node.isDecommissioned()) {\n        decommissioned +\u003d countableReplica;\n      } else if (excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block)) {\n        excess +\u003d countableReplica;\n      } else {\n        nodesContainingLiveReplicas.add(storage);\n        live +\u003d countableReplica;\n      }\n      if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n        readonly++;\n      }\n      containingNodes.add(node);\n      // Check if this replica is corrupt\n      // If so, do not select the node as src node\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 nodesCorrupt.contains(node))\n        continue;\n      if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n          \u0026\u0026 !node.isDecommissionInProgress() \n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams)\n      {\n        continue; // already reached replication limit\n      }\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit)\n      {\n        continue;\n      }\n      // the block must not be scheduled for removal on srcNode\n      if(excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block))\n        continue;\n      // never use already decommissioned nodes\n      if(node.isDecommissioned())\n        continue;\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add(((BlockInfoStriped) block).\n              getStorageBlockIndex(storage));\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (!isStriped \u0026\u0026 ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n    if(numReplicas !\u003d null)\n      numReplicas.set(live, readonly, decommissioned, decommissioning, corrupt,\n          excess, 0);\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "5411dc559d5f73e4153e76fdff94a26869c17a37": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9205. Do not schedule corrupt blocks for replication.  (szetszwo)\n",
      "commitDate": "15/10/15 3:07 AM",
      "commitName": "5411dc559d5f73e4153e76fdff94a26869c17a37",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "14/10/15 4:17 PM",
      "commitNameOld": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.45,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,80 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cShort\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     int live \u003d 0;\n+    int readonly \u003d 0;\n     int decommissioned \u003d 0;\n     int decommissioning \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n \n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(block);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d\n         excessReplicateMap.get(node.getDatanodeUuid());\n       int countableReplica \u003d storage.getState() \u003d\u003d State.NORMAL ? 1 : 0;\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node)))\n         corrupt +\u003d countableReplica;\n       else if (node.isDecommissionInProgress()) {\n         decommissioning +\u003d countableReplica;\n       } else if (node.isDecommissioned()) {\n         decommissioned +\u003d countableReplica;\n       } else if (excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block)) {\n         excess +\u003d countableReplica;\n       } else {\n         nodesContainingLiveReplicas.add(storage);\n         live +\u003d countableReplica;\n       }\n+      if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n+        readonly++;\n+      }\n       containingNodes.add(node);\n       // Check if this replica is corrupt\n       // If so, do not select the node as src node\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 nodesCorrupt.contains(node))\n         continue;\n       if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n           \u0026\u0026 !node.isDecommissionInProgress() \n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams)\n       {\n         continue; // already reached replication limit\n       }\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit)\n       {\n         continue;\n       }\n       // the block must not be scheduled for removal on srcNode\n       if(excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block))\n         continue;\n       // never use already decommissioned nodes\n       if(node.isDecommissioned())\n         continue;\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           liveBlockIndices.add((short) ((BlockInfoStriped) block).\n               getStorageBlockIndex(storage));\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (!isStriped \u0026\u0026 ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n     if(numReplicas !\u003d null)\n-      numReplicas.initialize(live, decommissioned, decommissioning, corrupt,\n+      numReplicas.set(live, readonly, decommissioned, decommissioning, corrupt,\n           excess, 0);\n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cShort\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    int live \u003d 0;\n    int readonly \u003d 0;\n    int decommissioned \u003d 0;\n    int decommissioning \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(block);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d\n        excessReplicateMap.get(node.getDatanodeUuid());\n      int countableReplica \u003d storage.getState() \u003d\u003d State.NORMAL ? 1 : 0;\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node)))\n        corrupt +\u003d countableReplica;\n      else if (node.isDecommissionInProgress()) {\n        decommissioning +\u003d countableReplica;\n      } else if (node.isDecommissioned()) {\n        decommissioned +\u003d countableReplica;\n      } else if (excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block)) {\n        excess +\u003d countableReplica;\n      } else {\n        nodesContainingLiveReplicas.add(storage);\n        live +\u003d countableReplica;\n      }\n      if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n        readonly++;\n      }\n      containingNodes.add(node);\n      // Check if this replica is corrupt\n      // If so, do not select the node as src node\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 nodesCorrupt.contains(node))\n        continue;\n      if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n          \u0026\u0026 !node.isDecommissionInProgress() \n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams)\n      {\n        continue; // already reached replication limit\n      }\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit)\n      {\n        continue;\n      }\n      // the block must not be scheduled for removal on srcNode\n      if(excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block))\n        continue;\n      // never use already decommissioned nodes\n      if(node.isDecommissioned())\n        continue;\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add((short) ((BlockInfoStriped) block).\n              getStorageBlockIndex(storage));\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (!isStriped \u0026\u0026 ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n    if(numReplicas !\u003d null)\n      numReplicas.set(live, readonly, decommissioned, decommissioning, corrupt,\n          excess, 0);\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "73b86a5046fe3262dde7b05be46b18575e35fd5f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8988. Use LightWeightHashSet instead of LightWeightLinkedSet in BlockManager#excessReplicateMap. (yliu)\n",
      "commitDate": "11/10/15 11:40 PM",
      "commitName": "73b86a5046fe3262dde7b05be46b18575e35fd5f",
      "commitAuthor": "yliu",
      "commitDateOld": "23/09/15 1:34 PM",
      "commitNameOld": "c09dc258a8f64fab852bf6f26187163480dbee3c",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 18.42,
      "commitsBetweenForRepo": 126,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,76 @@\n   DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n       List\u003cDatanodeDescriptor\u003e containingNodes,\n       List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n       NumberReplicas numReplicas,\n       List\u003cShort\u003e liveBlockIndices, int priority) {\n     containingNodes.clear();\n     nodesContainingLiveReplicas.clear();\n     List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n     int live \u003d 0;\n     int decommissioned \u003d 0;\n     int decommissioning \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     liveBlockIndices.clear();\n     final boolean isStriped \u003d block.isStriped();\n \n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(block);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n-      LightWeightLinkedSet\u003cBlockInfo\u003e excessBlocks \u003d\n+      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d\n         excessReplicateMap.get(node.getDatanodeUuid());\n       int countableReplica \u003d storage.getState() \u003d\u003d State.NORMAL ? 1 : 0;\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node)))\n         corrupt +\u003d countableReplica;\n       else if (node.isDecommissionInProgress()) {\n         decommissioning +\u003d countableReplica;\n       } else if (node.isDecommissioned()) {\n         decommissioned +\u003d countableReplica;\n       } else if (excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block)) {\n         excess +\u003d countableReplica;\n       } else {\n         nodesContainingLiveReplicas.add(storage);\n         live +\u003d countableReplica;\n       }\n       containingNodes.add(node);\n       // Check if this replica is corrupt\n       // If so, do not select the node as src node\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 nodesCorrupt.contains(node))\n         continue;\n       if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n           \u0026\u0026 !node.isDecommissionInProgress() \n           \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams)\n       {\n         continue; // already reached replication limit\n       }\n       if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit)\n       {\n         continue;\n       }\n       // the block must not be scheduled for removal on srcNode\n       if(excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block))\n         continue;\n       // never use already decommissioned nodes\n       if(node.isDecommissioned())\n         continue;\n \n       if(isStriped || srcNodes.isEmpty()) {\n         srcNodes.add(node);\n         if (isStriped) {\n           liveBlockIndices.add((short) ((BlockInfoStriped) block).\n               getStorageBlockIndex(storage));\n         }\n         continue;\n       }\n       // for replicated block, switch to a different node randomly\n       // this to prevent from deterministically selecting the same node even\n       // if the node failed to replicate the block on previous iterations\n       if (!isStriped \u0026\u0026 ThreadLocalRandom.current().nextBoolean()) {\n         srcNodes.set(0, node);\n       }\n     }\n     if(numReplicas !\u003d null)\n       numReplicas.initialize(live, decommissioned, decommissioning, corrupt,\n           excess, 0);\n     return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeDescriptor[] chooseSourceDatanodes(BlockInfo block,\n      List\u003cDatanodeDescriptor\u003e containingNodes,\n      List\u003cDatanodeStorageInfo\u003e nodesContainingLiveReplicas,\n      NumberReplicas numReplicas,\n      List\u003cShort\u003e liveBlockIndices, int priority) {\n    containingNodes.clear();\n    nodesContainingLiveReplicas.clear();\n    List\u003cDatanodeDescriptor\u003e srcNodes \u003d new ArrayList\u003c\u003e();\n    int live \u003d 0;\n    int decommissioned \u003d 0;\n    int decommissioning \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    liveBlockIndices.clear();\n    final boolean isStriped \u003d block.isStriped();\n\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(block);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d\n        excessReplicateMap.get(node.getDatanodeUuid());\n      int countableReplica \u003d storage.getState() \u003d\u003d State.NORMAL ? 1 : 0;\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node)))\n        corrupt +\u003d countableReplica;\n      else if (node.isDecommissionInProgress()) {\n        decommissioning +\u003d countableReplica;\n      } else if (node.isDecommissioned()) {\n        decommissioned +\u003d countableReplica;\n      } else if (excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block)) {\n        excess +\u003d countableReplica;\n      } else {\n        nodesContainingLiveReplicas.add(storage);\n        live +\u003d countableReplica;\n      }\n      containingNodes.add(node);\n      // Check if this replica is corrupt\n      // If so, do not select the node as src node\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 nodesCorrupt.contains(node))\n        continue;\n      if(priority !\u003d UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY \n          \u0026\u0026 !node.isDecommissionInProgress() \n          \u0026\u0026 node.getNumberOfBlocksToBeReplicated() \u003e\u003d maxReplicationStreams)\n      {\n        continue; // already reached replication limit\n      }\n      if (node.getNumberOfBlocksToBeReplicated() \u003e\u003d replicationStreamsHardLimit)\n      {\n        continue;\n      }\n      // the block must not be scheduled for removal on srcNode\n      if(excessBlocks !\u003d null \u0026\u0026 excessBlocks.contains(block))\n        continue;\n      // never use already decommissioned nodes\n      if(node.isDecommissioned())\n        continue;\n\n      if(isStriped || srcNodes.isEmpty()) {\n        srcNodes.add(node);\n        if (isStriped) {\n          liveBlockIndices.add((short) ((BlockInfoStriped) block).\n              getStorageBlockIndex(storage));\n        }\n        continue;\n      }\n      // for replicated block, switch to a different node randomly\n      // this to prevent from deterministically selecting the same node even\n      // if the node failed to replicate the block on previous iterations\n      if (!isStriped \u0026\u0026 ThreadLocalRandom.current().nextBoolean()) {\n        srcNodes.set(0, node);\n      }\n    }\n    if(numReplicas !\u003d null)\n      numReplicas.initialize(live, decommissioned, decommissioning, corrupt,\n          excess, 0);\n    return srcNodes.toArray(new DatanodeDescriptor[srcNodes.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    }
  }
}