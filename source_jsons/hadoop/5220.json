{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirWriteFileOp.java",
  "functionName": "validateAddBlock",
  "functionId": "validateAddBlock___fsn-FSNamesystem__pc-FSPermissionChecker__src-String__fileId-long__clientName-String__previous-ExtendedBlock__onRetryBlock-LocatedBlock[]",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
  "functionStartLine": 160,
  "functionEndLine": 205,
  "numCommitsSeen": 65,
  "timeTaken": 6192,
  "changeHistory": [
    "fbe06b58805aac4861fb27dfa273914b69e8bdc6",
    "de9994bd893af70fffdd68af6252fc45020e0e69",
    "3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd",
    "3085a604300ed76d06a0011bd5555e419897b6cd",
    "a2a5d7b5bca715835d92816e7b267b59f7270708",
    "869393643de23dcb010cc33091c8eb398de0fd6c",
    "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2",
    "45c763ad6171bc7808c2ddcb9099a4215113da2a",
    "c09dc258a8f64fab852bf6f26187163480dbee3c",
    "e53fa769c97416af69ea567aecd44f67e896688b",
    "d8ea443af0b1c8289a1dd738945831ff8be0e9c1",
    "e5afac5896a1a88e152746598527d91f73cbb724"
  ],
  "changeHistoryShort": {
    "fbe06b58805aac4861fb27dfa273914b69e8bdc6": "Ybodychange",
    "de9994bd893af70fffdd68af6252fc45020e0e69": "Ybodychange",
    "3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd": "Ybodychange",
    "3085a604300ed76d06a0011bd5555e419897b6cd": "Ybodychange",
    "a2a5d7b5bca715835d92816e7b267b59f7270708": "Ybodychange",
    "869393643de23dcb010cc33091c8eb398de0fd6c": "Ybodychange",
    "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2": "Ybodychange",
    "45c763ad6171bc7808c2ddcb9099a4215113da2a": "Ybodychange",
    "c09dc258a8f64fab852bf6f26187163480dbee3c": "Ybodychange",
    "e53fa769c97416af69ea567aecd44f67e896688b": "Ybodychange",
    "d8ea443af0b1c8289a1dd738945831ff8be0e9c1": "Ybodychange",
    "e5afac5896a1a88e152746598527d91f73cbb724": "Yintroduced"
  },
  "changeHistoryDetails": {
    "fbe06b58805aac4861fb27dfa273914b69e8bdc6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12349. Improve log message when it could not alloc enough blocks for EC. (Lei (Eddy) Xu)\n",
      "commitDate": "15/09/17 12:12 PM",
      "commitName": "fbe06b58805aac4861fb27dfa273914b69e8bdc6",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "11/09/17 10:06 AM",
      "commitNameOld": "de9994bd893af70fffdd68af6252fc45020e0e69",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 4.09,
      "commitsBetweenForRepo": 94,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n   static ValidateAddBlockResult validateAddBlock(\n       FSNamesystem fsn, FSPermissionChecker pc,\n       String src, long fileId, String clientName,\n       ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n     final long blockSize;\n     final short numTargets;\n     final byte storagePolicyID;\n     String clientMachine;\n     final BlockType blockType;\n \n     INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n     FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                            previous, onRetryBlock);\n     if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n       // This is a retry. No need to generate new locations.\n       // Use the last block if it has locations.\n       return null;\n     }\n \n     final INodeFile pendingFile \u003d fileState.inode;\n     if (!fsn.checkFileProgress(src, pendingFile, false)) {\n       throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n     }\n     if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n       throw new IOException(\"File has reached the limit on maximum number of\"\n           + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n           + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n           + fsn.maxBlocksPerFile);\n     }\n     blockSize \u003d pendingFile.getPreferredBlockSize();\n     clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n         .getClientMachine();\n     blockType \u003d pendingFile.getBlockType();\n     ErasureCodingPolicy ecPolicy \u003d null;\n     if (blockType \u003d\u003d BlockType.STRIPED) {\n       ecPolicy \u003d\n           FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(fsn, iip);\n       numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n           + ecPolicy.getSchema().getNumParityUnits());\n     } else {\n       numTargets \u003d pendingFile.getFileReplication();\n     }\n     storagePolicyID \u003d pendingFile.getStoragePolicyID();\n     return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n-                                      clientMachine, blockType);\n+                                      clientMachine, blockType, ecPolicy);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final BlockType blockType;\n\n    INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n    FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                           previous, onRetryBlock);\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n\n    final INodeFile pendingFile \u003d fileState.inode;\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    blockType \u003d pendingFile.getBlockType();\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (blockType \u003d\u003d BlockType.STRIPED) {\n      ecPolicy \u003d\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(fsn, iip);\n      numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n          + ecPolicy.getSchema().getNumParityUnits());\n    } else {\n      numTargets \u003d pendingFile.getFileReplication();\n    }\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, blockType, ecPolicy);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "de9994bd893af70fffdd68af6252fc45020e0e69": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-12349. Improve log message when it could not alloc enough blocks for EC. (lei)\"\n\nThis reverts commit 3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd.\n",
      "commitDate": "11/09/17 10:06 AM",
      "commitName": "de9994bd893af70fffdd68af6252fc45020e0e69",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "07/09/17 6:01 PM",
      "commitNameOld": "3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 3.67,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n   static ValidateAddBlockResult validateAddBlock(\n       FSNamesystem fsn, FSPermissionChecker pc,\n       String src, long fileId, String clientName,\n       ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n     final long blockSize;\n     final short numTargets;\n     final byte storagePolicyID;\n     String clientMachine;\n     final BlockType blockType;\n \n     INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n     FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                            previous, onRetryBlock);\n     if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n       // This is a retry. No need to generate new locations.\n       // Use the last block if it has locations.\n       return null;\n     }\n \n     final INodeFile pendingFile \u003d fileState.inode;\n     if (!fsn.checkFileProgress(src, pendingFile, false)) {\n       throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n     }\n     if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n       throw new IOException(\"File has reached the limit on maximum number of\"\n           + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n           + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n           + fsn.maxBlocksPerFile);\n     }\n     blockSize \u003d pendingFile.getPreferredBlockSize();\n     clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n         .getClientMachine();\n     blockType \u003d pendingFile.getBlockType();\n     ErasureCodingPolicy ecPolicy \u003d null;\n     if (blockType \u003d\u003d BlockType.STRIPED) {\n       ecPolicy \u003d\n           FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(fsn, iip);\n       numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n           + ecPolicy.getSchema().getNumParityUnits());\n     } else {\n       numTargets \u003d pendingFile.getFileReplication();\n     }\n     storagePolicyID \u003d pendingFile.getStoragePolicyID();\n     return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n-                                      clientMachine, blockType, ecPolicy);\n+                                      clientMachine, blockType);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final BlockType blockType;\n\n    INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n    FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                           previous, onRetryBlock);\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n\n    final INodeFile pendingFile \u003d fileState.inode;\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    blockType \u003d pendingFile.getBlockType();\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (blockType \u003d\u003d BlockType.STRIPED) {\n      ecPolicy \u003d\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(fsn, iip);\n      numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n          + ecPolicy.getSchema().getNumParityUnits());\n    } else {\n      numTargets \u003d pendingFile.getFileReplication();\n    }\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, blockType);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12349. Improve log message when it could not alloc enough blocks for EC. (lei)\n",
      "commitDate": "07/09/17 6:01 PM",
      "commitName": "3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "23/08/17 5:06 PM",
      "commitNameOld": "1000a2af04b24c123a3b08168f36b4e90420cab7",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 15.04,
      "commitsBetweenForRepo": 160,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n   static ValidateAddBlockResult validateAddBlock(\n       FSNamesystem fsn, FSPermissionChecker pc,\n       String src, long fileId, String clientName,\n       ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n     final long blockSize;\n     final short numTargets;\n     final byte storagePolicyID;\n     String clientMachine;\n     final BlockType blockType;\n \n     INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n     FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                            previous, onRetryBlock);\n     if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n       // This is a retry. No need to generate new locations.\n       // Use the last block if it has locations.\n       return null;\n     }\n \n     final INodeFile pendingFile \u003d fileState.inode;\n     if (!fsn.checkFileProgress(src, pendingFile, false)) {\n       throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n     }\n     if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n       throw new IOException(\"File has reached the limit on maximum number of\"\n           + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n           + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n           + fsn.maxBlocksPerFile);\n     }\n     blockSize \u003d pendingFile.getPreferredBlockSize();\n     clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n         .getClientMachine();\n     blockType \u003d pendingFile.getBlockType();\n     ErasureCodingPolicy ecPolicy \u003d null;\n     if (blockType \u003d\u003d BlockType.STRIPED) {\n       ecPolicy \u003d\n           FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(fsn, iip);\n       numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n           + ecPolicy.getSchema().getNumParityUnits());\n     } else {\n       numTargets \u003d pendingFile.getFileReplication();\n     }\n     storagePolicyID \u003d pendingFile.getStoragePolicyID();\n     return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n-                                      clientMachine, blockType);\n+                                      clientMachine, blockType, ecPolicy);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final BlockType blockType;\n\n    INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n    FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                           previous, onRetryBlock);\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n\n    final INodeFile pendingFile \u003d fileState.inode;\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    blockType \u003d pendingFile.getBlockType();\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (blockType \u003d\u003d BlockType.STRIPED) {\n      ecPolicy \u003d\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(fsn, iip);\n      numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n          + ecPolicy.getSchema().getNumParityUnits());\n    } else {\n      numTargets \u003d pendingFile.getFileReplication();\n    }\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, blockType, ecPolicy);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "3085a604300ed76d06a0011bd5555e419897b6cd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8112. Relax permission checking for EC related operations.\n",
      "commitDate": "03/03/17 1:00 PM",
      "commitName": "3085a604300ed76d06a0011bd5555e419897b6cd",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/02/17 5:07 PM",
      "commitNameOld": "55c07bbed2f475f7b584a86112ee1b6fe0221e98",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 3.83,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,46 @@\n   static ValidateAddBlockResult validateAddBlock(\n       FSNamesystem fsn, FSPermissionChecker pc,\n       String src, long fileId, String clientName,\n       ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n     final long blockSize;\n     final short numTargets;\n     final byte storagePolicyID;\n     String clientMachine;\n     final BlockType blockType;\n \n     INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n     FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                            previous, onRetryBlock);\n     if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n       // This is a retry. No need to generate new locations.\n       // Use the last block if it has locations.\n       return null;\n     }\n \n     final INodeFile pendingFile \u003d fileState.inode;\n     if (!fsn.checkFileProgress(src, pendingFile, false)) {\n       throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n     }\n     if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n       throw new IOException(\"File has reached the limit on maximum number of\"\n           + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n           + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n           + fsn.maxBlocksPerFile);\n     }\n     blockSize \u003d pendingFile.getPreferredBlockSize();\n     clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n         .getClientMachine();\n     blockType \u003d pendingFile.getBlockType();\n     ErasureCodingPolicy ecPolicy \u003d null;\n     if (blockType \u003d\u003d BlockType.STRIPED) {\n-      ecPolicy \u003d FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n+      ecPolicy \u003d\n+          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(fsn, iip);\n       numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n           + ecPolicy.getSchema().getNumParityUnits());\n     } else {\n       numTargets \u003d pendingFile.getFileReplication();\n     }\n     storagePolicyID \u003d pendingFile.getStoragePolicyID();\n     return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                       clientMachine, blockType);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final BlockType blockType;\n\n    INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n    FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                           previous, onRetryBlock);\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n\n    final INodeFile pendingFile \u003d fileState.inode;\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    blockType \u003d pendingFile.getBlockType();\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (blockType \u003d\u003d BlockType.STRIPED) {\n      ecPolicy \u003d\n          FSDirErasureCodingOp.unprotectedGetErasureCodingPolicy(fsn, iip);\n      numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n          + ecPolicy.getSchema().getNumParityUnits());\n    } else {\n      numTargets \u003d pendingFile.getFileReplication();\n    }\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, blockType);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "a2a5d7b5bca715835d92816e7b267b59f7270708": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10759. Change fsimage bool isStriped from boolean to an enum. Contributed by Ewan Higgs.\n",
      "commitDate": "18/01/17 1:31 PM",
      "commitName": "a2a5d7b5bca715835d92816e7b267b59f7270708",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "24/10/16 3:14 PM",
      "commitNameOld": "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 85.97,
      "commitsBetweenForRepo": 568,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   static ValidateAddBlockResult validateAddBlock(\n       FSNamesystem fsn, FSPermissionChecker pc,\n       String src, long fileId, String clientName,\n       ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n     final long blockSize;\n     final short numTargets;\n     final byte storagePolicyID;\n     String clientMachine;\n-    final boolean isStriped;\n+    final BlockType blockType;\n \n     INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n     FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                            previous, onRetryBlock);\n     if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n       // This is a retry. No need to generate new locations.\n       // Use the last block if it has locations.\n       return null;\n     }\n \n     final INodeFile pendingFile \u003d fileState.inode;\n     if (!fsn.checkFileProgress(src, pendingFile, false)) {\n       throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n     }\n     if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n       throw new IOException(\"File has reached the limit on maximum number of\"\n           + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n           + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n           + fsn.maxBlocksPerFile);\n     }\n     blockSize \u003d pendingFile.getPreferredBlockSize();\n     clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n         .getClientMachine();\n-    isStriped \u003d pendingFile.isStriped();\n+    blockType \u003d pendingFile.getBlockType();\n     ErasureCodingPolicy ecPolicy \u003d null;\n-    if (isStriped) {\n+    if (blockType \u003d\u003d BlockType.STRIPED) {\n       ecPolicy \u003d FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n       numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n           + ecPolicy.getSchema().getNumParityUnits());\n     } else {\n       numTargets \u003d pendingFile.getFileReplication();\n     }\n     storagePolicyID \u003d pendingFile.getStoragePolicyID();\n     return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n-                                      clientMachine, isStriped);\n+                                      clientMachine, blockType);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final BlockType blockType;\n\n    INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n    FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                           previous, onRetryBlock);\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n\n    final INodeFile pendingFile \u003d fileState.inode;\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    blockType \u003d pendingFile.getBlockType();\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (blockType \u003d\u003d BlockType.STRIPED) {\n      ecPolicy \u003d FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n      numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n          + ecPolicy.getSchema().getNumParityUnits());\n    } else {\n      numTargets \u003d pendingFile.getFileReplication();\n    }\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, blockType);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "869393643de23dcb010cc33091c8eb398de0fd6c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10745. Directly resolve paths into INodesInPath. Contributed by Daryn Sharp.\n",
      "commitDate": "17/08/16 1:53 PM",
      "commitName": "869393643de23dcb010cc33091c8eb398de0fd6c",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "15/08/16 2:45 PM",
      "commitNameOld": "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 1.96,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   static ValidateAddBlockResult validateAddBlock(\n       FSNamesystem fsn, FSPermissionChecker pc,\n       String src, long fileId, String clientName,\n       ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n     final long blockSize;\n     final short numTargets;\n     final byte storagePolicyID;\n     String clientMachine;\n     final boolean isStriped;\n \n-    src \u003d fsn.dir.resolvePath(pc, src);\n-    FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n+    INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n+    FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                            previous, onRetryBlock);\n     if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n       // This is a retry. No need to generate new locations.\n       // Use the last block if it has locations.\n       return null;\n     }\n \n     final INodeFile pendingFile \u003d fileState.inode;\n     if (!fsn.checkFileProgress(src, pendingFile, false)) {\n       throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n     }\n     if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n       throw new IOException(\"File has reached the limit on maximum number of\"\n           + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n           + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n           + fsn.maxBlocksPerFile);\n     }\n     blockSize \u003d pendingFile.getPreferredBlockSize();\n     clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n         .getClientMachine();\n     isStriped \u003d pendingFile.isStriped();\n     ErasureCodingPolicy ecPolicy \u003d null;\n     if (isStriped) {\n       ecPolicy \u003d FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n       numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n           + ecPolicy.getSchema().getNumParityUnits());\n     } else {\n       numTargets \u003d pendingFile.getFileReplication();\n     }\n     storagePolicyID \u003d pendingFile.getStoragePolicyID();\n     return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                       clientMachine, isStriped);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final boolean isStriped;\n\n    INodesInPath iip \u003d fsn.dir.resolvePath(pc, src, fileId);\n    FileState fileState \u003d analyzeFileState(fsn, iip, fileId, clientName,\n                                           previous, onRetryBlock);\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n\n    final INodeFile pendingFile \u003d fileState.inode;\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    isStriped \u003d pendingFile.isStriped();\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (isStriped) {\n      ecPolicy \u003d FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n      numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n          + ecPolicy.getSchema().getNumParityUnits());\n    } else {\n      numTargets \u003d pendingFile.getFileReplication();\n    }\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, isStriped);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10744. Internally optimize path component resolution. Contributed by Daryn Sharp.\n",
      "commitDate": "15/08/16 2:45 PM",
      "commitName": "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "04/08/16 7:07 AM",
      "commitNameOld": "6ae39199dac6ac7be6802b31452552c76da16e24",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 11.32,
      "commitsBetweenForRepo": 73,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,45 @@\n   static ValidateAddBlockResult validateAddBlock(\n       FSNamesystem fsn, FSPermissionChecker pc,\n       String src, long fileId, String clientName,\n       ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n     final long blockSize;\n     final short numTargets;\n     final byte storagePolicyID;\n     String clientMachine;\n     final boolean isStriped;\n \n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    src \u003d fsn.dir.resolvePath(pc, src, pathComponents);\n+    src \u003d fsn.dir.resolvePath(pc, src);\n     FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n                                            previous, onRetryBlock);\n     if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n       // This is a retry. No need to generate new locations.\n       // Use the last block if it has locations.\n       return null;\n     }\n \n     final INodeFile pendingFile \u003d fileState.inode;\n     if (!fsn.checkFileProgress(src, pendingFile, false)) {\n       throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n     }\n     if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n       throw new IOException(\"File has reached the limit on maximum number of\"\n           + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n           + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n           + fsn.maxBlocksPerFile);\n     }\n     blockSize \u003d pendingFile.getPreferredBlockSize();\n     clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n         .getClientMachine();\n     isStriped \u003d pendingFile.isStriped();\n     ErasureCodingPolicy ecPolicy \u003d null;\n     if (isStriped) {\n       ecPolicy \u003d FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n       numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n           + ecPolicy.getSchema().getNumParityUnits());\n     } else {\n       numTargets \u003d pendingFile.getFileReplication();\n     }\n     storagePolicyID \u003d pendingFile.getStoragePolicyID();\n     return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                       clientMachine, isStriped);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final boolean isStriped;\n\n    src \u003d fsn.dir.resolvePath(pc, src);\n    FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n                                           previous, onRetryBlock);\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n\n    final INodeFile pendingFile \u003d fileState.inode;\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    isStriped \u003d pendingFile.isStriped();\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (isStriped) {\n      ecPolicy \u003d FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n      numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n          + ecPolicy.getSchema().getNumParityUnits());\n    } else {\n      numTargets \u003d pendingFile.getFileReplication();\n    }\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, isStriped);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "45c763ad6171bc7808c2ddcb9099a4215113da2a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9690. ClientProtocol.addBlock is not idempotent after HDFS-8071.\n",
      "commitDate": "25/01/16 7:20 PM",
      "commitName": "45c763ad6171bc7808c2ddcb9099a4215113da2a",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "25/01/16 6:32 PM",
      "commitNameOld": "bd909ed9f2d853f614f04a50e2230a7932732776",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,46 @@\n   static ValidateAddBlockResult validateAddBlock(\n       FSNamesystem fsn, FSPermissionChecker pc,\n       String src, long fileId, String clientName,\n       ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n     final long blockSize;\n     final short numTargets;\n     final byte storagePolicyID;\n     String clientMachine;\n     final boolean isStriped;\n \n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     src \u003d fsn.dir.resolvePath(pc, src, pathComponents);\n     FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n                                            previous, onRetryBlock);\n-    final INodeFile pendingFile \u003d fileState.inode;\n-    // Check if the penultimate block is minimally replicated\n-    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n-      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n-    }\n-\n     if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n       // This is a retry. No need to generate new locations.\n       // Use the last block if it has locations.\n       return null;\n     }\n+\n+    final INodeFile pendingFile \u003d fileState.inode;\n+    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n+      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n+    }\n     if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n       throw new IOException(\"File has reached the limit on maximum number of\"\n           + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n           + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n           + fsn.maxBlocksPerFile);\n     }\n     blockSize \u003d pendingFile.getPreferredBlockSize();\n     clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n         .getClientMachine();\n     isStriped \u003d pendingFile.isStriped();\n     ErasureCodingPolicy ecPolicy \u003d null;\n     if (isStriped) {\n       ecPolicy \u003d FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n       numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n           + ecPolicy.getSchema().getNumParityUnits());\n     } else {\n       numTargets \u003d pendingFile.getFileReplication();\n     }\n     storagePolicyID \u003d pendingFile.getStoragePolicyID();\n     return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                       clientMachine, isStriped);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final boolean isStriped;\n\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    src \u003d fsn.dir.resolvePath(pc, src, pathComponents);\n    FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n                                           previous, onRetryBlock);\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n\n    final INodeFile pendingFile \u003d fileState.inode;\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    isStriped \u003d pendingFile.isStriped();\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (isStriped) {\n      ecPolicy \u003d FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n      numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n          + ecPolicy.getSchema().getNumParityUnits());\n    } else {\n      numTargets \u003d pendingFile.getFileReplication();\n    }\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, isStriped);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "c09dc258a8f64fab852bf6f26187163480dbee3c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8882. Erasure Coding: Use datablocks, parityblocks and cell size from ErasureCodingPolicy. Contributed by Vinayakumar B.\n\nChange-Id: Ic56da0b426f47c63dac440aef6f5fc8554f6cf13\n",
      "commitDate": "23/09/15 1:34 PM",
      "commitName": "c09dc258a8f64fab852bf6f26187163480dbee3c",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "09/09/15 11:07 PM",
      "commitNameOld": "f62237bc2f02afe11ce185e13aa51a60b5960037",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 13.6,
      "commitsBetweenForRepo": 91,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,47 @@\n   static ValidateAddBlockResult validateAddBlock(\n       FSNamesystem fsn, FSPermissionChecker pc,\n       String src, long fileId, String clientName,\n       ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n     final long blockSize;\n     final short numTargets;\n     final byte storagePolicyID;\n     String clientMachine;\n     final boolean isStriped;\n \n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     src \u003d fsn.dir.resolvePath(pc, src, pathComponents);\n     FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n                                            previous, onRetryBlock);\n     final INodeFile pendingFile \u003d fileState.inode;\n     // Check if the penultimate block is minimally replicated\n     if (!fsn.checkFileProgress(src, pendingFile, false)) {\n       throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n     }\n \n     if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n       // This is a retry. No need to generate new locations.\n       // Use the last block if it has locations.\n       return null;\n     }\n     if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n       throw new IOException(\"File has reached the limit on maximum number of\"\n           + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n           + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n           + fsn.maxBlocksPerFile);\n     }\n     blockSize \u003d pendingFile.getPreferredBlockSize();\n     clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n         .getClientMachine();\n     isStriped \u003d pendingFile.isStriped();\n-    numTargets \u003d isStriped ?\n-        HdfsConstants.NUM_DATA_BLOCKS + HdfsConstants.NUM_PARITY_BLOCKS :\n-        pendingFile.getFileReplication();\n+    ErasureCodingPolicy ecPolicy \u003d null;\n+    if (isStriped) {\n+      ecPolicy \u003d FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n+      numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n+          + ecPolicy.getSchema().getNumParityUnits());\n+    } else {\n+      numTargets \u003d pendingFile.getFileReplication();\n+    }\n     storagePolicyID \u003d pendingFile.getStoragePolicyID();\n     return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                       clientMachine, isStriped);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final boolean isStriped;\n\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    src \u003d fsn.dir.resolvePath(pc, src, pathComponents);\n    FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n                                           previous, onRetryBlock);\n    final INodeFile pendingFile \u003d fileState.inode;\n    // Check if the penultimate block is minimally replicated\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    isStriped \u003d pendingFile.isStriped();\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (isStriped) {\n      ecPolicy \u003d FSDirErasureCodingOp.getErasureCodingPolicy(fsn, src);\n      numTargets \u003d (short) (ecPolicy.getSchema().getNumDataUnits()\n          + ecPolicy.getSchema().getNumParityUnits());\n    } else {\n      numTargets \u003d pendingFile.getFileReplication();\n    }\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, isStriped);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "e53fa769c97416af69ea567aecd44f67e896688b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8186. Erasure coding: Make block placement policy for EC file configurable. Contributed by Walter Su.\n",
      "commitDate": "26/05/15 12:02 PM",
      "commitName": "e53fa769c97416af69ea567aecd44f67e896688b",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 12:02 PM",
      "commitNameOld": "c9103e9cacc67a614940e32fa87c5dbc3daa60de",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,42 @@\n   static ValidateAddBlockResult validateAddBlock(\n       FSNamesystem fsn, FSPermissionChecker pc,\n       String src, long fileId, String clientName,\n       ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n     final long blockSize;\n     final short numTargets;\n     final byte storagePolicyID;\n     String clientMachine;\n+    final boolean isStriped;\n \n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     src \u003d fsn.dir.resolvePath(pc, src, pathComponents);\n     FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n                                            previous, onRetryBlock);\n     final INodeFile pendingFile \u003d fileState.inode;\n     // Check if the penultimate block is minimally replicated\n     if (!fsn.checkFileProgress(src, pendingFile, false)) {\n       throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n     }\n \n     if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n       // This is a retry. No need to generate new locations.\n       // Use the last block if it has locations.\n       return null;\n     }\n     if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n       throw new IOException(\"File has reached the limit on maximum number of\"\n           + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n           + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n           + fsn.maxBlocksPerFile);\n     }\n     blockSize \u003d pendingFile.getPreferredBlockSize();\n     clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n         .getClientMachine();\n-    boolean isStriped \u003d pendingFile.isStriped();\n+    isStriped \u003d pendingFile.isStriped();\n     numTargets \u003d isStriped ?\n         HdfsConstants.NUM_DATA_BLOCKS + HdfsConstants.NUM_PARITY_BLOCKS :\n         pendingFile.getFileReplication();\n     storagePolicyID \u003d pendingFile.getStoragePolicyID();\n     return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n-                                      clientMachine);\n+                                      clientMachine, isStriped);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n    final boolean isStriped;\n\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    src \u003d fsn.dir.resolvePath(pc, src, pathComponents);\n    FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n                                           previous, onRetryBlock);\n    final INodeFile pendingFile \u003d fileState.inode;\n    // Check if the penultimate block is minimally replicated\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    isStriped \u003d pendingFile.isStriped();\n    numTargets \u003d isStriped ?\n        HdfsConstants.NUM_DATA_BLOCKS + HdfsConstants.NUM_PARITY_BLOCKS :\n        pendingFile.getFileReplication();\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine, isStriped);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "d8ea443af0b1c8289a1dd738945831ff8be0e9c1": {
      "type": "Ybodychange",
      "commitMessage": "Merge HDFS-8394 from trunk: Move getAdditionalBlock() and related functionalities into a separate class.\n",
      "commitDate": "26/05/15 12:02 PM",
      "commitName": "d8ea443af0b1c8289a1dd738945831ff8be0e9c1",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:55 AM",
      "commitNameOld": "1af8c148626effe1b41fc536019fd3349f485d59",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 74,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,41 @@\n   static ValidateAddBlockResult validateAddBlock(\n       FSNamesystem fsn, FSPermissionChecker pc,\n       String src, long fileId, String clientName,\n       ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n     final long blockSize;\n-    final int replication;\n+    final short numTargets;\n     final byte storagePolicyID;\n     String clientMachine;\n \n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     src \u003d fsn.dir.resolvePath(pc, src, pathComponents);\n     FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n                                            previous, onRetryBlock);\n     final INodeFile pendingFile \u003d fileState.inode;\n     // Check if the penultimate block is minimally replicated\n     if (!fsn.checkFileProgress(src, pendingFile, false)) {\n       throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n     }\n \n     if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n       // This is a retry. No need to generate new locations.\n       // Use the last block if it has locations.\n       return null;\n     }\n     if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n       throw new IOException(\"File has reached the limit on maximum number of\"\n           + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n           + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n           + fsn.maxBlocksPerFile);\n     }\n     blockSize \u003d pendingFile.getPreferredBlockSize();\n     clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n         .getClientMachine();\n-    replication \u003d pendingFile.getFileReplication();\n+    boolean isStriped \u003d pendingFile.isStriped();\n+    numTargets \u003d isStriped ?\n+        HdfsConstants.NUM_DATA_BLOCKS + HdfsConstants.NUM_PARITY_BLOCKS :\n+        pendingFile.getFileReplication();\n     storagePolicyID \u003d pendingFile.getStoragePolicyID();\n-    return new ValidateAddBlockResult(blockSize, replication, storagePolicyID,\n-                                    clientMachine);\n+    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n+                                      clientMachine);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final short numTargets;\n    final byte storagePolicyID;\n    String clientMachine;\n\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    src \u003d fsn.dir.resolvePath(pc, src, pathComponents);\n    FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n                                           previous, onRetryBlock);\n    final INodeFile pendingFile \u003d fileState.inode;\n    // Check if the penultimate block is minimally replicated\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    boolean isStriped \u003d pendingFile.isStriped();\n    numTargets \u003d isStriped ?\n        HdfsConstants.NUM_DATA_BLOCKS + HdfsConstants.NUM_PARITY_BLOCKS :\n        pendingFile.getFileReplication();\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, numTargets, storagePolicyID,\n                                      clientMachine);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java",
      "extendedDetails": {}
    },
    "e5afac5896a1a88e152746598527d91f73cbb724": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8394. Move getAdditionalBlock() and related functionalities into a separate class. Contributed by Haohui Mai.\n",
      "commitDate": "15/05/15 7:09 PM",
      "commitName": "e5afac5896a1a88e152746598527d91f73cbb724",
      "commitAuthor": "Haohui Mai",
      "diff": "@@ -0,0 +1,38 @@\n+  static ValidateAddBlockResult validateAddBlock(\n+      FSNamesystem fsn, FSPermissionChecker pc,\n+      String src, long fileId, String clientName,\n+      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n+    final long blockSize;\n+    final int replication;\n+    final byte storagePolicyID;\n+    String clientMachine;\n+\n+    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n+    src \u003d fsn.dir.resolvePath(pc, src, pathComponents);\n+    FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n+                                           previous, onRetryBlock);\n+    final INodeFile pendingFile \u003d fileState.inode;\n+    // Check if the penultimate block is minimally replicated\n+    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n+      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n+    }\n+\n+    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n+      // This is a retry. No need to generate new locations.\n+      // Use the last block if it has locations.\n+      return null;\n+    }\n+    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n+      throw new IOException(\"File has reached the limit on maximum number of\"\n+          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n+          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n+          + fsn.maxBlocksPerFile);\n+    }\n+    blockSize \u003d pendingFile.getPreferredBlockSize();\n+    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n+        .getClientMachine();\n+    replication \u003d pendingFile.getFileReplication();\n+    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n+    return new ValidateAddBlockResult(blockSize, replication, storagePolicyID,\n+                                    clientMachine);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  static ValidateAddBlockResult validateAddBlock(\n      FSNamesystem fsn, FSPermissionChecker pc,\n      String src, long fileId, String clientName,\n      ExtendedBlock previous, LocatedBlock[] onRetryBlock) throws IOException {\n    final long blockSize;\n    final int replication;\n    final byte storagePolicyID;\n    String clientMachine;\n\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    src \u003d fsn.dir.resolvePath(pc, src, pathComponents);\n    FileState fileState \u003d analyzeFileState(fsn, src, fileId, clientName,\n                                           previous, onRetryBlock);\n    final INodeFile pendingFile \u003d fileState.inode;\n    // Check if the penultimate block is minimally replicated\n    if (!fsn.checkFileProgress(src, pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n\n    if (onRetryBlock[0] !\u003d null \u0026\u0026 onRetryBlock[0].getLocations().length \u003e 0) {\n      // This is a retry. No need to generate new locations.\n      // Use the last block if it has locations.\n      return null;\n    }\n    if (pendingFile.getBlocks().length \u003e\u003d fsn.maxBlocksPerFile) {\n      throw new IOException(\"File has reached the limit on maximum number of\"\n          + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n          + \"): \" + pendingFile.getBlocks().length + \" \u003e\u003d \"\n          + fsn.maxBlocksPerFile);\n    }\n    blockSize \u003d pendingFile.getPreferredBlockSize();\n    clientMachine \u003d pendingFile.getFileUnderConstructionFeature()\n        .getClientMachine();\n    replication \u003d pendingFile.getFileReplication();\n    storagePolicyID \u003d pendingFile.getStoragePolicyID();\n    return new ValidateAddBlockResult(blockSize, replication, storagePolicyID,\n                                    clientMachine);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java"
    }
  }
}