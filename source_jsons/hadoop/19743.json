{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "MapTask.java",
  "functionName": "init",
  "functionId": "init___context-MapOutputCollector.Context",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
  "functionStartLine": 828,
  "functionEndLine": 851,
  "numCommitsSeen": 30,
  "timeTaken": 1987,
  "changeHistory": [
    "683e0c71fe09600a24bdd7b707a613fe70ff1f6e",
    "618ba707f0f2ddc353414dbd0eee0ab9e83b8013",
    "dea1b2e84aed0d2e597036f070ae8830f579a498",
    "8329fae686cf7a68679d177c25623311beec3384"
  ],
  "changeHistoryShort": {
    "683e0c71fe09600a24bdd7b707a613fe70ff1f6e": "Ybodychange",
    "618ba707f0f2ddc353414dbd0eee0ab9e83b8013": "Ybodychange",
    "dea1b2e84aed0d2e597036f070ae8830f579a498": "Ybodychange",
    "8329fae686cf7a68679d177c25623311beec3384": "Yintroduced"
  },
  "changeHistoryDetails": {
    "683e0c71fe09600a24bdd7b707a613fe70ff1f6e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6793. io.sort.factor code default and mapred-default.xml values inconsistent. Contributed by Gera Shegalov.\n",
      "commitDate": "21/11/16 8:40 AM",
      "commitName": "683e0c71fe09600a24bdd7b707a613fe70ff1f6e",
      "commitAuthor": "Rohith Sharma K S",
      "commitDateOld": "09/09/16 11:12 AM",
      "commitNameOld": "9f192cc5ac4a6145e2eeaecba0a754d31e601898",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 72.94,
      "commitsBetweenForRepo": 561,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,107 +1,108 @@\n     public void init(MapOutputCollector.Context context\n                     ) throws IOException, ClassNotFoundException {\n       job \u003d context.getJobConf();\n       reporter \u003d context.getReporter();\n       mapTask \u003d context.getMapTask();\n       mapOutputFile \u003d mapTask.getMapOutputFile();\n       sortPhase \u003d mapTask.getSortPhase();\n       spilledRecordsCounter \u003d reporter.getCounter(TaskCounter.SPILLED_RECORDS);\n       partitions \u003d job.getNumReduceTasks();\n       rfs \u003d ((LocalFileSystem)FileSystem.getLocal(job)).getRaw();\n \n       //sanity checks\n       final float spillper \u003d\n         job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);\n-      final int sortmb \u003d job.getInt(JobContext.IO_SORT_MB, 100);\n+      final int sortmb \u003d job.getInt(MRJobConfig.IO_SORT_MB,\n+          MRJobConfig.DEFAULT_IO_SORT_MB);\n       indexCacheMemoryLimit \u003d job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,\n                                          INDEX_CACHE_MEMORY_LIMIT_DEFAULT);\n       if (spillper \u003e (float)1.0 || spillper \u003c\u003d (float)0.0) {\n         throw new IOException(\"Invalid \\\"\" + JobContext.MAP_SORT_SPILL_PERCENT +\n             \"\\\": \" + spillper);\n       }\n       if ((sortmb \u0026 0x7FF) !\u003d sortmb) {\n         throw new IOException(\n             \"Invalid \\\"\" + JobContext.IO_SORT_MB + \"\\\": \" + sortmb);\n       }\n       sorter \u003d ReflectionUtils.newInstance(job.getClass(\n                    MRJobConfig.MAP_SORT_CLASS, QuickSort.class,\n                    IndexedSorter.class), job);\n       // buffers and accounting\n       int maxMemUsage \u003d sortmb \u003c\u003c 20;\n       maxMemUsage -\u003d maxMemUsage % METASIZE;\n       kvbuffer \u003d new byte[maxMemUsage];\n       bufvoid \u003d kvbuffer.length;\n       kvmeta \u003d ByteBuffer.wrap(kvbuffer)\n          .order(ByteOrder.nativeOrder())\n          .asIntBuffer();\n       setEquator(0);\n       bufstart \u003d bufend \u003d bufindex \u003d equator;\n       kvstart \u003d kvend \u003d kvindex;\n \n       maxRec \u003d kvmeta.capacity() / NMETA;\n       softLimit \u003d (int)(kvbuffer.length * spillper);\n       bufferRemaining \u003d softLimit;\n       LOG.info(JobContext.IO_SORT_MB + \": \" + sortmb);\n       LOG.info(\"soft limit at \" + softLimit);\n       LOG.info(\"bufstart \u003d \" + bufstart + \"; bufvoid \u003d \" + bufvoid);\n       LOG.info(\"kvstart \u003d \" + kvstart + \"; length \u003d \" + maxRec);\n \n       // k/v serialization\n       comparator \u003d job.getOutputKeyComparator();\n       keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n       valClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n       serializationFactory \u003d new SerializationFactory(job);\n       keySerializer \u003d serializationFactory.getSerializer(keyClass);\n       keySerializer.open(bb);\n       valSerializer \u003d serializationFactory.getSerializer(valClass);\n       valSerializer.open(bb);\n \n       // output counters\n       mapOutputByteCounter \u003d reporter.getCounter(TaskCounter.MAP_OUTPUT_BYTES);\n       mapOutputRecordCounter \u003d\n         reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);\n       fileOutputByteCounter \u003d reporter\n           .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);\n \n       // compression\n       if (job.getCompressMapOutput()) {\n         Class\u003c? extends CompressionCodec\u003e codecClass \u003d\n           job.getMapOutputCompressorClass(DefaultCodec.class);\n         codec \u003d ReflectionUtils.newInstance(codecClass, job);\n       } else {\n         codec \u003d null;\n       }\n \n       // combiner\n       final Counters.Counter combineInputCounter \u003d\n         reporter.getCounter(TaskCounter.COMBINE_INPUT_RECORDS);\n       combinerRunner \u003d CombinerRunner.create(job, getTaskID(), \n                                              combineInputCounter,\n                                              reporter, null);\n       if (combinerRunner !\u003d null) {\n         final Counters.Counter combineOutputCounter \u003d\n           reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);\n         combineCollector\u003d new CombineOutputCollector\u003cK,V\u003e(combineOutputCounter, reporter, job);\n       } else {\n         combineCollector \u003d null;\n       }\n       spillInProgress \u003d false;\n       minSpillsForCombine \u003d job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);\n       spillThread.setDaemon(true);\n       spillThread.setName(\"SpillThread\");\n       spillLock.lock();\n       try {\n         spillThread.start();\n         while (!spillThreadRunning) {\n           spillDone.await();\n         }\n       } catch (InterruptedException e) {\n         throw new IOException(\"Spill thread failed to initialize\", e);\n       } finally {\n         spillLock.unlock();\n       }\n       if (sortSpillException !\u003d null) {\n         throw new IOException(\"Spill thread failed to initialize\",\n             sortSpillException);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void init(MapOutputCollector.Context context\n                    ) throws IOException, ClassNotFoundException {\n      job \u003d context.getJobConf();\n      reporter \u003d context.getReporter();\n      mapTask \u003d context.getMapTask();\n      mapOutputFile \u003d mapTask.getMapOutputFile();\n      sortPhase \u003d mapTask.getSortPhase();\n      spilledRecordsCounter \u003d reporter.getCounter(TaskCounter.SPILLED_RECORDS);\n      partitions \u003d job.getNumReduceTasks();\n      rfs \u003d ((LocalFileSystem)FileSystem.getLocal(job)).getRaw();\n\n      //sanity checks\n      final float spillper \u003d\n        job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);\n      final int sortmb \u003d job.getInt(MRJobConfig.IO_SORT_MB,\n          MRJobConfig.DEFAULT_IO_SORT_MB);\n      indexCacheMemoryLimit \u003d job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,\n                                         INDEX_CACHE_MEMORY_LIMIT_DEFAULT);\n      if (spillper \u003e (float)1.0 || spillper \u003c\u003d (float)0.0) {\n        throw new IOException(\"Invalid \\\"\" + JobContext.MAP_SORT_SPILL_PERCENT +\n            \"\\\": \" + spillper);\n      }\n      if ((sortmb \u0026 0x7FF) !\u003d sortmb) {\n        throw new IOException(\n            \"Invalid \\\"\" + JobContext.IO_SORT_MB + \"\\\": \" + sortmb);\n      }\n      sorter \u003d ReflectionUtils.newInstance(job.getClass(\n                   MRJobConfig.MAP_SORT_CLASS, QuickSort.class,\n                   IndexedSorter.class), job);\n      // buffers and accounting\n      int maxMemUsage \u003d sortmb \u003c\u003c 20;\n      maxMemUsage -\u003d maxMemUsage % METASIZE;\n      kvbuffer \u003d new byte[maxMemUsage];\n      bufvoid \u003d kvbuffer.length;\n      kvmeta \u003d ByteBuffer.wrap(kvbuffer)\n         .order(ByteOrder.nativeOrder())\n         .asIntBuffer();\n      setEquator(0);\n      bufstart \u003d bufend \u003d bufindex \u003d equator;\n      kvstart \u003d kvend \u003d kvindex;\n\n      maxRec \u003d kvmeta.capacity() / NMETA;\n      softLimit \u003d (int)(kvbuffer.length * spillper);\n      bufferRemaining \u003d softLimit;\n      LOG.info(JobContext.IO_SORT_MB + \": \" + sortmb);\n      LOG.info(\"soft limit at \" + softLimit);\n      LOG.info(\"bufstart \u003d \" + bufstart + \"; bufvoid \u003d \" + bufvoid);\n      LOG.info(\"kvstart \u003d \" + kvstart + \"; length \u003d \" + maxRec);\n\n      // k/v serialization\n      comparator \u003d job.getOutputKeyComparator();\n      keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n      valClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n      serializationFactory \u003d new SerializationFactory(job);\n      keySerializer \u003d serializationFactory.getSerializer(keyClass);\n      keySerializer.open(bb);\n      valSerializer \u003d serializationFactory.getSerializer(valClass);\n      valSerializer.open(bb);\n\n      // output counters\n      mapOutputByteCounter \u003d reporter.getCounter(TaskCounter.MAP_OUTPUT_BYTES);\n      mapOutputRecordCounter \u003d\n        reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);\n      fileOutputByteCounter \u003d reporter\n          .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);\n\n      // compression\n      if (job.getCompressMapOutput()) {\n        Class\u003c? extends CompressionCodec\u003e codecClass \u003d\n          job.getMapOutputCompressorClass(DefaultCodec.class);\n        codec \u003d ReflectionUtils.newInstance(codecClass, job);\n      } else {\n        codec \u003d null;\n      }\n\n      // combiner\n      final Counters.Counter combineInputCounter \u003d\n        reporter.getCounter(TaskCounter.COMBINE_INPUT_RECORDS);\n      combinerRunner \u003d CombinerRunner.create(job, getTaskID(), \n                                             combineInputCounter,\n                                             reporter, null);\n      if (combinerRunner !\u003d null) {\n        final Counters.Counter combineOutputCounter \u003d\n          reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);\n        combineCollector\u003d new CombineOutputCollector\u003cK,V\u003e(combineOutputCounter, reporter, job);\n      } else {\n        combineCollector \u003d null;\n      }\n      spillInProgress \u003d false;\n      minSpillsForCombine \u003d job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);\n      spillThread.setDaemon(true);\n      spillThread.setName(\"SpillThread\");\n      spillLock.lock();\n      try {\n        spillThread.start();\n        while (!spillThreadRunning) {\n          spillDone.await();\n        }\n      } catch (InterruptedException e) {\n        throw new IOException(\"Spill thread failed to initialize\", e);\n      } finally {\n        spillLock.unlock();\n      }\n      if (sortSpillException !\u003d null) {\n        throw new IOException(\"Spill thread failed to initialize\",\n            sortSpillException);\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "618ba707f0f2ddc353414dbd0eee0ab9e83b8013": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6057. Remove obsolete entries from mapred-default.xml (Ray Chiang via aw)\n",
      "commitDate": "26/04/15 8:31 PM",
      "commitName": "618ba707f0f2ddc353414dbd0eee0ab9e83b8013",
      "commitAuthor": "Allen Wittenauer",
      "commitDateOld": "16/03/15 10:55 PM",
      "commitNameOld": "bb243cea93b6872ef8956311a2290ca0b83ebb22",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 40.9,
      "commitsBetweenForRepo": 373,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,106 +1,107 @@\n     public void init(MapOutputCollector.Context context\n                     ) throws IOException, ClassNotFoundException {\n       job \u003d context.getJobConf();\n       reporter \u003d context.getReporter();\n       mapTask \u003d context.getMapTask();\n       mapOutputFile \u003d mapTask.getMapOutputFile();\n       sortPhase \u003d mapTask.getSortPhase();\n       spilledRecordsCounter \u003d reporter.getCounter(TaskCounter.SPILLED_RECORDS);\n       partitions \u003d job.getNumReduceTasks();\n       rfs \u003d ((LocalFileSystem)FileSystem.getLocal(job)).getRaw();\n \n       //sanity checks\n       final float spillper \u003d\n         job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);\n       final int sortmb \u003d job.getInt(JobContext.IO_SORT_MB, 100);\n       indexCacheMemoryLimit \u003d job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,\n                                          INDEX_CACHE_MEMORY_LIMIT_DEFAULT);\n       if (spillper \u003e (float)1.0 || spillper \u003c\u003d (float)0.0) {\n         throw new IOException(\"Invalid \\\"\" + JobContext.MAP_SORT_SPILL_PERCENT +\n             \"\\\": \" + spillper);\n       }\n       if ((sortmb \u0026 0x7FF) !\u003d sortmb) {\n         throw new IOException(\n             \"Invalid \\\"\" + JobContext.IO_SORT_MB + \"\\\": \" + sortmb);\n       }\n-      sorter \u003d ReflectionUtils.newInstance(job.getClass(\"map.sort.class\",\n-            QuickSort.class, IndexedSorter.class), job);\n+      sorter \u003d ReflectionUtils.newInstance(job.getClass(\n+                   MRJobConfig.MAP_SORT_CLASS, QuickSort.class,\n+                   IndexedSorter.class), job);\n       // buffers and accounting\n       int maxMemUsage \u003d sortmb \u003c\u003c 20;\n       maxMemUsage -\u003d maxMemUsage % METASIZE;\n       kvbuffer \u003d new byte[maxMemUsage];\n       bufvoid \u003d kvbuffer.length;\n       kvmeta \u003d ByteBuffer.wrap(kvbuffer)\n          .order(ByteOrder.nativeOrder())\n          .asIntBuffer();\n       setEquator(0);\n       bufstart \u003d bufend \u003d bufindex \u003d equator;\n       kvstart \u003d kvend \u003d kvindex;\n \n       maxRec \u003d kvmeta.capacity() / NMETA;\n       softLimit \u003d (int)(kvbuffer.length * spillper);\n       bufferRemaining \u003d softLimit;\n       LOG.info(JobContext.IO_SORT_MB + \": \" + sortmb);\n       LOG.info(\"soft limit at \" + softLimit);\n       LOG.info(\"bufstart \u003d \" + bufstart + \"; bufvoid \u003d \" + bufvoid);\n       LOG.info(\"kvstart \u003d \" + kvstart + \"; length \u003d \" + maxRec);\n \n       // k/v serialization\n       comparator \u003d job.getOutputKeyComparator();\n       keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n       valClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n       serializationFactory \u003d new SerializationFactory(job);\n       keySerializer \u003d serializationFactory.getSerializer(keyClass);\n       keySerializer.open(bb);\n       valSerializer \u003d serializationFactory.getSerializer(valClass);\n       valSerializer.open(bb);\n \n       // output counters\n       mapOutputByteCounter \u003d reporter.getCounter(TaskCounter.MAP_OUTPUT_BYTES);\n       mapOutputRecordCounter \u003d\n         reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);\n       fileOutputByteCounter \u003d reporter\n           .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);\n \n       // compression\n       if (job.getCompressMapOutput()) {\n         Class\u003c? extends CompressionCodec\u003e codecClass \u003d\n           job.getMapOutputCompressorClass(DefaultCodec.class);\n         codec \u003d ReflectionUtils.newInstance(codecClass, job);\n       } else {\n         codec \u003d null;\n       }\n \n       // combiner\n       final Counters.Counter combineInputCounter \u003d\n         reporter.getCounter(TaskCounter.COMBINE_INPUT_RECORDS);\n       combinerRunner \u003d CombinerRunner.create(job, getTaskID(), \n                                              combineInputCounter,\n                                              reporter, null);\n       if (combinerRunner !\u003d null) {\n         final Counters.Counter combineOutputCounter \u003d\n           reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);\n         combineCollector\u003d new CombineOutputCollector\u003cK,V\u003e(combineOutputCounter, reporter, job);\n       } else {\n         combineCollector \u003d null;\n       }\n       spillInProgress \u003d false;\n       minSpillsForCombine \u003d job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);\n       spillThread.setDaemon(true);\n       spillThread.setName(\"SpillThread\");\n       spillLock.lock();\n       try {\n         spillThread.start();\n         while (!spillThreadRunning) {\n           spillDone.await();\n         }\n       } catch (InterruptedException e) {\n         throw new IOException(\"Spill thread failed to initialize\", e);\n       } finally {\n         spillLock.unlock();\n       }\n       if (sortSpillException !\u003d null) {\n         throw new IOException(\"Spill thread failed to initialize\",\n             sortSpillException);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void init(MapOutputCollector.Context context\n                    ) throws IOException, ClassNotFoundException {\n      job \u003d context.getJobConf();\n      reporter \u003d context.getReporter();\n      mapTask \u003d context.getMapTask();\n      mapOutputFile \u003d mapTask.getMapOutputFile();\n      sortPhase \u003d mapTask.getSortPhase();\n      spilledRecordsCounter \u003d reporter.getCounter(TaskCounter.SPILLED_RECORDS);\n      partitions \u003d job.getNumReduceTasks();\n      rfs \u003d ((LocalFileSystem)FileSystem.getLocal(job)).getRaw();\n\n      //sanity checks\n      final float spillper \u003d\n        job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);\n      final int sortmb \u003d job.getInt(JobContext.IO_SORT_MB, 100);\n      indexCacheMemoryLimit \u003d job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,\n                                         INDEX_CACHE_MEMORY_LIMIT_DEFAULT);\n      if (spillper \u003e (float)1.0 || spillper \u003c\u003d (float)0.0) {\n        throw new IOException(\"Invalid \\\"\" + JobContext.MAP_SORT_SPILL_PERCENT +\n            \"\\\": \" + spillper);\n      }\n      if ((sortmb \u0026 0x7FF) !\u003d sortmb) {\n        throw new IOException(\n            \"Invalid \\\"\" + JobContext.IO_SORT_MB + \"\\\": \" + sortmb);\n      }\n      sorter \u003d ReflectionUtils.newInstance(job.getClass(\n                   MRJobConfig.MAP_SORT_CLASS, QuickSort.class,\n                   IndexedSorter.class), job);\n      // buffers and accounting\n      int maxMemUsage \u003d sortmb \u003c\u003c 20;\n      maxMemUsage -\u003d maxMemUsage % METASIZE;\n      kvbuffer \u003d new byte[maxMemUsage];\n      bufvoid \u003d kvbuffer.length;\n      kvmeta \u003d ByteBuffer.wrap(kvbuffer)\n         .order(ByteOrder.nativeOrder())\n         .asIntBuffer();\n      setEquator(0);\n      bufstart \u003d bufend \u003d bufindex \u003d equator;\n      kvstart \u003d kvend \u003d kvindex;\n\n      maxRec \u003d kvmeta.capacity() / NMETA;\n      softLimit \u003d (int)(kvbuffer.length * spillper);\n      bufferRemaining \u003d softLimit;\n      LOG.info(JobContext.IO_SORT_MB + \": \" + sortmb);\n      LOG.info(\"soft limit at \" + softLimit);\n      LOG.info(\"bufstart \u003d \" + bufstart + \"; bufvoid \u003d \" + bufvoid);\n      LOG.info(\"kvstart \u003d \" + kvstart + \"; length \u003d \" + maxRec);\n\n      // k/v serialization\n      comparator \u003d job.getOutputKeyComparator();\n      keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n      valClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n      serializationFactory \u003d new SerializationFactory(job);\n      keySerializer \u003d serializationFactory.getSerializer(keyClass);\n      keySerializer.open(bb);\n      valSerializer \u003d serializationFactory.getSerializer(valClass);\n      valSerializer.open(bb);\n\n      // output counters\n      mapOutputByteCounter \u003d reporter.getCounter(TaskCounter.MAP_OUTPUT_BYTES);\n      mapOutputRecordCounter \u003d\n        reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);\n      fileOutputByteCounter \u003d reporter\n          .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);\n\n      // compression\n      if (job.getCompressMapOutput()) {\n        Class\u003c? extends CompressionCodec\u003e codecClass \u003d\n          job.getMapOutputCompressorClass(DefaultCodec.class);\n        codec \u003d ReflectionUtils.newInstance(codecClass, job);\n      } else {\n        codec \u003d null;\n      }\n\n      // combiner\n      final Counters.Counter combineInputCounter \u003d\n        reporter.getCounter(TaskCounter.COMBINE_INPUT_RECORDS);\n      combinerRunner \u003d CombinerRunner.create(job, getTaskID(), \n                                             combineInputCounter,\n                                             reporter, null);\n      if (combinerRunner !\u003d null) {\n        final Counters.Counter combineOutputCounter \u003d\n          reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);\n        combineCollector\u003d new CombineOutputCollector\u003cK,V\u003e(combineOutputCounter, reporter, job);\n      } else {\n        combineCollector \u003d null;\n      }\n      spillInProgress \u003d false;\n      minSpillsForCombine \u003d job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);\n      spillThread.setDaemon(true);\n      spillThread.setName(\"SpillThread\");\n      spillLock.lock();\n      try {\n        spillThread.start();\n        while (!spillThreadRunning) {\n          spillDone.await();\n        }\n      } catch (InterruptedException e) {\n        throw new IOException(\"Spill thread failed to initialize\", e);\n      } finally {\n        spillLock.unlock();\n      }\n      if (sortSpillException !\u003d null) {\n        throw new IOException(\"Spill thread failed to initialize\",\n            sortSpillException);\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "dea1b2e84aed0d2e597036f070ae8830f579a498": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10005. No need to check INFO severity level is enabled or not. Contributed by Jackie Chang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1532907 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 2:00 PM",
      "commitName": "dea1b2e84aed0d2e597036f070ae8830f579a498",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "16/08/13 1:11 AM",
      "commitNameOld": "cae55de2cd1f9ea068f3410c8bdea14cf55738cb",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 61.53,
      "commitsBetweenForRepo": 369,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,108 +1,106 @@\n     public void init(MapOutputCollector.Context context\n                     ) throws IOException, ClassNotFoundException {\n       job \u003d context.getJobConf();\n       reporter \u003d context.getReporter();\n       mapTask \u003d context.getMapTask();\n       mapOutputFile \u003d mapTask.getMapOutputFile();\n       sortPhase \u003d mapTask.getSortPhase();\n       spilledRecordsCounter \u003d reporter.getCounter(TaskCounter.SPILLED_RECORDS);\n       partitions \u003d job.getNumReduceTasks();\n       rfs \u003d ((LocalFileSystem)FileSystem.getLocal(job)).getRaw();\n \n       //sanity checks\n       final float spillper \u003d\n         job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);\n       final int sortmb \u003d job.getInt(JobContext.IO_SORT_MB, 100);\n       indexCacheMemoryLimit \u003d job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,\n                                          INDEX_CACHE_MEMORY_LIMIT_DEFAULT);\n       if (spillper \u003e (float)1.0 || spillper \u003c\u003d (float)0.0) {\n         throw new IOException(\"Invalid \\\"\" + JobContext.MAP_SORT_SPILL_PERCENT +\n             \"\\\": \" + spillper);\n       }\n       if ((sortmb \u0026 0x7FF) !\u003d sortmb) {\n         throw new IOException(\n             \"Invalid \\\"\" + JobContext.IO_SORT_MB + \"\\\": \" + sortmb);\n       }\n       sorter \u003d ReflectionUtils.newInstance(job.getClass(\"map.sort.class\",\n             QuickSort.class, IndexedSorter.class), job);\n       // buffers and accounting\n       int maxMemUsage \u003d sortmb \u003c\u003c 20;\n       maxMemUsage -\u003d maxMemUsage % METASIZE;\n       kvbuffer \u003d new byte[maxMemUsage];\n       bufvoid \u003d kvbuffer.length;\n       kvmeta \u003d ByteBuffer.wrap(kvbuffer)\n          .order(ByteOrder.nativeOrder())\n          .asIntBuffer();\n       setEquator(0);\n       bufstart \u003d bufend \u003d bufindex \u003d equator;\n       kvstart \u003d kvend \u003d kvindex;\n \n       maxRec \u003d kvmeta.capacity() / NMETA;\n       softLimit \u003d (int)(kvbuffer.length * spillper);\n       bufferRemaining \u003d softLimit;\n-      if (LOG.isInfoEnabled()) {\n-        LOG.info(JobContext.IO_SORT_MB + \": \" + sortmb);\n-        LOG.info(\"soft limit at \" + softLimit);\n-        LOG.info(\"bufstart \u003d \" + bufstart + \"; bufvoid \u003d \" + bufvoid);\n-        LOG.info(\"kvstart \u003d \" + kvstart + \"; length \u003d \" + maxRec);\n-      }\n+      LOG.info(JobContext.IO_SORT_MB + \": \" + sortmb);\n+      LOG.info(\"soft limit at \" + softLimit);\n+      LOG.info(\"bufstart \u003d \" + bufstart + \"; bufvoid \u003d \" + bufvoid);\n+      LOG.info(\"kvstart \u003d \" + kvstart + \"; length \u003d \" + maxRec);\n \n       // k/v serialization\n       comparator \u003d job.getOutputKeyComparator();\n       keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n       valClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n       serializationFactory \u003d new SerializationFactory(job);\n       keySerializer \u003d serializationFactory.getSerializer(keyClass);\n       keySerializer.open(bb);\n       valSerializer \u003d serializationFactory.getSerializer(valClass);\n       valSerializer.open(bb);\n \n       // output counters\n       mapOutputByteCounter \u003d reporter.getCounter(TaskCounter.MAP_OUTPUT_BYTES);\n       mapOutputRecordCounter \u003d\n         reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);\n       fileOutputByteCounter \u003d reporter\n           .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);\n \n       // compression\n       if (job.getCompressMapOutput()) {\n         Class\u003c? extends CompressionCodec\u003e codecClass \u003d\n           job.getMapOutputCompressorClass(DefaultCodec.class);\n         codec \u003d ReflectionUtils.newInstance(codecClass, job);\n       } else {\n         codec \u003d null;\n       }\n \n       // combiner\n       final Counters.Counter combineInputCounter \u003d\n         reporter.getCounter(TaskCounter.COMBINE_INPUT_RECORDS);\n       combinerRunner \u003d CombinerRunner.create(job, getTaskID(), \n                                              combineInputCounter,\n                                              reporter, null);\n       if (combinerRunner !\u003d null) {\n         final Counters.Counter combineOutputCounter \u003d\n           reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);\n         combineCollector\u003d new CombineOutputCollector\u003cK,V\u003e(combineOutputCounter, reporter, job);\n       } else {\n         combineCollector \u003d null;\n       }\n       spillInProgress \u003d false;\n       minSpillsForCombine \u003d job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);\n       spillThread.setDaemon(true);\n       spillThread.setName(\"SpillThread\");\n       spillLock.lock();\n       try {\n         spillThread.start();\n         while (!spillThreadRunning) {\n           spillDone.await();\n         }\n       } catch (InterruptedException e) {\n         throw new IOException(\"Spill thread failed to initialize\", e);\n       } finally {\n         spillLock.unlock();\n       }\n       if (sortSpillException !\u003d null) {\n         throw new IOException(\"Spill thread failed to initialize\",\n             sortSpillException);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void init(MapOutputCollector.Context context\n                    ) throws IOException, ClassNotFoundException {\n      job \u003d context.getJobConf();\n      reporter \u003d context.getReporter();\n      mapTask \u003d context.getMapTask();\n      mapOutputFile \u003d mapTask.getMapOutputFile();\n      sortPhase \u003d mapTask.getSortPhase();\n      spilledRecordsCounter \u003d reporter.getCounter(TaskCounter.SPILLED_RECORDS);\n      partitions \u003d job.getNumReduceTasks();\n      rfs \u003d ((LocalFileSystem)FileSystem.getLocal(job)).getRaw();\n\n      //sanity checks\n      final float spillper \u003d\n        job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);\n      final int sortmb \u003d job.getInt(JobContext.IO_SORT_MB, 100);\n      indexCacheMemoryLimit \u003d job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,\n                                         INDEX_CACHE_MEMORY_LIMIT_DEFAULT);\n      if (spillper \u003e (float)1.0 || spillper \u003c\u003d (float)0.0) {\n        throw new IOException(\"Invalid \\\"\" + JobContext.MAP_SORT_SPILL_PERCENT +\n            \"\\\": \" + spillper);\n      }\n      if ((sortmb \u0026 0x7FF) !\u003d sortmb) {\n        throw new IOException(\n            \"Invalid \\\"\" + JobContext.IO_SORT_MB + \"\\\": \" + sortmb);\n      }\n      sorter \u003d ReflectionUtils.newInstance(job.getClass(\"map.sort.class\",\n            QuickSort.class, IndexedSorter.class), job);\n      // buffers and accounting\n      int maxMemUsage \u003d sortmb \u003c\u003c 20;\n      maxMemUsage -\u003d maxMemUsage % METASIZE;\n      kvbuffer \u003d new byte[maxMemUsage];\n      bufvoid \u003d kvbuffer.length;\n      kvmeta \u003d ByteBuffer.wrap(kvbuffer)\n         .order(ByteOrder.nativeOrder())\n         .asIntBuffer();\n      setEquator(0);\n      bufstart \u003d bufend \u003d bufindex \u003d equator;\n      kvstart \u003d kvend \u003d kvindex;\n\n      maxRec \u003d kvmeta.capacity() / NMETA;\n      softLimit \u003d (int)(kvbuffer.length * spillper);\n      bufferRemaining \u003d softLimit;\n      LOG.info(JobContext.IO_SORT_MB + \": \" + sortmb);\n      LOG.info(\"soft limit at \" + softLimit);\n      LOG.info(\"bufstart \u003d \" + bufstart + \"; bufvoid \u003d \" + bufvoid);\n      LOG.info(\"kvstart \u003d \" + kvstart + \"; length \u003d \" + maxRec);\n\n      // k/v serialization\n      comparator \u003d job.getOutputKeyComparator();\n      keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n      valClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n      serializationFactory \u003d new SerializationFactory(job);\n      keySerializer \u003d serializationFactory.getSerializer(keyClass);\n      keySerializer.open(bb);\n      valSerializer \u003d serializationFactory.getSerializer(valClass);\n      valSerializer.open(bb);\n\n      // output counters\n      mapOutputByteCounter \u003d reporter.getCounter(TaskCounter.MAP_OUTPUT_BYTES);\n      mapOutputRecordCounter \u003d\n        reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);\n      fileOutputByteCounter \u003d reporter\n          .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);\n\n      // compression\n      if (job.getCompressMapOutput()) {\n        Class\u003c? extends CompressionCodec\u003e codecClass \u003d\n          job.getMapOutputCompressorClass(DefaultCodec.class);\n        codec \u003d ReflectionUtils.newInstance(codecClass, job);\n      } else {\n        codec \u003d null;\n      }\n\n      // combiner\n      final Counters.Counter combineInputCounter \u003d\n        reporter.getCounter(TaskCounter.COMBINE_INPUT_RECORDS);\n      combinerRunner \u003d CombinerRunner.create(job, getTaskID(), \n                                             combineInputCounter,\n                                             reporter, null);\n      if (combinerRunner !\u003d null) {\n        final Counters.Counter combineOutputCounter \u003d\n          reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);\n        combineCollector\u003d new CombineOutputCollector\u003cK,V\u003e(combineOutputCounter, reporter, job);\n      } else {\n        combineCollector \u003d null;\n      }\n      spillInProgress \u003d false;\n      minSpillsForCombine \u003d job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);\n      spillThread.setDaemon(true);\n      spillThread.setName(\"SpillThread\");\n      spillLock.lock();\n      try {\n        spillThread.start();\n        while (!spillThreadRunning) {\n          spillDone.await();\n        }\n      } catch (InterruptedException e) {\n        throw new IOException(\"Spill thread failed to initialize\", e);\n      } finally {\n        spillLock.unlock();\n      }\n      if (sortSpillException !\u003d null) {\n        throw new IOException(\"Spill thread failed to initialize\",\n            sortSpillException);\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "8329fae686cf7a68679d177c25623311beec3384": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-4807. Allow MapOutputBuffer to be pluggable. (masokan via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1422345 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/12/12 12:23 PM",
      "commitName": "8329fae686cf7a68679d177c25623311beec3384",
      "commitAuthor": "Alejandro Abdelnur",
      "diff": "@@ -0,0 +1,108 @@\n+    public void init(MapOutputCollector.Context context\n+                    ) throws IOException, ClassNotFoundException {\n+      job \u003d context.getJobConf();\n+      reporter \u003d context.getReporter();\n+      mapTask \u003d context.getMapTask();\n+      mapOutputFile \u003d mapTask.getMapOutputFile();\n+      sortPhase \u003d mapTask.getSortPhase();\n+      spilledRecordsCounter \u003d reporter.getCounter(TaskCounter.SPILLED_RECORDS);\n+      partitions \u003d job.getNumReduceTasks();\n+      rfs \u003d ((LocalFileSystem)FileSystem.getLocal(job)).getRaw();\n+\n+      //sanity checks\n+      final float spillper \u003d\n+        job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);\n+      final int sortmb \u003d job.getInt(JobContext.IO_SORT_MB, 100);\n+      indexCacheMemoryLimit \u003d job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,\n+                                         INDEX_CACHE_MEMORY_LIMIT_DEFAULT);\n+      if (spillper \u003e (float)1.0 || spillper \u003c\u003d (float)0.0) {\n+        throw new IOException(\"Invalid \\\"\" + JobContext.MAP_SORT_SPILL_PERCENT +\n+            \"\\\": \" + spillper);\n+      }\n+      if ((sortmb \u0026 0x7FF) !\u003d sortmb) {\n+        throw new IOException(\n+            \"Invalid \\\"\" + JobContext.IO_SORT_MB + \"\\\": \" + sortmb);\n+      }\n+      sorter \u003d ReflectionUtils.newInstance(job.getClass(\"map.sort.class\",\n+            QuickSort.class, IndexedSorter.class), job);\n+      // buffers and accounting\n+      int maxMemUsage \u003d sortmb \u003c\u003c 20;\n+      maxMemUsage -\u003d maxMemUsage % METASIZE;\n+      kvbuffer \u003d new byte[maxMemUsage];\n+      bufvoid \u003d kvbuffer.length;\n+      kvmeta \u003d ByteBuffer.wrap(kvbuffer)\n+         .order(ByteOrder.nativeOrder())\n+         .asIntBuffer();\n+      setEquator(0);\n+      bufstart \u003d bufend \u003d bufindex \u003d equator;\n+      kvstart \u003d kvend \u003d kvindex;\n+\n+      maxRec \u003d kvmeta.capacity() / NMETA;\n+      softLimit \u003d (int)(kvbuffer.length * spillper);\n+      bufferRemaining \u003d softLimit;\n+      if (LOG.isInfoEnabled()) {\n+        LOG.info(JobContext.IO_SORT_MB + \": \" + sortmb);\n+        LOG.info(\"soft limit at \" + softLimit);\n+        LOG.info(\"bufstart \u003d \" + bufstart + \"; bufvoid \u003d \" + bufvoid);\n+        LOG.info(\"kvstart \u003d \" + kvstart + \"; length \u003d \" + maxRec);\n+      }\n+\n+      // k/v serialization\n+      comparator \u003d job.getOutputKeyComparator();\n+      keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n+      valClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n+      serializationFactory \u003d new SerializationFactory(job);\n+      keySerializer \u003d serializationFactory.getSerializer(keyClass);\n+      keySerializer.open(bb);\n+      valSerializer \u003d serializationFactory.getSerializer(valClass);\n+      valSerializer.open(bb);\n+\n+      // output counters\n+      mapOutputByteCounter \u003d reporter.getCounter(TaskCounter.MAP_OUTPUT_BYTES);\n+      mapOutputRecordCounter \u003d\n+        reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);\n+      fileOutputByteCounter \u003d reporter\n+          .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);\n+\n+      // compression\n+      if (job.getCompressMapOutput()) {\n+        Class\u003c? extends CompressionCodec\u003e codecClass \u003d\n+          job.getMapOutputCompressorClass(DefaultCodec.class);\n+        codec \u003d ReflectionUtils.newInstance(codecClass, job);\n+      } else {\n+        codec \u003d null;\n+      }\n+\n+      // combiner\n+      final Counters.Counter combineInputCounter \u003d\n+        reporter.getCounter(TaskCounter.COMBINE_INPUT_RECORDS);\n+      combinerRunner \u003d CombinerRunner.create(job, getTaskID(), \n+                                             combineInputCounter,\n+                                             reporter, null);\n+      if (combinerRunner !\u003d null) {\n+        final Counters.Counter combineOutputCounter \u003d\n+          reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);\n+        combineCollector\u003d new CombineOutputCollector\u003cK,V\u003e(combineOutputCounter, reporter, job);\n+      } else {\n+        combineCollector \u003d null;\n+      }\n+      spillInProgress \u003d false;\n+      minSpillsForCombine \u003d job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);\n+      spillThread.setDaemon(true);\n+      spillThread.setName(\"SpillThread\");\n+      spillLock.lock();\n+      try {\n+        spillThread.start();\n+        while (!spillThreadRunning) {\n+          spillDone.await();\n+        }\n+      } catch (InterruptedException e) {\n+        throw new IOException(\"Spill thread failed to initialize\", e);\n+      } finally {\n+        spillLock.unlock();\n+      }\n+      if (sortSpillException !\u003d null) {\n+        throw new IOException(\"Spill thread failed to initialize\",\n+            sortSpillException);\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public void init(MapOutputCollector.Context context\n                    ) throws IOException, ClassNotFoundException {\n      job \u003d context.getJobConf();\n      reporter \u003d context.getReporter();\n      mapTask \u003d context.getMapTask();\n      mapOutputFile \u003d mapTask.getMapOutputFile();\n      sortPhase \u003d mapTask.getSortPhase();\n      spilledRecordsCounter \u003d reporter.getCounter(TaskCounter.SPILLED_RECORDS);\n      partitions \u003d job.getNumReduceTasks();\n      rfs \u003d ((LocalFileSystem)FileSystem.getLocal(job)).getRaw();\n\n      //sanity checks\n      final float spillper \u003d\n        job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);\n      final int sortmb \u003d job.getInt(JobContext.IO_SORT_MB, 100);\n      indexCacheMemoryLimit \u003d job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,\n                                         INDEX_CACHE_MEMORY_LIMIT_DEFAULT);\n      if (spillper \u003e (float)1.0 || spillper \u003c\u003d (float)0.0) {\n        throw new IOException(\"Invalid \\\"\" + JobContext.MAP_SORT_SPILL_PERCENT +\n            \"\\\": \" + spillper);\n      }\n      if ((sortmb \u0026 0x7FF) !\u003d sortmb) {\n        throw new IOException(\n            \"Invalid \\\"\" + JobContext.IO_SORT_MB + \"\\\": \" + sortmb);\n      }\n      sorter \u003d ReflectionUtils.newInstance(job.getClass(\"map.sort.class\",\n            QuickSort.class, IndexedSorter.class), job);\n      // buffers and accounting\n      int maxMemUsage \u003d sortmb \u003c\u003c 20;\n      maxMemUsage -\u003d maxMemUsage % METASIZE;\n      kvbuffer \u003d new byte[maxMemUsage];\n      bufvoid \u003d kvbuffer.length;\n      kvmeta \u003d ByteBuffer.wrap(kvbuffer)\n         .order(ByteOrder.nativeOrder())\n         .asIntBuffer();\n      setEquator(0);\n      bufstart \u003d bufend \u003d bufindex \u003d equator;\n      kvstart \u003d kvend \u003d kvindex;\n\n      maxRec \u003d kvmeta.capacity() / NMETA;\n      softLimit \u003d (int)(kvbuffer.length * spillper);\n      bufferRemaining \u003d softLimit;\n      if (LOG.isInfoEnabled()) {\n        LOG.info(JobContext.IO_SORT_MB + \": \" + sortmb);\n        LOG.info(\"soft limit at \" + softLimit);\n        LOG.info(\"bufstart \u003d \" + bufstart + \"; bufvoid \u003d \" + bufvoid);\n        LOG.info(\"kvstart \u003d \" + kvstart + \"; length \u003d \" + maxRec);\n      }\n\n      // k/v serialization\n      comparator \u003d job.getOutputKeyComparator();\n      keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n      valClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n      serializationFactory \u003d new SerializationFactory(job);\n      keySerializer \u003d serializationFactory.getSerializer(keyClass);\n      keySerializer.open(bb);\n      valSerializer \u003d serializationFactory.getSerializer(valClass);\n      valSerializer.open(bb);\n\n      // output counters\n      mapOutputByteCounter \u003d reporter.getCounter(TaskCounter.MAP_OUTPUT_BYTES);\n      mapOutputRecordCounter \u003d\n        reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);\n      fileOutputByteCounter \u003d reporter\n          .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);\n\n      // compression\n      if (job.getCompressMapOutput()) {\n        Class\u003c? extends CompressionCodec\u003e codecClass \u003d\n          job.getMapOutputCompressorClass(DefaultCodec.class);\n        codec \u003d ReflectionUtils.newInstance(codecClass, job);\n      } else {\n        codec \u003d null;\n      }\n\n      // combiner\n      final Counters.Counter combineInputCounter \u003d\n        reporter.getCounter(TaskCounter.COMBINE_INPUT_RECORDS);\n      combinerRunner \u003d CombinerRunner.create(job, getTaskID(), \n                                             combineInputCounter,\n                                             reporter, null);\n      if (combinerRunner !\u003d null) {\n        final Counters.Counter combineOutputCounter \u003d\n          reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);\n        combineCollector\u003d new CombineOutputCollector\u003cK,V\u003e(combineOutputCounter, reporter, job);\n      } else {\n        combineCollector \u003d null;\n      }\n      spillInProgress \u003d false;\n      minSpillsForCombine \u003d job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);\n      spillThread.setDaemon(true);\n      spillThread.setName(\"SpillThread\");\n      spillLock.lock();\n      try {\n        spillThread.start();\n        while (!spillThreadRunning) {\n          spillDone.await();\n        }\n      } catch (InterruptedException e) {\n        throw new IOException(\"Spill thread failed to initialize\", e);\n      } finally {\n        spillLock.unlock();\n      }\n      if (sortSpillException !\u003d null) {\n        throw new IOException(\"Spill thread failed to initialize\",\n            sortSpillException);\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java"
    }
  }
}