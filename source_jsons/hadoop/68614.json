{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StreamJob.java",
  "functionName": "setJobConf",
  "functionId": "setJobConf",
  "sourceFilePath": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
  "functionStartLine": 733,
  "functionEndLine": 982,
  "numCommitsSeen": 22,
  "timeTaken": 4923,
  "changeHistory": [
    "806073867ead9492da09ad95596e8e11606ee5c3",
    "735b50e8bd23f7fbeff3a08cf8f3fff8cbff7449",
    "26447229ba2c3d43db978c1b3ce95613669182ee",
    "3e4efbb609ff25036aa6b04dc379b6db72e5a31f",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "806073867ead9492da09ad95596e8e11606ee5c3": "Ybodychange",
    "735b50e8bd23f7fbeff3a08cf8f3fff8cbff7449": "Ybodychange",
    "26447229ba2c3d43db978c1b3ce95613669182ee": "Yfilerename",
    "3e4efbb609ff25036aa6b04dc379b6db72e5a31f": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "806073867ead9492da09ad95596e8e11606ee5c3": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5113. Streaming input/output types are ignored with java mapper/reducer. (sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1463307 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/13 2:42 PM",
      "commitName": "806073867ead9492da09ad95596e8e11606ee5c3",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "21/12/12 3:05 PM",
      "commitNameOld": "40c3b7f0b2d0e5bfcdc748bddb3de6a8dd3648d4",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 100.94,
      "commitsBetweenForRepo": 452,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,250 +1,250 @@\n   protected void setJobConf() throws IOException {\n     if (additionalConfSpec_ !\u003d null) {\n       LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n       config_.addResource(new Path(additionalConfSpec_));\n     }\n \n     // general MapRed job properties\n     jobConf_ \u003d new JobConf(config_, StreamJob.class);\n \n     // All streaming jobs get the task timeout value\n     // from the configuration settings.\n \n     // The correct FS must be set before this is called!\n     // (to resolve local vs. dfs drive letter differences)\n     // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n     for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n       FileInputFormat.addInputPaths(jobConf_,\n                         (String) inputSpecs_.get(i));\n     }\n \n     String defaultPackage \u003d this.getClass().getPackage().getName();\n     Class c;\n     Class fmt \u003d null;\n     if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n       fmt \u003d TextInputFormat.class;\n     } else if (inputFormatSpec_ !\u003d null) {\n       if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n           || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n           || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n         fmt \u003d TextInputFormat.class;\n       } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n           .getName())\n           || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n               .getCanonicalName())\n           || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n         if (inReaderSpec_ \u003d\u003d null) {\n           fmt \u003d KeyValueTextInputFormat.class;\n         }\n       } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n           .getName())\n           || inputFormatSpec_\n               .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n                   .getCanonicalName())\n           || inputFormatSpec_\n               .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n         if (inReaderSpec_ \u003d\u003d null) {\n           fmt \u003d SequenceFileInputFormat.class;\n         }\n       } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n           .getName())\n           || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n               .getCanonicalName())\n           || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n         fmt \u003d SequenceFileAsTextInputFormat.class;\n       } else {\n         c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n         if (c !\u003d null) {\n           fmt \u003d c;\n         } else {\n           fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n         }\n       }\n     }\n     if (fmt \u003d\u003d null) {\n       fmt \u003d StreamInputFormat.class;\n     }\n \n     jobConf_.setInputFormat(fmt);\n \n     if (ioSpec_ !\u003d null) {\n       jobConf_.set(\"stream.map.input\", ioSpec_);\n       jobConf_.set(\"stream.map.output\", ioSpec_);\n       jobConf_.set(\"stream.reduce.input\", ioSpec_);\n       jobConf_.set(\"stream.reduce.output\", ioSpec_);\n     }\n \n     Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d\n       jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n         IdentifierResolver.class, IdentifierResolver.class);\n     IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n \n     idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.map.input.writer.class\",\n       idResolver.getInputWriterClass(), InputWriter.class);\n \n     idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.reduce.input.writer.class\",\n       idResolver.getInputWriterClass(), InputWriter.class);\n \n     jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n \n     boolean isMapperACommand \u003d false;\n     if (mapCmd_ !\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setMapperClass(c);\n       } else {\n         isMapperACommand \u003d true;\n         jobConf_.setMapperClass(PipeMapper.class);\n         jobConf_.setMapRunnerClass(PipeMapRunner.class);\n         jobConf_.set(\"stream.map.streamprocessor\",\n                      URLEncoder.encode(mapCmd_, \"UTF-8\"));\n       }\n     }\n \n     if (comCmd_ !\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setCombinerClass(c);\n       } else {\n         jobConf_.setCombinerClass(PipeCombiner.class);\n         jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n                 comCmd_, \"UTF-8\"));\n       }\n     }\n \n     if (numReduceTasksSpec_!\u003d null) {\n       int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n       jobConf_.setNumReduceTasks(numReduceTasks);\n     }\n \n     boolean isReducerACommand \u003d false;\n     if (redCmd_ !\u003d null) {\n       if (redCmd_.equals(REDUCE_NONE)) {\n         jobConf_.setNumReduceTasks(0);\n       }\n       if (jobConf_.getNumReduceTasks() !\u003d 0) {\n         if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n           jobConf_.setReducerClass(ValueAggregatorReducer.class);\n           jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n         } else {\n \n           c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n           if (c !\u003d null) {\n             jobConf_.setReducerClass(c);\n           } else {\n             isReducerACommand \u003d true;\n             jobConf_.setReducerClass(PipeReducer.class);\n             jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n                 redCmd_, \"UTF-8\"));\n           }\n         }\n       }\n     }\n \n     idResolver.resolve(jobConf_.get(\"stream.map.output\",\n         IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.map.output.reader.class\",\n       idResolver.getOutputReaderClass(), OutputReader.class);\n-    if (isMapperACommand) {\n+    if (isMapperACommand || jobConf_.get(\"stream.map.output\") !\u003d null) {\n       // if mapper is a command, then map output key/value classes come from the\n       // idResolver\n       jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n       jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n \n       if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n         jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n         jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n       }\n     }\n \n     idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n         IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.reduce.output.reader.class\",\n       idResolver.getOutputReaderClass(), OutputReader.class);\n-    if (isReducerACommand) {\n+    if (isReducerACommand || jobConf_.get(\"stream.reduce.output\") !\u003d null) {\n       // if reducer is a command, then output key/value classes come from the\n       // idResolver\n       jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n       jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n     }\n \n     if (inReaderSpec_ !\u003d null) {\n       String[] args \u003d inReaderSpec_.split(\",\");\n       String readerClass \u003d args[0];\n       // this argument can only be a Java class\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.set(\"stream.recordreader.class\", c.getName());\n       } else {\n         fail(\"-inputreader: class not found: \" + readerClass);\n       }\n       for (int i \u003d 1; i \u003c args.length; i++) {\n         String[] nv \u003d args[i].split(\"\u003d\", 2);\n         String k \u003d \"stream.recordreader.\" + nv[0];\n         String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n         jobConf_.set(k, v);\n       }\n     }\n \n     FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n     fmt \u003d null;\n     if (outputFormatSpec_!\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n       if (c !\u003d null) {\n         fmt \u003d c;\n       } else {\n         fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n       }\n     }\n     if (fmt \u003d\u003d null) {\n       fmt \u003d TextOutputFormat.class;\n     }\n     if (lazyOutput_) {\n       LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n     } else {\n       jobConf_.setOutputFormat(fmt);\n     }\n \n     if (partitionerSpec_!\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setPartitionerClass(c);\n       } else {\n         fail(\"-partitioner : class not found : \" + partitionerSpec_);\n       }\n     }\n \n     if(mapDebugSpec_ !\u003d null){\n     \tjobConf_.setMapDebugScript(mapDebugSpec_);\n     }\n     if(reduceDebugSpec_ !\u003d null){\n     \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n     }\n     // last, allow user to override anything\n     // (although typically used with properties we didn\u0027t touch)\n \n     jar_ \u003d packageJobJar();\n     if (jar_ !\u003d null) {\n       jobConf_.setJar(jar_);\n     }\n \n     if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n       getURIs(cacheArchives, cacheFiles);\n       boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n       if (!b)\n         fail(LINK_URI);\n     }\n     // set the jobconf for the caching parameters\n     if (cacheArchives !\u003d null)\n       DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n     if (cacheFiles !\u003d null)\n       DistributedCache.setCacheFiles(fileURIs, jobConf_);\n \n     if (verbose_) {\n       listJobConfProperties();\n     }\n \n     msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void setJobConf() throws IOException {\n    if (additionalConfSpec_ !\u003d null) {\n      LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n      config_.addResource(new Path(additionalConfSpec_));\n    }\n\n    // general MapRed job properties\n    jobConf_ \u003d new JobConf(config_, StreamJob.class);\n\n    // All streaming jobs get the task timeout value\n    // from the configuration settings.\n\n    // The correct FS must be set before this is called!\n    // (to resolve local vs. dfs drive letter differences)\n    // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n    for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n      FileInputFormat.addInputPaths(jobConf_,\n                        (String) inputSpecs_.get(i));\n    }\n\n    String defaultPackage \u003d this.getClass().getPackage().getName();\n    Class c;\n    Class fmt \u003d null;\n    if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n      fmt \u003d TextInputFormat.class;\n    } else if (inputFormatSpec_ !\u003d null) {\n      if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n        fmt \u003d TextInputFormat.class;\n      } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d KeyValueTextInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n          .getName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n                  .getCanonicalName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d SequenceFileInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n        fmt \u003d SequenceFileAsTextInputFormat.class;\n      } else {\n        c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n        if (c !\u003d null) {\n          fmt \u003d c;\n        } else {\n          fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n        }\n      }\n    }\n    if (fmt \u003d\u003d null) {\n      fmt \u003d StreamInputFormat.class;\n    }\n\n    jobConf_.setInputFormat(fmt);\n\n    if (ioSpec_ !\u003d null) {\n      jobConf_.set(\"stream.map.input\", ioSpec_);\n      jobConf_.set(\"stream.map.output\", ioSpec_);\n      jobConf_.set(\"stream.reduce.input\", ioSpec_);\n      jobConf_.set(\"stream.reduce.output\", ioSpec_);\n    }\n\n    Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d\n      jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n        IdentifierResolver.class, IdentifierResolver.class);\n    IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n\n    idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n\n    idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n\n    jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n\n    boolean isMapperACommand \u003d false;\n    if (mapCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setMapperClass(c);\n      } else {\n        isMapperACommand \u003d true;\n        jobConf_.setMapperClass(PipeMapper.class);\n        jobConf_.setMapRunnerClass(PipeMapRunner.class);\n        jobConf_.set(\"stream.map.streamprocessor\",\n                     URLEncoder.encode(mapCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (comCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setCombinerClass(c);\n      } else {\n        jobConf_.setCombinerClass(PipeCombiner.class);\n        jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n                comCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (numReduceTasksSpec_!\u003d null) {\n      int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n      jobConf_.setNumReduceTasks(numReduceTasks);\n    }\n\n    boolean isReducerACommand \u003d false;\n    if (redCmd_ !\u003d null) {\n      if (redCmd_.equals(REDUCE_NONE)) {\n        jobConf_.setNumReduceTasks(0);\n      }\n      if (jobConf_.getNumReduceTasks() !\u003d 0) {\n        if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n          jobConf_.setReducerClass(ValueAggregatorReducer.class);\n          jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n        } else {\n\n          c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n          if (c !\u003d null) {\n            jobConf_.setReducerClass(c);\n          } else {\n            isReducerACommand \u003d true;\n            jobConf_.setReducerClass(PipeReducer.class);\n            jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n                redCmd_, \"UTF-8\"));\n          }\n        }\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.map.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isMapperACommand || jobConf_.get(\"stream.map.output\") !\u003d null) {\n      // if mapper is a command, then map output key/value classes come from the\n      // idResolver\n      jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n\n      if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n        jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n        jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isReducerACommand || jobConf_.get(\"stream.reduce.output\") !\u003d null) {\n      // if reducer is a command, then output key/value classes come from the\n      // idResolver\n      jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n    }\n\n    if (inReaderSpec_ !\u003d null) {\n      String[] args \u003d inReaderSpec_.split(\",\");\n      String readerClass \u003d args[0];\n      // this argument can only be a Java class\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.set(\"stream.recordreader.class\", c.getName());\n      } else {\n        fail(\"-inputreader: class not found: \" + readerClass);\n      }\n      for (int i \u003d 1; i \u003c args.length; i++) {\n        String[] nv \u003d args[i].split(\"\u003d\", 2);\n        String k \u003d \"stream.recordreader.\" + nv[0];\n        String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n        jobConf_.set(k, v);\n      }\n    }\n\n    FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n    fmt \u003d null;\n    if (outputFormatSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n      if (c !\u003d null) {\n        fmt \u003d c;\n      } else {\n        fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n      }\n    }\n    if (fmt \u003d\u003d null) {\n      fmt \u003d TextOutputFormat.class;\n    }\n    if (lazyOutput_) {\n      LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n    } else {\n      jobConf_.setOutputFormat(fmt);\n    }\n\n    if (partitionerSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setPartitionerClass(c);\n      } else {\n        fail(\"-partitioner : class not found : \" + partitionerSpec_);\n      }\n    }\n\n    if(mapDebugSpec_ !\u003d null){\n    \tjobConf_.setMapDebugScript(mapDebugSpec_);\n    }\n    if(reduceDebugSpec_ !\u003d null){\n    \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n    }\n    // last, allow user to override anything\n    // (although typically used with properties we didn\u0027t touch)\n\n    jar_ \u003d packageJobJar();\n    if (jar_ !\u003d null) {\n      jobConf_.setJar(jar_);\n    }\n\n    if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n      getURIs(cacheArchives, cacheFiles);\n      boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n      if (!b)\n        fail(LINK_URI);\n    }\n    // set the jobconf for the caching parameters\n    if (cacheArchives !\u003d null)\n      DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n    if (cacheFiles !\u003d null)\n      DistributedCache.setCacheFiles(fileURIs, jobConf_);\n\n    if (verbose_) {\n      listJobConfProperties();\n    }\n\n    msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n  }",
      "path": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {}
    },
    "735b50e8bd23f7fbeff3a08cf8f3fff8cbff7449": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4493. Distibuted Cache Compatability Issues (Robert Evans via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1367713 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/07/12 12:20 PM",
      "commitName": "735b50e8bd23f7fbeff3a08cf8f3fff8cbff7449",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "08/05/12 6:20 AM",
      "commitNameOld": "a9808de0d9a73a99c10a3e4290ec20778fed4f24",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 84.25,
      "commitsBetweenForRepo": 440,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,251 +1,250 @@\n   protected void setJobConf() throws IOException {\n     if (additionalConfSpec_ !\u003d null) {\n       LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n       config_.addResource(new Path(additionalConfSpec_));\n     }\n \n     // general MapRed job properties\n     jobConf_ \u003d new JobConf(config_, StreamJob.class);\n \n     // All streaming jobs get the task timeout value\n     // from the configuration settings.\n \n     // The correct FS must be set before this is called!\n     // (to resolve local vs. dfs drive letter differences)\n     // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n     for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n       FileInputFormat.addInputPaths(jobConf_,\n                         (String) inputSpecs_.get(i));\n     }\n \n     String defaultPackage \u003d this.getClass().getPackage().getName();\n     Class c;\n     Class fmt \u003d null;\n     if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n       fmt \u003d TextInputFormat.class;\n     } else if (inputFormatSpec_ !\u003d null) {\n       if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n           || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n           || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n         fmt \u003d TextInputFormat.class;\n       } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n           .getName())\n           || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n               .getCanonicalName())\n           || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n         if (inReaderSpec_ \u003d\u003d null) {\n           fmt \u003d KeyValueTextInputFormat.class;\n         }\n       } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n           .getName())\n           || inputFormatSpec_\n               .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n                   .getCanonicalName())\n           || inputFormatSpec_\n               .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n         if (inReaderSpec_ \u003d\u003d null) {\n           fmt \u003d SequenceFileInputFormat.class;\n         }\n       } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n           .getName())\n           || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n               .getCanonicalName())\n           || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n         fmt \u003d SequenceFileAsTextInputFormat.class;\n       } else {\n         c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n         if (c !\u003d null) {\n           fmt \u003d c;\n         } else {\n           fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n         }\n       }\n     }\n     if (fmt \u003d\u003d null) {\n       fmt \u003d StreamInputFormat.class;\n     }\n \n     jobConf_.setInputFormat(fmt);\n \n     if (ioSpec_ !\u003d null) {\n       jobConf_.set(\"stream.map.input\", ioSpec_);\n       jobConf_.set(\"stream.map.output\", ioSpec_);\n       jobConf_.set(\"stream.reduce.input\", ioSpec_);\n       jobConf_.set(\"stream.reduce.output\", ioSpec_);\n     }\n \n     Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d\n       jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n         IdentifierResolver.class, IdentifierResolver.class);\n     IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n \n     idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.map.input.writer.class\",\n       idResolver.getInputWriterClass(), InputWriter.class);\n \n     idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.reduce.input.writer.class\",\n       idResolver.getInputWriterClass(), InputWriter.class);\n \n     jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n \n     boolean isMapperACommand \u003d false;\n     if (mapCmd_ !\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setMapperClass(c);\n       } else {\n         isMapperACommand \u003d true;\n         jobConf_.setMapperClass(PipeMapper.class);\n         jobConf_.setMapRunnerClass(PipeMapRunner.class);\n         jobConf_.set(\"stream.map.streamprocessor\",\n                      URLEncoder.encode(mapCmd_, \"UTF-8\"));\n       }\n     }\n \n     if (comCmd_ !\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setCombinerClass(c);\n       } else {\n         jobConf_.setCombinerClass(PipeCombiner.class);\n         jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n                 comCmd_, \"UTF-8\"));\n       }\n     }\n \n     if (numReduceTasksSpec_!\u003d null) {\n       int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n       jobConf_.setNumReduceTasks(numReduceTasks);\n     }\n \n     boolean isReducerACommand \u003d false;\n     if (redCmd_ !\u003d null) {\n       if (redCmd_.equals(REDUCE_NONE)) {\n         jobConf_.setNumReduceTasks(0);\n       }\n       if (jobConf_.getNumReduceTasks() !\u003d 0) {\n         if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n           jobConf_.setReducerClass(ValueAggregatorReducer.class);\n           jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n         } else {\n \n           c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n           if (c !\u003d null) {\n             jobConf_.setReducerClass(c);\n           } else {\n             isReducerACommand \u003d true;\n             jobConf_.setReducerClass(PipeReducer.class);\n             jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n                 redCmd_, \"UTF-8\"));\n           }\n         }\n       }\n     }\n \n     idResolver.resolve(jobConf_.get(\"stream.map.output\",\n         IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.map.output.reader.class\",\n       idResolver.getOutputReaderClass(), OutputReader.class);\n     if (isMapperACommand) {\n       // if mapper is a command, then map output key/value classes come from the\n       // idResolver\n       jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n       jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n \n       if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n         jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n         jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n       }\n     }\n \n     idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n         IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.reduce.output.reader.class\",\n       idResolver.getOutputReaderClass(), OutputReader.class);\n     if (isReducerACommand) {\n       // if reducer is a command, then output key/value classes come from the\n       // idResolver\n       jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n       jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n     }\n \n     if (inReaderSpec_ !\u003d null) {\n       String[] args \u003d inReaderSpec_.split(\",\");\n       String readerClass \u003d args[0];\n       // this argument can only be a Java class\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.set(\"stream.recordreader.class\", c.getName());\n       } else {\n         fail(\"-inputreader: class not found: \" + readerClass);\n       }\n       for (int i \u003d 1; i \u003c args.length; i++) {\n         String[] nv \u003d args[i].split(\"\u003d\", 2);\n         String k \u003d \"stream.recordreader.\" + nv[0];\n         String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n         jobConf_.set(k, v);\n       }\n     }\n \n     FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n     fmt \u003d null;\n     if (outputFormatSpec_!\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n       if (c !\u003d null) {\n         fmt \u003d c;\n       } else {\n         fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n       }\n     }\n     if (fmt \u003d\u003d null) {\n       fmt \u003d TextOutputFormat.class;\n     }\n     if (lazyOutput_) {\n       LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n     } else {\n       jobConf_.setOutputFormat(fmt);\n     }\n \n     if (partitionerSpec_!\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setPartitionerClass(c);\n       } else {\n         fail(\"-partitioner : class not found : \" + partitionerSpec_);\n       }\n     }\n \n     if(mapDebugSpec_ !\u003d null){\n     \tjobConf_.setMapDebugScript(mapDebugSpec_);\n     }\n     if(reduceDebugSpec_ !\u003d null){\n     \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n     }\n     // last, allow user to override anything\n     // (although typically used with properties we didn\u0027t touch)\n \n     jar_ \u003d packageJobJar();\n     if (jar_ !\u003d null) {\n       jobConf_.setJar(jar_);\n     }\n \n     if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n       getURIs(cacheArchives, cacheFiles);\n       boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n       if (!b)\n         fail(LINK_URI);\n     }\n-    DistributedCache.createSymlink(jobConf_);\n     // set the jobconf for the caching parameters\n     if (cacheArchives !\u003d null)\n       DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n     if (cacheFiles !\u003d null)\n       DistributedCache.setCacheFiles(fileURIs, jobConf_);\n \n     if (verbose_) {\n       listJobConfProperties();\n     }\n \n     msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void setJobConf() throws IOException {\n    if (additionalConfSpec_ !\u003d null) {\n      LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n      config_.addResource(new Path(additionalConfSpec_));\n    }\n\n    // general MapRed job properties\n    jobConf_ \u003d new JobConf(config_, StreamJob.class);\n\n    // All streaming jobs get the task timeout value\n    // from the configuration settings.\n\n    // The correct FS must be set before this is called!\n    // (to resolve local vs. dfs drive letter differences)\n    // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n    for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n      FileInputFormat.addInputPaths(jobConf_,\n                        (String) inputSpecs_.get(i));\n    }\n\n    String defaultPackage \u003d this.getClass().getPackage().getName();\n    Class c;\n    Class fmt \u003d null;\n    if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n      fmt \u003d TextInputFormat.class;\n    } else if (inputFormatSpec_ !\u003d null) {\n      if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n        fmt \u003d TextInputFormat.class;\n      } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d KeyValueTextInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n          .getName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n                  .getCanonicalName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d SequenceFileInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n        fmt \u003d SequenceFileAsTextInputFormat.class;\n      } else {\n        c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n        if (c !\u003d null) {\n          fmt \u003d c;\n        } else {\n          fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n        }\n      }\n    }\n    if (fmt \u003d\u003d null) {\n      fmt \u003d StreamInputFormat.class;\n    }\n\n    jobConf_.setInputFormat(fmt);\n\n    if (ioSpec_ !\u003d null) {\n      jobConf_.set(\"stream.map.input\", ioSpec_);\n      jobConf_.set(\"stream.map.output\", ioSpec_);\n      jobConf_.set(\"stream.reduce.input\", ioSpec_);\n      jobConf_.set(\"stream.reduce.output\", ioSpec_);\n    }\n\n    Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d\n      jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n        IdentifierResolver.class, IdentifierResolver.class);\n    IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n\n    idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n\n    idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n\n    jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n\n    boolean isMapperACommand \u003d false;\n    if (mapCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setMapperClass(c);\n      } else {\n        isMapperACommand \u003d true;\n        jobConf_.setMapperClass(PipeMapper.class);\n        jobConf_.setMapRunnerClass(PipeMapRunner.class);\n        jobConf_.set(\"stream.map.streamprocessor\",\n                     URLEncoder.encode(mapCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (comCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setCombinerClass(c);\n      } else {\n        jobConf_.setCombinerClass(PipeCombiner.class);\n        jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n                comCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (numReduceTasksSpec_!\u003d null) {\n      int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n      jobConf_.setNumReduceTasks(numReduceTasks);\n    }\n\n    boolean isReducerACommand \u003d false;\n    if (redCmd_ !\u003d null) {\n      if (redCmd_.equals(REDUCE_NONE)) {\n        jobConf_.setNumReduceTasks(0);\n      }\n      if (jobConf_.getNumReduceTasks() !\u003d 0) {\n        if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n          jobConf_.setReducerClass(ValueAggregatorReducer.class);\n          jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n        } else {\n\n          c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n          if (c !\u003d null) {\n            jobConf_.setReducerClass(c);\n          } else {\n            isReducerACommand \u003d true;\n            jobConf_.setReducerClass(PipeReducer.class);\n            jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n                redCmd_, \"UTF-8\"));\n          }\n        }\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.map.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isMapperACommand) {\n      // if mapper is a command, then map output key/value classes come from the\n      // idResolver\n      jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n\n      if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n        jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n        jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isReducerACommand) {\n      // if reducer is a command, then output key/value classes come from the\n      // idResolver\n      jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n    }\n\n    if (inReaderSpec_ !\u003d null) {\n      String[] args \u003d inReaderSpec_.split(\",\");\n      String readerClass \u003d args[0];\n      // this argument can only be a Java class\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.set(\"stream.recordreader.class\", c.getName());\n      } else {\n        fail(\"-inputreader: class not found: \" + readerClass);\n      }\n      for (int i \u003d 1; i \u003c args.length; i++) {\n        String[] nv \u003d args[i].split(\"\u003d\", 2);\n        String k \u003d \"stream.recordreader.\" + nv[0];\n        String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n        jobConf_.set(k, v);\n      }\n    }\n\n    FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n    fmt \u003d null;\n    if (outputFormatSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n      if (c !\u003d null) {\n        fmt \u003d c;\n      } else {\n        fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n      }\n    }\n    if (fmt \u003d\u003d null) {\n      fmt \u003d TextOutputFormat.class;\n    }\n    if (lazyOutput_) {\n      LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n    } else {\n      jobConf_.setOutputFormat(fmt);\n    }\n\n    if (partitionerSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setPartitionerClass(c);\n      } else {\n        fail(\"-partitioner : class not found : \" + partitionerSpec_);\n      }\n    }\n\n    if(mapDebugSpec_ !\u003d null){\n    \tjobConf_.setMapDebugScript(mapDebugSpec_);\n    }\n    if(reduceDebugSpec_ !\u003d null){\n    \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n    }\n    // last, allow user to override anything\n    // (although typically used with properties we didn\u0027t touch)\n\n    jar_ \u003d packageJobJar();\n    if (jar_ !\u003d null) {\n      jobConf_.setJar(jar_);\n    }\n\n    if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n      getURIs(cacheArchives, cacheFiles);\n      boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n      if (!b)\n        fail(LINK_URI);\n    }\n    // set the jobconf for the caching parameters\n    if (cacheArchives !\u003d null)\n      DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n    if (cacheFiles !\u003d null)\n      DistributedCache.setCacheFiles(fileURIs, jobConf_);\n\n    if (verbose_) {\n      listJobConfProperties();\n    }\n\n    msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n  }",
      "path": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {}
    },
    "26447229ba2c3d43db978c1b3ce95613669182ee": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7590. Mavenize streaming and MR examples. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203941 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 5:24 PM",
      "commitName": "26447229ba2c3d43db978c1b3ce95613669182ee",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "18/11/11 1:04 AM",
      "commitNameOld": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.68,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,251 +1,251 @@\n   protected void setJobConf() throws IOException {\n     if (additionalConfSpec_ !\u003d null) {\n       LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n       config_.addResource(new Path(additionalConfSpec_));\n     }\n \n     // general MapRed job properties\n     jobConf_ \u003d new JobConf(config_, StreamJob.class);\n-    \n+\n     // All streaming jobs get the task timeout value\n     // from the configuration settings.\n \n     // The correct FS must be set before this is called!\n-    // (to resolve local vs. dfs drive letter differences) \n+    // (to resolve local vs. dfs drive letter differences)\n     // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n     for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n-      FileInputFormat.addInputPaths(jobConf_, \n+      FileInputFormat.addInputPaths(jobConf_,\n                         (String) inputSpecs_.get(i));\n     }\n \n     String defaultPackage \u003d this.getClass().getPackage().getName();\n     Class c;\n     Class fmt \u003d null;\n     if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n       fmt \u003d TextInputFormat.class;\n     } else if (inputFormatSpec_ !\u003d null) {\n       if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n           || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n           || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n         fmt \u003d TextInputFormat.class;\n       } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n           .getName())\n           || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n               .getCanonicalName())\n           || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n         if (inReaderSpec_ \u003d\u003d null) {\n           fmt \u003d KeyValueTextInputFormat.class;\n         }\n       } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n           .getName())\n           || inputFormatSpec_\n               .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n                   .getCanonicalName())\n           || inputFormatSpec_\n               .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n         if (inReaderSpec_ \u003d\u003d null) {\n           fmt \u003d SequenceFileInputFormat.class;\n         }\n       } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n           .getName())\n           || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n               .getCanonicalName())\n           || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n         fmt \u003d SequenceFileAsTextInputFormat.class;\n       } else {\n         c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n         if (c !\u003d null) {\n           fmt \u003d c;\n         } else {\n           fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n         }\n       }\n-    } \n+    }\n     if (fmt \u003d\u003d null) {\n       fmt \u003d StreamInputFormat.class;\n     }\n \n     jobConf_.setInputFormat(fmt);\n \n     if (ioSpec_ !\u003d null) {\n       jobConf_.set(\"stream.map.input\", ioSpec_);\n       jobConf_.set(\"stream.map.output\", ioSpec_);\n       jobConf_.set(\"stream.reduce.input\", ioSpec_);\n       jobConf_.set(\"stream.reduce.output\", ioSpec_);\n     }\n-    \n-    Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d \n+\n+    Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d\n       jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n         IdentifierResolver.class, IdentifierResolver.class);\n     IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n-    \n+\n     idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.map.input.writer.class\",\n       idResolver.getInputWriterClass(), InputWriter.class);\n-    \n+\n     idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.reduce.input.writer.class\",\n       idResolver.getInputWriterClass(), InputWriter.class);\n-    \n+\n     jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n \n     boolean isMapperACommand \u003d false;\n     if (mapCmd_ !\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setMapperClass(c);\n       } else {\n         isMapperACommand \u003d true;\n         jobConf_.setMapperClass(PipeMapper.class);\n         jobConf_.setMapRunnerClass(PipeMapRunner.class);\n-        jobConf_.set(\"stream.map.streamprocessor\", \n+        jobConf_.set(\"stream.map.streamprocessor\",\n                      URLEncoder.encode(mapCmd_, \"UTF-8\"));\n       }\n     }\n \n     if (comCmd_ !\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setCombinerClass(c);\n       } else {\n         jobConf_.setCombinerClass(PipeCombiner.class);\n         jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n                 comCmd_, \"UTF-8\"));\n       }\n     }\n \n     if (numReduceTasksSpec_!\u003d null) {\n       int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n       jobConf_.setNumReduceTasks(numReduceTasks);\n     }\n \n     boolean isReducerACommand \u003d false;\n     if (redCmd_ !\u003d null) {\n       if (redCmd_.equals(REDUCE_NONE)) {\n         jobConf_.setNumReduceTasks(0);\n       }\n       if (jobConf_.getNumReduceTasks() !\u003d 0) {\n         if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n           jobConf_.setReducerClass(ValueAggregatorReducer.class);\n           jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n         } else {\n \n           c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n           if (c !\u003d null) {\n             jobConf_.setReducerClass(c);\n           } else {\n             isReducerACommand \u003d true;\n             jobConf_.setReducerClass(PipeReducer.class);\n             jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n                 redCmd_, \"UTF-8\"));\n           }\n         }\n       }\n     }\n \n     idResolver.resolve(jobConf_.get(\"stream.map.output\",\n         IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.map.output.reader.class\",\n       idResolver.getOutputReaderClass(), OutputReader.class);\n     if (isMapperACommand) {\n       // if mapper is a command, then map output key/value classes come from the\n       // idResolver\n       jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n       jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n \n       if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n         jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n         jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n       }\n     }\n \n     idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n         IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.reduce.output.reader.class\",\n       idResolver.getOutputReaderClass(), OutputReader.class);\n     if (isReducerACommand) {\n       // if reducer is a command, then output key/value classes come from the\n       // idResolver\n       jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n       jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n     }\n \n     if (inReaderSpec_ !\u003d null) {\n       String[] args \u003d inReaderSpec_.split(\",\");\n       String readerClass \u003d args[0];\n       // this argument can only be a Java class\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.set(\"stream.recordreader.class\", c.getName());\n       } else {\n         fail(\"-inputreader: class not found: \" + readerClass);\n       }\n       for (int i \u003d 1; i \u003c args.length; i++) {\n         String[] nv \u003d args[i].split(\"\u003d\", 2);\n         String k \u003d \"stream.recordreader.\" + nv[0];\n         String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n         jobConf_.set(k, v);\n       }\n     }\n-    \n+\n     FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n     fmt \u003d null;\n     if (outputFormatSpec_!\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n       if (c !\u003d null) {\n         fmt \u003d c;\n       } else {\n         fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n       }\n     }\n     if (fmt \u003d\u003d null) {\n       fmt \u003d TextOutputFormat.class;\n     }\n     if (lazyOutput_) {\n       LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n     } else {\n       jobConf_.setOutputFormat(fmt);\n     }\n \n     if (partitionerSpec_!\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setPartitionerClass(c);\n       } else {\n         fail(\"-partitioner : class not found : \" + partitionerSpec_);\n       }\n     }\n-    \n+\n     if(mapDebugSpec_ !\u003d null){\n     \tjobConf_.setMapDebugScript(mapDebugSpec_);\n     }\n     if(reduceDebugSpec_ !\u003d null){\n     \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n     }\n     // last, allow user to override anything\n     // (although typically used with properties we didn\u0027t touch)\n \n     jar_ \u003d packageJobJar();\n     if (jar_ !\u003d null) {\n       jobConf_.setJar(jar_);\n     }\n-    \n+\n     if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n       getURIs(cacheArchives, cacheFiles);\n       boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n       if (!b)\n         fail(LINK_URI);\n     }\n     DistributedCache.createSymlink(jobConf_);\n     // set the jobconf for the caching parameters\n     if (cacheArchives !\u003d null)\n       DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n     if (cacheFiles !\u003d null)\n       DistributedCache.setCacheFiles(fileURIs, jobConf_);\n-    \n+\n     if (verbose_) {\n       listJobConfProperties();\n     }\n-   \n+\n     msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void setJobConf() throws IOException {\n    if (additionalConfSpec_ !\u003d null) {\n      LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n      config_.addResource(new Path(additionalConfSpec_));\n    }\n\n    // general MapRed job properties\n    jobConf_ \u003d new JobConf(config_, StreamJob.class);\n\n    // All streaming jobs get the task timeout value\n    // from the configuration settings.\n\n    // The correct FS must be set before this is called!\n    // (to resolve local vs. dfs drive letter differences)\n    // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n    for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n      FileInputFormat.addInputPaths(jobConf_,\n                        (String) inputSpecs_.get(i));\n    }\n\n    String defaultPackage \u003d this.getClass().getPackage().getName();\n    Class c;\n    Class fmt \u003d null;\n    if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n      fmt \u003d TextInputFormat.class;\n    } else if (inputFormatSpec_ !\u003d null) {\n      if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n        fmt \u003d TextInputFormat.class;\n      } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d KeyValueTextInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n          .getName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n                  .getCanonicalName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d SequenceFileInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n        fmt \u003d SequenceFileAsTextInputFormat.class;\n      } else {\n        c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n        if (c !\u003d null) {\n          fmt \u003d c;\n        } else {\n          fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n        }\n      }\n    }\n    if (fmt \u003d\u003d null) {\n      fmt \u003d StreamInputFormat.class;\n    }\n\n    jobConf_.setInputFormat(fmt);\n\n    if (ioSpec_ !\u003d null) {\n      jobConf_.set(\"stream.map.input\", ioSpec_);\n      jobConf_.set(\"stream.map.output\", ioSpec_);\n      jobConf_.set(\"stream.reduce.input\", ioSpec_);\n      jobConf_.set(\"stream.reduce.output\", ioSpec_);\n    }\n\n    Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d\n      jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n        IdentifierResolver.class, IdentifierResolver.class);\n    IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n\n    idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n\n    idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n\n    jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n\n    boolean isMapperACommand \u003d false;\n    if (mapCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setMapperClass(c);\n      } else {\n        isMapperACommand \u003d true;\n        jobConf_.setMapperClass(PipeMapper.class);\n        jobConf_.setMapRunnerClass(PipeMapRunner.class);\n        jobConf_.set(\"stream.map.streamprocessor\",\n                     URLEncoder.encode(mapCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (comCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setCombinerClass(c);\n      } else {\n        jobConf_.setCombinerClass(PipeCombiner.class);\n        jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n                comCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (numReduceTasksSpec_!\u003d null) {\n      int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n      jobConf_.setNumReduceTasks(numReduceTasks);\n    }\n\n    boolean isReducerACommand \u003d false;\n    if (redCmd_ !\u003d null) {\n      if (redCmd_.equals(REDUCE_NONE)) {\n        jobConf_.setNumReduceTasks(0);\n      }\n      if (jobConf_.getNumReduceTasks() !\u003d 0) {\n        if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n          jobConf_.setReducerClass(ValueAggregatorReducer.class);\n          jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n        } else {\n\n          c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n          if (c !\u003d null) {\n            jobConf_.setReducerClass(c);\n          } else {\n            isReducerACommand \u003d true;\n            jobConf_.setReducerClass(PipeReducer.class);\n            jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n                redCmd_, \"UTF-8\"));\n          }\n        }\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.map.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isMapperACommand) {\n      // if mapper is a command, then map output key/value classes come from the\n      // idResolver\n      jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n\n      if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n        jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n        jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isReducerACommand) {\n      // if reducer is a command, then output key/value classes come from the\n      // idResolver\n      jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n    }\n\n    if (inReaderSpec_ !\u003d null) {\n      String[] args \u003d inReaderSpec_.split(\",\");\n      String readerClass \u003d args[0];\n      // this argument can only be a Java class\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.set(\"stream.recordreader.class\", c.getName());\n      } else {\n        fail(\"-inputreader: class not found: \" + readerClass);\n      }\n      for (int i \u003d 1; i \u003c args.length; i++) {\n        String[] nv \u003d args[i].split(\"\u003d\", 2);\n        String k \u003d \"stream.recordreader.\" + nv[0];\n        String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n        jobConf_.set(k, v);\n      }\n    }\n\n    FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n    fmt \u003d null;\n    if (outputFormatSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n      if (c !\u003d null) {\n        fmt \u003d c;\n      } else {\n        fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n      }\n    }\n    if (fmt \u003d\u003d null) {\n      fmt \u003d TextOutputFormat.class;\n    }\n    if (lazyOutput_) {\n      LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n    } else {\n      jobConf_.setOutputFormat(fmt);\n    }\n\n    if (partitionerSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setPartitionerClass(c);\n      } else {\n        fail(\"-partitioner : class not found : \" + partitionerSpec_);\n      }\n    }\n\n    if(mapDebugSpec_ !\u003d null){\n    \tjobConf_.setMapDebugScript(mapDebugSpec_);\n    }\n    if(reduceDebugSpec_ !\u003d null){\n    \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n    }\n    // last, allow user to override anything\n    // (although typically used with properties we didn\u0027t touch)\n\n    jar_ \u003d packageJobJar();\n    if (jar_ !\u003d null) {\n      jobConf_.setJar(jar_);\n    }\n\n    if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n      getURIs(cacheArchives, cacheFiles);\n      boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n      if (!b)\n        fail(LINK_URI);\n    }\n    DistributedCache.createSymlink(jobConf_);\n    // set the jobconf for the caching parameters\n    if (cacheArchives !\u003d null)\n      DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n    if (cacheFiles !\u003d null)\n      DistributedCache.setCacheFiles(fileURIs, jobConf_);\n\n    if (verbose_) {\n      listJobConfProperties();\n    }\n\n    msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n  }",
      "path": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "3e4efbb609ff25036aa6b04dc379b6db72e5a31f": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3254. Fixed streaming to set the job.jar by using the right JobConf ctor. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189587 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/10/11 7:47 PM",
      "commitName": "3e4efbb609ff25036aa6b04dc379b6db72e5a31f",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "03/10/11 5:51 PM",
      "commitNameOld": "a48e6f2519c5110b281251727d86f8fd7837facf",
      "commitAuthorOld": "Eric Yang",
      "daysBetweenCommits": 23.08,
      "commitsBetweenForRepo": 190,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,251 +1,251 @@\n   protected void setJobConf() throws IOException {\n     if (additionalConfSpec_ !\u003d null) {\n       LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n       config_.addResource(new Path(additionalConfSpec_));\n     }\n \n     // general MapRed job properties\n-    jobConf_ \u003d new JobConf(config_);\n+    jobConf_ \u003d new JobConf(config_, StreamJob.class);\n     \n     // All streaming jobs get the task timeout value\n     // from the configuration settings.\n \n     // The correct FS must be set before this is called!\n     // (to resolve local vs. dfs drive letter differences) \n     // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n     for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n       FileInputFormat.addInputPaths(jobConf_, \n                         (String) inputSpecs_.get(i));\n     }\n \n     String defaultPackage \u003d this.getClass().getPackage().getName();\n     Class c;\n     Class fmt \u003d null;\n     if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n       fmt \u003d TextInputFormat.class;\n     } else if (inputFormatSpec_ !\u003d null) {\n       if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n           || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n           || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n         fmt \u003d TextInputFormat.class;\n       } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n           .getName())\n           || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n               .getCanonicalName())\n           || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n         if (inReaderSpec_ \u003d\u003d null) {\n           fmt \u003d KeyValueTextInputFormat.class;\n         }\n       } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n           .getName())\n           || inputFormatSpec_\n               .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n                   .getCanonicalName())\n           || inputFormatSpec_\n               .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n         if (inReaderSpec_ \u003d\u003d null) {\n           fmt \u003d SequenceFileInputFormat.class;\n         }\n       } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n           .getName())\n           || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n               .getCanonicalName())\n           || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n         fmt \u003d SequenceFileAsTextInputFormat.class;\n       } else {\n         c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n         if (c !\u003d null) {\n           fmt \u003d c;\n         } else {\n           fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n         }\n       }\n     } \n     if (fmt \u003d\u003d null) {\n       fmt \u003d StreamInputFormat.class;\n     }\n \n     jobConf_.setInputFormat(fmt);\n \n     if (ioSpec_ !\u003d null) {\n       jobConf_.set(\"stream.map.input\", ioSpec_);\n       jobConf_.set(\"stream.map.output\", ioSpec_);\n       jobConf_.set(\"stream.reduce.input\", ioSpec_);\n       jobConf_.set(\"stream.reduce.output\", ioSpec_);\n     }\n     \n     Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d \n       jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n         IdentifierResolver.class, IdentifierResolver.class);\n     IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n     \n     idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.map.input.writer.class\",\n       idResolver.getInputWriterClass(), InputWriter.class);\n     \n     idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.reduce.input.writer.class\",\n       idResolver.getInputWriterClass(), InputWriter.class);\n     \n     jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n \n     boolean isMapperACommand \u003d false;\n     if (mapCmd_ !\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setMapperClass(c);\n       } else {\n         isMapperACommand \u003d true;\n         jobConf_.setMapperClass(PipeMapper.class);\n         jobConf_.setMapRunnerClass(PipeMapRunner.class);\n         jobConf_.set(\"stream.map.streamprocessor\", \n                      URLEncoder.encode(mapCmd_, \"UTF-8\"));\n       }\n     }\n \n     if (comCmd_ !\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setCombinerClass(c);\n       } else {\n         jobConf_.setCombinerClass(PipeCombiner.class);\n         jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n                 comCmd_, \"UTF-8\"));\n       }\n     }\n \n     if (numReduceTasksSpec_!\u003d null) {\n       int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n       jobConf_.setNumReduceTasks(numReduceTasks);\n     }\n \n     boolean isReducerACommand \u003d false;\n     if (redCmd_ !\u003d null) {\n       if (redCmd_.equals(REDUCE_NONE)) {\n         jobConf_.setNumReduceTasks(0);\n       }\n       if (jobConf_.getNumReduceTasks() !\u003d 0) {\n         if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n           jobConf_.setReducerClass(ValueAggregatorReducer.class);\n           jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n         } else {\n \n           c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n           if (c !\u003d null) {\n             jobConf_.setReducerClass(c);\n           } else {\n             isReducerACommand \u003d true;\n             jobConf_.setReducerClass(PipeReducer.class);\n             jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n                 redCmd_, \"UTF-8\"));\n           }\n         }\n       }\n     }\n \n     idResolver.resolve(jobConf_.get(\"stream.map.output\",\n         IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.map.output.reader.class\",\n       idResolver.getOutputReaderClass(), OutputReader.class);\n     if (isMapperACommand) {\n       // if mapper is a command, then map output key/value classes come from the\n       // idResolver\n       jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n       jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n \n       if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n         jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n         jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n       }\n     }\n \n     idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n         IdentifierResolver.TEXT_ID));\n     jobConf_.setClass(\"stream.reduce.output.reader.class\",\n       idResolver.getOutputReaderClass(), OutputReader.class);\n     if (isReducerACommand) {\n       // if reducer is a command, then output key/value classes come from the\n       // idResolver\n       jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n       jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n     }\n \n     if (inReaderSpec_ !\u003d null) {\n       String[] args \u003d inReaderSpec_.split(\",\");\n       String readerClass \u003d args[0];\n       // this argument can only be a Java class\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.set(\"stream.recordreader.class\", c.getName());\n       } else {\n         fail(\"-inputreader: class not found: \" + readerClass);\n       }\n       for (int i \u003d 1; i \u003c args.length; i++) {\n         String[] nv \u003d args[i].split(\"\u003d\", 2);\n         String k \u003d \"stream.recordreader.\" + nv[0];\n         String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n         jobConf_.set(k, v);\n       }\n     }\n     \n     FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n     fmt \u003d null;\n     if (outputFormatSpec_!\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n       if (c !\u003d null) {\n         fmt \u003d c;\n       } else {\n         fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n       }\n     }\n     if (fmt \u003d\u003d null) {\n       fmt \u003d TextOutputFormat.class;\n     }\n     if (lazyOutput_) {\n       LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n     } else {\n       jobConf_.setOutputFormat(fmt);\n     }\n \n     if (partitionerSpec_!\u003d null) {\n       c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n       if (c !\u003d null) {\n         jobConf_.setPartitionerClass(c);\n       } else {\n         fail(\"-partitioner : class not found : \" + partitionerSpec_);\n       }\n     }\n     \n     if(mapDebugSpec_ !\u003d null){\n     \tjobConf_.setMapDebugScript(mapDebugSpec_);\n     }\n     if(reduceDebugSpec_ !\u003d null){\n     \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n     }\n     // last, allow user to override anything\n     // (although typically used with properties we didn\u0027t touch)\n \n     jar_ \u003d packageJobJar();\n     if (jar_ !\u003d null) {\n       jobConf_.setJar(jar_);\n     }\n     \n     if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n       getURIs(cacheArchives, cacheFiles);\n       boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n       if (!b)\n         fail(LINK_URI);\n     }\n     DistributedCache.createSymlink(jobConf_);\n     // set the jobconf for the caching parameters\n     if (cacheArchives !\u003d null)\n       DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n     if (cacheFiles !\u003d null)\n       DistributedCache.setCacheFiles(fileURIs, jobConf_);\n     \n     if (verbose_) {\n       listJobConfProperties();\n     }\n    \n     msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void setJobConf() throws IOException {\n    if (additionalConfSpec_ !\u003d null) {\n      LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n      config_.addResource(new Path(additionalConfSpec_));\n    }\n\n    // general MapRed job properties\n    jobConf_ \u003d new JobConf(config_, StreamJob.class);\n    \n    // All streaming jobs get the task timeout value\n    // from the configuration settings.\n\n    // The correct FS must be set before this is called!\n    // (to resolve local vs. dfs drive letter differences) \n    // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n    for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n      FileInputFormat.addInputPaths(jobConf_, \n                        (String) inputSpecs_.get(i));\n    }\n\n    String defaultPackage \u003d this.getClass().getPackage().getName();\n    Class c;\n    Class fmt \u003d null;\n    if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n      fmt \u003d TextInputFormat.class;\n    } else if (inputFormatSpec_ !\u003d null) {\n      if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n        fmt \u003d TextInputFormat.class;\n      } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d KeyValueTextInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n          .getName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n                  .getCanonicalName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d SequenceFileInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n        fmt \u003d SequenceFileAsTextInputFormat.class;\n      } else {\n        c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n        if (c !\u003d null) {\n          fmt \u003d c;\n        } else {\n          fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n        }\n      }\n    } \n    if (fmt \u003d\u003d null) {\n      fmt \u003d StreamInputFormat.class;\n    }\n\n    jobConf_.setInputFormat(fmt);\n\n    if (ioSpec_ !\u003d null) {\n      jobConf_.set(\"stream.map.input\", ioSpec_);\n      jobConf_.set(\"stream.map.output\", ioSpec_);\n      jobConf_.set(\"stream.reduce.input\", ioSpec_);\n      jobConf_.set(\"stream.reduce.output\", ioSpec_);\n    }\n    \n    Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d \n      jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n        IdentifierResolver.class, IdentifierResolver.class);\n    IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n    \n    idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n    \n    idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n    \n    jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n\n    boolean isMapperACommand \u003d false;\n    if (mapCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setMapperClass(c);\n      } else {\n        isMapperACommand \u003d true;\n        jobConf_.setMapperClass(PipeMapper.class);\n        jobConf_.setMapRunnerClass(PipeMapRunner.class);\n        jobConf_.set(\"stream.map.streamprocessor\", \n                     URLEncoder.encode(mapCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (comCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setCombinerClass(c);\n      } else {\n        jobConf_.setCombinerClass(PipeCombiner.class);\n        jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n                comCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (numReduceTasksSpec_!\u003d null) {\n      int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n      jobConf_.setNumReduceTasks(numReduceTasks);\n    }\n\n    boolean isReducerACommand \u003d false;\n    if (redCmd_ !\u003d null) {\n      if (redCmd_.equals(REDUCE_NONE)) {\n        jobConf_.setNumReduceTasks(0);\n      }\n      if (jobConf_.getNumReduceTasks() !\u003d 0) {\n        if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n          jobConf_.setReducerClass(ValueAggregatorReducer.class);\n          jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n        } else {\n\n          c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n          if (c !\u003d null) {\n            jobConf_.setReducerClass(c);\n          } else {\n            isReducerACommand \u003d true;\n            jobConf_.setReducerClass(PipeReducer.class);\n            jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n                redCmd_, \"UTF-8\"));\n          }\n        }\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.map.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isMapperACommand) {\n      // if mapper is a command, then map output key/value classes come from the\n      // idResolver\n      jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n\n      if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n        jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n        jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isReducerACommand) {\n      // if reducer is a command, then output key/value classes come from the\n      // idResolver\n      jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n    }\n\n    if (inReaderSpec_ !\u003d null) {\n      String[] args \u003d inReaderSpec_.split(\",\");\n      String readerClass \u003d args[0];\n      // this argument can only be a Java class\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.set(\"stream.recordreader.class\", c.getName());\n      } else {\n        fail(\"-inputreader: class not found: \" + readerClass);\n      }\n      for (int i \u003d 1; i \u003c args.length; i++) {\n        String[] nv \u003d args[i].split(\"\u003d\", 2);\n        String k \u003d \"stream.recordreader.\" + nv[0];\n        String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n        jobConf_.set(k, v);\n      }\n    }\n    \n    FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n    fmt \u003d null;\n    if (outputFormatSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n      if (c !\u003d null) {\n        fmt \u003d c;\n      } else {\n        fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n      }\n    }\n    if (fmt \u003d\u003d null) {\n      fmt \u003d TextOutputFormat.class;\n    }\n    if (lazyOutput_) {\n      LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n    } else {\n      jobConf_.setOutputFormat(fmt);\n    }\n\n    if (partitionerSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setPartitionerClass(c);\n      } else {\n        fail(\"-partitioner : class not found : \" + partitionerSpec_);\n      }\n    }\n    \n    if(mapDebugSpec_ !\u003d null){\n    \tjobConf_.setMapDebugScript(mapDebugSpec_);\n    }\n    if(reduceDebugSpec_ !\u003d null){\n    \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n    }\n    // last, allow user to override anything\n    // (although typically used with properties we didn\u0027t touch)\n\n    jar_ \u003d packageJobJar();\n    if (jar_ !\u003d null) {\n      jobConf_.setJar(jar_);\n    }\n    \n    if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n      getURIs(cacheArchives, cacheFiles);\n      boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n      if (!b)\n        fail(LINK_URI);\n    }\n    DistributedCache.createSymlink(jobConf_);\n    // set the jobconf for the caching parameters\n    if (cacheArchives !\u003d null)\n      DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n    if (cacheFiles !\u003d null)\n      DistributedCache.setCacheFiles(fileURIs, jobConf_);\n    \n    if (verbose_) {\n      listJobConfProperties();\n    }\n   \n    msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n  }",
      "path": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void setJobConf() throws IOException {\n    if (additionalConfSpec_ !\u003d null) {\n      LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n      config_.addResource(new Path(additionalConfSpec_));\n    }\n\n    // general MapRed job properties\n    jobConf_ \u003d new JobConf(config_);\n    \n    // All streaming jobs get the task timeout value\n    // from the configuration settings.\n\n    // The correct FS must be set before this is called!\n    // (to resolve local vs. dfs drive letter differences) \n    // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n    for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n      FileInputFormat.addInputPaths(jobConf_, \n                        (String) inputSpecs_.get(i));\n    }\n\n    String defaultPackage \u003d this.getClass().getPackage().getName();\n    Class c;\n    Class fmt \u003d null;\n    if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n      fmt \u003d TextInputFormat.class;\n    } else if (inputFormatSpec_ !\u003d null) {\n      if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n        fmt \u003d TextInputFormat.class;\n      } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d KeyValueTextInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n          .getName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n                  .getCanonicalName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d SequenceFileInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n        fmt \u003d SequenceFileAsTextInputFormat.class;\n      } else {\n        c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n        if (c !\u003d null) {\n          fmt \u003d c;\n        } else {\n          fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n        }\n      }\n    } \n    if (fmt \u003d\u003d null) {\n      fmt \u003d StreamInputFormat.class;\n    }\n\n    jobConf_.setInputFormat(fmt);\n\n    if (ioSpec_ !\u003d null) {\n      jobConf_.set(\"stream.map.input\", ioSpec_);\n      jobConf_.set(\"stream.map.output\", ioSpec_);\n      jobConf_.set(\"stream.reduce.input\", ioSpec_);\n      jobConf_.set(\"stream.reduce.output\", ioSpec_);\n    }\n    \n    Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d \n      jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n        IdentifierResolver.class, IdentifierResolver.class);\n    IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n    \n    idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n    \n    idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n    \n    jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n\n    boolean isMapperACommand \u003d false;\n    if (mapCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setMapperClass(c);\n      } else {\n        isMapperACommand \u003d true;\n        jobConf_.setMapperClass(PipeMapper.class);\n        jobConf_.setMapRunnerClass(PipeMapRunner.class);\n        jobConf_.set(\"stream.map.streamprocessor\", \n                     URLEncoder.encode(mapCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (comCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setCombinerClass(c);\n      } else {\n        jobConf_.setCombinerClass(PipeCombiner.class);\n        jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n                comCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (numReduceTasksSpec_!\u003d null) {\n      int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n      jobConf_.setNumReduceTasks(numReduceTasks);\n    }\n\n    boolean isReducerACommand \u003d false;\n    if (redCmd_ !\u003d null) {\n      if (redCmd_.equals(REDUCE_NONE)) {\n        jobConf_.setNumReduceTasks(0);\n      }\n      if (jobConf_.getNumReduceTasks() !\u003d 0) {\n        if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n          jobConf_.setReducerClass(ValueAggregatorReducer.class);\n          jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n        } else {\n\n          c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n          if (c !\u003d null) {\n            jobConf_.setReducerClass(c);\n          } else {\n            isReducerACommand \u003d true;\n            jobConf_.setReducerClass(PipeReducer.class);\n            jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n                redCmd_, \"UTF-8\"));\n          }\n        }\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.map.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isMapperACommand) {\n      // if mapper is a command, then map output key/value classes come from the\n      // idResolver\n      jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n\n      if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n        jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n        jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isReducerACommand) {\n      // if reducer is a command, then output key/value classes come from the\n      // idResolver\n      jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n    }\n\n    if (inReaderSpec_ !\u003d null) {\n      String[] args \u003d inReaderSpec_.split(\",\");\n      String readerClass \u003d args[0];\n      // this argument can only be a Java class\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.set(\"stream.recordreader.class\", c.getName());\n      } else {\n        fail(\"-inputreader: class not found: \" + readerClass);\n      }\n      for (int i \u003d 1; i \u003c args.length; i++) {\n        String[] nv \u003d args[i].split(\"\u003d\", 2);\n        String k \u003d \"stream.recordreader.\" + nv[0];\n        String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n        jobConf_.set(k, v);\n      }\n    }\n    \n    FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n    fmt \u003d null;\n    if (outputFormatSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n      if (c !\u003d null) {\n        fmt \u003d c;\n      } else {\n        fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n      }\n    }\n    if (fmt \u003d\u003d null) {\n      fmt \u003d TextOutputFormat.class;\n    }\n    if (lazyOutput_) {\n      LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n    } else {\n      jobConf_.setOutputFormat(fmt);\n    }\n\n    if (partitionerSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setPartitionerClass(c);\n      } else {\n        fail(\"-partitioner : class not found : \" + partitionerSpec_);\n      }\n    }\n    \n    if(mapDebugSpec_ !\u003d null){\n    \tjobConf_.setMapDebugScript(mapDebugSpec_);\n    }\n    if(reduceDebugSpec_ !\u003d null){\n    \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n    }\n    // last, allow user to override anything\n    // (although typically used with properties we didn\u0027t touch)\n\n    jar_ \u003d packageJobJar();\n    if (jar_ !\u003d null) {\n      jobConf_.setJar(jar_);\n    }\n    \n    if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n      getURIs(cacheArchives, cacheFiles);\n      boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n      if (!b)\n        fail(LINK_URI);\n    }\n    DistributedCache.createSymlink(jobConf_);\n    // set the jobconf for the caching parameters\n    if (cacheArchives !\u003d null)\n      DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n    if (cacheFiles !\u003d null)\n      DistributedCache.setCacheFiles(fileURIs, jobConf_);\n    \n    if (verbose_) {\n      listJobConfProperties();\n    }\n   \n    msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n  }",
      "path": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void setJobConf() throws IOException {\n    if (additionalConfSpec_ !\u003d null) {\n      LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n      config_.addResource(new Path(additionalConfSpec_));\n    }\n\n    // general MapRed job properties\n    jobConf_ \u003d new JobConf(config_);\n    \n    // All streaming jobs get the task timeout value\n    // from the configuration settings.\n\n    // The correct FS must be set before this is called!\n    // (to resolve local vs. dfs drive letter differences) \n    // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n    for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n      FileInputFormat.addInputPaths(jobConf_, \n                        (String) inputSpecs_.get(i));\n    }\n\n    String defaultPackage \u003d this.getClass().getPackage().getName();\n    Class c;\n    Class fmt \u003d null;\n    if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n      fmt \u003d TextInputFormat.class;\n    } else if (inputFormatSpec_ !\u003d null) {\n      if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n        fmt \u003d TextInputFormat.class;\n      } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d KeyValueTextInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n          .getName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n                  .getCanonicalName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d SequenceFileInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n        fmt \u003d SequenceFileAsTextInputFormat.class;\n      } else {\n        c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n        if (c !\u003d null) {\n          fmt \u003d c;\n        } else {\n          fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n        }\n      }\n    } \n    if (fmt \u003d\u003d null) {\n      fmt \u003d StreamInputFormat.class;\n    }\n\n    jobConf_.setInputFormat(fmt);\n\n    if (ioSpec_ !\u003d null) {\n      jobConf_.set(\"stream.map.input\", ioSpec_);\n      jobConf_.set(\"stream.map.output\", ioSpec_);\n      jobConf_.set(\"stream.reduce.input\", ioSpec_);\n      jobConf_.set(\"stream.reduce.output\", ioSpec_);\n    }\n    \n    Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d \n      jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n        IdentifierResolver.class, IdentifierResolver.class);\n    IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n    \n    idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n    \n    idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n    \n    jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n\n    boolean isMapperACommand \u003d false;\n    if (mapCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setMapperClass(c);\n      } else {\n        isMapperACommand \u003d true;\n        jobConf_.setMapperClass(PipeMapper.class);\n        jobConf_.setMapRunnerClass(PipeMapRunner.class);\n        jobConf_.set(\"stream.map.streamprocessor\", \n                     URLEncoder.encode(mapCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (comCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setCombinerClass(c);\n      } else {\n        jobConf_.setCombinerClass(PipeCombiner.class);\n        jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n                comCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (numReduceTasksSpec_!\u003d null) {\n      int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n      jobConf_.setNumReduceTasks(numReduceTasks);\n    }\n\n    boolean isReducerACommand \u003d false;\n    if (redCmd_ !\u003d null) {\n      if (redCmd_.equals(REDUCE_NONE)) {\n        jobConf_.setNumReduceTasks(0);\n      }\n      if (jobConf_.getNumReduceTasks() !\u003d 0) {\n        if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n          jobConf_.setReducerClass(ValueAggregatorReducer.class);\n          jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n        } else {\n\n          c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n          if (c !\u003d null) {\n            jobConf_.setReducerClass(c);\n          } else {\n            isReducerACommand \u003d true;\n            jobConf_.setReducerClass(PipeReducer.class);\n            jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n                redCmd_, \"UTF-8\"));\n          }\n        }\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.map.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isMapperACommand) {\n      // if mapper is a command, then map output key/value classes come from the\n      // idResolver\n      jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n\n      if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n        jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n        jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isReducerACommand) {\n      // if reducer is a command, then output key/value classes come from the\n      // idResolver\n      jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n    }\n\n    if (inReaderSpec_ !\u003d null) {\n      String[] args \u003d inReaderSpec_.split(\",\");\n      String readerClass \u003d args[0];\n      // this argument can only be a Java class\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.set(\"stream.recordreader.class\", c.getName());\n      } else {\n        fail(\"-inputreader: class not found: \" + readerClass);\n      }\n      for (int i \u003d 1; i \u003c args.length; i++) {\n        String[] nv \u003d args[i].split(\"\u003d\", 2);\n        String k \u003d \"stream.recordreader.\" + nv[0];\n        String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n        jobConf_.set(k, v);\n      }\n    }\n    \n    FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n    fmt \u003d null;\n    if (outputFormatSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n      if (c !\u003d null) {\n        fmt \u003d c;\n      } else {\n        fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n      }\n    }\n    if (fmt \u003d\u003d null) {\n      fmt \u003d TextOutputFormat.class;\n    }\n    if (lazyOutput_) {\n      LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n    } else {\n      jobConf_.setOutputFormat(fmt);\n    }\n\n    if (partitionerSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setPartitionerClass(c);\n      } else {\n        fail(\"-partitioner : class not found : \" + partitionerSpec_);\n      }\n    }\n    \n    if(mapDebugSpec_ !\u003d null){\n    \tjobConf_.setMapDebugScript(mapDebugSpec_);\n    }\n    if(reduceDebugSpec_ !\u003d null){\n    \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n    }\n    // last, allow user to override anything\n    // (although typically used with properties we didn\u0027t touch)\n\n    jar_ \u003d packageJobJar();\n    if (jar_ !\u003d null) {\n      jobConf_.setJar(jar_);\n    }\n    \n    if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n      getURIs(cacheArchives, cacheFiles);\n      boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n      if (!b)\n        fail(LINK_URI);\n    }\n    DistributedCache.createSymlink(jobConf_);\n    // set the jobconf for the caching parameters\n    if (cacheArchives !\u003d null)\n      DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n    if (cacheFiles !\u003d null)\n      DistributedCache.setCacheFiles(fileURIs, jobConf_);\n    \n    if (verbose_) {\n      listJobConfProperties();\n    }\n   \n    msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n  }",
      "path": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,251 @@\n+  protected void setJobConf() throws IOException {\n+    if (additionalConfSpec_ !\u003d null) {\n+      LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n+      config_.addResource(new Path(additionalConfSpec_));\n+    }\n+\n+    // general MapRed job properties\n+    jobConf_ \u003d new JobConf(config_);\n+    \n+    // All streaming jobs get the task timeout value\n+    // from the configuration settings.\n+\n+    // The correct FS must be set before this is called!\n+    // (to resolve local vs. dfs drive letter differences) \n+    // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n+    for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n+      FileInputFormat.addInputPaths(jobConf_, \n+                        (String) inputSpecs_.get(i));\n+    }\n+\n+    String defaultPackage \u003d this.getClass().getPackage().getName();\n+    Class c;\n+    Class fmt \u003d null;\n+    if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n+      fmt \u003d TextInputFormat.class;\n+    } else if (inputFormatSpec_ !\u003d null) {\n+      if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n+          || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n+          || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n+        fmt \u003d TextInputFormat.class;\n+      } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n+          .getName())\n+          || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n+              .getCanonicalName())\n+          || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n+        if (inReaderSpec_ \u003d\u003d null) {\n+          fmt \u003d KeyValueTextInputFormat.class;\n+        }\n+      } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n+          .getName())\n+          || inputFormatSpec_\n+              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n+                  .getCanonicalName())\n+          || inputFormatSpec_\n+              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n+        if (inReaderSpec_ \u003d\u003d null) {\n+          fmt \u003d SequenceFileInputFormat.class;\n+        }\n+      } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n+          .getName())\n+          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n+              .getCanonicalName())\n+          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n+        fmt \u003d SequenceFileAsTextInputFormat.class;\n+      } else {\n+        c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n+        if (c !\u003d null) {\n+          fmt \u003d c;\n+        } else {\n+          fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n+        }\n+      }\n+    } \n+    if (fmt \u003d\u003d null) {\n+      fmt \u003d StreamInputFormat.class;\n+    }\n+\n+    jobConf_.setInputFormat(fmt);\n+\n+    if (ioSpec_ !\u003d null) {\n+      jobConf_.set(\"stream.map.input\", ioSpec_);\n+      jobConf_.set(\"stream.map.output\", ioSpec_);\n+      jobConf_.set(\"stream.reduce.input\", ioSpec_);\n+      jobConf_.set(\"stream.reduce.output\", ioSpec_);\n+    }\n+    \n+    Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d \n+      jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n+        IdentifierResolver.class, IdentifierResolver.class);\n+    IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n+    \n+    idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n+    jobConf_.setClass(\"stream.map.input.writer.class\",\n+      idResolver.getInputWriterClass(), InputWriter.class);\n+    \n+    idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n+    jobConf_.setClass(\"stream.reduce.input.writer.class\",\n+      idResolver.getInputWriterClass(), InputWriter.class);\n+    \n+    jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n+\n+    boolean isMapperACommand \u003d false;\n+    if (mapCmd_ !\u003d null) {\n+      c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n+      if (c !\u003d null) {\n+        jobConf_.setMapperClass(c);\n+      } else {\n+        isMapperACommand \u003d true;\n+        jobConf_.setMapperClass(PipeMapper.class);\n+        jobConf_.setMapRunnerClass(PipeMapRunner.class);\n+        jobConf_.set(\"stream.map.streamprocessor\", \n+                     URLEncoder.encode(mapCmd_, \"UTF-8\"));\n+      }\n+    }\n+\n+    if (comCmd_ !\u003d null) {\n+      c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n+      if (c !\u003d null) {\n+        jobConf_.setCombinerClass(c);\n+      } else {\n+        jobConf_.setCombinerClass(PipeCombiner.class);\n+        jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n+                comCmd_, \"UTF-8\"));\n+      }\n+    }\n+\n+    if (numReduceTasksSpec_!\u003d null) {\n+      int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n+      jobConf_.setNumReduceTasks(numReduceTasks);\n+    }\n+\n+    boolean isReducerACommand \u003d false;\n+    if (redCmd_ !\u003d null) {\n+      if (redCmd_.equals(REDUCE_NONE)) {\n+        jobConf_.setNumReduceTasks(0);\n+      }\n+      if (jobConf_.getNumReduceTasks() !\u003d 0) {\n+        if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n+          jobConf_.setReducerClass(ValueAggregatorReducer.class);\n+          jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n+        } else {\n+\n+          c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n+          if (c !\u003d null) {\n+            jobConf_.setReducerClass(c);\n+          } else {\n+            isReducerACommand \u003d true;\n+            jobConf_.setReducerClass(PipeReducer.class);\n+            jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n+                redCmd_, \"UTF-8\"));\n+          }\n+        }\n+      }\n+    }\n+\n+    idResolver.resolve(jobConf_.get(\"stream.map.output\",\n+        IdentifierResolver.TEXT_ID));\n+    jobConf_.setClass(\"stream.map.output.reader.class\",\n+      idResolver.getOutputReaderClass(), OutputReader.class);\n+    if (isMapperACommand) {\n+      // if mapper is a command, then map output key/value classes come from the\n+      // idResolver\n+      jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n+      jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n+\n+      if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n+        jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n+        jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n+      }\n+    }\n+\n+    idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n+        IdentifierResolver.TEXT_ID));\n+    jobConf_.setClass(\"stream.reduce.output.reader.class\",\n+      idResolver.getOutputReaderClass(), OutputReader.class);\n+    if (isReducerACommand) {\n+      // if reducer is a command, then output key/value classes come from the\n+      // idResolver\n+      jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n+      jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n+    }\n+\n+    if (inReaderSpec_ !\u003d null) {\n+      String[] args \u003d inReaderSpec_.split(\",\");\n+      String readerClass \u003d args[0];\n+      // this argument can only be a Java class\n+      c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n+      if (c !\u003d null) {\n+        jobConf_.set(\"stream.recordreader.class\", c.getName());\n+      } else {\n+        fail(\"-inputreader: class not found: \" + readerClass);\n+      }\n+      for (int i \u003d 1; i \u003c args.length; i++) {\n+        String[] nv \u003d args[i].split(\"\u003d\", 2);\n+        String k \u003d \"stream.recordreader.\" + nv[0];\n+        String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n+        jobConf_.set(k, v);\n+      }\n+    }\n+    \n+    FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n+    fmt \u003d null;\n+    if (outputFormatSpec_!\u003d null) {\n+      c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n+      if (c !\u003d null) {\n+        fmt \u003d c;\n+      } else {\n+        fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n+      }\n+    }\n+    if (fmt \u003d\u003d null) {\n+      fmt \u003d TextOutputFormat.class;\n+    }\n+    if (lazyOutput_) {\n+      LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n+    } else {\n+      jobConf_.setOutputFormat(fmt);\n+    }\n+\n+    if (partitionerSpec_!\u003d null) {\n+      c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n+      if (c !\u003d null) {\n+        jobConf_.setPartitionerClass(c);\n+      } else {\n+        fail(\"-partitioner : class not found : \" + partitionerSpec_);\n+      }\n+    }\n+    \n+    if(mapDebugSpec_ !\u003d null){\n+    \tjobConf_.setMapDebugScript(mapDebugSpec_);\n+    }\n+    if(reduceDebugSpec_ !\u003d null){\n+    \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n+    }\n+    // last, allow user to override anything\n+    // (although typically used with properties we didn\u0027t touch)\n+\n+    jar_ \u003d packageJobJar();\n+    if (jar_ !\u003d null) {\n+      jobConf_.setJar(jar_);\n+    }\n+    \n+    if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n+      getURIs(cacheArchives, cacheFiles);\n+      boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n+      if (!b)\n+        fail(LINK_URI);\n+    }\n+    DistributedCache.createSymlink(jobConf_);\n+    // set the jobconf for the caching parameters\n+    if (cacheArchives !\u003d null)\n+      DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n+    if (cacheFiles !\u003d null)\n+      DistributedCache.setCacheFiles(fileURIs, jobConf_);\n+    \n+    if (verbose_) {\n+      listJobConfProperties();\n+    }\n+   \n+    msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void setJobConf() throws IOException {\n    if (additionalConfSpec_ !\u003d null) {\n      LOG.warn(\"-additionalconfspec option is deprecated, please use -conf instead.\");\n      config_.addResource(new Path(additionalConfSpec_));\n    }\n\n    // general MapRed job properties\n    jobConf_ \u003d new JobConf(config_);\n    \n    // All streaming jobs get the task timeout value\n    // from the configuration settings.\n\n    // The correct FS must be set before this is called!\n    // (to resolve local vs. dfs drive letter differences) \n    // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)\n    for (int i \u003d 0; i \u003c inputSpecs_.size(); i++) {\n      FileInputFormat.addInputPaths(jobConf_, \n                        (String) inputSpecs_.get(i));\n    }\n\n    String defaultPackage \u003d this.getClass().getPackage().getName();\n    Class c;\n    Class fmt \u003d null;\n    if (inReaderSpec_ \u003d\u003d null \u0026\u0026 inputFormatSpec_ \u003d\u003d null) {\n      fmt \u003d TextInputFormat.class;\n    } else if (inputFormatSpec_ !\u003d null) {\n      if (inputFormatSpec_.equals(TextInputFormat.class.getName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())\n          || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {\n        fmt \u003d TextInputFormat.class;\n      } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d KeyValueTextInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class\n          .getName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class\n                  .getCanonicalName())\n          || inputFormatSpec_\n              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {\n        if (inReaderSpec_ \u003d\u003d null) {\n          fmt \u003d SequenceFileInputFormat.class;\n        }\n      } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n          .getName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class\n              .getCanonicalName())\n          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {\n        fmt \u003d SequenceFileAsTextInputFormat.class;\n      } else {\n        c \u003d StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);\n        if (c !\u003d null) {\n          fmt \u003d c;\n        } else {\n          fail(\"-inputformat : class not found : \" + inputFormatSpec_);\n        }\n      }\n    } \n    if (fmt \u003d\u003d null) {\n      fmt \u003d StreamInputFormat.class;\n    }\n\n    jobConf_.setInputFormat(fmt);\n\n    if (ioSpec_ !\u003d null) {\n      jobConf_.set(\"stream.map.input\", ioSpec_);\n      jobConf_.set(\"stream.map.output\", ioSpec_);\n      jobConf_.set(\"stream.reduce.input\", ioSpec_);\n      jobConf_.set(\"stream.reduce.output\", ioSpec_);\n    }\n    \n    Class\u003c? extends IdentifierResolver\u003e idResolverClass \u003d \n      jobConf_.getClass(\"stream.io.identifier.resolver.class\",\n        IdentifierResolver.class, IdentifierResolver.class);\n    IdentifierResolver idResolver \u003d ReflectionUtils.newInstance(idResolverClass, jobConf_);\n    \n    idResolver.resolve(jobConf_.get(\"stream.map.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n    \n    idResolver.resolve(jobConf_.get(\"stream.reduce.input\", IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.input.writer.class\",\n      idResolver.getInputWriterClass(), InputWriter.class);\n    \n    jobConf_.set(\"stream.addenvironment\", addTaskEnvironment_);\n\n    boolean isMapperACommand \u003d false;\n    if (mapCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setMapperClass(c);\n      } else {\n        isMapperACommand \u003d true;\n        jobConf_.setMapperClass(PipeMapper.class);\n        jobConf_.setMapRunnerClass(PipeMapRunner.class);\n        jobConf_.set(\"stream.map.streamprocessor\", \n                     URLEncoder.encode(mapCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (comCmd_ !\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setCombinerClass(c);\n      } else {\n        jobConf_.setCombinerClass(PipeCombiner.class);\n        jobConf_.set(\"stream.combine.streamprocessor\", URLEncoder.encode(\n                comCmd_, \"UTF-8\"));\n      }\n    }\n\n    if (numReduceTasksSpec_!\u003d null) {\n      int numReduceTasks \u003d Integer.parseInt(numReduceTasksSpec_);\n      jobConf_.setNumReduceTasks(numReduceTasks);\n    }\n\n    boolean isReducerACommand \u003d false;\n    if (redCmd_ !\u003d null) {\n      if (redCmd_.equals(REDUCE_NONE)) {\n        jobConf_.setNumReduceTasks(0);\n      }\n      if (jobConf_.getNumReduceTasks() !\u003d 0) {\n        if (redCmd_.compareToIgnoreCase(\"aggregate\") \u003d\u003d 0) {\n          jobConf_.setReducerClass(ValueAggregatorReducer.class);\n          jobConf_.setCombinerClass(ValueAggregatorCombiner.class);\n        } else {\n\n          c \u003d StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);\n          if (c !\u003d null) {\n            jobConf_.setReducerClass(c);\n          } else {\n            isReducerACommand \u003d true;\n            jobConf_.setReducerClass(PipeReducer.class);\n            jobConf_.set(\"stream.reduce.streamprocessor\", URLEncoder.encode(\n                redCmd_, \"UTF-8\"));\n          }\n        }\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.map.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.map.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isMapperACommand) {\n      // if mapper is a command, then map output key/value classes come from the\n      // idResolver\n      jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());\n\n      if (jobConf_.getNumReduceTasks() \u003d\u003d 0) {\n        jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n        jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n      }\n    }\n\n    idResolver.resolve(jobConf_.get(\"stream.reduce.output\",\n        IdentifierResolver.TEXT_ID));\n    jobConf_.setClass(\"stream.reduce.output.reader.class\",\n      idResolver.getOutputReaderClass(), OutputReader.class);\n    if (isReducerACommand) {\n      // if reducer is a command, then output key/value classes come from the\n      // idResolver\n      jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());\n      jobConf_.setOutputValueClass(idResolver.getOutputValueClass());\n    }\n\n    if (inReaderSpec_ !\u003d null) {\n      String[] args \u003d inReaderSpec_.split(\",\");\n      String readerClass \u003d args[0];\n      // this argument can only be a Java class\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.set(\"stream.recordreader.class\", c.getName());\n      } else {\n        fail(\"-inputreader: class not found: \" + readerClass);\n      }\n      for (int i \u003d 1; i \u003c args.length; i++) {\n        String[] nv \u003d args[i].split(\"\u003d\", 2);\n        String k \u003d \"stream.recordreader.\" + nv[0];\n        String v \u003d (nv.length \u003e 1) ? nv[1] : \"\";\n        jobConf_.set(k, v);\n      }\n    }\n    \n    FileOutputFormat.setOutputPath(jobConf_, new Path(output_));\n    fmt \u003d null;\n    if (outputFormatSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);\n      if (c !\u003d null) {\n        fmt \u003d c;\n      } else {\n        fail(\"-outputformat : class not found : \" + outputFormatSpec_);\n      }\n    }\n    if (fmt \u003d\u003d null) {\n      fmt \u003d TextOutputFormat.class;\n    }\n    if (lazyOutput_) {\n      LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);\n    } else {\n      jobConf_.setOutputFormat(fmt);\n    }\n\n    if (partitionerSpec_!\u003d null) {\n      c \u003d StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);\n      if (c !\u003d null) {\n        jobConf_.setPartitionerClass(c);\n      } else {\n        fail(\"-partitioner : class not found : \" + partitionerSpec_);\n      }\n    }\n    \n    if(mapDebugSpec_ !\u003d null){\n    \tjobConf_.setMapDebugScript(mapDebugSpec_);\n    }\n    if(reduceDebugSpec_ !\u003d null){\n    \tjobConf_.setReduceDebugScript(reduceDebugSpec_);\n    }\n    // last, allow user to override anything\n    // (although typically used with properties we didn\u0027t touch)\n\n    jar_ \u003d packageJobJar();\n    if (jar_ !\u003d null) {\n      jobConf_.setJar(jar_);\n    }\n    \n    if ((cacheArchives !\u003d null) || (cacheFiles !\u003d null)){\n      getURIs(cacheArchives, cacheFiles);\n      boolean b \u003d DistributedCache.checkURIs(fileURIs, archiveURIs);\n      if (!b)\n        fail(LINK_URI);\n    }\n    DistributedCache.createSymlink(jobConf_);\n    // set the jobconf for the caching parameters\n    if (cacheArchives !\u003d null)\n      DistributedCache.setCacheArchives(archiveURIs, jobConf_);\n    if (cacheFiles !\u003d null)\n      DistributedCache.setCacheFiles(fileURIs, jobConf_);\n    \n    if (verbose_) {\n      listJobConfProperties();\n    }\n   \n    msg(\"submitting to jobconf: \" + getJobTrackerHostPort());\n  }",
      "path": "mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
    }
  }
}