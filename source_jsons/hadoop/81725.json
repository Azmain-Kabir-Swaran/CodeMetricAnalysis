{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "SysInfoLinux.java",
  "functionName": "readDiskBlockInformation",
  "functionId": "readDiskBlockInformation___diskName-String__defSector-int",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SysInfoLinux.java",
  "functionStartLine": 550,
  "functionEndLine": 591,
  "numCommitsSeen": 8,
  "timeTaken": 1199,
  "changeHistory": [
    "7a3188d054481b9bd563e337901e93476303ce7f",
    "a431ed9075cf6f467be5ff10f4ffb131cb1d3216"
  ],
  "changeHistoryShort": {
    "7a3188d054481b9bd563e337901e93476303ce7f": "Ybodychange",
    "a431ed9075cf6f467be5ff10f4ffb131cb1d3216": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7a3188d054481b9bd563e337901e93476303ce7f": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16282. Avoid FileStream to improve performance. Contributed by Ayush Saxena.\n",
      "commitDate": "02/05/19 12:58 PM",
      "commitName": "7a3188d054481b9bd563e337901e93476303ce7f",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "22/06/18 10:15 AM",
      "commitNameOld": "6432128622d64f3f9dd638b9c254c77cdf5408aa",
      "commitAuthorOld": "Eric E Payne",
      "daysBetweenCommits": 314.11,
      "commitsBetweenForRepo": 2397,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n   int readDiskBlockInformation(String diskName, int defSector) {\n \n     assert perDiskSectorSize !\u003d null \u0026\u0026 diskName !\u003d null;\n \n     String procfsDiskSectorFile \u003d\n             \"/sys/block/\" + diskName + \"/queue/hw_sector_size\";\n \n     BufferedReader in;\n     try {\n       in \u003d new BufferedReader(new InputStreamReader(\n-            new FileInputStream(procfsDiskSectorFile),\n+          Files.newInputStream(Paths.get(procfsDiskSectorFile)),\n               Charset.forName(\"UTF-8\")));\n-    } catch (FileNotFoundException f) {\n+    } catch (IOException f) {\n       return defSector;\n     }\n \n     Matcher mat;\n     try {\n       String str \u003d in.readLine();\n       while (str !\u003d null) {\n         mat \u003d PROCFS_DISKSECTORFILE_FORMAT.matcher(str);\n         if (mat.find()) {\n           String secSize \u003d mat.group(1);\n           if (secSize !\u003d null) {\n             return Integer.parseInt(secSize);\n           }\n         }\n         str \u003d in.readLine();\n       }\n       return defSector;\n     } catch (IOException|NumberFormatException e) {\n       LOG.warn(\"Error reading the stream \" + procfsDiskSectorFile, e);\n       return defSector;\n     } finally {\n       // Close the streams\n       try {\n         in.close();\n       } catch (IOException e) {\n         LOG.warn(\"Error closing the stream \" + procfsDiskSectorFile, e);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  int readDiskBlockInformation(String diskName, int defSector) {\n\n    assert perDiskSectorSize !\u003d null \u0026\u0026 diskName !\u003d null;\n\n    String procfsDiskSectorFile \u003d\n            \"/sys/block/\" + diskName + \"/queue/hw_sector_size\";\n\n    BufferedReader in;\n    try {\n      in \u003d new BufferedReader(new InputStreamReader(\n          Files.newInputStream(Paths.get(procfsDiskSectorFile)),\n              Charset.forName(\"UTF-8\")));\n    } catch (IOException f) {\n      return defSector;\n    }\n\n    Matcher mat;\n    try {\n      String str \u003d in.readLine();\n      while (str !\u003d null) {\n        mat \u003d PROCFS_DISKSECTORFILE_FORMAT.matcher(str);\n        if (mat.find()) {\n          String secSize \u003d mat.group(1);\n          if (secSize !\u003d null) {\n            return Integer.parseInt(secSize);\n          }\n        }\n        str \u003d in.readLine();\n      }\n      return defSector;\n    } catch (IOException|NumberFormatException e) {\n      LOG.warn(\"Error reading the stream \" + procfsDiskSectorFile, e);\n      return defSector;\n    } finally {\n      // Close the streams\n      try {\n        in.close();\n      } catch (IOException e) {\n        LOG.warn(\"Error closing the stream \" + procfsDiskSectorFile, e);\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SysInfoLinux.java",
      "extendedDetails": {}
    },
    "a431ed9075cf6f467be5ff10f4ffb131cb1d3216": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-12211. Collect disk usage on the node. Contributed by Robert Grandl\n",
      "commitDate": "13/07/15 3:36 PM",
      "commitName": "a431ed9075cf6f467be5ff10f4ffb131cb1d3216",
      "commitAuthor": "Chris Douglas",
      "diff": "@@ -0,0 +1,42 @@\n+  int readDiskBlockInformation(String diskName, int defSector) {\n+\n+    assert perDiskSectorSize !\u003d null \u0026\u0026 diskName !\u003d null;\n+\n+    String procfsDiskSectorFile \u003d\n+            \"/sys/block/\" + diskName + \"/queue/hw_sector_size\";\n+\n+    BufferedReader in;\n+    try {\n+      in \u003d new BufferedReader(new InputStreamReader(\n+            new FileInputStream(procfsDiskSectorFile),\n+              Charset.forName(\"UTF-8\")));\n+    } catch (FileNotFoundException f) {\n+      return defSector;\n+    }\n+\n+    Matcher mat;\n+    try {\n+      String str \u003d in.readLine();\n+      while (str !\u003d null) {\n+        mat \u003d PROCFS_DISKSECTORFILE_FORMAT.matcher(str);\n+        if (mat.find()) {\n+          String secSize \u003d mat.group(1);\n+          if (secSize !\u003d null) {\n+            return Integer.parseInt(secSize);\n+          }\n+        }\n+        str \u003d in.readLine();\n+      }\n+      return defSector;\n+    } catch (IOException|NumberFormatException e) {\n+      LOG.warn(\"Error reading the stream \" + procfsDiskSectorFile, e);\n+      return defSector;\n+    } finally {\n+      // Close the streams\n+      try {\n+        in.close();\n+      } catch (IOException e) {\n+        LOG.warn(\"Error closing the stream \" + procfsDiskSectorFile, e);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  int readDiskBlockInformation(String diskName, int defSector) {\n\n    assert perDiskSectorSize !\u003d null \u0026\u0026 diskName !\u003d null;\n\n    String procfsDiskSectorFile \u003d\n            \"/sys/block/\" + diskName + \"/queue/hw_sector_size\";\n\n    BufferedReader in;\n    try {\n      in \u003d new BufferedReader(new InputStreamReader(\n            new FileInputStream(procfsDiskSectorFile),\n              Charset.forName(\"UTF-8\")));\n    } catch (FileNotFoundException f) {\n      return defSector;\n    }\n\n    Matcher mat;\n    try {\n      String str \u003d in.readLine();\n      while (str !\u003d null) {\n        mat \u003d PROCFS_DISKSECTORFILE_FORMAT.matcher(str);\n        if (mat.find()) {\n          String secSize \u003d mat.group(1);\n          if (secSize !\u003d null) {\n            return Integer.parseInt(secSize);\n          }\n        }\n        str \u003d in.readLine();\n      }\n      return defSector;\n    } catch (IOException|NumberFormatException e) {\n      LOG.warn(\"Error reading the stream \" + procfsDiskSectorFile, e);\n      return defSector;\n    } finally {\n      // Close the streams\n      try {\n        in.close();\n      } catch (IOException e) {\n        LOG.warn(\"Error closing the stream \" + procfsDiskSectorFile, e);\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SysInfoLinux.java"
    }
  }
}