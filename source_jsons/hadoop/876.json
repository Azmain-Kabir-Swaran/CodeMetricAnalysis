{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSOutputStream.java",
  "functionName": "hflush",
  "functionId": "hflush",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
  "functionStartLine": 583,
  "functionEndLine": 587,
  "numCommitsSeen": 146,
  "timeTaken": 7857,
  "changeHistory": [
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
    "571da54179f731eb8421ffc681169799588f76bc",
    "83cf475050dba27e72b4e399491638c670621175",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": "Ybodychange",
    "571da54179f731eb8421ffc681169799588f76bc": "Ybodychange",
    "83cf475050dba27e72b4e399491638c670621175": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,5 @@\n   public void hflush() throws IOException {\n-    TraceScope scope \u003d\n-        dfsClient.newPathTraceScope(\"hflush\", src);\n-    try {\n+    try (TraceScope ignored \u003d dfsClient.newPathTraceScope(\"hflush\", src)) {\n       flushOrSync(false, EnumSet.noneOf(SyncFlag.class));\n-    } finally {\n-      scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void hflush() throws IOException {\n    try (TraceScope ignored \u003d dfsClient.newPathTraceScope(\"hflush\", src)) {\n      flushOrSync(false, EnumSet.noneOf(SyncFlag.class));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/09/15 11:08 AM",
      "commitNameOld": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.86,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,9 @@\n   public void hflush() throws IOException {\n     TraceScope scope \u003d\n-        dfsClient.getPathTraceScope(\"hflush\", src);\n+        dfsClient.newPathTraceScope(\"hflush\", src);\n     try {\n       flushOrSync(false, EnumSet.noneOf(SyncFlag.class));\n     } finally {\n       scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void hflush() throws IOException {\n    TraceScope scope \u003d\n        dfsClient.newPathTraceScope(\"hflush\", src);\n    try {\n      flushOrSync(false, EnumSet.noneOf(SyncFlag.class));\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void hflush() throws IOException {\n    TraceScope scope \u003d\n        dfsClient.getPathTraceScope(\"hflush\", src);\n    try {\n      flushOrSync(false, EnumSet.noneOf(SyncFlag.class));\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7054. Make DFSOutputStream tracing more fine-grained (cmccabe)\n",
      "commitDate": "18/03/15 6:14 PM",
      "commitName": "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "16/03/15 9:58 PM",
      "commitNameOld": "046521cd6511b7fc6d9478cb2bed90d8e75fca20",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,3 +1,9 @@\n   public void hflush() throws IOException {\n-    flushOrSync(false, EnumSet.noneOf(SyncFlag.class));\n+    TraceScope scope \u003d\n+        dfsClient.getPathTraceScope(\"hflush\", src);\n+    try {\n+      flushOrSync(false, EnumSet.noneOf(SyncFlag.class));\n+    } finally {\n+      scope.close();\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void hflush() throws IOException {\n    TraceScope scope \u003d\n        dfsClient.getPathTraceScope(\"hflush\", src);\n    try {\n      flushOrSync(false, EnumSet.noneOf(SyncFlag.class));\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "571da54179f731eb8421ffc681169799588f76bc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4213. Add an API to hsync for updating the last block length at the namenode. Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1415799 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/11/12 11:24 AM",
      "commitName": "571da54179f731eb8421ffc681169799588f76bc",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "28/10/12 4:10 PM",
      "commitNameOld": "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 32.84,
      "commitsBetweenForRepo": 162,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,3 +1,3 @@\n   public void hflush() throws IOException {\n-    flushOrSync(false);\n+    flushOrSync(false, EnumSet.noneOf(SyncFlag.class));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void hflush() throws IOException {\n    flushOrSync(false, EnumSet.noneOf(SyncFlag.class));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "83cf475050dba27e72b4e399491638c670621175": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-744. Support hsync in HDFS. Contributed by Lars Hofhans\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1344419 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/05/12 12:10 PM",
      "commitName": "83cf475050dba27e72b4e399491638c670621175",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "29/05/12 12:37 PM",
      "commitNameOld": "47a29c63291f1f9f09b89ce6f3305c0a2ef27b3f",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.98,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,89 +1,3 @@\n   public void hflush() throws IOException {\n-    dfsClient.checkOpen();\n-    isClosed();\n-    try {\n-      long toWaitFor;\n-      synchronized (this) {\n-        /* Record current blockOffset. This might be changed inside\n-         * flushBuffer() where a partial checksum chunk might be flushed.\n-         * After the flush, reset the bytesCurBlock back to its previous value,\n-         * any partial checksum chunk will be sent now and in next packet.\n-         */\n-        long saveOffset \u003d bytesCurBlock;\n-        Packet oldCurrentPacket \u003d currentPacket;\n-        // flush checksum buffer, but keep checksum buffer intact\n-        flushBuffer(true);\n-        // bytesCurBlock potentially incremented if there was buffered data\n-\n-        if (DFSClient.LOG.isDebugEnabled()) {\n-          DFSClient.LOG.debug(\n-            \"DFSClient flush() : saveOffset \" + saveOffset +  \n-            \" bytesCurBlock \" + bytesCurBlock +\n-            \" lastFlushOffset \" + lastFlushOffset);\n-        }\n-        // Flush only if we haven\u0027t already flushed till this offset.\n-        if (lastFlushOffset !\u003d bytesCurBlock) {\n-          assert bytesCurBlock \u003e lastFlushOffset;\n-          // record the valid offset of this flush\n-          lastFlushOffset \u003d bytesCurBlock;\n-          waitAndQueueCurrentPacket();\n-        } else {\n-          // We already flushed up to this offset.\n-          // This means that we haven\u0027t written anything since the last flush\n-          // (or the beginning of the file). Hence, we should not have any\n-          // packet queued prior to this call, since the last flush set\n-          // currentPacket \u003d null.\n-          assert oldCurrentPacket \u003d\u003d null :\n-            \"Empty flush should not occur with a currentPacket\";\n-\n-          // just discard the current packet since it is already been sent.\n-          currentPacket \u003d null;\n-        }\n-        // Restore state of stream. Record the last flush offset \n-        // of the last full chunk that was flushed.\n-        //\n-        bytesCurBlock \u003d saveOffset;\n-        toWaitFor \u003d lastQueuedSeqno;\n-      } // end synchronized\n-\n-      waitForAckedSeqno(toWaitFor);\n-\n-      // If any new blocks were allocated since the last flush, \n-      // then persist block locations on namenode. \n-      //\n-      if (persistBlocks.getAndSet(false)) {\n-        try {\n-          dfsClient.namenode.fsync(src, dfsClient.clientName);\n-        } catch (IOException ioe) {\n-          DFSClient.LOG.warn(\"Unable to persist blocks in hflush for \" + src, ioe);\n-          // If we got an error here, it might be because some other thread called\n-          // close before our hflush completed. In that case, we should throw an\n-          // exception that the stream is closed.\n-          isClosed();\n-          // If we aren\u0027t closed but failed to sync, we should expose that to the\n-          // caller.\n-          throw ioe;\n-        }\n-      }\n-\n-      synchronized(this) {\n-        if (streamer !\u003d null) {\n-          streamer.setHflush();\n-        }\n-      }\n-    } catch (InterruptedIOException interrupt) {\n-      // This kind of error doesn\u0027t mean that the stream itself is broken - just the\n-      // flushing thread got interrupted. So, we shouldn\u0027t close down the writer,\n-      // but instead just propagate the error\n-      throw interrupt;\n-    } catch (IOException e) {\n-      DFSClient.LOG.warn(\"Error while syncing\", e);\n-      synchronized (this) {\n-        if (!closed) {\n-          lastException \u003d new IOException(\"IOException flush:\" + e);\n-          closeThreads(true);\n-        }\n-      }\n-      throw e;\n-    }\n+    flushOrSync(false);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void hflush() throws IOException {\n    flushOrSync(false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void hflush() throws IOException {\n    dfsClient.checkOpen();\n    isClosed();\n    try {\n      long toWaitFor;\n      synchronized (this) {\n        /* Record current blockOffset. This might be changed inside\n         * flushBuffer() where a partial checksum chunk might be flushed.\n         * After the flush, reset the bytesCurBlock back to its previous value,\n         * any partial checksum chunk will be sent now and in next packet.\n         */\n        long saveOffset \u003d bytesCurBlock;\n        Packet oldCurrentPacket \u003d currentPacket;\n        // flush checksum buffer, but keep checksum buffer intact\n        flushBuffer(true);\n        // bytesCurBlock potentially incremented if there was buffered data\n\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\n            \"DFSClient flush() : saveOffset \" + saveOffset +  \n            \" bytesCurBlock \" + bytesCurBlock +\n            \" lastFlushOffset \" + lastFlushOffset);\n        }\n        // Flush only if we haven\u0027t already flushed till this offset.\n        if (lastFlushOffset !\u003d bytesCurBlock) {\n          assert bytesCurBlock \u003e lastFlushOffset;\n          // record the valid offset of this flush\n          lastFlushOffset \u003d bytesCurBlock;\n          waitAndQueueCurrentPacket();\n        } else {\n          // We already flushed up to this offset.\n          // This means that we haven\u0027t written anything since the last flush\n          // (or the beginning of the file). Hence, we should not have any\n          // packet queued prior to this call, since the last flush set\n          // currentPacket \u003d null.\n          assert oldCurrentPacket \u003d\u003d null :\n            \"Empty flush should not occur with a currentPacket\";\n\n          // just discard the current packet since it is already been sent.\n          currentPacket \u003d null;\n        }\n        // Restore state of stream. Record the last flush offset \n        // of the last full chunk that was flushed.\n        //\n        bytesCurBlock \u003d saveOffset;\n        toWaitFor \u003d lastQueuedSeqno;\n      } // end synchronized\n\n      waitForAckedSeqno(toWaitFor);\n\n      // If any new blocks were allocated since the last flush, \n      // then persist block locations on namenode. \n      //\n      if (persistBlocks.getAndSet(false)) {\n        try {\n          dfsClient.namenode.fsync(src, dfsClient.clientName);\n        } catch (IOException ioe) {\n          DFSClient.LOG.warn(\"Unable to persist blocks in hflush for \" + src, ioe);\n          // If we got an error here, it might be because some other thread called\n          // close before our hflush completed. In that case, we should throw an\n          // exception that the stream is closed.\n          isClosed();\n          // If we aren\u0027t closed but failed to sync, we should expose that to the\n          // caller.\n          throw ioe;\n        }\n      }\n\n      synchronized(this) {\n        if (streamer !\u003d null) {\n          streamer.setHflush();\n        }\n      }\n    } catch (InterruptedIOException interrupt) {\n      // This kind of error doesn\u0027t mean that the stream itself is broken - just the\n      // flushing thread got interrupted. So, we shouldn\u0027t close down the writer,\n      // but instead just propagate the error\n      throw interrupt;\n    } catch (IOException e) {\n      DFSClient.LOG.warn(\"Error while syncing\", e);\n      synchronized (this) {\n        if (!closed) {\n          lastException \u003d new IOException(\"IOException flush:\" + e);\n          closeThreads(true);\n        }\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void hflush() throws IOException {\n    dfsClient.checkOpen();\n    isClosed();\n    try {\n      long toWaitFor;\n      synchronized (this) {\n        /* Record current blockOffset. This might be changed inside\n         * flushBuffer() where a partial checksum chunk might be flushed.\n         * After the flush, reset the bytesCurBlock back to its previous value,\n         * any partial checksum chunk will be sent now and in next packet.\n         */\n        long saveOffset \u003d bytesCurBlock;\n        Packet oldCurrentPacket \u003d currentPacket;\n        // flush checksum buffer, but keep checksum buffer intact\n        flushBuffer(true);\n        // bytesCurBlock potentially incremented if there was buffered data\n\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\n            \"DFSClient flush() : saveOffset \" + saveOffset +  \n            \" bytesCurBlock \" + bytesCurBlock +\n            \" lastFlushOffset \" + lastFlushOffset);\n        }\n        // Flush only if we haven\u0027t already flushed till this offset.\n        if (lastFlushOffset !\u003d bytesCurBlock) {\n          assert bytesCurBlock \u003e lastFlushOffset;\n          // record the valid offset of this flush\n          lastFlushOffset \u003d bytesCurBlock;\n          waitAndQueueCurrentPacket();\n        } else {\n          // We already flushed up to this offset.\n          // This means that we haven\u0027t written anything since the last flush\n          // (or the beginning of the file). Hence, we should not have any\n          // packet queued prior to this call, since the last flush set\n          // currentPacket \u003d null.\n          assert oldCurrentPacket \u003d\u003d null :\n            \"Empty flush should not occur with a currentPacket\";\n\n          // just discard the current packet since it is already been sent.\n          currentPacket \u003d null;\n        }\n        // Restore state of stream. Record the last flush offset \n        // of the last full chunk that was flushed.\n        //\n        bytesCurBlock \u003d saveOffset;\n        toWaitFor \u003d lastQueuedSeqno;\n      } // end synchronized\n\n      waitForAckedSeqno(toWaitFor);\n\n      // If any new blocks were allocated since the last flush, \n      // then persist block locations on namenode. \n      //\n      if (persistBlocks.getAndSet(false)) {\n        try {\n          dfsClient.namenode.fsync(src, dfsClient.clientName);\n        } catch (IOException ioe) {\n          DFSClient.LOG.warn(\"Unable to persist blocks in hflush for \" + src, ioe);\n          // If we got an error here, it might be because some other thread called\n          // close before our hflush completed. In that case, we should throw an\n          // exception that the stream is closed.\n          isClosed();\n          // If we aren\u0027t closed but failed to sync, we should expose that to the\n          // caller.\n          throw ioe;\n        }\n      }\n\n      synchronized(this) {\n        if (streamer !\u003d null) {\n          streamer.setHflush();\n        }\n      }\n    } catch (InterruptedIOException interrupt) {\n      // This kind of error doesn\u0027t mean that the stream itself is broken - just the\n      // flushing thread got interrupted. So, we shouldn\u0027t close down the writer,\n      // but instead just propagate the error\n      throw interrupt;\n    } catch (IOException e) {\n      DFSClient.LOG.warn(\"Error while syncing\", e);\n      synchronized (this) {\n        if (!closed) {\n          lastException \u003d new IOException(\"IOException flush:\" + e);\n          closeThreads(true);\n        }\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,89 @@\n+  public void hflush() throws IOException {\n+    dfsClient.checkOpen();\n+    isClosed();\n+    try {\n+      long toWaitFor;\n+      synchronized (this) {\n+        /* Record current blockOffset. This might be changed inside\n+         * flushBuffer() where a partial checksum chunk might be flushed.\n+         * After the flush, reset the bytesCurBlock back to its previous value,\n+         * any partial checksum chunk will be sent now and in next packet.\n+         */\n+        long saveOffset \u003d bytesCurBlock;\n+        Packet oldCurrentPacket \u003d currentPacket;\n+        // flush checksum buffer, but keep checksum buffer intact\n+        flushBuffer(true);\n+        // bytesCurBlock potentially incremented if there was buffered data\n+\n+        if (DFSClient.LOG.isDebugEnabled()) {\n+          DFSClient.LOG.debug(\n+            \"DFSClient flush() : saveOffset \" + saveOffset +  \n+            \" bytesCurBlock \" + bytesCurBlock +\n+            \" lastFlushOffset \" + lastFlushOffset);\n+        }\n+        // Flush only if we haven\u0027t already flushed till this offset.\n+        if (lastFlushOffset !\u003d bytesCurBlock) {\n+          assert bytesCurBlock \u003e lastFlushOffset;\n+          // record the valid offset of this flush\n+          lastFlushOffset \u003d bytesCurBlock;\n+          waitAndQueueCurrentPacket();\n+        } else {\n+          // We already flushed up to this offset.\n+          // This means that we haven\u0027t written anything since the last flush\n+          // (or the beginning of the file). Hence, we should not have any\n+          // packet queued prior to this call, since the last flush set\n+          // currentPacket \u003d null.\n+          assert oldCurrentPacket \u003d\u003d null :\n+            \"Empty flush should not occur with a currentPacket\";\n+\n+          // just discard the current packet since it is already been sent.\n+          currentPacket \u003d null;\n+        }\n+        // Restore state of stream. Record the last flush offset \n+        // of the last full chunk that was flushed.\n+        //\n+        bytesCurBlock \u003d saveOffset;\n+        toWaitFor \u003d lastQueuedSeqno;\n+      } // end synchronized\n+\n+      waitForAckedSeqno(toWaitFor);\n+\n+      // If any new blocks were allocated since the last flush, \n+      // then persist block locations on namenode. \n+      //\n+      if (persistBlocks.getAndSet(false)) {\n+        try {\n+          dfsClient.namenode.fsync(src, dfsClient.clientName);\n+        } catch (IOException ioe) {\n+          DFSClient.LOG.warn(\"Unable to persist blocks in hflush for \" + src, ioe);\n+          // If we got an error here, it might be because some other thread called\n+          // close before our hflush completed. In that case, we should throw an\n+          // exception that the stream is closed.\n+          isClosed();\n+          // If we aren\u0027t closed but failed to sync, we should expose that to the\n+          // caller.\n+          throw ioe;\n+        }\n+      }\n+\n+      synchronized(this) {\n+        if (streamer !\u003d null) {\n+          streamer.setHflush();\n+        }\n+      }\n+    } catch (InterruptedIOException interrupt) {\n+      // This kind of error doesn\u0027t mean that the stream itself is broken - just the\n+      // flushing thread got interrupted. So, we shouldn\u0027t close down the writer,\n+      // but instead just propagate the error\n+      throw interrupt;\n+    } catch (IOException e) {\n+      DFSClient.LOG.warn(\"Error while syncing\", e);\n+      synchronized (this) {\n+        if (!closed) {\n+          lastException \u003d new IOException(\"IOException flush:\" + e);\n+          closeThreads(true);\n+        }\n+      }\n+      throw e;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void hflush() throws IOException {\n    dfsClient.checkOpen();\n    isClosed();\n    try {\n      long toWaitFor;\n      synchronized (this) {\n        /* Record current blockOffset. This might be changed inside\n         * flushBuffer() where a partial checksum chunk might be flushed.\n         * After the flush, reset the bytesCurBlock back to its previous value,\n         * any partial checksum chunk will be sent now and in next packet.\n         */\n        long saveOffset \u003d bytesCurBlock;\n        Packet oldCurrentPacket \u003d currentPacket;\n        // flush checksum buffer, but keep checksum buffer intact\n        flushBuffer(true);\n        // bytesCurBlock potentially incremented if there was buffered data\n\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\n            \"DFSClient flush() : saveOffset \" + saveOffset +  \n            \" bytesCurBlock \" + bytesCurBlock +\n            \" lastFlushOffset \" + lastFlushOffset);\n        }\n        // Flush only if we haven\u0027t already flushed till this offset.\n        if (lastFlushOffset !\u003d bytesCurBlock) {\n          assert bytesCurBlock \u003e lastFlushOffset;\n          // record the valid offset of this flush\n          lastFlushOffset \u003d bytesCurBlock;\n          waitAndQueueCurrentPacket();\n        } else {\n          // We already flushed up to this offset.\n          // This means that we haven\u0027t written anything since the last flush\n          // (or the beginning of the file). Hence, we should not have any\n          // packet queued prior to this call, since the last flush set\n          // currentPacket \u003d null.\n          assert oldCurrentPacket \u003d\u003d null :\n            \"Empty flush should not occur with a currentPacket\";\n\n          // just discard the current packet since it is already been sent.\n          currentPacket \u003d null;\n        }\n        // Restore state of stream. Record the last flush offset \n        // of the last full chunk that was flushed.\n        //\n        bytesCurBlock \u003d saveOffset;\n        toWaitFor \u003d lastQueuedSeqno;\n      } // end synchronized\n\n      waitForAckedSeqno(toWaitFor);\n\n      // If any new blocks were allocated since the last flush, \n      // then persist block locations on namenode. \n      //\n      if (persistBlocks.getAndSet(false)) {\n        try {\n          dfsClient.namenode.fsync(src, dfsClient.clientName);\n        } catch (IOException ioe) {\n          DFSClient.LOG.warn(\"Unable to persist blocks in hflush for \" + src, ioe);\n          // If we got an error here, it might be because some other thread called\n          // close before our hflush completed. In that case, we should throw an\n          // exception that the stream is closed.\n          isClosed();\n          // If we aren\u0027t closed but failed to sync, we should expose that to the\n          // caller.\n          throw ioe;\n        }\n      }\n\n      synchronized(this) {\n        if (streamer !\u003d null) {\n          streamer.setHflush();\n        }\n      }\n    } catch (InterruptedIOException interrupt) {\n      // This kind of error doesn\u0027t mean that the stream itself is broken - just the\n      // flushing thread got interrupted. So, we shouldn\u0027t close down the writer,\n      // but instead just propagate the error\n      throw interrupt;\n    } catch (IOException e) {\n      DFSClient.LOG.warn(\"Error while syncing\", e);\n      synchronized (this) {\n        if (!closed) {\n          lastException \u003d new IOException(\"IOException flush:\" + e);\n          closeThreads(true);\n        }\n      }\n      throw e;\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}