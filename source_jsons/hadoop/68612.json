{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StreamJob.java",
  "functionName": "packageJobJar",
  "functionId": "packageJobJar",
  "sourceFilePath": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
  "functionStartLine": 671,
  "functionEndLine": 721,
  "numCommitsSeen": 22,
  "timeTaken": 5035,
  "changeHistory": [
    "0a74610d1c7c7f183d2b2d0b7a775add53cf6c94",
    "26447229ba2c3d43db978c1b3ce95613669182ee",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "0a74610d1c7c7f183d2b2d0b7a775add53cf6c94": "Ybodychange",
    "26447229ba2c3d43db978c1b3ce95613669182ee": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "0a74610d1c7c7f183d2b2d0b7a775add53cf6c94": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11393. Revert HADOOP_PREFIX, go back to HADOOP_HOME (aw)\n",
      "commitDate": "31/03/16 7:51 AM",
      "commitName": "0a74610d1c7c7f183d2b2d0b7a775add53cf6c94",
      "commitAuthor": "Allen Wittenauer",
      "commitDateOld": "03/06/15 6:41 PM",
      "commitNameOld": "cc70df98e74142331043a611a3bd8a53ff6a2242",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 301.55,
      "commitsBetweenForRepo": 2013,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   protected String packageJobJar() throws IOException {\n     ArrayList\u003cString\u003e unjarFiles \u003d new ArrayList\u003cString\u003e();\n \n     // Runtime code: ship same version of code as self (job submitter code)\n     // usually found in: build/contrib or build/hadoop-\u003cversion\u003e-dev-streaming.jar\n \n     // First try an explicit spec: it\u0027s too hard to find our own location in this case:\n-    // $HADOOP_PREFIX/bin/hadoop jar /not/first/on/classpath/custom-hadoop-streaming.jar\n-    // where findInClasspath() would find the version of hadoop-streaming.jar in $HADOOP_PREFIX\n+    // $HADOOP_HOME/bin/hadoop jar /not/first/on/classpath/custom-hadoop-streaming.jar\n+    // where findInClasspath() would find the version of hadoop-streaming.jar in $HADOOP_HOME\n     String runtimeClasses \u003d config_.get(\"stream.shipped.hadoopstreaming\"); // jar or class dir\n \n     if (runtimeClasses \u003d\u003d null) {\n       runtimeClasses \u003d StreamUtil.findInClasspath(StreamJob.class.getName());\n     }\n     if (runtimeClasses \u003d\u003d null) {\n       throw new IOException(\"runtime classes not found: \" + getClass().getPackage());\n     } else {\n       msg(\"Found runtime classes in: \" + runtimeClasses);\n     }\n     if (isLocalHadoop()) {\n       // don\u0027t package class files (they might get unpackaged in \".\" and then\n       //  hide the intended CLASSPATH entry)\n       // we still package everything else (so that scripts and executable are found in\n       //  Task workdir like distributed Hadoop)\n     } else {\n       if (new File(runtimeClasses).isDirectory()) {\n         packageFiles_.add(runtimeClasses);\n       } else {\n         unjarFiles.add(runtimeClasses);\n       }\n     }\n     if (packageFiles_.size() + unjarFiles.size() \u003d\u003d 0) {\n       return null;\n     }\n     String tmp \u003d jobConf_.get(\"stream.tmpdir\"); //, \"/tmp/${mapreduce.job.user.name}/\"\n     File tmpDir \u003d (tmp \u003d\u003d null) ? null : new File(tmp);\n     // tmpDir\u003dnull means OS default tmp dir\n     File jobJar \u003d File.createTempFile(\"streamjob\", \".jar\", tmpDir);\n     System.out.println(\"packageJobJar: \" + packageFiles_ + \" \" + unjarFiles + \" \" + jobJar\n                        + \" tmpDir\u003d\" + tmpDir);\n     if (debug_ \u003d\u003d 0) {\n       jobJar.deleteOnExit();\n     }\n     JarBuilder builder \u003d new JarBuilder();\n     if (verbose_) {\n       builder.setVerbose(true);\n     }\n     String jobJarName \u003d jobJar.getAbsolutePath();\n     builder.merge(packageFiles_, unjarFiles, jobJarName);\n     return jobJarName;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected String packageJobJar() throws IOException {\n    ArrayList\u003cString\u003e unjarFiles \u003d new ArrayList\u003cString\u003e();\n\n    // Runtime code: ship same version of code as self (job submitter code)\n    // usually found in: build/contrib or build/hadoop-\u003cversion\u003e-dev-streaming.jar\n\n    // First try an explicit spec: it\u0027s too hard to find our own location in this case:\n    // $HADOOP_HOME/bin/hadoop jar /not/first/on/classpath/custom-hadoop-streaming.jar\n    // where findInClasspath() would find the version of hadoop-streaming.jar in $HADOOP_HOME\n    String runtimeClasses \u003d config_.get(\"stream.shipped.hadoopstreaming\"); // jar or class dir\n\n    if (runtimeClasses \u003d\u003d null) {\n      runtimeClasses \u003d StreamUtil.findInClasspath(StreamJob.class.getName());\n    }\n    if (runtimeClasses \u003d\u003d null) {\n      throw new IOException(\"runtime classes not found: \" + getClass().getPackage());\n    } else {\n      msg(\"Found runtime classes in: \" + runtimeClasses);\n    }\n    if (isLocalHadoop()) {\n      // don\u0027t package class files (they might get unpackaged in \".\" and then\n      //  hide the intended CLASSPATH entry)\n      // we still package everything else (so that scripts and executable are found in\n      //  Task workdir like distributed Hadoop)\n    } else {\n      if (new File(runtimeClasses).isDirectory()) {\n        packageFiles_.add(runtimeClasses);\n      } else {\n        unjarFiles.add(runtimeClasses);\n      }\n    }\n    if (packageFiles_.size() + unjarFiles.size() \u003d\u003d 0) {\n      return null;\n    }\n    String tmp \u003d jobConf_.get(\"stream.tmpdir\"); //, \"/tmp/${mapreduce.job.user.name}/\"\n    File tmpDir \u003d (tmp \u003d\u003d null) ? null : new File(tmp);\n    // tmpDir\u003dnull means OS default tmp dir\n    File jobJar \u003d File.createTempFile(\"streamjob\", \".jar\", tmpDir);\n    System.out.println(\"packageJobJar: \" + packageFiles_ + \" \" + unjarFiles + \" \" + jobJar\n                       + \" tmpDir\u003d\" + tmpDir);\n    if (debug_ \u003d\u003d 0) {\n      jobJar.deleteOnExit();\n    }\n    JarBuilder builder \u003d new JarBuilder();\n    if (verbose_) {\n      builder.setVerbose(true);\n    }\n    String jobJarName \u003d jobJar.getAbsolutePath();\n    builder.merge(packageFiles_, unjarFiles, jobJarName);\n    return jobJarName;\n  }",
      "path": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {}
    },
    "26447229ba2c3d43db978c1b3ce95613669182ee": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7590. Mavenize streaming and MR examples. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203941 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 5:24 PM",
      "commitName": "26447229ba2c3d43db978c1b3ce95613669182ee",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "18/11/11 1:04 AM",
      "commitNameOld": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.68,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   protected String packageJobJar() throws IOException {\n     ArrayList\u003cString\u003e unjarFiles \u003d new ArrayList\u003cString\u003e();\n \n     // Runtime code: ship same version of code as self (job submitter code)\n     // usually found in: build/contrib or build/hadoop-\u003cversion\u003e-dev-streaming.jar\n \n     // First try an explicit spec: it\u0027s too hard to find our own location in this case:\n     // $HADOOP_PREFIX/bin/hadoop jar /not/first/on/classpath/custom-hadoop-streaming.jar\n     // where findInClasspath() would find the version of hadoop-streaming.jar in $HADOOP_PREFIX\n     String runtimeClasses \u003d config_.get(\"stream.shipped.hadoopstreaming\"); // jar or class dir\n-    \n+\n     if (runtimeClasses \u003d\u003d null) {\n       runtimeClasses \u003d StreamUtil.findInClasspath(StreamJob.class.getName());\n     }\n     if (runtimeClasses \u003d\u003d null) {\n       throw new IOException(\"runtime classes not found: \" + getClass().getPackage());\n     } else {\n       msg(\"Found runtime classes in: \" + runtimeClasses);\n     }\n     if (isLocalHadoop()) {\n       // don\u0027t package class files (they might get unpackaged in \".\" and then\n       //  hide the intended CLASSPATH entry)\n       // we still package everything else (so that scripts and executable are found in\n       //  Task workdir like distributed Hadoop)\n     } else {\n       if (new File(runtimeClasses).isDirectory()) {\n         packageFiles_.add(runtimeClasses);\n       } else {\n         unjarFiles.add(runtimeClasses);\n       }\n     }\n     if (packageFiles_.size() + unjarFiles.size() \u003d\u003d 0) {\n       return null;\n     }\n     String tmp \u003d jobConf_.get(\"stream.tmpdir\"); //, \"/tmp/${mapreduce.job.user.name}/\"\n     File tmpDir \u003d (tmp \u003d\u003d null) ? null : new File(tmp);\n     // tmpDir\u003dnull means OS default tmp dir\n     File jobJar \u003d File.createTempFile(\"streamjob\", \".jar\", tmpDir);\n     System.out.println(\"packageJobJar: \" + packageFiles_ + \" \" + unjarFiles + \" \" + jobJar\n                        + \" tmpDir\u003d\" + tmpDir);\n     if (debug_ \u003d\u003d 0) {\n       jobJar.deleteOnExit();\n     }\n     JarBuilder builder \u003d new JarBuilder();\n     if (verbose_) {\n       builder.setVerbose(true);\n     }\n     String jobJarName \u003d jobJar.getAbsolutePath();\n     builder.merge(packageFiles_, unjarFiles, jobJarName);\n     return jobJarName;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected String packageJobJar() throws IOException {\n    ArrayList\u003cString\u003e unjarFiles \u003d new ArrayList\u003cString\u003e();\n\n    // Runtime code: ship same version of code as self (job submitter code)\n    // usually found in: build/contrib or build/hadoop-\u003cversion\u003e-dev-streaming.jar\n\n    // First try an explicit spec: it\u0027s too hard to find our own location in this case:\n    // $HADOOP_PREFIX/bin/hadoop jar /not/first/on/classpath/custom-hadoop-streaming.jar\n    // where findInClasspath() would find the version of hadoop-streaming.jar in $HADOOP_PREFIX\n    String runtimeClasses \u003d config_.get(\"stream.shipped.hadoopstreaming\"); // jar or class dir\n\n    if (runtimeClasses \u003d\u003d null) {\n      runtimeClasses \u003d StreamUtil.findInClasspath(StreamJob.class.getName());\n    }\n    if (runtimeClasses \u003d\u003d null) {\n      throw new IOException(\"runtime classes not found: \" + getClass().getPackage());\n    } else {\n      msg(\"Found runtime classes in: \" + runtimeClasses);\n    }\n    if (isLocalHadoop()) {\n      // don\u0027t package class files (they might get unpackaged in \".\" and then\n      //  hide the intended CLASSPATH entry)\n      // we still package everything else (so that scripts and executable are found in\n      //  Task workdir like distributed Hadoop)\n    } else {\n      if (new File(runtimeClasses).isDirectory()) {\n        packageFiles_.add(runtimeClasses);\n      } else {\n        unjarFiles.add(runtimeClasses);\n      }\n    }\n    if (packageFiles_.size() + unjarFiles.size() \u003d\u003d 0) {\n      return null;\n    }\n    String tmp \u003d jobConf_.get(\"stream.tmpdir\"); //, \"/tmp/${mapreduce.job.user.name}/\"\n    File tmpDir \u003d (tmp \u003d\u003d null) ? null : new File(tmp);\n    // tmpDir\u003dnull means OS default tmp dir\n    File jobJar \u003d File.createTempFile(\"streamjob\", \".jar\", tmpDir);\n    System.out.println(\"packageJobJar: \" + packageFiles_ + \" \" + unjarFiles + \" \" + jobJar\n                       + \" tmpDir\u003d\" + tmpDir);\n    if (debug_ \u003d\u003d 0) {\n      jobJar.deleteOnExit();\n    }\n    JarBuilder builder \u003d new JarBuilder();\n    if (verbose_) {\n      builder.setVerbose(true);\n    }\n    String jobJarName \u003d jobJar.getAbsolutePath();\n    builder.merge(packageFiles_, unjarFiles, jobJarName);\n    return jobJarName;\n  }",
      "path": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected String packageJobJar() throws IOException {\n    ArrayList\u003cString\u003e unjarFiles \u003d new ArrayList\u003cString\u003e();\n\n    // Runtime code: ship same version of code as self (job submitter code)\n    // usually found in: build/contrib or build/hadoop-\u003cversion\u003e-dev-streaming.jar\n\n    // First try an explicit spec: it\u0027s too hard to find our own location in this case:\n    // $HADOOP_PREFIX/bin/hadoop jar /not/first/on/classpath/custom-hadoop-streaming.jar\n    // where findInClasspath() would find the version of hadoop-streaming.jar in $HADOOP_PREFIX\n    String runtimeClasses \u003d config_.get(\"stream.shipped.hadoopstreaming\"); // jar or class dir\n    \n    if (runtimeClasses \u003d\u003d null) {\n      runtimeClasses \u003d StreamUtil.findInClasspath(StreamJob.class.getName());\n    }\n    if (runtimeClasses \u003d\u003d null) {\n      throw new IOException(\"runtime classes not found: \" + getClass().getPackage());\n    } else {\n      msg(\"Found runtime classes in: \" + runtimeClasses);\n    }\n    if (isLocalHadoop()) {\n      // don\u0027t package class files (they might get unpackaged in \".\" and then\n      //  hide the intended CLASSPATH entry)\n      // we still package everything else (so that scripts and executable are found in\n      //  Task workdir like distributed Hadoop)\n    } else {\n      if (new File(runtimeClasses).isDirectory()) {\n        packageFiles_.add(runtimeClasses);\n      } else {\n        unjarFiles.add(runtimeClasses);\n      }\n    }\n    if (packageFiles_.size() + unjarFiles.size() \u003d\u003d 0) {\n      return null;\n    }\n    String tmp \u003d jobConf_.get(\"stream.tmpdir\"); //, \"/tmp/${mapreduce.job.user.name}/\"\n    File tmpDir \u003d (tmp \u003d\u003d null) ? null : new File(tmp);\n    // tmpDir\u003dnull means OS default tmp dir\n    File jobJar \u003d File.createTempFile(\"streamjob\", \".jar\", tmpDir);\n    System.out.println(\"packageJobJar: \" + packageFiles_ + \" \" + unjarFiles + \" \" + jobJar\n                       + \" tmpDir\u003d\" + tmpDir);\n    if (debug_ \u003d\u003d 0) {\n      jobJar.deleteOnExit();\n    }\n    JarBuilder builder \u003d new JarBuilder();\n    if (verbose_) {\n      builder.setVerbose(true);\n    }\n    String jobJarName \u003d jobJar.getAbsolutePath();\n    builder.merge(packageFiles_, unjarFiles, jobJarName);\n    return jobJarName;\n  }",
      "path": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected String packageJobJar() throws IOException {\n    ArrayList\u003cString\u003e unjarFiles \u003d new ArrayList\u003cString\u003e();\n\n    // Runtime code: ship same version of code as self (job submitter code)\n    // usually found in: build/contrib or build/hadoop-\u003cversion\u003e-dev-streaming.jar\n\n    // First try an explicit spec: it\u0027s too hard to find our own location in this case:\n    // $HADOOP_PREFIX/bin/hadoop jar /not/first/on/classpath/custom-hadoop-streaming.jar\n    // where findInClasspath() would find the version of hadoop-streaming.jar in $HADOOP_PREFIX\n    String runtimeClasses \u003d config_.get(\"stream.shipped.hadoopstreaming\"); // jar or class dir\n    \n    if (runtimeClasses \u003d\u003d null) {\n      runtimeClasses \u003d StreamUtil.findInClasspath(StreamJob.class.getName());\n    }\n    if (runtimeClasses \u003d\u003d null) {\n      throw new IOException(\"runtime classes not found: \" + getClass().getPackage());\n    } else {\n      msg(\"Found runtime classes in: \" + runtimeClasses);\n    }\n    if (isLocalHadoop()) {\n      // don\u0027t package class files (they might get unpackaged in \".\" and then\n      //  hide the intended CLASSPATH entry)\n      // we still package everything else (so that scripts and executable are found in\n      //  Task workdir like distributed Hadoop)\n    } else {\n      if (new File(runtimeClasses).isDirectory()) {\n        packageFiles_.add(runtimeClasses);\n      } else {\n        unjarFiles.add(runtimeClasses);\n      }\n    }\n    if (packageFiles_.size() + unjarFiles.size() \u003d\u003d 0) {\n      return null;\n    }\n    String tmp \u003d jobConf_.get(\"stream.tmpdir\"); //, \"/tmp/${mapreduce.job.user.name}/\"\n    File tmpDir \u003d (tmp \u003d\u003d null) ? null : new File(tmp);\n    // tmpDir\u003dnull means OS default tmp dir\n    File jobJar \u003d File.createTempFile(\"streamjob\", \".jar\", tmpDir);\n    System.out.println(\"packageJobJar: \" + packageFiles_ + \" \" + unjarFiles + \" \" + jobJar\n                       + \" tmpDir\u003d\" + tmpDir);\n    if (debug_ \u003d\u003d 0) {\n      jobJar.deleteOnExit();\n    }\n    JarBuilder builder \u003d new JarBuilder();\n    if (verbose_) {\n      builder.setVerbose(true);\n    }\n    String jobJarName \u003d jobJar.getAbsolutePath();\n    builder.merge(packageFiles_, unjarFiles, jobJarName);\n    return jobJarName;\n  }",
      "path": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,51 @@\n+  protected String packageJobJar() throws IOException {\n+    ArrayList\u003cString\u003e unjarFiles \u003d new ArrayList\u003cString\u003e();\n+\n+    // Runtime code: ship same version of code as self (job submitter code)\n+    // usually found in: build/contrib or build/hadoop-\u003cversion\u003e-dev-streaming.jar\n+\n+    // First try an explicit spec: it\u0027s too hard to find our own location in this case:\n+    // $HADOOP_PREFIX/bin/hadoop jar /not/first/on/classpath/custom-hadoop-streaming.jar\n+    // where findInClasspath() would find the version of hadoop-streaming.jar in $HADOOP_PREFIX\n+    String runtimeClasses \u003d config_.get(\"stream.shipped.hadoopstreaming\"); // jar or class dir\n+    \n+    if (runtimeClasses \u003d\u003d null) {\n+      runtimeClasses \u003d StreamUtil.findInClasspath(StreamJob.class.getName());\n+    }\n+    if (runtimeClasses \u003d\u003d null) {\n+      throw new IOException(\"runtime classes not found: \" + getClass().getPackage());\n+    } else {\n+      msg(\"Found runtime classes in: \" + runtimeClasses);\n+    }\n+    if (isLocalHadoop()) {\n+      // don\u0027t package class files (they might get unpackaged in \".\" and then\n+      //  hide the intended CLASSPATH entry)\n+      // we still package everything else (so that scripts and executable are found in\n+      //  Task workdir like distributed Hadoop)\n+    } else {\n+      if (new File(runtimeClasses).isDirectory()) {\n+        packageFiles_.add(runtimeClasses);\n+      } else {\n+        unjarFiles.add(runtimeClasses);\n+      }\n+    }\n+    if (packageFiles_.size() + unjarFiles.size() \u003d\u003d 0) {\n+      return null;\n+    }\n+    String tmp \u003d jobConf_.get(\"stream.tmpdir\"); //, \"/tmp/${mapreduce.job.user.name}/\"\n+    File tmpDir \u003d (tmp \u003d\u003d null) ? null : new File(tmp);\n+    // tmpDir\u003dnull means OS default tmp dir\n+    File jobJar \u003d File.createTempFile(\"streamjob\", \".jar\", tmpDir);\n+    System.out.println(\"packageJobJar: \" + packageFiles_ + \" \" + unjarFiles + \" \" + jobJar\n+                       + \" tmpDir\u003d\" + tmpDir);\n+    if (debug_ \u003d\u003d 0) {\n+      jobJar.deleteOnExit();\n+    }\n+    JarBuilder builder \u003d new JarBuilder();\n+    if (verbose_) {\n+      builder.setVerbose(true);\n+    }\n+    String jobJarName \u003d jobJar.getAbsolutePath();\n+    builder.merge(packageFiles_, unjarFiles, jobJarName);\n+    return jobJarName;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected String packageJobJar() throws IOException {\n    ArrayList\u003cString\u003e unjarFiles \u003d new ArrayList\u003cString\u003e();\n\n    // Runtime code: ship same version of code as self (job submitter code)\n    // usually found in: build/contrib or build/hadoop-\u003cversion\u003e-dev-streaming.jar\n\n    // First try an explicit spec: it\u0027s too hard to find our own location in this case:\n    // $HADOOP_PREFIX/bin/hadoop jar /not/first/on/classpath/custom-hadoop-streaming.jar\n    // where findInClasspath() would find the version of hadoop-streaming.jar in $HADOOP_PREFIX\n    String runtimeClasses \u003d config_.get(\"stream.shipped.hadoopstreaming\"); // jar or class dir\n    \n    if (runtimeClasses \u003d\u003d null) {\n      runtimeClasses \u003d StreamUtil.findInClasspath(StreamJob.class.getName());\n    }\n    if (runtimeClasses \u003d\u003d null) {\n      throw new IOException(\"runtime classes not found: \" + getClass().getPackage());\n    } else {\n      msg(\"Found runtime classes in: \" + runtimeClasses);\n    }\n    if (isLocalHadoop()) {\n      // don\u0027t package class files (they might get unpackaged in \".\" and then\n      //  hide the intended CLASSPATH entry)\n      // we still package everything else (so that scripts and executable are found in\n      //  Task workdir like distributed Hadoop)\n    } else {\n      if (new File(runtimeClasses).isDirectory()) {\n        packageFiles_.add(runtimeClasses);\n      } else {\n        unjarFiles.add(runtimeClasses);\n      }\n    }\n    if (packageFiles_.size() + unjarFiles.size() \u003d\u003d 0) {\n      return null;\n    }\n    String tmp \u003d jobConf_.get(\"stream.tmpdir\"); //, \"/tmp/${mapreduce.job.user.name}/\"\n    File tmpDir \u003d (tmp \u003d\u003d null) ? null : new File(tmp);\n    // tmpDir\u003dnull means OS default tmp dir\n    File jobJar \u003d File.createTempFile(\"streamjob\", \".jar\", tmpDir);\n    System.out.println(\"packageJobJar: \" + packageFiles_ + \" \" + unjarFiles + \" \" + jobJar\n                       + \" tmpDir\u003d\" + tmpDir);\n    if (debug_ \u003d\u003d 0) {\n      jobJar.deleteOnExit();\n    }\n    JarBuilder builder \u003d new JarBuilder();\n    if (verbose_) {\n      builder.setVerbose(true);\n    }\n    String jobJarName \u003d jobJar.getAbsolutePath();\n    builder.merge(packageFiles_, unjarFiles, jobJarName);\n    return jobJarName;\n  }",
      "path": "mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
    }
  }
}