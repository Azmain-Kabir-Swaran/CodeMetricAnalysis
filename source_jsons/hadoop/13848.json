{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "processReport",
  "functionId": "processReport___nodeID-DatanodeID(modifiers-final)__storage-DatanodeStorage(modifiers-final)__newReport-BlockListAsLongs(modifiers-final)__context-BlockReportContext",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 2717,
  "functionEndLine": 2796,
  "numCommitsSeen": 1832,
  "timeTaken": 24158,
  "changeHistory": [
    "6822193ee6d6ac8b08822fa76c89e1dd61c5ddca",
    "7314185c4a313842115e18b5f42d118392cee929",
    "ae4143a529d74d94f205ca627c31360abfa11bfa",
    "8239e3afb31d3c4485817d4b8b8b195b554acbe7",
    "3b1d30301bcd35bbe525a7e122d3e5acfab92c88",
    "546b95f4843f3cbbbdf72d90d202cad551696082",
    "d65df0f27395792c6e25f5e03b6ba1765e2ba925",
    "c22cf004425daa9c350df5e365b0db85b1628b40",
    "391ce535a739dc92cb90017d759217265a4fd969",
    "10e84c6a6e831fe2bea061fb21bd0dfe32bc9953",
    "c4463f2ef20d2cb634a1249246f83c451975f3dc",
    "85a20508bd04851d47c24b7562ec2927d5403446",
    "d5abd293a890a8a1da48a166a291ae1c5644ad57",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
    "f741476146574550a1a208d58ef8be76639e5ddc",
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
    "b5ce87f84d9de0a5347ab38c0567a5a70d1fbfd7",
    "12b5b06c063d93e6c683c9b6fac9a96912f59e59",
    "281d47a96937bc329b1b4051ffcb8f5fcac98354",
    "f9427f1760cce7e0befc3e066cebd0912652a411",
    "50ee8f4e67a66aa77c5359182f61f3e951844db6",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
    "b7923a356e9f111619375b94d12749d634069347",
    "390642acf35f3d599271617d30ba26c2f6406fc1",
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
    "5ac06c8b381f1ab63aeb5117b26e90b28bef026a",
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b",
    "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14"
  ],
  "changeHistoryShort": {
    "6822193ee6d6ac8b08822fa76c89e1dd61c5ddca": "Ybodychange",
    "7314185c4a313842115e18b5f42d118392cee929": "Ybodychange",
    "ae4143a529d74d94f205ca627c31360abfa11bfa": "Ybodychange",
    "8239e3afb31d3c4485817d4b8b8b195b554acbe7": "Ybodychange",
    "3b1d30301bcd35bbe525a7e122d3e5acfab92c88": "Ybodychange",
    "546b95f4843f3cbbbdf72d90d202cad551696082": "Ybodychange",
    "d65df0f27395792c6e25f5e03b6ba1765e2ba925": "Ybodychange",
    "c22cf004425daa9c350df5e365b0db85b1628b40": "Ybodychange",
    "391ce535a739dc92cb90017d759217265a4fd969": "Ymultichange(Yparameterchange,Ybodychange)",
    "10e84c6a6e831fe2bea061fb21bd0dfe32bc9953": "Ybodychange",
    "c4463f2ef20d2cb634a1249246f83c451975f3dc": "Ymultichange(Yparameterchange,Ybodychange)",
    "85a20508bd04851d47c24b7562ec2927d5403446": "Ymultichange(Yparameterchange,Ybodychange)",
    "d5abd293a890a8a1da48a166a291ae1c5644ad57": "Ybodychange",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": "Ybodychange",
    "f741476146574550a1a208d58ef8be76639e5ddc": "Ybodychange",
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4": "Ybodychange",
    "b5ce87f84d9de0a5347ab38c0567a5a70d1fbfd7": "Ybodychange",
    "12b5b06c063d93e6c683c9b6fac9a96912f59e59": "Ybodychange",
    "281d47a96937bc329b1b4051ffcb8f5fcac98354": "Ybodychange",
    "f9427f1760cce7e0befc3e066cebd0912652a411": "Ybodychange",
    "50ee8f4e67a66aa77c5359182f61f3e951844db6": "Ymultichange(Yparameterchange,Ybodychange)",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": "Ybodychange",
    "b7923a356e9f111619375b94d12749d634069347": "Ybodychange",
    "390642acf35f3d599271617d30ba26c2f6406fc1": "Ybodychange",
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": "Ymultichange(Yparameterchange,Ybodychange)",
    "5ac06c8b381f1ab63aeb5117b26e90b28bef026a": "Ybodychange",
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b": "Ybodychange",
    "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14": "Ymultichange(Yreturntypechange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "6822193ee6d6ac8b08822fa76c89e1dd61c5ddca": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12914. Block report leases cause missing blocks until next report. Contributed by Santosh Marella, He Xiaoqiao.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\nCo-authored-by: He Xiaoqiao \u003chexiaoqiao@apache.org\u003e\n",
      "commitDate": "17/06/19 4:20 PM",
      "commitName": "6822193ee6d6ac8b08822fa76c89e1dd61c5ddca",
      "commitAuthor": "Santosh Marella",
      "commitDateOld": "17/06/19 4:18 PM",
      "commitNameOld": "7314185c4a313842115e18b5f42d118392cee929",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,80 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n       BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n     String strBlockReportId \u003d\n         context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       // Register DN with provided storage, not with storage owned by DN\n       // DN should still have a ref to the DNStorageInfo.\n       DatanodeStorageInfo storageInfo \u003d\n           providedStorageMap.getStorage(node, storage);\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport 0x{}: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\",\n             strBlockReportId, nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n-      if (context !\u003d null) {\n-        if (!blockReportLeaseManager.checkLease(node, startTime,\n-              context.getLeaseId())) {\n-          return false;\n-        }\n-      }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n             + \"storage report for {} from datanode {}\",\n             strBlockReportId,\n             storageInfo.getStorageID(),\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         // Block reports for provided storage are not\n         // maintained by DN heartbeats\n         if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n           invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n         }\n       }\n       storageInfo.receivedBlockReport();\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n           + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addStorageBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n         nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n    String strBlockReportId \u003d\n        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      // Register DN with provided storage, not with storage owned by DN\n      // DN should still have a ref to the DNStorageInfo.\n      DatanodeStorageInfo storageInfo \u003d\n          providedStorageMap.getStorage(node, storage);\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport 0x{}: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\",\n            strBlockReportId, nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n            + \"storage report for {} from datanode {}\",\n            strBlockReportId,\n            storageInfo.getStorageID(),\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        // Block reports for provided storage are not\n        // maintained by DN heartbeats\n        if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n          invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n        }\n      }\n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addStorageBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n        nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "7314185c4a313842115e18b5f42d118392cee929": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-12914. Block report leases cause missing blocks until next report. Contributed by Santosh Marella, He Xiaoqiao.\"\n\nThis reverts commit ae4143a529d74d94f205ca627c31360abfa11bfa.\n",
      "commitDate": "17/06/19 4:18 PM",
      "commitName": "7314185c4a313842115e18b5f42d118392cee929",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "14/06/19 10:42 AM",
      "commitNameOld": "ae4143a529d74d94f205ca627c31360abfa11bfa",
      "commitAuthorOld": "Santosh Marella",
      "daysBetweenCommits": 3.23,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,86 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n       BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n     String strBlockReportId \u003d\n         context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       // Register DN with provided storage, not with storage owned by DN\n       // DN should still have a ref to the DNStorageInfo.\n       DatanodeStorageInfo storageInfo \u003d\n           providedStorageMap.getStorage(node, storage);\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport 0x{}: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\",\n             strBlockReportId, nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n+      if (context !\u003d null) {\n+        if (!blockReportLeaseManager.checkLease(node, startTime,\n+              context.getLeaseId())) {\n+          return false;\n+        }\n+      }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n             + \"storage report for {} from datanode {}\",\n             strBlockReportId,\n             storageInfo.getStorageID(),\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         // Block reports for provided storage are not\n         // maintained by DN heartbeats\n         if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n           invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n         }\n       }\n       storageInfo.receivedBlockReport();\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n           + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addStorageBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n         nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n    String strBlockReportId \u003d\n        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      // Register DN with provided storage, not with storage owned by DN\n      // DN should still have a ref to the DNStorageInfo.\n      DatanodeStorageInfo storageInfo \u003d\n          providedStorageMap.getStorage(node, storage);\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport 0x{}: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\",\n            strBlockReportId, nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n            + \"storage report for {} from datanode {}\",\n            strBlockReportId,\n            storageInfo.getStorageID(),\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        // Block reports for provided storage are not\n        // maintained by DN heartbeats\n        if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n          invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n        }\n      }\n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addStorageBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n        nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "ae4143a529d74d94f205ca627c31360abfa11bfa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12914. Block report leases cause missing blocks until next report. Contributed by Santosh Marella, He Xiaoqiao.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\nCo-authored-by: He Xiaoqiao \u003chexiaoqiao@apache.org\u003e\n",
      "commitDate": "14/06/19 10:42 AM",
      "commitName": "ae4143a529d74d94f205ca627c31360abfa11bfa",
      "commitAuthor": "Santosh Marella",
      "commitDateOld": "13/06/19 6:26 PM",
      "commitNameOld": "4f455290b15902e7e44c4b1a762bf915414b2bb6",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 0.68,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,80 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n       BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n     String strBlockReportId \u003d\n         context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       // Register DN with provided storage, not with storage owned by DN\n       // DN should still have a ref to the DNStorageInfo.\n       DatanodeStorageInfo storageInfo \u003d\n           providedStorageMap.getStorage(node, storage);\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport 0x{}: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\",\n             strBlockReportId, nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n-      if (context !\u003d null) {\n-        if (!blockReportLeaseManager.checkLease(node, startTime,\n-              context.getLeaseId())) {\n-          return false;\n-        }\n-      }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n             + \"storage report for {} from datanode {}\",\n             strBlockReportId,\n             storageInfo.getStorageID(),\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         // Block reports for provided storage are not\n         // maintained by DN heartbeats\n         if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n           invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n         }\n       }\n       storageInfo.receivedBlockReport();\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n           + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addStorageBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n         nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n    String strBlockReportId \u003d\n        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      // Register DN with provided storage, not with storage owned by DN\n      // DN should still have a ref to the DNStorageInfo.\n      DatanodeStorageInfo storageInfo \u003d\n          providedStorageMap.getStorage(node, storage);\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport 0x{}: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\",\n            strBlockReportId, nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n            + \"storage report for {} from datanode {}\",\n            strBlockReportId,\n            storageInfo.getStorageID(),\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        // Block reports for provided storage are not\n        // maintained by DN heartbeats\n        if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n          invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n        }\n      }\n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addStorageBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n        nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8239e3afb31d3c4485817d4b8b8b195b554acbe7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12712. [9806] Code style cleanup\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "8239e3afb31d3c4485817d4b8b8b195b554acbe7",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "c89b29bd421152f0e7e16936f18d9e852895c37a",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,86 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n       BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n     String strBlockReportId \u003d\n         context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n-      // !#! Register DN with provided storage, not with storage owned by DN\n-      // !#! DN should still have a ref to the DNStorageInfo\n+      // Register DN with provided storage, not with storage owned by DN\n+      // DN should still have a ref to the DNStorageInfo.\n       DatanodeStorageInfo storageInfo \u003d\n           providedStorageMap.getStorage(node, storage);\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport 0x{}: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\",\n             strBlockReportId, nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n             + \"storage report for {} from datanode {}\",\n             strBlockReportId,\n             storageInfo.getStorageID(),\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         // Block reports for provided storage are not\n         // maintained by DN heartbeats\n         if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n           invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n         }\n       }\n       storageInfo.receivedBlockReport();\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n           + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addStorageBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n         nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n    String strBlockReportId \u003d\n        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      // Register DN with provided storage, not with storage owned by DN\n      // DN should still have a ref to the DNStorageInfo.\n      DatanodeStorageInfo storageInfo \u003d\n          providedStorageMap.getStorage(node, storage);\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport 0x{}: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\",\n            strBlockReportId, nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n            + \"storage report for {} from datanode {}\",\n            strBlockReportId,\n            storageInfo.getStorageID(),\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        // Block reports for provided storage are not\n        // maintained by DN heartbeats\n        if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n          invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n        }\n      }\n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addStorageBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n        nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "3b1d30301bcd35bbe525a7e122d3e5acfab92c88": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12775. [READ] Fix reporting of Provided volumes\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "3b1d30301bcd35bbe525a7e122d3e5acfab92c88",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "546b95f4843f3cbbbdf72d90d202cad551696082",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,86 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n       BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n     String strBlockReportId \u003d\n         context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       // !#! Register DN with provided storage, not with storage owned by DN\n       // !#! DN should still have a ref to the DNStorageInfo\n       DatanodeStorageInfo storageInfo \u003d\n-          providedStorageMap.getStorage(node, storage, context);\n+          providedStorageMap.getStorage(node, storage);\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport 0x{}: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\",\n             strBlockReportId, nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n             + \"storage report for {} from datanode {}\",\n             strBlockReportId,\n             storageInfo.getStorageID(),\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         // Block reports for provided storage are not\n         // maintained by DN heartbeats\n         if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n           invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n         }\n       }\n       storageInfo.receivedBlockReport();\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n           + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addStorageBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n         nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n    String strBlockReportId \u003d\n        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      // !#! Register DN with provided storage, not with storage owned by DN\n      // !#! DN should still have a ref to the DNStorageInfo\n      DatanodeStorageInfo storageInfo \u003d\n          providedStorageMap.getStorage(node, storage);\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport 0x{}: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\",\n            strBlockReportId, nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n            + \"storage report for {} from datanode {}\",\n            strBlockReportId,\n            storageInfo.getStorageID(),\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        // Block reports for provided storage are not\n        // maintained by DN heartbeats\n        if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n          invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n        }\n      }\n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addStorageBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n        nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "546b95f4843f3cbbbdf72d90d202cad551696082": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11673. [READ] Handle failures of Datanode with PROVIDED storage\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "546b95f4843f3cbbbdf72d90d202cad551696082",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "d65df0f27395792c6e25f5e03b6ba1765e2ba925",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,86 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n       BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n     String strBlockReportId \u003d\n         context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       // !#! Register DN with provided storage, not with storage owned by DN\n       // !#! DN should still have a ref to the DNStorageInfo\n       DatanodeStorageInfo storageInfo \u003d\n-          providedStorageMap.getStorage(node, storage);\n+          providedStorageMap.getStorage(node, storage, context);\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport 0x{}: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\",\n             strBlockReportId, nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n             + \"storage report for {} from datanode {}\",\n             strBlockReportId,\n             storageInfo.getStorageID(),\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         // Block reports for provided storage are not\n         // maintained by DN heartbeats\n         if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n           invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n         }\n       }\n       storageInfo.receivedBlockReport();\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n           + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addStorageBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n         nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n    String strBlockReportId \u003d\n        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      // !#! Register DN with provided storage, not with storage owned by DN\n      // !#! DN should still have a ref to the DNStorageInfo\n      DatanodeStorageInfo storageInfo \u003d\n          providedStorageMap.getStorage(node, storage, context);\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport 0x{}: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\",\n            strBlockReportId, nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n            + \"storage report for {} from datanode {}\",\n            strBlockReportId,\n            storageInfo.getStorageID(),\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        // Block reports for provided storage are not\n        // maintained by DN heartbeats\n        if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n          invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n        }\n      }\n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addStorageBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n        nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "d65df0f27395792c6e25f5e03b6ba1765e2ba925": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11190. [READ] Namenode support for data stored in external stores.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "d65df0f27395792c6e25f5e03b6ba1765e2ba925",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "05/12/17 8:48 PM",
      "commitNameOld": "56b1ff80dd9fbcde8d21a604eff0babb3a16418f",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 9.88,
      "commitsBetweenForRepo": 75,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,86 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n       BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n     String strBlockReportId \u003d\n         context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n-      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n+      // !#! Register DN with provided storage, not with storage owned by DN\n+      // !#! DN should still have a ref to the DNStorageInfo\n+      DatanodeStorageInfo storageInfo \u003d\n+          providedStorageMap.getStorage(node, storage);\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport 0x{}: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\",\n             strBlockReportId, nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n             + \"storage report for {} from datanode {}\",\n             strBlockReportId,\n             storageInfo.getStorageID(),\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n-        invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n+        // Block reports for provided storage are not\n+        // maintained by DN heartbeats\n+        if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n+          invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n+        }\n       }\n-      \n       storageInfo.receivedBlockReport();\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n           + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addStorageBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n         nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n    String strBlockReportId \u003d\n        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      // !#! Register DN with provided storage, not with storage owned by DN\n      // !#! DN should still have a ref to the DNStorageInfo\n      DatanodeStorageInfo storageInfo \u003d\n          providedStorageMap.getStorage(node, storage);\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport 0x{}: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\",\n            strBlockReportId, nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n            + \"storage report for {} from datanode {}\",\n            strBlockReportId,\n            storageInfo.getStorageID(),\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        // Block reports for provided storage are not\n        // maintained by DN heartbeats\n        if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {\n          invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n        }\n      }\n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addStorageBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n        nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "c22cf004425daa9c350df5e365b0db85b1628b40": {
      "type": "Ybodychange",
      "commitMessage": "Confusion/name conflict between NameNodeActivity#BlockReportNumOps and RpcDetailedActivity#BlockReportNumOps. Contributed by Erik Krogen.\n",
      "commitDate": "21/06/17 4:34 PM",
      "commitName": "c22cf004425daa9c350df5e365b0db85b1628b40",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "20/06/17 7:11 AM",
      "commitNameOld": "8c0769dee4b455f4de08ccce36334f0be9e79e2c",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 1.39,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,80 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n       BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n     String strBlockReportId \u003d\n         context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport 0x{}: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\",\n             strBlockReportId, nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n             + \"storage report for {} from datanode {}\",\n             strBlockReportId,\n             storageInfo.getStorageID(),\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n       }\n       \n       storageInfo.receivedBlockReport();\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n           + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n-      metrics.addBlockReport((int) (endTime - startTime));\n+      metrics.addStorageBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n         nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n    String strBlockReportId \u003d\n        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport 0x{}: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\",\n            strBlockReportId, nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n            + \"storage report for {} from datanode {}\",\n            strBlockReportId,\n            storageInfo.getStorageID(),\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n      }\n      \n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addStorageBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n        nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "391ce535a739dc92cb90017d759217265a4fd969": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10301. Remove FBR tracking state to fix false zombie storage detection for interleaving block reports. Contributed by Vinitha Gankidi.",
      "commitDate": "14/10/16 6:13 PM",
      "commitName": "391ce535a739dc92cb90017d759217265a4fd969",
      "commitAuthor": "Vinitha Reddy Gankidi",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10301. Remove FBR tracking state to fix false zombie storage detection for interleaving block reports. Contributed by Vinitha Gankidi.",
          "commitDate": "14/10/16 6:13 PM",
          "commitName": "391ce535a739dc92cb90017d759217265a4fd969",
          "commitAuthor": "Vinitha Reddy Gankidi",
          "commitDateOld": "07/10/16 10:44 PM",
          "commitNameOld": "4d106213c0f4835b723c9a50bd8080a9017122d7",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 6.81,
          "commitsBetweenForRepo": 51,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,106 +1,80 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n-      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n+      BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n     String strBlockReportId \u003d\n         context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport 0x{}: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\",\n             strBlockReportId, nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n             + \"storage report for {} from datanode {}\",\n             strBlockReportId,\n             storageInfo.getStorageID(),\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n       }\n       \n       storageInfo.receivedBlockReport();\n-      if (context !\u003d null) {\n-        storageInfo.setLastBlockReportId(context.getReportId());\n-        if (lastStorageInRpc) {\n-          int rpcsSeen \u003d node.updateBlockReportContext(context);\n-          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n-            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n-            BlockManagerFaultInjector.getInstance().\n-                removeBlockReportLease(node, leaseId);\n-            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n-            if (zombies.isEmpty()) {\n-              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n-                  Long.toHexString(context.getReportId()));\n-            } else {\n-              for (DatanodeStorageInfo zombie : zombies) {\n-                removeZombieReplicas(context, zombie);\n-              }\n-            }\n-            node.clearBlockReportContext();\n-          } else {\n-            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n-                    \"report.\", Long.toHexString(context.getReportId()),\n-                (context.getTotalRpcs() - rpcsSeen)\n-            );\n-          }\n-        }\n-      }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n           + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n         nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n    String strBlockReportId \u003d\n        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport 0x{}: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\",\n            strBlockReportId, nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n            + \"storage report for {} from datanode {}\",\n            strBlockReportId,\n            storageInfo.getStorageID(),\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n      }\n      \n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n        nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[nodeID-DatanodeID(modifiers-final), storage-DatanodeStorage(modifiers-final), newReport-BlockListAsLongs(modifiers-final), context-BlockReportContext, lastStorageInRpc-boolean]",
            "newValue": "[nodeID-DatanodeID(modifiers-final), storage-DatanodeStorage(modifiers-final), newReport-BlockListAsLongs(modifiers-final), context-BlockReportContext]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10301. Remove FBR tracking state to fix false zombie storage detection for interleaving block reports. Contributed by Vinitha Gankidi.",
          "commitDate": "14/10/16 6:13 PM",
          "commitName": "391ce535a739dc92cb90017d759217265a4fd969",
          "commitAuthor": "Vinitha Reddy Gankidi",
          "commitDateOld": "07/10/16 10:44 PM",
          "commitNameOld": "4d106213c0f4835b723c9a50bd8080a9017122d7",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 6.81,
          "commitsBetweenForRepo": 51,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,106 +1,80 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n-      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n+      BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n     String strBlockReportId \u003d\n         context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport 0x{}: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\",\n             strBlockReportId, nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n             + \"storage report for {} from datanode {}\",\n             strBlockReportId,\n             storageInfo.getStorageID(),\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n       }\n       \n       storageInfo.receivedBlockReport();\n-      if (context !\u003d null) {\n-        storageInfo.setLastBlockReportId(context.getReportId());\n-        if (lastStorageInRpc) {\n-          int rpcsSeen \u003d node.updateBlockReportContext(context);\n-          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n-            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n-            BlockManagerFaultInjector.getInstance().\n-                removeBlockReportLease(node, leaseId);\n-            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n-            if (zombies.isEmpty()) {\n-              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n-                  Long.toHexString(context.getReportId()));\n-            } else {\n-              for (DatanodeStorageInfo zombie : zombies) {\n-                removeZombieReplicas(context, zombie);\n-              }\n-            }\n-            node.clearBlockReportContext();\n-          } else {\n-            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n-                    \"report.\", Long.toHexString(context.getReportId()),\n-                (context.getTotalRpcs() - rpcsSeen)\n-            );\n-          }\n-        }\n-      }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n           + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n         nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n    String strBlockReportId \u003d\n        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport 0x{}: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\",\n            strBlockReportId, nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n            + \"storage report for {} from datanode {}\",\n            strBlockReportId,\n            storageInfo.getStorageID(),\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n      }\n      \n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n        nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "10e84c6a6e831fe2bea061fb21bd0dfe32bc9953": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10694. processReport() should print blockReportId in each log message. Contributed by Yuanbo Liu.",
      "commitDate": "10/08/16 11:07 AM",
      "commitName": "10e84c6a6e831fe2bea061fb21bd0dfe32bc9953",
      "commitAuthor": "Yuanbo Liu",
      "commitDateOld": "09/08/16 9:56 AM",
      "commitNameOld": "b10c936020e2616609dcb3b2126e8c34328c10ca",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 1.05,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,102 +1,106 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n       BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n+    String strBlockReportId \u003d\n+        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n-        blockLog.info(\"BLOCK* processReport: \"\n+        blockLog.info(\"BLOCK* processReport 0x{}: \"\n             + \"discarded non-initial block report from {}\"\n-            + \" because namenode still in startup phase\", nodeID);\n+            + \" because namenode still in startup phase\",\n+            strBlockReportId, nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n-        LOG.info(\"Processing first storage report for \" +\n-            storageInfo.getStorageID() + \" from datanode \" +\n+        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n+            + \"storage report for {} from datanode {}\",\n+            strBlockReportId,\n+            storageInfo.getStorageID(),\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n-        invalidatedBlocks \u003d processReport(storageInfo, newReport,\n-            context !\u003d null ? context.isSorted() : false);\n+        invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n         storageInfo.setLastBlockReportId(context.getReportId());\n         if (lastStorageInRpc) {\n           int rpcsSeen \u003d node.updateBlockReportContext(context);\n           if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n             long leaseId \u003d blockReportLeaseManager.removeLease(node);\n             BlockManagerFaultInjector.getInstance().\n                 removeBlockReportLease(node, leaseId);\n             List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n             if (zombies.isEmpty()) {\n               LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                   Long.toHexString(context.getReportId()));\n             } else {\n               for (DatanodeStorageInfo zombie : zombies) {\n                 removeZombieReplicas(context, zombie);\n               }\n             }\n             node.clearBlockReportContext();\n           } else {\n             LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                     \"report.\", Long.toHexString(context.getReportId()),\n                 (context.getTotalRpcs() - rpcsSeen)\n             );\n           }\n         }\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n-      blockLog.debug(\"BLOCK* processReport: {} on node {} size {} does not \" +\n-          \"belong to any file\", b, node, b.getNumBytes());\n+      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n+          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n-    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n+    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n-        \"invalidatedBlocks: {}\", storage.getStorageID(), nodeID,\n-        newReport.getNumberOfBlocks(),\n+        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n+        nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n    String strBlockReportId \u003d\n        context !\u003d null ? Long.toHexString(context.getReportId()) : \"\";\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport 0x{}: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\",\n            strBlockReportId, nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        blockLog.info(\"BLOCK* processReport 0x{}: Processing first \"\n            + \"storage report for {} from datanode {}\",\n            strBlockReportId,\n            storageInfo.getStorageID(),\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport, context);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n            BlockManagerFaultInjector.getInstance().\n                removeBlockReportLease(node, leaseId);\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport 0x{}: {} on node {} size {} does not\"\n          + \" belong to any file\", strBlockReportId, b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport 0x{}: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", strBlockReportId, storage.getStorageID(),\n        nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "c4463f2ef20d2cb634a1249246f83c451975f3dc": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "Revert \"HDFS-10301. Interleaving processing of storages from repeated block reports causes false zombie storage detection, removes valid blocks. Contributed by Vinitha Gankidi.\"\n\nThis reverts commit 85a20508bd04851d47c24b7562ec2927d5403446.\n",
      "commitDate": "01/08/16 10:34 PM",
      "commitName": "c4463f2ef20d2cb634a1249246f83c451975f3dc",
      "commitAuthor": "Konstantin V Shvachko",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "Revert \"HDFS-10301. Interleaving processing of storages from repeated block reports causes false zombie storage detection, removes valid blocks. Contributed by Vinitha Gankidi.\"\n\nThis reverts commit 85a20508bd04851d47c24b7562ec2927d5403446.\n",
          "commitDate": "01/08/16 10:34 PM",
          "commitName": "c4463f2ef20d2cb634a1249246f83c451975f3dc",
          "commitAuthor": "Konstantin V Shvachko",
          "commitDateOld": "25/07/16 6:50 PM",
          "commitNameOld": "85a20508bd04851d47c24b7562ec2927d5403446",
          "commitAuthorOld": "Vinitha Reddy Gankidi",
          "daysBetweenCommits": 7.16,
          "commitsBetweenForRepo": 45,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,86 +1,102 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n-      BlockReportContext context) throws IOException {\n+      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         LOG.info(\"Processing first storage report for \" +\n             storageInfo.getStorageID() + \" from datanode \" +\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport,\n             context !\u003d null ? context.isSorted() : false);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n-        if (context.getTotalRpcs() \u003d\u003d context.getCurRpc() + 1) {\n-          long leaseId \u003d this.getBlockReportLeaseManager().removeLease(node);\n-          BlockManagerFaultInjector.getInstance().\n-              removeBlockReportLease(node, leaseId);\n+        storageInfo.setLastBlockReportId(context.getReportId());\n+        if (lastStorageInRpc) {\n+          int rpcsSeen \u003d node.updateBlockReportContext(context);\n+          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n+            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n+            BlockManagerFaultInjector.getInstance().\n+                removeBlockReportLease(node, leaseId);\n+            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n+            if (zombies.isEmpty()) {\n+              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n+                  Long.toHexString(context.getReportId()));\n+            } else {\n+              for (DatanodeStorageInfo zombie : zombies) {\n+                removeZombieReplicas(context, zombie);\n+              }\n+            }\n+            node.clearBlockReportContext();\n+          } else {\n+            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n+                    \"report.\", Long.toHexString(context.getReportId()),\n+                (context.getTotalRpcs() - rpcsSeen)\n+            );\n+          }\n         }\n-        LOG.debug(\"Processing RPC with index {} out of total {} RPCs in \"\n-                + \"processReport 0x{}\", context.getCurRpc(),\n-            context.getTotalRpcs(), Long.toHexString(context.getReportId()));\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport: {} on node {} size {} does not \" +\n           \"belong to any file\", b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", storage.getStorageID(), nodeID,\n         newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        LOG.info(\"Processing first storage report for \" +\n            storageInfo.getStorageID() + \" from datanode \" +\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport,\n            context !\u003d null ? context.isSorted() : false);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n            BlockManagerFaultInjector.getInstance().\n                removeBlockReportLease(node, leaseId);\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport: {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", storage.getStorageID(), nodeID,\n        newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[nodeID-DatanodeID(modifiers-final), storage-DatanodeStorage(modifiers-final), newReport-BlockListAsLongs(modifiers-final), context-BlockReportContext]",
            "newValue": "[nodeID-DatanodeID(modifiers-final), storage-DatanodeStorage(modifiers-final), newReport-BlockListAsLongs(modifiers-final), context-BlockReportContext, lastStorageInRpc-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "Revert \"HDFS-10301. Interleaving processing of storages from repeated block reports causes false zombie storage detection, removes valid blocks. Contributed by Vinitha Gankidi.\"\n\nThis reverts commit 85a20508bd04851d47c24b7562ec2927d5403446.\n",
          "commitDate": "01/08/16 10:34 PM",
          "commitName": "c4463f2ef20d2cb634a1249246f83c451975f3dc",
          "commitAuthor": "Konstantin V Shvachko",
          "commitDateOld": "25/07/16 6:50 PM",
          "commitNameOld": "85a20508bd04851d47c24b7562ec2927d5403446",
          "commitAuthorOld": "Vinitha Reddy Gankidi",
          "daysBetweenCommits": 7.16,
          "commitsBetweenForRepo": 45,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,86 +1,102 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n-      BlockReportContext context) throws IOException {\n+      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         LOG.info(\"Processing first storage report for \" +\n             storageInfo.getStorageID() + \" from datanode \" +\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport,\n             context !\u003d null ? context.isSorted() : false);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n-        if (context.getTotalRpcs() \u003d\u003d context.getCurRpc() + 1) {\n-          long leaseId \u003d this.getBlockReportLeaseManager().removeLease(node);\n-          BlockManagerFaultInjector.getInstance().\n-              removeBlockReportLease(node, leaseId);\n+        storageInfo.setLastBlockReportId(context.getReportId());\n+        if (lastStorageInRpc) {\n+          int rpcsSeen \u003d node.updateBlockReportContext(context);\n+          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n+            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n+            BlockManagerFaultInjector.getInstance().\n+                removeBlockReportLease(node, leaseId);\n+            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n+            if (zombies.isEmpty()) {\n+              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n+                  Long.toHexString(context.getReportId()));\n+            } else {\n+              for (DatanodeStorageInfo zombie : zombies) {\n+                removeZombieReplicas(context, zombie);\n+              }\n+            }\n+            node.clearBlockReportContext();\n+          } else {\n+            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n+                    \"report.\", Long.toHexString(context.getReportId()),\n+                (context.getTotalRpcs() - rpcsSeen)\n+            );\n+          }\n         }\n-        LOG.debug(\"Processing RPC with index {} out of total {} RPCs in \"\n-                + \"processReport 0x{}\", context.getCurRpc(),\n-            context.getTotalRpcs(), Long.toHexString(context.getReportId()));\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport: {} on node {} size {} does not \" +\n           \"belong to any file\", b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", storage.getStorageID(), nodeID,\n         newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        LOG.info(\"Processing first storage report for \" +\n            storageInfo.getStorageID() + \" from datanode \" +\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport,\n            context !\u003d null ? context.isSorted() : false);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n            BlockManagerFaultInjector.getInstance().\n                removeBlockReportLease(node, leaseId);\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport: {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", storage.getStorageID(), nodeID,\n        newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "85a20508bd04851d47c24b7562ec2927d5403446": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10301. Interleaving processing of storages from repeated block reports causes false zombie storage detection, removes valid blocks. Contributed by Vinitha Gankidi.",
      "commitDate": "25/07/16 6:50 PM",
      "commitName": "85a20508bd04851d47c24b7562ec2927d5403446",
      "commitAuthor": "Vinitha Reddy Gankidi",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10301. Interleaving processing of storages from repeated block reports causes false zombie storage detection, removes valid blocks. Contributed by Vinitha Gankidi.",
          "commitDate": "25/07/16 6:50 PM",
          "commitName": "85a20508bd04851d47c24b7562ec2927d5403446",
          "commitAuthor": "Vinitha Reddy Gankidi",
          "commitDateOld": "19/07/16 3:13 PM",
          "commitNameOld": "f7dabe3addf3f6eb32ea9b8ec1354fb442ce4222",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 6.15,
          "commitsBetweenForRepo": 33,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,102 +1,86 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n-      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n+      BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         LOG.info(\"Processing first storage report for \" +\n             storageInfo.getStorageID() + \" from datanode \" +\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport,\n             context !\u003d null ? context.isSorted() : false);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n-        storageInfo.setLastBlockReportId(context.getReportId());\n-        if (lastStorageInRpc) {\n-          int rpcsSeen \u003d node.updateBlockReportContext(context);\n-          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n-            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n-            BlockManagerFaultInjector.getInstance().\n-                removeBlockReportLease(node, leaseId);\n-            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n-            if (zombies.isEmpty()) {\n-              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n-                  Long.toHexString(context.getReportId()));\n-            } else {\n-              for (DatanodeStorageInfo zombie : zombies) {\n-                removeZombieReplicas(context, zombie);\n-              }\n-            }\n-            node.clearBlockReportContext();\n-          } else {\n-            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n-                    \"report.\", Long.toHexString(context.getReportId()),\n-                (context.getTotalRpcs() - rpcsSeen)\n-            );\n-          }\n+        if (context.getTotalRpcs() \u003d\u003d context.getCurRpc() + 1) {\n+          long leaseId \u003d this.getBlockReportLeaseManager().removeLease(node);\n+          BlockManagerFaultInjector.getInstance().\n+              removeBlockReportLease(node, leaseId);\n         }\n+        LOG.debug(\"Processing RPC with index {} out of total {} RPCs in \"\n+                + \"processReport 0x{}\", context.getCurRpc(),\n+            context.getTotalRpcs(), Long.toHexString(context.getReportId()));\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport: {} on node {} size {} does not \" +\n           \"belong to any file\", b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", storage.getStorageID(), nodeID,\n         newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        LOG.info(\"Processing first storage report for \" +\n            storageInfo.getStorageID() + \" from datanode \" +\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport,\n            context !\u003d null ? context.isSorted() : false);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        if (context.getTotalRpcs() \u003d\u003d context.getCurRpc() + 1) {\n          long leaseId \u003d this.getBlockReportLeaseManager().removeLease(node);\n          BlockManagerFaultInjector.getInstance().\n              removeBlockReportLease(node, leaseId);\n        }\n        LOG.debug(\"Processing RPC with index {} out of total {} RPCs in \"\n                + \"processReport 0x{}\", context.getCurRpc(),\n            context.getTotalRpcs(), Long.toHexString(context.getReportId()));\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport: {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", storage.getStorageID(), nodeID,\n        newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[nodeID-DatanodeID(modifiers-final), storage-DatanodeStorage(modifiers-final), newReport-BlockListAsLongs(modifiers-final), context-BlockReportContext, lastStorageInRpc-boolean]",
            "newValue": "[nodeID-DatanodeID(modifiers-final), storage-DatanodeStorage(modifiers-final), newReport-BlockListAsLongs(modifiers-final), context-BlockReportContext]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10301. Interleaving processing of storages from repeated block reports causes false zombie storage detection, removes valid blocks. Contributed by Vinitha Gankidi.",
          "commitDate": "25/07/16 6:50 PM",
          "commitName": "85a20508bd04851d47c24b7562ec2927d5403446",
          "commitAuthor": "Vinitha Reddy Gankidi",
          "commitDateOld": "19/07/16 3:13 PM",
          "commitNameOld": "f7dabe3addf3f6eb32ea9b8ec1354fb442ce4222",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 6.15,
          "commitsBetweenForRepo": 33,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,102 +1,86 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n-      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n+      BlockReportContext context) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         LOG.info(\"Processing first storage report for \" +\n             storageInfo.getStorageID() + \" from datanode \" +\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport,\n             context !\u003d null ? context.isSorted() : false);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n-        storageInfo.setLastBlockReportId(context.getReportId());\n-        if (lastStorageInRpc) {\n-          int rpcsSeen \u003d node.updateBlockReportContext(context);\n-          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n-            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n-            BlockManagerFaultInjector.getInstance().\n-                removeBlockReportLease(node, leaseId);\n-            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n-            if (zombies.isEmpty()) {\n-              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n-                  Long.toHexString(context.getReportId()));\n-            } else {\n-              for (DatanodeStorageInfo zombie : zombies) {\n-                removeZombieReplicas(context, zombie);\n-              }\n-            }\n-            node.clearBlockReportContext();\n-          } else {\n-            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n-                    \"report.\", Long.toHexString(context.getReportId()),\n-                (context.getTotalRpcs() - rpcsSeen)\n-            );\n-          }\n+        if (context.getTotalRpcs() \u003d\u003d context.getCurRpc() + 1) {\n+          long leaseId \u003d this.getBlockReportLeaseManager().removeLease(node);\n+          BlockManagerFaultInjector.getInstance().\n+              removeBlockReportLease(node, leaseId);\n         }\n+        LOG.debug(\"Processing RPC with index {} out of total {} RPCs in \"\n+                + \"processReport 0x{}\", context.getCurRpc(),\n+            context.getTotalRpcs(), Long.toHexString(context.getReportId()));\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     for (Block b : invalidatedBlocks) {\n       blockLog.debug(\"BLOCK* processReport: {} on node {} size {} does not \" +\n           \"belong to any file\", b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n         \"invalidatedBlocks: {}\", storage.getStorageID(), nodeID,\n         newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime),\n         invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        LOG.info(\"Processing first storage report for \" +\n            storageInfo.getStorageID() + \" from datanode \" +\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport,\n            context !\u003d null ? context.isSorted() : false);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        if (context.getTotalRpcs() \u003d\u003d context.getCurRpc() + 1) {\n          long leaseId \u003d this.getBlockReportLeaseManager().removeLease(node);\n          BlockManagerFaultInjector.getInstance().\n              removeBlockReportLease(node, leaseId);\n        }\n        LOG.debug(\"Processing RPC with index {} out of total {} RPCs in \"\n                + \"processReport 0x{}\", context.getCurRpc(),\n            context.getTotalRpcs(), Long.toHexString(context.getReportId()));\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport: {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", storage.getStorageID(), nodeID,\n        newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "d5abd293a890a8a1da48a166a291ae1c5644ad57": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9839. Reduce verbosity of processReport logging. (Contributed by Arpit Agarwal)\n\nThis closes #78\n",
      "commitDate": "20/02/16 11:19 PM",
      "commitName": "d5abd293a890a8a1da48a166a291ae1c5644ad57",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "19/02/16 7:02 PM",
      "commitNameOld": "e54cc2931262bf49682a8323da9811976218c03b",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.18,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,102 +1,102 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport,\n       BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n-    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n+    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         LOG.info(\"Processing first storage report for \" +\n             storageInfo.getStorageID() + \" from datanode \" +\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport,\n             context !\u003d null ? context.isSorted() : false);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n         storageInfo.setLastBlockReportId(context.getReportId());\n         if (lastStorageInRpc) {\n           int rpcsSeen \u003d node.updateBlockReportContext(context);\n           if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n             long leaseId \u003d blockReportLeaseManager.removeLease(node);\n             BlockManagerFaultInjector.getInstance().\n                 removeBlockReportLease(node, leaseId);\n             List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n             if (zombies.isEmpty()) {\n               LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                   Long.toHexString(context.getReportId()));\n             } else {\n               for (DatanodeStorageInfo zombie : zombies) {\n                 removeZombieReplicas(context, zombie);\n               }\n             }\n             node.clearBlockReportContext();\n           } else {\n             LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                     \"report.\", Long.toHexString(context.getReportId()),\n                 (context.getTotalRpcs() - rpcsSeen)\n             );\n           }\n         }\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n-    if (invalidatedBlocks !\u003d null) {\n-      for (Block b : invalidatedBlocks) {\n-        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n-            \"belong to any file\", b, node, b.getNumBytes());\n-      }\n+    for (Block b : invalidatedBlocks) {\n+      blockLog.debug(\"BLOCK* processReport: {} on node {} size {} does not \" +\n+          \"belong to any file\", b, node, b.getNumBytes());\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n-            \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n-            .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n-        node.hasStaleStorages(), (endTime - startTime));\n+        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n+        \"invalidatedBlocks: {}\", storage.getStorageID(), nodeID,\n+        newReport.getNumberOfBlocks(),\n+        node.hasStaleStorages(), (endTime - startTime),\n+        invalidatedBlocks.size());\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d Collections.emptyList();\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        LOG.info(\"Processing first storage report for \" +\n            storageInfo.getStorageID() + \" from datanode \" +\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport,\n            context !\u003d null ? context.isSorted() : false);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n            BlockManagerFaultInjector.getInstance().\n                removeBlockReportLease(node, leaseId);\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    for (Block b : invalidatedBlocks) {\n      blockLog.debug(\"BLOCK* processReport: {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs, \" +\n        \"invalidatedBlocks: {}\", storage.getStorageID(), nodeID,\n        newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime),\n        invalidatedBlocks.size());\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9260. Improve the performance and GC friendliness of NameNode startup and full block reports (Staffan Friberg via cmccabe)\n",
      "commitDate": "02/02/16 11:23 AM",
      "commitName": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "31/01/16 11:54 PM",
      "commitNameOld": "e418bd1fb0568ce7ae22f588fea2dd9c95567383",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 1.48,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,102 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n-      final BlockListAsLongs newReport, BlockReportContext context,\n-      boolean lastStorageInRpc) throws IOException {\n+      final BlockListAsLongs newReport,\n+      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         LOG.info(\"Processing first storage report for \" +\n             storageInfo.getStorageID() + \" from datanode \" +\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n-        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n+        invalidatedBlocks \u003d processReport(storageInfo, newReport,\n+            context !\u003d null ? context.isSorted() : false);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n         storageInfo.setLastBlockReportId(context.getReportId());\n         if (lastStorageInRpc) {\n           int rpcsSeen \u003d node.updateBlockReportContext(context);\n           if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n             long leaseId \u003d blockReportLeaseManager.removeLease(node);\n             BlockManagerFaultInjector.getInstance().\n                 removeBlockReportLease(node, leaseId);\n             List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n             if (zombies.isEmpty()) {\n               LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                   Long.toHexString(context.getReportId()));\n             } else {\n               for (DatanodeStorageInfo zombie : zombies) {\n                 removeZombieReplicas(context, zombie);\n               }\n             }\n             node.clearBlockReportContext();\n           } else {\n             LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                     \"report.\", Long.toHexString(context.getReportId()),\n                 (context.getTotalRpcs() - rpcsSeen)\n             );\n           }\n         }\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n         blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n             \"belong to any file\", b, node, b.getNumBytes());\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n             \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n             .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime));\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport,\n      BlockReportContext context, boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        LOG.info(\"Processing first storage report for \" +\n            storageInfo.getStorageID() + \" from datanode \" +\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport,\n            context !\u003d null ? context.isSorted() : false);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n            BlockManagerFaultInjector.getInstance().\n                removeBlockReportLease(node, leaseId);\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n            \"belong to any file\", b, node, b.getNumBytes());\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n            \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n            .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime));\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "f741476146574550a1a208d58ef8be76639e5ddc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9198. Coalesce IBR processing in the NN. (Daryn Sharp via umamahesh)\n",
      "commitDate": "16/12/15 6:16 PM",
      "commitName": "f741476146574550a1a208d58ef8be76639e5ddc",
      "commitAuthor": "Uma Mahesh",
      "commitDateOld": "15/12/15 10:47 AM",
      "commitNameOld": "8602692338d6f493647205e0241e4116211fab75",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.31,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,101 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport, BlockReportContext context,\n       boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive()) {\n+      if (node \u003d\u003d null || !node.isRegistered()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         LOG.info(\"Processing first storage report for \" +\n             storageInfo.getStorageID() + \" from datanode \" +\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n         storageInfo.setLastBlockReportId(context.getReportId());\n         if (lastStorageInRpc) {\n           int rpcsSeen \u003d node.updateBlockReportContext(context);\n           if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n             long leaseId \u003d blockReportLeaseManager.removeLease(node);\n             BlockManagerFaultInjector.getInstance().\n                 removeBlockReportLease(node, leaseId);\n             List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n             if (zombies.isEmpty()) {\n               LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                   Long.toHexString(context.getReportId()));\n             } else {\n               for (DatanodeStorageInfo zombie : zombies) {\n                 removeZombieReplicas(context, zombie);\n               }\n             }\n             node.clearBlockReportContext();\n           } else {\n             LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                     \"report.\", Long.toHexString(context.getReportId()),\n                 (context.getTotalRpcs() - rpcsSeen)\n             );\n           }\n         }\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n         blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n             \"belong to any file\", b, node, b.getNumBytes());\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n             \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n             .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime));\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport, BlockReportContext context,\n      boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isRegistered()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        LOG.info(\"Processing first storage report for \" +\n            storageInfo.getStorageID() + \" from datanode \" +\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n            BlockManagerFaultInjector.getInstance().\n                removeBlockReportLease(node, leaseId);\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n            \"belong to any file\", b, node, b.getNumBytes());\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n            \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n            .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime));\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9223. Code cleanup for DatanodeDescriptor and HeartbeatManager. Contributed by Jing Zhao.\n",
      "commitDate": "14/10/15 4:17 PM",
      "commitName": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "13/10/15 11:00 PM",
      "commitNameOld": "2a987243423eb5c7e191de2ba969b7591a441c70",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.72,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,101 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport, BlockReportContext context,\n       boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive) {\n+      if (node \u003d\u003d null || !node.isAlive()) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         LOG.info(\"Processing first storage report for \" +\n             storageInfo.getStorageID() + \" from datanode \" +\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n         storageInfo.setLastBlockReportId(context.getReportId());\n         if (lastStorageInRpc) {\n           int rpcsSeen \u003d node.updateBlockReportContext(context);\n           if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n             long leaseId \u003d blockReportLeaseManager.removeLease(node);\n             BlockManagerFaultInjector.getInstance().\n                 removeBlockReportLease(node, leaseId);\n             List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n             if (zombies.isEmpty()) {\n               LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                   Long.toHexString(context.getReportId()));\n             } else {\n               for (DatanodeStorageInfo zombie : zombies) {\n                 removeZombieReplicas(context, zombie);\n               }\n             }\n             node.clearBlockReportContext();\n           } else {\n             LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                     \"report.\", Long.toHexString(context.getReportId()),\n                 (context.getTotalRpcs() - rpcsSeen)\n             );\n           }\n         }\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n         blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n             \"belong to any file\", b, node, b.getNumBytes());\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n             \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n             .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime));\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport, BlockReportContext context,\n      boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive()) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        LOG.info(\"Processing first storage report for \" +\n            storageInfo.getStorageID() + \" from datanode \" +\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n            BlockManagerFaultInjector.getInstance().\n                removeBlockReportLease(node, leaseId);\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n            \"belong to any file\", b, node, b.getNumBytes());\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n            \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n            .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime));\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "b5ce87f84d9de0a5347ab38c0567a5a70d1fbfd7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8930. Block report lease may leak if the 2nd full block report comes when NN is still in safemode (Colin P. McCabe via Jing Zhao)\n",
      "commitDate": "24/08/15 11:31 AM",
      "commitName": "b5ce87f84d9de0a5347ab38c0567a5a70d1fbfd7",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "22/08/15 12:09 AM",
      "commitNameOld": "745d04be59accf80feda0ad38efcc74ba362f2ca",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 2.47,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,100 +1,101 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport, BlockReportContext context,\n       boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n+        blockReportLeaseManager.removeLease(node);\n         return !node.hasStaleStorages();\n       }\n       if (context !\u003d null) {\n         if (!blockReportLeaseManager.checkLease(node, startTime,\n               context.getLeaseId())) {\n           return false;\n         }\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         LOG.info(\"Processing first storage report for \" +\n             storageInfo.getStorageID() + \" from datanode \" +\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n         storageInfo.setLastBlockReportId(context.getReportId());\n         if (lastStorageInRpc) {\n           int rpcsSeen \u003d node.updateBlockReportContext(context);\n           if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n             long leaseId \u003d blockReportLeaseManager.removeLease(node);\n             BlockManagerFaultInjector.getInstance().\n                 removeBlockReportLease(node, leaseId);\n             List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n             if (zombies.isEmpty()) {\n               LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                   Long.toHexString(context.getReportId()));\n             } else {\n               for (DatanodeStorageInfo zombie : zombies) {\n                 removeZombieReplicas(context, zombie);\n               }\n             }\n             node.clearBlockReportContext();\n           } else {\n             LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                     \"report.\", Long.toHexString(context.getReportId()),\n                 (context.getTotalRpcs() - rpcsSeen)\n             );\n           }\n         }\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n         blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n             \"belong to any file\", b, node, b.getNumBytes());\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n         .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime));\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport, BlockReportContext context,\n      boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        blockReportLeaseManager.removeLease(node);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        LOG.info(\"Processing first storage report for \" +\n            storageInfo.getStorageID() + \" from datanode \" +\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n            BlockManagerFaultInjector.getInstance().\n                removeBlockReportLease(node, leaseId);\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n            \"belong to any file\", b, node, b.getNumBytes());\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n        .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime));\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "12b5b06c063d93e6c683c9b6fac9a96912f59e59": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7923. The DataNodes should rate-limit their full block reports by asking the NN on heartbeat messages (cmccabe)\n",
      "commitDate": "12/06/15 11:17 AM",
      "commitName": "12b5b06c063d93e6c683c9b6fac9a96912f59e59",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "01/06/15 11:42 AM",
      "commitNameOld": "cdc13efb1af54d931585d25c5ba696a012412828",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 10.98,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,100 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport, BlockReportContext context,\n       boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         return !node.hasStaleStorages();\n       }\n+      if (context !\u003d null) {\n+        if (!blockReportLeaseManager.checkLease(node, startTime,\n+              context.getLeaseId())) {\n+          return false;\n+        }\n+      }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         LOG.info(\"Processing first storage report for \" +\n             storageInfo.getStorageID() + \" from datanode \" +\n             nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n         storageInfo.setLastBlockReportId(context.getReportId());\n         if (lastStorageInRpc) {\n           int rpcsSeen \u003d node.updateBlockReportContext(context);\n           if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n+            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n+            BlockManagerFaultInjector.getInstance().\n+                removeBlockReportLease(node, leaseId);\n             List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n             if (zombies.isEmpty()) {\n               LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                   Long.toHexString(context.getReportId()));\n             } else {\n               for (DatanodeStorageInfo zombie : zombies) {\n                 removeZombieReplicas(context, zombie);\n               }\n             }\n             node.clearBlockReportContext();\n           } else {\n             LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                     \"report.\", Long.toHexString(context.getReportId()),\n                 (context.getTotalRpcs() - rpcsSeen)\n             );\n           }\n         }\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n         blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n             \"belong to any file\", b, node, b.getNumBytes());\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n         .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime));\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport, BlockReportContext context,\n      boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        return !node.hasStaleStorages();\n      }\n      if (context !\u003d null) {\n        if (!blockReportLeaseManager.checkLease(node, startTime,\n              context.getLeaseId())) {\n          return false;\n        }\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        LOG.info(\"Processing first storage report for \" +\n            storageInfo.getStorageID() + \" from datanode \" +\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            long leaseId \u003d blockReportLeaseManager.removeLease(node);\n            BlockManagerFaultInjector.getInstance().\n                removeBlockReportLease(node, leaseId);\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n            \"belong to any file\", b, node, b.getNumBytes());\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n        .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime));\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "281d47a96937bc329b1b4051ffcb8f5fcac98354": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8380. Always call addStoredBlock on blocks which have been shifted from one storage to another (cmccabe)\n",
      "commitDate": "13/05/15 2:29 PM",
      "commitName": "281d47a96937bc329b1b4051ffcb8f5fcac98354",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "12/05/15 6:29 AM",
      "commitNameOld": "6d5da9484185ca9f585195d6da069b9cd5be4044",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 1.33,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,88 +1,91 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport, BlockReportContext context,\n       boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n+        LOG.info(\"Processing first storage report for \" +\n+            storageInfo.getStorageID() + \" from datanode \" +\n+            nodeID.getDatanodeUuid());\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n         storageInfo.setLastBlockReportId(context.getReportId());\n         if (lastStorageInRpc) {\n           int rpcsSeen \u003d node.updateBlockReportContext(context);\n           if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n             List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n             if (zombies.isEmpty()) {\n               LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                   Long.toHexString(context.getReportId()));\n             } else {\n               for (DatanodeStorageInfo zombie : zombies) {\n                 removeZombieReplicas(context, zombie);\n               }\n             }\n             node.clearBlockReportContext();\n           } else {\n             LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                     \"report.\", Long.toHexString(context.getReportId()),\n                 (context.getTotalRpcs() - rpcsSeen)\n             );\n           }\n         }\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n         blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n             \"belong to any file\", b, node, b.getNumBytes());\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n         .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime));\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport, BlockReportContext context,\n      boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        LOG.info(\"Processing first storage report for \" +\n            storageInfo.getStorageID() + \" from datanode \" +\n            nodeID.getDatanodeUuid());\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n            \"belong to any file\", b, node, b.getNumBytes());\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n        .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime));\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "f9427f1760cce7e0befc3e066cebd0912652a411": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7980. Incremental BlockReport will dramatically slow down namenode startup.  Contributed by Walter Su\n",
      "commitDate": "07/05/15 11:36 AM",
      "commitName": "f9427f1760cce7e0befc3e066cebd0912652a411",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "01/05/15 8:42 AM",
      "commitNameOld": "279958b772c25e0633bd967828b7d27d5c0a6a56",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 6.12,
      "commitsBetweenForRepo": 64,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,88 +1,88 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport, BlockReportContext context,\n       boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         return !node.hasStaleStorages();\n       }\n \n-      if (storageInfo.numBlocks() \u003d\u003d 0) {\n+      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n       storageInfo.receivedBlockReport();\n       if (context !\u003d null) {\n         storageInfo.setLastBlockReportId(context.getReportId());\n         if (lastStorageInRpc) {\n           int rpcsSeen \u003d node.updateBlockReportContext(context);\n           if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n             List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n             if (zombies.isEmpty()) {\n               LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                   Long.toHexString(context.getReportId()));\n             } else {\n               for (DatanodeStorageInfo zombie : zombies) {\n                 removeZombieReplicas(context, zombie);\n               }\n             }\n             node.clearBlockReportContext();\n           } else {\n             LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                     \"report.\", Long.toHexString(context.getReportId()),\n                 (context.getTotalRpcs() - rpcsSeen)\n             );\n           }\n         }\n       }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n         blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n             \"belong to any file\", b, node, b.getNumBytes());\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n         .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime));\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport, BlockReportContext context,\n      boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.getBlockReportCount() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n            \"belong to any file\", b, node, b.getNumBytes());\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n        .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime));\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "50ee8f4e67a66aa77c5359182f61f3e951844db6": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7960. The full block report should prune zombie storages even if they\u0027re not empty. Contributed by Colin McCabe and Eddy Xu.\n",
      "commitDate": "23/03/15 10:00 PM",
      "commitName": "50ee8f4e67a66aa77c5359182f61f3e951844db6",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7960. The full block report should prune zombie storages even if they\u0027re not empty. Contributed by Colin McCabe and Eddy Xu.\n",
          "commitDate": "23/03/15 10:00 PM",
          "commitName": "50ee8f4e67a66aa77c5359182f61f3e951844db6",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "20/03/15 12:02 PM",
          "commitNameOld": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 3.42,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,64 +1,88 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n-      final BlockListAsLongs newReport) throws IOException {\n+      final BlockListAsLongs newReport, BlockReportContext context,\n+      boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n       storageInfo.receivedBlockReport();\n+      if (context !\u003d null) {\n+        storageInfo.setLastBlockReportId(context.getReportId());\n+        if (lastStorageInRpc) {\n+          int rpcsSeen \u003d node.updateBlockReportContext(context);\n+          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n+            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n+            if (zombies.isEmpty()) {\n+              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n+                  Long.toHexString(context.getReportId()));\n+            } else {\n+              for (DatanodeStorageInfo zombie : zombies) {\n+                removeZombieReplicas(context, zombie);\n+              }\n+            }\n+            node.clearBlockReportContext();\n+          } else {\n+            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n+                    \"report.\", Long.toHexString(context.getReportId()),\n+                (context.getTotalRpcs() - rpcsSeen)\n+            );\n+          }\n+        }\n+      }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n         blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n             \"belong to any file\", b, node, b.getNumBytes());\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n         .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime));\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport, BlockReportContext context,\n      boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n            \"belong to any file\", b, node, b.getNumBytes());\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n        .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime));\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[nodeID-DatanodeID(modifiers-final), storage-DatanodeStorage(modifiers-final), newReport-BlockListAsLongs(modifiers-final)]",
            "newValue": "[nodeID-DatanodeID(modifiers-final), storage-DatanodeStorage(modifiers-final), newReport-BlockListAsLongs(modifiers-final), context-BlockReportContext, lastStorageInRpc-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7960. The full block report should prune zombie storages even if they\u0027re not empty. Contributed by Colin McCabe and Eddy Xu.\n",
          "commitDate": "23/03/15 10:00 PM",
          "commitName": "50ee8f4e67a66aa77c5359182f61f3e951844db6",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "20/03/15 12:02 PM",
          "commitNameOld": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 3.42,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,64 +1,88 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n-      final BlockListAsLongs newReport) throws IOException {\n+      final BlockListAsLongs newReport, BlockReportContext context,\n+      boolean lastStorageInRpc) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n       storageInfo.receivedBlockReport();\n+      if (context !\u003d null) {\n+        storageInfo.setLastBlockReportId(context.getReportId());\n+        if (lastStorageInRpc) {\n+          int rpcsSeen \u003d node.updateBlockReportContext(context);\n+          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n+            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n+            if (zombies.isEmpty()) {\n+              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n+                  Long.toHexString(context.getReportId()));\n+            } else {\n+              for (DatanodeStorageInfo zombie : zombies) {\n+                removeZombieReplicas(context, zombie);\n+              }\n+            }\n+            node.clearBlockReportContext();\n+          } else {\n+            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n+                    \"report.\", Long.toHexString(context.getReportId()),\n+                (context.getTotalRpcs() - rpcsSeen)\n+            );\n+          }\n+        }\n+      }\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n         blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n             \"belong to any file\", b, node, b.getNumBytes());\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n         .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime));\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport, BlockReportContext context,\n      boolean lastStorageInRpc) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      storageInfo.receivedBlockReport();\n      if (context !\u003d null) {\n        storageInfo.setLastBlockReportId(context.getReportId());\n        if (lastStorageInRpc) {\n          int rpcsSeen \u003d node.updateBlockReportContext(context);\n          if (rpcsSeen \u003e\u003d context.getTotalRpcs()) {\n            List\u003cDatanodeStorageInfo\u003e zombies \u003d node.removeZombieStorages();\n            if (zombies.isEmpty()) {\n              LOG.debug(\"processReport 0x{}: no zombie storages found.\",\n                  Long.toHexString(context.getReportId()));\n            } else {\n              for (DatanodeStorageInfo zombie : zombies) {\n                removeZombieReplicas(context, zombie);\n              }\n            }\n            node.clearBlockReportContext();\n          } else {\n            LOG.debug(\"processReport 0x{}: {} more RPCs remaining in this \" +\n                    \"report.\", Long.toHexString(context.getReportId()),\n                (context.getTotalRpcs() - rpcsSeen)\n            );\n          }\n        }\n      }\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n            \"belong to any file\", b, node, b.getNumBytes());\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n        .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime));\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "19/03/15 2:29 PM",
      "commitNameOld": "978ef11f26794c22c7289582653b32268478e23e",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 0.9,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport) throws IOException {\n     namesystem.writeLock();\n-    final long startTime \u003d Time.now(); //after acquiring write lock\n+    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from {}\"\n             + \" because namenode still in startup phase\", nodeID);\n         return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n       storageInfo.receivedBlockReport();\n     } finally {\n-      endTime \u003d Time.now();\n+      endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n         blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n             \"belong to any file\", b, node, b.getNumBytes());\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n         \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n         .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n         node.hasStaleStorages(), (endTime - startTime));\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n            \"belong to any file\", b, node, b.getNumBytes());\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n        .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime));\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7712. Switch blockStateChangeLog to use slf4j.\n",
      "commitDate": "03/02/15 3:01 PM",
      "commitName": "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/01/15 11:33 AM",
      "commitNameOld": "951b3608a8cb1d9063b9be9c740b524c137b816f",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.14,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,64 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.now(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n-            + \"discarded non-initial block report from \" + nodeID\n-            + \" because namenode still in startup phase\");\n+            + \"discarded non-initial block report from {}\"\n+            + \" because namenode still in startup phase\", nodeID);\n         return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n       storageInfo.receivedBlockReport();\n     } finally {\n       endTime \u003d Time.now();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n-        blockLog.info(\"BLOCK* processReport: \" + b + \" on \" + node\n-                          + \" size \" + b.getNumBytes()\n-                          + \" does not belong to any file\");\n+        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n+            \"belong to any file\", b, node, b.getNumBytes());\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n-    blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n-        + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n-        + \", hasStaleStorages: \" + node.hasStaleStorages()\n-        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n+    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n+        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n+        .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n+        node.hasStaleStorages(), (endTime - startTime));\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.now(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from {}\"\n            + \" because namenode still in startup phase\", nodeID);\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.now();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: {} on node {} size {} does not \" +\n            \"belong to any file\", b, node, b.getNumBytes());\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage {} node {}, \" +\n        \"blocks: {}, hasStaleStorage: {}, processing time: {} msecs\", storage\n        .getStorageID(), nodeID, newReport.getNumberOfBlocks(),\n        node.hasStaleStorages(), (endTime - startTime));\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "b7923a356e9f111619375b94d12749d634069347": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6425. Large postponedMisreplicatedBlocks has impact on blockReport latency. Contributed by Ming Ma.\n",
      "commitDate": "16/12/14 8:30 AM",
      "commitName": "b7923a356e9f111619375b94d12749d634069347",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "10/12/14 11:44 PM",
      "commitNameOld": "390642acf35f3d599271617d30ba26c2f6406fc1",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 5.37,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,65 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.now(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n \n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from \" + nodeID\n             + \" because namenode still in startup phase\");\n         return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n         invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n-      // Now that we have an up-to-date block report, we know that any\n-      // deletions from a previous NN iteration have been accounted for.\n-      boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n       storageInfo.receivedBlockReport();\n-      if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n-        LOG.info(\"BLOCK* processReport: Received first block report from \"\n-            + storage + \" after starting up or becoming active. Its block \"\n-            + \"contents are no longer considered stale\");\n-        rescanPostponedMisreplicatedBlocks();\n-      }\n-      \n     } finally {\n       endTime \u003d Time.now();\n       namesystem.writeUnlock();\n     }\n \n     if (invalidatedBlocks !\u003d null) {\n       for (Block b : invalidatedBlocks) {\n         blockLog.info(\"BLOCK* processReport: \" + b + \" on \" + node\n                           + \" size \" + b.getNumBytes()\n                           + \" does not belong to any file\");\n       }\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n         + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n         + \", hasStaleStorages: \" + node.hasStaleStorages()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.now(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from \" + nodeID\n            + \" because namenode still in startup phase\");\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      storageInfo.receivedBlockReport();\n    } finally {\n      endTime \u003d Time.now();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: \" + b + \" on \" + node\n                          + \" size \" + b.getNumBytes()\n                          + \" does not belong to any file\");\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n        + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n        + \", hasStaleStorages: \" + node.hasStaleStorages()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "390642acf35f3d599271617d30ba26c2f6406fc1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7503. Namenode restart after large deletions can cause slow processReport (Arpit Agarwal)\n",
      "commitDate": "10/12/14 11:44 PM",
      "commitName": "390642acf35f3d599271617d30ba26c2f6406fc1",
      "commitAuthor": "arp",
      "commitDateOld": "26/11/14 9:57 AM",
      "commitNameOld": "058af60c56207907f2bedf76df4284e86d923e0c",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 14.57,
      "commitsBetweenForRepo": 100,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,75 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage,\n       final BlockListAsLongs newReport) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.now(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n+    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n+\n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from \" + nodeID\n             + \" because namenode still in startup phase\");\n         return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         processFirstBlockReport(storageInfo, newReport);\n       } else {\n-        processReport(storageInfo, newReport);\n+        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n       }\n       \n       // Now that we have an up-to-date block report, we know that any\n       // deletions from a previous NN iteration have been accounted for.\n       boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n       storageInfo.receivedBlockReport();\n       if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n         LOG.info(\"BLOCK* processReport: Received first block report from \"\n             + storage + \" after starting up or becoming active. Its block \"\n             + \"contents are no longer considered stale\");\n         rescanPostponedMisreplicatedBlocks();\n       }\n       \n     } finally {\n       endTime \u003d Time.now();\n       namesystem.writeUnlock();\n     }\n \n+    if (invalidatedBlocks !\u003d null) {\n+      for (Block b : invalidatedBlocks) {\n+        blockLog.info(\"BLOCK* processReport: \" + b + \" on \" + node\n+                          + \" size \" + b.getNumBytes()\n+                          + \" does not belong to any file\");\n+      }\n+    }\n+\n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n         + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n         + \", hasStaleStorages: \" + node.hasStaleStorages()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.now(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    Collection\u003cBlock\u003e invalidatedBlocks \u003d null;\n\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from \" + nodeID\n            + \" because namenode still in startup phase\");\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        invalidatedBlocks \u003d processReport(storageInfo, newReport);\n      }\n      \n      // Now that we have an up-to-date block report, we know that any\n      // deletions from a previous NN iteration have been accounted for.\n      boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n      storageInfo.receivedBlockReport();\n      if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n        LOG.info(\"BLOCK* processReport: Received first block report from \"\n            + storage + \" after starting up or becoming active. Its block \"\n            + \"contents are no longer considered stale\");\n        rescanPostponedMisreplicatedBlocks();\n      }\n      \n    } finally {\n      endTime \u003d Time.now();\n      namesystem.writeUnlock();\n    }\n\n    if (invalidatedBlocks !\u003d null) {\n      for (Block b : invalidatedBlocks) {\n        blockLog.info(\"BLOCK* processReport: \" + b + \" on \" + node\n                          + \" size \" + b.getNumBytes()\n                          + \" does not belong to any file\");\n      }\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n        + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n        + \", hasStaleStorages: \" + node.hasStaleStorages()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/14 9:58 AM",
      "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "01/08/14 9:58 AM",
          "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "31/07/14 6:05 PM",
          "commitNameOld": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.66,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,65 +1,65 @@\n   public boolean processReport(final DatanodeID nodeID,\n-      final DatanodeStorage storage, final String poolId,\n+      final DatanodeStorage storage,\n       final BlockListAsLongs newReport) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.now(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from \" + nodeID\n             + \" because namenode still in startup phase\");\n         return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n-        processFirstBlockReport(node, storage.getStorageID(), newReport);\n+        processFirstBlockReport(storageInfo, newReport);\n       } else {\n-        processReport(node, storage, newReport);\n+        processReport(storageInfo, newReport);\n       }\n       \n       // Now that we have an up-to-date block report, we know that any\n       // deletions from a previous NN iteration have been accounted for.\n       boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n       storageInfo.receivedBlockReport();\n       if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n         LOG.info(\"BLOCK* processReport: Received first block report from \"\n             + storage + \" after starting up or becoming active. Its block \"\n             + \"contents are no longer considered stale\");\n         rescanPostponedMisreplicatedBlocks();\n       }\n       \n     } finally {\n       endTime \u003d Time.now();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n         + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n         + \", hasStaleStorages: \" + node.hasStaleStorages()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.now(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from \" + nodeID\n            + \" because namenode still in startup phase\");\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        processReport(storageInfo, newReport);\n      }\n      \n      // Now that we have an up-to-date block report, we know that any\n      // deletions from a previous NN iteration have been accounted for.\n      boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n      storageInfo.receivedBlockReport();\n      if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n        LOG.info(\"BLOCK* processReport: Received first block report from \"\n            + storage + \" after starting up or becoming active. Its block \"\n            + \"contents are no longer considered stale\");\n        rescanPostponedMisreplicatedBlocks();\n      }\n      \n    } finally {\n      endTime \u003d Time.now();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n        + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n        + \", hasStaleStorages: \" + node.hasStaleStorages()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[nodeID-DatanodeID(modifiers-final), storage-DatanodeStorage(modifiers-final), poolId-String(modifiers-final), newReport-BlockListAsLongs(modifiers-final)]",
            "newValue": "[nodeID-DatanodeID(modifiers-final), storage-DatanodeStorage(modifiers-final), newReport-BlockListAsLongs(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "01/08/14 9:58 AM",
          "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "31/07/14 6:05 PM",
          "commitNameOld": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.66,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,65 +1,65 @@\n   public boolean processReport(final DatanodeID nodeID,\n-      final DatanodeStorage storage, final String poolId,\n+      final DatanodeStorage storage,\n       final BlockListAsLongs newReport) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.now(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from \" + nodeID\n             + \" because namenode still in startup phase\");\n         return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n-        processFirstBlockReport(node, storage.getStorageID(), newReport);\n+        processFirstBlockReport(storageInfo, newReport);\n       } else {\n-        processReport(node, storage, newReport);\n+        processReport(storageInfo, newReport);\n       }\n       \n       // Now that we have an up-to-date block report, we know that any\n       // deletions from a previous NN iteration have been accounted for.\n       boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n       storageInfo.receivedBlockReport();\n       if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n         LOG.info(\"BLOCK* processReport: Received first block report from \"\n             + storage + \" after starting up or becoming active. Its block \"\n             + \"contents are no longer considered stale\");\n         rescanPostponedMisreplicatedBlocks();\n       }\n       \n     } finally {\n       endTime \u003d Time.now();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n         + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n         + \", hasStaleStorages: \" + node.hasStaleStorages()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage,\n      final BlockListAsLongs newReport) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.now(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from \" + nodeID\n            + \" because namenode still in startup phase\");\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(storageInfo, newReport);\n      } else {\n        processReport(storageInfo, newReport);\n      }\n      \n      // Now that we have an up-to-date block report, we know that any\n      // deletions from a previous NN iteration have been accounted for.\n      boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n      storageInfo.receivedBlockReport();\n      if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n        LOG.info(\"BLOCK* processReport: Received first block report from \"\n            + storage + \" after starting up or becoming active. Its block \"\n            + \"contents are no longer considered stale\");\n        rescanPostponedMisreplicatedBlocks();\n      }\n      \n    } finally {\n      endTime \u003d Time.now();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n        + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n        + \", hasStaleStorages: \" + node.hasStaleStorages()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "5ac06c8b381f1ab63aeb5117b26e90b28bef026a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6578. add toString method to DatanodeStorage for easier debugging. (Contributed by Yongjun Zhang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1604942 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/06/14 2:30 PM",
      "commitName": "5ac06c8b381f1ab63aeb5117b26e90b28bef026a",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "12/06/14 2:06 PM",
      "commitNameOld": "be01103af7e60fededeb76fa60776edc3f7018fa",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 11.02,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,65 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage, final String poolId,\n       final BlockListAsLongs newReport) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.now(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from \" + nodeID\n             + \" because namenode still in startup phase\");\n         return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         processFirstBlockReport(node, storage.getStorageID(), newReport);\n       } else {\n         processReport(node, storage, newReport);\n       }\n       \n       // Now that we have an up-to-date block report, we know that any\n       // deletions from a previous NN iteration have been accounted for.\n       boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n       storageInfo.receivedBlockReport();\n       if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n         LOG.info(\"BLOCK* processReport: Received first block report from \"\n             + storage + \" after starting up or becoming active. Its block \"\n             + \"contents are no longer considered stale\");\n         rescanPostponedMisreplicatedBlocks();\n       }\n       \n     } finally {\n       endTime \u003d Time.now();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n         + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n+        + \", hasStaleStorages: \" + node.hasStaleStorages()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage, final String poolId,\n      final BlockListAsLongs newReport) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.now(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from \" + nodeID\n            + \" because namenode still in startup phase\");\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(node, storage.getStorageID(), newReport);\n      } else {\n        processReport(node, storage, newReport);\n      }\n      \n      // Now that we have an up-to-date block report, we know that any\n      // deletions from a previous NN iteration have been accounted for.\n      boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n      storageInfo.receivedBlockReport();\n      if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n        LOG.info(\"BLOCK* processReport: Received first block report from \"\n            + storage + \" after starting up or becoming active. Its block \"\n            + \"contents are no longer considered stale\");\n        rescanPostponedMisreplicatedBlocks();\n      }\n      \n    } finally {\n      endTime \u003d Time.now();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n        + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n        + \", hasStaleStorages: \" + node.hasStaleStorages()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6094. The same block can be counted twice towards safe mode threshold. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1578478 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/03/14 10:37 AM",
      "commitName": "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "19/02/14 8:38 PM",
      "commitNameOld": "55aec006f499fcf4ec98568d594b0585836cfa5e",
      "commitAuthorOld": "",
      "daysBetweenCommits": 25.54,
      "commitsBetweenForRepo": 225,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,64 @@\n   public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage, final String poolId,\n       final BlockListAsLongs newReport) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.now(); //after acquiring write lock\n     final long endTime;\n     DatanodeDescriptor node;\n     try {\n       node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n       DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n \n       if (storageInfo \u003d\u003d null) {\n         // We handle this for backwards compatibility.\n         storageInfo \u003d node.updateStorage(storage);\n-        LOG.warn(\"Unknown storageId \" + storage.getStorageID() +\n-                    \", updating storageMap. This indicates a buggy \" +\n-                    \"DataNode that isn\u0027t heartbeating correctly.\");\n       }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from \" + nodeID\n             + \" because namenode still in startup phase\");\n         return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         processFirstBlockReport(node, storage.getStorageID(), newReport);\n       } else {\n         processReport(node, storage, newReport);\n       }\n       \n       // Now that we have an up-to-date block report, we know that any\n       // deletions from a previous NN iteration have been accounted for.\n       boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n       storageInfo.receivedBlockReport();\n       if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n         LOG.info(\"BLOCK* processReport: Received first block report from \"\n             + storage + \" after starting up or becoming active. Its block \"\n             + \"contents are no longer considered stale\");\n         rescanPostponedMisreplicatedBlocks();\n       }\n       \n     } finally {\n       endTime \u003d Time.now();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n     blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n         + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n     return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage, final String poolId,\n      final BlockListAsLongs newReport) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.now(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from \" + nodeID\n            + \" because namenode still in startup phase\");\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(node, storage.getStorageID(), newReport);\n      } else {\n        processReport(node, storage, newReport);\n      }\n      \n      // Now that we have an up-to-date block report, we know that any\n      // deletions from a previous NN iteration have been accounted for.\n      boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n      storageInfo.receivedBlockReport();\n      if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n        LOG.info(\"BLOCK* processReport: Received first block report from \"\n            + storage + \" after starting up or becoming active. Its block \"\n            + \"contents are no longer considered stale\");\n        rescanPostponedMisreplicatedBlocks();\n      }\n      \n    } finally {\n      endTime \u003d Time.now();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n        + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n    return !node.hasStaleStorages();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-5153. Datanode should send block reports for each storage in a separate message. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563254 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/01/14 1:00 PM",
      "commitName": "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5153. Datanode should send block reports for each storage in a separate message. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563254 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "31/01/14 1:00 PM",
          "commitName": "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "13/01/14 6:02 PM",
          "commitNameOld": "e210519d32b07923ea19d6ac159abd558a1d8c68",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 17.79,
          "commitsBetweenForRepo": 123,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,67 @@\n-  public void processReport(final DatanodeID nodeID,\n+  public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage, final String poolId,\n       final BlockListAsLongs newReport) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.now(); //after acquiring write lock\n     final long endTime;\n+    DatanodeDescriptor node;\n     try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n+      node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n-      final DatanodeStorageInfo storageInfo \u003d node.updateStorage(storage);\n+      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n+\n+      if (storageInfo \u003d\u003d null) {\n+        // We handle this for backwards compatibility.\n+        storageInfo \u003d node.updateStorage(storage);\n+        LOG.warn(\"Unknown storageId \" + storage.getStorageID() +\n+                    \", updating storageMap. This indicates a buggy \" +\n+                    \"DataNode that isn\u0027t heartbeating correctly.\");\n+      }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from \" + nodeID\n             + \" because namenode still in startup phase\");\n-        return;\n+        return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         processFirstBlockReport(node, storage.getStorageID(), newReport);\n       } else {\n         processReport(node, storage, newReport);\n       }\n       \n       // Now that we have an up-to-date block report, we know that any\n       // deletions from a previous NN iteration have been accounted for.\n       boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n       storageInfo.receivedBlockReport();\n       if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n         LOG.info(\"BLOCK* processReport: Received first block report from \"\n-            + node + \" after starting up or becoming active. Its block \"\n+            + storage + \" after starting up or becoming active. Its block \"\n             + \"contents are no longer considered stale\");\n         rescanPostponedMisreplicatedBlocks();\n       }\n       \n     } finally {\n       endTime \u003d Time.now();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n-    blockLog.info(\"BLOCK* processReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n+    blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n+        + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n+    return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage, final String poolId,\n      final BlockListAsLongs newReport) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.now(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n        LOG.warn(\"Unknown storageId \" + storage.getStorageID() +\n                    \", updating storageMap. This indicates a buggy \" +\n                    \"DataNode that isn\u0027t heartbeating correctly.\");\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from \" + nodeID\n            + \" because namenode still in startup phase\");\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(node, storage.getStorageID(), newReport);\n      } else {\n        processReport(node, storage, newReport);\n      }\n      \n      // Now that we have an up-to-date block report, we know that any\n      // deletions from a previous NN iteration have been accounted for.\n      boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n      storageInfo.receivedBlockReport();\n      if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n        LOG.info(\"BLOCK* processReport: Received first block report from \"\n            + storage + \" after starting up or becoming active. Its block \"\n            + \"contents are no longer considered stale\");\n        rescanPostponedMisreplicatedBlocks();\n      }\n      \n    } finally {\n      endTime \u003d Time.now();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n        + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "boolean"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5153. Datanode should send block reports for each storage in a separate message. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563254 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "31/01/14 1:00 PM",
          "commitName": "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "13/01/14 6:02 PM",
          "commitNameOld": "e210519d32b07923ea19d6ac159abd558a1d8c68",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 17.79,
          "commitsBetweenForRepo": 123,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,67 @@\n-  public void processReport(final DatanodeID nodeID,\n+  public boolean processReport(final DatanodeID nodeID,\n       final DatanodeStorage storage, final String poolId,\n       final BlockListAsLongs newReport) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.now(); //after acquiring write lock\n     final long endTime;\n+    DatanodeDescriptor node;\n     try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n+      node \u003d datanodeManager.getDatanode(nodeID);\n       if (node \u003d\u003d null || !node.isAlive) {\n         throw new IOException(\n             \"ProcessReport from dead or unregistered node: \" + nodeID);\n       }\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n-      final DatanodeStorageInfo storageInfo \u003d node.updateStorage(storage);\n+      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n+\n+      if (storageInfo \u003d\u003d null) {\n+        // We handle this for backwards compatibility.\n+        storageInfo \u003d node.updateStorage(storage);\n+        LOG.warn(\"Unknown storageId \" + storage.getStorageID() +\n+                    \", updating storageMap. This indicates a buggy \" +\n+                    \"DataNode that isn\u0027t heartbeating correctly.\");\n+      }\n       if (namesystem.isInStartupSafeMode()\n           \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n         blockLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from \" + nodeID\n             + \" because namenode still in startup phase\");\n-        return;\n+        return !node.hasStaleStorages();\n       }\n \n       if (storageInfo.numBlocks() \u003d\u003d 0) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         processFirstBlockReport(node, storage.getStorageID(), newReport);\n       } else {\n         processReport(node, storage, newReport);\n       }\n       \n       // Now that we have an up-to-date block report, we know that any\n       // deletions from a previous NN iteration have been accounted for.\n       boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n       storageInfo.receivedBlockReport();\n       if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n         LOG.info(\"BLOCK* processReport: Received first block report from \"\n-            + node + \" after starting up or becoming active. Its block \"\n+            + storage + \" after starting up or becoming active. Its block \"\n             + \"contents are no longer considered stale\");\n         rescanPostponedMisreplicatedBlocks();\n       }\n       \n     } finally {\n       endTime \u003d Time.now();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addBlockReport((int) (endTime - startTime));\n     }\n-    blockLog.info(\"BLOCK* processReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n+    blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n+        + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n+    return !node.hasStaleStorages();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public boolean processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage, final String poolId,\n      final BlockListAsLongs newReport) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.now(); //after acquiring write lock\n    final long endTime;\n    DatanodeDescriptor node;\n    try {\n      node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      DatanodeStorageInfo storageInfo \u003d node.getStorageInfo(storage.getStorageID());\n\n      if (storageInfo \u003d\u003d null) {\n        // We handle this for backwards compatibility.\n        storageInfo \u003d node.updateStorage(storage);\n        LOG.warn(\"Unknown storageId \" + storage.getStorageID() +\n                    \", updating storageMap. This indicates a buggy \" +\n                    \"DataNode that isn\u0027t heartbeating correctly.\");\n      }\n      if (namesystem.isInStartupSafeMode()\n          \u0026\u0026 storageInfo.getBlockReportCount() \u003e 0) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from \" + nodeID\n            + \" because namenode still in startup phase\");\n        return !node.hasStaleStorages();\n      }\n\n      if (storageInfo.numBlocks() \u003d\u003d 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(node, storage.getStorageID(), newReport);\n      } else {\n        processReport(node, storage, newReport);\n      }\n      \n      // Now that we have an up-to-date block report, we know that any\n      // deletions from a previous NN iteration have been accounted for.\n      boolean staleBefore \u003d storageInfo.areBlockContentsStale();\n      storageInfo.receivedBlockReport();\n      if (staleBefore \u0026\u0026 !storageInfo.areBlockContentsStale()) {\n        LOG.info(\"BLOCK* processReport: Received first block report from \"\n            + storage + \" after starting up or becoming active. Its block \"\n            + \"contents are no longer considered stale\");\n        rescanPostponedMisreplicatedBlocks();\n      }\n      \n    } finally {\n      endTime \u003d Time.now();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from storage \" + storage.getStorageID()\n        + \" node \" + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n    return !node.hasStaleStorages();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}