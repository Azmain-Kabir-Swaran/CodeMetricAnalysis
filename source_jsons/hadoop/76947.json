{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HHXORErasureDecodingStep.java",
  "functionName": "doDecodeMultiAndParity",
  "functionId": "doDecodeMultiAndParity___inputs-ByteBuffer[][]__outputs-ByteBuffer[][]__erasedLocationToFix-int[]__bufSize-int",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java",
  "functionStartLine": 265,
  "functionEndLine": 351,
  "numCommitsSeen": 3,
  "timeTaken": 990,
  "changeHistory": [
    "31ebccc96238136560f4210bdf6766fe18e0650c",
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743"
  ],
  "changeHistoryShort": {
    "31ebccc96238136560f4210bdf6766fe18e0650c": "Yexceptionschange",
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743": "Yintroduced"
  },
  "changeHistoryDetails": {
    "31ebccc96238136560f4210bdf6766fe18e0650c": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-12613. Native EC coder should implement release() as idempotent function. (Lei (Eddy) Xu)\n",
      "commitDate": "16/10/17 7:44 PM",
      "commitName": "31ebccc96238136560f4210bdf6766fe18e0650c",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "17/10/16 11:02 PM",
      "commitNameOld": "c023c748869063fb67d14ea996569c42578d1cea",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 363.86,
      "commitsBetweenForRepo": 2347,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,87 @@\n   private void doDecodeMultiAndParity(ByteBuffer[][] inputs,\n                                       ByteBuffer[][] outputs,\n-                                      int[] erasedLocationToFix, int bufSize) {\n+                                      int[] erasedLocationToFix, int bufSize)\n+      throws IOException {\n     final int numDataUnits \u003d rsRawDecoder.getNumDataUnits();\n     final int numParityUnits \u003d rsRawDecoder.getNumParityUnits();\n     final int numTotalUnits \u003d numDataUnits + numParityUnits;\n     int[] parityToFixFlag \u003d new int[numTotalUnits];\n \n     for (int i \u003d 0; i \u003c erasedLocationToFix.length; ++i) {\n       if (erasedLocationToFix[i] \u003e\u003d numDataUnits) {\n         parityToFixFlag[erasedLocationToFix[i]] \u003d 1;\n       }\n     }\n \n     int[] inputPositions \u003d new int[inputs[0].length];\n     for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n       if (inputs[0][i] !\u003d null) {\n         inputPositions[i] \u003d inputs[0][i].position();\n       }\n     }\n \n     // decoded first sub-stripe\n     rsRawDecoder.decode(inputs[0], erasedLocationToFix, outputs[0]);\n \n     for (int i \u003d 0; i \u003c inputs[0].length; i++) {\n       if (inputs[0][i] !\u003d null) {\n         // dataLen bytes consumed\n         inputs[0][i].position(inputPositions[i]);\n       }\n     }\n \n     ByteBuffer[] tempInput \u003d new ByteBuffer[numDataUnits];\n     for (int i \u003d 0; i \u003c numDataUnits; ++i) {\n       tempInput[i] \u003d inputs[0][i];\n //\n //      if (!isDirect \u0026\u0026 tempInput[i] !\u003d null) {\n //        tempInput[i].position(tempInput[i].position() - bufSize);\n //      }\n     }\n \n     for (int i \u003d 0; i \u003c erasedLocationToFix.length; ++i) {\n       if (erasedLocationToFix[i] \u003c numDataUnits) {\n         tempInput[erasedLocationToFix[i]] \u003d outputs[0][i];\n       }\n     }\n \n     ByteBuffer[] piggyBack \u003d HHUtil.getPiggyBacksFromInput(tempInput,\n             piggyBackIndex, numParityUnits, 0, xorRawEncoder);\n \n     for (int j \u003d numDataUnits + 1; j \u003c numTotalUnits; ++j) {\n       if (parityToFixFlag[j] \u003d\u003d 0 \u0026\u0026 inputs[1][j] !\u003d null) {\n         // f(b) + f(a1,a2,a3....)\n         for (int k \u003d inputs[1][j].position(),\n              m \u003d piggyBack[j - numDataUnits - 1].position();\n              k \u003c inputs[1][j].limit(); ++k, ++m) {\n           inputs[1][j].put(k, (byte)\n                   (inputs[1][j].get(k) ^\n                           piggyBack[j - numDataUnits - 1].get(m)));\n         }\n       }\n     }\n \n     // decoded second sub-stripe\n     rsRawDecoder.decode(inputs[1], erasedLocationToFix, outputs[1]);\n \n     // parity index \u003d 0, the data have no piggyBack\n     for (int j \u003d 0; j \u003c erasedLocationToFix.length; ++j) {\n       if (erasedLocationToFix[j] \u003c numTotalUnits\n               \u0026\u0026 erasedLocationToFix[j] \u003e numDataUnits) {\n         int parityIndex \u003d erasedLocationToFix[j] - numDataUnits - 1;\n         for (int k \u003d outputs[1][j].position(),\n              m \u003d piggyBack[parityIndex].position();\n              k \u003c outputs[1][j].limit(); ++k, ++m) {\n           outputs[1][j].put(k, (byte)\n                   (outputs[1][j].get(k) ^ piggyBack[parityIndex].get(m)));\n         }\n       }\n     }\n \n     for (int i \u003d 0; i \u003c inputs[0].length; i++) {\n       if (inputs[0][i] !\u003d null) {\n         // dataLen bytes consumed\n         inputs[0][i].position(inputPositions[i] + bufSize);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void doDecodeMultiAndParity(ByteBuffer[][] inputs,\n                                      ByteBuffer[][] outputs,\n                                      int[] erasedLocationToFix, int bufSize)\n      throws IOException {\n    final int numDataUnits \u003d rsRawDecoder.getNumDataUnits();\n    final int numParityUnits \u003d rsRawDecoder.getNumParityUnits();\n    final int numTotalUnits \u003d numDataUnits + numParityUnits;\n    int[] parityToFixFlag \u003d new int[numTotalUnits];\n\n    for (int i \u003d 0; i \u003c erasedLocationToFix.length; ++i) {\n      if (erasedLocationToFix[i] \u003e\u003d numDataUnits) {\n        parityToFixFlag[erasedLocationToFix[i]] \u003d 1;\n      }\n    }\n\n    int[] inputPositions \u003d new int[inputs[0].length];\n    for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n      if (inputs[0][i] !\u003d null) {\n        inputPositions[i] \u003d inputs[0][i].position();\n      }\n    }\n\n    // decoded first sub-stripe\n    rsRawDecoder.decode(inputs[0], erasedLocationToFix, outputs[0]);\n\n    for (int i \u003d 0; i \u003c inputs[0].length; i++) {\n      if (inputs[0][i] !\u003d null) {\n        // dataLen bytes consumed\n        inputs[0][i].position(inputPositions[i]);\n      }\n    }\n\n    ByteBuffer[] tempInput \u003d new ByteBuffer[numDataUnits];\n    for (int i \u003d 0; i \u003c numDataUnits; ++i) {\n      tempInput[i] \u003d inputs[0][i];\n//\n//      if (!isDirect \u0026\u0026 tempInput[i] !\u003d null) {\n//        tempInput[i].position(tempInput[i].position() - bufSize);\n//      }\n    }\n\n    for (int i \u003d 0; i \u003c erasedLocationToFix.length; ++i) {\n      if (erasedLocationToFix[i] \u003c numDataUnits) {\n        tempInput[erasedLocationToFix[i]] \u003d outputs[0][i];\n      }\n    }\n\n    ByteBuffer[] piggyBack \u003d HHUtil.getPiggyBacksFromInput(tempInput,\n            piggyBackIndex, numParityUnits, 0, xorRawEncoder);\n\n    for (int j \u003d numDataUnits + 1; j \u003c numTotalUnits; ++j) {\n      if (parityToFixFlag[j] \u003d\u003d 0 \u0026\u0026 inputs[1][j] !\u003d null) {\n        // f(b) + f(a1,a2,a3....)\n        for (int k \u003d inputs[1][j].position(),\n             m \u003d piggyBack[j - numDataUnits - 1].position();\n             k \u003c inputs[1][j].limit(); ++k, ++m) {\n          inputs[1][j].put(k, (byte)\n                  (inputs[1][j].get(k) ^\n                          piggyBack[j - numDataUnits - 1].get(m)));\n        }\n      }\n    }\n\n    // decoded second sub-stripe\n    rsRawDecoder.decode(inputs[1], erasedLocationToFix, outputs[1]);\n\n    // parity index \u003d 0, the data have no piggyBack\n    for (int j \u003d 0; j \u003c erasedLocationToFix.length; ++j) {\n      if (erasedLocationToFix[j] \u003c numTotalUnits\n              \u0026\u0026 erasedLocationToFix[j] \u003e numDataUnits) {\n        int parityIndex \u003d erasedLocationToFix[j] - numDataUnits - 1;\n        for (int k \u003d outputs[1][j].position(),\n             m \u003d piggyBack[parityIndex].position();\n             k \u003c outputs[1][j].limit(); ++k, ++m) {\n          outputs[1][j].put(k, (byte)\n                  (outputs[1][j].get(k) ^ piggyBack[parityIndex].get(m)));\n        }\n      }\n    }\n\n    for (int i \u003d 0; i \u003c inputs[0].length; i++) {\n      if (inputs[0][i] !\u003d null) {\n        // dataLen bytes consumed\n        inputs[0][i].position(inputPositions[i] + bufSize);\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[IOException]"
      }
    },
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-11828. Implement the Hitchhiker erasure coding algorithm. Contributed by Jack Liu Quan.\n\nChange-Id: If43475ccc2574df60949c947af562722db076251\n",
      "commitDate": "21/01/16 10:30 AM",
      "commitName": "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743",
      "commitAuthor": "Zhe Zhang",
      "diff": "@@ -0,0 +1,86 @@\n+  private void doDecodeMultiAndParity(ByteBuffer[][] inputs,\n+                                      ByteBuffer[][] outputs,\n+                                      int[] erasedLocationToFix, int bufSize) {\n+    final int numDataUnits \u003d rsRawDecoder.getNumDataUnits();\n+    final int numParityUnits \u003d rsRawDecoder.getNumParityUnits();\n+    final int numTotalUnits \u003d numDataUnits + numParityUnits;\n+    int[] parityToFixFlag \u003d new int[numTotalUnits];\n+\n+    for (int i \u003d 0; i \u003c erasedLocationToFix.length; ++i) {\n+      if (erasedLocationToFix[i] \u003e\u003d numDataUnits) {\n+        parityToFixFlag[erasedLocationToFix[i]] \u003d 1;\n+      }\n+    }\n+\n+    int[] inputPositions \u003d new int[inputs[0].length];\n+    for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n+      if (inputs[0][i] !\u003d null) {\n+        inputPositions[i] \u003d inputs[0][i].position();\n+      }\n+    }\n+\n+    // decoded first sub-stripe\n+    rsRawDecoder.decode(inputs[0], erasedLocationToFix, outputs[0]);\n+\n+    for (int i \u003d 0; i \u003c inputs[0].length; i++) {\n+      if (inputs[0][i] !\u003d null) {\n+        // dataLen bytes consumed\n+        inputs[0][i].position(inputPositions[i]);\n+      }\n+    }\n+\n+    ByteBuffer[] tempInput \u003d new ByteBuffer[numDataUnits];\n+    for (int i \u003d 0; i \u003c numDataUnits; ++i) {\n+      tempInput[i] \u003d inputs[0][i];\n+//\n+//      if (!isDirect \u0026\u0026 tempInput[i] !\u003d null) {\n+//        tempInput[i].position(tempInput[i].position() - bufSize);\n+//      }\n+    }\n+\n+    for (int i \u003d 0; i \u003c erasedLocationToFix.length; ++i) {\n+      if (erasedLocationToFix[i] \u003c numDataUnits) {\n+        tempInput[erasedLocationToFix[i]] \u003d outputs[0][i];\n+      }\n+    }\n+\n+    ByteBuffer[] piggyBack \u003d HHUtil.getPiggyBacksFromInput(tempInput,\n+            piggyBackIndex, numParityUnits, 0, xorRawEncoder);\n+\n+    for (int j \u003d numDataUnits + 1; j \u003c numTotalUnits; ++j) {\n+      if (parityToFixFlag[j] \u003d\u003d 0 \u0026\u0026 inputs[1][j] !\u003d null) {\n+        // f(b) + f(a1,a2,a3....)\n+        for (int k \u003d inputs[1][j].position(),\n+             m \u003d piggyBack[j - numDataUnits - 1].position();\n+             k \u003c inputs[1][j].limit(); ++k, ++m) {\n+          inputs[1][j].put(k, (byte)\n+                  (inputs[1][j].get(k) ^\n+                          piggyBack[j - numDataUnits - 1].get(m)));\n+        }\n+      }\n+    }\n+\n+    // decoded second sub-stripe\n+    rsRawDecoder.decode(inputs[1], erasedLocationToFix, outputs[1]);\n+\n+    // parity index \u003d 0, the data have no piggyBack\n+    for (int j \u003d 0; j \u003c erasedLocationToFix.length; ++j) {\n+      if (erasedLocationToFix[j] \u003c numTotalUnits\n+              \u0026\u0026 erasedLocationToFix[j] \u003e numDataUnits) {\n+        int parityIndex \u003d erasedLocationToFix[j] - numDataUnits - 1;\n+        for (int k \u003d outputs[1][j].position(),\n+             m \u003d piggyBack[parityIndex].position();\n+             k \u003c outputs[1][j].limit(); ++k, ++m) {\n+          outputs[1][j].put(k, (byte)\n+                  (outputs[1][j].get(k) ^ piggyBack[parityIndex].get(m)));\n+        }\n+      }\n+    }\n+\n+    for (int i \u003d 0; i \u003c inputs[0].length; i++) {\n+      if (inputs[0][i] !\u003d null) {\n+        // dataLen bytes consumed\n+        inputs[0][i].position(inputPositions[i] + bufSize);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void doDecodeMultiAndParity(ByteBuffer[][] inputs,\n                                      ByteBuffer[][] outputs,\n                                      int[] erasedLocationToFix, int bufSize) {\n    final int numDataUnits \u003d rsRawDecoder.getNumDataUnits();\n    final int numParityUnits \u003d rsRawDecoder.getNumParityUnits();\n    final int numTotalUnits \u003d numDataUnits + numParityUnits;\n    int[] parityToFixFlag \u003d new int[numTotalUnits];\n\n    for (int i \u003d 0; i \u003c erasedLocationToFix.length; ++i) {\n      if (erasedLocationToFix[i] \u003e\u003d numDataUnits) {\n        parityToFixFlag[erasedLocationToFix[i]] \u003d 1;\n      }\n    }\n\n    int[] inputPositions \u003d new int[inputs[0].length];\n    for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n      if (inputs[0][i] !\u003d null) {\n        inputPositions[i] \u003d inputs[0][i].position();\n      }\n    }\n\n    // decoded first sub-stripe\n    rsRawDecoder.decode(inputs[0], erasedLocationToFix, outputs[0]);\n\n    for (int i \u003d 0; i \u003c inputs[0].length; i++) {\n      if (inputs[0][i] !\u003d null) {\n        // dataLen bytes consumed\n        inputs[0][i].position(inputPositions[i]);\n      }\n    }\n\n    ByteBuffer[] tempInput \u003d new ByteBuffer[numDataUnits];\n    for (int i \u003d 0; i \u003c numDataUnits; ++i) {\n      tempInput[i] \u003d inputs[0][i];\n//\n//      if (!isDirect \u0026\u0026 tempInput[i] !\u003d null) {\n//        tempInput[i].position(tempInput[i].position() - bufSize);\n//      }\n    }\n\n    for (int i \u003d 0; i \u003c erasedLocationToFix.length; ++i) {\n      if (erasedLocationToFix[i] \u003c numDataUnits) {\n        tempInput[erasedLocationToFix[i]] \u003d outputs[0][i];\n      }\n    }\n\n    ByteBuffer[] piggyBack \u003d HHUtil.getPiggyBacksFromInput(tempInput,\n            piggyBackIndex, numParityUnits, 0, xorRawEncoder);\n\n    for (int j \u003d numDataUnits + 1; j \u003c numTotalUnits; ++j) {\n      if (parityToFixFlag[j] \u003d\u003d 0 \u0026\u0026 inputs[1][j] !\u003d null) {\n        // f(b) + f(a1,a2,a3....)\n        for (int k \u003d inputs[1][j].position(),\n             m \u003d piggyBack[j - numDataUnits - 1].position();\n             k \u003c inputs[1][j].limit(); ++k, ++m) {\n          inputs[1][j].put(k, (byte)\n                  (inputs[1][j].get(k) ^\n                          piggyBack[j - numDataUnits - 1].get(m)));\n        }\n      }\n    }\n\n    // decoded second sub-stripe\n    rsRawDecoder.decode(inputs[1], erasedLocationToFix, outputs[1]);\n\n    // parity index \u003d 0, the data have no piggyBack\n    for (int j \u003d 0; j \u003c erasedLocationToFix.length; ++j) {\n      if (erasedLocationToFix[j] \u003c numTotalUnits\n              \u0026\u0026 erasedLocationToFix[j] \u003e numDataUnits) {\n        int parityIndex \u003d erasedLocationToFix[j] - numDataUnits - 1;\n        for (int k \u003d outputs[1][j].position(),\n             m \u003d piggyBack[parityIndex].position();\n             k \u003c outputs[1][j].limit(); ++k, ++m) {\n          outputs[1][j].put(k, (byte)\n                  (outputs[1][j].get(k) ^ piggyBack[parityIndex].get(m)));\n        }\n      }\n    }\n\n    for (int i \u003d 0; i \u003c inputs[0].length; i++) {\n      if (inputs[0][i] !\u003d null) {\n        // dataLen bytes consumed\n        inputs[0][i].position(inputPositions[i] + bufSize);\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java"
    }
  }
}