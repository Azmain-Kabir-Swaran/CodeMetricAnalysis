{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockPlacementPolicyDefault.java",
  "functionName": "chooseTarget",
  "functionId": "chooseTarget___srcPath-String__numOfReplicas-int__writer-Node__chosen-List__DatanodeStorageInfo____returnChosenNodes-boolean__excludedNodes-Set__Node____blocksize-long__storagePolicy-BlockStoragePolicy__flags-EnumSet__AddBlockFlag____storageTypes-EnumMap__StorageType,Integer__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
  "functionStartLine": 157,
  "functionEndLine": 163,
  "numCommitsSeen": 266,
  "timeTaken": 6533,
  "changeHistory": [
    "c1caab40f27e3e4f58ff1b5ef3e93efc56bbecbe",
    "849c45db187224095b13fe297a4d7377fbb9d2cd",
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
    "0f5f9846edab3ea7e80f35000072136f998bcd46",
    "ed841dd9a96e54cb84d9cae5507e47ff1c8cdf6e",
    "ac5e8aed7ca1e9493f96f8795d0caafd5282b9a7"
  ],
  "changeHistoryShort": {
    "c1caab40f27e3e4f58ff1b5ef3e93efc56bbecbe": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange,Yparametermetachange)",
    "849c45db187224095b13fe297a4d7377fbb9d2cd": "Ybodychange",
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7": "Ymultichange(Yparameterchange,Ybodychange)",
    "0f5f9846edab3ea7e80f35000072136f998bcd46": "Ybodychange",
    "ed841dd9a96e54cb84d9cae5507e47ff1c8cdf6e": "Ybodychange",
    "ac5e8aed7ca1e9493f96f8795d0caafd5282b9a7": "Ybodychange"
  },
  "changeHistoryDetails": {
    "c1caab40f27e3e4f58ff1b5ef3e93efc56bbecbe": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange,Yparametermetachange)",
      "commitMessage": "HDFS-14512. ONE_SSD policy will be violated while write data with DistributedFileSystem.create(....favoredNodes). Contributed by Ayush Saxena.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "29/05/19 8:56 PM",
      "commitName": "c1caab40f27e3e4f58ff1b5ef3e93efc56bbecbe",
      "commitAuthor": "Ayush Saxena",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-14512. ONE_SSD policy will be violated while write data with DistributedFileSystem.create(....favoredNodes). Contributed by Ayush Saxena.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
          "commitDate": "29/05/19 8:56 PM",
          "commitName": "c1caab40f27e3e4f58ff1b5ef3e93efc56bbecbe",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "13/03/19 1:15 PM",
          "commitNameOld": "66357574ae1da09ced735da36bf7d80a40c3fa1b",
          "commitAuthorOld": "Erik Krogen",
          "daysBetweenCommits": 77.32,
          "commitsBetweenForRepo": 467,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,68 +1,7 @@\n-  private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n-                                    Node writer,\n-                                    List\u003cDatanodeStorageInfo\u003e chosenStorage,\n-                                    boolean returnChosenNodes,\n-                                    Set\u003cNode\u003e excludedNodes,\n-                                    long blocksize,\n-                                    final BlockStoragePolicy storagePolicy,\n-                                    EnumSet\u003cAddBlockFlag\u003e addBlockFlags) {\n-    if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n-      return DatanodeStorageInfo.EMPTY_ARRAY;\n-    }\n-      \n-    if (excludedNodes \u003d\u003d null) {\n-      excludedNodes \u003d new HashSet\u003c\u003e();\n-    }\n-     \n-    int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n-    numOfReplicas \u003d result[0];\n-    int maxNodesPerRack \u003d result[1];\n-      \n-    for (DatanodeStorageInfo storage : chosenStorage) {\n-      // add localMachine and related nodes to excludedNodes\n-      addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n-    }\n-\n-    List\u003cDatanodeStorageInfo\u003e results \u003d null;\n-    Node localNode \u003d null;\n-    boolean avoidStaleNodes \u003d (stats !\u003d null\n-        \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n-    boolean avoidLocalNode \u003d (addBlockFlags !\u003d null\n-        \u0026\u0026 addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)\n-        \u0026\u0026 writer !\u003d null\n-        \u0026\u0026 !excludedNodes.contains(writer));\n-    // Attempt to exclude local node if the client suggests so. If no enough\n-    // nodes can be obtained, it falls back to the default block placement\n-    // policy.\n-    if (avoidLocalNode) {\n-      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n-      Set\u003cNode\u003e excludedNodeCopy \u003d new HashSet\u003c\u003e(excludedNodes);\n-      if (writer !\u003d null) {\n-        excludedNodeCopy.add(writer);\n-      }\n-      localNode \u003d chooseTarget(numOfReplicas, writer,\n-          excludedNodeCopy, blocksize, maxNodesPerRack, results,\n-          avoidStaleNodes, storagePolicy,\n-          EnumSet.noneOf(StorageType.class), results.isEmpty());\n-      if (results.size() \u003c numOfReplicas) {\n-        // not enough nodes; discard results and fall back\n-        results \u003d null;\n-      }\n-    }\n-    if (results \u003d\u003d null) {\n-      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n-      localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n-          blocksize, maxNodesPerRack, results, avoidStaleNodes,\n-          storagePolicy, EnumSet.noneOf(StorageType.class), results.isEmpty());\n-    }\n-\n-    if (!returnChosenNodes) {  \n-      results.removeAll(chosenStorage);\n-    }\n-      \n-    // sorting nodes to form a pipeline\n-    return getPipeline(\n-        (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n-            : localNode,\n-        results.toArray(new DatanodeStorageInfo[results.size()]));\n+  public DatanodeStorageInfo[] chooseTarget(String srcPath, int numOfReplicas,\n+      Node writer, List\u003cDatanodeStorageInfo\u003e chosen, boolean returnChosenNodes,\n+      Set\u003cNode\u003e excludedNodes, long blocksize, BlockStoragePolicy storagePolicy,\n+      EnumSet\u003cAddBlockFlag\u003e flags, EnumMap\u003cStorageType, Integer\u003e storageTypes) {\n+    return chooseTarget(numOfReplicas, writer, chosen, returnChosenNodes,\n+        excludedNodes, blocksize, storagePolicy, flags, storageTypes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DatanodeStorageInfo[] chooseTarget(String srcPath, int numOfReplicas,\n      Node writer, List\u003cDatanodeStorageInfo\u003e chosen, boolean returnChosenNodes,\n      Set\u003cNode\u003e excludedNodes, long blocksize, BlockStoragePolicy storagePolicy,\n      EnumSet\u003cAddBlockFlag\u003e flags, EnumMap\u003cStorageType, Integer\u003e storageTypes) {\n    return chooseTarget(numOfReplicas, writer, chosen, returnChosenNodes,\n        excludedNodes, blocksize, storagePolicy, flags, storageTypes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
          "extendedDetails": {
            "oldValue": "[numOfReplicas-int, writer-Node, chosenStorage-List\u003cDatanodeStorageInfo\u003e, returnChosenNodes-boolean, excludedNodes-Set\u003cNode\u003e, blocksize-long, storagePolicy-BlockStoragePolicy(modifiers-final), addBlockFlags-EnumSet\u003cAddBlockFlag\u003e]",
            "newValue": "[srcPath-String, numOfReplicas-int, writer-Node, chosen-List\u003cDatanodeStorageInfo\u003e, returnChosenNodes-boolean, excludedNodes-Set\u003cNode\u003e, blocksize-long, storagePolicy-BlockStoragePolicy, flags-EnumSet\u003cAddBlockFlag\u003e, storageTypes-EnumMap\u003cStorageType,Integer\u003e]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-14512. ONE_SSD policy will be violated while write data with DistributedFileSystem.create(....favoredNodes). Contributed by Ayush Saxena.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
          "commitDate": "29/05/19 8:56 PM",
          "commitName": "c1caab40f27e3e4f58ff1b5ef3e93efc56bbecbe",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "13/03/19 1:15 PM",
          "commitNameOld": "66357574ae1da09ced735da36bf7d80a40c3fa1b",
          "commitAuthorOld": "Erik Krogen",
          "daysBetweenCommits": 77.32,
          "commitsBetweenForRepo": 467,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,68 +1,7 @@\n-  private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n-                                    Node writer,\n-                                    List\u003cDatanodeStorageInfo\u003e chosenStorage,\n-                                    boolean returnChosenNodes,\n-                                    Set\u003cNode\u003e excludedNodes,\n-                                    long blocksize,\n-                                    final BlockStoragePolicy storagePolicy,\n-                                    EnumSet\u003cAddBlockFlag\u003e addBlockFlags) {\n-    if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n-      return DatanodeStorageInfo.EMPTY_ARRAY;\n-    }\n-      \n-    if (excludedNodes \u003d\u003d null) {\n-      excludedNodes \u003d new HashSet\u003c\u003e();\n-    }\n-     \n-    int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n-    numOfReplicas \u003d result[0];\n-    int maxNodesPerRack \u003d result[1];\n-      \n-    for (DatanodeStorageInfo storage : chosenStorage) {\n-      // add localMachine and related nodes to excludedNodes\n-      addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n-    }\n-\n-    List\u003cDatanodeStorageInfo\u003e results \u003d null;\n-    Node localNode \u003d null;\n-    boolean avoidStaleNodes \u003d (stats !\u003d null\n-        \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n-    boolean avoidLocalNode \u003d (addBlockFlags !\u003d null\n-        \u0026\u0026 addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)\n-        \u0026\u0026 writer !\u003d null\n-        \u0026\u0026 !excludedNodes.contains(writer));\n-    // Attempt to exclude local node if the client suggests so. If no enough\n-    // nodes can be obtained, it falls back to the default block placement\n-    // policy.\n-    if (avoidLocalNode) {\n-      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n-      Set\u003cNode\u003e excludedNodeCopy \u003d new HashSet\u003c\u003e(excludedNodes);\n-      if (writer !\u003d null) {\n-        excludedNodeCopy.add(writer);\n-      }\n-      localNode \u003d chooseTarget(numOfReplicas, writer,\n-          excludedNodeCopy, blocksize, maxNodesPerRack, results,\n-          avoidStaleNodes, storagePolicy,\n-          EnumSet.noneOf(StorageType.class), results.isEmpty());\n-      if (results.size() \u003c numOfReplicas) {\n-        // not enough nodes; discard results and fall back\n-        results \u003d null;\n-      }\n-    }\n-    if (results \u003d\u003d null) {\n-      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n-      localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n-          blocksize, maxNodesPerRack, results, avoidStaleNodes,\n-          storagePolicy, EnumSet.noneOf(StorageType.class), results.isEmpty());\n-    }\n-\n-    if (!returnChosenNodes) {  \n-      results.removeAll(chosenStorage);\n-    }\n-      \n-    // sorting nodes to form a pipeline\n-    return getPipeline(\n-        (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n-            : localNode,\n-        results.toArray(new DatanodeStorageInfo[results.size()]));\n+  public DatanodeStorageInfo[] chooseTarget(String srcPath, int numOfReplicas,\n+      Node writer, List\u003cDatanodeStorageInfo\u003e chosen, boolean returnChosenNodes,\n+      Set\u003cNode\u003e excludedNodes, long blocksize, BlockStoragePolicy storagePolicy,\n+      EnumSet\u003cAddBlockFlag\u003e flags, EnumMap\u003cStorageType, Integer\u003e storageTypes) {\n+    return chooseTarget(numOfReplicas, writer, chosen, returnChosenNodes,\n+        excludedNodes, blocksize, storagePolicy, flags, storageTypes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DatanodeStorageInfo[] chooseTarget(String srcPath, int numOfReplicas,\n      Node writer, List\u003cDatanodeStorageInfo\u003e chosen, boolean returnChosenNodes,\n      Set\u003cNode\u003e excludedNodes, long blocksize, BlockStoragePolicy storagePolicy,\n      EnumSet\u003cAddBlockFlag\u003e flags, EnumMap\u003cStorageType, Integer\u003e storageTypes) {\n    return chooseTarget(numOfReplicas, writer, chosen, returnChosenNodes,\n        excludedNodes, blocksize, storagePolicy, flags, storageTypes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-14512. ONE_SSD policy will be violated while write data with DistributedFileSystem.create(....favoredNodes). Contributed by Ayush Saxena.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
          "commitDate": "29/05/19 8:56 PM",
          "commitName": "c1caab40f27e3e4f58ff1b5ef3e93efc56bbecbe",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "13/03/19 1:15 PM",
          "commitNameOld": "66357574ae1da09ced735da36bf7d80a40c3fa1b",
          "commitAuthorOld": "Erik Krogen",
          "daysBetweenCommits": 77.32,
          "commitsBetweenForRepo": 467,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,68 +1,7 @@\n-  private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n-                                    Node writer,\n-                                    List\u003cDatanodeStorageInfo\u003e chosenStorage,\n-                                    boolean returnChosenNodes,\n-                                    Set\u003cNode\u003e excludedNodes,\n-                                    long blocksize,\n-                                    final BlockStoragePolicy storagePolicy,\n-                                    EnumSet\u003cAddBlockFlag\u003e addBlockFlags) {\n-    if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n-      return DatanodeStorageInfo.EMPTY_ARRAY;\n-    }\n-      \n-    if (excludedNodes \u003d\u003d null) {\n-      excludedNodes \u003d new HashSet\u003c\u003e();\n-    }\n-     \n-    int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n-    numOfReplicas \u003d result[0];\n-    int maxNodesPerRack \u003d result[1];\n-      \n-    for (DatanodeStorageInfo storage : chosenStorage) {\n-      // add localMachine and related nodes to excludedNodes\n-      addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n-    }\n-\n-    List\u003cDatanodeStorageInfo\u003e results \u003d null;\n-    Node localNode \u003d null;\n-    boolean avoidStaleNodes \u003d (stats !\u003d null\n-        \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n-    boolean avoidLocalNode \u003d (addBlockFlags !\u003d null\n-        \u0026\u0026 addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)\n-        \u0026\u0026 writer !\u003d null\n-        \u0026\u0026 !excludedNodes.contains(writer));\n-    // Attempt to exclude local node if the client suggests so. If no enough\n-    // nodes can be obtained, it falls back to the default block placement\n-    // policy.\n-    if (avoidLocalNode) {\n-      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n-      Set\u003cNode\u003e excludedNodeCopy \u003d new HashSet\u003c\u003e(excludedNodes);\n-      if (writer !\u003d null) {\n-        excludedNodeCopy.add(writer);\n-      }\n-      localNode \u003d chooseTarget(numOfReplicas, writer,\n-          excludedNodeCopy, blocksize, maxNodesPerRack, results,\n-          avoidStaleNodes, storagePolicy,\n-          EnumSet.noneOf(StorageType.class), results.isEmpty());\n-      if (results.size() \u003c numOfReplicas) {\n-        // not enough nodes; discard results and fall back\n-        results \u003d null;\n-      }\n-    }\n-    if (results \u003d\u003d null) {\n-      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n-      localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n-          blocksize, maxNodesPerRack, results, avoidStaleNodes,\n-          storagePolicy, EnumSet.noneOf(StorageType.class), results.isEmpty());\n-    }\n-\n-    if (!returnChosenNodes) {  \n-      results.removeAll(chosenStorage);\n-    }\n-      \n-    // sorting nodes to form a pipeline\n-    return getPipeline(\n-        (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n-            : localNode,\n-        results.toArray(new DatanodeStorageInfo[results.size()]));\n+  public DatanodeStorageInfo[] chooseTarget(String srcPath, int numOfReplicas,\n+      Node writer, List\u003cDatanodeStorageInfo\u003e chosen, boolean returnChosenNodes,\n+      Set\u003cNode\u003e excludedNodes, long blocksize, BlockStoragePolicy storagePolicy,\n+      EnumSet\u003cAddBlockFlag\u003e flags, EnumMap\u003cStorageType, Integer\u003e storageTypes) {\n+    return chooseTarget(numOfReplicas, writer, chosen, returnChosenNodes,\n+        excludedNodes, blocksize, storagePolicy, flags, storageTypes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DatanodeStorageInfo[] chooseTarget(String srcPath, int numOfReplicas,\n      Node writer, List\u003cDatanodeStorageInfo\u003e chosen, boolean returnChosenNodes,\n      Set\u003cNode\u003e excludedNodes, long blocksize, BlockStoragePolicy storagePolicy,\n      EnumSet\u003cAddBlockFlag\u003e flags, EnumMap\u003cStorageType, Integer\u003e storageTypes) {\n    return chooseTarget(numOfReplicas, writer, chosen, returnChosenNodes,\n        excludedNodes, blocksize, storagePolicy, flags, storageTypes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HDFS-14512. ONE_SSD policy will be violated while write data with DistributedFileSystem.create(....favoredNodes). Contributed by Ayush Saxena.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
          "commitDate": "29/05/19 8:56 PM",
          "commitName": "c1caab40f27e3e4f58ff1b5ef3e93efc56bbecbe",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "13/03/19 1:15 PM",
          "commitNameOld": "66357574ae1da09ced735da36bf7d80a40c3fa1b",
          "commitAuthorOld": "Erik Krogen",
          "daysBetweenCommits": 77.32,
          "commitsBetweenForRepo": 467,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,68 +1,7 @@\n-  private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n-                                    Node writer,\n-                                    List\u003cDatanodeStorageInfo\u003e chosenStorage,\n-                                    boolean returnChosenNodes,\n-                                    Set\u003cNode\u003e excludedNodes,\n-                                    long blocksize,\n-                                    final BlockStoragePolicy storagePolicy,\n-                                    EnumSet\u003cAddBlockFlag\u003e addBlockFlags) {\n-    if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n-      return DatanodeStorageInfo.EMPTY_ARRAY;\n-    }\n-      \n-    if (excludedNodes \u003d\u003d null) {\n-      excludedNodes \u003d new HashSet\u003c\u003e();\n-    }\n-     \n-    int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n-    numOfReplicas \u003d result[0];\n-    int maxNodesPerRack \u003d result[1];\n-      \n-    for (DatanodeStorageInfo storage : chosenStorage) {\n-      // add localMachine and related nodes to excludedNodes\n-      addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n-    }\n-\n-    List\u003cDatanodeStorageInfo\u003e results \u003d null;\n-    Node localNode \u003d null;\n-    boolean avoidStaleNodes \u003d (stats !\u003d null\n-        \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n-    boolean avoidLocalNode \u003d (addBlockFlags !\u003d null\n-        \u0026\u0026 addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)\n-        \u0026\u0026 writer !\u003d null\n-        \u0026\u0026 !excludedNodes.contains(writer));\n-    // Attempt to exclude local node if the client suggests so. If no enough\n-    // nodes can be obtained, it falls back to the default block placement\n-    // policy.\n-    if (avoidLocalNode) {\n-      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n-      Set\u003cNode\u003e excludedNodeCopy \u003d new HashSet\u003c\u003e(excludedNodes);\n-      if (writer !\u003d null) {\n-        excludedNodeCopy.add(writer);\n-      }\n-      localNode \u003d chooseTarget(numOfReplicas, writer,\n-          excludedNodeCopy, blocksize, maxNodesPerRack, results,\n-          avoidStaleNodes, storagePolicy,\n-          EnumSet.noneOf(StorageType.class), results.isEmpty());\n-      if (results.size() \u003c numOfReplicas) {\n-        // not enough nodes; discard results and fall back\n-        results \u003d null;\n-      }\n-    }\n-    if (results \u003d\u003d null) {\n-      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n-      localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n-          blocksize, maxNodesPerRack, results, avoidStaleNodes,\n-          storagePolicy, EnumSet.noneOf(StorageType.class), results.isEmpty());\n-    }\n-\n-    if (!returnChosenNodes) {  \n-      results.removeAll(chosenStorage);\n-    }\n-      \n-    // sorting nodes to form a pipeline\n-    return getPipeline(\n-        (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n-            : localNode,\n-        results.toArray(new DatanodeStorageInfo[results.size()]));\n+  public DatanodeStorageInfo[] chooseTarget(String srcPath, int numOfReplicas,\n+      Node writer, List\u003cDatanodeStorageInfo\u003e chosen, boolean returnChosenNodes,\n+      Set\u003cNode\u003e excludedNodes, long blocksize, BlockStoragePolicy storagePolicy,\n+      EnumSet\u003cAddBlockFlag\u003e flags, EnumMap\u003cStorageType, Integer\u003e storageTypes) {\n+    return chooseTarget(numOfReplicas, writer, chosen, returnChosenNodes,\n+        excludedNodes, blocksize, storagePolicy, flags, storageTypes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DatanodeStorageInfo[] chooseTarget(String srcPath, int numOfReplicas,\n      Node writer, List\u003cDatanodeStorageInfo\u003e chosen, boolean returnChosenNodes,\n      Set\u003cNode\u003e excludedNodes, long blocksize, BlockStoragePolicy storagePolicy,\n      EnumSet\u003cAddBlockFlag\u003e flags, EnumMap\u003cStorageType, Integer\u003e storageTypes) {\n    return chooseTarget(numOfReplicas, writer, chosen, returnChosenNodes,\n        excludedNodes, blocksize, storagePolicy, flags, storageTypes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
          "extendedDetails": {
            "oldValue": "[numOfReplicas-int, writer-Node, chosenStorage-List\u003cDatanodeStorageInfo\u003e, returnChosenNodes-boolean, excludedNodes-Set\u003cNode\u003e, blocksize-long, storagePolicy-BlockStoragePolicy(modifiers-final), addBlockFlags-EnumSet\u003cAddBlockFlag\u003e]",
            "newValue": "[srcPath-String, numOfReplicas-int, writer-Node, chosen-List\u003cDatanodeStorageInfo\u003e, returnChosenNodes-boolean, excludedNodes-Set\u003cNode\u003e, blocksize-long, storagePolicy-BlockStoragePolicy, flags-EnumSet\u003cAddBlockFlag\u003e, storageTypes-EnumMap\u003cStorageType,Integer\u003e]"
          }
        }
      ]
    },
    "849c45db187224095b13fe297a4d7377fbb9d2cd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13448. HDFS Block Placement - Ignore Locality for First Block Replica\n(Contributed by BELUGA BEHR via Daniel Templeton)\n\nChange-Id: I965d1cfa642ad24296038b83e3d5c9983545267d\n",
      "commitDate": "24/07/18 4:05 PM",
      "commitName": "849c45db187224095b13fe297a4d7377fbb9d2cd",
      "commitAuthor": "Daniel Templeton",
      "commitDateOld": "04/06/18 7:03 AM",
      "commitNameOld": "bccdfeee0aaef9cb98d09ee39909b63fdcbeeafc",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 50.38,
      "commitsBetweenForRepo": 323,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,68 @@\n   private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                     Node writer,\n                                     List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                     boolean returnChosenNodes,\n                                     Set\u003cNode\u003e excludedNodes,\n                                     long blocksize,\n                                     final BlockStoragePolicy storagePolicy,\n                                     EnumSet\u003cAddBlockFlag\u003e addBlockFlags) {\n     if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n       return DatanodeStorageInfo.EMPTY_ARRAY;\n     }\n       \n     if (excludedNodes \u003d\u003d null) {\n       excludedNodes \u003d new HashSet\u003c\u003e();\n     }\n      \n     int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n     numOfReplicas \u003d result[0];\n     int maxNodesPerRack \u003d result[1];\n       \n     for (DatanodeStorageInfo storage : chosenStorage) {\n       // add localMachine and related nodes to excludedNodes\n       addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n     }\n \n     List\u003cDatanodeStorageInfo\u003e results \u003d null;\n     Node localNode \u003d null;\n     boolean avoidStaleNodes \u003d (stats !\u003d null\n         \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n     boolean avoidLocalNode \u003d (addBlockFlags !\u003d null\n         \u0026\u0026 addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)\n         \u0026\u0026 writer !\u003d null\n         \u0026\u0026 !excludedNodes.contains(writer));\n     // Attempt to exclude local node if the client suggests so. If no enough\n     // nodes can be obtained, it falls back to the default block placement\n     // policy.\n     if (avoidLocalNode) {\n       results \u003d new ArrayList\u003c\u003e(chosenStorage);\n       Set\u003cNode\u003e excludedNodeCopy \u003d new HashSet\u003c\u003e(excludedNodes);\n-      excludedNodeCopy.add(writer);\n+      if (writer !\u003d null) {\n+        excludedNodeCopy.add(writer);\n+      }\n       localNode \u003d chooseTarget(numOfReplicas, writer,\n           excludedNodeCopy, blocksize, maxNodesPerRack, results,\n           avoidStaleNodes, storagePolicy,\n           EnumSet.noneOf(StorageType.class), results.isEmpty());\n       if (results.size() \u003c numOfReplicas) {\n         // not enough nodes; discard results and fall back\n         results \u003d null;\n       }\n     }\n     if (results \u003d\u003d null) {\n       results \u003d new ArrayList\u003c\u003e(chosenStorage);\n       localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n           blocksize, maxNodesPerRack, results, avoidStaleNodes,\n           storagePolicy, EnumSet.noneOf(StorageType.class), results.isEmpty());\n     }\n \n     if (!returnChosenNodes) {  \n       results.removeAll(chosenStorage);\n     }\n       \n     // sorting nodes to form a pipeline\n     return getPipeline(\n         (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n             : localNode,\n         results.toArray(new DatanodeStorageInfo[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                    Node writer,\n                                    List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                    boolean returnChosenNodes,\n                                    Set\u003cNode\u003e excludedNodes,\n                                    long blocksize,\n                                    final BlockStoragePolicy storagePolicy,\n                                    EnumSet\u003cAddBlockFlag\u003e addBlockFlags) {\n    if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n      return DatanodeStorageInfo.EMPTY_ARRAY;\n    }\n      \n    if (excludedNodes \u003d\u003d null) {\n      excludedNodes \u003d new HashSet\u003c\u003e();\n    }\n     \n    int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n    numOfReplicas \u003d result[0];\n    int maxNodesPerRack \u003d result[1];\n      \n    for (DatanodeStorageInfo storage : chosenStorage) {\n      // add localMachine and related nodes to excludedNodes\n      addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n    }\n\n    List\u003cDatanodeStorageInfo\u003e results \u003d null;\n    Node localNode \u003d null;\n    boolean avoidStaleNodes \u003d (stats !\u003d null\n        \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n    boolean avoidLocalNode \u003d (addBlockFlags !\u003d null\n        \u0026\u0026 addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)\n        \u0026\u0026 writer !\u003d null\n        \u0026\u0026 !excludedNodes.contains(writer));\n    // Attempt to exclude local node if the client suggests so. If no enough\n    // nodes can be obtained, it falls back to the default block placement\n    // policy.\n    if (avoidLocalNode) {\n      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n      Set\u003cNode\u003e excludedNodeCopy \u003d new HashSet\u003c\u003e(excludedNodes);\n      if (writer !\u003d null) {\n        excludedNodeCopy.add(writer);\n      }\n      localNode \u003d chooseTarget(numOfReplicas, writer,\n          excludedNodeCopy, blocksize, maxNodesPerRack, results,\n          avoidStaleNodes, storagePolicy,\n          EnumSet.noneOf(StorageType.class), results.isEmpty());\n      if (results.size() \u003c numOfReplicas) {\n        // not enough nodes; discard results and fall back\n        results \u003d null;\n      }\n    }\n    if (results \u003d\u003d null) {\n      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n      localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n          blocksize, maxNodesPerRack, results, avoidStaleNodes,\n          storagePolicy, EnumSet.noneOf(StorageType.class), results.isEmpty());\n    }\n\n    if (!returnChosenNodes) {  \n      results.removeAll(chosenStorage);\n    }\n      \n    // sorting nodes to form a pipeline\n    return getPipeline(\n        (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n            : localNode,\n        results.toArray(new DatanodeStorageInfo[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
      "extendedDetails": {}
    },
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-3702. Add an option for NOT writing the blocks locally if there is a datanode on the same box as the client. (Contributed by Lei (Eddy) Xu)\n",
      "commitDate": "27/04/16 2:22 PM",
      "commitName": "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
      "commitAuthor": "Lei Xu",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-3702. Add an option for NOT writing the blocks locally if there is a datanode on the same box as the client. (Contributed by Lei (Eddy) Xu)\n",
          "commitDate": "27/04/16 2:22 PM",
          "commitName": "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "18/04/16 5:58 AM",
          "commitNameOld": "d8b729e16fb253e6c84f414d419b5663d9219a43",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 9.35,
          "commitsBetweenForRepo": 63,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,66 @@\n   private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                     Node writer,\n                                     List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                     boolean returnChosenNodes,\n                                     Set\u003cNode\u003e excludedNodes,\n                                     long blocksize,\n-                                    final BlockStoragePolicy storagePolicy) {\n+                                    final BlockStoragePolicy storagePolicy,\n+                                    EnumSet\u003cAddBlockFlag\u003e addBlockFlags) {\n     if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n       return DatanodeStorageInfo.EMPTY_ARRAY;\n     }\n       \n     if (excludedNodes \u003d\u003d null) {\n       excludedNodes \u003d new HashSet\u003c\u003e();\n     }\n      \n     int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n     numOfReplicas \u003d result[0];\n     int maxNodesPerRack \u003d result[1];\n       \n-    final List\u003cDatanodeStorageInfo\u003e results \u003d new ArrayList\u003c\u003e(chosenStorage);\n     for (DatanodeStorageInfo storage : chosenStorage) {\n       // add localMachine and related nodes to excludedNodes\n       addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n     }\n \n+    List\u003cDatanodeStorageInfo\u003e results \u003d null;\n+    Node localNode \u003d null;\n     boolean avoidStaleNodes \u003d (stats !\u003d null\n         \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n-    final Node localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n-        blocksize, maxNodesPerRack, results, avoidStaleNodes, storagePolicy,\n-        EnumSet.noneOf(StorageType.class), results.isEmpty());\n+    boolean avoidLocalNode \u003d (addBlockFlags !\u003d null\n+        \u0026\u0026 addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)\n+        \u0026\u0026 writer !\u003d null\n+        \u0026\u0026 !excludedNodes.contains(writer));\n+    // Attempt to exclude local node if the client suggests so. If no enough\n+    // nodes can be obtained, it falls back to the default block placement\n+    // policy.\n+    if (avoidLocalNode) {\n+      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n+      Set\u003cNode\u003e excludedNodeCopy \u003d new HashSet\u003c\u003e(excludedNodes);\n+      excludedNodeCopy.add(writer);\n+      localNode \u003d chooseTarget(numOfReplicas, writer,\n+          excludedNodeCopy, blocksize, maxNodesPerRack, results,\n+          avoidStaleNodes, storagePolicy,\n+          EnumSet.noneOf(StorageType.class), results.isEmpty());\n+      if (results.size() \u003c numOfReplicas) {\n+        // not enough nodes; discard results and fall back\n+        results \u003d null;\n+      }\n+    }\n+    if (results \u003d\u003d null) {\n+      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n+      localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n+          blocksize, maxNodesPerRack, results, avoidStaleNodes,\n+          storagePolicy, EnumSet.noneOf(StorageType.class), results.isEmpty());\n+    }\n+\n     if (!returnChosenNodes) {  \n       results.removeAll(chosenStorage);\n     }\n       \n     // sorting nodes to form a pipeline\n     return getPipeline(\n         (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n             : localNode,\n         results.toArray(new DatanodeStorageInfo[results.size()]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                    Node writer,\n                                    List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                    boolean returnChosenNodes,\n                                    Set\u003cNode\u003e excludedNodes,\n                                    long blocksize,\n                                    final BlockStoragePolicy storagePolicy,\n                                    EnumSet\u003cAddBlockFlag\u003e addBlockFlags) {\n    if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n      return DatanodeStorageInfo.EMPTY_ARRAY;\n    }\n      \n    if (excludedNodes \u003d\u003d null) {\n      excludedNodes \u003d new HashSet\u003c\u003e();\n    }\n     \n    int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n    numOfReplicas \u003d result[0];\n    int maxNodesPerRack \u003d result[1];\n      \n    for (DatanodeStorageInfo storage : chosenStorage) {\n      // add localMachine and related nodes to excludedNodes\n      addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n    }\n\n    List\u003cDatanodeStorageInfo\u003e results \u003d null;\n    Node localNode \u003d null;\n    boolean avoidStaleNodes \u003d (stats !\u003d null\n        \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n    boolean avoidLocalNode \u003d (addBlockFlags !\u003d null\n        \u0026\u0026 addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)\n        \u0026\u0026 writer !\u003d null\n        \u0026\u0026 !excludedNodes.contains(writer));\n    // Attempt to exclude local node if the client suggests so. If no enough\n    // nodes can be obtained, it falls back to the default block placement\n    // policy.\n    if (avoidLocalNode) {\n      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n      Set\u003cNode\u003e excludedNodeCopy \u003d new HashSet\u003c\u003e(excludedNodes);\n      excludedNodeCopy.add(writer);\n      localNode \u003d chooseTarget(numOfReplicas, writer,\n          excludedNodeCopy, blocksize, maxNodesPerRack, results,\n          avoidStaleNodes, storagePolicy,\n          EnumSet.noneOf(StorageType.class), results.isEmpty());\n      if (results.size() \u003c numOfReplicas) {\n        // not enough nodes; discard results and fall back\n        results \u003d null;\n      }\n    }\n    if (results \u003d\u003d null) {\n      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n      localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n          blocksize, maxNodesPerRack, results, avoidStaleNodes,\n          storagePolicy, EnumSet.noneOf(StorageType.class), results.isEmpty());\n    }\n\n    if (!returnChosenNodes) {  \n      results.removeAll(chosenStorage);\n    }\n      \n    // sorting nodes to form a pipeline\n    return getPipeline(\n        (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n            : localNode,\n        results.toArray(new DatanodeStorageInfo[results.size()]));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
          "extendedDetails": {
            "oldValue": "[numOfReplicas-int, writer-Node, chosenStorage-List\u003cDatanodeStorageInfo\u003e, returnChosenNodes-boolean, excludedNodes-Set\u003cNode\u003e, blocksize-long, storagePolicy-BlockStoragePolicy(modifiers-final)]",
            "newValue": "[numOfReplicas-int, writer-Node, chosenStorage-List\u003cDatanodeStorageInfo\u003e, returnChosenNodes-boolean, excludedNodes-Set\u003cNode\u003e, blocksize-long, storagePolicy-BlockStoragePolicy(modifiers-final), addBlockFlags-EnumSet\u003cAddBlockFlag\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3702. Add an option for NOT writing the blocks locally if there is a datanode on the same box as the client. (Contributed by Lei (Eddy) Xu)\n",
          "commitDate": "27/04/16 2:22 PM",
          "commitName": "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "18/04/16 5:58 AM",
          "commitNameOld": "d8b729e16fb253e6c84f414d419b5663d9219a43",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 9.35,
          "commitsBetweenForRepo": 63,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,66 @@\n   private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                     Node writer,\n                                     List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                     boolean returnChosenNodes,\n                                     Set\u003cNode\u003e excludedNodes,\n                                     long blocksize,\n-                                    final BlockStoragePolicy storagePolicy) {\n+                                    final BlockStoragePolicy storagePolicy,\n+                                    EnumSet\u003cAddBlockFlag\u003e addBlockFlags) {\n     if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n       return DatanodeStorageInfo.EMPTY_ARRAY;\n     }\n       \n     if (excludedNodes \u003d\u003d null) {\n       excludedNodes \u003d new HashSet\u003c\u003e();\n     }\n      \n     int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n     numOfReplicas \u003d result[0];\n     int maxNodesPerRack \u003d result[1];\n       \n-    final List\u003cDatanodeStorageInfo\u003e results \u003d new ArrayList\u003c\u003e(chosenStorage);\n     for (DatanodeStorageInfo storage : chosenStorage) {\n       // add localMachine and related nodes to excludedNodes\n       addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n     }\n \n+    List\u003cDatanodeStorageInfo\u003e results \u003d null;\n+    Node localNode \u003d null;\n     boolean avoidStaleNodes \u003d (stats !\u003d null\n         \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n-    final Node localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n-        blocksize, maxNodesPerRack, results, avoidStaleNodes, storagePolicy,\n-        EnumSet.noneOf(StorageType.class), results.isEmpty());\n+    boolean avoidLocalNode \u003d (addBlockFlags !\u003d null\n+        \u0026\u0026 addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)\n+        \u0026\u0026 writer !\u003d null\n+        \u0026\u0026 !excludedNodes.contains(writer));\n+    // Attempt to exclude local node if the client suggests so. If no enough\n+    // nodes can be obtained, it falls back to the default block placement\n+    // policy.\n+    if (avoidLocalNode) {\n+      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n+      Set\u003cNode\u003e excludedNodeCopy \u003d new HashSet\u003c\u003e(excludedNodes);\n+      excludedNodeCopy.add(writer);\n+      localNode \u003d chooseTarget(numOfReplicas, writer,\n+          excludedNodeCopy, blocksize, maxNodesPerRack, results,\n+          avoidStaleNodes, storagePolicy,\n+          EnumSet.noneOf(StorageType.class), results.isEmpty());\n+      if (results.size() \u003c numOfReplicas) {\n+        // not enough nodes; discard results and fall back\n+        results \u003d null;\n+      }\n+    }\n+    if (results \u003d\u003d null) {\n+      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n+      localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n+          blocksize, maxNodesPerRack, results, avoidStaleNodes,\n+          storagePolicy, EnumSet.noneOf(StorageType.class), results.isEmpty());\n+    }\n+\n     if (!returnChosenNodes) {  \n       results.removeAll(chosenStorage);\n     }\n       \n     // sorting nodes to form a pipeline\n     return getPipeline(\n         (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n             : localNode,\n         results.toArray(new DatanodeStorageInfo[results.size()]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                    Node writer,\n                                    List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                    boolean returnChosenNodes,\n                                    Set\u003cNode\u003e excludedNodes,\n                                    long blocksize,\n                                    final BlockStoragePolicy storagePolicy,\n                                    EnumSet\u003cAddBlockFlag\u003e addBlockFlags) {\n    if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n      return DatanodeStorageInfo.EMPTY_ARRAY;\n    }\n      \n    if (excludedNodes \u003d\u003d null) {\n      excludedNodes \u003d new HashSet\u003c\u003e();\n    }\n     \n    int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n    numOfReplicas \u003d result[0];\n    int maxNodesPerRack \u003d result[1];\n      \n    for (DatanodeStorageInfo storage : chosenStorage) {\n      // add localMachine and related nodes to excludedNodes\n      addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n    }\n\n    List\u003cDatanodeStorageInfo\u003e results \u003d null;\n    Node localNode \u003d null;\n    boolean avoidStaleNodes \u003d (stats !\u003d null\n        \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n    boolean avoidLocalNode \u003d (addBlockFlags !\u003d null\n        \u0026\u0026 addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)\n        \u0026\u0026 writer !\u003d null\n        \u0026\u0026 !excludedNodes.contains(writer));\n    // Attempt to exclude local node if the client suggests so. If no enough\n    // nodes can be obtained, it falls back to the default block placement\n    // policy.\n    if (avoidLocalNode) {\n      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n      Set\u003cNode\u003e excludedNodeCopy \u003d new HashSet\u003c\u003e(excludedNodes);\n      excludedNodeCopy.add(writer);\n      localNode \u003d chooseTarget(numOfReplicas, writer,\n          excludedNodeCopy, blocksize, maxNodesPerRack, results,\n          avoidStaleNodes, storagePolicy,\n          EnumSet.noneOf(StorageType.class), results.isEmpty());\n      if (results.size() \u003c numOfReplicas) {\n        // not enough nodes; discard results and fall back\n        results \u003d null;\n      }\n    }\n    if (results \u003d\u003d null) {\n      results \u003d new ArrayList\u003c\u003e(chosenStorage);\n      localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n          blocksize, maxNodesPerRack, results, avoidStaleNodes,\n          storagePolicy, EnumSet.noneOf(StorageType.class), results.isEmpty());\n    }\n\n    if (!returnChosenNodes) {  \n      results.removeAll(chosenStorage);\n    }\n      \n    // sorting nodes to form a pipeline\n    return getPipeline(\n        (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n            : localNode,\n        results.toArray(new DatanodeStorageInfo[results.size()]));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
          "extendedDetails": {}
        }
      ]
    },
    "0f5f9846edab3ea7e80f35000072136f998bcd46": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9006. Provide BlockPlacementPolicy that supports upgrade domain. (Ming Ma via lei)\n",
      "commitDate": "12/10/15 4:24 PM",
      "commitName": "0f5f9846edab3ea7e80f35000072136f998bcd46",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "31/08/15 5:52 PM",
      "commitNameOld": "8fa41d9dd4b923bf4141f019414a1a8b079124c6",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 41.94,
      "commitsBetweenForRepo": 298,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                     Node writer,\n                                     List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                     boolean returnChosenNodes,\n                                     Set\u003cNode\u003e excludedNodes,\n                                     long blocksize,\n                                     final BlockStoragePolicy storagePolicy) {\n     if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n       return DatanodeStorageInfo.EMPTY_ARRAY;\n     }\n       \n     if (excludedNodes \u003d\u003d null) {\n-      excludedNodes \u003d new HashSet\u003cNode\u003e();\n+      excludedNodes \u003d new HashSet\u003c\u003e();\n     }\n      \n     int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n     numOfReplicas \u003d result[0];\n     int maxNodesPerRack \u003d result[1];\n       \n-    final List\u003cDatanodeStorageInfo\u003e results \u003d new ArrayList\u003cDatanodeStorageInfo\u003e(chosenStorage);\n+    final List\u003cDatanodeStorageInfo\u003e results \u003d new ArrayList\u003c\u003e(chosenStorage);\n     for (DatanodeStorageInfo storage : chosenStorage) {\n       // add localMachine and related nodes to excludedNodes\n       addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n     }\n \n     boolean avoidStaleNodes \u003d (stats !\u003d null\n         \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n     final Node localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n         blocksize, maxNodesPerRack, results, avoidStaleNodes, storagePolicy,\n         EnumSet.noneOf(StorageType.class), results.isEmpty());\n     if (!returnChosenNodes) {  \n       results.removeAll(chosenStorage);\n     }\n       \n     // sorting nodes to form a pipeline\n     return getPipeline(\n         (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n             : localNode,\n         results.toArray(new DatanodeStorageInfo[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                    Node writer,\n                                    List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                    boolean returnChosenNodes,\n                                    Set\u003cNode\u003e excludedNodes,\n                                    long blocksize,\n                                    final BlockStoragePolicy storagePolicy) {\n    if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n      return DatanodeStorageInfo.EMPTY_ARRAY;\n    }\n      \n    if (excludedNodes \u003d\u003d null) {\n      excludedNodes \u003d new HashSet\u003c\u003e();\n    }\n     \n    int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n    numOfReplicas \u003d result[0];\n    int maxNodesPerRack \u003d result[1];\n      \n    final List\u003cDatanodeStorageInfo\u003e results \u003d new ArrayList\u003c\u003e(chosenStorage);\n    for (DatanodeStorageInfo storage : chosenStorage) {\n      // add localMachine and related nodes to excludedNodes\n      addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n    }\n\n    boolean avoidStaleNodes \u003d (stats !\u003d null\n        \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n    final Node localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n        blocksize, maxNodesPerRack, results, avoidStaleNodes, storagePolicy,\n        EnumSet.noneOf(StorageType.class), results.isEmpty());\n    if (!returnChosenNodes) {  \n      results.removeAll(chosenStorage);\n    }\n      \n    // sorting nodes to form a pipeline\n    return getPipeline(\n        (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n            : localNode,\n        results.toArray(new DatanodeStorageInfo[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
      "extendedDetails": {}
    },
    "ed841dd9a96e54cb84d9cae5507e47ff1c8cdf6e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6995. Block should be placed in the client\u0027s \u0027rack-local\u0027 node if \u0027client-local\u0027 node is not available (vinayakumarb)\n",
      "commitDate": "06/10/14 2:01 AM",
      "commitName": "ed841dd9a96e54cb84d9cae5507e47ff1c8cdf6e",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "24/09/14 10:05 AM",
      "commitNameOld": "073bbd805c6680f47bbfcc6e8efd708ad729bca4",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 11.66,
      "commitsBetweenForRepo": 131,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,40 @@\n   private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                     Node writer,\n                                     List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                     boolean returnChosenNodes,\n                                     Set\u003cNode\u003e excludedNodes,\n                                     long blocksize,\n                                     final BlockStoragePolicy storagePolicy) {\n     if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n       return DatanodeStorageInfo.EMPTY_ARRAY;\n     }\n       \n     if (excludedNodes \u003d\u003d null) {\n       excludedNodes \u003d new HashSet\u003cNode\u003e();\n     }\n      \n     int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n     numOfReplicas \u003d result[0];\n     int maxNodesPerRack \u003d result[1];\n       \n     final List\u003cDatanodeStorageInfo\u003e results \u003d new ArrayList\u003cDatanodeStorageInfo\u003e(chosenStorage);\n     for (DatanodeStorageInfo storage : chosenStorage) {\n       // add localMachine and related nodes to excludedNodes\n       addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n     }\n-      \n-    if (!clusterMap.contains(writer)) {\n-      writer \u003d null;\n-    }\n-      \n+\n     boolean avoidStaleNodes \u003d (stats !\u003d null\n         \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n     final Node localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n         blocksize, maxNodesPerRack, results, avoidStaleNodes, storagePolicy,\n         EnumSet.noneOf(StorageType.class), results.isEmpty());\n     if (!returnChosenNodes) {  \n       results.removeAll(chosenStorage);\n     }\n       \n     // sorting nodes to form a pipeline\n-    return getPipeline((writer\u003d\u003dnull)?localNode:writer,\n-                       results.toArray(new DatanodeStorageInfo[results.size()]));\n+    return getPipeline(\n+        (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n+            : localNode,\n+        results.toArray(new DatanodeStorageInfo[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                    Node writer,\n                                    List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                    boolean returnChosenNodes,\n                                    Set\u003cNode\u003e excludedNodes,\n                                    long blocksize,\n                                    final BlockStoragePolicy storagePolicy) {\n    if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n      return DatanodeStorageInfo.EMPTY_ARRAY;\n    }\n      \n    if (excludedNodes \u003d\u003d null) {\n      excludedNodes \u003d new HashSet\u003cNode\u003e();\n    }\n     \n    int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n    numOfReplicas \u003d result[0];\n    int maxNodesPerRack \u003d result[1];\n      \n    final List\u003cDatanodeStorageInfo\u003e results \u003d new ArrayList\u003cDatanodeStorageInfo\u003e(chosenStorage);\n    for (DatanodeStorageInfo storage : chosenStorage) {\n      // add localMachine and related nodes to excludedNodes\n      addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n    }\n\n    boolean avoidStaleNodes \u003d (stats !\u003d null\n        \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n    final Node localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n        blocksize, maxNodesPerRack, results, avoidStaleNodes, storagePolicy,\n        EnumSet.noneOf(StorageType.class), results.isEmpty());\n    if (!returnChosenNodes) {  \n      results.removeAll(chosenStorage);\n    }\n      \n    // sorting nodes to form a pipeline\n    return getPipeline(\n        (writer !\u003d null \u0026\u0026 writer instanceof DatanodeDescriptor) ? writer\n            : localNode,\n        results.toArray(new DatanodeStorageInfo[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
      "extendedDetails": {}
    },
    "ac5e8aed7ca1e9493f96f8795d0caafd5282b9a7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6686. Change BlockPlacementPolicy to use fallback when some storage types are unavailable.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-6584@1612880 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/07/14 10:25 AM",
      "commitName": "ac5e8aed7ca1e9493f96f8795d0caafd5282b9a7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "21/07/14 4:37 PM",
      "commitNameOld": "3de6c61f860c6494ed7843e0858c1d7a6a0918a2",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.74,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,42 @@\n   private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                     Node writer,\n                                     List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                     boolean returnChosenNodes,\n                                     Set\u003cNode\u003e excludedNodes,\n                                     long blocksize,\n                                     final BlockStoragePolicy storagePolicy) {\n     if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n       return DatanodeStorageInfo.EMPTY_ARRAY;\n     }\n       \n     if (excludedNodes \u003d\u003d null) {\n       excludedNodes \u003d new HashSet\u003cNode\u003e();\n     }\n      \n     int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n     numOfReplicas \u003d result[0];\n     int maxNodesPerRack \u003d result[1];\n       \n     final List\u003cDatanodeStorageInfo\u003e results \u003d new ArrayList\u003cDatanodeStorageInfo\u003e(chosenStorage);\n     for (DatanodeStorageInfo storage : chosenStorage) {\n       // add localMachine and related nodes to excludedNodes\n       addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n     }\n       \n     if (!clusterMap.contains(writer)) {\n       writer \u003d null;\n     }\n       \n     boolean avoidStaleNodes \u003d (stats !\u003d null\n         \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n     final Node localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n-        blocksize, maxNodesPerRack, results, avoidStaleNodes, storagePolicy);\n+        blocksize, maxNodesPerRack, results, avoidStaleNodes, storagePolicy,\n+        EnumSet.noneOf(StorageType.class), results.isEmpty());\n     if (!returnChosenNodes) {  \n       results.removeAll(chosenStorage);\n     }\n       \n     // sorting nodes to form a pipeline\n     return getPipeline((writer\u003d\u003dnull)?localNode:writer,\n                        results.toArray(new DatanodeStorageInfo[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DatanodeStorageInfo[] chooseTarget(int numOfReplicas,\n                                    Node writer,\n                                    List\u003cDatanodeStorageInfo\u003e chosenStorage,\n                                    boolean returnChosenNodes,\n                                    Set\u003cNode\u003e excludedNodes,\n                                    long blocksize,\n                                    final BlockStoragePolicy storagePolicy) {\n    if (numOfReplicas \u003d\u003d 0 || clusterMap.getNumOfLeaves()\u003d\u003d0) {\n      return DatanodeStorageInfo.EMPTY_ARRAY;\n    }\n      \n    if (excludedNodes \u003d\u003d null) {\n      excludedNodes \u003d new HashSet\u003cNode\u003e();\n    }\n     \n    int[] result \u003d getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);\n    numOfReplicas \u003d result[0];\n    int maxNodesPerRack \u003d result[1];\n      \n    final List\u003cDatanodeStorageInfo\u003e results \u003d new ArrayList\u003cDatanodeStorageInfo\u003e(chosenStorage);\n    for (DatanodeStorageInfo storage : chosenStorage) {\n      // add localMachine and related nodes to excludedNodes\n      addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);\n    }\n      \n    if (!clusterMap.contains(writer)) {\n      writer \u003d null;\n    }\n      \n    boolean avoidStaleNodes \u003d (stats !\u003d null\n        \u0026\u0026 stats.isAvoidingStaleDataNodesForWrite());\n    final Node localNode \u003d chooseTarget(numOfReplicas, writer, excludedNodes,\n        blocksize, maxNodesPerRack, results, avoidStaleNodes, storagePolicy,\n        EnumSet.noneOf(StorageType.class), results.isEmpty());\n    if (!returnChosenNodes) {  \n      results.removeAll(chosenStorage);\n    }\n      \n    // sorting nodes to form a pipeline\n    return getPipeline((writer\u003d\u003dnull)?localNode:writer,\n                       results.toArray(new DatanodeStorageInfo[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
      "extendedDetails": {}
    }
  }
}