{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PBHelperClient.java",
  "functionName": "convert",
  "functionId": "convert___fs-HdfsFileStatusProto",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
  "functionStartLine": 1706,
  "functionEndLine": 1743,
  "numCommitsSeen": 226,
  "timeTaken": 10154,
  "changeHistory": [
    "0e560f3b8d194c10dce06443979df4074e14b0db",
    "675e9a8f57570771a0219d95940681b067d36b94",
    "b85603e3f85e85da406241b991f3a9974384c3aa",
    "12e44e7bdaf53d3720a89d32f0cc2717241bd6b2",
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "5c97db07fb306842f49d73a67a90cecec19a7833",
    "76e7264e8d6407f527bd877009aca11f7bb63bd7",
    "bb84f1fccb18c6c7373851e05d2451d55e908242",
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4",
    "a7bcc9535860214380e235641d1d5d2dd15aee58",
    "9b250d74f029f8ccf3a613f9fb74f59838a66ec1",
    "2efea952139b30dd1c881eed0b443ffa72be6dce",
    "bdee397e95e98ece071345822e2e4d3f690f09c3",
    "4f68aa060090319b8de5c30f41d193632503ed17",
    "6ecf78a99b4b10d4c569cc2b335060ab988b8001",
    "4525c4a25ba90163c9543116e2bd54239e0dd097",
    "89553be90eeab6a8488fd2e7ab2bfda450798508",
    "48da033901d3471ef176a94104158546152353e9"
  ],
  "changeHistoryShort": {
    "0e560f3b8d194c10dce06443979df4074e14b0db": "Ybodychange",
    "675e9a8f57570771a0219d95940681b067d36b94": "Ybodychange",
    "b85603e3f85e85da406241b991f3a9974384c3aa": "Ybodychange",
    "12e44e7bdaf53d3720a89d32f0cc2717241bd6b2": "Ybodychange",
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ymultichange(Ymovefromfile,Ybodychange)",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "5c97db07fb306842f49d73a67a90cecec19a7833": "Ybodychange",
    "76e7264e8d6407f527bd877009aca11f7bb63bd7": "Ybodychange",
    "bb84f1fccb18c6c7373851e05d2451d55e908242": "Ybodychange",
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4": "Ybodychange",
    "a7bcc9535860214380e235641d1d5d2dd15aee58": "Ybodychange",
    "9b250d74f029f8ccf3a613f9fb74f59838a66ec1": "Ybodychange",
    "2efea952139b30dd1c881eed0b443ffa72be6dce": "Ybodychange",
    "bdee397e95e98ece071345822e2e4d3f690f09c3": "Ybodychange",
    "4f68aa060090319b8de5c30f41d193632503ed17": "Ybodychange",
    "6ecf78a99b4b10d4c569cc2b335060ab988b8001": "Ybodychange",
    "4525c4a25ba90163c9543116e2bd54239e0dd097": "Ybodychange",
    "89553be90eeab6a8488fd2e7ab2bfda450798508": "Ybodychange",
    "48da033901d3471ef176a94104158546152353e9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "0e560f3b8d194c10dce06443979df4074e14b0db": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12681. Make HdfsLocatedFileStatus a subtype of LocatedFileStatus\n",
      "commitDate": "29/11/17 8:28 PM",
      "commitName": "0e560f3b8d194c10dce06443979df4074e14b0db",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "15/11/17 7:20 PM",
      "commitNameOld": "675e9a8f57570771a0219d95940681b067d36b94",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 14.05,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,38 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null) {\n       return null;\n     }\n     EnumSet\u003cHdfsFileStatus.Flags\u003e flags \u003d fs.hasFlags()\n         ? convertFlags(fs.getFlags())\n         : convertFlags(fs.getPermission());\n-    return new HdfsLocatedFileStatus(\n-        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR),\n-        fs.getBlockReplication(), fs.getBlocksize(),\n-        fs.getModificationTime(), fs.getAccessTime(),\n-        convert(fs.getPermission()),\n-        flags,\n-        fs.getOwner(), fs.getGroup(),\n-        fs.getFileType().equals(FileType.IS_SYMLINK) ?\n-            fs.getSymlink().toByteArray() : null,\n-        fs.getPath().toByteArray(),\n-        fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n-        fs.hasLocations() ? convert(fs.getLocations()) : null,\n-        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n-        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n-        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n-            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n-        fs.hasEcPolicy() ? convertErasureCodingPolicy(fs.getEcPolicy()) : null);\n+    return new HdfsFileStatus.Builder()\n+        .length(fs.getLength())\n+        .isdir(fs.getFileType().equals(FileType.IS_DIR))\n+        .replication(fs.getBlockReplication())\n+        .blocksize(fs.getBlocksize())\n+        .mtime(fs.getModificationTime())\n+        .atime(fs.getAccessTime())\n+        .perm(convert(fs.getPermission()))\n+        .flags(flags)\n+        .owner(fs.getOwner())\n+        .group(fs.getGroup())\n+        .symlink(FileType.IS_SYMLINK.equals(fs.getFileType())\n+            ? fs.getSymlink().toByteArray()\n+            : null)\n+        .path(fs.getPath().toByteArray())\n+        .fileId(fs.hasFileId()\n+            ? fs.getFileId()\n+            : HdfsConstants.GRANDFATHER_INODE_ID)\n+        .locations(fs.hasLocations() ? convert(fs.getLocations()) : null)\n+        .children(fs.hasChildrenNum() ? fs.getChildrenNum() : -1)\n+        .feInfo(fs.hasFileEncryptionInfo()\n+            ? convert(fs.getFileEncryptionInfo())\n+            : null)\n+        .storagePolicy(fs.hasStoragePolicy()\n+            ? (byte) fs.getStoragePolicy()\n+            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED)\n+        .ecPolicy(fs.hasEcPolicy()\n+            ? convertErasureCodingPolicy(fs.getEcPolicy())\n+            : null)\n+        .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null) {\n      return null;\n    }\n    EnumSet\u003cHdfsFileStatus.Flags\u003e flags \u003d fs.hasFlags()\n        ? convertFlags(fs.getFlags())\n        : convertFlags(fs.getPermission());\n    return new HdfsFileStatus.Builder()\n        .length(fs.getLength())\n        .isdir(fs.getFileType().equals(FileType.IS_DIR))\n        .replication(fs.getBlockReplication())\n        .blocksize(fs.getBlocksize())\n        .mtime(fs.getModificationTime())\n        .atime(fs.getAccessTime())\n        .perm(convert(fs.getPermission()))\n        .flags(flags)\n        .owner(fs.getOwner())\n        .group(fs.getGroup())\n        .symlink(FileType.IS_SYMLINK.equals(fs.getFileType())\n            ? fs.getSymlink().toByteArray()\n            : null)\n        .path(fs.getPath().toByteArray())\n        .fileId(fs.hasFileId()\n            ? fs.getFileId()\n            : HdfsConstants.GRANDFATHER_INODE_ID)\n        .locations(fs.hasLocations() ? convert(fs.getLocations()) : null)\n        .children(fs.hasChildrenNum() ? fs.getChildrenNum() : -1)\n        .feInfo(fs.hasFileEncryptionInfo()\n            ? convert(fs.getFileEncryptionInfo())\n            : null)\n        .storagePolicy(fs.hasStoragePolicy()\n            ? (byte) fs.getStoragePolicy()\n            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED)\n        .ecPolicy(fs.hasEcPolicy()\n            ? convertErasureCodingPolicy(fs.getEcPolicy())\n            : null)\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "675e9a8f57570771a0219d95940681b067d36b94": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-12681. Fold HdfsLocatedFileStatus into HdfsFileStatus.\"\n\nThis reverts commit b85603e3f85e85da406241b991f3a9974384c3aa.\n",
      "commitDate": "15/11/17 7:20 PM",
      "commitName": "675e9a8f57570771a0219d95940681b067d36b94",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "03/11/17 2:30 PM",
      "commitNameOld": "b85603e3f85e85da406241b991f3a9974384c3aa",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 12.24,
      "commitsBetweenForRepo": 169,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,25 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null) {\n       return null;\n     }\n     EnumSet\u003cHdfsFileStatus.Flags\u003e flags \u003d fs.hasFlags()\n         ? convertFlags(fs.getFlags())\n         : convertFlags(fs.getPermission());\n-    return new HdfsFileStatus.Builder()\n-        .length(fs.getLength())\n-        .isdir(fs.getFileType().equals(FileType.IS_DIR))\n-        .replication(fs.getBlockReplication())\n-        .blocksize(fs.getBlocksize())\n-        .mtime(fs.getModificationTime())\n-        .atime(fs.getAccessTime())\n-        .perm(convert(fs.getPermission()))\n-        .flags(flags)\n-        .owner(fs.getOwner())\n-        .group(fs.getGroup())\n-        .symlink(FileType.IS_SYMLINK.equals(fs.getFileType())\n-            ? fs.getSymlink().toByteArray()\n-            : null)\n-        .path(fs.getPath().toByteArray())\n-        .fileId(fs.hasFileId()\n-            ? fs.getFileId()\n-            : HdfsConstants.GRANDFATHER_INODE_ID)\n-        .locations(fs.hasLocations() ? convert(fs.getLocations()) : null)\n-        .children(fs.hasChildrenNum() ? fs.getChildrenNum() : -1)\n-        .feInfo(fs.hasFileEncryptionInfo()\n-            ? convert(fs.getFileEncryptionInfo())\n-            : null)\n-        .storagePolicy(fs.hasStoragePolicy()\n-            ? (byte) fs.getStoragePolicy()\n-            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED)\n-        .ecPolicy(fs.hasEcPolicy()\n-            ? convertErasureCodingPolicy(fs.getEcPolicy())\n-            : null)\n-        .build();\n+    return new HdfsLocatedFileStatus(\n+        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR),\n+        fs.getBlockReplication(), fs.getBlocksize(),\n+        fs.getModificationTime(), fs.getAccessTime(),\n+        convert(fs.getPermission()),\n+        flags,\n+        fs.getOwner(), fs.getGroup(),\n+        fs.getFileType().equals(FileType.IS_SYMLINK) ?\n+            fs.getSymlink().toByteArray() : null,\n+        fs.getPath().toByteArray(),\n+        fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n+        fs.hasLocations() ? convert(fs.getLocations()) : null,\n+        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n+        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n+        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n+            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n+        fs.hasEcPolicy() ? convertErasureCodingPolicy(fs.getEcPolicy()) : null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null) {\n      return null;\n    }\n    EnumSet\u003cHdfsFileStatus.Flags\u003e flags \u003d fs.hasFlags()\n        ? convertFlags(fs.getFlags())\n        : convertFlags(fs.getPermission());\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR),\n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        convert(fs.getPermission()),\n        flags,\n        fs.getOwner(), fs.getGroup(),\n        fs.getFileType().equals(FileType.IS_SYMLINK) ?\n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n        fs.hasEcPolicy() ? convertErasureCodingPolicy(fs.getEcPolicy()) : null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "b85603e3f85e85da406241b991f3a9974384c3aa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12681. Fold HdfsLocatedFileStatus into HdfsFileStatus.\n",
      "commitDate": "03/11/17 2:30 PM",
      "commitName": "b85603e3f85e85da406241b991f3a9974384c3aa",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "02/11/17 9:27 PM",
      "commitNameOld": "e565b5277d5b890dad107fe85e295a3907e4bfc1",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 0.71,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,38 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null) {\n       return null;\n     }\n     EnumSet\u003cHdfsFileStatus.Flags\u003e flags \u003d fs.hasFlags()\n         ? convertFlags(fs.getFlags())\n         : convertFlags(fs.getPermission());\n-    return new HdfsLocatedFileStatus(\n-        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR),\n-        fs.getBlockReplication(), fs.getBlocksize(),\n-        fs.getModificationTime(), fs.getAccessTime(),\n-        convert(fs.getPermission()),\n-        flags,\n-        fs.getOwner(), fs.getGroup(),\n-        fs.getFileType().equals(FileType.IS_SYMLINK) ?\n-            fs.getSymlink().toByteArray() : null,\n-        fs.getPath().toByteArray(),\n-        fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n-        fs.hasLocations() ? convert(fs.getLocations()) : null,\n-        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n-        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n-        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n-            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n-        fs.hasEcPolicy() ? convertErasureCodingPolicy(fs.getEcPolicy()) : null);\n+    return new HdfsFileStatus.Builder()\n+        .length(fs.getLength())\n+        .isdir(fs.getFileType().equals(FileType.IS_DIR))\n+        .replication(fs.getBlockReplication())\n+        .blocksize(fs.getBlocksize())\n+        .mtime(fs.getModificationTime())\n+        .atime(fs.getAccessTime())\n+        .perm(convert(fs.getPermission()))\n+        .flags(flags)\n+        .owner(fs.getOwner())\n+        .group(fs.getGroup())\n+        .symlink(FileType.IS_SYMLINK.equals(fs.getFileType())\n+            ? fs.getSymlink().toByteArray()\n+            : null)\n+        .path(fs.getPath().toByteArray())\n+        .fileId(fs.hasFileId()\n+            ? fs.getFileId()\n+            : HdfsConstants.GRANDFATHER_INODE_ID)\n+        .locations(fs.hasLocations() ? convert(fs.getLocations()) : null)\n+        .children(fs.hasChildrenNum() ? fs.getChildrenNum() : -1)\n+        .feInfo(fs.hasFileEncryptionInfo()\n+            ? convert(fs.getFileEncryptionInfo())\n+            : null)\n+        .storagePolicy(fs.hasStoragePolicy()\n+            ? (byte) fs.getStoragePolicy()\n+            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED)\n+        .ecPolicy(fs.hasEcPolicy()\n+            ? convertErasureCodingPolicy(fs.getEcPolicy())\n+            : null)\n+        .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null) {\n      return null;\n    }\n    EnumSet\u003cHdfsFileStatus.Flags\u003e flags \u003d fs.hasFlags()\n        ? convertFlags(fs.getFlags())\n        : convertFlags(fs.getPermission());\n    return new HdfsFileStatus.Builder()\n        .length(fs.getLength())\n        .isdir(fs.getFileType().equals(FileType.IS_DIR))\n        .replication(fs.getBlockReplication())\n        .blocksize(fs.getBlocksize())\n        .mtime(fs.getModificationTime())\n        .atime(fs.getAccessTime())\n        .perm(convert(fs.getPermission()))\n        .flags(flags)\n        .owner(fs.getOwner())\n        .group(fs.getGroup())\n        .symlink(FileType.IS_SYMLINK.equals(fs.getFileType())\n            ? fs.getSymlink().toByteArray()\n            : null)\n        .path(fs.getPath().toByteArray())\n        .fileId(fs.hasFileId()\n            ? fs.getFileId()\n            : HdfsConstants.GRANDFATHER_INODE_ID)\n        .locations(fs.hasLocations() ? convert(fs.getLocations()) : null)\n        .children(fs.hasChildrenNum() ? fs.getChildrenNum() : -1)\n        .feInfo(fs.hasFileEncryptionInfo()\n            ? convert(fs.getFileEncryptionInfo())\n            : null)\n        .storagePolicy(fs.hasStoragePolicy()\n            ? (byte) fs.getStoragePolicy()\n            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED)\n        .ecPolicy(fs.hasEcPolicy()\n            ? convertErasureCodingPolicy(fs.getEcPolicy())\n            : null)\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "12e44e7bdaf53d3720a89d32f0cc2717241bd6b2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6984. Serialize FileStatus via protobuf.\n",
      "commitDate": "02/08/17 12:12 PM",
      "commitName": "12e44e7bdaf53d3720a89d32f0cc2717241bd6b2",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "29/06/17 6:38 AM",
      "commitNameOld": "16c8dbde574f49827fde5ee9add1861ee65d4645",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 34.23,
      "commitsBetweenForRepo": 222,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,25 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n-    if (fs \u003d\u003d null)\n+    if (fs \u003d\u003d null) {\n       return null;\n+    }\n+    EnumSet\u003cHdfsFileStatus.Flags\u003e flags \u003d fs.hasFlags()\n+        ? convertFlags(fs.getFlags())\n+        : convertFlags(fs.getPermission());\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR),\n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n-        convert(fs.getPermission()), fs.getOwner(), fs.getGroup(),\n+        convert(fs.getPermission()),\n+        flags,\n+        fs.getOwner(), fs.getGroup(),\n         fs.getFileType().equals(FileType.IS_SYMLINK) ?\n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? convert(fs.getLocations()) : null,\n         fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n         fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n         fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n             : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n         fs.hasEcPolicy() ? convertErasureCodingPolicy(fs.getEcPolicy()) : null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null) {\n      return null;\n    }\n    EnumSet\u003cHdfsFileStatus.Flags\u003e flags \u003d fs.hasFlags()\n        ? convertFlags(fs.getFlags())\n        : convertFlags(fs.getPermission());\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR),\n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        convert(fs.getPermission()),\n        flags,\n        fs.getOwner(), fs.getGroup(),\n        fs.getFileType().equals(FileType.IS_SYMLINK) ?\n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n        fs.hasEcPolicy() ? convertErasureCodingPolicy(fs.getEcPolicy()) : null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
          "commitDate": "21/09/15 6:53 PM",
          "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/09/15 5:51 PM",
          "commitNameOld": "8e01b0d97ac3d74b049a801dfa1cc6e77d8f680a",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,18 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n-        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n+        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR),\n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n-        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n-        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n+        convert(fs.getPermission()), fs.getOwner(), fs.getGroup(),\n+        fs.getFileType().equals(FileType.IS_SYMLINK) ?\n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n-        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n+        fs.hasLocations() ? convert(fs.getLocations()) : null,\n         fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n         fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n         fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n             : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR),\n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        convert(fs.getPermission()), fs.getOwner(), fs.getGroup(),\n        fs.getFileType().equals(FileType.IS_SYMLINK) ?\n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
            "oldMethodName": "convert",
            "newMethodName": "convert"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
          "commitDate": "21/09/15 6:53 PM",
          "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/09/15 5:51 PM",
          "commitNameOld": "8e01b0d97ac3d74b049a801dfa1cc6e77d8f680a",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,18 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n-        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n+        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR),\n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n-        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n-        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n+        convert(fs.getPermission()), fs.getOwner(), fs.getGroup(),\n+        fs.getFileType().equals(FileType.IS_SYMLINK) ?\n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n-        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n+        fs.hasLocations() ? convert(fs.getLocations()) : null,\n         fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n         fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n         fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n             : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR),\n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        convert(fs.getPermission()), fs.getOwner(), fs.getGroup(),\n        fs.getFileType().equals(FileType.IS_SYMLINK) ?\n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "20/04/15 12:36 AM",
      "commitNameOld": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 12.39,
      "commitsBetweenForRepo": 126,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n-        fs.hasFileId()? fs.getFileId(): HdfsConstantsClient.GRANDFATHER_INODE_ID,\n+        fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n         fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n         fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n         fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n-            : HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);\n+            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): HdfsConstants.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n            : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "5c97db07fb306842f49d73a67a90cecec19a7833": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8169. Move LocatedBlocks and related classes to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "20/04/15 12:36 AM",
      "commitName": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "16/04/15 10:49 PM",
      "commitNameOld": "76e7264e8d6407f527bd877009aca11f7bb63bd7",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.07,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): HdfsConstantsClient.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n         fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n         fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n         fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n-            : BlockStoragePolicySuite.ID_UNSPECIFIED);\n+            : HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): HdfsConstantsClient.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n            : HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "76e7264e8d6407f527bd877009aca11f7bb63bd7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8165. Move GRANDFATHER_GENERATION_STAMP and GRANDFATER_INODE_ID to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "16/04/15 10:49 PM",
      "commitName": "76e7264e8d6407f527bd877009aca11f7bb63bd7",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/03/15 3:25 PM",
      "commitNameOld": "1a495fbb489c9e9a23b341a52696d10e9e272b04",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 17.31,
      "commitsBetweenForRepo": 147,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n-        fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n+        fs.hasFileId()? fs.getFileId(): HdfsConstantsClient.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n         fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n         fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n         fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n             : BlockStoragePolicySuite.ID_UNSPECIFIED);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): HdfsConstantsClient.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n            : BlockStoragePolicySuite.ID_UNSPECIFIED);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "bb84f1fccb18c6c7373851e05d2451d55e908242": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7159. Use block storage policy to set lazy persist preference. (Arpit Agarwal)\n",
      "commitDate": "29/09/14 10:27 PM",
      "commitName": "bb84f1fccb18c6c7373851e05d2451d55e908242",
      "commitAuthor": "arp",
      "commitDateOld": "25/09/14 7:50 PM",
      "commitNameOld": "032e0eba6b5acda3e33f4b99b48845eedb5ed149",
      "commitAuthorOld": "",
      "daysBetweenCommits": 4.11,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,18 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n-        fs.hasIsLazyPersist() ? fs.getIsLazyPersist() : false,\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n         fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n         fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n         fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n             : BlockStoragePolicySuite.ID_UNSPECIFIED);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n            : BlockStoragePolicySuite.ID_UNSPECIFIED);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7081. Add new DistributedFileSystem API for getting all the existing storage policies. Contributed by Jing Zhao.\n",
      "commitDate": "24/09/14 10:05 AM",
      "commitName": "073bbd805c6680f47bbfcc6e8efd708ad729bca4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "21/09/14 9:29 PM",
      "commitNameOld": "1737950d0fc83c68f386881b843c41b0b1e342de",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.52,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n         fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n         fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n         fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n-            : BlockStoragePolicy.ID_UNSPECIFIED);\n+            : BlockStoragePolicySuite.ID_UNSPECIFIED);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null,\n        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n            : BlockStoragePolicySuite.ID_UNSPECIFIED);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "a7bcc9535860214380e235641d1d5d2dd15aee58": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6921. Add LazyPersist flag to FileStatus. (Arpit Agarwal)\n",
      "commitDate": "27/08/14 9:47 PM",
      "commitName": "a7bcc9535860214380e235641d1d5d2dd15aee58",
      "commitAuthor": "arp",
      "commitDateOld": "04/08/14 7:30 PM",
      "commitNameOld": "ac73d416f3cfbc9561286ce4bc7624113db4e751",
      "commitAuthorOld": "",
      "daysBetweenCommits": 23.09,
      "commitsBetweenForRepo": 190,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,18 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n+        fs.hasIsLazyPersist() ? fs.getIsLazyPersist() : false,\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n         fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n         fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) :\n             null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.hasIsLazyPersist() ? fs.getIsLazyPersist() : false,\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) :\n            null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "9b250d74f029f8ccf3a613f9fb74f59838a66ec1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6847. Support storage policy on directories and include storage policy in HdfsFileStatus.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-6584@1618416 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/08/14 1:58 PM",
      "commitName": "9b250d74f029f8ccf3a613f9fb74f59838a66ec1",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "31/07/14 6:29 PM",
      "commitNameOld": "dc7744d2e50350fb9ac5f2a1f761cdc63b220899",
      "commitAuthorOld": "",
      "daysBetweenCommits": 15.81,
      "commitsBetweenForRepo": 87,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,17 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n-        fs.hasChildrenNum() ? fs.getChildrenNum() : -1);\n+        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n+        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n+            : BlockStoragePolicy.ID_UNSPECIFIED);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy()\n            : BlockStoragePolicy.ID_UNSPECIFIED);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "2efea952139b30dd1c881eed0b443ffa72be6dce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6391. Get the Key/IV from the NameNode for encrypted files in DFSClient. Contributed by Charles Lamb and Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1606220 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/06/14 1:43 PM",
      "commitName": "2efea952139b30dd1c881eed0b443ffa72be6dce",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "19/06/14 11:32 AM",
      "commitNameOld": "97583dbb0a07ac054c06c1a317855110c8629ab3",
      "commitAuthorOld": "",
      "daysBetweenCommits": 8.09,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,17 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n         fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n-        fs.hasKey() ? fs.getKey().toByteArray() : null,\n-        fs.hasIv() ? fs.getIv().toByteArray() : null);\n+        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) :\n+            null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) :\n            null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "bdee397e95e98ece071345822e2e4d3f690f09c3": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-6392. Wire crypto streams for encrypted files in DFSClient. (clamb and yliu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1600582 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/06/14 3:10 AM",
      "commitName": "bdee397e95e98ece071345822e2e4d3f690f09c3",
      "commitAuthor": "Charles Lamb",
      "commitDateOld": "21/05/14 6:57 AM",
      "commitNameOld": "ac23a55547716df29b3e25c98a113399e184d9d1",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 14.84,
      "commitsBetweenForRepo": 63,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,17 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n-        fs.hasChildrenNum() ? fs.getChildrenNum() : -1);\n+        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n+        fs.hasKey() ? fs.getKey().toByteArray() : null,\n+        fs.hasIv() ? fs.getIv().toByteArray() : null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1,\n        fs.hasKey() ? fs.getKey().toByteArray() : null,\n        fs.hasIv() ? fs.getIv().toByteArray() : null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "4f68aa060090319b8de5c30f41d193632503ed17": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5043. For HdfsFileStatus, set default value of childrenNum to -1 instead of 0 to avoid confusing applications. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508694 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/07/13 5:49 PM",
      "commitName": "4f68aa060090319b8de5c30f41d193632503ed17",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "20/06/13 5:32 PM",
      "commitNameOld": "6ecf78a99b4b10d4c569cc2b335060ab988b8001",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 40.01,
      "commitsBetweenForRepo": 212,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n-        fs.hasChildrenNum() ? fs.getChildrenNum() : 0);\n+        fs.hasChildrenNum() ? fs.getChildrenNum() : -1);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : -1);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "6ecf78a99b4b10d4c569cc2b335060ab988b8001": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4772. Add number of children in HdfsFileStatus. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1495253 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/06/13 5:32 PM",
      "commitName": "6ecf78a99b4b10d4c569cc2b335060ab988b8001",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "14/02/13 3:07 PM",
      "commitNameOld": "d9e2514d21c2ae356ee7fe8d4a857748b5defa4c",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 126.06,
      "commitsBetweenForRepo": 784,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,15 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n         fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n-        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null);\n+        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n+        fs.hasChildrenNum() ? fs.getChildrenNum() : 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null,\n        fs.hasChildrenNum() ? fs.getChildrenNum() : 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "4525c4a25ba90163c9543116e2bd54239e0dd097": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4340. Update addBlock() to inculde inode id as additional argument. Contributed Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1443169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/02/13 11:52 AM",
      "commitName": "4525c4a25ba90163c9543116e2bd54239e0dd097",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "09/01/13 2:29 PM",
      "commitNameOld": "3555e7c574de5a6d163c5375a31de290776b2ab0",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 27.89,
      "commitsBetweenForRepo": 147,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,14 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n     return new HdfsLocatedFileStatus(\n         fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n         fs.getBlockReplication(), fs.getBlocksize(),\n         fs.getModificationTime(), fs.getAccessTime(),\n         PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n         fs.getFileType().equals(FileType.IS_SYMLINK) ? \n             fs.getSymlink().toByteArray() : null,\n         fs.getPath().toByteArray(),\n+        fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n         fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasFileId()? fs.getFileId(): INodeId.GRANDFATHER_INODE_ID,\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "89553be90eeab6a8488fd2e7ab2bfda450798508": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2687. Tests failing with ClassCastException post protobuf RPC changes. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1215366 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/12/11 4:26 PM",
      "commitName": "89553be90eeab6a8488fd2e7ab2bfda450798508",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/12/11 1:27 AM",
      "commitNameOld": "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e",
      "commitAuthorOld": "Sanjay Radia",
      "daysBetweenCommits": 2.62,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,13 @@\n   public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n     if (fs \u003d\u003d null)\n       return null;\n-    if (fs.hasLocations()) {\n-      return new HdfsLocatedFileStatus(\n-          fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n-          fs.getBlockReplication(), fs.getBlocksize(),\n-          fs.getModificationTime(), fs.getAccessTime(),\n-          PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n-          fs.getFileType().equals(FileType.IS_SYMLINK) ? \n-              fs.getSymlink().toByteArray() : null,\n-          fs.getPath().toByteArray(),\n-          PBHelper.convert(fs.hasLocations() ? fs.getLocations() : null));\n-    }\n-    return new HdfsFileStatus(\n-      fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n-      fs.getBlockReplication(), fs.getBlocksize(),\n-      fs.getModificationTime(), fs.getAccessTime(),\n-      PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n-      fs.getFileType().equals(FileType.IS_SYMLINK) ? \n-          fs.getSymlink().toByteArray() : null,\n-      fs.getPath().toByteArray());\n+    return new HdfsLocatedFileStatus(\n+        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n+        fs.getBlockReplication(), fs.getBlocksize(),\n+        fs.getModificationTime(), fs.getAccessTime(),\n+        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n+        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n+            fs.getSymlink().toByteArray() : null,\n+        fs.getPath().toByteArray(),\n+        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    return new HdfsLocatedFileStatus(\n        fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n        fs.getBlockReplication(), fs.getBlocksize(),\n        fs.getModificationTime(), fs.getAccessTime(),\n        PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n        fs.getFileType().equals(FileType.IS_SYMLINK) ? \n            fs.getSymlink().toByteArray() : null,\n        fs.getPath().toByteArray(),\n        fs.hasLocations() ? PBHelper.convert(fs.getLocations()) : null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "48da033901d3471ef176a94104158546152353e9": {
      "type": "Yintroduced",
      "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/11 9:36 PM",
      "commitName": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthor": "Sanjay Radia",
      "diff": "@@ -0,0 +1,23 @@\n+  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n+    if (fs \u003d\u003d null)\n+      return null;\n+    if (fs.hasLocations()) {\n+      return new HdfsLocatedFileStatus(\n+          fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n+          fs.getBlockReplication(), fs.getBlocksize(),\n+          fs.getModificationTime(), fs.getAccessTime(),\n+          PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n+          fs.getFileType().equals(FileType.IS_SYMLINK) ? \n+              fs.getSymlink().toByteArray() : null,\n+          fs.getPath().toByteArray(),\n+          PBHelper.convert(fs.hasLocations() ? fs.getLocations() : null));\n+    }\n+    return new HdfsFileStatus(\n+      fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n+      fs.getBlockReplication(), fs.getBlocksize(),\n+      fs.getModificationTime(), fs.getAccessTime(),\n+      PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n+      fs.getFileType().equals(FileType.IS_SYMLINK) ? \n+          fs.getSymlink().toByteArray() : null,\n+      fs.getPath().toByteArray());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static HdfsFileStatus convert(HdfsFileStatusProto fs) {\n    if (fs \u003d\u003d null)\n      return null;\n    if (fs.hasLocations()) {\n      return new HdfsLocatedFileStatus(\n          fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n          fs.getBlockReplication(), fs.getBlocksize(),\n          fs.getModificationTime(), fs.getAccessTime(),\n          PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n          fs.getFileType().equals(FileType.IS_SYMLINK) ? \n              fs.getSymlink().toByteArray() : null,\n          fs.getPath().toByteArray(),\n          PBHelper.convert(fs.hasLocations() ? fs.getLocations() : null));\n    }\n    return new HdfsFileStatus(\n      fs.getLength(), fs.getFileType().equals(FileType.IS_DIR), \n      fs.getBlockReplication(), fs.getBlocksize(),\n      fs.getModificationTime(), fs.getAccessTime(),\n      PBHelper.convert(fs.getPermission()), fs.getOwner(), fs.getGroup(), \n      fs.getFileType().equals(FileType.IS_SYMLINK) ? \n          fs.getSymlink().toByteArray() : null,\n      fs.getPath().toByteArray());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java"
    }
  }
}