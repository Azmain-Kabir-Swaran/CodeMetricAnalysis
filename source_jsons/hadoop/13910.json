{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "isPlacementPolicySatisfied",
  "functionId": "isPlacementPolicySatisfied___storedBlock-BlockInfo",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 4712,
  "functionEndLine": 4715,
  "numCommitsSeen": 477,
  "timeTaken": 10724,
  "changeHistory": [
    "c99a12167ff9566012ef32104a3964887d62c899",
    "c89b29bd421152f0e7e16936f18d9e852895c37a",
    "4812518b23cac496ab5cdad5258773bcd9728770",
    "a2a5d7b5bca715835d92816e7b267b59f7270708",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "e27c2ae8bafc94f18eb38f5d839dcef5652d424e"
  ],
  "changeHistoryShort": {
    "c99a12167ff9566012ef32104a3964887d62c899": "Ybodychange",
    "c89b29bd421152f0e7e16936f18d9e852895c37a": "Ybodychange",
    "4812518b23cac496ab5cdad5258773bcd9728770": "Ybodychange",
    "a2a5d7b5bca715835d92816e7b267b59f7270708": "Ybodychange",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "e27c2ae8bafc94f18eb38f5d839dcef5652d424e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c99a12167ff9566012ef32104a3964887d62c899": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14637. Namenode may not replicate blocks to meet the policy after enabling upgradeDomain. Contributed by Stephen O\u0027Donnell.\n\nReviewed-by: Ayush Saxena \u003cayushsaxena@apache.org\u003e\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "03/10/19 10:13 PM",
      "commitName": "c99a12167ff9566012ef32104a3964887d62c899",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "28/09/19 9:14 AM",
      "commitNameOld": "c4c8d5fd0e3c17ccdcf18ece8e005f510328b060",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 5.54,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,4 @@\n   boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n-    List\u003cDatanodeDescriptor\u003e liveNodes \u003d new ArrayList\u003c\u003e();\n-    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n-        .getNodes(storedBlock);\n-    for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n-      if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED\n-          \u0026\u0026 storage.getState() \u003d\u003d State.NORMAL) {\n-        // assume the policy is satisfied for blocks on PROVIDED storage\n-        // as long as the storage is in normal state.\n-        return true;\n-      }\n-      final DatanodeDescriptor cur \u003d getDatanodeDescriptorFromStorage(storage);\n-      // Nodes under maintenance should be counted as valid replicas from\n-      // rack policy point of view.\n-      if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()\n-          \u0026\u0026 ((corruptNodes \u003d\u003d null) || !corruptNodes.contains(cur))) {\n-        liveNodes.add(cur);\n-      }\n-    }\n-    DatanodeInfo[] locs \u003d liveNodes.toArray(new DatanodeInfo[liveNodes.size()]);\n-    BlockType blockType \u003d storedBlock.getBlockType();\n-    BlockPlacementPolicy placementPolicy \u003d placementPolicies\n-        .getPolicy(blockType);\n-    int numReplicas \u003d blockType \u003d\u003d STRIPED ? ((BlockInfoStriped) storedBlock)\n-        .getRealTotalBlockNum() : storedBlock.getReplication();\n-    return placementPolicy.verifyBlockPlacement(locs, numReplicas)\n+    return getBlockPlacementStatus(storedBlock, null)\n         .isPlacementPolicySatisfied();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n    return getBlockPlacementStatus(storedBlock, null)\n        .isPlacementPolicySatisfied();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "c89b29bd421152f0e7e16936f18d9e852895c37a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12893. [READ] Support replication of Provided blocks with non-default topologies.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "c89b29bd421152f0e7e16936f18d9e852895c37a",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "fb996a32a98a25c0fe34a8ebb28563b53cd6e20e",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,28 @@\n   boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n     List\u003cDatanodeDescriptor\u003e liveNodes \u003d new ArrayList\u003c\u003e();\n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n         .getNodes(storedBlock);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n-      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n+      if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED\n+          \u0026\u0026 storage.getState() \u003d\u003d State.NORMAL) {\n+        // assume the policy is satisfied for blocks on PROVIDED storage\n+        // as long as the storage is in normal state.\n+        return true;\n+      }\n+      final DatanodeDescriptor cur \u003d getDatanodeDescriptorFromStorage(storage);\n       // Nodes under maintenance should be counted as valid replicas from\n       // rack policy point of view.\n       if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()\n           \u0026\u0026 ((corruptNodes \u003d\u003d null) || !corruptNodes.contains(cur))) {\n         liveNodes.add(cur);\n       }\n     }\n     DatanodeInfo[] locs \u003d liveNodes.toArray(new DatanodeInfo[liveNodes.size()]);\n     BlockType blockType \u003d storedBlock.getBlockType();\n     BlockPlacementPolicy placementPolicy \u003d placementPolicies\n         .getPolicy(blockType);\n     int numReplicas \u003d blockType \u003d\u003d STRIPED ? ((BlockInfoStriped) storedBlock)\n         .getRealTotalBlockNum() : storedBlock.getReplication();\n     return placementPolicy.verifyBlockPlacement(locs, numReplicas)\n         .isPlacementPolicySatisfied();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n    List\u003cDatanodeDescriptor\u003e liveNodes \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(storedBlock);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n      if (storage.getStorageType() \u003d\u003d StorageType.PROVIDED\n          \u0026\u0026 storage.getState() \u003d\u003d State.NORMAL) {\n        // assume the policy is satisfied for blocks on PROVIDED storage\n        // as long as the storage is in normal state.\n        return true;\n      }\n      final DatanodeDescriptor cur \u003d getDatanodeDescriptorFromStorage(storage);\n      // Nodes under maintenance should be counted as valid replicas from\n      // rack policy point of view.\n      if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()\n          \u0026\u0026 ((corruptNodes \u003d\u003d null) || !corruptNodes.contains(cur))) {\n        liveNodes.add(cur);\n      }\n    }\n    DatanodeInfo[] locs \u003d liveNodes.toArray(new DatanodeInfo[liveNodes.size()]);\n    BlockType blockType \u003d storedBlock.getBlockType();\n    BlockPlacementPolicy placementPolicy \u003d placementPolicies\n        .getPolicy(blockType);\n    int numReplicas \u003d blockType \u003d\u003d STRIPED ? ((BlockInfoStriped) storedBlock)\n        .getRealTotalBlockNum() : storedBlock.getReplication();\n    return placementPolicy.verifyBlockPlacement(locs, numReplicas)\n        .isPlacementPolicySatisfied();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4812518b23cac496ab5cdad5258773bcd9728770": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10530. BlockManager reconstruction work scheduling should correctly adhere to EC block placement policy. Contributed by Manoj Govindassamy and Rui Gao.\n",
      "commitDate": "16/03/17 3:07 PM",
      "commitName": "4812518b23cac496ab5cdad5258773bcd9728770",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "09/03/17 4:13 AM",
      "commitNameOld": "385d2cb777a0272ac20c62336c944fad295d5d12",
      "commitAuthorOld": "Masatake Iwasaki",
      "daysBetweenCommits": 7.41,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n     List\u003cDatanodeDescriptor\u003e liveNodes \u003d new ArrayList\u003c\u003e();\n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n         .getNodes(storedBlock);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n       final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n       // Nodes under maintenance should be counted as valid replicas from\n       // rack policy point of view.\n       if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()\n           \u0026\u0026 ((corruptNodes \u003d\u003d null) || !corruptNodes.contains(cur))) {\n         liveNodes.add(cur);\n       }\n     }\n     DatanodeInfo[] locs \u003d liveNodes.toArray(new DatanodeInfo[liveNodes.size()]);\n     BlockType blockType \u003d storedBlock.getBlockType();\n     BlockPlacementPolicy placementPolicy \u003d placementPolicies\n         .getPolicy(blockType);\n     int numReplicas \u003d blockType \u003d\u003d STRIPED ? ((BlockInfoStriped) storedBlock)\n-        .getRealDataBlockNum() : storedBlock.getReplication();\n+        .getRealTotalBlockNum() : storedBlock.getReplication();\n     return placementPolicy.verifyBlockPlacement(locs, numReplicas)\n         .isPlacementPolicySatisfied();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n    List\u003cDatanodeDescriptor\u003e liveNodes \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(storedBlock);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n      // Nodes under maintenance should be counted as valid replicas from\n      // rack policy point of view.\n      if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()\n          \u0026\u0026 ((corruptNodes \u003d\u003d null) || !corruptNodes.contains(cur))) {\n        liveNodes.add(cur);\n      }\n    }\n    DatanodeInfo[] locs \u003d liveNodes.toArray(new DatanodeInfo[liveNodes.size()]);\n    BlockType blockType \u003d storedBlock.getBlockType();\n    BlockPlacementPolicy placementPolicy \u003d placementPolicies\n        .getPolicy(blockType);\n    int numReplicas \u003d blockType \u003d\u003d STRIPED ? ((BlockInfoStriped) storedBlock)\n        .getRealTotalBlockNum() : storedBlock.getReplication();\n    return placementPolicy.verifyBlockPlacement(locs, numReplicas)\n        .isPlacementPolicySatisfied();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "a2a5d7b5bca715835d92816e7b267b59f7270708": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10759. Change fsimage bool isStriped from boolean to an enum. Contributed by Ewan Higgs.\n",
      "commitDate": "18/01/17 1:31 PM",
      "commitName": "a2a5d7b5bca715835d92816e7b267b59f7270708",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "05/12/16 10:54 AM",
      "commitNameOld": "1b5cceaffbdde50a87ede81552dc380832db8e79",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 44.11,
      "commitsBetweenForRepo": 218,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,22 @@\n   boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n     List\u003cDatanodeDescriptor\u003e liveNodes \u003d new ArrayList\u003c\u003e();\n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n         .getNodes(storedBlock);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n       final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n       // Nodes under maintenance should be counted as valid replicas from\n       // rack policy point of view.\n       if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()\n           \u0026\u0026 ((corruptNodes \u003d\u003d null) || !corruptNodes.contains(cur))) {\n         liveNodes.add(cur);\n       }\n     }\n     DatanodeInfo[] locs \u003d liveNodes.toArray(new DatanodeInfo[liveNodes.size()]);\n+    BlockType blockType \u003d storedBlock.getBlockType();\n     BlockPlacementPolicy placementPolicy \u003d placementPolicies\n-        .getPolicy(storedBlock.isStriped());\n-    int numReplicas \u003d storedBlock.isStriped() ? ((BlockInfoStriped) storedBlock)\n+        .getPolicy(blockType);\n+    int numReplicas \u003d blockType \u003d\u003d STRIPED ? ((BlockInfoStriped) storedBlock)\n         .getRealDataBlockNum() : storedBlock.getReplication();\n     return placementPolicy.verifyBlockPlacement(locs, numReplicas)\n         .isPlacementPolicySatisfied();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n    List\u003cDatanodeDescriptor\u003e liveNodes \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(storedBlock);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n      // Nodes under maintenance should be counted as valid replicas from\n      // rack policy point of view.\n      if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()\n          \u0026\u0026 ((corruptNodes \u003d\u003d null) || !corruptNodes.contains(cur))) {\n        liveNodes.add(cur);\n      }\n    }\n    DatanodeInfo[] locs \u003d liveNodes.toArray(new DatanodeInfo[liveNodes.size()]);\n    BlockType blockType \u003d storedBlock.getBlockType();\n    BlockPlacementPolicy placementPolicy \u003d placementPolicies\n        .getPolicy(blockType);\n    int numReplicas \u003d blockType \u003d\u003d STRIPED ? ((BlockInfoStriped) storedBlock)\n        .getRealDataBlockNum() : storedBlock.getReplication();\n    return placementPolicy.verifyBlockPlacement(locs, numReplicas)\n        .isPlacementPolicySatisfied();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "14/10/16 6:13 PM",
      "commitNameOld": "391ce535a739dc92cb90017d759217265a4fd969",
      "commitAuthorOld": "Vinitha Reddy Gankidi",
      "daysBetweenCommits": 2.98,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,21 @@\n   boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n     List\u003cDatanodeDescriptor\u003e liveNodes \u003d new ArrayList\u003c\u003e();\n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n         .getNodes(storedBlock);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n       final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n+      // Nodes under maintenance should be counted as valid replicas from\n+      // rack policy point of view.\n       if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()\n           \u0026\u0026 ((corruptNodes \u003d\u003d null) || !corruptNodes.contains(cur))) {\n         liveNodes.add(cur);\n       }\n     }\n     DatanodeInfo[] locs \u003d liveNodes.toArray(new DatanodeInfo[liveNodes.size()]);\n     BlockPlacementPolicy placementPolicy \u003d placementPolicies\n         .getPolicy(storedBlock.isStriped());\n     int numReplicas \u003d storedBlock.isStriped() ? ((BlockInfoStriped) storedBlock)\n         .getRealDataBlockNum() : storedBlock.getReplication();\n     return placementPolicy.verifyBlockPlacement(locs, numReplicas)\n         .isPlacementPolicySatisfied();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n    List\u003cDatanodeDescriptor\u003e liveNodes \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(storedBlock);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n      // Nodes under maintenance should be counted as valid replicas from\n      // rack policy point of view.\n      if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()\n          \u0026\u0026 ((corruptNodes \u003d\u003d null) || !corruptNodes.contains(cur))) {\n        liveNodes.add(cur);\n      }\n    }\n    DatanodeInfo[] locs \u003d liveNodes.toArray(new DatanodeInfo[liveNodes.size()]);\n    BlockPlacementPolicy placementPolicy \u003d placementPolicies\n        .getPolicy(storedBlock.isStriped());\n    int numReplicas \u003d storedBlock.isStriped() ? ((BlockInfoStriped) storedBlock)\n        .getRealDataBlockNum() : storedBlock.getReplication();\n    return placementPolicy.verifyBlockPlacement(locs, numReplicas)\n        .isPlacementPolicySatisfied();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "e27c2ae8bafc94f18eb38f5d839dcef5652d424e": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8647. Abstract BlockManager\u0027s rack policy into BlockPlacementPolicy. (Brahma Reddy Battula via mingma)\n",
      "commitDate": "21/10/15 8:06 AM",
      "commitName": "e27c2ae8bafc94f18eb38f5d839dcef5652d424e",
      "commitAuthor": "Ming Ma",
      "diff": "@@ -0,0 +1,18 @@\n+  boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n+    List\u003cDatanodeDescriptor\u003e liveNodes \u003d new ArrayList\u003c\u003e();\n+    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n+        .getNodes(storedBlock);\n+    for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n+      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n+      if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()\n+          \u0026\u0026 ((corruptNodes \u003d\u003d null) || !corruptNodes.contains(cur))) {\n+        liveNodes.add(cur);\n+      }\n+    }\n+    DatanodeInfo[] locs \u003d liveNodes.toArray(new DatanodeInfo[liveNodes.size()]);\n+    BlockPlacementPolicy placementPolicy \u003d placementPolicies\n+        .getPolicy(storedBlock.isStriped());\n+    int numReplicas \u003d storedBlock.isStriped() ? ((BlockInfoStriped) storedBlock)\n+        .getRealDataBlockNum() : storedBlock.getReplication();\n+    return placementPolicy.verifyBlockPlacement(locs, numReplicas).isPlacementPolicySatisfied();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  boolean isPlacementPolicySatisfied(BlockInfo storedBlock) {\n    List\u003cDatanodeDescriptor\u003e liveNodes \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(storedBlock);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n      if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()\n          \u0026\u0026 ((corruptNodes \u003d\u003d null) || !corruptNodes.contains(cur))) {\n        liveNodes.add(cur);\n      }\n    }\n    DatanodeInfo[] locs \u003d liveNodes.toArray(new DatanodeInfo[liveNodes.size()]);\n    BlockPlacementPolicy placementPolicy \u003d placementPolicies\n        .getPolicy(storedBlock.isStriped());\n    int numReplicas \u003d storedBlock.isStriped() ? ((BlockInfoStriped) storedBlock)\n        .getRealDataBlockNum() : storedBlock.getReplication();\n    return placementPolicy.verifyBlockPlacement(locs, numReplicas).isPlacementPolicySatisfied();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    }
  }
}