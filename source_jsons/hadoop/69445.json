{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "InputStriper.java",
  "functionName": "splitFor",
  "functionId": "splitFor___inputDir-FilePool__bytes-long__nLocs-int",
  "sourceFilePath": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java",
  "functionStartLine": 74,
  "functionEndLine": 121,
  "numCommitsSeen": 6,
  "timeTaken": 4175,
  "changeHistory": [
    "dcf84707ab50662add112bd6b01c0bfd63374853",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "dcf84707ab50662add112bd6b01c0bfd63374853": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "dcf84707ab50662add112bd6b01c0bfd63374853": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3543. Mavenize Gridmix. (tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1339629 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/05/12 8:06 AM",
      "commitName": "dcf84707ab50662add112bd6b01c0bfd63374853",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "17/05/12 7:20 AM",
      "commitNameOld": "e1f09365ca0bee093f849fcf2e546dd6e2c0a965",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  CombineFileSplit splitFor(FilePool inputDir, long bytes, int nLocs)\n      throws IOException {\n    final ArrayList\u003cPath\u003e paths \u003d new ArrayList\u003cPath\u003e();\n    final ArrayList\u003cLong\u003e start \u003d new ArrayList\u003cLong\u003e();\n    final ArrayList\u003cLong\u003e length \u003d new ArrayList\u003cLong\u003e();\n    final HashMap\u003cString,Double\u003e sb \u003d new HashMap\u003cString,Double\u003e();\n    do {\n      paths.add(current.getPath());\n      start.add(currentStart);\n      final long fromFile \u003d Math.min(bytes, current.getLen() - currentStart);\n      length.add(fromFile);\n      for (BlockLocation loc :\n          inputDir.locationsFor(current, currentStart, fromFile)) {\n        final double tedium \u003d loc.getLength() / (1.0 * bytes);\n        for (String l : loc.getHosts()) {\n          Double j \u003d sb.get(l);\n          if (null \u003d\u003d j) {\n            sb.put(l, tedium);\n          } else {\n            sb.put(l, j.doubleValue() + tedium);\n          }\n        }\n      }\n      currentStart +\u003d fromFile;\n      bytes -\u003d fromFile;\n      // Switch to a new file if\n      //  - the current file is uncompressed and completely used\n      //  - the current file is compressed\n      \n      CompressionCodecFactory compressionCodecs \u003d \n        new CompressionCodecFactory(conf);\n      CompressionCodec codec \u003d compressionCodecs.getCodec(current.getPath());\n      if (current.getLen() - currentStart \u003d\u003d 0\n          || codec !\u003d null) {\n        current \u003d files.get(++idx % files.size());\n        currentStart \u003d 0;\n      }\n    } while (bytes \u003e 0);\n    final ArrayList\u003cEntry\u003cString,Double\u003e\u003e sort \u003d\n      new ArrayList\u003cEntry\u003cString,Double\u003e\u003e(sb.entrySet());\n    Collections.sort(sort, hostRank);\n    final String[] hosts \u003d new String[Math.min(nLocs, sort.size())];\n    for (int i \u003d 0; i \u003c nLocs \u0026\u0026 i \u003c sort.size(); ++i) {\n      hosts[i] \u003d sort.get(i).getKey();\n    }\n    return new CombineFileSplit(paths.toArray(new Path[0]),\n        toLongArray(start), toLongArray(length), hosts);\n  }",
      "path": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java",
        "newPath": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  CombineFileSplit splitFor(FilePool inputDir, long bytes, int nLocs)\n      throws IOException {\n    final ArrayList\u003cPath\u003e paths \u003d new ArrayList\u003cPath\u003e();\n    final ArrayList\u003cLong\u003e start \u003d new ArrayList\u003cLong\u003e();\n    final ArrayList\u003cLong\u003e length \u003d new ArrayList\u003cLong\u003e();\n    final HashMap\u003cString,Double\u003e sb \u003d new HashMap\u003cString,Double\u003e();\n    do {\n      paths.add(current.getPath());\n      start.add(currentStart);\n      final long fromFile \u003d Math.min(bytes, current.getLen() - currentStart);\n      length.add(fromFile);\n      for (BlockLocation loc :\n          inputDir.locationsFor(current, currentStart, fromFile)) {\n        final double tedium \u003d loc.getLength() / (1.0 * bytes);\n        for (String l : loc.getHosts()) {\n          Double j \u003d sb.get(l);\n          if (null \u003d\u003d j) {\n            sb.put(l, tedium);\n          } else {\n            sb.put(l, j.doubleValue() + tedium);\n          }\n        }\n      }\n      currentStart +\u003d fromFile;\n      bytes -\u003d fromFile;\n      // Switch to a new file if\n      //  - the current file is uncompressed and completely used\n      //  - the current file is compressed\n      \n      CompressionCodecFactory compressionCodecs \u003d \n        new CompressionCodecFactory(conf);\n      CompressionCodec codec \u003d compressionCodecs.getCodec(current.getPath());\n      if (current.getLen() - currentStart \u003d\u003d 0\n          || codec !\u003d null) {\n        current \u003d files.get(++idx % files.size());\n        currentStart \u003d 0;\n      }\n    } while (bytes \u003e 0);\n    final ArrayList\u003cEntry\u003cString,Double\u003e\u003e sort \u003d\n      new ArrayList\u003cEntry\u003cString,Double\u003e\u003e(sb.entrySet());\n    Collections.sort(sort, hostRank);\n    final String[] hosts \u003d new String[Math.min(nLocs, sort.size())];\n    for (int i \u003d 0; i \u003c nLocs \u0026\u0026 i \u003c sort.size(); ++i) {\n      hosts[i] \u003d sort.get(i).getKey();\n    }\n    return new CombineFileSplit(paths.toArray(new Path[0]),\n        toLongArray(start), toLongArray(length), hosts);\n  }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java",
        "newPath": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  CombineFileSplit splitFor(FilePool inputDir, long bytes, int nLocs)\n      throws IOException {\n    final ArrayList\u003cPath\u003e paths \u003d new ArrayList\u003cPath\u003e();\n    final ArrayList\u003cLong\u003e start \u003d new ArrayList\u003cLong\u003e();\n    final ArrayList\u003cLong\u003e length \u003d new ArrayList\u003cLong\u003e();\n    final HashMap\u003cString,Double\u003e sb \u003d new HashMap\u003cString,Double\u003e();\n    do {\n      paths.add(current.getPath());\n      start.add(currentStart);\n      final long fromFile \u003d Math.min(bytes, current.getLen() - currentStart);\n      length.add(fromFile);\n      for (BlockLocation loc :\n          inputDir.locationsFor(current, currentStart, fromFile)) {\n        final double tedium \u003d loc.getLength() / (1.0 * bytes);\n        for (String l : loc.getHosts()) {\n          Double j \u003d sb.get(l);\n          if (null \u003d\u003d j) {\n            sb.put(l, tedium);\n          } else {\n            sb.put(l, j.doubleValue() + tedium);\n          }\n        }\n      }\n      currentStart +\u003d fromFile;\n      bytes -\u003d fromFile;\n      // Switch to a new file if\n      //  - the current file is uncompressed and completely used\n      //  - the current file is compressed\n      \n      CompressionCodecFactory compressionCodecs \u003d \n        new CompressionCodecFactory(conf);\n      CompressionCodec codec \u003d compressionCodecs.getCodec(current.getPath());\n      if (current.getLen() - currentStart \u003d\u003d 0\n          || codec !\u003d null) {\n        current \u003d files.get(++idx % files.size());\n        currentStart \u003d 0;\n      }\n    } while (bytes \u003e 0);\n    final ArrayList\u003cEntry\u003cString,Double\u003e\u003e sort \u003d\n      new ArrayList\u003cEntry\u003cString,Double\u003e\u003e(sb.entrySet());\n    Collections.sort(sort, hostRank);\n    final String[] hosts \u003d new String[Math.min(nLocs, sort.size())];\n    for (int i \u003d 0; i \u003c nLocs \u0026\u0026 i \u003c sort.size(); ++i) {\n      hosts[i] \u003d sort.get(i).getKey();\n    }\n    return new CombineFileSplit(paths.toArray(new Path[0]),\n        toLongArray(start), toLongArray(length), hosts);\n  }",
      "path": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java",
        "newPath": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,48 @@\n+  CombineFileSplit splitFor(FilePool inputDir, long bytes, int nLocs)\n+      throws IOException {\n+    final ArrayList\u003cPath\u003e paths \u003d new ArrayList\u003cPath\u003e();\n+    final ArrayList\u003cLong\u003e start \u003d new ArrayList\u003cLong\u003e();\n+    final ArrayList\u003cLong\u003e length \u003d new ArrayList\u003cLong\u003e();\n+    final HashMap\u003cString,Double\u003e sb \u003d new HashMap\u003cString,Double\u003e();\n+    do {\n+      paths.add(current.getPath());\n+      start.add(currentStart);\n+      final long fromFile \u003d Math.min(bytes, current.getLen() - currentStart);\n+      length.add(fromFile);\n+      for (BlockLocation loc :\n+          inputDir.locationsFor(current, currentStart, fromFile)) {\n+        final double tedium \u003d loc.getLength() / (1.0 * bytes);\n+        for (String l : loc.getHosts()) {\n+          Double j \u003d sb.get(l);\n+          if (null \u003d\u003d j) {\n+            sb.put(l, tedium);\n+          } else {\n+            sb.put(l, j.doubleValue() + tedium);\n+          }\n+        }\n+      }\n+      currentStart +\u003d fromFile;\n+      bytes -\u003d fromFile;\n+      // Switch to a new file if\n+      //  - the current file is uncompressed and completely used\n+      //  - the current file is compressed\n+      \n+      CompressionCodecFactory compressionCodecs \u003d \n+        new CompressionCodecFactory(conf);\n+      CompressionCodec codec \u003d compressionCodecs.getCodec(current.getPath());\n+      if (current.getLen() - currentStart \u003d\u003d 0\n+          || codec !\u003d null) {\n+        current \u003d files.get(++idx % files.size());\n+        currentStart \u003d 0;\n+      }\n+    } while (bytes \u003e 0);\n+    final ArrayList\u003cEntry\u003cString,Double\u003e\u003e sort \u003d\n+      new ArrayList\u003cEntry\u003cString,Double\u003e\u003e(sb.entrySet());\n+    Collections.sort(sort, hostRank);\n+    final String[] hosts \u003d new String[Math.min(nLocs, sort.size())];\n+    for (int i \u003d 0; i \u003c nLocs \u0026\u0026 i \u003c sort.size(); ++i) {\n+      hosts[i] \u003d sort.get(i).getKey();\n+    }\n+    return new CombineFileSplit(paths.toArray(new Path[0]),\n+        toLongArray(start), toLongArray(length), hosts);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  CombineFileSplit splitFor(FilePool inputDir, long bytes, int nLocs)\n      throws IOException {\n    final ArrayList\u003cPath\u003e paths \u003d new ArrayList\u003cPath\u003e();\n    final ArrayList\u003cLong\u003e start \u003d new ArrayList\u003cLong\u003e();\n    final ArrayList\u003cLong\u003e length \u003d new ArrayList\u003cLong\u003e();\n    final HashMap\u003cString,Double\u003e sb \u003d new HashMap\u003cString,Double\u003e();\n    do {\n      paths.add(current.getPath());\n      start.add(currentStart);\n      final long fromFile \u003d Math.min(bytes, current.getLen() - currentStart);\n      length.add(fromFile);\n      for (BlockLocation loc :\n          inputDir.locationsFor(current, currentStart, fromFile)) {\n        final double tedium \u003d loc.getLength() / (1.0 * bytes);\n        for (String l : loc.getHosts()) {\n          Double j \u003d sb.get(l);\n          if (null \u003d\u003d j) {\n            sb.put(l, tedium);\n          } else {\n            sb.put(l, j.doubleValue() + tedium);\n          }\n        }\n      }\n      currentStart +\u003d fromFile;\n      bytes -\u003d fromFile;\n      // Switch to a new file if\n      //  - the current file is uncompressed and completely used\n      //  - the current file is compressed\n      \n      CompressionCodecFactory compressionCodecs \u003d \n        new CompressionCodecFactory(conf);\n      CompressionCodec codec \u003d compressionCodecs.getCodec(current.getPath());\n      if (current.getLen() - currentStart \u003d\u003d 0\n          || codec !\u003d null) {\n        current \u003d files.get(++idx % files.size());\n        currentStart \u003d 0;\n      }\n    } while (bytes \u003e 0);\n    final ArrayList\u003cEntry\u003cString,Double\u003e\u003e sort \u003d\n      new ArrayList\u003cEntry\u003cString,Double\u003e\u003e(sb.entrySet());\n    Collections.sort(sort, hostRank);\n    final String[] hosts \u003d new String[Math.min(nLocs, sort.size())];\n    for (int i \u003d 0; i \u003c nLocs \u0026\u0026 i \u003c sort.size(); ++i) {\n      hosts[i] \u003d sort.get(i).getKey();\n    }\n    return new CombineFileSplit(paths.toArray(new Path[0]),\n        toLongArray(start), toLongArray(length), hosts);\n  }",
      "path": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java"
    }
  }
}