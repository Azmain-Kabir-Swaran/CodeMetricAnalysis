{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirRenameOp.java",
  "functionName": "removeDst",
  "functionId": "removeDst",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java",
  "functionStartLine": 709,
  "functionEndLine": 718,
  "numCommitsSeen": 36,
  "timeTaken": 2195,
  "changeHistory": [
    "8cb473124c1cf1c6f68ead7bde06558ebf7ce47e",
    "d244574d03903b0514b0308da85d2f06c2384524",
    "5caebbae8c2fc9ba2e32384657aee21641a1a6d0"
  ],
  "changeHistoryShort": {
    "8cb473124c1cf1c6f68ead7bde06558ebf7ce47e": "Yexceptionschange",
    "d244574d03903b0514b0308da85d2f06c2384524": "Ybodychange",
    "5caebbae8c2fc9ba2e32384657aee21641a1a6d0": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8cb473124c1cf1c6f68ead7bde06558ebf7ce47e": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-6651. Deletion failure can leak inodes permanently. Contributed by Jing Zhao.\n",
      "commitDate": "02/02/15 4:32 PM",
      "commitName": "8cb473124c1cf1c6f68ead7bde06558ebf7ce47e",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/01/15 3:25 PM",
      "commitNameOld": "d244574d03903b0514b0308da85d2f06c2384524",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 5.05,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,10 @@\n-    long removeDst() throws IOException {\n+    long removeDst() {\n       long removedNum \u003d fsd.removeLastINode(dstIIP);\n       if (removedNum !\u003d -1) {\n         oldDstChild \u003d dstIIP.getLastINode();\n         // update the quota count if necessary\n         fsd.updateCountForDelete(oldDstChild, dstIIP);\n         dstIIP \u003d INodesInPath.replace(dstIIP, dstIIP.length() - 1, null);\n       }\n       return removedNum;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    long removeDst() {\n      long removedNum \u003d fsd.removeLastINode(dstIIP);\n      if (removedNum !\u003d -1) {\n        oldDstChild \u003d dstIIP.getLastINode();\n        // update the quota count if necessary\n        fsd.updateCountForDelete(oldDstChild, dstIIP);\n        dstIIP \u003d INodesInPath.replace(dstIIP, dstIIP.length() - 1, null);\n      }\n      return removedNum;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java",
      "extendedDetails": {
        "oldValue": "[IOException]",
        "newValue": "[]"
      }
    },
    "d244574d03903b0514b0308da85d2f06c2384524": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7611. deleteSnapshot and delete of a file can leave orphaned blocks in the blocksMap on NameNode restart. Contributed by Jing Zhao and Byron Wong.\n",
      "commitDate": "28/01/15 3:25 PM",
      "commitName": "d244574d03903b0514b0308da85d2f06c2384524",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "17/01/15 12:56 PM",
      "commitNameOld": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 11.1,
      "commitsBetweenForRepo": 82,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,10 @@\n     long removeDst() throws IOException {\n       long removedNum \u003d fsd.removeLastINode(dstIIP);\n       if (removedNum !\u003d -1) {\n         oldDstChild \u003d dstIIP.getLastINode();\n+        // update the quota count if necessary\n+        fsd.updateCountForDelete(oldDstChild, dstIIP);\n         dstIIP \u003d INodesInPath.replace(dstIIP, dstIIP.length() - 1, null);\n       }\n       return removedNum;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    long removeDst() throws IOException {\n      long removedNum \u003d fsd.removeLastINode(dstIIP);\n      if (removedNum !\u003d -1) {\n        oldDstChild \u003d dstIIP.getLastINode();\n        // update the quota count if necessary\n        fsd.updateCountForDelete(oldDstChild, dstIIP);\n        dstIIP \u003d INodesInPath.replace(dstIIP, dstIIP.length() - 1, null);\n      }\n      return removedNum;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java",
      "extendedDetails": {}
    },
    "5caebbae8c2fc9ba2e32384657aee21641a1a6d0": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7484. Make FSDirectory#addINode take existing INodes as its parameter. Contributed by Jing Zhao.\n",
      "commitDate": "22/12/14 11:19 PM",
      "commitName": "5caebbae8c2fc9ba2e32384657aee21641a1a6d0",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,8 @@\n+    long removeDst() throws IOException {\n+      long removedNum \u003d fsd.removeLastINode(dstIIP);\n+      if (removedNum !\u003d -1) {\n+        oldDstChild \u003d dstIIP.getLastINode();\n+        dstIIP \u003d INodesInPath.replace(dstIIP, dstIIP.length() - 1, null);\n+      }\n+      return removedNum;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    long removeDst() throws IOException {\n      long removedNum \u003d fsd.removeLastINode(dstIIP);\n      if (removedNum !\u003d -1) {\n        oldDstChild \u003d dstIIP.getLastINode();\n        dstIIP \u003d INodesInPath.replace(dstIIP, dstIIP.length() - 1, null);\n      }\n      return removedNum;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java"
    }
  }
}