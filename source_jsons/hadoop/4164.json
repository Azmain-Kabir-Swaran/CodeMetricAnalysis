{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ECPolicyLoader.java",
  "functionName": "loadSchema",
  "functionId": "loadSchema___element-Element",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ECPolicyLoader.java",
  "functionStartLine": 245,
  "functionEndLine": 273,
  "numCommitsSeen": 3,
  "timeTaken": 441,
  "changeHistory": [
    "b0803388fc5ec03b774aa003f52232deb8db6f69"
  ],
  "changeHistoryShort": {
    "b0803388fc5ec03b774aa003f52232deb8db6f69": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b0803388fc5ec03b774aa003f52232deb8db6f69": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11604. Define and parse erasure code policies. Contributed by Lin Zeng\n",
      "commitDate": "20/04/17 10:33 PM",
      "commitName": "b0803388fc5ec03b774aa003f52232deb8db6f69",
      "commitAuthor": "Kai Zheng",
      "diff": "@@ -0,0 +1,29 @@\n+  private ECSchema loadSchema(Element element) {\n+    Map\u003cString, String\u003e schemaOptions \u003d new HashMap\u003cString, String\u003e();\n+    NodeList fields \u003d element.getChildNodes();\n+\n+    for (int i \u003d 0; i \u003c fields.getLength(); i++) {\n+      Node fieldNode \u003d fields.item(i);\n+      if (fieldNode instanceof Element) {\n+        Element field \u003d (Element) fieldNode;\n+        String tagName \u003d field.getTagName();\n+        if (\"k\".equals(tagName)) {\n+          tagName \u003d \"numDataUnits\";\n+        } else if (\"m\".equals(tagName)) {\n+          tagName \u003d \"numParityUnits\";\n+        }\n+\n+        // Get the nonnull text value.\n+        Text text \u003d (Text) field.getFirstChild();\n+        if (text !\u003d null) {\n+          String value \u003d text.getData().trim();\n+          schemaOptions.put(tagName, value);\n+        } else {\n+          throw new IllegalArgumentException(\"Value of \u003c\" + tagName\n+              + \"\u003e is null\");\n+        }\n+      }\n+    }\n+\n+    return new ECSchema(schemaOptions);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private ECSchema loadSchema(Element element) {\n    Map\u003cString, String\u003e schemaOptions \u003d new HashMap\u003cString, String\u003e();\n    NodeList fields \u003d element.getChildNodes();\n\n    for (int i \u003d 0; i \u003c fields.getLength(); i++) {\n      Node fieldNode \u003d fields.item(i);\n      if (fieldNode instanceof Element) {\n        Element field \u003d (Element) fieldNode;\n        String tagName \u003d field.getTagName();\n        if (\"k\".equals(tagName)) {\n          tagName \u003d \"numDataUnits\";\n        } else if (\"m\".equals(tagName)) {\n          tagName \u003d \"numParityUnits\";\n        }\n\n        // Get the nonnull text value.\n        Text text \u003d (Text) field.getFirstChild();\n        if (text !\u003d null) {\n          String value \u003d text.getData().trim();\n          schemaOptions.put(tagName, value);\n        } else {\n          throw new IllegalArgumentException(\"Value of \u003c\" + tagName\n              + \"\u003e is null\");\n        }\n      }\n    }\n\n    return new ECSchema(schemaOptions);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ECPolicyLoader.java"
    }
  }
}