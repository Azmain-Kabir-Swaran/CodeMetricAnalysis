{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSImageFormatPBINode.java",
  "functionName": "save",
  "functionId": "save___out-OutputStream__n-INodeFile",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
  "functionStartLine": 832,
  "functionEndLine": 855,
  "numCommitsSeen": 63,
  "timeTaken": 4859,
  "changeHistory": [
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "1d37a8812160bb030244a1e6b1c753f962d8d2ed",
    "7e091de1366f4b57b5433bc19d738199dc05313d",
    "c12a974ccf5f52f63e4f825d8b4d2385953cd119",
    "683332b36de1040eb8901d676e666527e8c5f8fe",
    "1e1e93040748231dc913190aec1e031c379d8271",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b",
    "3f82484218d5694e62ddcb23376d0e4e332aa8b8",
    "d03acc756094a332f98167426a39db8faf38f450",
    "5c978a43c3052cc1466b23653c354399186b4e10",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305"
  ],
  "changeHistoryShort": {
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ybodychange",
    "1d37a8812160bb030244a1e6b1c753f962d8d2ed": "Ybodychange",
    "7e091de1366f4b57b5433bc19d738199dc05313d": "Ybodychange",
    "c12a974ccf5f52f63e4f825d8b4d2385953cd119": "Ybodychange",
    "683332b36de1040eb8901d676e666527e8c5f8fe": "Ybodychange",
    "1e1e93040748231dc913190aec1e031c379d8271": "Ybodychange",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": "Ybodychange",
    "3f82484218d5694e62ddcb23376d0e4e332aa8b8": "Ybodychange",
    "d03acc756094a332f98167426a39db8faf38f450": "Ybodychange",
    "5c978a43c3052cc1466b23653c354399186b4e10": "Ybodychange",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Yintroduced"
  },
  "changeHistoryDetails": {
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "25/08/15 1:16 AM",
      "commitNameOld": "eee0d4563c62647cfaaed6605ee713aaf69add78",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 27.73,
      "commitsBetweenForRepo": 164,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n     private void save(OutputStream out, INodeFile n) throws IOException {\n       INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n           parent.getSaverContext());\n \n       if (n.getBlocks() !\u003d null) {\n         for (Block block : n.getBlocks()) {\n-          b.addBlocks(PBHelper.convert(block));\n+          b.addBlocks(PBHelperClient.convert(block));\n         }\n       }\n \n       FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n       if (uc !\u003d null) {\n         INodeSection.FileUnderConstructionFeature f \u003d\n             INodeSection.FileUnderConstructionFeature\n             .newBuilder().setClientName(uc.getClientName())\n             .setClientMachine(uc.getClientMachine()).build();\n         b.setFileUC(f);\n       }\n \n       INodeSection.INode r \u003d buildINodeCommon(n)\n           .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n       r.writeDelimitedTo(out);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void save(OutputStream out, INodeFile n) throws IOException {\n      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n          parent.getSaverContext());\n\n      if (n.getBlocks() !\u003d null) {\n        for (Block block : n.getBlocks()) {\n          b.addBlocks(PBHelperClient.convert(block));\n        }\n      }\n\n      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n      if (uc !\u003d null) {\n        INodeSection.FileUnderConstructionFeature f \u003d\n            INodeSection.FileUnderConstructionFeature\n            .newBuilder().setClientName(uc.getClientName())\n            .setClientMachine(uc.getClientMachine()).build();\n        b.setFileUC(f);\n      }\n\n      INodeSection.INode r \u003d buildINodeCommon(n)\n          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n      r.writeDelimitedTo(out);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "1d37a8812160bb030244a1e6b1c753f962d8d2ed": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8854. Erasure coding: add ECPolicy to replace schema+cellSize in hadoop-hdfs. Contributed by Walter Su.\n",
      "commitDate": "13/08/15 10:04 AM",
      "commitName": "1d37a8812160bb030244a1e6b1c753f962d8d2ed",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "15/07/15 8:13 PM",
      "commitNameOld": "4fdd9abd7e43a0fb7b569982954a8f9660b9268b",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 28.58,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,24 @@\n     private void save(OutputStream out, INodeFile n) throws IOException {\n       INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n           parent.getSaverContext());\n       BlockInfo[] blocks \u003d n.getBlocks();\n \n       if (blocks !\u003d null) {\n         for (Block block : n.getBlocks()) {\n           b.addBlocks(PBHelper.convert(block));\n         }\n       }\n \n-      if (n.isStriped()) {\n-        if (blocks !\u003d null \u0026\u0026 blocks.length \u003e 0) {\n-          BlockInfo firstBlock \u003d blocks[0];\n-          Preconditions.checkState(firstBlock.isStriped());\n-          b.setStripingCellSize(((BlockInfoStriped)firstBlock).getCellSize());\n-        } else {\n-          b.setStripingCellSize(HdfsConstants.BLOCK_STRIPED_CELL_SIZE);\n-        }\n-      }\n-\n       FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n       if (uc !\u003d null) {\n         INodeSection.FileUnderConstructionFeature f \u003d\n             INodeSection.FileUnderConstructionFeature\n             .newBuilder().setClientName(uc.getClientName())\n             .setClientMachine(uc.getClientMachine()).build();\n         b.setFileUC(f);\n       }\n \n       INodeSection.INode r \u003d buildINodeCommon(n)\n           .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n       r.writeDelimitedTo(out);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void save(OutputStream out, INodeFile n) throws IOException {\n      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n          parent.getSaverContext());\n      BlockInfo[] blocks \u003d n.getBlocks();\n\n      if (blocks !\u003d null) {\n        for (Block block : n.getBlocks()) {\n          b.addBlocks(PBHelper.convert(block));\n        }\n      }\n\n      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n      if (uc !\u003d null) {\n        INodeSection.FileUnderConstructionFeature f \u003d\n            INodeSection.FileUnderConstructionFeature\n            .newBuilder().setClientName(uc.getClientName())\n            .setClientMachine(uc.getClientMachine()).build();\n        b.setFileUC(f);\n      }\n\n      INodeSection.INode r \u003d buildINodeCommon(n)\n          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n      r.writeDelimitedTo(out);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "7e091de1366f4b57b5433bc19d738199dc05313d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8058. Erasure coding: use BlockInfo[] for both striped and contiguous blocks in INodeFile. Contributed by Zhe Zhang and Yi Liu.\n",
      "commitDate": "15/07/15 9:49 AM",
      "commitName": "7e091de1366f4b57b5433bc19d738199dc05313d",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "17/06/15 11:35 AM",
      "commitNameOld": "c12a974ccf5f52f63e4f825d8b4d2385953cd119",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 27.93,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,34 @@\n     private void save(OutputStream out, INodeFile n) throws IOException {\n       INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n           parent.getSaverContext());\n+      BlockInfo[] blocks \u003d n.getBlocks();\n \n-      BlockInfoContiguous[] cBlks \u003d n.getContiguousBlocks();\n-      if (cBlks !\u003d null) {\n-        for (Block block : cBlks) {\n+      if (blocks !\u003d null) {\n+        for (Block block : n.getBlocks()) {\n           b.addBlocks(PBHelper.convert(block));\n         }\n       }\n \n-      FileWithStripedBlocksFeature sb \u003d n.getStripedBlocksFeature();\n-      if (sb !\u003d null) {\n-        StripedBlocksFeature.Builder builder \u003d\n-            StripedBlocksFeature.newBuilder();\n-        BlockInfoStriped[] sblocks \u003d sb.getBlocks();\n-        if (sblocks !\u003d null \u0026\u0026 sblocks.length \u003e 0) {\n-          final int cellSize \u003d sblocks[0].getCellSize();\n-          for (BlockInfoStriped sblk : sblocks) {\n-            assert cellSize \u003d\u003d sblk.getCellSize();\n-            b.addBlocks(PBHelper.convert(sblk));\n-          }\n-          builder.setCellSize(cellSize);\n+      if (n.isStriped()) {\n+        if (blocks !\u003d null \u0026\u0026 blocks.length \u003e 0) {\n+          BlockInfo firstBlock \u003d blocks[0];\n+          Preconditions.checkState(firstBlock.isStriped());\n+          b.setStripingCellSize(((BlockInfoStriped)firstBlock).getCellSize());\n+        } else {\n+          b.setStripingCellSize(HdfsConstants.BLOCK_STRIPED_CELL_SIZE);\n         }\n-        b.setStripedBlocks(builder.build());\n       }\n \n       FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n       if (uc !\u003d null) {\n         INodeSection.FileUnderConstructionFeature f \u003d\n             INodeSection.FileUnderConstructionFeature\n             .newBuilder().setClientName(uc.getClientName())\n             .setClientMachine(uc.getClientMachine()).build();\n         b.setFileUC(f);\n       }\n \n       INodeSection.INode r \u003d buildINodeCommon(n)\n           .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n       r.writeDelimitedTo(out);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void save(OutputStream out, INodeFile n) throws IOException {\n      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n          parent.getSaverContext());\n      BlockInfo[] blocks \u003d n.getBlocks();\n\n      if (blocks !\u003d null) {\n        for (Block block : n.getBlocks()) {\n          b.addBlocks(PBHelper.convert(block));\n        }\n      }\n\n      if (n.isStriped()) {\n        if (blocks !\u003d null \u0026\u0026 blocks.length \u003e 0) {\n          BlockInfo firstBlock \u003d blocks[0];\n          Preconditions.checkState(firstBlock.isStriped());\n          b.setStripingCellSize(((BlockInfoStriped)firstBlock).getCellSize());\n        } else {\n          b.setStripingCellSize(HdfsConstants.BLOCK_STRIPED_CELL_SIZE);\n        }\n      }\n\n      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n      if (uc !\u003d null) {\n        INodeSection.FileUnderConstructionFeature f \u003d\n            INodeSection.FileUnderConstructionFeature\n            .newBuilder().setClientName(uc.getClientName())\n            .setClientMachine(uc.getClientMachine()).build();\n        b.setFileUC(f);\n      }\n\n      INodeSection.INode r \u003d buildINodeCommon(n)\n          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n      r.writeDelimitedTo(out);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "c12a974ccf5f52f63e4f825d8b4d2385953cd119": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8580. Erasure coding: Persist cellSize in BlockInfoStriped and StripedBlocksFeature. Contributed by Walter Su.\n",
      "commitDate": "17/06/15 11:35 AM",
      "commitName": "c12a974ccf5f52f63e4f825d8b4d2385953cd119",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/06/15 2:48 PM",
      "commitNameOld": "683332b36de1040eb8901d676e666527e8c5f8fe",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 4.87,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,40 @@\n     private void save(OutputStream out, INodeFile n) throws IOException {\n       INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n           parent.getSaverContext());\n \n       BlockInfoContiguous[] cBlks \u003d n.getContiguousBlocks();\n       if (cBlks !\u003d null) {\n         for (Block block : cBlks) {\n           b.addBlocks(PBHelper.convert(block));\n         }\n       }\n \n       FileWithStripedBlocksFeature sb \u003d n.getStripedBlocksFeature();\n       if (sb !\u003d null) {\n-        BlockInfoStriped[] sblocks \u003d sb.getBlocks();\n-        if (sblocks !\u003d null) {\n-          for (BlockInfoStriped sblk : sblocks) {\n-            b.addBlocks(PBHelper.convert(sblk));\n-          }\n-        }\n         StripedBlocksFeature.Builder builder \u003d\n             StripedBlocksFeature.newBuilder();\n+        BlockInfoStriped[] sblocks \u003d sb.getBlocks();\n+        if (sblocks !\u003d null \u0026\u0026 sblocks.length \u003e 0) {\n+          final int cellSize \u003d sblocks[0].getCellSize();\n+          for (BlockInfoStriped sblk : sblocks) {\n+            assert cellSize \u003d\u003d sblk.getCellSize();\n+            b.addBlocks(PBHelper.convert(sblk));\n+          }\n+          builder.setCellSize(cellSize);\n+        }\n         b.setStripedBlocks(builder.build());\n       }\n \n       FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n       if (uc !\u003d null) {\n         INodeSection.FileUnderConstructionFeature f \u003d\n             INodeSection.FileUnderConstructionFeature\n             .newBuilder().setClientName(uc.getClientName())\n             .setClientMachine(uc.getClientMachine()).build();\n         b.setFileUC(f);\n       }\n \n       INodeSection.INode r \u003d buildINodeCommon(n)\n           .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n       r.writeDelimitedTo(out);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void save(OutputStream out, INodeFile n) throws IOException {\n      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n          parent.getSaverContext());\n\n      BlockInfoContiguous[] cBlks \u003d n.getContiguousBlocks();\n      if (cBlks !\u003d null) {\n        for (Block block : cBlks) {\n          b.addBlocks(PBHelper.convert(block));\n        }\n      }\n\n      FileWithStripedBlocksFeature sb \u003d n.getStripedBlocksFeature();\n      if (sb !\u003d null) {\n        StripedBlocksFeature.Builder builder \u003d\n            StripedBlocksFeature.newBuilder();\n        BlockInfoStriped[] sblocks \u003d sb.getBlocks();\n        if (sblocks !\u003d null \u0026\u0026 sblocks.length \u003e 0) {\n          final int cellSize \u003d sblocks[0].getCellSize();\n          for (BlockInfoStriped sblk : sblocks) {\n            assert cellSize \u003d\u003d sblk.getCellSize();\n            b.addBlocks(PBHelper.convert(sblk));\n          }\n          builder.setCellSize(cellSize);\n        }\n        b.setStripedBlocks(builder.build());\n      }\n\n      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n      if (uc !\u003d null) {\n        INodeSection.FileUnderConstructionFeature f \u003d\n            INodeSection.FileUnderConstructionFeature\n            .newBuilder().setClientName(uc.getClientName())\n            .setClientMachine(uc.getClientMachine()).build();\n        b.setFileUC(f);\n      }\n\n      INodeSection.INode r \u003d buildINodeCommon(n)\n          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n      r.writeDelimitedTo(out);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "683332b36de1040eb8901d676e666527e8c5f8fe": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8585. Erasure Coding: Remove dataBlockNum and parityBlockNum from StripedBlockProto. Contributed by Yi Liu.\n",
      "commitDate": "12/06/15 2:48 PM",
      "commitName": "683332b36de1040eb8901d676e666527e8c5f8fe",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 12:02 PM",
      "commitNameOld": "c9103e9cacc67a614940e32fa87c5dbc3daa60de",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 17.12,
      "commitsBetweenForRepo": 42,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n     private void save(OutputStream out, INodeFile n) throws IOException {\n       INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n           parent.getSaverContext());\n \n       BlockInfoContiguous[] cBlks \u003d n.getContiguousBlocks();\n       if (cBlks !\u003d null) {\n         for (Block block : cBlks) {\n           b.addBlocks(PBHelper.convert(block));\n         }\n       }\n \n       FileWithStripedBlocksFeature sb \u003d n.getStripedBlocksFeature();\n       if (sb !\u003d null) {\n-        StripedBlocksFeature.Builder builder \u003d\n-            StripedBlocksFeature.newBuilder();\n         BlockInfoStriped[] sblocks \u003d sb.getBlocks();\n         if (sblocks !\u003d null) {\n           for (BlockInfoStriped sblk : sblocks) {\n-            builder.addBlocks(PBHelper.convert(sblk));\n+            b.addBlocks(PBHelper.convert(sblk));\n           }\n         }\n+        StripedBlocksFeature.Builder builder \u003d\n+            StripedBlocksFeature.newBuilder();\n         b.setStripedBlocks(builder.build());\n       }\n \n       FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n       if (uc !\u003d null) {\n         INodeSection.FileUnderConstructionFeature f \u003d\n             INodeSection.FileUnderConstructionFeature\n             .newBuilder().setClientName(uc.getClientName())\n             .setClientMachine(uc.getClientMachine()).build();\n         b.setFileUC(f);\n       }\n \n       INodeSection.INode r \u003d buildINodeCommon(n)\n           .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n       r.writeDelimitedTo(out);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void save(OutputStream out, INodeFile n) throws IOException {\n      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n          parent.getSaverContext());\n\n      BlockInfoContiguous[] cBlks \u003d n.getContiguousBlocks();\n      if (cBlks !\u003d null) {\n        for (Block block : cBlks) {\n          b.addBlocks(PBHelper.convert(block));\n        }\n      }\n\n      FileWithStripedBlocksFeature sb \u003d n.getStripedBlocksFeature();\n      if (sb !\u003d null) {\n        BlockInfoStriped[] sblocks \u003d sb.getBlocks();\n        if (sblocks !\u003d null) {\n          for (BlockInfoStriped sblk : sblocks) {\n            b.addBlocks(PBHelper.convert(sblk));\n          }\n        }\n        StripedBlocksFeature.Builder builder \u003d\n            StripedBlocksFeature.newBuilder();\n        b.setStripedBlocks(builder.build());\n      }\n\n      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n      if (uc !\u003d null) {\n        INodeSection.FileUnderConstructionFeature f \u003d\n            INodeSection.FileUnderConstructionFeature\n            .newBuilder().setClientName(uc.getClientName())\n            .setClientMachine(uc.getClientMachine()).build();\n        b.setFileUC(f);\n      }\n\n      INodeSection.INode r \u003d buildINodeCommon(n)\n          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n      r.writeDelimitedTo(out);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "1e1e93040748231dc913190aec1e031c379d8271": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7837. Erasure Coding: allocate and persist striped blocks in NameNode. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:32 AM",
      "commitName": "1e1e93040748231dc913190aec1e031c379d8271",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:07 AM",
      "commitNameOld": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,37 @@\n     private void save(OutputStream out, INodeFile n) throws IOException {\n       INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n           parent.getSaverContext());\n \n-      if (n.getBlocks() !\u003d null) {\n-        for (Block block : n.getBlocks()) {\n+      BlockInfoContiguous[] cBlks \u003d n.getContiguousBlocks();\n+      if (cBlks !\u003d null) {\n+        for (Block block : cBlks) {\n           b.addBlocks(PBHelper.convert(block));\n         }\n       }\n \n       FileWithStripedBlocksFeature sb \u003d n.getStripedBlocksFeature();\n       if (sb !\u003d null) {\n         StripedBlocksFeature.Builder builder \u003d\n             StripedBlocksFeature.newBuilder();\n         BlockInfoStriped[] sblocks \u003d sb.getBlocks();\n         if (sblocks !\u003d null) {\n           for (BlockInfoStriped sblk : sblocks) {\n             builder.addBlocks(PBHelper.convert(sblk));\n           }\n         }\n         b.setStripedBlocks(builder.build());\n       }\n \n       FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n       if (uc !\u003d null) {\n         INodeSection.FileUnderConstructionFeature f \u003d\n             INodeSection.FileUnderConstructionFeature\n             .newBuilder().setClientName(uc.getClientName())\n             .setClientMachine(uc.getClientMachine()).build();\n         b.setFileUC(f);\n       }\n \n       INodeSection.INode r \u003d buildINodeCommon(n)\n           .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n       r.writeDelimitedTo(out);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void save(OutputStream out, INodeFile n) throws IOException {\n      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n          parent.getSaverContext());\n\n      BlockInfoContiguous[] cBlks \u003d n.getContiguousBlocks();\n      if (cBlks !\u003d null) {\n        for (Block block : cBlks) {\n          b.addBlocks(PBHelper.convert(block));\n        }\n      }\n\n      FileWithStripedBlocksFeature sb \u003d n.getStripedBlocksFeature();\n      if (sb !\u003d null) {\n        StripedBlocksFeature.Builder builder \u003d\n            StripedBlocksFeature.newBuilder();\n        BlockInfoStriped[] sblocks \u003d sb.getBlocks();\n        if (sblocks !\u003d null) {\n          for (BlockInfoStriped sblk : sblocks) {\n            builder.addBlocks(PBHelper.convert(sblk));\n          }\n        }\n        b.setStripedBlocks(builder.build());\n      }\n\n      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n      if (uc !\u003d null) {\n        INodeSection.FileUnderConstructionFeature f \u003d\n            INodeSection.FileUnderConstructionFeature\n            .newBuilder().setClientName(uc.getClientName())\n            .setClientMachine(uc.getClientMachine()).build();\n        b.setFileUC(f);\n      }\n\n      INodeSection.INode r \u003d buildINodeCommon(n)\n          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n      r.writeDelimitedTo(out);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7749. Erasure Coding: Add striped block support in INodeFile. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "08/05/15 11:04 PM",
      "commitNameOld": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 17.5,
      "commitsBetweenForRepo": 144,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,36 @@\n     private void save(OutputStream out, INodeFile n) throws IOException {\n       INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n           parent.getSaverContext());\n \n       if (n.getBlocks() !\u003d null) {\n         for (Block block : n.getBlocks()) {\n           b.addBlocks(PBHelper.convert(block));\n         }\n       }\n \n+      FileWithStripedBlocksFeature sb \u003d n.getStripedBlocksFeature();\n+      if (sb !\u003d null) {\n+        StripedBlocksFeature.Builder builder \u003d\n+            StripedBlocksFeature.newBuilder();\n+        BlockInfoStriped[] sblocks \u003d sb.getBlocks();\n+        if (sblocks !\u003d null) {\n+          for (BlockInfoStriped sblk : sblocks) {\n+            builder.addBlocks(PBHelper.convert(sblk));\n+          }\n+        }\n+        b.setStripedBlocks(builder.build());\n+      }\n+\n       FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n       if (uc !\u003d null) {\n         INodeSection.FileUnderConstructionFeature f \u003d\n             INodeSection.FileUnderConstructionFeature\n             .newBuilder().setClientName(uc.getClientName())\n             .setClientMachine(uc.getClientMachine()).build();\n         b.setFileUC(f);\n       }\n \n       INodeSection.INode r \u003d buildINodeCommon(n)\n           .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n       r.writeDelimitedTo(out);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void save(OutputStream out, INodeFile n) throws IOException {\n      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n          parent.getSaverContext());\n\n      if (n.getBlocks() !\u003d null) {\n        for (Block block : n.getBlocks()) {\n          b.addBlocks(PBHelper.convert(block));\n        }\n      }\n\n      FileWithStripedBlocksFeature sb \u003d n.getStripedBlocksFeature();\n      if (sb !\u003d null) {\n        StripedBlocksFeature.Builder builder \u003d\n            StripedBlocksFeature.newBuilder();\n        BlockInfoStriped[] sblocks \u003d sb.getBlocks();\n        if (sblocks !\u003d null) {\n          for (BlockInfoStriped sblk : sblocks) {\n            builder.addBlocks(PBHelper.convert(sblk));\n          }\n        }\n        b.setStripedBlocks(builder.build());\n      }\n\n      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n      if (uc !\u003d null) {\n        INodeSection.FileUnderConstructionFeature f \u003d\n            INodeSection.FileUnderConstructionFeature\n            .newBuilder().setClientName(uc.getClientName())\n            .setClientMachine(uc.getClientMachine()).build();\n        b.setFileUC(f);\n      }\n\n      INodeSection.INode r \u003d buildINodeCommon(n)\n          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n      r.writeDelimitedTo(out);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "3f82484218d5694e62ddcb23376d0e4e332aa8b8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6563. NameNode cannot save fsimage in certain circumstances when snapshots are in use. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603712 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/06/14 10:17 PM",
      "commitName": "3f82484218d5694e62ddcb23376d0e4e332aa8b8",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "21/05/14 6:57 AM",
      "commitNameOld": "ac23a55547716df29b3e25c98a113399e184d9d1",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 28.64,
      "commitsBetweenForRepo": 168,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,23 @@\n     private void save(OutputStream out, INodeFile n) throws IOException {\n       INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n           parent.getSaverContext());\n \n-      for (Block block : n.getBlocks()) {\n-        b.addBlocks(PBHelper.convert(block));\n+      if (n.getBlocks() !\u003d null) {\n+        for (Block block : n.getBlocks()) {\n+          b.addBlocks(PBHelper.convert(block));\n+        }\n       }\n \n       FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n       if (uc !\u003d null) {\n         INodeSection.FileUnderConstructionFeature f \u003d\n             INodeSection.FileUnderConstructionFeature\n             .newBuilder().setClientName(uc.getClientName())\n             .setClientMachine(uc.getClientMachine()).build();\n         b.setFileUC(f);\n       }\n \n       INodeSection.INode r \u003d buildINodeCommon(n)\n           .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n       r.writeDelimitedTo(out);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void save(OutputStream out, INodeFile n) throws IOException {\n      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n          parent.getSaverContext());\n\n      if (n.getBlocks() !\u003d null) {\n        for (Block block : n.getBlocks()) {\n          b.addBlocks(PBHelper.convert(block));\n        }\n      }\n\n      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n      if (uc !\u003d null) {\n        INodeSection.FileUnderConstructionFeature f \u003d\n            INodeSection.FileUnderConstructionFeature\n            .newBuilder().setClientName(uc.getClientName())\n            .setClientMachine(uc.getClientMachine()).build();\n        b.setFileUC(f);\n      }\n\n      INodeSection.INode r \u003d buildINodeCommon(n)\n          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n      r.writeDelimitedTo(out);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "d03acc756094a332f98167426a39db8faf38f450": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5914. Incorporate ACLs with the changes from HDFS-5698. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4685@1566991 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/02/14 10:25 PM",
      "commitName": "d03acc756094a332f98167426a39db8faf38f450",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "10/02/14 3:13 PM",
      "commitNameOld": "5c978a43c3052cc1466b23653c354399186b4e10",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 0.3,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n     private void save(OutputStream out, INodeFile n) throws IOException {\n       INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n-          parent.getSaverContext().getStringMap());\n+          parent.getSaverContext());\n \n       for (Block block : n.getBlocks()) {\n         b.addBlocks(PBHelper.convert(block));\n       }\n \n       FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n       if (uc !\u003d null) {\n         INodeSection.FileUnderConstructionFeature f \u003d\n             INodeSection.FileUnderConstructionFeature\n             .newBuilder().setClientName(uc.getClientName())\n             .setClientMachine(uc.getClientMachine()).build();\n         b.setFileUC(f);\n       }\n \n       INodeSection.INode r \u003d buildINodeCommon(n)\n           .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n       r.writeDelimitedTo(out);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void save(OutputStream out, INodeFile n) throws IOException {\n      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n          parent.getSaverContext());\n\n      for (Block block : n.getBlocks()) {\n        b.addBlocks(PBHelper.convert(block));\n      }\n\n      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n      if (uc !\u003d null) {\n        INodeSection.FileUnderConstructionFeature f \u003d\n            INodeSection.FileUnderConstructionFeature\n            .newBuilder().setClientName(uc.getClientName())\n            .setClientMachine(uc.getClientMachine()).build();\n        b.setFileUC(f);\n      }\n\n      INodeSection.INode r \u003d buildINodeCommon(n)\n          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n      r.writeDelimitedTo(out);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "5c978a43c3052cc1466b23653c354399186b4e10": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5915. Refactor FSImageFormatProtobuf to simplify cross section reads. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566824 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/02/14 3:13 PM",
      "commitName": "5c978a43c3052cc1466b23653c354399186b4e10",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "09/02/14 11:18 AM",
      "commitNameOld": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.16,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n     private void save(OutputStream out, INodeFile n) throws IOException {\n       INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n-          parent.getStringMap());\n+          parent.getSaverContext().getStringMap());\n \n       for (Block block : n.getBlocks()) {\n         b.addBlocks(PBHelper.convert(block));\n       }\n \n       FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n       if (uc !\u003d null) {\n         INodeSection.FileUnderConstructionFeature f \u003d\n             INodeSection.FileUnderConstructionFeature\n             .newBuilder().setClientName(uc.getClientName())\n             .setClientMachine(uc.getClientMachine()).build();\n         b.setFileUC(f);\n       }\n \n       INodeSection.INode r \u003d buildINodeCommon(n)\n           .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n       r.writeDelimitedTo(out);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void save(OutputStream out, INodeFile n) throws IOException {\n      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n          parent.getSaverContext().getStringMap());\n\n      for (Block block : n.getBlocks()) {\n        b.addBlocks(PBHelper.convert(block));\n      }\n\n      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n      if (uc !\u003d null) {\n        INodeSection.FileUnderConstructionFeature f \u003d\n            INodeSection.FileUnderConstructionFeature\n            .newBuilder().setClientName(uc.getClientName())\n            .setClientMachine(uc.getClientMachine()).build();\n        b.setFileUC(f);\n      }\n\n      INodeSection.INode r \u003d buildINodeCommon(n)\n          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n      r.writeDelimitedTo(out);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,21 @@\n+    private void save(OutputStream out, INodeFile n) throws IOException {\n+      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n+          parent.getStringMap());\n+\n+      for (Block block : n.getBlocks()) {\n+        b.addBlocks(PBHelper.convert(block));\n+      }\n+\n+      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n+      if (uc !\u003d null) {\n+        INodeSection.FileUnderConstructionFeature f \u003d\n+            INodeSection.FileUnderConstructionFeature\n+            .newBuilder().setClientName(uc.getClientName())\n+            .setClientMachine(uc.getClientMachine()).build();\n+        b.setFileUC(f);\n+      }\n+\n+      INodeSection.INode r \u003d buildINodeCommon(n)\n+          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n+      r.writeDelimitedTo(out);\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void save(OutputStream out, INodeFile n) throws IOException {\n      INodeSection.INodeFile.Builder b \u003d buildINodeFile(n,\n          parent.getStringMap());\n\n      for (Block block : n.getBlocks()) {\n        b.addBlocks(PBHelper.convert(block));\n      }\n\n      FileUnderConstructionFeature uc \u003d n.getFileUnderConstructionFeature();\n      if (uc !\u003d null) {\n        INodeSection.FileUnderConstructionFeature f \u003d\n            INodeSection.FileUnderConstructionFeature\n            .newBuilder().setClientName(uc.getClientName())\n            .setClientMachine(uc.getClientMachine()).build();\n        b.setFileUC(f);\n      }\n\n      INodeSection.INode r \u003d buildINodeCommon(n)\n          .setType(INodeSection.INode.Type.FILE).setFile(b).build();\n      r.writeDelimitedTo(out);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java"
    }
  }
}