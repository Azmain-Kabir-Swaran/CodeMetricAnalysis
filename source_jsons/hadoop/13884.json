{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "addBlock",
  "functionId": "addBlock___block-BlockInfo__results-List__BlockWithLocations__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 4104,
  "functionEndLine": 4136,
  "numCommitsSeen": 477,
  "timeTaken": 8587,
  "changeHistory": [
    "c09dc258a8f64fab852bf6f26187163480dbee3c"
  ],
  "changeHistoryShort": {
    "c09dc258a8f64fab852bf6f26187163480dbee3c": "Ybodychange"
  },
  "changeHistoryDetails": {
    "c09dc258a8f64fab852bf6f26187163480dbee3c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8882. Erasure Coding: Use datablocks, parityblocks and cell size from ErasureCodingPolicy. Contributed by Vinayakumar B.\n\nChange-Id: Ic56da0b426f47c63dac440aef6f5fc8554f6cf13\n",
      "commitDate": "23/09/15 1:34 PM",
      "commitName": "c09dc258a8f64fab852bf6f26187163480dbee3c",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "22/09/15 1:27 PM",
      "commitNameOld": "1080c3730068177ddd10dc313890ac1f5dc58f1a",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   private long addBlock(BlockInfo block, List\u003cBlockWithLocations\u003e results) {\n     final List\u003cDatanodeStorageInfo\u003e locations \u003d getValidLocations(block);\n     if(locations.size() \u003d\u003d 0) {\n       return 0;\n     } else {\n       final String[] datanodeUuids \u003d new String[locations.size()];\n       final String[] storageIDs \u003d new String[datanodeUuids.length];\n       final StorageType[] storageTypes \u003d new StorageType[datanodeUuids.length];\n       for(int i \u003d 0; i \u003c locations.size(); i++) {\n         final DatanodeStorageInfo s \u003d locations.get(i);\n         datanodeUuids[i] \u003d s.getDatanodeDescriptor().getDatanodeUuid();\n         storageIDs[i] \u003d s.getStorageID();\n         storageTypes[i] \u003d s.getStorageType();\n       }\n       BlockWithLocations blkWithLocs \u003d new BlockWithLocations(block,\n           datanodeUuids, storageIDs, storageTypes);\n       if(block.isStriped()) {\n         BlockInfoStriped blockStriped \u003d (BlockInfoStriped) block;\n         byte[] indices \u003d new byte[locations.size()];\n         for (int i \u003d 0; i \u003c locations.size(); i++) {\n           indices[i] \u003d\n               (byte) blockStriped.getStorageBlockIndex(locations.get(i));\n         }\n         results.add(new StripedBlockWithLocations(blkWithLocs, indices,\n-            blockStriped.getDataBlockNum()));\n+            blockStriped.getDataBlockNum(), blockStriped.getCellSize()));\n         // approximate size\n         return block.getNumBytes() / blockStriped.getDataBlockNum();\n       }else{\n         results.add(blkWithLocs);\n         return block.getNumBytes();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long addBlock(BlockInfo block, List\u003cBlockWithLocations\u003e results) {\n    final List\u003cDatanodeStorageInfo\u003e locations \u003d getValidLocations(block);\n    if(locations.size() \u003d\u003d 0) {\n      return 0;\n    } else {\n      final String[] datanodeUuids \u003d new String[locations.size()];\n      final String[] storageIDs \u003d new String[datanodeUuids.length];\n      final StorageType[] storageTypes \u003d new StorageType[datanodeUuids.length];\n      for(int i \u003d 0; i \u003c locations.size(); i++) {\n        final DatanodeStorageInfo s \u003d locations.get(i);\n        datanodeUuids[i] \u003d s.getDatanodeDescriptor().getDatanodeUuid();\n        storageIDs[i] \u003d s.getStorageID();\n        storageTypes[i] \u003d s.getStorageType();\n      }\n      BlockWithLocations blkWithLocs \u003d new BlockWithLocations(block,\n          datanodeUuids, storageIDs, storageTypes);\n      if(block.isStriped()) {\n        BlockInfoStriped blockStriped \u003d (BlockInfoStriped) block;\n        byte[] indices \u003d new byte[locations.size()];\n        for (int i \u003d 0; i \u003c locations.size(); i++) {\n          indices[i] \u003d\n              (byte) blockStriped.getStorageBlockIndex(locations.get(i));\n        }\n        results.add(new StripedBlockWithLocations(blkWithLocs, indices,\n            blockStriped.getDataBlockNum(), blockStriped.getCellSize()));\n        // approximate size\n        return block.getNumBytes() / blockStriped.getDataBlockNum();\n      }else{\n        results.add(blkWithLocs);\n        return block.getNumBytes();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    }
  }
}