{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Journal.java",
  "functionName": "prepareRecovery",
  "functionId": "prepareRecovery___reqInfo-RequestInfo__segmentTxId-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
  "functionStartLine": 814,
  "functionEndLine": 853,
  "numCommitsSeen": 64,
  "timeTaken": 2868,
  "changeHistory": [
    "6beb25ab7e4f5454dba0315a296081e61753f301",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "3ccd905d8a0fe5e3a206ac955b689a6f02b25e67",
    "83c14fbd24353b5e882f065faec81e58449afed3",
    "60c20e559b8036410e2d9081b9c60d1e04e56253",
    "8021d9199f278345aca6211f318145342ad036f4",
    "1e68d4726b225fb4a62eb8d79a3160dd03059ccb",
    "f765fdb65701e61887daedb2b369af4be12cb432",
    "74d4573a23db5586c6e47ff2277aa7c35237da34"
  ],
  "changeHistoryShort": {
    "6beb25ab7e4f5454dba0315a296081e61753f301": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "3ccd905d8a0fe5e3a206ac955b689a6f02b25e67": "Ybodychange",
    "83c14fbd24353b5e882f065faec81e58449afed3": "Ybodychange",
    "60c20e559b8036410e2d9081b9c60d1e04e56253": "Ybodychange",
    "8021d9199f278345aca6211f318145342ad036f4": "Ybodychange",
    "1e68d4726b225fb4a62eb8d79a3160dd03059ccb": "Ybodychange",
    "f765fdb65701e61887daedb2b369af4be12cb432": "Ybodychange",
    "74d4573a23db5586c6e47ff2277aa7c35237da34": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6beb25ab7e4f5454dba0315a296081e61753f301": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13544. Improve logging for JournalNode in federated cluster.\n",
      "commitDate": "14/05/18 10:12 AM",
      "commitName": "6beb25ab7e4f5454dba0315a296081e61753f301",
      "commitAuthor": "Hanisha Koneru",
      "commitDateOld": "13/10/17 2:22 PM",
      "commitNameOld": "8dd1eeb94fef59feaf19182dd8f1fcf1389c7f34",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 212.83,
      "commitsBetweenForRepo": 2051,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   public synchronized PrepareRecoveryResponseProto prepareRecovery(\n       RequestInfo reqInfo, long segmentTxId) throws IOException {\n     checkFormatted();\n     checkRequest(reqInfo);\n     \n     abortCurSegment();\n     \n     PrepareRecoveryResponseProto.Builder builder \u003d\n         PrepareRecoveryResponseProto.newBuilder();\n \n     PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n     completeHalfDoneAcceptRecovery(previouslyAccepted);\n \n     SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n     boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n \n     if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n       SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n       assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() :\n             \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n             \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n             \n       builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n         .setSegmentState(previouslyAccepted.getSegmentState());\n     } else {\n       if (segInfo !\u003d null) {\n         builder.setSegmentState(segInfo);\n       }\n     }\n     \n     builder.setLastWriterEpoch(lastWriterEpoch.get());\n     if (committedTxnId.get() !\u003d HdfsServerConstants.INVALID_TXID) {\n       builder.setLastCommittedTxId(committedTxnId.get());\n     }\n     \n     PrepareRecoveryResponseProto resp \u003d builder.build();\n     LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n-        TextFormat.shortDebugString(resp));\n+        TextFormat.shortDebugString(resp) + \" ; journal id: \" + journalId);\n     return resp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized PrepareRecoveryResponseProto prepareRecovery(\n      RequestInfo reqInfo, long segmentTxId) throws IOException {\n    checkFormatted();\n    checkRequest(reqInfo);\n    \n    abortCurSegment();\n    \n    PrepareRecoveryResponseProto.Builder builder \u003d\n        PrepareRecoveryResponseProto.newBuilder();\n\n    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n    completeHalfDoneAcceptRecovery(previouslyAccepted);\n\n    SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n    boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n\n    if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n      SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n      assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() :\n            \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n            \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n            \n      builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n        .setSegmentState(previouslyAccepted.getSegmentState());\n    } else {\n      if (segInfo !\u003d null) {\n        builder.setSegmentState(segInfo);\n      }\n    }\n    \n    builder.setLastWriterEpoch(lastWriterEpoch.get());\n    if (committedTxnId.get() !\u003d HdfsServerConstants.INVALID_TXID) {\n      builder.setLastCommittedTxId(committedTxnId.get());\n    }\n    \n    PrepareRecoveryResponseProto resp \u003d builder.build();\n    LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n        TextFormat.shortDebugString(resp) + \" ; journal id: \" + journalId);\n    return resp;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "08/01/15 4:09 PM",
      "commitNameOld": "ae91b13a4b1896b893268253104f935c3078d345",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 113.7,
      "commitsBetweenForRepo": 1004,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   public synchronized PrepareRecoveryResponseProto prepareRecovery(\n       RequestInfo reqInfo, long segmentTxId) throws IOException {\n     checkFormatted();\n     checkRequest(reqInfo);\n     \n     abortCurSegment();\n     \n     PrepareRecoveryResponseProto.Builder builder \u003d\n         PrepareRecoveryResponseProto.newBuilder();\n \n     PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n     completeHalfDoneAcceptRecovery(previouslyAccepted);\n \n     SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n     boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n \n     if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n       SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n       assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() :\n             \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n             \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n             \n       builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n         .setSegmentState(previouslyAccepted.getSegmentState());\n     } else {\n       if (segInfo !\u003d null) {\n         builder.setSegmentState(segInfo);\n       }\n     }\n     \n     builder.setLastWriterEpoch(lastWriterEpoch.get());\n-    if (committedTxnId.get() !\u003d HdfsConstants.INVALID_TXID) {\n+    if (committedTxnId.get() !\u003d HdfsServerConstants.INVALID_TXID) {\n       builder.setLastCommittedTxId(committedTxnId.get());\n     }\n     \n     PrepareRecoveryResponseProto resp \u003d builder.build();\n     LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n         TextFormat.shortDebugString(resp));\n     return resp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized PrepareRecoveryResponseProto prepareRecovery(\n      RequestInfo reqInfo, long segmentTxId) throws IOException {\n    checkFormatted();\n    checkRequest(reqInfo);\n    \n    abortCurSegment();\n    \n    PrepareRecoveryResponseProto.Builder builder \u003d\n        PrepareRecoveryResponseProto.newBuilder();\n\n    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n    completeHalfDoneAcceptRecovery(previouslyAccepted);\n\n    SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n    boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n\n    if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n      SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n      assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() :\n            \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n            \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n            \n      builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n        .setSegmentState(previouslyAccepted.getSegmentState());\n    } else {\n      if (segInfo !\u003d null) {\n        builder.setSegmentState(segInfo);\n      }\n    }\n    \n    builder.setLastWriterEpoch(lastWriterEpoch.get());\n    if (committedTxnId.get() !\u003d HdfsServerConstants.INVALID_TXID) {\n      builder.setLastCommittedTxId(committedTxnId.get());\n    }\n    \n    PrepareRecoveryResponseProto resp \u003d builder.build();\n    LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n        TextFormat.shortDebugString(resp));\n    return resp;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "3ccd905d8a0fe5e3a206ac955b689a6f02b25e67": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3955. QJM: Make acceptRecovery() atomic. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1387706 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/09/12 11:57 AM",
      "commitName": "3ccd905d8a0fe5e3a206ac955b689a6f02b25e67",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "19/09/12 11:52 AM",
      "commitNameOld": "663e7484c04c197eed53f10a7808140f1c955277",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,40 @@\n   public synchronized PrepareRecoveryResponseProto prepareRecovery(\n       RequestInfo reqInfo, long segmentTxId) throws IOException {\n     checkFormatted();\n     checkRequest(reqInfo);\n     \n     abortCurSegment();\n     \n     PrepareRecoveryResponseProto.Builder builder \u003d\n         PrepareRecoveryResponseProto.newBuilder();\n \n+    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n+    completeHalfDoneAcceptRecovery(previouslyAccepted);\n+\n     SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n     boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n-    \n-    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n \n     if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n       SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n       assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() :\n             \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n             \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n             \n       builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n         .setSegmentState(previouslyAccepted.getSegmentState());\n     } else {\n       if (segInfo !\u003d null) {\n         builder.setSegmentState(segInfo);\n       }\n     }\n     \n     builder.setLastWriterEpoch(lastWriterEpoch.get());\n     if (committedTxnId.get() !\u003d HdfsConstants.INVALID_TXID) {\n       builder.setLastCommittedTxId(committedTxnId.get());\n     }\n     \n     PrepareRecoveryResponseProto resp \u003d builder.build();\n     LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n         TextFormat.shortDebugString(resp));\n     return resp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized PrepareRecoveryResponseProto prepareRecovery(\n      RequestInfo reqInfo, long segmentTxId) throws IOException {\n    checkFormatted();\n    checkRequest(reqInfo);\n    \n    abortCurSegment();\n    \n    PrepareRecoveryResponseProto.Builder builder \u003d\n        PrepareRecoveryResponseProto.newBuilder();\n\n    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n    completeHalfDoneAcceptRecovery(previouslyAccepted);\n\n    SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n    boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n\n    if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n      SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n      assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() :\n            \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n            \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n            \n      builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n        .setSegmentState(previouslyAccepted.getSegmentState());\n    } else {\n      if (segInfo !\u003d null) {\n        builder.setSegmentState(segInfo);\n      }\n    }\n    \n    builder.setLastWriterEpoch(lastWriterEpoch.get());\n    if (committedTxnId.get() !\u003d HdfsConstants.INVALID_TXID) {\n      builder.setLastCommittedTxId(committedTxnId.get());\n    }\n    \n    PrepareRecoveryResponseProto resp \u003d builder.build();\n    LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n        TextFormat.shortDebugString(resp));\n    return resp;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "83c14fbd24353b5e882f065faec81e58449afed3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3943. QJM: remove currently-unused md5sum field. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1386863 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/09/12 2:51 PM",
      "commitName": "83c14fbd24353b5e882f065faec81e58449afed3",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "10/09/12 11:33 PM",
      "commitNameOld": "a93ba1648ac78ae0ad9e7c75c35e8594d8c48af4",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 6.64,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,39 @@\n   public synchronized PrepareRecoveryResponseProto prepareRecovery(\n       RequestInfo reqInfo, long segmentTxId) throws IOException {\n     checkFormatted();\n     checkRequest(reqInfo);\n     \n     abortCurSegment();\n     \n     PrepareRecoveryResponseProto.Builder builder \u003d\n         PrepareRecoveryResponseProto.newBuilder();\n \n     SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n     boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n     \n     PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n \n     if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n       SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n-      assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() \u0026\u0026\n-             acceptedState.getMd5Sum().equals(segInfo.getMd5Sum()) :\n+      assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() :\n             \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n             \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n             \n       builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n         .setSegmentState(previouslyAccepted.getSegmentState());\n     } else {\n       if (segInfo !\u003d null) {\n         builder.setSegmentState(segInfo);\n       }\n     }\n     \n     builder.setLastWriterEpoch(lastWriterEpoch.get());\n     if (committedTxnId.get() !\u003d HdfsConstants.INVALID_TXID) {\n       builder.setLastCommittedTxId(committedTxnId.get());\n     }\n     \n     PrepareRecoveryResponseProto resp \u003d builder.build();\n     LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n         TextFormat.shortDebugString(resp));\n     return resp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized PrepareRecoveryResponseProto prepareRecovery(\n      RequestInfo reqInfo, long segmentTxId) throws IOException {\n    checkFormatted();\n    checkRequest(reqInfo);\n    \n    abortCurSegment();\n    \n    PrepareRecoveryResponseProto.Builder builder \u003d\n        PrepareRecoveryResponseProto.newBuilder();\n\n    SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n    boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n    \n    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n\n    if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n      SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n      assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() :\n            \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n            \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n            \n      builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n        .setSegmentState(previouslyAccepted.getSegmentState());\n    } else {\n      if (segInfo !\u003d null) {\n        builder.setSegmentState(segInfo);\n      }\n    }\n    \n    builder.setLastWriterEpoch(lastWriterEpoch.get());\n    if (committedTxnId.get() !\u003d HdfsConstants.INVALID_TXID) {\n      builder.setLastCommittedTxId(committedTxnId.get());\n    }\n    \n    PrepareRecoveryResponseProto resp \u003d builder.build();\n    LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n        TextFormat.shortDebugString(resp));\n    return resp;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "60c20e559b8036410e2d9081b9c60d1e04e56253": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3900. QJM: avoid validating log segments on log rolls. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1383041 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/09/12 11:53 AM",
      "commitName": "60c20e559b8036410e2d9081b9c60d1e04e56253",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "10/09/12 11:51 AM",
      "commitNameOld": "ca4582222e89114e4c61d38fbf973a66d2867abf",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,40 @@\n   public synchronized PrepareRecoveryResponseProto prepareRecovery(\n       RequestInfo reqInfo, long segmentTxId) throws IOException {\n     checkFormatted();\n     checkRequest(reqInfo);\n     \n+    abortCurSegment();\n+    \n     PrepareRecoveryResponseProto.Builder builder \u003d\n         PrepareRecoveryResponseProto.newBuilder();\n \n     SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n     boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n     \n     PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n \n     if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n       SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n       assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() \u0026\u0026\n              acceptedState.getMd5Sum().equals(segInfo.getMd5Sum()) :\n             \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n             \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n             \n       builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n         .setSegmentState(previouslyAccepted.getSegmentState());\n     } else {\n       if (segInfo !\u003d null) {\n         builder.setSegmentState(segInfo);\n       }\n     }\n     \n     builder.setLastWriterEpoch(lastWriterEpoch.get());\n     if (committedTxnId.get() !\u003d HdfsConstants.INVALID_TXID) {\n       builder.setLastCommittedTxId(committedTxnId.get());\n     }\n     \n     PrepareRecoveryResponseProto resp \u003d builder.build();\n     LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n         TextFormat.shortDebugString(resp));\n     return resp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized PrepareRecoveryResponseProto prepareRecovery(\n      RequestInfo reqInfo, long segmentTxId) throws IOException {\n    checkFormatted();\n    checkRequest(reqInfo);\n    \n    abortCurSegment();\n    \n    PrepareRecoveryResponseProto.Builder builder \u003d\n        PrepareRecoveryResponseProto.newBuilder();\n\n    SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n    boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n    \n    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n\n    if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n      SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n      assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() \u0026\u0026\n             acceptedState.getMd5Sum().equals(segInfo.getMd5Sum()) :\n            \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n            \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n            \n      builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n        .setSegmentState(previouslyAccepted.getSegmentState());\n    } else {\n      if (segInfo !\u003d null) {\n        builder.setSegmentState(segInfo);\n      }\n    }\n    \n    builder.setLastWriterEpoch(lastWriterEpoch.get());\n    if (committedTxnId.get() !\u003d HdfsConstants.INVALID_TXID) {\n      builder.setLastCommittedTxId(committedTxnId.get());\n    }\n    \n    PrepareRecoveryResponseProto resp \u003d builder.build();\n    LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n        TextFormat.shortDebugString(resp));\n    return resp;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "8021d9199f278345aca6211f318145342ad036f4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3863. Track last \"committed\" txid in QJM. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1380976 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/12 9:13 PM",
      "commitName": "8021d9199f278345aca6211f318145342ad036f4",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "27/08/12 12:55 PM",
      "commitNameOld": "1e68d4726b225fb4a62eb8d79a3160dd03059ccb",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 8.35,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,38 @@\n   public synchronized PrepareRecoveryResponseProto prepareRecovery(\n       RequestInfo reqInfo, long segmentTxId) throws IOException {\n-    checkRequest(reqInfo);\n     checkFormatted();\n+    checkRequest(reqInfo);\n     \n     PrepareRecoveryResponseProto.Builder builder \u003d\n         PrepareRecoveryResponseProto.newBuilder();\n \n     SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n     boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n     \n     PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n \n     if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n       SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n       assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() \u0026\u0026\n              acceptedState.getMd5Sum().equals(segInfo.getMd5Sum()) :\n             \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n             \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n             \n       builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n         .setSegmentState(previouslyAccepted.getSegmentState());\n     } else {\n       if (segInfo !\u003d null) {\n         builder.setSegmentState(segInfo);\n       }\n     }\n     \n     builder.setLastWriterEpoch(lastWriterEpoch.get());\n+    if (committedTxnId.get() !\u003d HdfsConstants.INVALID_TXID) {\n+      builder.setLastCommittedTxId(committedTxnId.get());\n+    }\n     \n     PrepareRecoveryResponseProto resp \u003d builder.build();\n     LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n         TextFormat.shortDebugString(resp));\n     return resp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized PrepareRecoveryResponseProto prepareRecovery(\n      RequestInfo reqInfo, long segmentTxId) throws IOException {\n    checkFormatted();\n    checkRequest(reqInfo);\n    \n    PrepareRecoveryResponseProto.Builder builder \u003d\n        PrepareRecoveryResponseProto.newBuilder();\n\n    SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n    boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n    \n    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n\n    if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n      SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n      assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() \u0026\u0026\n             acceptedState.getMd5Sum().equals(segInfo.getMd5Sum()) :\n            \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n            \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n            \n      builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n        .setSegmentState(previouslyAccepted.getSegmentState());\n    } else {\n      if (segInfo !\u003d null) {\n        builder.setSegmentState(segInfo);\n      }\n    }\n    \n    builder.setLastWriterEpoch(lastWriterEpoch.get());\n    if (committedTxnId.get() !\u003d HdfsConstants.INVALID_TXID) {\n      builder.setLastCommittedTxId(committedTxnId.get());\n    }\n    \n    PrepareRecoveryResponseProto resp \u003d builder.build();\n    LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n        TextFormat.shortDebugString(resp));\n    return resp;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "1e68d4726b225fb4a62eb8d79a3160dd03059ccb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3845. Fixes for edge cases in QJM recovery protocol. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1377809 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/08/12 12:55 PM",
      "commitName": "1e68d4726b225fb4a62eb8d79a3160dd03059ccb",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "15/08/12 11:58 AM",
      "commitNameOld": "42cdc1b0835abb4a331d40f30f2c210143b747bc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 12.04,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,35 @@\n   public synchronized PrepareRecoveryResponseProto prepareRecovery(\n       RequestInfo reqInfo, long segmentTxId) throws IOException {\n     checkRequest(reqInfo);\n     checkFormatted();\n     \n     PrepareRecoveryResponseProto.Builder builder \u003d\n         PrepareRecoveryResponseProto.newBuilder();\n+\n+    SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n+    boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n     \n     PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n-    if (previouslyAccepted !\u003d null) {\n+\n+    if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n+      SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n+      assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() \u0026\u0026\n+             acceptedState.getMd5Sum().equals(segInfo.getMd5Sum()) :\n+            \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n+            \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n+            \n       builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n         .setSegmentState(previouslyAccepted.getSegmentState());\n     } else {\n-      SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n       if (segInfo !\u003d null) {\n         builder.setSegmentState(segInfo);\n       }\n     }\n     \n+    builder.setLastWriterEpoch(lastWriterEpoch.get());\n+    \n     PrepareRecoveryResponseProto resp \u003d builder.build();\n     LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n         TextFormat.shortDebugString(resp));\n     return resp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized PrepareRecoveryResponseProto prepareRecovery(\n      RequestInfo reqInfo, long segmentTxId) throws IOException {\n    checkRequest(reqInfo);\n    checkFormatted();\n    \n    PrepareRecoveryResponseProto.Builder builder \u003d\n        PrepareRecoveryResponseProto.newBuilder();\n\n    SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n    boolean hasFinalizedSegment \u003d segInfo !\u003d null \u0026\u0026 !segInfo.getIsInProgress();\n    \n    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n\n    if (previouslyAccepted !\u003d null \u0026\u0026 !hasFinalizedSegment) {\n      SegmentStateProto acceptedState \u003d previouslyAccepted.getSegmentState();\n      assert acceptedState.getEndTxId() \u003d\u003d segInfo.getEndTxId() \u0026\u0026\n             acceptedState.getMd5Sum().equals(segInfo.getMd5Sum()) :\n            \"prev accepted: \" + TextFormat.shortDebugString(previouslyAccepted)+ \"\\n\" +\n            \"on disk:       \" + TextFormat.shortDebugString(segInfo);\n            \n      builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n        .setSegmentState(previouslyAccepted.getSegmentState());\n    } else {\n      if (segInfo !\u003d null) {\n        builder.setSegmentState(segInfo);\n      }\n    }\n    \n    builder.setLastWriterEpoch(lastWriterEpoch.get());\n    \n    PrepareRecoveryResponseProto resp \u003d builder.build();\n    LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n        TextFormat.shortDebugString(resp));\n    return resp;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "f765fdb65701e61887daedb2b369af4be12cb432": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3793. Implement genericized format() in QJM. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1373177 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/08/12 5:48 PM",
      "commitName": "f765fdb65701e61887daedb2b369af4be12cb432",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "25/07/12 2:47 PM",
      "commitNameOld": "b17018e4b821ec860144d8bd38bc1fcb0d7eeaa5",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 20.13,
      "commitsBetweenForRepo": 82,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   public synchronized PrepareRecoveryResponseProto prepareRecovery(\n       RequestInfo reqInfo, long segmentTxId) throws IOException {\n     checkRequest(reqInfo);\n+    checkFormatted();\n     \n     PrepareRecoveryResponseProto.Builder builder \u003d\n         PrepareRecoveryResponseProto.newBuilder();\n     \n     PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n     if (previouslyAccepted !\u003d null) {\n       builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n         .setSegmentState(previouslyAccepted.getSegmentState());\n     } else {\n       SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n       if (segInfo !\u003d null) {\n         builder.setSegmentState(segInfo);\n       }\n     }\n     \n     PrepareRecoveryResponseProto resp \u003d builder.build();\n     LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n         TextFormat.shortDebugString(resp));\n     return resp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized PrepareRecoveryResponseProto prepareRecovery(\n      RequestInfo reqInfo, long segmentTxId) throws IOException {\n    checkRequest(reqInfo);\n    checkFormatted();\n    \n    PrepareRecoveryResponseProto.Builder builder \u003d\n        PrepareRecoveryResponseProto.newBuilder();\n    \n    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n    if (previouslyAccepted !\u003d null) {\n      builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n        .setSegmentState(previouslyAccepted.getSegmentState());\n    } else {\n      SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n      if (segInfo !\u003d null) {\n        builder.setSegmentState(segInfo);\n      }\n    }\n    \n    PrepareRecoveryResponseProto resp \u003d builder.build();\n    LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n        TextFormat.shortDebugString(resp));\n    return resp;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "74d4573a23db5586c6e47ff2277aa7c35237da34": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3077. Quorum-based protocol for reading and writing edit logs. Contributed by Todd Lipcon based on initial work from Brandon Li and Hari Mankude.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1363596 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/07/12 5:25 PM",
      "commitName": "74d4573a23db5586c6e47ff2277aa7c35237da34",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,23 @@\n+  public synchronized PrepareRecoveryResponseProto prepareRecovery(\n+      RequestInfo reqInfo, long segmentTxId) throws IOException {\n+    checkRequest(reqInfo);\n+    \n+    PrepareRecoveryResponseProto.Builder builder \u003d\n+        PrepareRecoveryResponseProto.newBuilder();\n+    \n+    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n+    if (previouslyAccepted !\u003d null) {\n+      builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n+        .setSegmentState(previouslyAccepted.getSegmentState());\n+    } else {\n+      SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n+      if (segInfo !\u003d null) {\n+        builder.setSegmentState(segInfo);\n+      }\n+    }\n+    \n+    PrepareRecoveryResponseProto resp \u003d builder.build();\n+    LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n+        TextFormat.shortDebugString(resp));\n+    return resp;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized PrepareRecoveryResponseProto prepareRecovery(\n      RequestInfo reqInfo, long segmentTxId) throws IOException {\n    checkRequest(reqInfo);\n    \n    PrepareRecoveryResponseProto.Builder builder \u003d\n        PrepareRecoveryResponseProto.newBuilder();\n    \n    PersistedRecoveryPaxosData previouslyAccepted \u003d getPersistedPaxosData(segmentTxId);\n    if (previouslyAccepted !\u003d null) {\n      builder.setAcceptedInEpoch(previouslyAccepted.getAcceptedInEpoch())\n        .setSegmentState(previouslyAccepted.getSegmentState());\n    } else {\n      SegmentStateProto segInfo \u003d getSegmentInfo(segmentTxId);\n      if (segInfo !\u003d null) {\n        builder.setSegmentState(segInfo);\n      }\n    }\n    \n    PrepareRecoveryResponseProto resp \u003d builder.build();\n    LOG.info(\"Prepared recovery for segment \" + segmentTxId + \": \" +\n        TextFormat.shortDebugString(resp));\n    return resp;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java"
    }
  }
}