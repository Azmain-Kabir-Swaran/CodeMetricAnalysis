{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CacheReplicationMonitor.java",
  "functionName": "addNewPendingCached",
  "functionId": "addNewPendingCached___neededCached-int(modifiers-final)__cachedBlock-CachedBlock__cached-List__DatanodeDescriptor____pendingCached-List__DatanodeDescriptor__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
  "functionStartLine": 653,
  "functionEndLine": 744,
  "numCommitsSeen": 41,
  "timeTaken": 4676,
  "changeHistory": [
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "211c78c09073e5b34db309b49d8de939a7a812f5",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "93e23a99157c30b51752fc49748c3c210745a187",
    "bab90b2222abb41d0e3382a92c2f9a8dc568f0e0",
    "8deb7a60575ad33b78a5167673276275ba7bece5",
    "d85c017d0488930d806f267141057fc73e68c728",
    "07e4fb1455abc33584fc666ef745abe256ebd7d1",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a"
  ],
  "changeHistoryShort": {
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "211c78c09073e5b34db309b49d8de939a7a812f5": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "93e23a99157c30b51752fc49748c3c210745a187": "Ybodychange",
    "bab90b2222abb41d0e3382a92c2f9a8dc568f0e0": "Ybodychange",
    "8deb7a60575ad33b78a5167673276275ba7bece5": "Ymultichange(Ybodychange,Yparametermetachange)",
    "d85c017d0488930d806f267141057fc73e68c728": "Ybodychange",
    "07e4fb1455abc33584fc666ef745abe256ebd7d1": "Ybodychange",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "15/06/16 10:47 PM",
      "commitNameOld": "2ca73445f5c2929d9c2ff4232dca58a63a0570a0",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 123.79,
      "commitsBetweenForRepo": 949,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,92 @@\n   private void addNewPendingCached(final int neededCached,\n       CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n       List\u003cDatanodeDescriptor\u003e pendingCached) {\n     // To figure out which replicas can be cached, we consult the\n     // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n     BlockInfo blockInfo \u003d blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo \u003d\u003d null) {\n       LOG.debug(\"Block {}: can\u0027t add new cached replicas,\" +\n           \" because there is no record of this block \" +\n           \"on the NameNode.\", cachedBlock.getBlockId());\n       return;\n     }\n     if (!blockInfo.isComplete()) {\n       LOG.debug(\"Block {}: can\u0027t cache this block, because it is not yet\"\n           + \" complete.\", cachedBlock.getBlockId());\n       return;\n     }\n     // Filter the list of replicas to only the valid targets\n     List\u003cDatanodeDescriptor\u003e possibilities \u003d\n         new LinkedList\u003cDatanodeDescriptor\u003e();\n     int numReplicas \u003d blockInfo.getCapacity();\n     Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n         blockManager.getCorruptReplicas(blockInfo);\n     int outOfCapacity \u003d 0;\n     for (int i \u003d 0; i \u003c numReplicas; i++) {\n       DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n       if (datanode \u003d\u003d null) {\n         continue;\n       }\n-      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n+      if (!datanode.isInService()) {\n         continue;\n       }\n       if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n         continue;\n       }\n       if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n         continue;\n       }\n       long pendingBytes \u003d 0;\n       // Subtract pending cached blocks from effective capacity\n       Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n         BlockInfo info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n           pendingBytes -\u003d info.getNumBytes();\n         }\n       }\n       it \u003d datanode.getPendingUncached().iterator();\n       // Add pending uncached blocks from effective capacity\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n         BlockInfo info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n           pendingBytes +\u003d info.getNumBytes();\n         }\n       }\n       long pendingCapacity \u003d pendingBytes + datanode.getCacheRemaining();\n       if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n         LOG.trace(\"Block {}: DataNode {} is not a valid possibility \" +\n             \"because the block has size {}, but the DataNode only has {} \" +\n             \"bytes of cache remaining ({} pending bytes, {} already cached.)\",\n             blockInfo.getBlockId(), datanode.getDatanodeUuid(),\n             blockInfo.getNumBytes(), pendingCapacity, pendingBytes,\n             datanode.getCacheRemaining());\n         outOfCapacity++;\n         continue;\n       }\n       possibilities.add(datanode);\n     }\n     List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n         neededCached, blockManager.getDatanodeManager().getStaleInterval());\n     for (DatanodeDescriptor datanode : chosen) {\n       LOG.trace(\"Block {}: added to PENDING_CACHED on DataNode {}\",\n           blockInfo.getBlockId(), datanode.getDatanodeUuid());\n       pendingCached.add(datanode);\n       boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n       assert added;\n     }\n     // We were unable to satisfy the requested replication factor\n     if (neededCached \u003e chosen.size()) {\n       LOG.debug(\"Block {}: we only have {} of {} cached replicas.\"\n               + \" {} DataNodes have insufficient cache capacity.\",\n           blockInfo.getBlockId(),\n           (cachedBlock.getReplication() - neededCached + chosen.size()),\n           cachedBlock.getReplication(), outOfCapacity\n       );\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addNewPendingCached(final int neededCached,\n      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n      List\u003cDatanodeDescriptor\u003e pendingCached) {\n    // To figure out which replicas can be cached, we consult the\n    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n    BlockInfo blockInfo \u003d blockManager.\n          getStoredBlock(new Block(cachedBlock.getBlockId()));\n    if (blockInfo \u003d\u003d null) {\n      LOG.debug(\"Block {}: can\u0027t add new cached replicas,\" +\n          \" because there is no record of this block \" +\n          \"on the NameNode.\", cachedBlock.getBlockId());\n      return;\n    }\n    if (!blockInfo.isComplete()) {\n      LOG.debug(\"Block {}: can\u0027t cache this block, because it is not yet\"\n          + \" complete.\", cachedBlock.getBlockId());\n      return;\n    }\n    // Filter the list of replicas to only the valid targets\n    List\u003cDatanodeDescriptor\u003e possibilities \u003d\n        new LinkedList\u003cDatanodeDescriptor\u003e();\n    int numReplicas \u003d blockInfo.getCapacity();\n    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n        blockManager.getCorruptReplicas(blockInfo);\n    int outOfCapacity \u003d 0;\n    for (int i \u003d 0; i \u003c numReplicas; i++) {\n      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n      if (datanode \u003d\u003d null) {\n        continue;\n      }\n      if (!datanode.isInService()) {\n        continue;\n      }\n      if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n        continue;\n      }\n      if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n        continue;\n      }\n      long pendingBytes \u003d 0;\n      // Subtract pending cached blocks from effective capacity\n      Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingBytes -\u003d info.getNumBytes();\n        }\n      }\n      it \u003d datanode.getPendingUncached().iterator();\n      // Add pending uncached blocks from effective capacity\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingBytes +\u003d info.getNumBytes();\n        }\n      }\n      long pendingCapacity \u003d pendingBytes + datanode.getCacheRemaining();\n      if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n        LOG.trace(\"Block {}: DataNode {} is not a valid possibility \" +\n            \"because the block has size {}, but the DataNode only has {} \" +\n            \"bytes of cache remaining ({} pending bytes, {} already cached.)\",\n            blockInfo.getBlockId(), datanode.getDatanodeUuid(),\n            blockInfo.getNumBytes(), pendingCapacity, pendingBytes,\n            datanode.getCacheRemaining());\n        outOfCapacity++;\n        continue;\n      }\n      possibilities.add(datanode);\n    }\n    List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n        neededCached, blockManager.getDatanodeManager().getStaleInterval());\n    for (DatanodeDescriptor datanode : chosen) {\n      LOG.trace(\"Block {}: added to PENDING_CACHED on DataNode {}\",\n          blockInfo.getBlockId(), datanode.getDatanodeUuid());\n      pendingCached.add(datanode);\n      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n      assert added;\n    }\n    // We were unable to satisfy the requested replication factor\n    if (neededCached \u003e chosen.size()) {\n      LOG.debug(\"Block {}: we only have {} of {} cached replicas.\"\n              + \" {} DataNodes have insufficient cache capacity.\",\n          blockInfo.getBlockId(),\n          (cachedBlock.getReplication() - neededCached + chosen.size()),\n          cachedBlock.getReplication(), outOfCapacity\n      );\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "211c78c09073e5b34db309b49d8de939a7a812f5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9549. TestCacheDirectives#testExceedsCapacity is flaky (Xiao Chen via cmccabe)\n",
      "commitDate": "23/02/16 12:01 PM",
      "commitName": "211c78c09073e5b34db309b49d8de939a7a812f5",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "29/06/15 12:12 PM",
      "commitNameOld": "2ffd84273ac490724fe7e7825664bb6d09ef0e99",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 239.03,
      "commitsBetweenForRepo": 1612,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,92 @@\n   private void addNewPendingCached(final int neededCached,\n       CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n       List\u003cDatanodeDescriptor\u003e pendingCached) {\n     // To figure out which replicas can be cached, we consult the\n     // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n     BlockInfo blockInfo \u003d blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo \u003d\u003d null) {\n       LOG.debug(\"Block {}: can\u0027t add new cached replicas,\" +\n           \" because there is no record of this block \" +\n           \"on the NameNode.\", cachedBlock.getBlockId());\n       return;\n     }\n     if (!blockInfo.isComplete()) {\n       LOG.debug(\"Block {}: can\u0027t cache this block, because it is not yet\"\n           + \" complete.\", cachedBlock.getBlockId());\n       return;\n     }\n     // Filter the list of replicas to only the valid targets\n     List\u003cDatanodeDescriptor\u003e possibilities \u003d\n         new LinkedList\u003cDatanodeDescriptor\u003e();\n     int numReplicas \u003d blockInfo.getCapacity();\n     Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n         blockManager.getCorruptReplicas(blockInfo);\n     int outOfCapacity \u003d 0;\n     for (int i \u003d 0; i \u003c numReplicas; i++) {\n       DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n       if (datanode \u003d\u003d null) {\n         continue;\n       }\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue;\n       }\n       if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n         continue;\n       }\n       if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n         continue;\n       }\n       long pendingBytes \u003d 0;\n       // Subtract pending cached blocks from effective capacity\n       Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n         BlockInfo info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n           pendingBytes -\u003d info.getNumBytes();\n         }\n       }\n       it \u003d datanode.getPendingUncached().iterator();\n       // Add pending uncached blocks from effective capacity\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n         BlockInfo info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n           pendingBytes +\u003d info.getNumBytes();\n         }\n       }\n       long pendingCapacity \u003d pendingBytes + datanode.getCacheRemaining();\n       if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n         LOG.trace(\"Block {}: DataNode {} is not a valid possibility \" +\n-            \"because the block has size {}, but the DataNode only has {}\" +\n-            \"bytes of cache remaining ({} pending bytes, {} already cached.\",\n+            \"because the block has size {}, but the DataNode only has {} \" +\n+            \"bytes of cache remaining ({} pending bytes, {} already cached.)\",\n             blockInfo.getBlockId(), datanode.getDatanodeUuid(),\n             blockInfo.getNumBytes(), pendingCapacity, pendingBytes,\n             datanode.getCacheRemaining());\n         outOfCapacity++;\n         continue;\n       }\n       possibilities.add(datanode);\n     }\n     List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n         neededCached, blockManager.getDatanodeManager().getStaleInterval());\n     for (DatanodeDescriptor datanode : chosen) {\n       LOG.trace(\"Block {}: added to PENDING_CACHED on DataNode {}\",\n           blockInfo.getBlockId(), datanode.getDatanodeUuid());\n       pendingCached.add(datanode);\n       boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n       assert added;\n     }\n     // We were unable to satisfy the requested replication factor\n     if (neededCached \u003e chosen.size()) {\n       LOG.debug(\"Block {}: we only have {} of {} cached replicas.\"\n               + \" {} DataNodes have insufficient cache capacity.\",\n           blockInfo.getBlockId(),\n           (cachedBlock.getReplication() - neededCached + chosen.size()),\n           cachedBlock.getReplication(), outOfCapacity\n       );\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addNewPendingCached(final int neededCached,\n      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n      List\u003cDatanodeDescriptor\u003e pendingCached) {\n    // To figure out which replicas can be cached, we consult the\n    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n    BlockInfo blockInfo \u003d blockManager.\n          getStoredBlock(new Block(cachedBlock.getBlockId()));\n    if (blockInfo \u003d\u003d null) {\n      LOG.debug(\"Block {}: can\u0027t add new cached replicas,\" +\n          \" because there is no record of this block \" +\n          \"on the NameNode.\", cachedBlock.getBlockId());\n      return;\n    }\n    if (!blockInfo.isComplete()) {\n      LOG.debug(\"Block {}: can\u0027t cache this block, because it is not yet\"\n          + \" complete.\", cachedBlock.getBlockId());\n      return;\n    }\n    // Filter the list of replicas to only the valid targets\n    List\u003cDatanodeDescriptor\u003e possibilities \u003d\n        new LinkedList\u003cDatanodeDescriptor\u003e();\n    int numReplicas \u003d blockInfo.getCapacity();\n    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n        blockManager.getCorruptReplicas(blockInfo);\n    int outOfCapacity \u003d 0;\n    for (int i \u003d 0; i \u003c numReplicas; i++) {\n      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n      if (datanode \u003d\u003d null) {\n        continue;\n      }\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue;\n      }\n      if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n        continue;\n      }\n      if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n        continue;\n      }\n      long pendingBytes \u003d 0;\n      // Subtract pending cached blocks from effective capacity\n      Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingBytes -\u003d info.getNumBytes();\n        }\n      }\n      it \u003d datanode.getPendingUncached().iterator();\n      // Add pending uncached blocks from effective capacity\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingBytes +\u003d info.getNumBytes();\n        }\n      }\n      long pendingCapacity \u003d pendingBytes + datanode.getCacheRemaining();\n      if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n        LOG.trace(\"Block {}: DataNode {} is not a valid possibility \" +\n            \"because the block has size {}, but the DataNode only has {} \" +\n            \"bytes of cache remaining ({} pending bytes, {} already cached.)\",\n            blockInfo.getBlockId(), datanode.getDatanodeUuid(),\n            blockInfo.getNumBytes(), pendingCapacity, pendingBytes,\n            datanode.getCacheRemaining());\n        outOfCapacity++;\n        continue;\n      }\n      possibilities.add(datanode);\n    }\n    List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n        neededCached, blockManager.getDatanodeManager().getStaleInterval());\n    for (DatanodeDescriptor datanode : chosen) {\n      LOG.trace(\"Block {}: added to PENDING_CACHED on DataNode {}\",\n          blockInfo.getBlockId(), datanode.getDatanodeUuid());\n      pendingCached.add(datanode);\n      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n      assert added;\n    }\n    // We were unable to satisfy the requested replication factor\n    if (neededCached \u003e chosen.size()) {\n      LOG.debug(\"Block {}: we only have {} of {} cached replicas.\"\n              + \" {} DataNodes have insufficient cache capacity.\",\n          blockInfo.getBlockId(),\n          (cachedBlock.getReplication() - neededCached + chosen.size()),\n          cachedBlock.getReplication(), outOfCapacity\n      );\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "08/02/15 11:51 AM",
      "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 108.12,
      "commitsBetweenForRepo": 1040,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,92 @@\n   private void addNewPendingCached(final int neededCached,\n       CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n       List\u003cDatanodeDescriptor\u003e pendingCached) {\n     // To figure out which replicas can be cached, we consult the\n     // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n-    BlockInfoContiguous blockInfo \u003d blockManager.\n+    BlockInfo blockInfo \u003d blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo \u003d\u003d null) {\n       LOG.debug(\"Block {}: can\u0027t add new cached replicas,\" +\n           \" because there is no record of this block \" +\n           \"on the NameNode.\", cachedBlock.getBlockId());\n       return;\n     }\n     if (!blockInfo.isComplete()) {\n       LOG.debug(\"Block {}: can\u0027t cache this block, because it is not yet\"\n           + \" complete.\", cachedBlock.getBlockId());\n       return;\n     }\n     // Filter the list of replicas to only the valid targets\n     List\u003cDatanodeDescriptor\u003e possibilities \u003d\n         new LinkedList\u003cDatanodeDescriptor\u003e();\n     int numReplicas \u003d blockInfo.getCapacity();\n     Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n         blockManager.getCorruptReplicas(blockInfo);\n     int outOfCapacity \u003d 0;\n     for (int i \u003d 0; i \u003c numReplicas; i++) {\n       DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n       if (datanode \u003d\u003d null) {\n         continue;\n       }\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue;\n       }\n       if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n         continue;\n       }\n       if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n         continue;\n       }\n       long pendingBytes \u003d 0;\n       // Subtract pending cached blocks from effective capacity\n       Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n-        BlockInfoContiguous info \u003d\n+        BlockInfo info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n           pendingBytes -\u003d info.getNumBytes();\n         }\n       }\n       it \u003d datanode.getPendingUncached().iterator();\n       // Add pending uncached blocks from effective capacity\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n-        BlockInfoContiguous info \u003d\n+        BlockInfo info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n           pendingBytes +\u003d info.getNumBytes();\n         }\n       }\n       long pendingCapacity \u003d pendingBytes + datanode.getCacheRemaining();\n       if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n         LOG.trace(\"Block {}: DataNode {} is not a valid possibility \" +\n             \"because the block has size {}, but the DataNode only has {}\" +\n             \"bytes of cache remaining ({} pending bytes, {} already cached.\",\n             blockInfo.getBlockId(), datanode.getDatanodeUuid(),\n             blockInfo.getNumBytes(), pendingCapacity, pendingBytes,\n             datanode.getCacheRemaining());\n         outOfCapacity++;\n         continue;\n       }\n       possibilities.add(datanode);\n     }\n     List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n         neededCached, blockManager.getDatanodeManager().getStaleInterval());\n     for (DatanodeDescriptor datanode : chosen) {\n       LOG.trace(\"Block {}: added to PENDING_CACHED on DataNode {}\",\n           blockInfo.getBlockId(), datanode.getDatanodeUuid());\n       pendingCached.add(datanode);\n       boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n       assert added;\n     }\n     // We were unable to satisfy the requested replication factor\n     if (neededCached \u003e chosen.size()) {\n       LOG.debug(\"Block {}: we only have {} of {} cached replicas.\"\n               + \" {} DataNodes have insufficient cache capacity.\",\n           blockInfo.getBlockId(),\n           (cachedBlock.getReplication() - neededCached + chosen.size()),\n           cachedBlock.getReplication(), outOfCapacity\n       );\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addNewPendingCached(final int neededCached,\n      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n      List\u003cDatanodeDescriptor\u003e pendingCached) {\n    // To figure out which replicas can be cached, we consult the\n    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n    BlockInfo blockInfo \u003d blockManager.\n          getStoredBlock(new Block(cachedBlock.getBlockId()));\n    if (blockInfo \u003d\u003d null) {\n      LOG.debug(\"Block {}: can\u0027t add new cached replicas,\" +\n          \" because there is no record of this block \" +\n          \"on the NameNode.\", cachedBlock.getBlockId());\n      return;\n    }\n    if (!blockInfo.isComplete()) {\n      LOG.debug(\"Block {}: can\u0027t cache this block, because it is not yet\"\n          + \" complete.\", cachedBlock.getBlockId());\n      return;\n    }\n    // Filter the list of replicas to only the valid targets\n    List\u003cDatanodeDescriptor\u003e possibilities \u003d\n        new LinkedList\u003cDatanodeDescriptor\u003e();\n    int numReplicas \u003d blockInfo.getCapacity();\n    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n        blockManager.getCorruptReplicas(blockInfo);\n    int outOfCapacity \u003d 0;\n    for (int i \u003d 0; i \u003c numReplicas; i++) {\n      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n      if (datanode \u003d\u003d null) {\n        continue;\n      }\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue;\n      }\n      if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n        continue;\n      }\n      if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n        continue;\n      }\n      long pendingBytes \u003d 0;\n      // Subtract pending cached blocks from effective capacity\n      Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingBytes -\u003d info.getNumBytes();\n        }\n      }\n      it \u003d datanode.getPendingUncached().iterator();\n      // Add pending uncached blocks from effective capacity\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingBytes +\u003d info.getNumBytes();\n        }\n      }\n      long pendingCapacity \u003d pendingBytes + datanode.getCacheRemaining();\n      if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n        LOG.trace(\"Block {}: DataNode {} is not a valid possibility \" +\n            \"because the block has size {}, but the DataNode only has {}\" +\n            \"bytes of cache remaining ({} pending bytes, {} already cached.\",\n            blockInfo.getBlockId(), datanode.getDatanodeUuid(),\n            blockInfo.getNumBytes(), pendingCapacity, pendingBytes,\n            datanode.getCacheRemaining());\n        outOfCapacity++;\n        continue;\n      }\n      possibilities.add(datanode);\n    }\n    List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n        neededCached, blockManager.getDatanodeManager().getStaleInterval());\n    for (DatanodeDescriptor datanode : chosen) {\n      LOG.trace(\"Block {}: added to PENDING_CACHED on DataNode {}\",\n          blockInfo.getBlockId(), datanode.getDatanodeUuid());\n      pendingCached.add(datanode);\n      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n      assert added;\n    }\n    // We were unable to satisfy the requested replication factor\n    if (neededCached \u003e chosen.size()) {\n      LOG.debug(\"Block {}: we only have {} of {} cached replicas.\"\n              + \" {} DataNodes have insufficient cache capacity.\",\n          blockInfo.getBlockId(),\n          (cachedBlock.getReplication() - neededCached + chosen.size()),\n          cachedBlock.getReplication(), outOfCapacity\n      );\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "23/10/14 11:58 PM",
      "commitNameOld": "0942c99eba12f6baf5609c9621cd07b09618a97e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 107.54,
      "commitsBetweenForRepo": 791,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,92 @@\n   private void addNewPendingCached(final int neededCached,\n       CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n       List\u003cDatanodeDescriptor\u003e pendingCached) {\n     // To figure out which replicas can be cached, we consult the\n     // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n-    BlockInfo blockInfo \u003d blockManager.\n+    BlockInfoContiguous blockInfo \u003d blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo \u003d\u003d null) {\n       LOG.debug(\"Block {}: can\u0027t add new cached replicas,\" +\n           \" because there is no record of this block \" +\n           \"on the NameNode.\", cachedBlock.getBlockId());\n       return;\n     }\n     if (!blockInfo.isComplete()) {\n       LOG.debug(\"Block {}: can\u0027t cache this block, because it is not yet\"\n           + \" complete.\", cachedBlock.getBlockId());\n       return;\n     }\n     // Filter the list of replicas to only the valid targets\n     List\u003cDatanodeDescriptor\u003e possibilities \u003d\n         new LinkedList\u003cDatanodeDescriptor\u003e();\n     int numReplicas \u003d blockInfo.getCapacity();\n     Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n         blockManager.getCorruptReplicas(blockInfo);\n     int outOfCapacity \u003d 0;\n     for (int i \u003d 0; i \u003c numReplicas; i++) {\n       DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n       if (datanode \u003d\u003d null) {\n         continue;\n       }\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue;\n       }\n       if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n         continue;\n       }\n       if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n         continue;\n       }\n       long pendingBytes \u003d 0;\n       // Subtract pending cached blocks from effective capacity\n       Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n-        BlockInfo info \u003d\n+        BlockInfoContiguous info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n           pendingBytes -\u003d info.getNumBytes();\n         }\n       }\n       it \u003d datanode.getPendingUncached().iterator();\n       // Add pending uncached blocks from effective capacity\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n-        BlockInfo info \u003d\n+        BlockInfoContiguous info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n           pendingBytes +\u003d info.getNumBytes();\n         }\n       }\n       long pendingCapacity \u003d pendingBytes + datanode.getCacheRemaining();\n       if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n         LOG.trace(\"Block {}: DataNode {} is not a valid possibility \" +\n             \"because the block has size {}, but the DataNode only has {}\" +\n             \"bytes of cache remaining ({} pending bytes, {} already cached.\",\n             blockInfo.getBlockId(), datanode.getDatanodeUuid(),\n             blockInfo.getNumBytes(), pendingCapacity, pendingBytes,\n             datanode.getCacheRemaining());\n         outOfCapacity++;\n         continue;\n       }\n       possibilities.add(datanode);\n     }\n     List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n         neededCached, blockManager.getDatanodeManager().getStaleInterval());\n     for (DatanodeDescriptor datanode : chosen) {\n       LOG.trace(\"Block {}: added to PENDING_CACHED on DataNode {}\",\n           blockInfo.getBlockId(), datanode.getDatanodeUuid());\n       pendingCached.add(datanode);\n       boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n       assert added;\n     }\n     // We were unable to satisfy the requested replication factor\n     if (neededCached \u003e chosen.size()) {\n       LOG.debug(\"Block {}: we only have {} of {} cached replicas.\"\n               + \" {} DataNodes have insufficient cache capacity.\",\n           blockInfo.getBlockId(),\n           (cachedBlock.getReplication() - neededCached + chosen.size()),\n           cachedBlock.getReplication(), outOfCapacity\n       );\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addNewPendingCached(final int neededCached,\n      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n      List\u003cDatanodeDescriptor\u003e pendingCached) {\n    // To figure out which replicas can be cached, we consult the\n    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n    BlockInfoContiguous blockInfo \u003d blockManager.\n          getStoredBlock(new Block(cachedBlock.getBlockId()));\n    if (blockInfo \u003d\u003d null) {\n      LOG.debug(\"Block {}: can\u0027t add new cached replicas,\" +\n          \" because there is no record of this block \" +\n          \"on the NameNode.\", cachedBlock.getBlockId());\n      return;\n    }\n    if (!blockInfo.isComplete()) {\n      LOG.debug(\"Block {}: can\u0027t cache this block, because it is not yet\"\n          + \" complete.\", cachedBlock.getBlockId());\n      return;\n    }\n    // Filter the list of replicas to only the valid targets\n    List\u003cDatanodeDescriptor\u003e possibilities \u003d\n        new LinkedList\u003cDatanodeDescriptor\u003e();\n    int numReplicas \u003d blockInfo.getCapacity();\n    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n        blockManager.getCorruptReplicas(blockInfo);\n    int outOfCapacity \u003d 0;\n    for (int i \u003d 0; i \u003c numReplicas; i++) {\n      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n      if (datanode \u003d\u003d null) {\n        continue;\n      }\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue;\n      }\n      if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n        continue;\n      }\n      if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n        continue;\n      }\n      long pendingBytes \u003d 0;\n      // Subtract pending cached blocks from effective capacity\n      Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfoContiguous info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingBytes -\u003d info.getNumBytes();\n        }\n      }\n      it \u003d datanode.getPendingUncached().iterator();\n      // Add pending uncached blocks from effective capacity\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfoContiguous info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingBytes +\u003d info.getNumBytes();\n        }\n      }\n      long pendingCapacity \u003d pendingBytes + datanode.getCacheRemaining();\n      if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n        LOG.trace(\"Block {}: DataNode {} is not a valid possibility \" +\n            \"because the block has size {}, but the DataNode only has {}\" +\n            \"bytes of cache remaining ({} pending bytes, {} already cached.\",\n            blockInfo.getBlockId(), datanode.getDatanodeUuid(),\n            blockInfo.getNumBytes(), pendingCapacity, pendingBytes,\n            datanode.getCacheRemaining());\n        outOfCapacity++;\n        continue;\n      }\n      possibilities.add(datanode);\n    }\n    List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n        neededCached, blockManager.getDatanodeManager().getStaleInterval());\n    for (DatanodeDescriptor datanode : chosen) {\n      LOG.trace(\"Block {}: added to PENDING_CACHED on DataNode {}\",\n          blockInfo.getBlockId(), datanode.getDatanodeUuid());\n      pendingCached.add(datanode);\n      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n      assert added;\n    }\n    // We were unable to satisfy the requested replication factor\n    if (neededCached \u003e chosen.size()) {\n      LOG.debug(\"Block {}: we only have {} of {} cached replicas.\"\n              + \" {} DataNodes have insufficient cache capacity.\",\n          blockInfo.getBlockId(),\n          (cachedBlock.getReplication() - neededCached + chosen.size()),\n          cachedBlock.getReplication(), outOfCapacity\n      );\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "93e23a99157c30b51752fc49748c3c210745a187": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6613. Improve logging in caching classes. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1607697 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/07/14 10:13 AM",
      "commitName": "93e23a99157c30b51752fc49748c3c210745a187",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "10/03/14 11:24 PM",
      "commitNameOld": "bab90b2222abb41d0e3382a92c2f9a8dc568f0e0",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 114.45,
      "commitsBetweenForRepo": 724,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,92 @@\n   private void addNewPendingCached(final int neededCached,\n       CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n       List\u003cDatanodeDescriptor\u003e pendingCached) {\n     // To figure out which replicas can be cached, we consult the\n     // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n     BlockInfo blockInfo \u003d blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo \u003d\u003d null) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Block \" + cachedBlock.getBlockId() + \": can\u0027t add new \" +\n-            \"cached replicas, because there is no record of this block \" +\n-            \"on the NameNode.\");\n-      }\n+      LOG.debug(\"Block {}: can\u0027t add new cached replicas,\" +\n+          \" because there is no record of this block \" +\n+          \"on the NameNode.\", cachedBlock.getBlockId());\n       return;\n     }\n     if (!blockInfo.isComplete()) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Block \" + cachedBlock.getBlockId() + \": can\u0027t cache this \" +\n-            \"block, because it is not yet complete.\");\n-      }\n+      LOG.debug(\"Block {}: can\u0027t cache this block, because it is not yet\"\n+          + \" complete.\", cachedBlock.getBlockId());\n       return;\n     }\n     // Filter the list of replicas to only the valid targets\n     List\u003cDatanodeDescriptor\u003e possibilities \u003d\n         new LinkedList\u003cDatanodeDescriptor\u003e();\n     int numReplicas \u003d blockInfo.getCapacity();\n     Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n         blockManager.getCorruptReplicas(blockInfo);\n     int outOfCapacity \u003d 0;\n     for (int i \u003d 0; i \u003c numReplicas; i++) {\n       DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n       if (datanode \u003d\u003d null) {\n         continue;\n       }\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue;\n       }\n       if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n         continue;\n       }\n       if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n         continue;\n       }\n-      long pendingCapacity \u003d datanode.getCacheRemaining();\n+      long pendingBytes \u003d 0;\n       // Subtract pending cached blocks from effective capacity\n       Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n         BlockInfo info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n-          pendingCapacity -\u003d info.getNumBytes();\n+          pendingBytes -\u003d info.getNumBytes();\n         }\n       }\n       it \u003d datanode.getPendingUncached().iterator();\n       // Add pending uncached blocks from effective capacity\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n         BlockInfo info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n-          pendingCapacity +\u003d info.getNumBytes();\n+          pendingBytes +\u003d info.getNumBytes();\n         }\n       }\n+      long pendingCapacity \u003d pendingBytes + datanode.getCacheRemaining();\n       if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n-        if (LOG.isTraceEnabled()) {\n-          LOG.trace(\"Block \" + blockInfo.getBlockId() + \": DataNode \" +\n-              datanode.getDatanodeUuid() + \" is not a valid possibility \" +\n-              \"because the block has size \" + blockInfo.getNumBytes() + \", but \" +\n-              \"the DataNode only has \" + datanode.getCacheRemaining() + \" \" +\n-              \"bytes of cache remaining.\");\n-        }\n+        LOG.trace(\"Block {}: DataNode {} is not a valid possibility \" +\n+            \"because the block has size {}, but the DataNode only has {}\" +\n+            \"bytes of cache remaining ({} pending bytes, {} already cached.\",\n+            blockInfo.getBlockId(), datanode.getDatanodeUuid(),\n+            blockInfo.getNumBytes(), pendingCapacity, pendingBytes,\n+            datanode.getCacheRemaining());\n         outOfCapacity++;\n         continue;\n       }\n       possibilities.add(datanode);\n     }\n     List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n         neededCached, blockManager.getDatanodeManager().getStaleInterval());\n     for (DatanodeDescriptor datanode : chosen) {\n-      if (LOG.isTraceEnabled()) {\n-          LOG.trace(\"Block \" + blockInfo.getBlockId() + \": added to \" +\n-              \"PENDING_CACHED on DataNode \" + datanode.getDatanodeUuid());\n-      }\n+      LOG.trace(\"Block {}: added to PENDING_CACHED on DataNode {}\",\n+          blockInfo.getBlockId(), datanode.getDatanodeUuid());\n       pendingCached.add(datanode);\n       boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n       assert added;\n     }\n     // We were unable to satisfy the requested replication factor\n     if (neededCached \u003e chosen.size()) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Block \" + blockInfo.getBlockId() + \": we only have \" +\n-            (cachedBlock.getReplication() - neededCached + chosen.size()) +\n-            \" of \" + cachedBlock.getReplication() + \" cached replicas.  \" +\n-            outOfCapacity + \" DataNodes have insufficient cache capacity.\");\n-      }\n+      LOG.debug(\"Block {}: we only have {} of {} cached replicas.\"\n+              + \" {} DataNodes have insufficient cache capacity.\",\n+          blockInfo.getBlockId(),\n+          (cachedBlock.getReplication() - neededCached + chosen.size()),\n+          cachedBlock.getReplication(), outOfCapacity\n+      );\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addNewPendingCached(final int neededCached,\n      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n      List\u003cDatanodeDescriptor\u003e pendingCached) {\n    // To figure out which replicas can be cached, we consult the\n    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n    BlockInfo blockInfo \u003d blockManager.\n          getStoredBlock(new Block(cachedBlock.getBlockId()));\n    if (blockInfo \u003d\u003d null) {\n      LOG.debug(\"Block {}: can\u0027t add new cached replicas,\" +\n          \" because there is no record of this block \" +\n          \"on the NameNode.\", cachedBlock.getBlockId());\n      return;\n    }\n    if (!blockInfo.isComplete()) {\n      LOG.debug(\"Block {}: can\u0027t cache this block, because it is not yet\"\n          + \" complete.\", cachedBlock.getBlockId());\n      return;\n    }\n    // Filter the list of replicas to only the valid targets\n    List\u003cDatanodeDescriptor\u003e possibilities \u003d\n        new LinkedList\u003cDatanodeDescriptor\u003e();\n    int numReplicas \u003d blockInfo.getCapacity();\n    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n        blockManager.getCorruptReplicas(blockInfo);\n    int outOfCapacity \u003d 0;\n    for (int i \u003d 0; i \u003c numReplicas; i++) {\n      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n      if (datanode \u003d\u003d null) {\n        continue;\n      }\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue;\n      }\n      if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n        continue;\n      }\n      if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n        continue;\n      }\n      long pendingBytes \u003d 0;\n      // Subtract pending cached blocks from effective capacity\n      Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingBytes -\u003d info.getNumBytes();\n        }\n      }\n      it \u003d datanode.getPendingUncached().iterator();\n      // Add pending uncached blocks from effective capacity\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingBytes +\u003d info.getNumBytes();\n        }\n      }\n      long pendingCapacity \u003d pendingBytes + datanode.getCacheRemaining();\n      if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n        LOG.trace(\"Block {}: DataNode {} is not a valid possibility \" +\n            \"because the block has size {}, but the DataNode only has {}\" +\n            \"bytes of cache remaining ({} pending bytes, {} already cached.\",\n            blockInfo.getBlockId(), datanode.getDatanodeUuid(),\n            blockInfo.getNumBytes(), pendingCapacity, pendingBytes,\n            datanode.getCacheRemaining());\n        outOfCapacity++;\n        continue;\n      }\n      possibilities.add(datanode);\n    }\n    List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n        neededCached, blockManager.getDatanodeManager().getStaleInterval());\n    for (DatanodeDescriptor datanode : chosen) {\n      LOG.trace(\"Block {}: added to PENDING_CACHED on DataNode {}\",\n          blockInfo.getBlockId(), datanode.getDatanodeUuid());\n      pendingCached.add(datanode);\n      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n      assert added;\n    }\n    // We were unable to satisfy the requested replication factor\n    if (neededCached \u003e chosen.size()) {\n      LOG.debug(\"Block {}: we only have {} of {} cached replicas.\"\n              + \" {} DataNodes have insufficient cache capacity.\",\n          blockInfo.getBlockId(),\n          (cachedBlock.getReplication() - neededCached + chosen.size()),\n          cachedBlock.getReplication(), outOfCapacity\n      );\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "bab90b2222abb41d0e3382a92c2f9a8dc568f0e0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6085. Improve CacheReplicationMonitor log messages a bit (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1576194 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/03/14 11:24 PM",
      "commitName": "bab90b2222abb41d0e3382a92c2f9a8dc568f0e0",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "07/01/14 12:52 PM",
      "commitNameOld": "70cff9e2f0c8f78c1dc54a064182971bb2106795",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 62.4,
      "commitsBetweenForRepo": 529,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,98 @@\n   private void addNewPendingCached(final int neededCached,\n       CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n       List\u003cDatanodeDescriptor\u003e pendingCached) {\n     // To figure out which replicas can be cached, we consult the\n     // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n     BlockInfo blockInfo \u003d blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo \u003d\u003d null) {\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Not caching block \" + cachedBlock + \" because there \" +\n-            \"is no record of it on the NameNode.\");\n+        LOG.debug(\"Block \" + cachedBlock.getBlockId() + \": can\u0027t add new \" +\n+            \"cached replicas, because there is no record of this block \" +\n+            \"on the NameNode.\");\n       }\n       return;\n     }\n     if (!blockInfo.isComplete()) {\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n-            \"is not yet complete.\");\n+        LOG.debug(\"Block \" + cachedBlock.getBlockId() + \": can\u0027t cache this \" +\n+            \"block, because it is not yet complete.\");\n       }\n       return;\n     }\n     // Filter the list of replicas to only the valid targets\n     List\u003cDatanodeDescriptor\u003e possibilities \u003d\n         new LinkedList\u003cDatanodeDescriptor\u003e();\n     int numReplicas \u003d blockInfo.getCapacity();\n     Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n         blockManager.getCorruptReplicas(blockInfo);\n     int outOfCapacity \u003d 0;\n     for (int i \u003d 0; i \u003c numReplicas; i++) {\n       DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n       if (datanode \u003d\u003d null) {\n         continue;\n       }\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue;\n       }\n       if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n         continue;\n       }\n       if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n         continue;\n       }\n       long pendingCapacity \u003d datanode.getCacheRemaining();\n       // Subtract pending cached blocks from effective capacity\n       Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n         BlockInfo info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n           pendingCapacity -\u003d info.getNumBytes();\n         }\n       }\n       it \u003d datanode.getPendingUncached().iterator();\n       // Add pending uncached blocks from effective capacity\n       while (it.hasNext()) {\n         CachedBlock cBlock \u003d it.next();\n         BlockInfo info \u003d\n             blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n         if (info !\u003d null) {\n           pendingCapacity +\u003d info.getNumBytes();\n         }\n       }\n       if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n         if (LOG.isTraceEnabled()) {\n-          LOG.trace(\"Datanode \" + datanode + \" is not a valid possibility for\"\n-              + \" block \" + blockInfo.getBlockId() + \" of size \"\n-              + blockInfo.getNumBytes() + \" bytes, only has \"\n-              + datanode.getCacheRemaining() + \" bytes of cache remaining.\");\n+          LOG.trace(\"Block \" + blockInfo.getBlockId() + \": DataNode \" +\n+              datanode.getDatanodeUuid() + \" is not a valid possibility \" +\n+              \"because the block has size \" + blockInfo.getNumBytes() + \", but \" +\n+              \"the DataNode only has \" + datanode.getCacheRemaining() + \" \" +\n+              \"bytes of cache remaining.\");\n         }\n         outOfCapacity++;\n         continue;\n       }\n       possibilities.add(datanode);\n     }\n     List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n         neededCached, blockManager.getDatanodeManager().getStaleInterval());\n     for (DatanodeDescriptor datanode : chosen) {\n+      if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"Block \" + blockInfo.getBlockId() + \": added to \" +\n+              \"PENDING_CACHED on DataNode \" + datanode.getDatanodeUuid());\n+      }\n       pendingCached.add(datanode);\n       boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n       assert added;\n     }\n     // We were unable to satisfy the requested replication factor\n     if (neededCached \u003e chosen.size()) {\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\n-            \"Only have \" +\n+        LOG.debug(\"Block \" + blockInfo.getBlockId() + \": we only have \" +\n             (cachedBlock.getReplication() - neededCached + chosen.size()) +\n-            \" of \" + cachedBlock.getReplication() + \" cached replicas for \" +\n-            cachedBlock + \" (\" + outOfCapacity + \" nodes have insufficient \" +\n-            \"capacity).\");\n+            \" of \" + cachedBlock.getReplication() + \" cached replicas.  \" +\n+            outOfCapacity + \" DataNodes have insufficient cache capacity.\");\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addNewPendingCached(final int neededCached,\n      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n      List\u003cDatanodeDescriptor\u003e pendingCached) {\n    // To figure out which replicas can be cached, we consult the\n    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n    BlockInfo blockInfo \u003d blockManager.\n          getStoredBlock(new Block(cachedBlock.getBlockId()));\n    if (blockInfo \u003d\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block \" + cachedBlock.getBlockId() + \": can\u0027t add new \" +\n            \"cached replicas, because there is no record of this block \" +\n            \"on the NameNode.\");\n      }\n      return;\n    }\n    if (!blockInfo.isComplete()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block \" + cachedBlock.getBlockId() + \": can\u0027t cache this \" +\n            \"block, because it is not yet complete.\");\n      }\n      return;\n    }\n    // Filter the list of replicas to only the valid targets\n    List\u003cDatanodeDescriptor\u003e possibilities \u003d\n        new LinkedList\u003cDatanodeDescriptor\u003e();\n    int numReplicas \u003d blockInfo.getCapacity();\n    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n        blockManager.getCorruptReplicas(blockInfo);\n    int outOfCapacity \u003d 0;\n    for (int i \u003d 0; i \u003c numReplicas; i++) {\n      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n      if (datanode \u003d\u003d null) {\n        continue;\n      }\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue;\n      }\n      if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n        continue;\n      }\n      if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n        continue;\n      }\n      long pendingCapacity \u003d datanode.getCacheRemaining();\n      // Subtract pending cached blocks from effective capacity\n      Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingCapacity -\u003d info.getNumBytes();\n        }\n      }\n      it \u003d datanode.getPendingUncached().iterator();\n      // Add pending uncached blocks from effective capacity\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingCapacity +\u003d info.getNumBytes();\n        }\n      }\n      if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Block \" + blockInfo.getBlockId() + \": DataNode \" +\n              datanode.getDatanodeUuid() + \" is not a valid possibility \" +\n              \"because the block has size \" + blockInfo.getNumBytes() + \", but \" +\n              \"the DataNode only has \" + datanode.getCacheRemaining() + \" \" +\n              \"bytes of cache remaining.\");\n        }\n        outOfCapacity++;\n        continue;\n      }\n      possibilities.add(datanode);\n    }\n    List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n        neededCached, blockManager.getDatanodeManager().getStaleInterval());\n    for (DatanodeDescriptor datanode : chosen) {\n      if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Block \" + blockInfo.getBlockId() + \": added to \" +\n              \"PENDING_CACHED on DataNode \" + datanode.getDatanodeUuid());\n      }\n      pendingCached.add(datanode);\n      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n      assert added;\n    }\n    // We were unable to satisfy the requested replication factor\n    if (neededCached \u003e chosen.size()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block \" + blockInfo.getBlockId() + \": we only have \" +\n            (cachedBlock.getReplication() - neededCached + chosen.size()) +\n            \" of \" + cachedBlock.getReplication() + \" cached replicas.  \" +\n            outOfCapacity + \" DataNodes have insufficient cache capacity.\");\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "8deb7a60575ad33b78a5167673276275ba7bece5": {
      "type": "Ymultichange(Ybodychange,Yparametermetachange)",
      "commitMessage": "HDFS-5589. Namenode loops caching and uncaching when data should be uncached. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1555996 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/01/14 11:45 AM",
      "commitName": "8deb7a60575ad33b78a5167673276275ba7bece5",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5589. Namenode loops caching and uncaching when data should be uncached. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1555996 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/01/14 11:45 AM",
          "commitName": "8deb7a60575ad33b78a5167673276275ba7bece5",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "02/01/14 6:45 PM",
          "commitNameOld": "d85c017d0488930d806f267141057fc73e68c728",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 3.71,
          "commitsBetweenForRepo": 11,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,52 +1,94 @@\n-  private void addNewPendingCached(int neededCached,\n+  private void addNewPendingCached(final int neededCached,\n       CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n       List\u003cDatanodeDescriptor\u003e pendingCached) {\n     // To figure out which replicas can be cached, we consult the\n     // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n     BlockInfo blockInfo \u003d blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo \u003d\u003d null) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Not caching block \" + cachedBlock + \" because there \" +\n             \"is no record of it on the NameNode.\");\n       }\n       return;\n     }\n     if (!blockInfo.isComplete()) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n             \"is not yet complete.\");\n       }\n       return;\n     }\n-    List\u003cDatanodeDescriptor\u003e possibilities \u003d new LinkedList\u003cDatanodeDescriptor\u003e();\n+    // Filter the list of replicas to only the valid targets\n+    List\u003cDatanodeDescriptor\u003e possibilities \u003d\n+        new LinkedList\u003cDatanodeDescriptor\u003e();\n     int numReplicas \u003d blockInfo.getCapacity();\n     Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n         blockManager.getCorruptReplicas(blockInfo);\n+    int outOfCapacity \u003d 0;\n     for (int i \u003d 0; i \u003c numReplicas; i++) {\n       DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n-      if ((datanode !\u003d null) \u0026\u0026 \n-          ((!pendingCached.contains(datanode)) \u0026\u0026\n-          ((corrupt \u003d\u003d null) || (!corrupt.contains(datanode))))) {\n-        possibilities.add(datanode);\n+      if (datanode \u003d\u003d null) {\n+        continue;\n       }\n+      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n+        continue;\n+      }\n+      if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n+        continue;\n+      }\n+      if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n+        continue;\n+      }\n+      long pendingCapacity \u003d datanode.getCacheRemaining();\n+      // Subtract pending cached blocks from effective capacity\n+      Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n+      while (it.hasNext()) {\n+        CachedBlock cBlock \u003d it.next();\n+        BlockInfo info \u003d\n+            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n+        if (info !\u003d null) {\n+          pendingCapacity -\u003d info.getNumBytes();\n+        }\n+      }\n+      it \u003d datanode.getPendingUncached().iterator();\n+      // Add pending uncached blocks from effective capacity\n+      while (it.hasNext()) {\n+        CachedBlock cBlock \u003d it.next();\n+        BlockInfo info \u003d\n+            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n+        if (info !\u003d null) {\n+          pendingCapacity +\u003d info.getNumBytes();\n+        }\n+      }\n+      if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"Datanode \" + datanode + \" is not a valid possibility for\"\n+              + \" block \" + blockInfo.getBlockId() + \" of size \"\n+              + blockInfo.getNumBytes() + \" bytes, only has \"\n+              + datanode.getCacheRemaining() + \" bytes of cache remaining.\");\n+        }\n+        outOfCapacity++;\n+        continue;\n+      }\n+      possibilities.add(datanode);\n     }\n-    while (neededCached \u003e 0) {\n-      if (possibilities.isEmpty()) {\n-        LOG.warn(\"We need \" + neededCached + \" more replica(s) than \" +\n-            \"actually exist to provide a cache replication of \" +\n-            cachedBlock.getReplication() + \" for \" + cachedBlock);\n-        return;\n-      }\n-      DatanodeDescriptor datanode \u003d\n-          possibilities.remove(random.nextInt(possibilities.size()));\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"AddNewPendingCached: datanode \" + datanode + \n-            \" will now cache block \" + cachedBlock);\n-      }\n+    List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n+        neededCached, blockManager.getDatanodeManager().getStaleInterval());\n+    for (DatanodeDescriptor datanode : chosen) {\n       pendingCached.add(datanode);\n       boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n       assert added;\n-      neededCached--;\n+    }\n+    // We were unable to satisfy the requested replication factor\n+    if (neededCached \u003e chosen.size()) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\n+            \"Only have \" +\n+            (cachedBlock.getReplication() - neededCached + chosen.size()) +\n+            \" of \" + cachedBlock.getReplication() + \" cached replicas for \" +\n+            cachedBlock + \" (\" + outOfCapacity + \" nodes have insufficient \" +\n+            \"capacity).\");\n+      }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void addNewPendingCached(final int neededCached,\n      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n      List\u003cDatanodeDescriptor\u003e pendingCached) {\n    // To figure out which replicas can be cached, we consult the\n    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n    BlockInfo blockInfo \u003d blockManager.\n          getStoredBlock(new Block(cachedBlock.getBlockId()));\n    if (blockInfo \u003d\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Not caching block \" + cachedBlock + \" because there \" +\n            \"is no record of it on the NameNode.\");\n      }\n      return;\n    }\n    if (!blockInfo.isComplete()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n            \"is not yet complete.\");\n      }\n      return;\n    }\n    // Filter the list of replicas to only the valid targets\n    List\u003cDatanodeDescriptor\u003e possibilities \u003d\n        new LinkedList\u003cDatanodeDescriptor\u003e();\n    int numReplicas \u003d blockInfo.getCapacity();\n    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n        blockManager.getCorruptReplicas(blockInfo);\n    int outOfCapacity \u003d 0;\n    for (int i \u003d 0; i \u003c numReplicas; i++) {\n      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n      if (datanode \u003d\u003d null) {\n        continue;\n      }\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue;\n      }\n      if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n        continue;\n      }\n      if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n        continue;\n      }\n      long pendingCapacity \u003d datanode.getCacheRemaining();\n      // Subtract pending cached blocks from effective capacity\n      Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingCapacity -\u003d info.getNumBytes();\n        }\n      }\n      it \u003d datanode.getPendingUncached().iterator();\n      // Add pending uncached blocks from effective capacity\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingCapacity +\u003d info.getNumBytes();\n        }\n      }\n      if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Datanode \" + datanode + \" is not a valid possibility for\"\n              + \" block \" + blockInfo.getBlockId() + \" of size \"\n              + blockInfo.getNumBytes() + \" bytes, only has \"\n              + datanode.getCacheRemaining() + \" bytes of cache remaining.\");\n        }\n        outOfCapacity++;\n        continue;\n      }\n      possibilities.add(datanode);\n    }\n    List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n        neededCached, blockManager.getDatanodeManager().getStaleInterval());\n    for (DatanodeDescriptor datanode : chosen) {\n      pendingCached.add(datanode);\n      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n      assert added;\n    }\n    // We were unable to satisfy the requested replication factor\n    if (neededCached \u003e chosen.size()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\n            \"Only have \" +\n            (cachedBlock.getReplication() - neededCached + chosen.size()) +\n            \" of \" + cachedBlock.getReplication() + \" cached replicas for \" +\n            cachedBlock + \" (\" + outOfCapacity + \" nodes have insufficient \" +\n            \"capacity).\");\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HDFS-5589. Namenode loops caching and uncaching when data should be uncached. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1555996 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/01/14 11:45 AM",
          "commitName": "8deb7a60575ad33b78a5167673276275ba7bece5",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "02/01/14 6:45 PM",
          "commitNameOld": "d85c017d0488930d806f267141057fc73e68c728",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 3.71,
          "commitsBetweenForRepo": 11,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,52 +1,94 @@\n-  private void addNewPendingCached(int neededCached,\n+  private void addNewPendingCached(final int neededCached,\n       CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n       List\u003cDatanodeDescriptor\u003e pendingCached) {\n     // To figure out which replicas can be cached, we consult the\n     // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n     BlockInfo blockInfo \u003d blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo \u003d\u003d null) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Not caching block \" + cachedBlock + \" because there \" +\n             \"is no record of it on the NameNode.\");\n       }\n       return;\n     }\n     if (!blockInfo.isComplete()) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n             \"is not yet complete.\");\n       }\n       return;\n     }\n-    List\u003cDatanodeDescriptor\u003e possibilities \u003d new LinkedList\u003cDatanodeDescriptor\u003e();\n+    // Filter the list of replicas to only the valid targets\n+    List\u003cDatanodeDescriptor\u003e possibilities \u003d\n+        new LinkedList\u003cDatanodeDescriptor\u003e();\n     int numReplicas \u003d blockInfo.getCapacity();\n     Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n         blockManager.getCorruptReplicas(blockInfo);\n+    int outOfCapacity \u003d 0;\n     for (int i \u003d 0; i \u003c numReplicas; i++) {\n       DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n-      if ((datanode !\u003d null) \u0026\u0026 \n-          ((!pendingCached.contains(datanode)) \u0026\u0026\n-          ((corrupt \u003d\u003d null) || (!corrupt.contains(datanode))))) {\n-        possibilities.add(datanode);\n+      if (datanode \u003d\u003d null) {\n+        continue;\n       }\n+      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n+        continue;\n+      }\n+      if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n+        continue;\n+      }\n+      if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n+        continue;\n+      }\n+      long pendingCapacity \u003d datanode.getCacheRemaining();\n+      // Subtract pending cached blocks from effective capacity\n+      Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n+      while (it.hasNext()) {\n+        CachedBlock cBlock \u003d it.next();\n+        BlockInfo info \u003d\n+            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n+        if (info !\u003d null) {\n+          pendingCapacity -\u003d info.getNumBytes();\n+        }\n+      }\n+      it \u003d datanode.getPendingUncached().iterator();\n+      // Add pending uncached blocks from effective capacity\n+      while (it.hasNext()) {\n+        CachedBlock cBlock \u003d it.next();\n+        BlockInfo info \u003d\n+            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n+        if (info !\u003d null) {\n+          pendingCapacity +\u003d info.getNumBytes();\n+        }\n+      }\n+      if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"Datanode \" + datanode + \" is not a valid possibility for\"\n+              + \" block \" + blockInfo.getBlockId() + \" of size \"\n+              + blockInfo.getNumBytes() + \" bytes, only has \"\n+              + datanode.getCacheRemaining() + \" bytes of cache remaining.\");\n+        }\n+        outOfCapacity++;\n+        continue;\n+      }\n+      possibilities.add(datanode);\n     }\n-    while (neededCached \u003e 0) {\n-      if (possibilities.isEmpty()) {\n-        LOG.warn(\"We need \" + neededCached + \" more replica(s) than \" +\n-            \"actually exist to provide a cache replication of \" +\n-            cachedBlock.getReplication() + \" for \" + cachedBlock);\n-        return;\n-      }\n-      DatanodeDescriptor datanode \u003d\n-          possibilities.remove(random.nextInt(possibilities.size()));\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"AddNewPendingCached: datanode \" + datanode + \n-            \" will now cache block \" + cachedBlock);\n-      }\n+    List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n+        neededCached, blockManager.getDatanodeManager().getStaleInterval());\n+    for (DatanodeDescriptor datanode : chosen) {\n       pendingCached.add(datanode);\n       boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n       assert added;\n-      neededCached--;\n+    }\n+    // We were unable to satisfy the requested replication factor\n+    if (neededCached \u003e chosen.size()) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\n+            \"Only have \" +\n+            (cachedBlock.getReplication() - neededCached + chosen.size()) +\n+            \" of \" + cachedBlock.getReplication() + \" cached replicas for \" +\n+            cachedBlock + \" (\" + outOfCapacity + \" nodes have insufficient \" +\n+            \"capacity).\");\n+      }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void addNewPendingCached(final int neededCached,\n      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n      List\u003cDatanodeDescriptor\u003e pendingCached) {\n    // To figure out which replicas can be cached, we consult the\n    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n    BlockInfo blockInfo \u003d blockManager.\n          getStoredBlock(new Block(cachedBlock.getBlockId()));\n    if (blockInfo \u003d\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Not caching block \" + cachedBlock + \" because there \" +\n            \"is no record of it on the NameNode.\");\n      }\n      return;\n    }\n    if (!blockInfo.isComplete()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n            \"is not yet complete.\");\n      }\n      return;\n    }\n    // Filter the list of replicas to only the valid targets\n    List\u003cDatanodeDescriptor\u003e possibilities \u003d\n        new LinkedList\u003cDatanodeDescriptor\u003e();\n    int numReplicas \u003d blockInfo.getCapacity();\n    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n        blockManager.getCorruptReplicas(blockInfo);\n    int outOfCapacity \u003d 0;\n    for (int i \u003d 0; i \u003c numReplicas; i++) {\n      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n      if (datanode \u003d\u003d null) {\n        continue;\n      }\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue;\n      }\n      if (corrupt !\u003d null \u0026\u0026 corrupt.contains(datanode)) {\n        continue;\n      }\n      if (pendingCached.contains(datanode) || cached.contains(datanode)) {\n        continue;\n      }\n      long pendingCapacity \u003d datanode.getCacheRemaining();\n      // Subtract pending cached blocks from effective capacity\n      Iterator\u003cCachedBlock\u003e it \u003d datanode.getPendingCached().iterator();\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingCapacity -\u003d info.getNumBytes();\n        }\n      }\n      it \u003d datanode.getPendingUncached().iterator();\n      // Add pending uncached blocks from effective capacity\n      while (it.hasNext()) {\n        CachedBlock cBlock \u003d it.next();\n        BlockInfo info \u003d\n            blockManager.getStoredBlock(new Block(cBlock.getBlockId()));\n        if (info !\u003d null) {\n          pendingCapacity +\u003d info.getNumBytes();\n        }\n      }\n      if (pendingCapacity \u003c blockInfo.getNumBytes()) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Datanode \" + datanode + \" is not a valid possibility for\"\n              + \" block \" + blockInfo.getBlockId() + \" of size \"\n              + blockInfo.getNumBytes() + \" bytes, only has \"\n              + datanode.getCacheRemaining() + \" bytes of cache remaining.\");\n        }\n        outOfCapacity++;\n        continue;\n      }\n      possibilities.add(datanode);\n    }\n    List\u003cDatanodeDescriptor\u003e chosen \u003d chooseDatanodesForCaching(possibilities,\n        neededCached, blockManager.getDatanodeManager().getStaleInterval());\n    for (DatanodeDescriptor datanode : chosen) {\n      pendingCached.add(datanode);\n      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n      assert added;\n    }\n    // We were unable to satisfy the requested replication factor\n    if (neededCached \u003e chosen.size()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\n            \"Only have \" +\n            (cachedBlock.getReplication() - neededCached + chosen.size()) +\n            \" of \" + cachedBlock.getReplication() + \" cached replicas for \" +\n            cachedBlock + \" (\" + outOfCapacity + \" nodes have insufficient \" +\n            \"capacity).\");\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
          "extendedDetails": {
            "oldValue": "[neededCached-int, cachedBlock-CachedBlock, cached-List\u003cDatanodeDescriptor\u003e, pendingCached-List\u003cDatanodeDescriptor\u003e]",
            "newValue": "[neededCached-int(modifiers-final), cachedBlock-CachedBlock, cached-List\u003cDatanodeDescriptor\u003e, pendingCached-List\u003cDatanodeDescriptor\u003e]"
          }
        }
      ]
    },
    "d85c017d0488930d806f267141057fc73e68c728": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5651. Remove dfs.namenode.caching.enabled and improve CRM locking. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1555002 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/01/14 6:45 PM",
      "commitName": "d85c017d0488930d806f267141057fc73e68c728",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "31/12/13 4:01 PM",
      "commitNameOld": "07e4fb1455abc33584fc666ef745abe256ebd7d1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.11,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,52 @@\n   private void addNewPendingCached(int neededCached,\n       CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n       List\u003cDatanodeDescriptor\u003e pendingCached) {\n-    if (!cacheManager.isActive()) {\n-      return;\n-    }\n     // To figure out which replicas can be cached, we consult the\n     // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n     BlockInfo blockInfo \u003d blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo \u003d\u003d null) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Not caching block \" + cachedBlock + \" because there \" +\n             \"is no record of it on the NameNode.\");\n       }\n       return;\n     }\n     if (!blockInfo.isComplete()) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n             \"is not yet complete.\");\n       }\n       return;\n     }\n     List\u003cDatanodeDescriptor\u003e possibilities \u003d new LinkedList\u003cDatanodeDescriptor\u003e();\n     int numReplicas \u003d blockInfo.getCapacity();\n     Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n         blockManager.getCorruptReplicas(blockInfo);\n     for (int i \u003d 0; i \u003c numReplicas; i++) {\n       DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n       if ((datanode !\u003d null) \u0026\u0026 \n           ((!pendingCached.contains(datanode)) \u0026\u0026\n           ((corrupt \u003d\u003d null) || (!corrupt.contains(datanode))))) {\n         possibilities.add(datanode);\n       }\n     }\n     while (neededCached \u003e 0) {\n       if (possibilities.isEmpty()) {\n         LOG.warn(\"We need \" + neededCached + \" more replica(s) than \" +\n             \"actually exist to provide a cache replication of \" +\n             cachedBlock.getReplication() + \" for \" + cachedBlock);\n         return;\n       }\n       DatanodeDescriptor datanode \u003d\n           possibilities.remove(random.nextInt(possibilities.size()));\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"AddNewPendingCached: datanode \" + datanode + \n             \" will now cache block \" + cachedBlock);\n       }\n       pendingCached.add(datanode);\n       boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n       assert added;\n       neededCached--;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addNewPendingCached(int neededCached,\n      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n      List\u003cDatanodeDescriptor\u003e pendingCached) {\n    // To figure out which replicas can be cached, we consult the\n    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n    BlockInfo blockInfo \u003d blockManager.\n          getStoredBlock(new Block(cachedBlock.getBlockId()));\n    if (blockInfo \u003d\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Not caching block \" + cachedBlock + \" because there \" +\n            \"is no record of it on the NameNode.\");\n      }\n      return;\n    }\n    if (!blockInfo.isComplete()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n            \"is not yet complete.\");\n      }\n      return;\n    }\n    List\u003cDatanodeDescriptor\u003e possibilities \u003d new LinkedList\u003cDatanodeDescriptor\u003e();\n    int numReplicas \u003d blockInfo.getCapacity();\n    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n        blockManager.getCorruptReplicas(blockInfo);\n    for (int i \u003d 0; i \u003c numReplicas; i++) {\n      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n      if ((datanode !\u003d null) \u0026\u0026 \n          ((!pendingCached.contains(datanode)) \u0026\u0026\n          ((corrupt \u003d\u003d null) || (!corrupt.contains(datanode))))) {\n        possibilities.add(datanode);\n      }\n    }\n    while (neededCached \u003e 0) {\n      if (possibilities.isEmpty()) {\n        LOG.warn(\"We need \" + neededCached + \" more replica(s) than \" +\n            \"actually exist to provide a cache replication of \" +\n            cachedBlock.getReplication() + \" for \" + cachedBlock);\n        return;\n      }\n      DatanodeDescriptor datanode \u003d\n          possibilities.remove(random.nextInt(possibilities.size()));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"AddNewPendingCached: datanode \" + datanode + \n            \" will now cache block \" + cachedBlock);\n      }\n      pendingCached.add(datanode);\n      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n      assert added;\n      neededCached--;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "07e4fb1455abc33584fc666ef745abe256ebd7d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5708. The CacheManager throws a NPE in the DataNode logs when processing cache reports that refer to a block not known to the BlockManager. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1554594 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/12/13 4:01 PM",
      "commitName": "07e4fb1455abc33584fc666ef745abe256ebd7d1",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "20/12/13 3:27 PM",
      "commitNameOld": "b9ae3087c0f83bfeeea47ded8e19932b46fd2350",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 11.02,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,55 @@\n   private void addNewPendingCached(int neededCached,\n       CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n       List\u003cDatanodeDescriptor\u003e pendingCached) {\n     if (!cacheManager.isActive()) {\n       return;\n     }\n     // To figure out which replicas can be cached, we consult the\n     // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n     BlockInfo blockInfo \u003d blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo \u003d\u003d null) {\n-      LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n-          \"was deleted from all DataNodes.\");\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Not caching block \" + cachedBlock + \" because there \" +\n+            \"is no record of it on the NameNode.\");\n+      }\n       return;\n     }\n     if (!blockInfo.isComplete()) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n             \"is not yet complete.\");\n       }\n       return;\n     }\n     List\u003cDatanodeDescriptor\u003e possibilities \u003d new LinkedList\u003cDatanodeDescriptor\u003e();\n     int numReplicas \u003d blockInfo.getCapacity();\n     Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n         blockManager.getCorruptReplicas(blockInfo);\n     for (int i \u003d 0; i \u003c numReplicas; i++) {\n       DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n       if ((datanode !\u003d null) \u0026\u0026 \n           ((!pendingCached.contains(datanode)) \u0026\u0026\n           ((corrupt \u003d\u003d null) || (!corrupt.contains(datanode))))) {\n         possibilities.add(datanode);\n       }\n     }\n     while (neededCached \u003e 0) {\n       if (possibilities.isEmpty()) {\n         LOG.warn(\"We need \" + neededCached + \" more replica(s) than \" +\n             \"actually exist to provide a cache replication of \" +\n             cachedBlock.getReplication() + \" for \" + cachedBlock);\n         return;\n       }\n       DatanodeDescriptor datanode \u003d\n           possibilities.remove(random.nextInt(possibilities.size()));\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"AddNewPendingCached: datanode \" + datanode + \n             \" will now cache block \" + cachedBlock);\n       }\n       pendingCached.add(datanode);\n       boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n       assert added;\n       neededCached--;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addNewPendingCached(int neededCached,\n      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n      List\u003cDatanodeDescriptor\u003e pendingCached) {\n    if (!cacheManager.isActive()) {\n      return;\n    }\n    // To figure out which replicas can be cached, we consult the\n    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n    BlockInfo blockInfo \u003d blockManager.\n          getStoredBlock(new Block(cachedBlock.getBlockId()));\n    if (blockInfo \u003d\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Not caching block \" + cachedBlock + \" because there \" +\n            \"is no record of it on the NameNode.\");\n      }\n      return;\n    }\n    if (!blockInfo.isComplete()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n            \"is not yet complete.\");\n      }\n      return;\n    }\n    List\u003cDatanodeDescriptor\u003e possibilities \u003d new LinkedList\u003cDatanodeDescriptor\u003e();\n    int numReplicas \u003d blockInfo.getCapacity();\n    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n        blockManager.getCorruptReplicas(blockInfo);\n    for (int i \u003d 0; i \u003c numReplicas; i++) {\n      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n      if ((datanode !\u003d null) \u0026\u0026 \n          ((!pendingCached.contains(datanode)) \u0026\u0026\n          ((corrupt \u003d\u003d null) || (!corrupt.contains(datanode))))) {\n        possibilities.add(datanode);\n      }\n    }\n    while (neededCached \u003e 0) {\n      if (possibilities.isEmpty()) {\n        LOG.warn(\"We need \" + neededCached + \" more replica(s) than \" +\n            \"actually exist to provide a cache replication of \" +\n            cachedBlock.getReplication() + \" for \" + cachedBlock);\n        return;\n      }\n      DatanodeDescriptor datanode \u003d\n          possibilities.remove(random.nextInt(possibilities.size()));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"AddNewPendingCached: datanode \" + datanode + \n            \" will now cache block \" + cachedBlock);\n      }\n      pendingCached.add(datanode);\n      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n      assert added;\n      neededCached--;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 3:15 PM",
      "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,53 @@\n+  private void addNewPendingCached(int neededCached,\n+      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n+      List\u003cDatanodeDescriptor\u003e pendingCached) {\n+    if (!cacheManager.isActive()) {\n+      return;\n+    }\n+    // To figure out which replicas can be cached, we consult the\n+    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n+    BlockInfo blockInfo \u003d blockManager.\n+          getStoredBlock(new Block(cachedBlock.getBlockId()));\n+    if (blockInfo \u003d\u003d null) {\n+      LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n+          \"was deleted from all DataNodes.\");\n+      return;\n+    }\n+    if (!blockInfo.isComplete()) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n+            \"is not yet complete.\");\n+      }\n+      return;\n+    }\n+    List\u003cDatanodeDescriptor\u003e possibilities \u003d new LinkedList\u003cDatanodeDescriptor\u003e();\n+    int numReplicas \u003d blockInfo.getCapacity();\n+    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n+        blockManager.getCorruptReplicas(blockInfo);\n+    for (int i \u003d 0; i \u003c numReplicas; i++) {\n+      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n+      if ((datanode !\u003d null) \u0026\u0026 \n+          ((!pendingCached.contains(datanode)) \u0026\u0026\n+          ((corrupt \u003d\u003d null) || (!corrupt.contains(datanode))))) {\n+        possibilities.add(datanode);\n+      }\n+    }\n+    while (neededCached \u003e 0) {\n+      if (possibilities.isEmpty()) {\n+        LOG.warn(\"We need \" + neededCached + \" more replica(s) than \" +\n+            \"actually exist to provide a cache replication of \" +\n+            cachedBlock.getReplication() + \" for \" + cachedBlock);\n+        return;\n+      }\n+      DatanodeDescriptor datanode \u003d\n+          possibilities.remove(random.nextInt(possibilities.size()));\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"AddNewPendingCached: datanode \" + datanode + \n+            \" will now cache block \" + cachedBlock);\n+      }\n+      pendingCached.add(datanode);\n+      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n+      assert added;\n+      neededCached--;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void addNewPendingCached(int neededCached,\n      CachedBlock cachedBlock, List\u003cDatanodeDescriptor\u003e cached,\n      List\u003cDatanodeDescriptor\u003e pendingCached) {\n    if (!cacheManager.isActive()) {\n      return;\n    }\n    // To figure out which replicas can be cached, we consult the\n    // blocksMap.  We don\u0027t want to try to cache a corrupt replica, though.\n    BlockInfo blockInfo \u003d blockManager.\n          getStoredBlock(new Block(cachedBlock.getBlockId()));\n    if (blockInfo \u003d\u003d null) {\n      LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n          \"was deleted from all DataNodes.\");\n      return;\n    }\n    if (!blockInfo.isComplete()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n            \"is not yet complete.\");\n      }\n      return;\n    }\n    List\u003cDatanodeDescriptor\u003e possibilities \u003d new LinkedList\u003cDatanodeDescriptor\u003e();\n    int numReplicas \u003d blockInfo.getCapacity();\n    Collection\u003cDatanodeDescriptor\u003e corrupt \u003d\n        blockManager.getCorruptReplicas(blockInfo);\n    for (int i \u003d 0; i \u003c numReplicas; i++) {\n      DatanodeDescriptor datanode \u003d blockInfo.getDatanode(i);\n      if ((datanode !\u003d null) \u0026\u0026 \n          ((!pendingCached.contains(datanode)) \u0026\u0026\n          ((corrupt \u003d\u003d null) || (!corrupt.contains(datanode))))) {\n        possibilities.add(datanode);\n      }\n    }\n    while (neededCached \u003e 0) {\n      if (possibilities.isEmpty()) {\n        LOG.warn(\"We need \" + neededCached + \" more replica(s) than \" +\n            \"actually exist to provide a cache replication of \" +\n            cachedBlock.getReplication() + \" for \" + cachedBlock);\n        return;\n      }\n      DatanodeDescriptor datanode \u003d\n          possibilities.remove(random.nextInt(possibilities.size()));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"AddNewPendingCached: datanode \" + datanode + \n            \" will now cache block \" + cachedBlock);\n      }\n      pendingCached.add(datanode);\n      boolean added \u003d datanode.getPendingCached().add(cachedBlock);\n      assert added;\n      neededCached--;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java"
    }
  }
}