{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BPServiceActor.java",
  "functionName": "blockReport",
  "functionId": "blockReport___fullBrLeaseId-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
  "functionStartLine": 364,
  "functionEndLine": 455,
  "numCommitsSeen": 234,
  "timeTaken": 13179,
  "changeHistory": [
    "528378784fe14e7069dd0471f3c4c478544b57c8",
    "460a94a10f9c314b77a25e14efbf7c4dc3f5d9aa",
    "1168ece59640d8ad3166e355d2e82deec2fbaf14",
    "8179f9a493c1b26deb6b1bffacd6a829586b7f98",
    "c4463f2ef20d2cb634a1249246f83c451975f3dc",
    "85a20508bd04851d47c24b7562ec2927d5403446",
    "2a0082c51da7cbe2770eddb5f72cd7f8d72fa5f6",
    "4e5e1c0f9938e51699c0437731e7b2eef699d6da",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
    "12b5b06c063d93e6c683c9b6fac9a96912f59e59",
    "dfc1c4c303cf15afc6c3361ed9d3238562f73cbd",
    "60882ab26d49f05cbf0686944af6559f86b3417d",
    "50ee8f4e67a66aa77c5359182f61f3e951844db6",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "d324164a51a43d72c02567248bd9f0f12b244a40",
    "7e2d9a32426d04b5f08c2835f61882b053612a20",
    "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
    "a1aa1836fb6831c25efe326cdfdc014370cf5957",
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8",
    "4551da302d94cffea0313eac79479ab6f9b7cb34",
    "9e108e61fb28244326d7cf4bb31d175eb75d2636",
    "846f97312c6db7b84b7401174acd0fc943baa093",
    "1e346aa829519f8a2aa830e76d9856f914861805",
    "39ce694d05c6d8c428bd87bc1b9c95f94dfdf6fd",
    "1f92266516c882e43fa453b876dd8ca09893c477",
    "905a127850d5e0cba85c2e075f989fa0f5cf129a",
    "0864ef19089f703232107d8aa26c4a7571ff132e",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "710e5a960e8af1d4c73e386041096aacfee8b828",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "528378784fe14e7069dd0471f3c4c478544b57c8": "Ybodychange",
    "460a94a10f9c314b77a25e14efbf7c4dc3f5d9aa": "Ybodychange",
    "1168ece59640d8ad3166e355d2e82deec2fbaf14": "Ybodychange",
    "8179f9a493c1b26deb6b1bffacd6a829586b7f98": "Ybodychange",
    "c4463f2ef20d2cb634a1249246f83c451975f3dc": "Ybodychange",
    "85a20508bd04851d47c24b7562ec2927d5403446": "Ybodychange",
    "2a0082c51da7cbe2770eddb5f72cd7f8d72fa5f6": "Ybodychange",
    "4e5e1c0f9938e51699c0437731e7b2eef699d6da": "Ybodychange",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": "Ybodychange",
    "12b5b06c063d93e6c683c9b6fac9a96912f59e59": "Ymultichange(Yparameterchange,Ybodychange)",
    "dfc1c4c303cf15afc6c3361ed9d3238562f73cbd": "Ybodychange",
    "60882ab26d49f05cbf0686944af6559f86b3417d": "Ybodychange",
    "50ee8f4e67a66aa77c5359182f61f3e951844db6": "Ybodychange",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "d324164a51a43d72c02567248bd9f0f12b244a40": "Ybodychange",
    "7e2d9a32426d04b5f08c2835f61882b053612a20": "Ybodychange",
    "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14": "Ymultichange(Yreturntypechange,Ybodychange)",
    "a1aa1836fb6831c25efe326cdfdc014370cf5957": "Ybodychange",
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8": "Ybodychange",
    "4551da302d94cffea0313eac79479ab6f9b7cb34": "Ybodychange",
    "9e108e61fb28244326d7cf4bb31d175eb75d2636": "Ybodychange",
    "846f97312c6db7b84b7401174acd0fc943baa093": "Ybodychange",
    "1e346aa829519f8a2aa830e76d9856f914861805": "Ymultichange(Ymovefromfile,Ybodychange)",
    "39ce694d05c6d8c428bd87bc1b9c95f94dfdf6fd": "Ymultichange(Ymovefromfile,Ybodychange)",
    "1f92266516c882e43fa453b876dd8ca09893c477": "Ybodychange",
    "905a127850d5e0cba85c2e075f989fa0f5cf129a": "Ybodychange",
    "0864ef19089f703232107d8aa26c4a7571ff132e": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "710e5a960e8af1d4c73e386041096aacfee8b828": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "528378784fe14e7069dd0471f3c4c478544b57c8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12012. Fix spelling mistakes in BPServiceActor.java. Contributed by chencan.\n",
      "commitDate": "17/08/19 4:37 AM",
      "commitName": "528378784fe14e7069dd0471f3c4c478544b57c8",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "05/05/19 4:03 AM",
      "commitNameOld": "69b903bbd8e2dafac6b2cb1d748ea666b6f877cf",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 104.02,
      "commitsBetweenForRepo": 830,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,92 @@\n   List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     ibrManager.sendIBRs(bpNamenode, bpRegistration,\n         bpos.getBlockPoolId(), getRpcMetricSuffix());\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     boolean useBlocksBuffer \u003d\n         bpRegistration.getNamespaceInfo().isCapabilitySupported(\n             NamespaceInfo.Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n     blockReportSizes.clear();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n               new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n         blockReportSizes.add(\n             calculateBlockReportPBSize(useBlocksBuffer, reports));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n           StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport,\n               new BlockReportContext(reports.length, r, reportId,\n                   fullBrLeaseId, true));\n           blockReportSizes.add(\n               calculateBlockReportPBSize(useBlocksBuffer, singleReport));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost, getRpcMetricSuffix());\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n-          \" msec to generate and \" + brSendCost +\n+          \" msecs to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduler.updateLastBlockReportTime(monotonicNow());\n     scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    ibrManager.sendIBRs(bpNamenode, bpRegistration,\n        bpos.getBlockPoolId(), getRpcMetricSuffix());\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    boolean useBlocksBuffer \u003d\n        bpRegistration.getNamespaceInfo().isCapabilitySupported(\n            NamespaceInfo.Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n    blockReportSizes.clear();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n        blockReportSizes.add(\n            calculateBlockReportPBSize(useBlocksBuffer, reports));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId,\n                  fullBrLeaseId, true));\n          blockReportSizes.add(\n              calculateBlockReportPBSize(useBlocksBuffer, singleReport));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost, getRpcMetricSuffix());\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msecs to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.updateLastBlockReportTime(monotonicNow());\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "460a94a10f9c314b77a25e14efbf7c4dc3f5d9aa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14045. Use different metrics in DataNode to better measure latency of heartbeat/blockReports/incrementalBlockReports of Active/Standby NN. Contributed by Jiandan Yang.\n",
      "commitDate": "15/11/18 10:58 AM",
      "commitName": "460a94a10f9c314b77a25e14efbf7c4dc3f5d9aa",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "16/08/18 4:29 PM",
      "commitNameOld": "1290e3c647092f0bfbb250731a6805aba1be8e4b",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 90.81,
      "commitsBetweenForRepo": 804,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,92 @@\n   List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     ibrManager.sendIBRs(bpNamenode, bpRegistration,\n-        bpos.getBlockPoolId());\n+        bpos.getBlockPoolId(), getRpcMetricSuffix());\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     boolean useBlocksBuffer \u003d\n         bpRegistration.getNamespaceInfo().isCapabilitySupported(\n             NamespaceInfo.Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n     blockReportSizes.clear();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n               new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n         blockReportSizes.add(\n             calculateBlockReportPBSize(useBlocksBuffer, reports));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n           StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport,\n               new BlockReportContext(reports.length, r, reportId,\n                   fullBrLeaseId, true));\n           blockReportSizes.add(\n               calculateBlockReportPBSize(useBlocksBuffer, singleReport));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n-      dn.getMetrics().addBlockReport(brSendCost);\n+      dn.getMetrics().addBlockReport(brSendCost, getRpcMetricSuffix());\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduler.updateLastBlockReportTime(monotonicNow());\n     scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    ibrManager.sendIBRs(bpNamenode, bpRegistration,\n        bpos.getBlockPoolId(), getRpcMetricSuffix());\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    boolean useBlocksBuffer \u003d\n        bpRegistration.getNamespaceInfo().isCapabilitySupported(\n            NamespaceInfo.Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n    blockReportSizes.clear();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n        blockReportSizes.add(\n            calculateBlockReportPBSize(useBlocksBuffer, reports));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId,\n                  fullBrLeaseId, true));\n          blockReportSizes.add(\n              calculateBlockReportPBSize(useBlocksBuffer, singleReport));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost, getRpcMetricSuffix());\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.updateLastBlockReportTime(monotonicNow());\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "1168ece59640d8ad3166e355d2e82deec2fbaf14": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11534. Add counters for number of blocks in pending IBR. Contributed by Xiaobing Zhou.\n",
      "commitDate": "24/03/17 2:33 PM",
      "commitName": "1168ece59640d8ad3166e355d2e82deec2fbaf14",
      "commitAuthor": "Xiaobing Zhou",
      "commitDateOld": "20/03/17 9:54 PM",
      "commitNameOld": "e7c8da614c37e36fb8081234f4c639d6054f6082",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 3.69,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,92 @@\n   List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     ibrManager.sendIBRs(bpNamenode, bpRegistration,\n-        bpos.getBlockPoolId(), dn.getMetrics());\n+        bpos.getBlockPoolId());\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     boolean useBlocksBuffer \u003d\n         bpRegistration.getNamespaceInfo().isCapabilitySupported(\n             NamespaceInfo.Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n     blockReportSizes.clear();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n               new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n         blockReportSizes.add(\n             calculateBlockReportPBSize(useBlocksBuffer, reports));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n           StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport,\n               new BlockReportContext(reports.length, r, reportId,\n                   fullBrLeaseId, true));\n           blockReportSizes.add(\n               calculateBlockReportPBSize(useBlocksBuffer, singleReport));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduler.updateLastBlockReportTime(monotonicNow());\n     scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    ibrManager.sendIBRs(bpNamenode, bpRegistration,\n        bpos.getBlockPoolId());\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    boolean useBlocksBuffer \u003d\n        bpRegistration.getNamespaceInfo().isCapabilitySupported(\n            NamespaceInfo.Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n    blockReportSizes.clear();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n        blockReportSizes.add(\n            calculateBlockReportPBSize(useBlocksBuffer, reports));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId,\n                  fullBrLeaseId, true));\n          blockReportSizes.add(\n              calculateBlockReportPBSize(useBlocksBuffer, singleReport));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.updateLastBlockReportTime(monotonicNow());\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "8179f9a493c1b26deb6b1bffacd6a829586b7f98": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10645. Make block report size as a metric and add this metric to datanode web ui. Contributed by Yuanbo Liu.\n",
      "commitDate": "19/08/16 12:15 AM",
      "commitName": "8179f9a493c1b26deb6b1bffacd6a829586b7f98",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "01/08/16 10:34 PM",
      "commitNameOld": "c4463f2ef20d2cb634a1249246f83c451975f3dc",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 17.07,
      "commitsBetweenForRepo": 139,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,92 @@\n   List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     ibrManager.sendIBRs(bpNamenode, bpRegistration,\n         bpos.getBlockPoolId(), dn.getMetrics());\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n+    boolean useBlocksBuffer \u003d\n+        bpRegistration.getNamespaceInfo().isCapabilitySupported(\n+            NamespaceInfo.Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n+    blockReportSizes.clear();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n               new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n+        blockReportSizes.add(\n+            calculateBlockReportPBSize(useBlocksBuffer, reports));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n           StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport,\n               new BlockReportContext(reports.length, r, reportId,\n                   fullBrLeaseId, true));\n+          blockReportSizes.add(\n+              calculateBlockReportPBSize(useBlocksBuffer, singleReport));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduler.updateLastBlockReportTime(monotonicNow());\n     scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    ibrManager.sendIBRs(bpNamenode, bpRegistration,\n        bpos.getBlockPoolId(), dn.getMetrics());\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    boolean useBlocksBuffer \u003d\n        bpRegistration.getNamespaceInfo().isCapabilitySupported(\n            NamespaceInfo.Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n    blockReportSizes.clear();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n        blockReportSizes.add(\n            calculateBlockReportPBSize(useBlocksBuffer, reports));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId,\n                  fullBrLeaseId, true));\n          blockReportSizes.add(\n              calculateBlockReportPBSize(useBlocksBuffer, singleReport));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.updateLastBlockReportTime(monotonicNow());\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "c4463f2ef20d2cb634a1249246f83c451975f3dc": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-10301. Interleaving processing of storages from repeated block reports causes false zombie storage detection, removes valid blocks. Contributed by Vinitha Gankidi.\"\n\nThis reverts commit 85a20508bd04851d47c24b7562ec2927d5403446.\n",
      "commitDate": "01/08/16 10:34 PM",
      "commitName": "c4463f2ef20d2cb634a1249246f83c451975f3dc",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "25/07/16 6:50 PM",
      "commitNameOld": "85a20508bd04851d47c24b7562ec2927d5403446",
      "commitAuthorOld": "Vinitha Reddy Gankidi",
      "daysBetweenCommits": 7.16,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,84 @@\n   List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     ibrManager.sendIBRs(bpNamenode, bpRegistration,\n         bpos.getBlockPoolId(), dn.getMetrics());\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n               new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n-          StorageBlockReport[] singleReport \u003d {reports[r]};\n-          DatanodeCommand cmd;\n-          if (r !\u003d reports.length - 1) {\n-            cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(),\n-                singleReport, new BlockReportContext(reports.length, r,\n-                    reportId, fullBrLeaseId, true));\n-          } else {\n-            StorageBlockReport[] lastSplitReport \u003d\n-                new StorageBlockReport[perVolumeBlockLists.size()];\n-            // When block reports are split, the last RPC in the block report\n-            // has the information about all storages in the block report.\n-            // See HDFS-10301 for more details. To achieve this, the last RPC\n-            // has \u0027n\u0027 storage reports, where \u0027n\u0027 is the number of storages in\n-            // a DN. The actual block replicas are reported only for the\n-            // last/n-th storage.\n-            i \u003d 0;\n-            for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair :\n-                perVolumeBlockLists.entrySet()) {\n-              lastSplitReport[i++] \u003d new StorageBlockReport(\n-                  kvPair.getKey(), BlockListAsLongs.STORAGE_REPORT);\n-              if (i \u003d\u003d r) {\n-                lastSplitReport[i] \u003d reports[r];\n-                break;\n-              }\n-            }\n-            cmd \u003d bpNamenode.blockReport(\n-                bpRegistration, bpos.getBlockPoolId(), lastSplitReport,\n-                new BlockReportContext(reports.length, r, reportId,\n-                    fullBrLeaseId, true));\n-          }\n+          StorageBlockReport singleReport[] \u003d { reports[r] };\n+          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n+              bpRegistration, bpos.getBlockPoolId(), singleReport,\n+              new BlockReportContext(reports.length, r, reportId,\n+                  fullBrLeaseId, true));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduler.updateLastBlockReportTime(monotonicNow());\n     scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    ibrManager.sendIBRs(bpNamenode, bpRegistration,\n        bpos.getBlockPoolId(), dn.getMetrics());\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId,\n                  fullBrLeaseId, true));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.updateLastBlockReportTime(monotonicNow());\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "85a20508bd04851d47c24b7562ec2927d5403446": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10301. Interleaving processing of storages from repeated block reports causes false zombie storage detection, removes valid blocks. Contributed by Vinitha Gankidi.",
      "commitDate": "25/07/16 6:50 PM",
      "commitName": "85a20508bd04851d47c24b7562ec2927d5403446",
      "commitAuthor": "Vinitha Reddy Gankidi",
      "commitDateOld": "25/07/16 6:41 PM",
      "commitNameOld": "0cde9e12a7175e4d8bc4ccd5c36055b280d1fbd6",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,109 @@\n   List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     ibrManager.sendIBRs(bpNamenode, bpRegistration,\n         bpos.getBlockPoolId(), dn.getMetrics());\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n               new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n-          StorageBlockReport singleReport[] \u003d { reports[r] };\n-          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n-              bpRegistration, bpos.getBlockPoolId(), singleReport,\n-              new BlockReportContext(reports.length, r, reportId,\n-                  fullBrLeaseId, true));\n+          StorageBlockReport[] singleReport \u003d {reports[r]};\n+          DatanodeCommand cmd;\n+          if (r !\u003d reports.length - 1) {\n+            cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(),\n+                singleReport, new BlockReportContext(reports.length, r,\n+                    reportId, fullBrLeaseId, true));\n+          } else {\n+            StorageBlockReport[] lastSplitReport \u003d\n+                new StorageBlockReport[perVolumeBlockLists.size()];\n+            // When block reports are split, the last RPC in the block report\n+            // has the information about all storages in the block report.\n+            // See HDFS-10301 for more details. To achieve this, the last RPC\n+            // has \u0027n\u0027 storage reports, where \u0027n\u0027 is the number of storages in\n+            // a DN. The actual block replicas are reported only for the\n+            // last/n-th storage.\n+            i \u003d 0;\n+            for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair :\n+                perVolumeBlockLists.entrySet()) {\n+              lastSplitReport[i++] \u003d new StorageBlockReport(\n+                  kvPair.getKey(), BlockListAsLongs.STORAGE_REPORT);\n+              if (i \u003d\u003d r) {\n+                lastSplitReport[i] \u003d reports[r];\n+                break;\n+              }\n+            }\n+            cmd \u003d bpNamenode.blockReport(\n+                bpRegistration, bpos.getBlockPoolId(), lastSplitReport,\n+                new BlockReportContext(reports.length, r, reportId,\n+                    fullBrLeaseId, true));\n+          }\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduler.updateLastBlockReportTime(monotonicNow());\n     scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    ibrManager.sendIBRs(bpNamenode, bpRegistration,\n        bpos.getBlockPoolId(), dn.getMetrics());\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport[] singleReport \u003d {reports[r]};\n          DatanodeCommand cmd;\n          if (r !\u003d reports.length - 1) {\n            cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(),\n                singleReport, new BlockReportContext(reports.length, r,\n                    reportId, fullBrLeaseId, true));\n          } else {\n            StorageBlockReport[] lastSplitReport \u003d\n                new StorageBlockReport[perVolumeBlockLists.size()];\n            // When block reports are split, the last RPC in the block report\n            // has the information about all storages in the block report.\n            // See HDFS-10301 for more details. To achieve this, the last RPC\n            // has \u0027n\u0027 storage reports, where \u0027n\u0027 is the number of storages in\n            // a DN. The actual block replicas are reported only for the\n            // last/n-th storage.\n            i \u003d 0;\n            for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair :\n                perVolumeBlockLists.entrySet()) {\n              lastSplitReport[i++] \u003d new StorageBlockReport(\n                  kvPair.getKey(), BlockListAsLongs.STORAGE_REPORT);\n              if (i \u003d\u003d r) {\n                lastSplitReport[i] \u003d reports[r];\n                break;\n              }\n            }\n            cmd \u003d bpNamenode.blockReport(\n                bpRegistration, bpos.getBlockPoolId(), lastSplitReport,\n                new BlockReportContext(reports.length, r, reportId,\n                    fullBrLeaseId, true));\n          }\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.updateLastBlockReportTime(monotonicNow());\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "2a0082c51da7cbe2770eddb5f72cd7f8d72fa5f6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10440. Improve DataNode web UI (Contributed by Weiwei Yang)\n",
      "commitDate": "28/06/16 4:19 AM",
      "commitName": "2a0082c51da7cbe2770eddb5f72cd7f8d72fa5f6",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "04/04/16 6:49 PM",
      "commitNameOld": "818d6b799eead13a17a0214172df60a269b046fb",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 84.4,
      "commitsBetweenForRepo": 582,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,84 @@\n   List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     ibrManager.sendIBRs(bpNamenode, bpRegistration,\n         bpos.getBlockPoolId(), dn.getMetrics());\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n               new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n           StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport,\n               new BlockReportContext(reports.length, r, reportId,\n                   fullBrLeaseId, true));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n+    scheduler.updateLastBlockReportTime(monotonicNow());\n     scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    ibrManager.sendIBRs(bpNamenode, bpRegistration,\n        bpos.getBlockPoolId(), dn.getMetrics());\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId,\n                  fullBrLeaseId, true));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.updateLastBlockReportTime(monotonicNow());\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "4e5e1c0f9938e51699c0437731e7b2eef699d6da": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9726. Refactor IBR code to a new class.\n",
      "commitDate": "05/02/16 7:17 AM",
      "commitName": "4e5e1c0f9938e51699c0437731e7b2eef699d6da",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "02/02/16 11:23 AM",
      "commitNameOld": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 2.83,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,83 @@\n   List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n-    reportReceivedDeletedBlocks();\n+    ibrManager.sendIBRs(bpNamenode, bpRegistration,\n+        bpos.getBlockPoolId(), dn.getMetrics());\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n               new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n           StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport,\n               new BlockReportContext(reports.length, r, reportId,\n                   fullBrLeaseId, true));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    ibrManager.sendIBRs(bpNamenode, bpRegistration,\n        bpos.getBlockPoolId(), dn.getMetrics());\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId,\n                  fullBrLeaseId, true));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9260. Improve the performance and GC friendliness of NameNode startup and full block reports (Staffan Friberg via cmccabe)\n",
      "commitDate": "02/02/16 11:23 AM",
      "commitName": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "01/12/15 1:32 PM",
      "commitNameOld": "58f6f54eeac779428ac995d196b60ffb90563f97",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 62.91,
      "commitsBetweenForRepo": 375,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,82 @@\n   List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     reportReceivedDeletedBlocks();\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n-              new BlockReportContext(1, 0, reportId, fullBrLeaseId));\n+              new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n           StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport,\n               new BlockReportContext(reports.length, r, reportId,\n-                  fullBrLeaseId));\n+                  fullBrLeaseId, true));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId, fullBrLeaseId, true));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId,\n                  fullBrLeaseId, true));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "12b5b06c063d93e6c683c9b6fac9a96912f59e59": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7923. The DataNodes should rate-limit their full block reports by asking the NN on heartbeat messages (cmccabe)\n",
      "commitDate": "12/06/15 11:17 AM",
      "commitName": "12b5b06c063d93e6c683c9b6fac9a96912f59e59",
      "commitAuthor": "Colin Patrick Mccabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7923. The DataNodes should rate-limit their full block reports by asking the NN on heartbeat messages (cmccabe)\n",
          "commitDate": "12/06/15 11:17 AM",
          "commitName": "12b5b06c063d93e6c683c9b6fac9a96912f59e59",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "19/05/15 10:50 AM",
          "commitNameOld": "470c87dbc6c24dd3b370f1ad9e7ab1f6dabd2080",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 24.02,
          "commitsBetweenForRepo": 173,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,86 +1,82 @@\n-  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n-    // send block report if timer has expired.\n-    if (!scheduler.isBlockReportDue()) {\n-      return null;\n-    }\n-\n+  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     reportReceivedDeletedBlocks();\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n-              new BlockReportContext(1, 0, reportId));\n+              new BlockReportContext(1, 0, reportId, fullBrLeaseId));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n           StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport,\n-              new BlockReportContext(reports.length, r, reportId));\n+              new BlockReportContext(reports.length, r, reportId,\n+                  fullBrLeaseId));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId, fullBrLeaseId));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId,\n                  fullBrLeaseId));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[fullBrLeaseId-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7923. The DataNodes should rate-limit their full block reports by asking the NN on heartbeat messages (cmccabe)\n",
          "commitDate": "12/06/15 11:17 AM",
          "commitName": "12b5b06c063d93e6c683c9b6fac9a96912f59e59",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "19/05/15 10:50 AM",
          "commitNameOld": "470c87dbc6c24dd3b370f1ad9e7ab1f6dabd2080",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 24.02,
          "commitsBetweenForRepo": 173,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,86 +1,82 @@\n-  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n-    // send block report if timer has expired.\n-    if (!scheduler.isBlockReportDue()) {\n-      return null;\n-    }\n-\n+  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     reportReceivedDeletedBlocks();\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n-              new BlockReportContext(1, 0, reportId));\n+              new BlockReportContext(1, 0, reportId, fullBrLeaseId));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n           StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport,\n-              new BlockReportContext(reports.length, r, reportId));\n+              new BlockReportContext(reports.length, r, reportId,\n+                  fullBrLeaseId));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cDatanodeCommand\u003e blockReport(long fullBrLeaseId) throws IOException {\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId, fullBrLeaseId));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId,\n                  fullBrLeaseId));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
          "extendedDetails": {}
        }
      ]
    },
    "dfc1c4c303cf15afc6c3361ed9d3238562f73cbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8163. Using monotonicNow for block report scheduling causes test failures on recently restarted systems. (Arpit Agarwal)\n",
      "commitDate": "21/04/15 10:58 AM",
      "commitName": "dfc1c4c303cf15afc6c3361ed9d3238562f73cbd",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "08/04/15 9:43 PM",
      "commitNameOld": "b1e059089d6a5b2b7006d7d384c6df81ed268bd9",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 12.55,
      "commitsBetweenForRepo": 85,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,86 @@\n   List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n     // send block report if timer has expired.\n-    final long startTime \u003d monotonicNow();\n-    if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n+    if (!scheduler.isBlockReportDue()) {\n       return null;\n     }\n \n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     reportReceivedDeletedBlocks();\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n               new BlockReportContext(1, 0, reportId));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n           StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport,\n               new BlockReportContext(reports.length, r, reportId));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n-    scheduleNextBlockReport(startTime);\n+    scheduler.scheduleNextBlockReport();\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n    // send block report if timer has expired.\n    if (!scheduler.isBlockReportDue()) {\n      return null;\n    }\n\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduler.scheduleNextBlockReport();\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "60882ab26d49f05cbf0686944af6559f86b3417d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7990. IBR delete ack should not be delayed. Contributed by Daryn Sharp.\n",
      "commitDate": "27/03/15 7:05 AM",
      "commitName": "60882ab26d49f05cbf0686944af6559f86b3417d",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "23/03/15 10:00 PM",
      "commitNameOld": "50ee8f4e67a66aa77c5359182f61f3e951844db6",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 3.38,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,88 +1,87 @@\n   List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n     // send block report if timer has expired.\n     final long startTime \u003d monotonicNow();\n     if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n       return null;\n     }\n \n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     reportReceivedDeletedBlocks();\n-    lastDeletedReport \u003d startTime;\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n     long reportId \u003d generateUniqueBlockReportId();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports,\n               new BlockReportContext(1, 0, reportId));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (int r \u003d 0; r \u003c reports.length; r++) {\n           StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport,\n               new BlockReportContext(reports.length, r, reportId));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent block report 0x\" +\n           Long.toHexString(reportId) + \",  containing \" + reports.length +\n           \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n           \" The reports had \" + totalBlockCount +\n           \" total blocks and used \" + numRPCs +\n           \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduleNextBlockReport(startTime);\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n    // send block report if timer has expired.\n    final long startTime \u003d monotonicNow();\n    if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n      return null;\n    }\n\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduleNextBlockReport(startTime);\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "50ee8f4e67a66aa77c5359182f61f3e951844db6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7960. The full block report should prune zombie storages even if they\u0027re not empty. Contributed by Colin McCabe and Eddy Xu.\n",
      "commitDate": "23/03/15 10:00 PM",
      "commitName": "50ee8f4e67a66aa77c5359182f61f3e951844db6",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "20/03/15 12:02 PM",
      "commitNameOld": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 3.42,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,88 @@\n   List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n     // send block report if timer has expired.\n     final long startTime \u003d monotonicNow();\n     if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n       return null;\n     }\n \n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     reportReceivedDeletedBlocks();\n     lastDeletedReport \u003d startTime;\n \n     long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d monotonicNow();\n+    long reportId \u003d generateUniqueBlockReportId();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n-            bpRegistration, bpos.getBlockPoolId(), reports);\n+            bpRegistration, bpos.getBlockPoolId(), reports,\n+              new BlockReportContext(1, 0, reportId));\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n-        for (StorageBlockReport report : reports) {\n-          StorageBlockReport singleReport[] \u003d { report };\n+        for (int r \u003d 0; r \u003c reports.length; r++) {\n+          StorageBlockReport singleReport[] \u003d { reports[r] };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n-              bpRegistration, bpos.getBlockPoolId(), singleReport);\n+              bpRegistration, bpos.getBlockPoolId(), singleReport,\n+              new BlockReportContext(reports.length, r, reportId));\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n-          \"uccessfully sent \" + numReportsSent +\n-          \" of \" + reports.length +\n-          \" blockreports for \" + totalBlockCount +\n-          \" total blocks using \" + numRPCs +\n-          \" RPCs. This took \" + brCreateCost +\n+          \"uccessfully sent block report 0x\" +\n+          Long.toHexString(reportId) + \",  containing \" + reports.length +\n+          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n+          \" The reports had \" + totalBlockCount +\n+          \" total blocks and used \" + numRPCs +\n+          \" RPC(s). This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduleNextBlockReport(startTime);\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n    // send block report if timer has expired.\n    final long startTime \u003d monotonicNow();\n    if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n      return null;\n    }\n\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n    lastDeletedReport \u003d startTime;\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    long reportId \u003d generateUniqueBlockReportId();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports,\n              new BlockReportContext(1, 0, reportId));\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (int r \u003d 0; r \u003c reports.length; r++) {\n          StorageBlockReport singleReport[] \u003d { reports[r] };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent block report 0x\" +\n          Long.toHexString(reportId) + \",  containing \" + reports.length +\n          \" storage report(s), of which we sent \" + numReportsSent + \".\" +\n          \" The reports had \" + totalBlockCount +\n          \" total blocks and used \" + numRPCs +\n          \" RPC(s). This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduleNextBlockReport(startTime);\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "13/03/15 12:23 PM",
      "commitNameOld": "d324164a51a43d72c02567248bd9f0f12b244a40",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 6.99,
      "commitsBetweenForRepo": 79,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,84 @@\n   List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n     // send block report if timer has expired.\n-    final long startTime \u003d now();\n+    final long startTime \u003d monotonicNow();\n     if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n       return null;\n     }\n \n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     reportReceivedDeletedBlocks();\n     lastDeletedReport \u003d startTime;\n \n-    long brCreateStartTime \u003d now();\n+    long brCreateStartTime \u003d monotonicNow();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n-    long brSendStartTime \u003d now();\n+    long brSendStartTime \u003d monotonicNow();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports);\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (StorageBlockReport report : reports) {\n           StorageBlockReport singleReport[] \u003d { report };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport);\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n-      long brSendCost \u003d now() - brSendStartTime;\n+      long brSendCost \u003d monotonicNow() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent \" + numReportsSent +\n           \" of \" + reports.length +\n           \" blockreports for \" + totalBlockCount +\n           \" total blocks using \" + numRPCs +\n           \" RPCs. This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduleNextBlockReport(startTime);\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n    // send block report if timer has expired.\n    final long startTime \u003d monotonicNow();\n    if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n      return null;\n    }\n\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n    lastDeletedReport \u003d startTime;\n\n    long brCreateStartTime \u003d monotonicNow();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d monotonicNow();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports);\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (StorageBlockReport report : reports) {\n          StorageBlockReport singleReport[] \u003d { report };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport);\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d monotonicNow() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent \" + numReportsSent +\n          \" of \" + reports.length +\n          \" blockreports for \" + totalBlockCount +\n          \" total blocks using \" + numRPCs +\n          \" RPCs. This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduleNextBlockReport(startTime);\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "d324164a51a43d72c02567248bd9f0f12b244a40": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7435. PB encoding of block reports is very inefficient. Contributed by Daryn Sharp.\n",
      "commitDate": "13/03/15 12:23 PM",
      "commitName": "d324164a51a43d72c02567248bd9f0f12b244a40",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "11/03/15 2:11 PM",
      "commitNameOld": "fb34f45727e63ea55377fe90241328025307d818",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 1.93,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,84 @@\n   List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n     // send block report if timer has expired.\n     final long startTime \u003d now();\n     if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n       return null;\n     }\n \n     final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     reportReceivedDeletedBlocks();\n     lastDeletedReport \u003d startTime;\n \n     long brCreateStartTime \u003d now();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n-      reports[i++] \u003d new StorageBlockReport(\n-          kvPair.getKey(), blockList.getBlockListAsLongs());\n+      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n     int numReportsSent \u003d 0;\n     int numRPCs \u003d 0;\n     boolean success \u003d false;\n     long brSendStartTime \u003d now();\n     try {\n       if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n         // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n             bpRegistration, bpos.getBlockPoolId(), reports);\n         numRPCs \u003d 1;\n         numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n       } else {\n         // Send one block report per message.\n         for (StorageBlockReport report : reports) {\n           StorageBlockReport singleReport[] \u003d { report };\n           DatanodeCommand cmd \u003d bpNamenode.blockReport(\n               bpRegistration, bpos.getBlockPoolId(), singleReport);\n           numReportsSent++;\n           numRPCs++;\n           if (cmd !\u003d null) {\n             cmds.add(cmd);\n           }\n         }\n       }\n       success \u003d true;\n     } finally {\n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d now() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       final int nCmds \u003d cmds.size();\n       LOG.info((success ? \"S\" : \"Uns\") +\n           \"uccessfully sent \" + numReportsSent +\n           \" of \" + reports.length +\n           \" blockreports for \" + totalBlockCount +\n           \" total blocks using \" + numRPCs +\n           \" RPCs. This took \" + brCreateCost +\n           \" msec to generate and \" + brSendCost +\n           \" msecs for RPC and NN processing.\" +\n           \" Got back \" +\n           ((nCmds \u003d\u003d 0) ? \"no commands\" :\n               ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                   (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n           \".\");\n     }\n     scheduleNextBlockReport(startTime);\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n    // send block report if timer has expired.\n    final long startTime \u003d now();\n    if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n      return null;\n    }\n\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n    lastDeletedReport \u003d startTime;\n\n    long brCreateStartTime \u003d now();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(kvPair.getKey(), blockList);\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d now();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports);\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (StorageBlockReport report : reports) {\n          StorageBlockReport singleReport[] \u003d { report };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport);\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d now() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent \" + numReportsSent +\n          \" of \" + reports.length +\n          \" blockreports for \" + totalBlockCount +\n          \" total blocks using \" + numRPCs +\n          \" RPCs. This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduleNextBlockReport(startTime);\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "7e2d9a32426d04b5f08c2835f61882b053612a20": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7579. Improve log reporting during block report rpc failure. Contributed by Charles Lamb.\n",
      "commitDate": "08/01/15 3:12 PM",
      "commitName": "7e2d9a32426d04b5f08c2835f61882b053612a20",
      "commitAuthor": "cnauroth",
      "commitDateOld": "27/10/14 9:53 AM",
      "commitNameOld": "baf794dc404ac54f4e8332654eadfac1bebacb8f",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 73.26,
      "commitsBetweenForRepo": 528,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,85 @@\n   List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n     // send block report if timer has expired.\n     final long startTime \u003d now();\n     if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n       return null;\n     }\n \n-    ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n+    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n \n     // Flush any block information that precedes the block report. Otherwise\n     // we have a chance that we will miss the delHint information\n     // or we will report an RBW replica after the BlockReport already reports\n     // a FINALIZED one.\n     reportReceivedDeletedBlocks();\n     lastDeletedReport \u003d startTime;\n \n     long brCreateStartTime \u003d now();\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n         dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n     // Convert the reports to the format expected by the NN.\n     int i \u003d 0;\n     int totalBlockCount \u003d 0;\n     StorageBlockReport reports[] \u003d\n         new StorageBlockReport[perVolumeBlockLists.size()];\n \n     for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n       BlockListAsLongs blockList \u003d kvPair.getValue();\n       reports[i++] \u003d new StorageBlockReport(\n           kvPair.getKey(), blockList.getBlockListAsLongs());\n       totalBlockCount +\u003d blockList.getNumberOfBlocks();\n     }\n \n     // Send the reports to the NN.\n-    int numReportsSent;\n+    int numReportsSent \u003d 0;\n+    int numRPCs \u003d 0;\n+    boolean success \u003d false;\n     long brSendStartTime \u003d now();\n-    if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n-      // Below split threshold, send all reports in a single message.\n-      numReportsSent \u003d 1;\n-      DatanodeCommand cmd \u003d\n-          bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), reports);\n-      if (cmd !\u003d null) {\n-        cmds.add(cmd);\n-      }\n-    } else {\n-      // Send one block report per message.\n-      numReportsSent \u003d i;\n-      for (StorageBlockReport report : reports) {\n-        StorageBlockReport singleReport[] \u003d { report };\n+    try {\n+      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n+        // Below split threshold, send all reports in a single message.\n         DatanodeCommand cmd \u003d bpNamenode.blockReport(\n-            bpRegistration, bpos.getBlockPoolId(), singleReport);\n+            bpRegistration, bpos.getBlockPoolId(), reports);\n+        numRPCs \u003d 1;\n+        numReportsSent \u003d reports.length;\n         if (cmd !\u003d null) {\n           cmds.add(cmd);\n         }\n+      } else {\n+        // Send one block report per message.\n+        for (StorageBlockReport report : reports) {\n+          StorageBlockReport singleReport[] \u003d { report };\n+          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n+              bpRegistration, bpos.getBlockPoolId(), singleReport);\n+          numReportsSent++;\n+          numRPCs++;\n+          if (cmd !\u003d null) {\n+            cmds.add(cmd);\n+          }\n+        }\n       }\n+      success \u003d true;\n+    } finally {\n+      // Log the block report processing stats from Datanode perspective\n+      long brSendCost \u003d now() - brSendStartTime;\n+      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n+      dn.getMetrics().addBlockReport(brSendCost);\n+      final int nCmds \u003d cmds.size();\n+      LOG.info((success ? \"S\" : \"Uns\") +\n+          \"uccessfully sent \" + numReportsSent +\n+          \" of \" + reports.length +\n+          \" blockreports for \" + totalBlockCount +\n+          \" total blocks using \" + numRPCs +\n+          \" RPCs. This took \" + brCreateCost +\n+          \" msec to generate and \" + brSendCost +\n+          \" msecs for RPC and NN processing.\" +\n+          \" Got back \" +\n+          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n+              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n+                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n+          \".\");\n     }\n-\n-    // Log the block report processing stats from Datanode perspective\n-    long brSendCost \u003d now() - brSendStartTime;\n-    long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n-    dn.getMetrics().addBlockReport(brSendCost);\n-    LOG.info(\"Sent \" + numReportsSent + \" blockreports \" + totalBlockCount +\n-        \" blocks total. Took \" + brCreateCost +\n-        \" msec to generate and \" + brSendCost +\n-        \" msecs for RPC and NN processing. \" +\n-        \" Got back commands \" +\n-            (cmds.size() \u003d\u003d 0 ? \"none\" : Joiner.on(\"; \").join(cmds)));\n-\n     scheduleNextBlockReport(startTime);\n     return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n    // send block report if timer has expired.\n    final long startTime \u003d now();\n    if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n      return null;\n    }\n\n    final ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n    lastDeletedReport \u003d startTime;\n\n    long brCreateStartTime \u003d now();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(\n          kvPair.getKey(), blockList.getBlockListAsLongs());\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent \u003d 0;\n    int numRPCs \u003d 0;\n    boolean success \u003d false;\n    long brSendStartTime \u003d now();\n    try {\n      if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n        // Below split threshold, send all reports in a single message.\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), reports);\n        numRPCs \u003d 1;\n        numReportsSent \u003d reports.length;\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      } else {\n        // Send one block report per message.\n        for (StorageBlockReport report : reports) {\n          StorageBlockReport singleReport[] \u003d { report };\n          DatanodeCommand cmd \u003d bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport);\n          numReportsSent++;\n          numRPCs++;\n          if (cmd !\u003d null) {\n            cmds.add(cmd);\n          }\n        }\n      }\n      success \u003d true;\n    } finally {\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d now() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      final int nCmds \u003d cmds.size();\n      LOG.info((success ? \"S\" : \"Uns\") +\n          \"uccessfully sent \" + numReportsSent +\n          \" of \" + reports.length +\n          \" blockreports for \" + totalBlockCount +\n          \" total blocks using \" + numRPCs +\n          \" RPCs. This took \" + brCreateCost +\n          \" msec to generate and \" + brSendCost +\n          \" msecs for RPC and NN processing.\" +\n          \" Got back \" +\n          ((nCmds \u003d\u003d 0) ? \"no commands\" :\n              ((nCmds \u003d\u003d 1) ? \"one command: \" + cmds.get(0) :\n                  (nCmds + \" commands: \" + Joiner.on(\"; \").join(cmds)))) +\n          \".\");\n    }\n    scheduleNextBlockReport(startTime);\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-5153. Datanode should send block reports for each storage in a separate message. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563254 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/01/14 1:00 PM",
      "commitName": "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5153. Datanode should send block reports for each storage in a separate message. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563254 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "31/01/14 1:00 PM",
          "commitName": "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "15/12/13 4:58 PM",
          "commitNameOld": "938565925adb9d866e8c6951361cd5582076e013",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 46.83,
          "commitsBetweenForRepo": 246,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,71 @@\n-  DatanodeCommand blockReport() throws IOException {\n+  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n     // send block report if timer has expired.\n-    DatanodeCommand cmd \u003d null;\n-    long startTime \u003d now();\n-    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n-\n-      // Flush any block information that precedes the block report. Otherwise\n-      // we have a chance that we will miss the delHint information\n-      // or we will report an RBW replica after the BlockReport already reports\n-      // a FINALIZED one.\n-      reportReceivedDeletedBlocks();\n-\n-      // Send one block report per known storage.\n-\n-      // Create block report\n-      long brCreateStartTime \u003d now();\n-      long totalBlockCount \u003d 0;\n-\n-      Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n-          dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n-\n-      // Send block report\n-      long brSendStartTime \u003d now();\n-      StorageBlockReport[] reports \u003d\n-          new StorageBlockReport[perVolumeBlockLists.size()];\n-\n-      int i \u003d 0;\n-      for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n-        DatanodeStorage dnStorage \u003d kvPair.getKey();\n-        BlockListAsLongs blockList \u003d kvPair.getValue();\n-        totalBlockCount +\u003d blockList.getNumberOfBlocks();\n-\n-        reports[i++] \u003d\n-            new StorageBlockReport(\n-              dnStorage, blockList.getBlockListAsLongs());\n-      }\n-\n-      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), reports);\n-\n-      // Log the block report processing stats from Datanode perspective\n-      long brSendCost \u003d now() - brSendStartTime;\n-      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n-      dn.getMetrics().addBlockReport(brSendCost);\n-      LOG.info(\"BlockReport of \" + totalBlockCount\n-          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n-          + brSendCost + \" msecs for RPC and NN processing\");\n-\n-      // If we have sent the first block report, then wait a random\n-      // time before we start the periodic block reports.\n-      if (resetBlockReportTime) {\n-        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n-        resetBlockReportTime \u003d false;\n-      } else {\n-        /* say the last block report was at 8:20:14. The current report\n-         * should have started around 9:20:14 (default 1 hour interval).\n-         * If current time is :\n-         *   1) normal like 9:20:18, next report should be at 10:20:14\n-         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n-         */\n-        lastBlockReport +\u003d (now() - lastBlockReport) /\n-        dnConf.blockReportInterval * dnConf.blockReportInterval;\n-      }\n-      LOG.info(\"sent block report, processed command:\" + cmd);\n+    final long startTime \u003d now();\n+    if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n+      return null;\n     }\n-    return cmd;\n+\n+    ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n+\n+    // Flush any block information that precedes the block report. Otherwise\n+    // we have a chance that we will miss the delHint information\n+    // or we will report an RBW replica after the BlockReport already reports\n+    // a FINALIZED one.\n+    reportReceivedDeletedBlocks();\n+    lastDeletedReport \u003d startTime;\n+\n+    long brCreateStartTime \u003d now();\n+    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n+        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n+\n+    // Convert the reports to the format expected by the NN.\n+    int i \u003d 0;\n+    int totalBlockCount \u003d 0;\n+    StorageBlockReport reports[] \u003d\n+        new StorageBlockReport[perVolumeBlockLists.size()];\n+\n+    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n+      BlockListAsLongs blockList \u003d kvPair.getValue();\n+      reports[i++] \u003d new StorageBlockReport(\n+          kvPair.getKey(), blockList.getBlockListAsLongs());\n+      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n+    }\n+\n+    // Send the reports to the NN.\n+    int numReportsSent;\n+    long brSendStartTime \u003d now();\n+    if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n+      // Below split threshold, send all reports in a single message.\n+      numReportsSent \u003d 1;\n+      DatanodeCommand cmd \u003d\n+          bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), reports);\n+      if (cmd !\u003d null) {\n+        cmds.add(cmd);\n+      }\n+    } else {\n+      // Send one block report per message.\n+      numReportsSent \u003d i;\n+      for (StorageBlockReport report : reports) {\n+        StorageBlockReport singleReport[] \u003d { report };\n+        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n+            bpRegistration, bpos.getBlockPoolId(), singleReport);\n+        if (cmd !\u003d null) {\n+          cmds.add(cmd);\n+        }\n+      }\n+    }\n+\n+    // Log the block report processing stats from Datanode perspective\n+    long brSendCost \u003d now() - brSendStartTime;\n+    long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n+    dn.getMetrics().addBlockReport(brSendCost);\n+    LOG.info(\"Sent \" + numReportsSent + \" blockreports \" + totalBlockCount +\n+        \" blocks total. Took \" + brCreateCost +\n+        \" msec to generate and \" + brSendCost +\n+        \" msecs for RPC and NN processing. \" +\n+        \" Got back commands \" +\n+            (cmds.size() \u003d\u003d 0 ? \"none\" : Joiner.on(\"; \").join(cmds)));\n+\n+    scheduleNextBlockReport(startTime);\n+    return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n    // send block report if timer has expired.\n    final long startTime \u003d now();\n    if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n      return null;\n    }\n\n    ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n    lastDeletedReport \u003d startTime;\n\n    long brCreateStartTime \u003d now();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(\n          kvPair.getKey(), blockList.getBlockListAsLongs());\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent;\n    long brSendStartTime \u003d now();\n    if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n      // Below split threshold, send all reports in a single message.\n      numReportsSent \u003d 1;\n      DatanodeCommand cmd \u003d\n          bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), reports);\n      if (cmd !\u003d null) {\n        cmds.add(cmd);\n      }\n    } else {\n      // Send one block report per message.\n      numReportsSent \u003d i;\n      for (StorageBlockReport report : reports) {\n        StorageBlockReport singleReport[] \u003d { report };\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), singleReport);\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      }\n    }\n\n    // Log the block report processing stats from Datanode perspective\n    long brSendCost \u003d now() - brSendStartTime;\n    long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n    dn.getMetrics().addBlockReport(brSendCost);\n    LOG.info(\"Sent \" + numReportsSent + \" blockreports \" + totalBlockCount +\n        \" blocks total. Took \" + brCreateCost +\n        \" msec to generate and \" + brSendCost +\n        \" msecs for RPC and NN processing. \" +\n        \" Got back commands \" +\n            (cmds.size() \u003d\u003d 0 ? \"none\" : Joiner.on(\"; \").join(cmds)));\n\n    scheduleNextBlockReport(startTime);\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
          "extendedDetails": {
            "oldValue": "DatanodeCommand",
            "newValue": "List\u003cDatanodeCommand\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5153. Datanode should send block reports for each storage in a separate message. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563254 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "31/01/14 1:00 PM",
          "commitName": "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "15/12/13 4:58 PM",
          "commitNameOld": "938565925adb9d866e8c6951361cd5582076e013",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 46.83,
          "commitsBetweenForRepo": 246,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,71 @@\n-  DatanodeCommand blockReport() throws IOException {\n+  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n     // send block report if timer has expired.\n-    DatanodeCommand cmd \u003d null;\n-    long startTime \u003d now();\n-    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n-\n-      // Flush any block information that precedes the block report. Otherwise\n-      // we have a chance that we will miss the delHint information\n-      // or we will report an RBW replica after the BlockReport already reports\n-      // a FINALIZED one.\n-      reportReceivedDeletedBlocks();\n-\n-      // Send one block report per known storage.\n-\n-      // Create block report\n-      long brCreateStartTime \u003d now();\n-      long totalBlockCount \u003d 0;\n-\n-      Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n-          dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n-\n-      // Send block report\n-      long brSendStartTime \u003d now();\n-      StorageBlockReport[] reports \u003d\n-          new StorageBlockReport[perVolumeBlockLists.size()];\n-\n-      int i \u003d 0;\n-      for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n-        DatanodeStorage dnStorage \u003d kvPair.getKey();\n-        BlockListAsLongs blockList \u003d kvPair.getValue();\n-        totalBlockCount +\u003d blockList.getNumberOfBlocks();\n-\n-        reports[i++] \u003d\n-            new StorageBlockReport(\n-              dnStorage, blockList.getBlockListAsLongs());\n-      }\n-\n-      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), reports);\n-\n-      // Log the block report processing stats from Datanode perspective\n-      long brSendCost \u003d now() - brSendStartTime;\n-      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n-      dn.getMetrics().addBlockReport(brSendCost);\n-      LOG.info(\"BlockReport of \" + totalBlockCount\n-          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n-          + brSendCost + \" msecs for RPC and NN processing\");\n-\n-      // If we have sent the first block report, then wait a random\n-      // time before we start the periodic block reports.\n-      if (resetBlockReportTime) {\n-        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n-        resetBlockReportTime \u003d false;\n-      } else {\n-        /* say the last block report was at 8:20:14. The current report\n-         * should have started around 9:20:14 (default 1 hour interval).\n-         * If current time is :\n-         *   1) normal like 9:20:18, next report should be at 10:20:14\n-         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n-         */\n-        lastBlockReport +\u003d (now() - lastBlockReport) /\n-        dnConf.blockReportInterval * dnConf.blockReportInterval;\n-      }\n-      LOG.info(\"sent block report, processed command:\" + cmd);\n+    final long startTime \u003d now();\n+    if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n+      return null;\n     }\n-    return cmd;\n+\n+    ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n+\n+    // Flush any block information that precedes the block report. Otherwise\n+    // we have a chance that we will miss the delHint information\n+    // or we will report an RBW replica after the BlockReport already reports\n+    // a FINALIZED one.\n+    reportReceivedDeletedBlocks();\n+    lastDeletedReport \u003d startTime;\n+\n+    long brCreateStartTime \u003d now();\n+    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n+        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n+\n+    // Convert the reports to the format expected by the NN.\n+    int i \u003d 0;\n+    int totalBlockCount \u003d 0;\n+    StorageBlockReport reports[] \u003d\n+        new StorageBlockReport[perVolumeBlockLists.size()];\n+\n+    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n+      BlockListAsLongs blockList \u003d kvPair.getValue();\n+      reports[i++] \u003d new StorageBlockReport(\n+          kvPair.getKey(), blockList.getBlockListAsLongs());\n+      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n+    }\n+\n+    // Send the reports to the NN.\n+    int numReportsSent;\n+    long brSendStartTime \u003d now();\n+    if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n+      // Below split threshold, send all reports in a single message.\n+      numReportsSent \u003d 1;\n+      DatanodeCommand cmd \u003d\n+          bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), reports);\n+      if (cmd !\u003d null) {\n+        cmds.add(cmd);\n+      }\n+    } else {\n+      // Send one block report per message.\n+      numReportsSent \u003d i;\n+      for (StorageBlockReport report : reports) {\n+        StorageBlockReport singleReport[] \u003d { report };\n+        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n+            bpRegistration, bpos.getBlockPoolId(), singleReport);\n+        if (cmd !\u003d null) {\n+          cmds.add(cmd);\n+        }\n+      }\n+    }\n+\n+    // Log the block report processing stats from Datanode perspective\n+    long brSendCost \u003d now() - brSendStartTime;\n+    long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n+    dn.getMetrics().addBlockReport(brSendCost);\n+    LOG.info(\"Sent \" + numReportsSent + \" blockreports \" + totalBlockCount +\n+        \" blocks total. Took \" + brCreateCost +\n+        \" msec to generate and \" + brSendCost +\n+        \" msecs for RPC and NN processing. \" +\n+        \" Got back commands \" +\n+            (cmds.size() \u003d\u003d 0 ? \"none\" : Joiner.on(\"; \").join(cmds)));\n+\n+    scheduleNextBlockReport(startTime);\n+    return cmds.size() \u003d\u003d 0 ? null : cmds;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cDatanodeCommand\u003e blockReport() throws IOException {\n    // send block report if timer has expired.\n    final long startTime \u003d now();\n    if (startTime - lastBlockReport \u003c\u003d dnConf.blockReportInterval) {\n      return null;\n    }\n\n    ArrayList\u003cDatanodeCommand\u003e cmds \u003d new ArrayList\u003cDatanodeCommand\u003e();\n\n    // Flush any block information that precedes the block report. Otherwise\n    // we have a chance that we will miss the delHint information\n    // or we will report an RBW replica after the BlockReport already reports\n    // a FINALIZED one.\n    reportReceivedDeletedBlocks();\n    lastDeletedReport \u003d startTime;\n\n    long brCreateStartTime \u003d now();\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n        dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n    // Convert the reports to the format expected by the NN.\n    int i \u003d 0;\n    int totalBlockCount \u003d 0;\n    StorageBlockReport reports[] \u003d\n        new StorageBlockReport[perVolumeBlockLists.size()];\n\n    for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList \u003d kvPair.getValue();\n      reports[i++] \u003d new StorageBlockReport(\n          kvPair.getKey(), blockList.getBlockListAsLongs());\n      totalBlockCount +\u003d blockList.getNumberOfBlocks();\n    }\n\n    // Send the reports to the NN.\n    int numReportsSent;\n    long brSendStartTime \u003d now();\n    if (totalBlockCount \u003c dnConf.blockReportSplitThreshold) {\n      // Below split threshold, send all reports in a single message.\n      numReportsSent \u003d 1;\n      DatanodeCommand cmd \u003d\n          bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), reports);\n      if (cmd !\u003d null) {\n        cmds.add(cmd);\n      }\n    } else {\n      // Send one block report per message.\n      numReportsSent \u003d i;\n      for (StorageBlockReport report : reports) {\n        StorageBlockReport singleReport[] \u003d { report };\n        DatanodeCommand cmd \u003d bpNamenode.blockReport(\n            bpRegistration, bpos.getBlockPoolId(), singleReport);\n        if (cmd !\u003d null) {\n          cmds.add(cmd);\n        }\n      }\n    }\n\n    // Log the block report processing stats from Datanode perspective\n    long brSendCost \u003d now() - brSendStartTime;\n    long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n    dn.getMetrics().addBlockReport(brSendCost);\n    LOG.info(\"Sent \" + numReportsSent + \" blockreports \" + totalBlockCount +\n        \" blocks total. Took \" + brCreateCost +\n        \" msec to generate and \" + brSendCost +\n        \" msecs for RPC and NN processing. \" +\n        \" Got back commands \" +\n            (cmds.size() \u003d\u003d 0 ? \"none\" : Joiner.on(\"; \").join(cmds)));\n\n    scheduleNextBlockReport(startTime);\n    return cmds.size() \u003d\u003d 0 ? null : cmds;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
          "extendedDetails": {}
        }
      ]
    },
    "a1aa1836fb6831c25efe326cdfdc014370cf5957": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5484. StorageType and State in DatanodeStorageInfo in NameNode is not accurate. (Contributed by Eric Sirianni)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1547462 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/12/13 8:30 AM",
      "commitName": "a1aa1836fb6831c25efe326cdfdc014370cf5957",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/11/13 12:07 PM",
      "commitNameOld": "97acde2d33967f7f870f7dfe96c6b558e6fe324b",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 10.85,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,66 @@\n   DatanodeCommand blockReport() throws IOException {\n     // send block report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     long startTime \u003d now();\n     if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n \n       // Flush any block information that precedes the block report. Otherwise\n       // we have a chance that we will miss the delHint information\n       // or we will report an RBW replica after the BlockReport already reports\n       // a FINALIZED one.\n       reportReceivedDeletedBlocks();\n \n       // Send one block report per known storage.\n \n       // Create block report\n       long brCreateStartTime \u003d now();\n       long totalBlockCount \u003d 0;\n \n-      Map\u003cString, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n+      Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n           dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n       // Send block report\n       long brSendStartTime \u003d now();\n       StorageBlockReport[] reports \u003d\n           new StorageBlockReport[perVolumeBlockLists.size()];\n \n       int i \u003d 0;\n-      for(Map.Entry\u003cString, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n-        String storageID \u003d kvPair.getKey();\n+      for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n+        DatanodeStorage dnStorage \u003d kvPair.getKey();\n         BlockListAsLongs blockList \u003d kvPair.getValue();\n         totalBlockCount +\u003d blockList.getNumberOfBlocks();\n \n-        // Dummy DatanodeStorage object just for sending the block report.\n-        DatanodeStorage dnStorage \u003d new DatanodeStorage(storageID);\n         reports[i++] \u003d\n             new StorageBlockReport(\n               dnStorage, blockList.getBlockListAsLongs());\n       }\n \n       cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), reports);\n \n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d now() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       LOG.info(\"BlockReport of \" + totalBlockCount\n           + \" blocks took \" + brCreateCost + \" msec to generate and \"\n           + brSendCost + \" msecs for RPC and NN processing\");\n \n       // If we have sent the first block report, then wait a random\n       // time before we start the periodic block reports.\n       if (resetBlockReportTime) {\n         lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n         resetBlockReportTime \u003d false;\n       } else {\n         /* say the last block report was at 8:20:14. The current report\n          * should have started around 9:20:14 (default 1 hour interval).\n          * If current time is :\n          *   1) normal like 9:20:18, next report should be at 10:20:14\n          *   2) unexpected like 11:35:43, next report should be at 12:20:14\n          */\n         lastBlockReport +\u003d (now() - lastBlockReport) /\n         dnConf.blockReportInterval * dnConf.blockReportInterval;\n       }\n       LOG.info(\"sent block report, processed command:\" + cmd);\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand blockReport() throws IOException {\n    // send block report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d now();\n    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n\n      // Flush any block information that precedes the block report. Otherwise\n      // we have a chance that we will miss the delHint information\n      // or we will report an RBW replica after the BlockReport already reports\n      // a FINALIZED one.\n      reportReceivedDeletedBlocks();\n\n      // Send one block report per known storage.\n\n      // Create block report\n      long brCreateStartTime \u003d now();\n      long totalBlockCount \u003d 0;\n\n      Map\u003cDatanodeStorage, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n          dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n      // Send block report\n      long brSendStartTime \u003d now();\n      StorageBlockReport[] reports \u003d\n          new StorageBlockReport[perVolumeBlockLists.size()];\n\n      int i \u003d 0;\n      for(Map.Entry\u003cDatanodeStorage, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n        DatanodeStorage dnStorage \u003d kvPair.getKey();\n        BlockListAsLongs blockList \u003d kvPair.getValue();\n        totalBlockCount +\u003d blockList.getNumberOfBlocks();\n\n        reports[i++] \u003d\n            new StorageBlockReport(\n              dnStorage, blockList.getBlockListAsLongs());\n      }\n\n      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), reports);\n\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d now() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      LOG.info(\"BlockReport of \" + totalBlockCount\n          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n          + brSendCost + \" msecs for RPC and NN processing\");\n\n      // If we have sent the first block report, then wait a random\n      // time before we start the periodic block reports.\n      if (resetBlockReportTime) {\n        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n        resetBlockReportTime \u003d false;\n      } else {\n        /* say the last block report was at 8:20:14. The current report\n         * should have started around 9:20:14 (default 1 hour interval).\n         * If current time is :\n         *   1) normal like 9:20:18, next report should be at 10:20:14\n         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n         */\n        lastBlockReport +\u003d (now() - lastBlockReport) /\n        dnConf.blockReportInterval * dnConf.blockReportInterval;\n      }\n      LOG.info(\"sent block report, processed command:\" + cmd);\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4988. Datanode must support all the volumes as individual storages.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1526969 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 9:05 AM",
      "commitName": "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/09/13 11:03 AM",
      "commitNameOld": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 4.92,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,68 @@\n   DatanodeCommand blockReport() throws IOException {\n     // send block report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     long startTime \u003d now();\n     if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n \n       // Flush any block information that precedes the block report. Otherwise\n       // we have a chance that we will miss the delHint information\n       // or we will report an RBW replica after the BlockReport already reports\n       // a FINALIZED one.\n       reportReceivedDeletedBlocks();\n \n+      // Send one block report per known storage.\n+\n       // Create block report\n       long brCreateStartTime \u003d now();\n-      BlockListAsLongs bReport \u003d dn.getFSDataset().getBlockReport(\n-          bpos.getBlockPoolId());\n+      long totalBlockCount \u003d 0;\n+\n+      Map\u003cString, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n+          dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n \n       // Send block report\n       long brSendStartTime \u003d now();\n-      StorageBlockReport[] report \u003d { new StorageBlockReport(\n-          new DatanodeStorage(bpRegistration.getDatanodeUuid()),\n-          bReport.getBlockListAsLongs()) };\n-      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), report);\n+      StorageBlockReport[] reports \u003d\n+          new StorageBlockReport[perVolumeBlockLists.size()];\n+\n+      int i \u003d 0;\n+      for(Map.Entry\u003cString, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n+        String storageID \u003d kvPair.getKey();\n+        BlockListAsLongs blockList \u003d kvPair.getValue();\n+        totalBlockCount +\u003d blockList.getNumberOfBlocks();\n+\n+        // Dummy DatanodeStorage object just for sending the block report.\n+        DatanodeStorage dnStorage \u003d new DatanodeStorage(storageID);\n+        reports[i++] \u003d\n+            new StorageBlockReport(\n+              dnStorage, blockList.getBlockListAsLongs());\n+      }\n+\n+      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), reports);\n \n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d now() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n-      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n+      LOG.info(\"BlockReport of \" + totalBlockCount\n           + \" blocks took \" + brCreateCost + \" msec to generate and \"\n           + brSendCost + \" msecs for RPC and NN processing\");\n \n       // If we have sent the first block report, then wait a random\n       // time before we start the periodic block reports.\n       if (resetBlockReportTime) {\n         lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n         resetBlockReportTime \u003d false;\n       } else {\n         /* say the last block report was at 8:20:14. The current report\n          * should have started around 9:20:14 (default 1 hour interval).\n          * If current time is :\n          *   1) normal like 9:20:18, next report should be at 10:20:14\n          *   2) unexpected like 11:35:43, next report should be at 12:20:14\n          */\n         lastBlockReport +\u003d (now() - lastBlockReport) /\n         dnConf.blockReportInterval * dnConf.blockReportInterval;\n       }\n       LOG.info(\"sent block report, processed command:\" + cmd);\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand blockReport() throws IOException {\n    // send block report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d now();\n    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n\n      // Flush any block information that precedes the block report. Otherwise\n      // we have a chance that we will miss the delHint information\n      // or we will report an RBW replica after the BlockReport already reports\n      // a FINALIZED one.\n      reportReceivedDeletedBlocks();\n\n      // Send one block report per known storage.\n\n      // Create block report\n      long brCreateStartTime \u003d now();\n      long totalBlockCount \u003d 0;\n\n      Map\u003cString, BlockListAsLongs\u003e perVolumeBlockLists \u003d\n          dn.getFSDataset().getBlockReports(bpos.getBlockPoolId());\n\n      // Send block report\n      long brSendStartTime \u003d now();\n      StorageBlockReport[] reports \u003d\n          new StorageBlockReport[perVolumeBlockLists.size()];\n\n      int i \u003d 0;\n      for(Map.Entry\u003cString, BlockListAsLongs\u003e kvPair : perVolumeBlockLists.entrySet()) {\n        String storageID \u003d kvPair.getKey();\n        BlockListAsLongs blockList \u003d kvPair.getValue();\n        totalBlockCount +\u003d blockList.getNumberOfBlocks();\n\n        // Dummy DatanodeStorage object just for sending the block report.\n        DatanodeStorage dnStorage \u003d new DatanodeStorage(storageID);\n        reports[i++] \u003d\n            new StorageBlockReport(\n              dnStorage, blockList.getBlockListAsLongs());\n      }\n\n      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), reports);\n\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d now() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      LOG.info(\"BlockReport of \" + totalBlockCount\n          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n          + brSendCost + \" msecs for RPC and NN processing\");\n\n      // If we have sent the first block report, then wait a random\n      // time before we start the periodic block reports.\n      if (resetBlockReportTime) {\n        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n        resetBlockReportTime \u003d false;\n      } else {\n        /* say the last block report was at 8:20:14. The current report\n         * should have started around 9:20:14 (default 1 hour interval).\n         * If current time is :\n         *   1) normal like 9:20:18, next report should be at 10:20:14\n         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n         */\n        lastBlockReport +\u003d (now() - lastBlockReport) /\n        dnConf.blockReportInterval * dnConf.blockReportInterval;\n      }\n      LOG.info(\"sent block report, processed command:\" + cmd);\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "4551da302d94cffea0313eac79479ab6f9b7cb34": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5233. Use Datanode UUID to identify Datanodes.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1525407 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/09/13 11:03 AM",
      "commitName": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "25/08/13 8:18 PM",
      "commitNameOld": "73d14311bc847a29c2b8ec30bbfbaf59cd3cb713",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 27.61,
      "commitsBetweenForRepo": 131,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   DatanodeCommand blockReport() throws IOException {\n     // send block report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     long startTime \u003d now();\n     if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n \n       // Flush any block information that precedes the block report. Otherwise\n       // we have a chance that we will miss the delHint information\n       // or we will report an RBW replica after the BlockReport already reports\n       // a FINALIZED one.\n       reportReceivedDeletedBlocks();\n \n       // Create block report\n       long brCreateStartTime \u003d now();\n       BlockListAsLongs bReport \u003d dn.getFSDataset().getBlockReport(\n           bpos.getBlockPoolId());\n \n       // Send block report\n       long brSendStartTime \u003d now();\n       StorageBlockReport[] report \u003d { new StorageBlockReport(\n-          new DatanodeStorage(bpRegistration.getStorageID()),\n+          new DatanodeStorage(bpRegistration.getDatanodeUuid()),\n           bReport.getBlockListAsLongs()) };\n       cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), report);\n \n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d now() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n           + \" blocks took \" + brCreateCost + \" msec to generate and \"\n           + brSendCost + \" msecs for RPC and NN processing\");\n \n       // If we have sent the first block report, then wait a random\n       // time before we start the periodic block reports.\n       if (resetBlockReportTime) {\n         lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n         resetBlockReportTime \u003d false;\n       } else {\n         /* say the last block report was at 8:20:14. The current report\n          * should have started around 9:20:14 (default 1 hour interval).\n          * If current time is :\n          *   1) normal like 9:20:18, next report should be at 10:20:14\n          *   2) unexpected like 11:35:43, next report should be at 12:20:14\n          */\n         lastBlockReport +\u003d (now() - lastBlockReport) /\n         dnConf.blockReportInterval * dnConf.blockReportInterval;\n       }\n       LOG.info(\"sent block report, processed command:\" + cmd);\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand blockReport() throws IOException {\n    // send block report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d now();\n    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n\n      // Flush any block information that precedes the block report. Otherwise\n      // we have a chance that we will miss the delHint information\n      // or we will report an RBW replica after the BlockReport already reports\n      // a FINALIZED one.\n      reportReceivedDeletedBlocks();\n\n      // Create block report\n      long brCreateStartTime \u003d now();\n      BlockListAsLongs bReport \u003d dn.getFSDataset().getBlockReport(\n          bpos.getBlockPoolId());\n\n      // Send block report\n      long brSendStartTime \u003d now();\n      StorageBlockReport[] report \u003d { new StorageBlockReport(\n          new DatanodeStorage(bpRegistration.getDatanodeUuid()),\n          bReport.getBlockListAsLongs()) };\n      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), report);\n\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d now() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n          + brSendCost + \" msecs for RPC and NN processing\");\n\n      // If we have sent the first block report, then wait a random\n      // time before we start the periodic block reports.\n      if (resetBlockReportTime) {\n        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n        resetBlockReportTime \u003d false;\n      } else {\n        /* say the last block report was at 8:20:14. The current report\n         * should have started around 9:20:14 (default 1 hour interval).\n         * If current time is :\n         *   1) normal like 9:20:18, next report should be at 10:20:14\n         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n         */\n        lastBlockReport +\u003d (now() - lastBlockReport) /\n        dnConf.blockReportInterval * dnConf.blockReportInterval;\n      }\n      LOG.info(\"sent block report, processed command:\" + cmd);\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "9e108e61fb28244326d7cf4bb31d175eb75d2636": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3086. Change Datanode not to send storage list in registration.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303318 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/03/12 2:07 AM",
      "commitName": "9e108e61fb28244326d7cf4bb31d175eb75d2636",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "28/02/12 5:09 PM",
      "commitNameOld": "978a8050e28b2afb193a3e00d82a8475fa4d2428",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 21.33,
      "commitsBetweenForRepo": 140,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,51 @@\n   DatanodeCommand blockReport() throws IOException {\n     // send block report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     long startTime \u003d now();\n     if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n \n       // Flush any block information that precedes the block report. Otherwise\n       // we have a chance that we will miss the delHint information\n       // or we will report an RBW replica after the BlockReport already reports\n       // a FINALIZED one.\n       reportReceivedDeletedBlocks();\n \n       // Create block report\n       long brCreateStartTime \u003d now();\n       BlockListAsLongs bReport \u003d dn.getFSDataset().getBlockReport(\n           bpos.getBlockPoolId());\n \n       // Send block report\n       long brSendStartTime \u003d now();\n       StorageBlockReport[] report \u003d { new StorageBlockReport(\n-          bpRegistration.getStorageID(), bReport.getBlockListAsLongs()) };\n+          new DatanodeStorage(bpRegistration.getStorageID()),\n+          bReport.getBlockListAsLongs()) };\n       cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), report);\n \n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d now() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n           + \" blocks took \" + brCreateCost + \" msec to generate and \"\n           + brSendCost + \" msecs for RPC and NN processing\");\n \n       // If we have sent the first block report, then wait a random\n       // time before we start the periodic block reports.\n       if (resetBlockReportTime) {\n         lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n         resetBlockReportTime \u003d false;\n       } else {\n         /* say the last block report was at 8:20:14. The current report\n          * should have started around 9:20:14 (default 1 hour interval).\n          * If current time is :\n          *   1) normal like 9:20:18, next report should be at 10:20:14\n          *   2) unexpected like 11:35:43, next report should be at 12:20:14\n          */\n         lastBlockReport +\u003d (now() - lastBlockReport) /\n         dnConf.blockReportInterval * dnConf.blockReportInterval;\n       }\n       LOG.info(\"sent block report, processed command:\" + cmd);\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand blockReport() throws IOException {\n    // send block report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d now();\n    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n\n      // Flush any block information that precedes the block report. Otherwise\n      // we have a chance that we will miss the delHint information\n      // or we will report an RBW replica after the BlockReport already reports\n      // a FINALIZED one.\n      reportReceivedDeletedBlocks();\n\n      // Create block report\n      long brCreateStartTime \u003d now();\n      BlockListAsLongs bReport \u003d dn.getFSDataset().getBlockReport(\n          bpos.getBlockPoolId());\n\n      // Send block report\n      long brSendStartTime \u003d now();\n      StorageBlockReport[] report \u003d { new StorageBlockReport(\n          new DatanodeStorage(bpRegistration.getStorageID()),\n          bReport.getBlockListAsLongs()) };\n      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), report);\n\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d now() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n          + brSendCost + \" msecs for RPC and NN processing\");\n\n      // If we have sent the first block report, then wait a random\n      // time before we start the periodic block reports.\n      if (resetBlockReportTime) {\n        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n        resetBlockReportTime \u003d false;\n      } else {\n        /* say the last block report was at 8:20:14. The current report\n         * should have started around 9:20:14 (default 1 hour interval).\n         * If current time is :\n         *   1) normal like 9:20:18, next report should be at 10:20:14\n         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n         */\n        lastBlockReport +\u003d (now() - lastBlockReport) /\n        dnConf.blockReportInterval * dnConf.blockReportInterval;\n      }\n      LOG.info(\"sent block report, processed command:\" + cmd);\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "846f97312c6db7b84b7401174acd0fc943baa093": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2691. Fixes for pipeline recovery in an HA cluster: report RBW replicas immediately upon pipeline creation. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1237935 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/01/12 11:16 AM",
      "commitName": "846f97312c6db7b84b7401174acd0fc943baa093",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "27/01/12 6:50 PM",
      "commitNameOld": "327c216c2fac682df543a6e82afe23e9296ab216",
      "commitAuthorOld": "",
      "daysBetweenCommits": 2.68,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,49 @@\n   DatanodeCommand blockReport() throws IOException {\n     // send block report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     long startTime \u003d now();\n     if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n \n+      // Flush any block information that precedes the block report. Otherwise\n+      // we have a chance that we will miss the delHint information\n+      // or we will report an RBW replica after the BlockReport already reports\n+      // a FINALIZED one.\n+      reportReceivedDeletedBlocks();\n+\n       // Create block report\n       long brCreateStartTime \u003d now();\n       BlockListAsLongs bReport \u003d dn.getFSDataset().getBlockReport(\n           bpos.getBlockPoolId());\n \n       // Send block report\n       long brSendStartTime \u003d now();\n       cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), bReport\n           .getBlockListAsLongs());\n \n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d now() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n       dn.getMetrics().addBlockReport(brSendCost);\n       LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n           + \" blocks took \" + brCreateCost + \" msec to generate and \"\n           + brSendCost + \" msecs for RPC and NN processing\");\n \n       // If we have sent the first block report, then wait a random\n       // time before we start the periodic block reports.\n       if (resetBlockReportTime) {\n         lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n         resetBlockReportTime \u003d false;\n       } else {\n         /* say the last block report was at 8:20:14. The current report\n          * should have started around 9:20:14 (default 1 hour interval).\n          * If current time is :\n          *   1) normal like 9:20:18, next report should be at 10:20:14\n          *   2) unexpected like 11:35:43, next report should be at 12:20:14\n          */\n         lastBlockReport +\u003d (now() - lastBlockReport) /\n         dnConf.blockReportInterval * dnConf.blockReportInterval;\n       }\n       LOG.info(\"sent block report, processed command:\" + cmd);\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand blockReport() throws IOException {\n    // send block report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d now();\n    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n\n      // Flush any block information that precedes the block report. Otherwise\n      // we have a chance that we will miss the delHint information\n      // or we will report an RBW replica after the BlockReport already reports\n      // a FINALIZED one.\n      reportReceivedDeletedBlocks();\n\n      // Create block report\n      long brCreateStartTime \u003d now();\n      BlockListAsLongs bReport \u003d dn.getFSDataset().getBlockReport(\n          bpos.getBlockPoolId());\n\n      // Send block report\n      long brSendStartTime \u003d now();\n      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), bReport\n          .getBlockListAsLongs());\n\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d now() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n          + brSendCost + \" msecs for RPC and NN processing\");\n\n      // If we have sent the first block report, then wait a random\n      // time before we start the periodic block reports.\n      if (resetBlockReportTime) {\n        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n        resetBlockReportTime \u003d false;\n      } else {\n        /* say the last block report was at 8:20:14. The current report\n         * should have started around 9:20:14 (default 1 hour interval).\n         * If current time is :\n         *   1) normal like 9:20:18, next report should be at 10:20:14\n         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n         */\n        lastBlockReport +\u003d (now() - lastBlockReport) /\n        dnConf.blockReportInterval * dnConf.blockReportInterval;\n      }\n      LOG.info(\"sent block report, processed command:\" + cmd);\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "1e346aa829519f8a2aa830e76d9856f914861805": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-1971. Send block report from datanode to both active and standby namenodes. (sanjay, todd via suresh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1208925 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/11/11 5:10 PM",
      "commitName": "1e346aa829519f8a2aa830e76d9856f914861805",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-1971. Send block report from datanode to both active and standby namenodes. (sanjay, todd via suresh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1208925 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/11/11 5:10 PM",
          "commitName": "1e346aa829519f8a2aa830e76d9856f914861805",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "30/11/11 1:46 PM",
          "commitNameOld": "f87a4b40bc99e76602a75906df31747cfdbff78a",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,43 @@\n   DatanodeCommand blockReport() throws IOException {\n     // send block report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     long startTime \u003d now();\n     if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n \n       // Create block report\n       long brCreateStartTime \u003d now();\n-      BlockListAsLongs bReport \u003d dn.data.getBlockReport(getBlockPoolId());\n+      BlockListAsLongs bReport \u003d dn.getFSDataset().getBlockReport(\n+          bpos.getBlockPoolId());\n \n       // Send block report\n       long brSendStartTime \u003d now();\n-      cmd \u003d bpNamenode.blockReport(bpRegistration, getBlockPoolId(), bReport\n+      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), bReport\n           .getBlockListAsLongs());\n \n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d now() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n-      dn.metrics.addBlockReport(brSendCost);\n+      dn.getMetrics().addBlockReport(brSendCost);\n       LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n           + \" blocks took \" + brCreateCost + \" msec to generate and \"\n           + brSendCost + \" msecs for RPC and NN processing\");\n \n       // If we have sent the first block report, then wait a random\n       // time before we start the periodic block reports.\n       if (resetBlockReportTime) {\n         lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n         resetBlockReportTime \u003d false;\n       } else {\n         /* say the last block report was at 8:20:14. The current report\n          * should have started around 9:20:14 (default 1 hour interval).\n          * If current time is :\n          *   1) normal like 9:20:18, next report should be at 10:20:14\n          *   2) unexpected like 11:35:43, next report should be at 12:20:14\n          */\n         lastBlockReport +\u003d (now() - lastBlockReport) /\n         dnConf.blockReportInterval * dnConf.blockReportInterval;\n       }\n       LOG.info(\"sent block report, processed command:\" + cmd);\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeCommand blockReport() throws IOException {\n    // send block report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d now();\n    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n\n      // Create block report\n      long brCreateStartTime \u003d now();\n      BlockListAsLongs bReport \u003d dn.getFSDataset().getBlockReport(\n          bpos.getBlockPoolId());\n\n      // Send block report\n      long brSendStartTime \u003d now();\n      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), bReport\n          .getBlockListAsLongs());\n\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d now() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n          + brSendCost + \" msecs for RPC and NN processing\");\n\n      // If we have sent the first block report, then wait a random\n      // time before we start the periodic block reports.\n      if (resetBlockReportTime) {\n        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n        resetBlockReportTime \u003d false;\n      } else {\n        /* say the last block report was at 8:20:14. The current report\n         * should have started around 9:20:14 (default 1 hour interval).\n         * If current time is :\n         *   1) normal like 9:20:18, next report should be at 10:20:14\n         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n         */\n        lastBlockReport +\u003d (now() - lastBlockReport) /\n        dnConf.blockReportInterval * dnConf.blockReportInterval;\n      }\n      LOG.info(\"sent block report, processed command:\" + cmd);\n    }\n    return cmd;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
            "oldMethodName": "blockReport",
            "newMethodName": "blockReport"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-1971. Send block report from datanode to both active and standby namenodes. (sanjay, todd via suresh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1208925 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/11/11 5:10 PM",
          "commitName": "1e346aa829519f8a2aa830e76d9856f914861805",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "30/11/11 1:46 PM",
          "commitNameOld": "f87a4b40bc99e76602a75906df31747cfdbff78a",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,43 @@\n   DatanodeCommand blockReport() throws IOException {\n     // send block report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     long startTime \u003d now();\n     if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n \n       // Create block report\n       long brCreateStartTime \u003d now();\n-      BlockListAsLongs bReport \u003d dn.data.getBlockReport(getBlockPoolId());\n+      BlockListAsLongs bReport \u003d dn.getFSDataset().getBlockReport(\n+          bpos.getBlockPoolId());\n \n       // Send block report\n       long brSendStartTime \u003d now();\n-      cmd \u003d bpNamenode.blockReport(bpRegistration, getBlockPoolId(), bReport\n+      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), bReport\n           .getBlockListAsLongs());\n \n       // Log the block report processing stats from Datanode perspective\n       long brSendCost \u003d now() - brSendStartTime;\n       long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n-      dn.metrics.addBlockReport(brSendCost);\n+      dn.getMetrics().addBlockReport(brSendCost);\n       LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n           + \" blocks took \" + brCreateCost + \" msec to generate and \"\n           + brSendCost + \" msecs for RPC and NN processing\");\n \n       // If we have sent the first block report, then wait a random\n       // time before we start the periodic block reports.\n       if (resetBlockReportTime) {\n         lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n         resetBlockReportTime \u003d false;\n       } else {\n         /* say the last block report was at 8:20:14. The current report\n          * should have started around 9:20:14 (default 1 hour interval).\n          * If current time is :\n          *   1) normal like 9:20:18, next report should be at 10:20:14\n          *   2) unexpected like 11:35:43, next report should be at 12:20:14\n          */\n         lastBlockReport +\u003d (now() - lastBlockReport) /\n         dnConf.blockReportInterval * dnConf.blockReportInterval;\n       }\n       LOG.info(\"sent block report, processed command:\" + cmd);\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeCommand blockReport() throws IOException {\n    // send block report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d now();\n    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n\n      // Create block report\n      long brCreateStartTime \u003d now();\n      BlockListAsLongs bReport \u003d dn.getFSDataset().getBlockReport(\n          bpos.getBlockPoolId());\n\n      // Send block report\n      long brSendStartTime \u003d now();\n      cmd \u003d bpNamenode.blockReport(bpRegistration, bpos.getBlockPoolId(), bReport\n          .getBlockListAsLongs());\n\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d now() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.getMetrics().addBlockReport(brSendCost);\n      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n          + brSendCost + \" msecs for RPC and NN processing\");\n\n      // If we have sent the first block report, then wait a random\n      // time before we start the periodic block reports.\n      if (resetBlockReportTime) {\n        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n        resetBlockReportTime \u003d false;\n      } else {\n        /* say the last block report was at 8:20:14. The current report\n         * should have started around 9:20:14 (default 1 hour interval).\n         * If current time is :\n         *   1) normal like 9:20:18, next report should be at 10:20:14\n         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n         */\n        lastBlockReport +\u003d (now() - lastBlockReport) /\n        dnConf.blockReportInterval * dnConf.blockReportInterval;\n      }\n      LOG.info(\"sent block report, processed command:\" + cmd);\n    }\n    return cmd;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
          "extendedDetails": {}
        }
      ]
    },
    "39ce694d05c6d8c428bd87bc1b9c95f94dfdf6fd": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-2566. Move BPOfferService to be a non-inner class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1204659 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/11 11:27 AM",
      "commitName": "39ce694d05c6d8c428bd87bc1b9c95f94dfdf6fd",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-2566. Move BPOfferService to be a non-inner class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1204659 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/11/11 11:27 AM",
          "commitName": "39ce694d05c6d8c428bd87bc1b9c95f94dfdf6fd",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "21/11/11 11:03 AM",
          "commitNameOld": "68173af69d2fbda3292404c90a5077483e14d6f4",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,42 @@\n-    DatanodeCommand blockReport() throws IOException {\n-      // send block report if timer has expired.\n-      DatanodeCommand cmd \u003d null;\n-      long startTime \u003d now();\n-      if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n+  DatanodeCommand blockReport() throws IOException {\n+    // send block report if timer has expired.\n+    DatanodeCommand cmd \u003d null;\n+    long startTime \u003d now();\n+    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n \n-        // Create block report\n-        long brCreateStartTime \u003d now();\n-        BlockListAsLongs bReport \u003d dn.data.getBlockReport(getBlockPoolId());\n+      // Create block report\n+      long brCreateStartTime \u003d now();\n+      BlockListAsLongs bReport \u003d dn.data.getBlockReport(getBlockPoolId());\n \n-        // Send block report\n-        long brSendStartTime \u003d now();\n-        cmd \u003d bpNamenode.blockReport(bpRegistration, getBlockPoolId(), bReport\n-            .getBlockListAsLongs());\n+      // Send block report\n+      long brSendStartTime \u003d now();\n+      cmd \u003d bpNamenode.blockReport(bpRegistration, getBlockPoolId(), bReport\n+          .getBlockListAsLongs());\n \n-        // Log the block report processing stats from Datanode perspective\n-        long brSendCost \u003d now() - brSendStartTime;\n-        long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n-        dn.metrics.addBlockReport(brSendCost);\n-        LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n-            + \" blocks took \" + brCreateCost + \" msec to generate and \"\n-            + brSendCost + \" msecs for RPC and NN processing\");\n+      // Log the block report processing stats from Datanode perspective\n+      long brSendCost \u003d now() - brSendStartTime;\n+      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n+      dn.metrics.addBlockReport(brSendCost);\n+      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n+          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n+          + brSendCost + \" msecs for RPC and NN processing\");\n \n-        // If we have sent the first block report, then wait a random\n-        // time before we start the periodic block reports.\n-        if (resetBlockReportTime) {\n-          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n-          resetBlockReportTime \u003d false;\n-        } else {\n-          /* say the last block report was at 8:20:14. The current report\n-           * should have started around 9:20:14 (default 1 hour interval).\n-           * If current time is :\n-           *   1) normal like 9:20:18, next report should be at 10:20:14\n-           *   2) unexpected like 11:35:43, next report should be at 12:20:14\n-           */\n-          lastBlockReport +\u003d (now() - lastBlockReport) /\n-          dnConf.blockReportInterval * dnConf.blockReportInterval;\n-        }\n-        LOG.info(\"sent block report, processed command:\" + cmd);\n+      // If we have sent the first block report, then wait a random\n+      // time before we start the periodic block reports.\n+      if (resetBlockReportTime) {\n+        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n+        resetBlockReportTime \u003d false;\n+      } else {\n+        /* say the last block report was at 8:20:14. The current report\n+         * should have started around 9:20:14 (default 1 hour interval).\n+         * If current time is :\n+         *   1) normal like 9:20:18, next report should be at 10:20:14\n+         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n+         */\n+        lastBlockReport +\u003d (now() - lastBlockReport) /\n+        dnConf.blockReportInterval * dnConf.blockReportInterval;\n       }\n-      return cmd;\n-    }\n\\ No newline at end of file\n+      LOG.info(\"sent block report, processed command:\" + cmd);\n+    }\n+    return cmd;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeCommand blockReport() throws IOException {\n    // send block report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d now();\n    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n\n      // Create block report\n      long brCreateStartTime \u003d now();\n      BlockListAsLongs bReport \u003d dn.data.getBlockReport(getBlockPoolId());\n\n      // Send block report\n      long brSendStartTime \u003d now();\n      cmd \u003d bpNamenode.blockReport(bpRegistration, getBlockPoolId(), bReport\n          .getBlockListAsLongs());\n\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d now() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.metrics.addBlockReport(brSendCost);\n      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n          + brSendCost + \" msecs for RPC and NN processing\");\n\n      // If we have sent the first block report, then wait a random\n      // time before we start the periodic block reports.\n      if (resetBlockReportTime) {\n        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n        resetBlockReportTime \u003d false;\n      } else {\n        /* say the last block report was at 8:20:14. The current report\n         * should have started around 9:20:14 (default 1 hour interval).\n         * If current time is :\n         *   1) normal like 9:20:18, next report should be at 10:20:14\n         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n         */\n        lastBlockReport +\u003d (now() - lastBlockReport) /\n        dnConf.blockReportInterval * dnConf.blockReportInterval;\n      }\n      LOG.info(\"sent block report, processed command:\" + cmd);\n    }\n    return cmd;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
            "oldMethodName": "blockReport",
            "newMethodName": "blockReport"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2566. Move BPOfferService to be a non-inner class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1204659 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/11/11 11:27 AM",
          "commitName": "39ce694d05c6d8c428bd87bc1b9c95f94dfdf6fd",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "21/11/11 11:03 AM",
          "commitNameOld": "68173af69d2fbda3292404c90a5077483e14d6f4",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,42 @@\n-    DatanodeCommand blockReport() throws IOException {\n-      // send block report if timer has expired.\n-      DatanodeCommand cmd \u003d null;\n-      long startTime \u003d now();\n-      if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n+  DatanodeCommand blockReport() throws IOException {\n+    // send block report if timer has expired.\n+    DatanodeCommand cmd \u003d null;\n+    long startTime \u003d now();\n+    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n \n-        // Create block report\n-        long brCreateStartTime \u003d now();\n-        BlockListAsLongs bReport \u003d dn.data.getBlockReport(getBlockPoolId());\n+      // Create block report\n+      long brCreateStartTime \u003d now();\n+      BlockListAsLongs bReport \u003d dn.data.getBlockReport(getBlockPoolId());\n \n-        // Send block report\n-        long brSendStartTime \u003d now();\n-        cmd \u003d bpNamenode.blockReport(bpRegistration, getBlockPoolId(), bReport\n-            .getBlockListAsLongs());\n+      // Send block report\n+      long brSendStartTime \u003d now();\n+      cmd \u003d bpNamenode.blockReport(bpRegistration, getBlockPoolId(), bReport\n+          .getBlockListAsLongs());\n \n-        // Log the block report processing stats from Datanode perspective\n-        long brSendCost \u003d now() - brSendStartTime;\n-        long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n-        dn.metrics.addBlockReport(brSendCost);\n-        LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n-            + \" blocks took \" + brCreateCost + \" msec to generate and \"\n-            + brSendCost + \" msecs for RPC and NN processing\");\n+      // Log the block report processing stats from Datanode perspective\n+      long brSendCost \u003d now() - brSendStartTime;\n+      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n+      dn.metrics.addBlockReport(brSendCost);\n+      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n+          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n+          + brSendCost + \" msecs for RPC and NN processing\");\n \n-        // If we have sent the first block report, then wait a random\n-        // time before we start the periodic block reports.\n-        if (resetBlockReportTime) {\n-          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n-          resetBlockReportTime \u003d false;\n-        } else {\n-          /* say the last block report was at 8:20:14. The current report\n-           * should have started around 9:20:14 (default 1 hour interval).\n-           * If current time is :\n-           *   1) normal like 9:20:18, next report should be at 10:20:14\n-           *   2) unexpected like 11:35:43, next report should be at 12:20:14\n-           */\n-          lastBlockReport +\u003d (now() - lastBlockReport) /\n-          dnConf.blockReportInterval * dnConf.blockReportInterval;\n-        }\n-        LOG.info(\"sent block report, processed command:\" + cmd);\n+      // If we have sent the first block report, then wait a random\n+      // time before we start the periodic block reports.\n+      if (resetBlockReportTime) {\n+        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n+        resetBlockReportTime \u003d false;\n+      } else {\n+        /* say the last block report was at 8:20:14. The current report\n+         * should have started around 9:20:14 (default 1 hour interval).\n+         * If current time is :\n+         *   1) normal like 9:20:18, next report should be at 10:20:14\n+         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n+         */\n+        lastBlockReport +\u003d (now() - lastBlockReport) /\n+        dnConf.blockReportInterval * dnConf.blockReportInterval;\n       }\n-      return cmd;\n-    }\n\\ No newline at end of file\n+      LOG.info(\"sent block report, processed command:\" + cmd);\n+    }\n+    return cmd;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeCommand blockReport() throws IOException {\n    // send block report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d now();\n    if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n\n      // Create block report\n      long brCreateStartTime \u003d now();\n      BlockListAsLongs bReport \u003d dn.data.getBlockReport(getBlockPoolId());\n\n      // Send block report\n      long brSendStartTime \u003d now();\n      cmd \u003d bpNamenode.blockReport(bpRegistration, getBlockPoolId(), bReport\n          .getBlockListAsLongs());\n\n      // Log the block report processing stats from Datanode perspective\n      long brSendCost \u003d now() - brSendStartTime;\n      long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n      dn.metrics.addBlockReport(brSendCost);\n      LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n          + \" blocks took \" + brCreateCost + \" msec to generate and \"\n          + brSendCost + \" msecs for RPC and NN processing\");\n\n      // If we have sent the first block report, then wait a random\n      // time before we start the periodic block reports.\n      if (resetBlockReportTime) {\n        lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n        resetBlockReportTime \u003d false;\n      } else {\n        /* say the last block report was at 8:20:14. The current report\n         * should have started around 9:20:14 (default 1 hour interval).\n         * If current time is :\n         *   1) normal like 9:20:18, next report should be at 10:20:14\n         *   2) unexpected like 11:35:43, next report should be at 12:20:14\n         */\n        lastBlockReport +\u003d (now() - lastBlockReport) /\n        dnConf.blockReportInterval * dnConf.blockReportInterval;\n      }\n      LOG.info(\"sent block report, processed command:\" + cmd);\n    }\n    return cmd;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
          "extendedDetails": {}
        }
      ]
    },
    "1f92266516c882e43fa453b876dd8ca09893c477": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2563. Some cleanup in BPOfferService. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203943 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 5:31 PM",
      "commitName": "1f92266516c882e43fa453b876dd8ca09893c477",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "18/11/11 1:04 AM",
      "commitNameOld": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.69,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n     DatanodeCommand blockReport() throws IOException {\n       // send block report if timer has expired.\n       DatanodeCommand cmd \u003d null;\n       long startTime \u003d now();\n       if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n \n         // Create block report\n         long brCreateStartTime \u003d now();\n-        BlockListAsLongs bReport \u003d dn.data.getBlockReport(blockPoolId);\n+        BlockListAsLongs bReport \u003d dn.data.getBlockReport(getBlockPoolId());\n \n         // Send block report\n         long brSendStartTime \u003d now();\n-        cmd \u003d bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n+        cmd \u003d bpNamenode.blockReport(bpRegistration, getBlockPoolId(), bReport\n             .getBlockListAsLongs());\n \n         // Log the block report processing stats from Datanode perspective\n         long brSendCost \u003d now() - brSendStartTime;\n         long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n         dn.metrics.addBlockReport(brSendCost);\n         LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n             + \" blocks took \" + brCreateCost + \" msec to generate and \"\n             + brSendCost + \" msecs for RPC and NN processing\");\n \n         // If we have sent the first block report, then wait a random\n         // time before we start the periodic block reports.\n         if (resetBlockReportTime) {\n           lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n           resetBlockReportTime \u003d false;\n         } else {\n           /* say the last block report was at 8:20:14. The current report\n            * should have started around 9:20:14 (default 1 hour interval).\n            * If current time is :\n            *   1) normal like 9:20:18, next report should be at 10:20:14\n            *   2) unexpected like 11:35:43, next report should be at 12:20:14\n            */\n           lastBlockReport +\u003d (now() - lastBlockReport) /\n           dnConf.blockReportInterval * dnConf.blockReportInterval;\n         }\n         LOG.info(\"sent block report, processed command:\" + cmd);\n       }\n       return cmd;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    DatanodeCommand blockReport() throws IOException {\n      // send block report if timer has expired.\n      DatanodeCommand cmd \u003d null;\n      long startTime \u003d now();\n      if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n\n        // Create block report\n        long brCreateStartTime \u003d now();\n        BlockListAsLongs bReport \u003d dn.data.getBlockReport(getBlockPoolId());\n\n        // Send block report\n        long brSendStartTime \u003d now();\n        cmd \u003d bpNamenode.blockReport(bpRegistration, getBlockPoolId(), bReport\n            .getBlockListAsLongs());\n\n        // Log the block report processing stats from Datanode perspective\n        long brSendCost \u003d now() - brSendStartTime;\n        long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n        dn.metrics.addBlockReport(brSendCost);\n        LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n            + \" blocks took \" + brCreateCost + \" msec to generate and \"\n            + brSendCost + \" msecs for RPC and NN processing\");\n\n        // If we have sent the first block report, then wait a random\n        // time before we start the periodic block reports.\n        if (resetBlockReportTime) {\n          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n          resetBlockReportTime \u003d false;\n        } else {\n          /* say the last block report was at 8:20:14. The current report\n           * should have started around 9:20:14 (default 1 hour interval).\n           * If current time is :\n           *   1) normal like 9:20:18, next report should be at 10:20:14\n           *   2) unexpected like 11:35:43, next report should be at 12:20:14\n           */\n          lastBlockReport +\u003d (now() - lastBlockReport) /\n          dnConf.blockReportInterval * dnConf.blockReportInterval;\n        }\n        LOG.info(\"sent block report, processed command:\" + cmd);\n      }\n      return cmd;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "905a127850d5e0cba85c2e075f989fa0f5cf129a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2562. Refactor DN configuration variables out of DataNode class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203543 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 1:04 AM",
      "commitName": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "17/11/11 4:45 PM",
      "commitNameOld": "0864ef19089f703232107d8aa26c4a7571ff132e",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.35,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n     DatanodeCommand blockReport() throws IOException {\n       // send block report if timer has expired.\n       DatanodeCommand cmd \u003d null;\n       long startTime \u003d now();\n-      if (startTime - lastBlockReport \u003e dn.blockReportInterval) {\n+      if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n \n         // Create block report\n         long brCreateStartTime \u003d now();\n         BlockListAsLongs bReport \u003d dn.data.getBlockReport(blockPoolId);\n \n         // Send block report\n         long brSendStartTime \u003d now();\n         cmd \u003d bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n             .getBlockListAsLongs());\n \n         // Log the block report processing stats from Datanode perspective\n         long brSendCost \u003d now() - brSendStartTime;\n         long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n         dn.metrics.addBlockReport(brSendCost);\n         LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n             + \" blocks took \" + brCreateCost + \" msec to generate and \"\n             + brSendCost + \" msecs for RPC and NN processing\");\n \n         // If we have sent the first block report, then wait a random\n         // time before we start the periodic block reports.\n         if (resetBlockReportTime) {\n-          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dn.blockReportInterval));\n+          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n           resetBlockReportTime \u003d false;\n         } else {\n           /* say the last block report was at 8:20:14. The current report\n            * should have started around 9:20:14 (default 1 hour interval).\n            * If current time is :\n            *   1) normal like 9:20:18, next report should be at 10:20:14\n            *   2) unexpected like 11:35:43, next report should be at 12:20:14\n            */\n           lastBlockReport +\u003d (now() - lastBlockReport) /\n-          dn.blockReportInterval * dn.blockReportInterval;\n+          dnConf.blockReportInterval * dnConf.blockReportInterval;\n         }\n         LOG.info(\"sent block report, processed command:\" + cmd);\n       }\n       return cmd;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    DatanodeCommand blockReport() throws IOException {\n      // send block report if timer has expired.\n      DatanodeCommand cmd \u003d null;\n      long startTime \u003d now();\n      if (startTime - lastBlockReport \u003e dnConf.blockReportInterval) {\n\n        // Create block report\n        long brCreateStartTime \u003d now();\n        BlockListAsLongs bReport \u003d dn.data.getBlockReport(blockPoolId);\n\n        // Send block report\n        long brSendStartTime \u003d now();\n        cmd \u003d bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n            .getBlockListAsLongs());\n\n        // Log the block report processing stats from Datanode perspective\n        long brSendCost \u003d now() - brSendStartTime;\n        long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n        dn.metrics.addBlockReport(brSendCost);\n        LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n            + \" blocks took \" + brCreateCost + \" msec to generate and \"\n            + brSendCost + \" msecs for RPC and NN processing\");\n\n        // If we have sent the first block report, then wait a random\n        // time before we start the periodic block reports.\n        if (resetBlockReportTime) {\n          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dnConf.blockReportInterval));\n          resetBlockReportTime \u003d false;\n        } else {\n          /* say the last block report was at 8:20:14. The current report\n           * should have started around 9:20:14 (default 1 hour interval).\n           * If current time is :\n           *   1) normal like 9:20:18, next report should be at 10:20:14\n           *   2) unexpected like 11:35:43, next report should be at 12:20:14\n           */\n          lastBlockReport +\u003d (now() - lastBlockReport) /\n          dnConf.blockReportInterval * dnConf.blockReportInterval;\n        }\n        LOG.info(\"sent block report, processed command:\" + cmd);\n      }\n      return cmd;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "0864ef19089f703232107d8aa26c4a7571ff132e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2560. Refactor BPOfferService to be a static inner class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203444 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/11/11 4:45 PM",
      "commitName": "0864ef19089f703232107d8aa26c4a7571ff132e",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "31/10/11 10:17 PM",
      "commitNameOld": "1c940637b14eee777a65d153d0d712a1aea3866c",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 16.81,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n     DatanodeCommand blockReport() throws IOException {\n       // send block report if timer has expired.\n       DatanodeCommand cmd \u003d null;\n       long startTime \u003d now();\n-      if (startTime - lastBlockReport \u003e blockReportInterval) {\n+      if (startTime - lastBlockReport \u003e dn.blockReportInterval) {\n \n         // Create block report\n         long brCreateStartTime \u003d now();\n-        BlockListAsLongs bReport \u003d data.getBlockReport(blockPoolId);\n+        BlockListAsLongs bReport \u003d dn.data.getBlockReport(blockPoolId);\n \n         // Send block report\n         long brSendStartTime \u003d now();\n         cmd \u003d bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n             .getBlockListAsLongs());\n \n         // Log the block report processing stats from Datanode perspective\n         long brSendCost \u003d now() - brSendStartTime;\n         long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n-        metrics.addBlockReport(brSendCost);\n+        dn.metrics.addBlockReport(brSendCost);\n         LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n             + \" blocks took \" + brCreateCost + \" msec to generate and \"\n             + brSendCost + \" msecs for RPC and NN processing\");\n \n         // If we have sent the first block report, then wait a random\n         // time before we start the periodic block reports.\n         if (resetBlockReportTime) {\n-          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(blockReportInterval));\n+          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dn.blockReportInterval));\n           resetBlockReportTime \u003d false;\n         } else {\n           /* say the last block report was at 8:20:14. The current report\n            * should have started around 9:20:14 (default 1 hour interval).\n            * If current time is :\n            *   1) normal like 9:20:18, next report should be at 10:20:14\n            *   2) unexpected like 11:35:43, next report should be at 12:20:14\n            */\n           lastBlockReport +\u003d (now() - lastBlockReport) /\n-          blockReportInterval * blockReportInterval;\n+          dn.blockReportInterval * dn.blockReportInterval;\n         }\n         LOG.info(\"sent block report, processed command:\" + cmd);\n       }\n       return cmd;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    DatanodeCommand blockReport() throws IOException {\n      // send block report if timer has expired.\n      DatanodeCommand cmd \u003d null;\n      long startTime \u003d now();\n      if (startTime - lastBlockReport \u003e dn.blockReportInterval) {\n\n        // Create block report\n        long brCreateStartTime \u003d now();\n        BlockListAsLongs bReport \u003d dn.data.getBlockReport(blockPoolId);\n\n        // Send block report\n        long brSendStartTime \u003d now();\n        cmd \u003d bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n            .getBlockListAsLongs());\n\n        // Log the block report processing stats from Datanode perspective\n        long brSendCost \u003d now() - brSendStartTime;\n        long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n        dn.metrics.addBlockReport(brSendCost);\n        LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n            + \" blocks took \" + brCreateCost + \" msec to generate and \"\n            + brSendCost + \" msecs for RPC and NN processing\");\n\n        // If we have sent the first block report, then wait a random\n        // time before we start the periodic block reports.\n        if (resetBlockReportTime) {\n          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(dn.blockReportInterval));\n          resetBlockReportTime \u003d false;\n        } else {\n          /* say the last block report was at 8:20:14. The current report\n           * should have started around 9:20:14 (default 1 hour interval).\n           * If current time is :\n           *   1) normal like 9:20:18, next report should be at 10:20:14\n           *   2) unexpected like 11:35:43, next report should be at 12:20:14\n           */\n          lastBlockReport +\u003d (now() - lastBlockReport) /\n          dn.blockReportInterval * dn.blockReportInterval;\n        }\n        LOG.info(\"sent block report, processed command:\" + cmd);\n      }\n      return cmd;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    DatanodeCommand blockReport() throws IOException {\n      // send block report if timer has expired.\n      DatanodeCommand cmd \u003d null;\n      long startTime \u003d now();\n      if (startTime - lastBlockReport \u003e blockReportInterval) {\n\n        // Create block report\n        long brCreateStartTime \u003d now();\n        BlockListAsLongs bReport \u003d data.getBlockReport(blockPoolId);\n\n        // Send block report\n        long brSendStartTime \u003d now();\n        cmd \u003d bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n            .getBlockListAsLongs());\n\n        // Log the block report processing stats from Datanode perspective\n        long brSendCost \u003d now() - brSendStartTime;\n        long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n        metrics.addBlockReport(brSendCost);\n        LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n            + \" blocks took \" + brCreateCost + \" msec to generate and \"\n            + brSendCost + \" msecs for RPC and NN processing\");\n\n        // If we have sent the first block report, then wait a random\n        // time before we start the periodic block reports.\n        if (resetBlockReportTime) {\n          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(blockReportInterval));\n          resetBlockReportTime \u003d false;\n        } else {\n          /* say the last block report was at 8:20:14. The current report\n           * should have started around 9:20:14 (default 1 hour interval).\n           * If current time is :\n           *   1) normal like 9:20:18, next report should be at 10:20:14\n           *   2) unexpected like 11:35:43, next report should be at 12:20:14\n           */\n          lastBlockReport +\u003d (now() - lastBlockReport) /\n          blockReportInterval * blockReportInterval;\n        }\n        LOG.info(\"sent block report, processed command:\" + cmd);\n      }\n      return cmd;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    DatanodeCommand blockReport() throws IOException {\n      // send block report if timer has expired.\n      DatanodeCommand cmd \u003d null;\n      long startTime \u003d now();\n      if (startTime - lastBlockReport \u003e blockReportInterval) {\n\n        // Create block report\n        long brCreateStartTime \u003d now();\n        BlockListAsLongs bReport \u003d data.getBlockReport(blockPoolId);\n\n        // Send block report\n        long brSendStartTime \u003d now();\n        cmd \u003d bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n            .getBlockListAsLongs());\n\n        // Log the block report processing stats from Datanode perspective\n        long brSendCost \u003d now() - brSendStartTime;\n        long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n        metrics.addBlockReport(brSendCost);\n        LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n            + \" blocks took \" + brCreateCost + \" msec to generate and \"\n            + brSendCost + \" msecs for RPC and NN processing\");\n\n        // If we have sent the first block report, then wait a random\n        // time before we start the periodic block reports.\n        if (resetBlockReportTime) {\n          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(blockReportInterval));\n          resetBlockReportTime \u003d false;\n        } else {\n          /* say the last block report was at 8:20:14. The current report\n           * should have started around 9:20:14 (default 1 hour interval).\n           * If current time is :\n           *   1) normal like 9:20:18, next report should be at 10:20:14\n           *   2) unexpected like 11:35:43, next report should be at 12:20:14\n           */\n          lastBlockReport +\u003d (now() - lastBlockReport) /\n          blockReportInterval * blockReportInterval;\n        }\n        LOG.info(\"sent block report, processed command:\" + cmd);\n      }\n      return cmd;\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
      }
    },
    "710e5a960e8af1d4c73e386041096aacfee8b828": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2161. Move createNamenode(..), createClientDatanodeProtocolProxy(..) and Random object creation to DFSUtil; move DFSClient.stringifyToken(..) to DelegationTokenIdentifier.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1148348 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/07/11 7:23 AM",
      "commitName": "710e5a960e8af1d4c73e386041096aacfee8b828",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "13/07/11 4:44 PM",
      "commitNameOld": "c54c117407bc441b539b8acfdb022586f17dafc1",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 5.61,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n     DatanodeCommand blockReport() throws IOException {\n       // send block report if timer has expired.\n       DatanodeCommand cmd \u003d null;\n       long startTime \u003d now();\n       if (startTime - lastBlockReport \u003e blockReportInterval) {\n \n         // Create block report\n         long brCreateStartTime \u003d now();\n         BlockListAsLongs bReport \u003d data.getBlockReport(blockPoolId);\n \n         // Send block report\n         long brSendStartTime \u003d now();\n         cmd \u003d bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n             .getBlockListAsLongs());\n \n         // Log the block report processing stats from Datanode perspective\n         long brSendCost \u003d now() - brSendStartTime;\n         long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n         metrics.addBlockReport(brSendCost);\n         LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n             + \" blocks took \" + brCreateCost + \" msec to generate and \"\n             + brSendCost + \" msecs for RPC and NN processing\");\n \n         // If we have sent the first block report, then wait a random\n         // time before we start the periodic block reports.\n         if (resetBlockReportTime) {\n-          lastBlockReport \u003d startTime - R.nextInt((int)(blockReportInterval));\n+          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(blockReportInterval));\n           resetBlockReportTime \u003d false;\n         } else {\n           /* say the last block report was at 8:20:14. The current report\n            * should have started around 9:20:14 (default 1 hour interval).\n            * If current time is :\n            *   1) normal like 9:20:18, next report should be at 10:20:14\n            *   2) unexpected like 11:35:43, next report should be at 12:20:14\n            */\n           lastBlockReport +\u003d (now() - lastBlockReport) /\n           blockReportInterval * blockReportInterval;\n         }\n         LOG.info(\"sent block report, processed command:\" + cmd);\n       }\n       return cmd;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    DatanodeCommand blockReport() throws IOException {\n      // send block report if timer has expired.\n      DatanodeCommand cmd \u003d null;\n      long startTime \u003d now();\n      if (startTime - lastBlockReport \u003e blockReportInterval) {\n\n        // Create block report\n        long brCreateStartTime \u003d now();\n        BlockListAsLongs bReport \u003d data.getBlockReport(blockPoolId);\n\n        // Send block report\n        long brSendStartTime \u003d now();\n        cmd \u003d bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n            .getBlockListAsLongs());\n\n        // Log the block report processing stats from Datanode perspective\n        long brSendCost \u003d now() - brSendStartTime;\n        long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n        metrics.addBlockReport(brSendCost);\n        LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n            + \" blocks took \" + brCreateCost + \" msec to generate and \"\n            + brSendCost + \" msecs for RPC and NN processing\");\n\n        // If we have sent the first block report, then wait a random\n        // time before we start the periodic block reports.\n        if (resetBlockReportTime) {\n          lastBlockReport \u003d startTime - DFSUtil.getRandom().nextInt((int)(blockReportInterval));\n          resetBlockReportTime \u003d false;\n        } else {\n          /* say the last block report was at 8:20:14. The current report\n           * should have started around 9:20:14 (default 1 hour interval).\n           * If current time is :\n           *   1) normal like 9:20:18, next report should be at 10:20:14\n           *   2) unexpected like 11:35:43, next report should be at 12:20:14\n           */\n          lastBlockReport +\u003d (now() - lastBlockReport) /\n          blockReportInterval * blockReportInterval;\n        }\n        LOG.info(\"sent block report, processed command:\" + cmd);\n      }\n      return cmd;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,42 @@\n+    DatanodeCommand blockReport() throws IOException {\n+      // send block report if timer has expired.\n+      DatanodeCommand cmd \u003d null;\n+      long startTime \u003d now();\n+      if (startTime - lastBlockReport \u003e blockReportInterval) {\n+\n+        // Create block report\n+        long brCreateStartTime \u003d now();\n+        BlockListAsLongs bReport \u003d data.getBlockReport(blockPoolId);\n+\n+        // Send block report\n+        long brSendStartTime \u003d now();\n+        cmd \u003d bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n+            .getBlockListAsLongs());\n+\n+        // Log the block report processing stats from Datanode perspective\n+        long brSendCost \u003d now() - brSendStartTime;\n+        long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n+        metrics.addBlockReport(brSendCost);\n+        LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n+            + \" blocks took \" + brCreateCost + \" msec to generate and \"\n+            + brSendCost + \" msecs for RPC and NN processing\");\n+\n+        // If we have sent the first block report, then wait a random\n+        // time before we start the periodic block reports.\n+        if (resetBlockReportTime) {\n+          lastBlockReport \u003d startTime - R.nextInt((int)(blockReportInterval));\n+          resetBlockReportTime \u003d false;\n+        } else {\n+          /* say the last block report was at 8:20:14. The current report\n+           * should have started around 9:20:14 (default 1 hour interval).\n+           * If current time is :\n+           *   1) normal like 9:20:18, next report should be at 10:20:14\n+           *   2) unexpected like 11:35:43, next report should be at 12:20:14\n+           */\n+          lastBlockReport +\u003d (now() - lastBlockReport) /\n+          blockReportInterval * blockReportInterval;\n+        }\n+        LOG.info(\"sent block report, processed command:\" + cmd);\n+      }\n+      return cmd;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    DatanodeCommand blockReport() throws IOException {\n      // send block report if timer has expired.\n      DatanodeCommand cmd \u003d null;\n      long startTime \u003d now();\n      if (startTime - lastBlockReport \u003e blockReportInterval) {\n\n        // Create block report\n        long brCreateStartTime \u003d now();\n        BlockListAsLongs bReport \u003d data.getBlockReport(blockPoolId);\n\n        // Send block report\n        long brSendStartTime \u003d now();\n        cmd \u003d bpNamenode.blockReport(bpRegistration, blockPoolId, bReport\n            .getBlockListAsLongs());\n\n        // Log the block report processing stats from Datanode perspective\n        long brSendCost \u003d now() - brSendStartTime;\n        long brCreateCost \u003d brSendStartTime - brCreateStartTime;\n        metrics.addBlockReport(brSendCost);\n        LOG.info(\"BlockReport of \" + bReport.getNumberOfBlocks()\n            + \" blocks took \" + brCreateCost + \" msec to generate and \"\n            + brSendCost + \" msecs for RPC and NN processing\");\n\n        // If we have sent the first block report, then wait a random\n        // time before we start the periodic block reports.\n        if (resetBlockReportTime) {\n          lastBlockReport \u003d startTime - R.nextInt((int)(blockReportInterval));\n          resetBlockReportTime \u003d false;\n        } else {\n          /* say the last block report was at 8:20:14. The current report\n           * should have started around 9:20:14 (default 1 hour interval).\n           * If current time is :\n           *   1) normal like 9:20:18, next report should be at 10:20:14\n           *   2) unexpected like 11:35:43, next report should be at 12:20:14\n           */\n          lastBlockReport +\u003d (now() - lastBlockReport) /\n          blockReportInterval * blockReportInterval;\n        }\n        LOG.info(\"sent block report, processed command:\" + cmd);\n      }\n      return cmd;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
    }
  }
}