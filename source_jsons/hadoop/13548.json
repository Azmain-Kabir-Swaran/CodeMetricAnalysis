{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "SlowDiskTracker.java",
  "functionName": "getSlowDisks",
  "functionId": "getSlowDisks___reports-Map__String,DiskLatency____numDisks-int__now-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.java",
  "functionStartLine": 218,
  "functionEndLine": 254,
  "numCommitsSeen": 5,
  "timeTaken": 750,
  "changeHistory": [
    "28cdc5a8dc37ade1f45bda3aede589ee8593945e"
  ],
  "changeHistoryShort": {
    "28cdc5a8dc37ade1f45bda3aede589ee8593945e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "28cdc5a8dc37ade1f45bda3aede589ee8593945e": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11551. Handle SlowDiskReport from DataNode at the NameNode. Contributed by Hanisha Koneru.\n",
      "commitDate": "30/03/17 10:41 PM",
      "commitName": "28cdc5a8dc37ade1f45bda3aede589ee8593945e",
      "commitAuthor": "Hanisha Koneru",
      "diff": "@@ -0,0 +1,37 @@\n+  private ArrayList\u003cDiskLatency\u003e getSlowDisks(\n+      Map\u003cString, DiskLatency\u003e reports, int numDisks, long now) {\n+    if (reports.isEmpty()) {\n+      return new ArrayList(ImmutableList.of());\n+    }\n+\n+    final PriorityQueue\u003cDiskLatency\u003e topNReports \u003d new PriorityQueue\u003c\u003e(\n+        reports.size(),\n+        new Comparator\u003cDiskLatency\u003e() {\n+          @Override\n+          public int compare(DiskLatency o1, DiskLatency o2) {\n+            return Doubles.compare(\n+                o1.getMaxLatency(), o2.getMaxLatency());\n+          }\n+        });\n+\n+    ArrayList\u003cDiskLatency\u003e oldSlowDiskIDs \u003d Lists.newArrayList();\n+\n+    for (Map.Entry\u003cString, DiskLatency\u003e entry : reports.entrySet()) {\n+      DiskLatency diskLatency \u003d entry.getValue();\n+      if (now - diskLatency.timestamp \u003c reportValidityMs) {\n+        if (topNReports.size() \u003c numDisks) {\n+          topNReports.add(diskLatency);\n+        } else if (topNReports.peek().getMaxLatency() \u003c\n+            diskLatency.getMaxLatency()) {\n+          topNReports.poll();\n+          topNReports.add(diskLatency);\n+        }\n+      } else {\n+        oldSlowDiskIDs.add(diskLatency);\n+      }\n+    }\n+\n+    oldSlowDisksCheck \u003d oldSlowDiskIDs;\n+\n+    return Lists.newArrayList(topNReports);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private ArrayList\u003cDiskLatency\u003e getSlowDisks(\n      Map\u003cString, DiskLatency\u003e reports, int numDisks, long now) {\n    if (reports.isEmpty()) {\n      return new ArrayList(ImmutableList.of());\n    }\n\n    final PriorityQueue\u003cDiskLatency\u003e topNReports \u003d new PriorityQueue\u003c\u003e(\n        reports.size(),\n        new Comparator\u003cDiskLatency\u003e() {\n          @Override\n          public int compare(DiskLatency o1, DiskLatency o2) {\n            return Doubles.compare(\n                o1.getMaxLatency(), o2.getMaxLatency());\n          }\n        });\n\n    ArrayList\u003cDiskLatency\u003e oldSlowDiskIDs \u003d Lists.newArrayList();\n\n    for (Map.Entry\u003cString, DiskLatency\u003e entry : reports.entrySet()) {\n      DiskLatency diskLatency \u003d entry.getValue();\n      if (now - diskLatency.timestamp \u003c reportValidityMs) {\n        if (topNReports.size() \u003c numDisks) {\n          topNReports.add(diskLatency);\n        } else if (topNReports.peek().getMaxLatency() \u003c\n            diskLatency.getMaxLatency()) {\n          topNReports.poll();\n          topNReports.add(diskLatency);\n        }\n      } else {\n        oldSlowDiskIDs.add(diskLatency);\n      }\n    }\n\n    oldSlowDisksCheck \u003d oldSlowDiskIDs;\n\n    return Lists.newArrayList(topNReports);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.java"
    }
  }
}