{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DfsClientConf.java",
  "functionName": "createChecksum",
  "functionId": "createChecksum___userOpt-ChecksumOpt",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java",
  "functionStartLine": 373,
  "functionEndLine": 386,
  "numCommitsSeen": 275,
  "timeTaken": 7725,
  "changeHistory": [
    "3aac4758b007a56e3d66998d457b2156effca528",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "463aec11718e47d4aabb86a7a539cb973460aae6",
    "b0ea77303ba62a400376ca32c63c5b138f32cbe7",
    "f84552ac35bb5221290be68fece9c779ebeaf4bc"
  ],
  "changeHistoryShort": {
    "3aac4758b007a56e3d66998d457b2156effca528": "Yfilerename",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange)",
    "463aec11718e47d4aabb86a7a539cb973460aae6": "Ymultichange(Yexceptionschange,Ybodychange)",
    "b0ea77303ba62a400376ca32c63c5b138f32cbe7": "Ymultichange(Yparameterchange,Yexceptionschange,Ybodychange)",
    "f84552ac35bb5221290be68fece9c779ebeaf4bc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3aac4758b007a56e3d66998d457b2156effca528": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8803. Move DfsClientConf to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "19/08/15 11:28 AM",
      "commitName": "3aac4758b007a56e3d66998d457b2156effca528",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/08/15 10:36 AM",
      "commitNameOld": "f61120d964a609ae5eabeb5c4d6c9afe0a15cad8",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public DataChecksum createChecksum(ChecksumOpt userOpt) {\n    // Fill in any missing field with the default.\n    ChecksumOpt opt \u003d ChecksumOpt.processChecksumOpt(\n        defaultChecksumOpt, userOpt);\n    DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n        opt.getChecksumType(),\n        opt.getBytesPerChecksum());\n    if (dataChecksum \u003d\u003d null) {\n      throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n          + userOpt + \", default\u003d\" + defaultChecksumOpt\n          + \", effective\u003dnull\");\n    }\n    return dataChecksum;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java"
      }
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
          "commitDate": "10/04/15 2:48 PM",
          "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "10/04/15 11:40 AM",
          "commitNameOld": "7660da95cb67cbfe034aa8fa2a5bf0f8c9fdf41a",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 0.13,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,14 @@\n-    private DataChecksum createChecksum(ChecksumOpt userOpt) {\n-      // Fill in any missing field with the default.\n-      ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n-          defaultChecksumOpt, userOpt);\n-      DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n-          myOpt.getChecksumType(),\n-          myOpt.getBytesPerChecksum());\n-      if (dataChecksum \u003d\u003d null) {\n-        throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n-            + userOpt + \", default\u003d\" + defaultChecksumOpt\n-            + \", effective\u003dnull\");\n-      }\n-      return dataChecksum;\n-    }\n\\ No newline at end of file\n+  public DataChecksum createChecksum(ChecksumOpt userOpt) {\n+    // Fill in any missing field with the default.\n+    ChecksumOpt opt \u003d ChecksumOpt.processChecksumOpt(\n+        defaultChecksumOpt, userOpt);\n+    DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n+        opt.getChecksumType(),\n+        opt.getBytesPerChecksum());\n+    if (dataChecksum \u003d\u003d null) {\n+      throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n+          + userOpt + \", default\u003d\" + defaultChecksumOpt\n+          + \", effective\u003dnull\");\n+    }\n+    return dataChecksum;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public DataChecksum createChecksum(ChecksumOpt userOpt) {\n    // Fill in any missing field with the default.\n    ChecksumOpt opt \u003d ChecksumOpt.processChecksumOpt(\n        defaultChecksumOpt, userOpt);\n    DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n        opt.getChecksumType(),\n        opt.getBytesPerChecksum());\n    if (dataChecksum \u003d\u003d null) {\n      throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n          + userOpt + \", default\u003d\" + defaultChecksumOpt\n          + \", effective\u003dnull\");\n    }\n    return dataChecksum;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java",
            "oldMethodName": "createChecksum",
            "newMethodName": "createChecksum"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
          "commitDate": "10/04/15 2:48 PM",
          "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "10/04/15 11:40 AM",
          "commitNameOld": "7660da95cb67cbfe034aa8fa2a5bf0f8c9fdf41a",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 0.13,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,14 @@\n-    private DataChecksum createChecksum(ChecksumOpt userOpt) {\n-      // Fill in any missing field with the default.\n-      ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n-          defaultChecksumOpt, userOpt);\n-      DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n-          myOpt.getChecksumType(),\n-          myOpt.getBytesPerChecksum());\n-      if (dataChecksum \u003d\u003d null) {\n-        throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n-            + userOpt + \", default\u003d\" + defaultChecksumOpt\n-            + \", effective\u003dnull\");\n-      }\n-      return dataChecksum;\n-    }\n\\ No newline at end of file\n+  public DataChecksum createChecksum(ChecksumOpt userOpt) {\n+    // Fill in any missing field with the default.\n+    ChecksumOpt opt \u003d ChecksumOpt.processChecksumOpt(\n+        defaultChecksumOpt, userOpt);\n+    DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n+        opt.getChecksumType(),\n+        opt.getBytesPerChecksum());\n+    if (dataChecksum \u003d\u003d null) {\n+      throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n+          + userOpt + \", default\u003d\" + defaultChecksumOpt\n+          + \", effective\u003dnull\");\n+    }\n+    return dataChecksum;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public DataChecksum createChecksum(ChecksumOpt userOpt) {\n    // Fill in any missing field with the default.\n    ChecksumOpt opt \u003d ChecksumOpt.processChecksumOpt(\n        defaultChecksumOpt, userOpt);\n    DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n        opt.getChecksumType(),\n        opt.getBytesPerChecksum());\n    if (dataChecksum \u003d\u003d null) {\n      throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n          + userOpt + \", default\u003d\" + defaultChecksumOpt\n          + \", effective\u003dnull\");\n    }\n    return dataChecksum;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
          "commitDate": "10/04/15 2:48 PM",
          "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "10/04/15 11:40 AM",
          "commitNameOld": "7660da95cb67cbfe034aa8fa2a5bf0f8c9fdf41a",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 0.13,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,14 @@\n-    private DataChecksum createChecksum(ChecksumOpt userOpt) {\n-      // Fill in any missing field with the default.\n-      ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n-          defaultChecksumOpt, userOpt);\n-      DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n-          myOpt.getChecksumType(),\n-          myOpt.getBytesPerChecksum());\n-      if (dataChecksum \u003d\u003d null) {\n-        throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n-            + userOpt + \", default\u003d\" + defaultChecksumOpt\n-            + \", effective\u003dnull\");\n-      }\n-      return dataChecksum;\n-    }\n\\ No newline at end of file\n+  public DataChecksum createChecksum(ChecksumOpt userOpt) {\n+    // Fill in any missing field with the default.\n+    ChecksumOpt opt \u003d ChecksumOpt.processChecksumOpt(\n+        defaultChecksumOpt, userOpt);\n+    DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n+        opt.getChecksumType(),\n+        opt.getBytesPerChecksum());\n+    if (dataChecksum \u003d\u003d null) {\n+      throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n+          + userOpt + \", default\u003d\" + defaultChecksumOpt\n+          + \", effective\u003dnull\");\n+    }\n+    return dataChecksum;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public DataChecksum createChecksum(ChecksumOpt userOpt) {\n    // Fill in any missing field with the default.\n    ChecksumOpt opt \u003d ChecksumOpt.processChecksumOpt(\n        defaultChecksumOpt, userOpt);\n    DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n        opt.getChecksumType(),\n        opt.getBytesPerChecksum());\n    if (dataChecksum \u003d\u003d null) {\n      throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n          + userOpt + \", default\u003d\" + defaultChecksumOpt\n          + \", effective\u003dnull\");\n    }\n    return dataChecksum;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java",
          "extendedDetails": {}
        }
      ]
    },
    "463aec11718e47d4aabb86a7a539cb973460aae6": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
      "commitDate": "27/10/14 9:38 AM",
      "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthor": "cnauroth",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
          "commitDate": "27/10/14 9:38 AM",
          "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
          "commitAuthor": "cnauroth",
          "commitDateOld": "23/10/14 12:28 PM",
          "commitNameOld": "8c5b23b5473e447384f818d69d907d5c35ed6d6a",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 3.88,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,14 @@\n-    private DataChecksum createChecksum(ChecksumOpt userOpt) \n-        throws IOException {\n+    private DataChecksum createChecksum(ChecksumOpt userOpt) {\n       // Fill in any missing field with the default.\n       ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n           defaultChecksumOpt, userOpt);\n       DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n           myOpt.getChecksumType(),\n           myOpt.getBytesPerChecksum());\n       if (dataChecksum \u003d\u003d null) {\n-        throw new IOException(\"Invalid checksum type specified: \"\n-            + myOpt.getChecksumType().name());\n+        throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n+            + userOpt + \", default\u003d\" + defaultChecksumOpt\n+            + \", effective\u003dnull\");\n       }\n       return dataChecksum;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private DataChecksum createChecksum(ChecksumOpt userOpt) {\n      // Fill in any missing field with the default.\n      ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n          defaultChecksumOpt, userOpt);\n      DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n          myOpt.getChecksumType(),\n          myOpt.getBytesPerChecksum());\n      if (dataChecksum \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n            + userOpt + \", default\u003d\" + defaultChecksumOpt\n            + \", effective\u003dnull\");\n      }\n      return dataChecksum;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldValue": "[IOException]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
          "commitDate": "27/10/14 9:38 AM",
          "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
          "commitAuthor": "cnauroth",
          "commitDateOld": "23/10/14 12:28 PM",
          "commitNameOld": "8c5b23b5473e447384f818d69d907d5c35ed6d6a",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 3.88,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,14 @@\n-    private DataChecksum createChecksum(ChecksumOpt userOpt) \n-        throws IOException {\n+    private DataChecksum createChecksum(ChecksumOpt userOpt) {\n       // Fill in any missing field with the default.\n       ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n           defaultChecksumOpt, userOpt);\n       DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n           myOpt.getChecksumType(),\n           myOpt.getBytesPerChecksum());\n       if (dataChecksum \u003d\u003d null) {\n-        throw new IOException(\"Invalid checksum type specified: \"\n-            + myOpt.getChecksumType().name());\n+        throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n+            + userOpt + \", default\u003d\" + defaultChecksumOpt\n+            + \", effective\u003dnull\");\n       }\n       return dataChecksum;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private DataChecksum createChecksum(ChecksumOpt userOpt) {\n      // Fill in any missing field with the default.\n      ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n          defaultChecksumOpt, userOpt);\n      DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n          myOpt.getChecksumType(),\n          myOpt.getBytesPerChecksum());\n      if (dataChecksum \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt\u003d\"\n            + userOpt + \", default\u003d\" + defaultChecksumOpt\n            + \", effective\u003dnull\");\n      }\n      return dataChecksum;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "b0ea77303ba62a400376ca32c63c5b138f32cbe7": {
      "type": "Ymultichange(Yparameterchange,Yexceptionschange,Ybodychange)",
      "commitMessage": "HADOOP-8240. Add a new API to allow users to specify a checksum type on FileSystem.create(..).  Contributed by Kihwal Lee\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1374696 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/12 10:46 PM",
      "commitName": "b0ea77303ba62a400376ca32c63c5b138f32cbe7",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-8240. Add a new API to allow users to specify a checksum type on FileSystem.create(..).  Contributed by Kihwal Lee\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1374696 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/12 10:46 PM",
          "commitName": "b0ea77303ba62a400376ca32c63c5b138f32cbe7",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "17/08/12 9:52 AM",
          "commitNameOld": "fccace6116713c85cd59a808c565ea39fb5d6944",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 1.54,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,14 @@\n-    private DataChecksum createChecksum() {\n-      return DataChecksum.newDataChecksum(\n-          checksumType, bytesPerChecksum);\n+    private DataChecksum createChecksum(ChecksumOpt userOpt) \n+        throws IOException {\n+      // Fill in any missing field with the default.\n+      ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n+          defaultChecksumOpt, userOpt);\n+      DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n+          myOpt.getChecksumType(),\n+          myOpt.getBytesPerChecksum());\n+      if (dataChecksum \u003d\u003d null) {\n+        throw new IOException(\"Invalid checksum type specified: \"\n+            + myOpt.getChecksumType().name());\n+      }\n+      return dataChecksum;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private DataChecksum createChecksum(ChecksumOpt userOpt) \n        throws IOException {\n      // Fill in any missing field with the default.\n      ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n          defaultChecksumOpt, userOpt);\n      DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n          myOpt.getChecksumType(),\n          myOpt.getBytesPerChecksum());\n      if (dataChecksum \u003d\u003d null) {\n        throw new IOException(\"Invalid checksum type specified: \"\n            + myOpt.getChecksumType().name());\n      }\n      return dataChecksum;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[userOpt-ChecksumOpt]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HADOOP-8240. Add a new API to allow users to specify a checksum type on FileSystem.create(..).  Contributed by Kihwal Lee\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1374696 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/12 10:46 PM",
          "commitName": "b0ea77303ba62a400376ca32c63c5b138f32cbe7",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "17/08/12 9:52 AM",
          "commitNameOld": "fccace6116713c85cd59a808c565ea39fb5d6944",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 1.54,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,14 @@\n-    private DataChecksum createChecksum() {\n-      return DataChecksum.newDataChecksum(\n-          checksumType, bytesPerChecksum);\n+    private DataChecksum createChecksum(ChecksumOpt userOpt) \n+        throws IOException {\n+      // Fill in any missing field with the default.\n+      ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n+          defaultChecksumOpt, userOpt);\n+      DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n+          myOpt.getChecksumType(),\n+          myOpt.getBytesPerChecksum());\n+      if (dataChecksum \u003d\u003d null) {\n+        throw new IOException(\"Invalid checksum type specified: \"\n+            + myOpt.getChecksumType().name());\n+      }\n+      return dataChecksum;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private DataChecksum createChecksum(ChecksumOpt userOpt) \n        throws IOException {\n      // Fill in any missing field with the default.\n      ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n          defaultChecksumOpt, userOpt);\n      DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n          myOpt.getChecksumType(),\n          myOpt.getBytesPerChecksum());\n      if (dataChecksum \u003d\u003d null) {\n        throw new IOException(\"Invalid checksum type specified: \"\n            + myOpt.getChecksumType().name());\n      }\n      return dataChecksum;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-8240. Add a new API to allow users to specify a checksum type on FileSystem.create(..).  Contributed by Kihwal Lee\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1374696 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/12 10:46 PM",
          "commitName": "b0ea77303ba62a400376ca32c63c5b138f32cbe7",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "17/08/12 9:52 AM",
          "commitNameOld": "fccace6116713c85cd59a808c565ea39fb5d6944",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 1.54,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,14 @@\n-    private DataChecksum createChecksum() {\n-      return DataChecksum.newDataChecksum(\n-          checksumType, bytesPerChecksum);\n+    private DataChecksum createChecksum(ChecksumOpt userOpt) \n+        throws IOException {\n+      // Fill in any missing field with the default.\n+      ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n+          defaultChecksumOpt, userOpt);\n+      DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n+          myOpt.getChecksumType(),\n+          myOpt.getBytesPerChecksum());\n+      if (dataChecksum \u003d\u003d null) {\n+        throw new IOException(\"Invalid checksum type specified: \"\n+            + myOpt.getChecksumType().name());\n+      }\n+      return dataChecksum;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private DataChecksum createChecksum(ChecksumOpt userOpt) \n        throws IOException {\n      // Fill in any missing field with the default.\n      ChecksumOpt myOpt \u003d ChecksumOpt.processChecksumOpt(\n          defaultChecksumOpt, userOpt);\n      DataChecksum dataChecksum \u003d DataChecksum.newDataChecksum(\n          myOpt.getChecksumType(),\n          myOpt.getBytesPerChecksum());\n      if (dataChecksum \u003d\u003d null) {\n        throw new IOException(\"Invalid checksum type specified: \"\n            + myOpt.getChecksumType().name());\n      }\n      return dataChecksum;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "f84552ac35bb5221290be68fece9c779ebeaf4bc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2130. Switch default checksum to CRC32C. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196889 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/11/11 5:35 PM",
      "commitName": "f84552ac35bb5221290be68fece9c779ebeaf4bc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,4 @@\n+    private DataChecksum createChecksum() {\n+      return DataChecksum.newDataChecksum(\n+          checksumType, bytesPerChecksum);\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private DataChecksum createChecksum() {\n      return DataChecksum.newDataChecksum(\n          checksumType, bytesPerChecksum);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java"
    }
  }
}