{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "NamenodeWebHdfsMethods.java",
  "functionName": "chooseDatanode",
  "functionId": "chooseDatanode___namenode-NameNode(modifiers-final)__path-String(modifiers-final)__op-HttpOpParam.Op(modifiers-final)__openOffset-long(modifiers-final)__blocksize-long(modifiers-final)__excludeDatanodes-String(modifiers-final)__remoteAddr-String(modifiers-final)__status-HdfsFileStatus(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
  "functionStartLine": 263,
  "functionEndLine": 334,
  "numCommitsSeen": 399,
  "timeTaken": 10930,
  "changeHistory": [
    "92b53c40f070bbfe65c736f6f3eca721b9d227f5",
    "2fa7963c3d8cdfc65f90efabc6fe51a160be5c78",
    "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73",
    "1b5cceaffbdde50a87ede81552dc380832db8e79",
    "b9522e86a55564c2ccb5ca3f1ca871965cbe74de",
    "85cd06f6636f295ad1f3bf2a90063f4714c9cca7",
    "1268cf5fbe4458fa75ad0662512d352f9e8d3470",
    "c4578760b67d5b5169949a1b059f4472a268ff1b",
    "7c18f8d55b899dc4a6e118d3b54447a9b36b960a",
    "b46fbd0275bfc7ec16a219c72cff555d912170d7",
    "256adb2106cb838f3aff21f5a77f2973807d0df3",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9",
    "f3927595cc516381b1ae568e2d883a1d89993cbb",
    "bf1649d5fd095ce027f013be57d216212fa14198",
    "7e5b60116eede44d1cb5b6a263b4fec13da3c473",
    "40fe96546fcd68696076db67053f30d38a39a0d5",
    "50cb2771e924d2d6d9d04e588cb0a94aefb25b70",
    "dc8464f943b61b795df0cc8baec171bf07355763",
    "6c3b59505b863f03629da52a1e9b886fe9b496d0",
    "61d0b7530c8978c095ab6f62d9d38e168bd829c6"
  ],
  "changeHistoryShort": {
    "92b53c40f070bbfe65c736f6f3eca721b9d227f5": "Ybodychange",
    "2fa7963c3d8cdfc65f90efabc6fe51a160be5c78": "Ybodychange",
    "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73": "Ymultichange(Yparameterchange,Ybodychange)",
    "1b5cceaffbdde50a87ede81552dc380832db8e79": "Ybodychange",
    "b9522e86a55564c2ccb5ca3f1ca871965cbe74de": "Ybodychange",
    "85cd06f6636f295ad1f3bf2a90063f4714c9cca7": "Ymultichange(Yparameterchange,Ybodychange)",
    "1268cf5fbe4458fa75ad0662512d352f9e8d3470": "Ybodychange",
    "c4578760b67d5b5169949a1b059f4472a268ff1b": "Ybodychange",
    "7c18f8d55b899dc4a6e118d3b54447a9b36b960a": "Ymultichange(Yparameterchange,Ybodychange)",
    "b46fbd0275bfc7ec16a219c72cff555d912170d7": "Ybodychange",
    "256adb2106cb838f3aff21f5a77f2973807d0df3": "Ymultichange(Yparameterchange,Ybodychange)",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": "Ymultichange(Ybodychange,Yparametermetachange)",
    "f3927595cc516381b1ae568e2d883a1d89993cbb": "Ybodychange",
    "bf1649d5fd095ce027f013be57d216212fa14198": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "7e5b60116eede44d1cb5b6a263b4fec13da3c473": "Ybodychange",
    "40fe96546fcd68696076db67053f30d38a39a0d5": "Ymultichange(Yparameterchange,Ybodychange)",
    "50cb2771e924d2d6d9d04e588cb0a94aefb25b70": "Ybodychange",
    "dc8464f943b61b795df0cc8baec171bf07355763": "Ybodychange",
    "6c3b59505b863f03629da52a1e9b886fe9b496d0": "Ymultichange(Yparameterchange,Ybodychange)",
    "61d0b7530c8978c095ab6f62d9d38e168bd829c6": "Yintroduced"
  },
  "changeHistoryDetails": {
    "92b53c40f070bbfe65c736f6f3eca721b9d227f5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14216. NullPointerException happens in NamenodeWebHdfs. Contributed by lujie.\n",
      "commitDate": "21/02/19 7:06 AM",
      "commitName": "92b53c40f070bbfe65c736f6f3eca721b9d227f5",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "20/02/19 1:55 PM",
      "commitNameOld": "f5b4e0f971b138666a1f7015f387ae960f85d589",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 0.72,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,72 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final long blocksize, final String excludeDatanodes,\n       final String remoteAddr, final HdfsFileStatus status) throws IOException {\n     FSNamesystem fsn \u003d namenode.getNamesystem();\n     if (fsn \u003d\u003d null) {\n       throw new IOException(\"Namesystem has not been initialized yet.\");\n     }\n     final BlockManager bm \u003d fsn.getBlockManager();\n     \n     HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n     if (excludeDatanodes !\u003d null) {\n       for (String host : StringUtils\n           .getTrimmedStringCollection(excludeDatanodes)) {\n         int idx \u003d host.indexOf(\":\");\n-        if (idx !\u003d -1) {          \n-          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n-              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n+        Node excludeNode \u003d null;\n+        if (idx !\u003d -1) {\n+          excludeNode \u003d bm.getDatanodeManager().getDatanodeByXferAddr(\n+             host.substring(0, idx), Integer.parseInt(host.substring(idx + 1)));\n         } else {\n-          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n+          excludeNode \u003d bm.getDatanodeManager().getDatanodeByHost(host);\n+        }\n+\n+        if (excludeNode !\u003d null) {\n+          excludes.add(excludeNode);\n+        } else {\n+          LOG.debug(\"DataNode {} was requested to be excluded, \"\n+                + \"but it was not found.\", host);\n         }\n       }\n     }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(remoteAddr);\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n             path, clientNode, excludes, blocksize);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n     }\n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT, excludes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes,\n      final String remoteAddr, final HdfsFileStatus status) throws IOException {\n    FSNamesystem fsn \u003d namenode.getNamesystem();\n    if (fsn \u003d\u003d null) {\n      throw new IOException(\"Namesystem has not been initialized yet.\");\n    }\n    final BlockManager bm \u003d fsn.getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        Node excludeNode \u003d null;\n        if (idx !\u003d -1) {\n          excludeNode \u003d bm.getDatanodeManager().getDatanodeByXferAddr(\n             host.substring(0, idx), Integer.parseInt(host.substring(idx + 1)));\n        } else {\n          excludeNode \u003d bm.getDatanodeManager().getDatanodeByHost(host);\n        }\n\n        if (excludeNode !\u003d null) {\n          excludes.add(excludeNode);\n        } else {\n          LOG.debug(\"DataNode {} was requested to be excluded, \"\n                + \"but it was not found.\", host);\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(remoteAddr);\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    }\n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT, excludes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "2fa7963c3d8cdfc65f90efabc6fe51a160be5c78": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15254. Correct the wrong word spelling \u0027intialize\u0027. Contributed by fang zhenyi.\n",
      "commitDate": "24/02/18 2:41 PM",
      "commitName": "2fa7963c3d8cdfc65f90efabc6fe51a160be5c78",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "23/02/18 7:35 PM",
      "commitNameOld": "1e84e46f1621fe694f806bfc41d3b2a06c9500b6",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 0.8,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final long blocksize, final String excludeDatanodes,\n       final String remoteAddr, final HdfsFileStatus status) throws IOException {\n     FSNamesystem fsn \u003d namenode.getNamesystem();\n     if (fsn \u003d\u003d null) {\n-      throw new IOException(\"Namesystem has not been intialized yet.\");\n+      throw new IOException(\"Namesystem has not been initialized yet.\");\n     }\n     final BlockManager bm \u003d fsn.getBlockManager();\n     \n     HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n     if (excludeDatanodes !\u003d null) {\n       for (String host : StringUtils\n           .getTrimmedStringCollection(excludeDatanodes)) {\n         int idx \u003d host.indexOf(\":\");\n         if (idx !\u003d -1) {          \n           excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n               host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n         } else {\n           excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n         }\n       }\n     }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(remoteAddr);\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n             path, clientNode, excludes, blocksize);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n     }\n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT, excludes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes,\n      final String remoteAddr, final HdfsFileStatus status) throws IOException {\n    FSNamesystem fsn \u003d namenode.getNamesystem();\n    if (fsn \u003d\u003d null) {\n      throw new IOException(\"Namesystem has not been initialized yet.\");\n    }\n    final BlockManager bm \u003d fsn.getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        if (idx !\u003d -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(remoteAddr);\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    }\n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT, excludes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-12574. Add CryptoInputStream to WebHdfsFileSystem read call. Contributed by Rushabh S Shah\n",
      "commitDate": "29/01/18 3:23 PM",
      "commitName": "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-12574. Add CryptoInputStream to WebHdfsFileSystem read call. Contributed by Rushabh S Shah\n",
          "commitDate": "29/01/18 3:23 PM",
          "commitName": "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "08/01/18 2:25 PM",
          "commitNameOld": "2ee0d64aceed876f57f09eb9efe1872b6de98d2e",
          "commitAuthorOld": "Wei Yan",
          "daysBetweenCommits": 21.04,
          "commitsBetweenForRepo": 117,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,65 +1,64 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final long blocksize, final String excludeDatanodes,\n-      final String remoteAddr) throws IOException {\n+      final String remoteAddr, final HdfsFileStatus status) throws IOException {\n     FSNamesystem fsn \u003d namenode.getNamesystem();\n     if (fsn \u003d\u003d null) {\n       throw new IOException(\"Namesystem has not been intialized yet.\");\n     }\n     final BlockManager bm \u003d fsn.getBlockManager();\n     \n     HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n     if (excludeDatanodes !\u003d null) {\n       for (String host : StringUtils\n           .getTrimmedStringCollection(excludeDatanodes)) {\n         int idx \u003d host.indexOf(\":\");\n         if (idx !\u003d -1) {          \n           excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n               host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n         } else {\n           excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n         }\n       }\n     }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(remoteAddr);\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n             path, clientNode, excludes, blocksize);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n-      final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n-    } \n+    }\n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT, excludes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes,\n      final String remoteAddr, final HdfsFileStatus status) throws IOException {\n    FSNamesystem fsn \u003d namenode.getNamesystem();\n    if (fsn \u003d\u003d null) {\n      throw new IOException(\"Namesystem has not been intialized yet.\");\n    }\n    final BlockManager bm \u003d fsn.getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        if (idx !\u003d -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(remoteAddr);\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    }\n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT, excludes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {
            "oldValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), blocksize-long(modifiers-final), excludeDatanodes-String(modifiers-final), remoteAddr-String(modifiers-final)]",
            "newValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), blocksize-long(modifiers-final), excludeDatanodes-String(modifiers-final), remoteAddr-String(modifiers-final), status-HdfsFileStatus(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-12574. Add CryptoInputStream to WebHdfsFileSystem read call. Contributed by Rushabh S Shah\n",
          "commitDate": "29/01/18 3:23 PM",
          "commitName": "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "08/01/18 2:25 PM",
          "commitNameOld": "2ee0d64aceed876f57f09eb9efe1872b6de98d2e",
          "commitAuthorOld": "Wei Yan",
          "daysBetweenCommits": 21.04,
          "commitsBetweenForRepo": 117,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,65 +1,64 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final long blocksize, final String excludeDatanodes,\n-      final String remoteAddr) throws IOException {\n+      final String remoteAddr, final HdfsFileStatus status) throws IOException {\n     FSNamesystem fsn \u003d namenode.getNamesystem();\n     if (fsn \u003d\u003d null) {\n       throw new IOException(\"Namesystem has not been intialized yet.\");\n     }\n     final BlockManager bm \u003d fsn.getBlockManager();\n     \n     HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n     if (excludeDatanodes !\u003d null) {\n       for (String host : StringUtils\n           .getTrimmedStringCollection(excludeDatanodes)) {\n         int idx \u003d host.indexOf(\":\");\n         if (idx !\u003d -1) {          \n           excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n               host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n         } else {\n           excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n         }\n       }\n     }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(remoteAddr);\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n             path, clientNode, excludes, blocksize);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n-      final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n-    } \n+    }\n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT, excludes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes,\n      final String remoteAddr, final HdfsFileStatus status) throws IOException {\n    FSNamesystem fsn \u003d namenode.getNamesystem();\n    if (fsn \u003d\u003d null) {\n      throw new IOException(\"Namesystem has not been intialized yet.\");\n    }\n    final BlockManager bm \u003d fsn.getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        if (idx !\u003d -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(remoteAddr);\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    }\n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT, excludes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {}
        }
      ]
    },
    "1b5cceaffbdde50a87ede81552dc380832db8e79": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-11201. Spelling errors in the logging, help, assertions and exception messages. Contributed by Grant Sohn.\"\n\nThis reverts commit b9522e86a55564c2ccb5ca3f1ca871965cbe74de.\n",
      "commitDate": "05/12/16 10:54 AM",
      "commitName": "1b5cceaffbdde50a87ede81552dc380832db8e79",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "05/12/16 10:48 AM",
      "commitNameOld": "b9522e86a55564c2ccb5ca3f1ca871965cbe74de",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,65 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final long blocksize, final String excludeDatanodes,\n       final String remoteAddr) throws IOException {\n     FSNamesystem fsn \u003d namenode.getNamesystem();\n     if (fsn \u003d\u003d null) {\n-      throw new IOException(\"Namesystem has not been initialized yet.\");\n+      throw new IOException(\"Namesystem has not been intialized yet.\");\n     }\n     final BlockManager bm \u003d fsn.getBlockManager();\n     \n     HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n     if (excludeDatanodes !\u003d null) {\n       for (String host : StringUtils\n           .getTrimmedStringCollection(excludeDatanodes)) {\n         int idx \u003d host.indexOf(\":\");\n         if (idx !\u003d -1) {          \n           excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n               host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n         } else {\n           excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n         }\n       }\n     }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(remoteAddr);\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n             path, clientNode, excludes, blocksize);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT, excludes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes,\n      final String remoteAddr) throws IOException {\n    FSNamesystem fsn \u003d namenode.getNamesystem();\n    if (fsn \u003d\u003d null) {\n      throw new IOException(\"Namesystem has not been intialized yet.\");\n    }\n    final BlockManager bm \u003d fsn.getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        if (idx !\u003d -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(remoteAddr);\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT, excludes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "b9522e86a55564c2ccb5ca3f1ca871965cbe74de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11201. Spelling errors in the logging, help, assertions and exception messages. Contributed by Grant Sohn.\n",
      "commitDate": "05/12/16 10:48 AM",
      "commitName": "b9522e86a55564c2ccb5ca3f1ca871965cbe74de",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "02/12/16 11:10 AM",
      "commitNameOld": "c7ff34f8dcca3a2024230c5383abd9299daa1b20",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 2.98,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,65 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final long blocksize, final String excludeDatanodes,\n       final String remoteAddr) throws IOException {\n     FSNamesystem fsn \u003d namenode.getNamesystem();\n     if (fsn \u003d\u003d null) {\n-      throw new IOException(\"Namesystem has not been intialized yet.\");\n+      throw new IOException(\"Namesystem has not been initialized yet.\");\n     }\n     final BlockManager bm \u003d fsn.getBlockManager();\n     \n     HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n     if (excludeDatanodes !\u003d null) {\n       for (String host : StringUtils\n           .getTrimmedStringCollection(excludeDatanodes)) {\n         int idx \u003d host.indexOf(\":\");\n         if (idx !\u003d -1) {          \n           excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n               host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n         } else {\n           excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n         }\n       }\n     }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(remoteAddr);\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n             path, clientNode, excludes, blocksize);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT, excludes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes,\n      final String remoteAddr) throws IOException {\n    FSNamesystem fsn \u003d namenode.getNamesystem();\n    if (fsn \u003d\u003d null) {\n      throw new IOException(\"Namesystem has not been initialized yet.\");\n    }\n    final BlockManager bm \u003d fsn.getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        if (idx !\u003d -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(remoteAddr);\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT, excludes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "85cd06f6636f295ad1f3bf2a90063f4714c9cca7": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10789. Route webhdfs through the RPC call queue. Contributed by Daryn Sharp and Rushabh S Shah.\n",
      "commitDate": "12/10/16 1:11 PM",
      "commitName": "85cd06f6636f295ad1f3bf2a90063f4714c9cca7",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10789. Route webhdfs through the RPC call queue. Contributed by Daryn Sharp and Rushabh S Shah.\n",
          "commitDate": "12/10/16 1:11 PM",
          "commitName": "85cd06f6636f295ad1f3bf2a90063f4714c9cca7",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "06/09/16 11:02 AM",
          "commitNameOld": "f0d5382ff3e31a47d13e4cb6c3a244cca82b17ce",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 36.09,
          "commitsBetweenForRepo": 241,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,64 +1,65 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n-      final long blocksize, final String excludeDatanodes) throws IOException {\n+      final long blocksize, final String excludeDatanodes,\n+      final String remoteAddr) throws IOException {\n     FSNamesystem fsn \u003d namenode.getNamesystem();\n     if (fsn \u003d\u003d null) {\n       throw new IOException(\"Namesystem has not been intialized yet.\");\n     }\n     final BlockManager bm \u003d fsn.getBlockManager();\n     \n     HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n     if (excludeDatanodes !\u003d null) {\n       for (String host : StringUtils\n           .getTrimmedStringCollection(excludeDatanodes)) {\n         int idx \u003d host.indexOf(\":\");\n         if (idx !\u003d -1) {          \n           excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n               host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n         } else {\n           excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n         }\n       }\n     }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n-          ).getDatanodeByHost(getRemoteAddress());\n+          ).getDatanodeByHost(remoteAddr);\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n             path, clientNode, excludes, blocksize);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT, excludes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes,\n      final String remoteAddr) throws IOException {\n    FSNamesystem fsn \u003d namenode.getNamesystem();\n    if (fsn \u003d\u003d null) {\n      throw new IOException(\"Namesystem has not been intialized yet.\");\n    }\n    final BlockManager bm \u003d fsn.getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        if (idx !\u003d -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(remoteAddr);\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT, excludes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {
            "oldValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), blocksize-long(modifiers-final), excludeDatanodes-String(modifiers-final)]",
            "newValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), blocksize-long(modifiers-final), excludeDatanodes-String(modifiers-final), remoteAddr-String(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10789. Route webhdfs through the RPC call queue. Contributed by Daryn Sharp and Rushabh S Shah.\n",
          "commitDate": "12/10/16 1:11 PM",
          "commitName": "85cd06f6636f295ad1f3bf2a90063f4714c9cca7",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "06/09/16 11:02 AM",
          "commitNameOld": "f0d5382ff3e31a47d13e4cb6c3a244cca82b17ce",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 36.09,
          "commitsBetweenForRepo": 241,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,64 +1,65 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n-      final long blocksize, final String excludeDatanodes) throws IOException {\n+      final long blocksize, final String excludeDatanodes,\n+      final String remoteAddr) throws IOException {\n     FSNamesystem fsn \u003d namenode.getNamesystem();\n     if (fsn \u003d\u003d null) {\n       throw new IOException(\"Namesystem has not been intialized yet.\");\n     }\n     final BlockManager bm \u003d fsn.getBlockManager();\n     \n     HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n     if (excludeDatanodes !\u003d null) {\n       for (String host : StringUtils\n           .getTrimmedStringCollection(excludeDatanodes)) {\n         int idx \u003d host.indexOf(\":\");\n         if (idx !\u003d -1) {          \n           excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n               host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n         } else {\n           excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n         }\n       }\n     }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n-          ).getDatanodeByHost(getRemoteAddress());\n+          ).getDatanodeByHost(remoteAddr);\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n             path, clientNode, excludes, blocksize);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT, excludes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes,\n      final String remoteAddr) throws IOException {\n    FSNamesystem fsn \u003d namenode.getNamesystem();\n    if (fsn \u003d\u003d null) {\n      throw new IOException(\"Namesystem has not been intialized yet.\");\n    }\n    final BlockManager bm \u003d fsn.getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        if (idx !\u003d -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(remoteAddr);\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT, excludes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {}
        }
      ]
    },
    "1268cf5fbe4458fa75ad0662512d352f9e8d3470": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10320. Rack failures may result in NN terminate. (Xiao Chen via mingma)\n",
      "commitDate": "04/05/16 5:02 PM",
      "commitName": "1268cf5fbe4458fa75ad0662512d352f9e8d3470",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "11/12/15 10:59 AM",
      "commitNameOld": "576b569b6c97bd5f57e52efdabdf8c2fa996a524",
      "commitAuthorOld": "Allen Wittenauer",
      "daysBetweenCommits": 145.21,
      "commitsBetweenForRepo": 899,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final long blocksize, final String excludeDatanodes) throws IOException {\n     FSNamesystem fsn \u003d namenode.getNamesystem();\n     if (fsn \u003d\u003d null) {\n       throw new IOException(\"Namesystem has not been intialized yet.\");\n     }\n     final BlockManager bm \u003d fsn.getBlockManager();\n     \n     HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n     if (excludeDatanodes !\u003d null) {\n       for (String host : StringUtils\n           .getTrimmedStringCollection(excludeDatanodes)) {\n         int idx \u003d host.indexOf(\":\");\n         if (idx !\u003d -1) {          \n           excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n               host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n         } else {\n           excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n         }\n       }\n     }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(getRemoteAddress());\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n             path, clientNode, excludes, blocksize);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n-        ).chooseRandom(NodeBase.ROOT);\n+        ).chooseRandom(NodeBase.ROOT, excludes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes) throws IOException {\n    FSNamesystem fsn \u003d namenode.getNamesystem();\n    if (fsn \u003d\u003d null) {\n      throw new IOException(\"Namesystem has not been intialized yet.\");\n    }\n    final BlockManager bm \u003d fsn.getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        if (idx !\u003d -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT, excludes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "c4578760b67d5b5169949a1b059f4472a268ff1b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8290. WebHDFS calls before namesystem initialization can cause NullPointerException. Contributed by Chris Nauroth.\n",
      "commitDate": "04/05/15 11:35 AM",
      "commitName": "c4578760b67d5b5169949a1b059f4472a268ff1b",
      "commitAuthor": "cnauroth",
      "commitDateOld": "20/04/15 12:36 AM",
      "commitNameOld": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 14.46,
      "commitsBetweenForRepo": 134,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,64 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final long blocksize, final String excludeDatanodes) throws IOException {\n-    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n+    FSNamesystem fsn \u003d namenode.getNamesystem();\n+    if (fsn \u003d\u003d null) {\n+      throw new IOException(\"Namesystem has not been intialized yet.\");\n+    }\n+    final BlockManager bm \u003d fsn.getBlockManager();\n     \n     HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n     if (excludeDatanodes !\u003d null) {\n       for (String host : StringUtils\n           .getTrimmedStringCollection(excludeDatanodes)) {\n         int idx \u003d host.indexOf(\":\");\n         if (idx !\u003d -1) {          \n           excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n               host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n         } else {\n           excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n         }\n       }\n     }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(getRemoteAddress());\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n             path, clientNode, excludes, blocksize);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes) throws IOException {\n    FSNamesystem fsn \u003d namenode.getNamesystem();\n    if (fsn \u003d\u003d null) {\n      throw new IOException(\"Namesystem has not been intialized yet.\");\n    }\n    final BlockManager bm \u003d fsn.getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        if (idx !\u003d -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "7c18f8d55b899dc4a6e118d3b54447a9b36b960a": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6616. Add exclude-datanodes feature to WebHDFS redirection so that it will not redirect retries to the same datanode. Contributed by zhaoyunjiong\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1611750 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/07/14 11:20 AM",
      "commitName": "7c18f8d55b899dc4a6e118d3b54447a9b36b960a",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6616. Add exclude-datanodes feature to WebHDFS redirection so that it will not redirect retries to the same datanode. Contributed by zhaoyunjiong\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1611750 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/07/14 11:20 AM",
          "commitName": "7c18f8d55b899dc4a6e118d3b54447a9b36b960a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "05/06/14 8:48 PM",
          "commitNameOld": "6a4f6d6b3e41e660c214b77c9ed43d4c65200b6a",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 42.61,
          "commitsBetweenForRepo": 298,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,63 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n-      final long blocksize) throws IOException {\n+      final long blocksize, final String excludeDatanodes) throws IOException {\n     final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n+    \n+    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n+    if (excludeDatanodes !\u003d null) {\n+      for (String host : StringUtils\n+          .getTrimmedStringCollection(excludeDatanodes)) {\n+        int idx \u003d host.indexOf(\":\");\n+        if (idx !\u003d -1) {          \n+          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n+              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n+        } else {\n+          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n+        }\n+      }\n+    }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(getRemoteAddress());\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n             .chooseTarget(path, 1, clientNode,\n-                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n+                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, excludes, blocksize,\n                 // TODO: get storage type from the file\n                 StorageType.DEFAULT);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n-          return bestNode(locations.get(0).getLocations());\n+          return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes) throws IOException {\n    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        if (idx !\u003d -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n            .chooseTarget(path, 1, clientNode,\n                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, excludes, blocksize,\n                // TODO: get storage type from the file\n                StorageType.DEFAULT);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {
            "oldValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), blocksize-long(modifiers-final)]",
            "newValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), blocksize-long(modifiers-final), excludeDatanodes-String(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6616. Add exclude-datanodes feature to WebHDFS redirection so that it will not redirect retries to the same datanode. Contributed by zhaoyunjiong\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1611750 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/07/14 11:20 AM",
          "commitName": "7c18f8d55b899dc4a6e118d3b54447a9b36b960a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "05/06/14 8:48 PM",
          "commitNameOld": "6a4f6d6b3e41e660c214b77c9ed43d4c65200b6a",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 42.61,
          "commitsBetweenForRepo": 298,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,63 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n-      final long blocksize) throws IOException {\n+      final long blocksize, final String excludeDatanodes) throws IOException {\n     final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n+    \n+    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n+    if (excludeDatanodes !\u003d null) {\n+      for (String host : StringUtils\n+          .getTrimmedStringCollection(excludeDatanodes)) {\n+        int idx \u003d host.indexOf(\":\");\n+        if (idx !\u003d -1) {          \n+          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n+              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n+        } else {\n+          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n+        }\n+      }\n+    }\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(getRemoteAddress());\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n             .chooseTarget(path, 1, clientNode,\n-                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n+                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, excludes, blocksize,\n                 // TODO: get storage type from the file\n                 StorageType.DEFAULT);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d getRPCServer(namenode);\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n-          return bestNode(locations.get(0).getLocations());\n+          return bestNode(locations.get(0).getLocations(), excludes);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes) throws IOException {\n    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n    \n    HashSet\u003cNode\u003e excludes \u003d new HashSet\u003cNode\u003e();\n    if (excludeDatanodes !\u003d null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx \u003d host.indexOf(\":\");\n        if (idx !\u003d -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n            .chooseTarget(path, 1, clientNode,\n                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, excludes, blocksize,\n                // TODO: get storage type from the file\n                StorageType.DEFAULT);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {}
        }
      ]
    },
    "b46fbd0275bfc7ec16a219c72cff555d912170d7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5857. TestWebHDFS#testNamenodeRestart fails intermittently with NPE. Contributed By Mit Desai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1574683 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/03/14 1:51 PM",
      "commitName": "b46fbd0275bfc7ec16a219c72cff555d912170d7",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "25/02/14 10:36 AM",
      "commitNameOld": "df6e1ab4916e41810f092474a1f3abd9845d9956",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 8.14,
      "commitsBetweenForRepo": 84,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final long blocksize) throws IOException {\n     final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(getRemoteAddress());\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n             .chooseTarget(path, 1, clientNode,\n                 new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n                 // TODO: get storage type from the file\n                 StorageType.DEFAULT);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n-      final NamenodeProtocols np \u003d namenode.getRpcServer();\n+      final NamenodeProtocols np \u003d getRPCServer(namenode);\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return bestNode(locations.get(0).getLocations());\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize) throws IOException {\n    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n            .chooseTarget(path, 1, clientNode,\n                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n                // TODO: get storage type from the file\n                StorageType.DEFAULT);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d getRPCServer(namenode);\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations());\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "256adb2106cb838f3aff21f5a77f2973807d0df3": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5891. webhdfs should not try connecting the DN during redirection. Contributed by Haohui Mai\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567810 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 3:57 PM",
      "commitName": "256adb2106cb838f3aff21f5a77f2973807d0df3",
      "commitAuthor": "Brandon Li",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5891. webhdfs should not try connecting the DN during redirection. Contributed by Haohui Mai\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567810 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "12/02/14 3:57 PM",
          "commitName": "256adb2106cb838f3aff21f5a77f2973807d0df3",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "05/12/13 3:41 PM",
          "commitNameOld": "00718c2ffaa11cbdabac6f5ef4b2de5dcf9d6859",
          "commitAuthorOld": "",
          "daysBetweenCommits": 69.01,
          "commitsBetweenForRepo": 380,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,49 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n-      final long blocksize, final Configuration conf) throws IOException {\n+      final long blocksize) throws IOException {\n     final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(getRemoteAddress());\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n             .chooseTarget(path, 1, clientNode,\n                 new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n                 // TODO: get storage type from the file\n                 StorageType.DEFAULT);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n-          return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n+          return bestNode(locations.get(0).getLocations());\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize) throws IOException {\n    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n            .chooseTarget(path, 1, clientNode,\n                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n                // TODO: get storage type from the file\n                StorageType.DEFAULT);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations());\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {
            "oldValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), blocksize-long(modifiers-final), conf-Configuration(modifiers-final)]",
            "newValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), blocksize-long(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5891. webhdfs should not try connecting the DN during redirection. Contributed by Haohui Mai\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567810 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "12/02/14 3:57 PM",
          "commitName": "256adb2106cb838f3aff21f5a77f2973807d0df3",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "05/12/13 3:41 PM",
          "commitNameOld": "00718c2ffaa11cbdabac6f5ef4b2de5dcf9d6859",
          "commitAuthorOld": "",
          "daysBetweenCommits": 69.01,
          "commitsBetweenForRepo": 380,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,49 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n-      final long blocksize, final Configuration conf) throws IOException {\n+      final long blocksize) throws IOException {\n     final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(getRemoteAddress());\n       if (clientNode !\u003d null) {\n         final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n             .chooseTarget(path, 1, clientNode,\n                 new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n                 // TODO: get storage type from the file\n                 StorageType.DEFAULT);\n         if (storages.length \u003e 0) {\n           return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n-          return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n+          return bestNode(locations.get(0).getLocations());\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize) throws IOException {\n    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n            .chooseTarget(path, 1, clientNode,\n                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n                // TODO: get storage type from the file\n                StorageType.DEFAULT);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return bestNode(locations.get(0).getLocations());\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {}
        }
      ]
    },
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": {
      "type": "Ymultichange(Ybodychange,Yparametermetachange)",
      "commitMessage": "HDFS-4990. Change BlockPlacementPolicy to choose storages instead of datanodes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1524444 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/13 8:12 AM",
      "commitName": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4990. Change BlockPlacementPolicy to choose storages instead of datanodes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1524444 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/09/13 8:12 AM",
          "commitName": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "02/04/13 12:11 PM",
          "commitNameOld": "8293e225657a09b9352539d07ced67008976816a",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 168.83,
          "commitsBetweenForRepo": 1026,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,49 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n-      final long blocksize, Configuration conf) throws IOException {\n+      final long blocksize, final Configuration conf) throws IOException {\n     final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(getRemoteAddress());\n       if (clientNode !\u003d null) {\n-        final DatanodeDescriptor[] datanodes \u003d bm.getBlockPlacementPolicy()\n+        final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n             .chooseTarget(path, 1, clientNode,\n-                new ArrayList\u003cDatanodeDescriptor\u003e(), false, null, blocksize);\n-        if (datanodes.length \u003e 0) {\n-          return datanodes[0];\n+                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n+                // TODO: get storage type from the file\n+                StorageType.DEFAULT);\n+        if (storages.length \u003e 0) {\n+          return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final Configuration conf) throws IOException {\n    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n            .chooseTarget(path, 1, clientNode,\n                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n                // TODO: get storage type from the file\n                StorageType.DEFAULT);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HDFS-4990. Change BlockPlacementPolicy to choose storages instead of datanodes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1524444 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/09/13 8:12 AM",
          "commitName": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "02/04/13 12:11 PM",
          "commitNameOld": "8293e225657a09b9352539d07ced67008976816a",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 168.83,
          "commitsBetweenForRepo": 1026,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,49 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n-      final long blocksize, Configuration conf) throws IOException {\n+      final long blocksize, final Configuration conf) throws IOException {\n     final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(getRemoteAddress());\n       if (clientNode !\u003d null) {\n-        final DatanodeDescriptor[] datanodes \u003d bm.getBlockPlacementPolicy()\n+        final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n             .chooseTarget(path, 1, clientNode,\n-                new ArrayList\u003cDatanodeDescriptor\u003e(), false, null, blocksize);\n-        if (datanodes.length \u003e 0) {\n-          return datanodes[0];\n+                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n+                // TODO: get storage type from the file\n+                StorageType.DEFAULT);\n+        if (storages.length \u003e 0) {\n+          return storages[0].getDatanodeDescriptor();\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final Configuration conf) throws IOException {\n    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeStorageInfo[] storages \u003d bm.getBlockPlacementPolicy()\n            .chooseTarget(path, 1, clientNode,\n                new ArrayList\u003cDatanodeStorageInfo\u003e(), false, null, blocksize,\n                // TODO: get storage type from the file\n                StorageType.DEFAULT);\n        if (storages.length \u003e 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {
            "oldValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), blocksize-long(modifiers-final), conf-Configuration]",
            "newValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), blocksize-long(modifiers-final), conf-Configuration(modifiers-final)]"
          }
        }
      ]
    },
    "f3927595cc516381b1ae568e2d883a1d89993cbb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3887. Remove redundant chooseTarget methods in BlockPlacementPolicy.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1380934 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/12 4:48 PM",
      "commitName": "f3927595cc516381b1ae568e2d883a1d89993cbb",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/08/12 9:08 PM",
      "commitNameOld": "7d1c8d92f9b4b83c6ee154cd9ff70724bc61599f",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 4.82,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,47 @@\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final long blocksize, Configuration conf) throws IOException {\n     final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n \n     if (op \u003d\u003d PutOpParam.Op.CREATE) {\n       //choose a datanode near to client \n       final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n           ).getDatanodeByHost(getRemoteAddress());\n       if (clientNode !\u003d null) {\n-        final DatanodeDescriptor[] datanodes \u003d bm.getBlockPlacementPolicy(\n-            ).chooseTarget(path, 1, clientNode, null, blocksize);\n+        final DatanodeDescriptor[] datanodes \u003d bm.getBlockPlacementPolicy()\n+            .chooseTarget(path, 1, clientNode,\n+                new ArrayList\u003cDatanodeDescriptor\u003e(), false, null, blocksize);\n         if (datanodes.length \u003e 0) {\n           return datanodes[0];\n         }\n       }\n     } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n         ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, Configuration conf) throws IOException {\n    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeDescriptor[] datanodes \u003d bm.getBlockPlacementPolicy()\n            .chooseTarget(path, 1, clientNode,\n                new ArrayList\u003cDatanodeDescriptor\u003e(), false, null, blocksize);\n        if (datanodes.length \u003e 0) {\n          return datanodes[0];\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "bf1649d5fd095ce027f013be57d216212fa14198": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-3551. WebHDFS CREATE should use client location for HTTP redirection.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1354316 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/06/12 7:51 PM",
      "commitName": "bf1649d5fd095ce027f013be57d216212fa14198",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-3551. WebHDFS CREATE should use client location for HTTP redirection.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1354316 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "26/06/12 7:51 PM",
          "commitName": "bf1649d5fd095ce027f013be57d216212fa14198",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "08/06/12 5:54 PM",
          "commitNameOld": "5991ed9cbd18520040159508ef8bd02b7b3bf5e5",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 18.08,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,46 @@\n-  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n+  static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n-      Configuration conf) throws IOException {\n-    if (op \u003d\u003d GetOpParam.Op.OPEN\n+      final long blocksize, Configuration conf) throws IOException {\n+    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n+\n+    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n+      //choose a datanode near to client \n+      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n+          ).getDatanodeByHost(getRemoteAddress());\n+      if (clientNode !\u003d null) {\n+        final DatanodeDescriptor[] datanodes \u003d bm.getBlockPlacementPolicy(\n+            ).chooseTarget(path, 1, clientNode, null, blocksize);\n+        if (datanodes.length \u003e 0) {\n+          return datanodes[0];\n+        }\n+      }\n+    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n+      //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n-          return JspHelper.bestNode(locations.get(0), conf);\n+          return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n         }\n       }\n     } \n \n-    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n-        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n-        NodeBase.ROOT);\n+    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n+        ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, Configuration conf) throws IOException {\n    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeDescriptor[] datanodes \u003d bm.getBlockPlacementPolicy(\n            ).chooseTarget(path, 1, clientNode, null, blocksize);\n        if (datanodes.length \u003e 0) {\n          return datanodes[0];\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {
            "oldValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), conf-Configuration]",
            "newValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), blocksize-long(modifiers-final), conf-Configuration]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-3551. WebHDFS CREATE should use client location for HTTP redirection.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1354316 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "26/06/12 7:51 PM",
          "commitName": "bf1649d5fd095ce027f013be57d216212fa14198",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "08/06/12 5:54 PM",
          "commitNameOld": "5991ed9cbd18520040159508ef8bd02b7b3bf5e5",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 18.08,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,46 @@\n-  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n+  static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n-      Configuration conf) throws IOException {\n-    if (op \u003d\u003d GetOpParam.Op.OPEN\n+      final long blocksize, Configuration conf) throws IOException {\n+    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n+\n+    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n+      //choose a datanode near to client \n+      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n+          ).getDatanodeByHost(getRemoteAddress());\n+      if (clientNode !\u003d null) {\n+        final DatanodeDescriptor[] datanodes \u003d bm.getBlockPlacementPolicy(\n+            ).chooseTarget(path, 1, clientNode, null, blocksize);\n+        if (datanodes.length \u003e 0) {\n+          return datanodes[0];\n+        }\n+      }\n+    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n+      //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n-          return JspHelper.bestNode(locations.get(0), conf);\n+          return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n         }\n       }\n     } \n \n-    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n-        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n-        NodeBase.ROOT);\n+    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n+        ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, Configuration conf) throws IOException {\n    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeDescriptor[] datanodes \u003d bm.getBlockPlacementPolicy(\n            ).chooseTarget(path, 1, clientNode, null, blocksize);\n        if (datanodes.length \u003e 0) {\n          return datanodes[0];\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {
            "oldValue": "[private, static]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3551. WebHDFS CREATE should use client location for HTTP redirection.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1354316 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "26/06/12 7:51 PM",
          "commitName": "bf1649d5fd095ce027f013be57d216212fa14198",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "08/06/12 5:54 PM",
          "commitNameOld": "5991ed9cbd18520040159508ef8bd02b7b3bf5e5",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 18.08,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,46 @@\n-  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n+  static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n-      Configuration conf) throws IOException {\n-    if (op \u003d\u003d GetOpParam.Op.OPEN\n+      final long blocksize, Configuration conf) throws IOException {\n+    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n+\n+    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n+      //choose a datanode near to client \n+      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n+          ).getDatanodeByHost(getRemoteAddress());\n+      if (clientNode !\u003d null) {\n+        final DatanodeDescriptor[] datanodes \u003d bm.getBlockPlacementPolicy(\n+            ).chooseTarget(path, 1, clientNode, null, blocksize);\n+        if (datanodes.length \u003e 0) {\n+          return datanodes[0];\n+        }\n+      }\n+    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n+      //choose a datanode containing a replica \n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN) {\n         if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n           throw new IOException(\"Offset\u003d\" + openOffset\n               + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n         }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n-          return JspHelper.bestNode(locations.get(0), conf);\n+          return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n         }\n       }\n     } \n \n-    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n-        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n-        NodeBase.ROOT);\n+    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n+        ).chooseRandom(NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, Configuration conf) throws IOException {\n    final BlockManager bm \u003d namenode.getNamesystem().getBlockManager();\n\n    if (op \u003d\u003d PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode \u003d bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode !\u003d null) {\n        final DatanodeDescriptor[] datanodes \u003d bm.getBlockPlacementPolicy(\n            ).chooseTarget(path, 1, clientNode, null, blocksize);\n        if (datanodes.length \u003e 0) {\n          return datanodes[0];\n        }\n      }\n    } else if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0).getLocations(), false, conf);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {}
        }
      ]
    },
    "7e5b60116eede44d1cb5b6a263b4fec13da3c473": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3101. Cannot read empty file using WebHDFS.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301287 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/03/12 5:01 PM",
      "commitName": "7e5b60116eede44d1cb5b6a263b4fec13da3c473",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "24/02/12 4:15 PM",
      "commitNameOld": "dacbeb5f6711b83bd293928b5329f7b846f2e66e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 19.99,
      "commitsBetweenForRepo": 138,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,33 @@\n   private static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       Configuration conf) throws IOException {\n     if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n-      if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n-        throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n-          + len + \"); \" + op + \", path\u003d\" + path);\n+      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n+        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n+          throw new IOException(\"Offset\u003d\" + openOffset\n+              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n+        }\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return JspHelper.bestNode(locations.get(0), conf);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n         ).getDatanodeManager().getNetworkTopology().chooseRandom(\n         NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      Configuration conf) throws IOException {\n    if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN) {\n        if (openOffset \u003c 0L || (openOffset \u003e\u003d len \u0026\u0026 len \u003e 0)) {\n          throw new IOException(\"Offset\u003d\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path\u003d\" + path);\n        }\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0), conf);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n        NodeBase.ROOT);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "40fe96546fcd68696076db67053f30d38a39a0d5": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-2129. Simplify BlockReader to not inherit from FSInputChecker. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196976 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/11/11 11:54 PM",
      "commitName": "40fe96546fcd68696076db67053f30d38a39a0d5",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2129. Simplify BlockReader to not inherit from FSInputChecker. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196976 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "02/11/11 11:54 PM",
          "commitName": "40fe96546fcd68696076db67053f30d38a39a0d5",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "01/11/11 7:08 PM",
          "commitNameOld": "bd21ddcb78350b311f271e233038b8ca78a65242",
          "commitAuthorOld": "Jitendra Nath Pandey",
          "daysBetweenCommits": 1.2,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,31 @@\n   private static DatanodeInfo chooseDatanode(final NameNode namenode,\n-      final String path, final HttpOpParam.Op op, final long openOffset\n-      ) throws IOException {\n+      final String path, final HttpOpParam.Op op, final long openOffset,\n+      Configuration conf) throws IOException {\n     if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n         throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n           + len + \"); \" + op + \", path\u003d\" + path);\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n-          return JspHelper.bestNode(locations.get(0));\n+          return JspHelper.bestNode(locations.get(0), conf);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n         ).getDatanodeManager().getNetworkTopology().chooseRandom(\n         NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      Configuration conf) throws IOException {\n    if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n        throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n          + len + \"); \" + op + \", path\u003d\" + path);\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0), conf);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n        NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {
            "oldValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final)]",
            "newValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final), conf-Configuration]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2129. Simplify BlockReader to not inherit from FSInputChecker. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196976 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "02/11/11 11:54 PM",
          "commitName": "40fe96546fcd68696076db67053f30d38a39a0d5",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "01/11/11 7:08 PM",
          "commitNameOld": "bd21ddcb78350b311f271e233038b8ca78a65242",
          "commitAuthorOld": "Jitendra Nath Pandey",
          "daysBetweenCommits": 1.2,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,31 @@\n   private static DatanodeInfo chooseDatanode(final NameNode namenode,\n-      final String path, final HttpOpParam.Op op, final long openOffset\n-      ) throws IOException {\n+      final String path, final HttpOpParam.Op op, final long openOffset,\n+      Configuration conf) throws IOException {\n     if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       if (status \u003d\u003d null) {\n         throw new FileNotFoundException(\"File \" + path + \" not found.\");\n       }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n         throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n           + len + \"); \" + op + \", path\u003d\" + path);\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n-          return JspHelper.bestNode(locations.get(0));\n+          return JspHelper.bestNode(locations.get(0), conf);\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n         ).getDatanodeManager().getNetworkTopology().chooseRandom(\n         NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      Configuration conf) throws IOException {\n    if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n        throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n          + len + \"); \" + op + \", path\u003d\" + path);\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0), conf);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n        NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {}
        }
      ]
    },
    "50cb2771e924d2d6d9d04e588cb0a94aefb25b70": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2439. Fix NullPointerException in webhdfs when opening a non-existing file or creating a file without specifying the replication parameter.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1183554 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/10/11 4:25 PM",
      "commitName": "50cb2771e924d2d6d9d04e588cb0a94aefb25b70",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "13/10/11 5:30 PM",
      "commitNameOld": "c46dbedaf94cb72e35e9e63d7f99e382ae9f0974",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.95,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,31 @@\n   private static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset\n       ) throws IOException {\n     if (op \u003d\u003d GetOpParam.Op.OPEN\n         || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n         || op \u003d\u003d PostOpParam.Op.APPEND) {\n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n+      if (status \u003d\u003d null) {\n+        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n+      }\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n         throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n           + len + \"); \" + op + \", path\u003d\" + path);\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return JspHelper.bestNode(locations.get(0));\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n         ).getDatanodeManager().getNetworkTopology().chooseRandom(\n         NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset\n      ) throws IOException {\n    if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      if (status \u003d\u003d null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n        throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n          + len + \"); \" + op + \", path\u003d\" + path);\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0));\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n        NodeBase.ROOT);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "dc8464f943b61b795df0cc8baec171bf07355763": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2348. Support getContentSummary and getFileChecksum in webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1177905 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/09/11 9:49 PM",
      "commitName": "dc8464f943b61b795df0cc8baec171bf07355763",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/09/11 11:01 AM",
      "commitNameOld": "f48280ac56108ccb1c2903012542e2ae982647da",
      "commitAuthorOld": "Doug Cutting",
      "daysBetweenCommits": 0.45,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,28 @@\n   private static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset\n       ) throws IOException {\n-    if (op \u003d\u003d GetOpParam.Op.OPEN || op \u003d\u003d PostOpParam.Op.APPEND) {\n+    if (op \u003d\u003d GetOpParam.Op.OPEN\n+        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n+        || op \u003d\u003d PostOpParam.Op.APPEND) {\n       final NamenodeProtocols np \u003d namenode.getRpcServer();\n       final HdfsFileStatus status \u003d np.getFileInfo(path);\n       final long len \u003d status.getLen();\n       if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n         throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n           + len + \"); \" + op + \", path\u003d\" + path);\n       }\n \n       if (len \u003e 0) {\n         final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n         final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n           return JspHelper.bestNode(locations.get(0));\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n         ).getDatanodeManager().getNetworkTopology().chooseRandom(\n         NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset\n      ) throws IOException {\n    if (op \u003d\u003d GetOpParam.Op.OPEN\n        || op \u003d\u003d GetOpParam.Op.GETFILECHECKSUM\n        || op \u003d\u003d PostOpParam.Op.APPEND) {\n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n        throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n          + len + \"); \" + op + \", path\u003d\" + path);\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0));\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n        NodeBase.ROOT);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "6c3b59505b863f03629da52a1e9b886fe9b496d0": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-2317. Support read access to HDFS in webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1170085 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/09/11 1:34 AM",
      "commitName": "6c3b59505b863f03629da52a1e9b886fe9b496d0",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2317. Support read access to HDFS in webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1170085 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/09/11 1:34 AM",
          "commitName": "6c3b59505b863f03629da52a1e9b886fe9b496d0",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "10/09/11 6:41 PM",
          "commitNameOld": "61d0b7530c8978c095ab6f62d9d38e168bd829c6",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.29,
          "commitsBetweenForRepo": 14,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,26 @@\n   private static DatanodeInfo chooseDatanode(final NameNode namenode,\n-      final String path, final HttpOpParam.Op op) throws IOException {\n-    if (op \u003d\u003d PostOpParam.Op.APPEND) {\n-      final HdfsFileStatus status \u003d namenode.getRpcServer().getFileInfo(path);\n+      final String path, final HttpOpParam.Op op, final long openOffset\n+      ) throws IOException {\n+    if (op \u003d\u003d GetOpParam.Op.OPEN || op \u003d\u003d PostOpParam.Op.APPEND) {\n+      final NamenodeProtocols np \u003d namenode.getRpcServer();\n+      final HdfsFileStatus status \u003d np.getFileInfo(path);\n       final long len \u003d status.getLen();\n+      if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n+        throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n+          + len + \"); \" + op + \", path\u003d\" + path);\n+      }\n+\n       if (len \u003e 0) {\n-        final LocatedBlocks locations \u003d namenode.getRpcServer().getBlockLocations(path, len-1, 1);\n+        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n+        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n-          return JspHelper.bestNode(locations.get(count - 1));\n+          return JspHelper.bestNode(locations.get(0));\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n         ).getDatanodeManager().getNetworkTopology().chooseRandom(\n         NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset\n      ) throws IOException {\n    if (op \u003d\u003d GetOpParam.Op.OPEN || op \u003d\u003d PostOpParam.Op.APPEND) {\n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n        throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n          + len + \"); \" + op + \", path\u003d\" + path);\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0));\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n        NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {
            "oldValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final)]",
            "newValue": "[namenode-NameNode(modifiers-final), path-String(modifiers-final), op-HttpOpParam.Op(modifiers-final), openOffset-long(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2317. Support read access to HDFS in webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1170085 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/09/11 1:34 AM",
          "commitName": "6c3b59505b863f03629da52a1e9b886fe9b496d0",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "10/09/11 6:41 PM",
          "commitNameOld": "61d0b7530c8978c095ab6f62d9d38e168bd829c6",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.29,
          "commitsBetweenForRepo": 14,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,26 @@\n   private static DatanodeInfo chooseDatanode(final NameNode namenode,\n-      final String path, final HttpOpParam.Op op) throws IOException {\n-    if (op \u003d\u003d PostOpParam.Op.APPEND) {\n-      final HdfsFileStatus status \u003d namenode.getRpcServer().getFileInfo(path);\n+      final String path, final HttpOpParam.Op op, final long openOffset\n+      ) throws IOException {\n+    if (op \u003d\u003d GetOpParam.Op.OPEN || op \u003d\u003d PostOpParam.Op.APPEND) {\n+      final NamenodeProtocols np \u003d namenode.getRpcServer();\n+      final HdfsFileStatus status \u003d np.getFileInfo(path);\n       final long len \u003d status.getLen();\n+      if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n+        throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n+          + len + \"); \" + op + \", path\u003d\" + path);\n+      }\n+\n       if (len \u003e 0) {\n-        final LocatedBlocks locations \u003d namenode.getRpcServer().getBlockLocations(path, len-1, 1);\n+        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n+        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n         final int count \u003d locations.locatedBlockCount();\n         if (count \u003e 0) {\n-          return JspHelper.bestNode(locations.get(count - 1));\n+          return JspHelper.bestNode(locations.get(0));\n         }\n       }\n     } \n \n     return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n         ).getDatanodeManager().getNetworkTopology().chooseRandom(\n         NodeBase.ROOT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset\n      ) throws IOException {\n    if (op \u003d\u003d GetOpParam.Op.OPEN || op \u003d\u003d PostOpParam.Op.APPEND) {\n      final NamenodeProtocols np \u003d namenode.getRpcServer();\n      final HdfsFileStatus status \u003d np.getFileInfo(path);\n      final long len \u003d status.getLen();\n      if (op \u003d\u003d GetOpParam.Op.OPEN \u0026\u0026 (openOffset \u003c 0L || openOffset \u003e\u003d len)) {\n        throw new IOException(\"Offset\u003d\" + openOffset + \" out of the range [0, \"\n          + len + \"); \" + op + \", path\u003d\" + path);\n      }\n\n      if (len \u003e 0) {\n        final long offset \u003d op \u003d\u003d GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations \u003d np.getBlockLocations(path, offset, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(0));\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n        NodeBase.ROOT);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
          "extendedDetails": {}
        }
      ]
    },
    "61d0b7530c8978c095ab6f62d9d38e168bd829c6": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2284. Add a new FileSystem, webhdfs://, for supporting write Http access to HDFS.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1167662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/09/11 6:41 PM",
      "commitName": "61d0b7530c8978c095ab6f62d9d38e168bd829c6",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,18 @@\n+  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n+      final String path, final HttpOpParam.Op op) throws IOException {\n+    if (op \u003d\u003d PostOpParam.Op.APPEND) {\n+      final HdfsFileStatus status \u003d namenode.getRpcServer().getFileInfo(path);\n+      final long len \u003d status.getLen();\n+      if (len \u003e 0) {\n+        final LocatedBlocks locations \u003d namenode.getRpcServer().getBlockLocations(path, len-1, 1);\n+        final int count \u003d locations.locatedBlockCount();\n+        if (count \u003e 0) {\n+          return JspHelper.bestNode(locations.get(count - 1));\n+        }\n+      }\n+    } \n+\n+    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n+        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n+        NodeBase.ROOT);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op) throws IOException {\n    if (op \u003d\u003d PostOpParam.Op.APPEND) {\n      final HdfsFileStatus status \u003d namenode.getRpcServer().getFileInfo(path);\n      final long len \u003d status.getLen();\n      if (len \u003e 0) {\n        final LocatedBlocks locations \u003d namenode.getRpcServer().getBlockLocations(path, len-1, 1);\n        final int count \u003d locations.locatedBlockCount();\n        if (count \u003e 0) {\n          return JspHelper.bestNode(locations.get(count - 1));\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)namenode.getNamesystem().getBlockManager(\n        ).getDatanodeManager().getNetworkTopology().chooseRandom(\n        NodeBase.ROOT);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java"
    }
  }
}