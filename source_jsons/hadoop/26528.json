{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "TaskAttemptImpl.java",
  "functionName": "createContainerLaunchContext",
  "functionId": "createContainerLaunchContext___applicationACLs-Map__ApplicationAccessType,String____conf-Configuration__jobToken-Token__JobTokenIdentifier____remoteTask-Task__oldJobId-org.apache.hadoop.mapred.JobID(modifiers-final)__jvmID-WrappedJvmID__taskAttemptListener-TaskAttemptListener__credentials-Credentials",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
  "functionStartLine": 1100,
  "functionEndLine": 1148,
  "numCommitsSeen": 240,
  "timeTaken": 9882,
  "changeHistory": [
    "4102e5882e17b75507ae5cf8b8979485b3e24cbc",
    "643155cbee54809e1a7febd96cbb7d8111689b38",
    "259edf8dca44de54033e96f7eb65a83aaa6096f2",
    "1a119f87b4f0a78d56e1bb998b1cbc081484fbd1",
    "e4c55e17fea55e2fcbef182bb2b0c4b22686f38c",
    "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05",
    "0870734787d7005d85697549eab5b6479d97d453",
    "df2991c0cbc3f35c2640b93680667507c4f810dd",
    "c1d90772b6e38bb4e4be7ed75cb5d34f3048ad7b",
    "d00b3c49f6fb3f6a617add6203c6b55f6c345940",
    "88b82a0f6687ce103817fbb460fd30d870f717a0",
    "6165875dc6bf67d72fc3ce1d96dfc80ba312d4a1",
    "ade0f0560f729e50382c6992f713f29e2dd5b270",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517"
  ],
  "changeHistoryShort": {
    "4102e5882e17b75507ae5cf8b8979485b3e24cbc": "Ybodychange",
    "643155cbee54809e1a7febd96cbb7d8111689b38": "Ybodychange",
    "259edf8dca44de54033e96f7eb65a83aaa6096f2": "Ybodychange",
    "1a119f87b4f0a78d56e1bb998b1cbc081484fbd1": "Ybodychange",
    "e4c55e17fea55e2fcbef182bb2b0c4b22686f38c": "Ymultichange(Yparameterchange,Ybodychange)",
    "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05": "Ymultichange(Yparameterchange,Ybodychange)",
    "0870734787d7005d85697549eab5b6479d97d453": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "df2991c0cbc3f35c2640b93680667507c4f810dd": "Ymultichange(Yparameterchange,Ybodychange)",
    "c1d90772b6e38bb4e4be7ed75cb5d34f3048ad7b": "Ybodychange",
    "d00b3c49f6fb3f6a617add6203c6b55f6c345940": "Ybodychange",
    "88b82a0f6687ce103817fbb460fd30d870f717a0": "Ybodychange",
    "6165875dc6bf67d72fc3ce1d96dfc80ba312d4a1": "Ybodychange",
    "ade0f0560f729e50382c6992f713f29e2dd5b270": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yintroduced"
  },
  "changeHistoryDetails": {
    "4102e5882e17b75507ae5cf8b8979485b3e24cbc": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3626. On Windows localized resources are not moved to the front of the classpath when they should be. Contributed by Craig Welch.\n",
      "commitDate": "27/05/15 2:31 PM",
      "commitName": "4102e5882e17b75507ae5cf8b8979485b3e24cbc",
      "commitAuthor": "cnauroth",
      "commitDateOld": "11/05/15 3:37 PM",
      "commitNameOld": "444836b3dcd3ee28238af7b5e753d644e8095788",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 15.95,
      "commitsBetweenForRepo": 123,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,49 @@\n   static ContainerLaunchContext createContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs,\n       Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       WrappedJvmID jvmID,\n       TaskAttemptListener taskAttemptListener,\n       Credentials credentials) {\n \n     synchronized (commonContainerSpecLock) {\n       if (commonContainerSpec \u003d\u003d null) {\n         commonContainerSpec \u003d createCommonContainerLaunchContext(\n             applicationACLs, conf, jobToken, oldJobId, credentials);\n       }\n     }\n \n     // Fill in the fields needed per-container that are missing in the common\n     // spec.\n \n+    boolean userClassesTakesPrecedence \u003d\n+      conf.getBoolean(MRJobConfig.MAPREDUCE_JOB_USER_CLASSPATH_FIRST, false);\n+\n     // Setup environment by cloning from common env.\n     Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n     Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n     myEnv.putAll(env);\n+    if (userClassesTakesPrecedence) {\n+      myEnv.put(Environment.CLASSPATH_PREPEND_DISTCACHE.name(), \"true\");\n+    }\n     MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, jvmID);\n \n     // Duplicate the ByteBuffers for access by multiple containers.\n     Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n     for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                 .getServiceData().entrySet()) {\n       myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n     }\n \n     // Construct the actual Container\n     ContainerLaunchContext container \u003d ContainerLaunchContext.newInstance(\n         commonContainerSpec.getLocalResources(), myEnv, commands,\n         myServiceData, commonContainerSpec.getTokens().duplicate(),\n         applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n      Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Credentials credentials) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec \u003d\u003d null) {\n        commonContainerSpec \u003d createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, credentials);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    boolean userClassesTakesPrecedence \u003d\n      conf.getBoolean(MRJobConfig.MAPREDUCE_JOB_USER_CLASSPATH_FIRST, false);\n\n    // Setup environment by cloning from common env.\n    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n    myEnv.putAll(env);\n    if (userClassesTakesPrecedence) {\n      myEnv.put(Environment.CLASSPATH_PREPEND_DISTCACHE.name(), \"true\");\n    }\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container \u003d ContainerLaunchContext.newInstance(\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "643155cbee54809e1a7febd96cbb7d8111689b38": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5270. Migrated MR app from using BuilderUtil factory methods to individual record factory methods. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1486271 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/05/13 6:46 PM",
      "commitName": "643155cbee54809e1a7febd96cbb7d8111689b38",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "23/05/13 8:22 PM",
      "commitNameOld": "259edf8dca44de54033e96f7eb65a83aaa6096f2",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 0.93,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n   static ContainerLaunchContext createContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs,\n       Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       WrappedJvmID jvmID,\n       TaskAttemptListener taskAttemptListener,\n       Credentials credentials) {\n \n     synchronized (commonContainerSpecLock) {\n       if (commonContainerSpec \u003d\u003d null) {\n         commonContainerSpec \u003d createCommonContainerLaunchContext(\n             applicationACLs, conf, jobToken, oldJobId, credentials);\n       }\n     }\n \n     // Fill in the fields needed per-container that are missing in the common\n     // spec.\n \n     // Setup environment by cloning from common env.\n     Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n     Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n     myEnv.putAll(env);\n     MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, jvmID);\n \n     // Duplicate the ByteBuffers for access by multiple containers.\n     Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n     for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                 .getServiceData().entrySet()) {\n       myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n     }\n \n     // Construct the actual Container\n-    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n+    ContainerLaunchContext container \u003d ContainerLaunchContext.newInstance(\n         commonContainerSpec.getLocalResources(), myEnv, commands,\n         myServiceData, commonContainerSpec.getTokens().duplicate(),\n         applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n      Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Credentials credentials) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec \u003d\u003d null) {\n        commonContainerSpec \u003d createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, credentials);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    // Setup environment by cloning from common env.\n    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n    myEnv.putAll(env);\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container \u003d ContainerLaunchContext.newInstance(\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "259edf8dca44de54033e96f7eb65a83aaa6096f2": {
      "type": "Ybodychange",
      "commitMessage": "YARN-571. Remove user from ContainerLaunchContext. Contributed by Omkar Vinit Joshi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1485928 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/05/13 8:22 PM",
      "commitName": "259edf8dca44de54033e96f7eb65a83aaa6096f2",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "13/05/13 9:11 PM",
      "commitNameOld": "1a119f87b4f0a78d56e1bb998b1cbc081484fbd1",
      "commitAuthorOld": "Siddharth Seth",
      "daysBetweenCommits": 9.97,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,43 @@\n   static ContainerLaunchContext createContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs,\n       Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       WrappedJvmID jvmID,\n       TaskAttemptListener taskAttemptListener,\n       Credentials credentials) {\n \n     synchronized (commonContainerSpecLock) {\n       if (commonContainerSpec \u003d\u003d null) {\n         commonContainerSpec \u003d createCommonContainerLaunchContext(\n             applicationACLs, conf, jobToken, oldJobId, credentials);\n       }\n     }\n \n     // Fill in the fields needed per-container that are missing in the common\n     // spec.\n \n     // Setup environment by cloning from common env.\n     Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n     Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n     myEnv.putAll(env);\n     MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, jvmID);\n \n     // Duplicate the ByteBuffers for access by multiple containers.\n     Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n     for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                 .getServiceData().entrySet()) {\n       myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n     }\n \n     // Construct the actual Container\n     ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n-        commonContainerSpec.getUser(),\n         commonContainerSpec.getLocalResources(), myEnv, commands,\n         myServiceData, commonContainerSpec.getTokens().duplicate(),\n         applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n      Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Credentials credentials) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec \u003d\u003d null) {\n        commonContainerSpec \u003d createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, credentials);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    // Setup environment by cloning from common env.\n    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n    myEnv.putAll(env);\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "1a119f87b4f0a78d56e1bb998b1cbc081484fbd1": {
      "type": "Ybodychange",
      "commitMessage": "YARN-615. Rename ContainerLaunchContext.containerTokens to tokens. Contributed by Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1482199 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/05/13 9:11 PM",
      "commitName": "1a119f87b4f0a78d56e1bb998b1cbc081484fbd1",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "25/04/13 8:50 PM",
      "commitNameOld": "fbb55784d93e1a819daf55d936e864d344579cbf",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 18.01,
      "commitsBetweenForRepo": 118,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,44 @@\n   static ContainerLaunchContext createContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs,\n       Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       WrappedJvmID jvmID,\n       TaskAttemptListener taskAttemptListener,\n       Credentials credentials) {\n \n     synchronized (commonContainerSpecLock) {\n       if (commonContainerSpec \u003d\u003d null) {\n         commonContainerSpec \u003d createCommonContainerLaunchContext(\n             applicationACLs, conf, jobToken, oldJobId, credentials);\n       }\n     }\n \n     // Fill in the fields needed per-container that are missing in the common\n     // spec.\n \n     // Setup environment by cloning from common env.\n     Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n     Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n     myEnv.putAll(env);\n     MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, jvmID);\n \n     // Duplicate the ByteBuffers for access by multiple containers.\n     Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n     for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                 .getServiceData().entrySet()) {\n       myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n     }\n \n     // Construct the actual Container\n     ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n         commonContainerSpec.getUser(),\n         commonContainerSpec.getLocalResources(), myEnv, commands,\n-        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n+        myServiceData, commonContainerSpec.getTokens().duplicate(),\n         applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n      Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Credentials credentials) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec \u003d\u003d null) {\n        commonContainerSpec \u003d createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, credentials);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    // Setup environment by cloning from common env.\n    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n    myEnv.putAll(env);\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n        commonContainerSpec.getUser(),\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "e4c55e17fea55e2fcbef182bb2b0c4b22686f38c": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-486. Changed NM\u0027s startContainer API to accept Container record given by RM as a direct parameter instead of as part of the ContainerLaunchContext record. Contributed by Xuan Gong.\nMAPREDUCE-5139. Update MR AM to use the modified startContainer API after YARN-486. Contributed by Xuan Gong.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1467063 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/04/13 12:28 PM",
      "commitName": "e4c55e17fea55e2fcbef182bb2b0c4b22686f38c",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-486. Changed NM\u0027s startContainer API to accept Container record given by RM as a direct parameter instead of as part of the ContainerLaunchContext record. Contributed by Xuan Gong.\nMAPREDUCE-5139. Update MR AM to use the modified startContainer API after YARN-486. Contributed by Xuan Gong.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1467063 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/04/13 12:28 PM",
          "commitName": "e4c55e17fea55e2fcbef182bb2b0c4b22686f38c",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "10/04/13 9:52 PM",
          "commitNameOld": "6a1c41111edcdc58c846fc50e53554fbba230171",
          "commitAuthorOld": "Siddharth Seth",
          "daysBetweenCommits": 0.61,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,44 @@\n   static ContainerLaunchContext createContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs,\n-      ContainerId containerID, Configuration conf,\n-      Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n+      Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n-      Resource assignedCapability, WrappedJvmID jvmID,\n+      WrappedJvmID jvmID,\n       TaskAttemptListener taskAttemptListener,\n       Credentials credentials) {\n \n     synchronized (commonContainerSpecLock) {\n       if (commonContainerSpec \u003d\u003d null) {\n         commonContainerSpec \u003d createCommonContainerLaunchContext(\n             applicationACLs, conf, jobToken, oldJobId, credentials);\n       }\n     }\n \n     // Fill in the fields needed per-container that are missing in the common\n     // spec.\n \n     // Setup environment by cloning from common env.\n     Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n     Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n     myEnv.putAll(env);\n     MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, jvmID);\n \n     // Duplicate the ByteBuffers for access by multiple containers.\n     Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n     for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                 .getServiceData().entrySet()) {\n       myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n     }\n \n     // Construct the actual Container\n     ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n-        containerID, commonContainerSpec.getUser(), assignedCapability,\n+        commonContainerSpec.getUser(),\n         commonContainerSpec.getLocalResources(), myEnv, commands,\n         myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n         applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n      Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Credentials credentials) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec \u003d\u003d null) {\n        commonContainerSpec \u003d createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, credentials);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    // Setup environment by cloning from common env.\n    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n    myEnv.putAll(env);\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n        commonContainerSpec.getUser(),\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {
            "oldValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e, containerID-ContainerId, conf-Configuration, jobToken-Token\u003cJobTokenIdentifier\u003e, remoteTask-Task, oldJobId-org.apache.hadoop.mapred.JobID(modifiers-final), assignedCapability-Resource, jvmID-WrappedJvmID, taskAttemptListener-TaskAttemptListener, credentials-Credentials]",
            "newValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e, conf-Configuration, jobToken-Token\u003cJobTokenIdentifier\u003e, remoteTask-Task, oldJobId-org.apache.hadoop.mapred.JobID(modifiers-final), jvmID-WrappedJvmID, taskAttemptListener-TaskAttemptListener, credentials-Credentials]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-486. Changed NM\u0027s startContainer API to accept Container record given by RM as a direct parameter instead of as part of the ContainerLaunchContext record. Contributed by Xuan Gong.\nMAPREDUCE-5139. Update MR AM to use the modified startContainer API after YARN-486. Contributed by Xuan Gong.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1467063 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/04/13 12:28 PM",
          "commitName": "e4c55e17fea55e2fcbef182bb2b0c4b22686f38c",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "10/04/13 9:52 PM",
          "commitNameOld": "6a1c41111edcdc58c846fc50e53554fbba230171",
          "commitAuthorOld": "Siddharth Seth",
          "daysBetweenCommits": 0.61,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,44 @@\n   static ContainerLaunchContext createContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs,\n-      ContainerId containerID, Configuration conf,\n-      Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n+      Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n-      Resource assignedCapability, WrappedJvmID jvmID,\n+      WrappedJvmID jvmID,\n       TaskAttemptListener taskAttemptListener,\n       Credentials credentials) {\n \n     synchronized (commonContainerSpecLock) {\n       if (commonContainerSpec \u003d\u003d null) {\n         commonContainerSpec \u003d createCommonContainerLaunchContext(\n             applicationACLs, conf, jobToken, oldJobId, credentials);\n       }\n     }\n \n     // Fill in the fields needed per-container that are missing in the common\n     // spec.\n \n     // Setup environment by cloning from common env.\n     Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n     Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n     myEnv.putAll(env);\n     MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, jvmID);\n \n     // Duplicate the ByteBuffers for access by multiple containers.\n     Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n     for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                 .getServiceData().entrySet()) {\n       myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n     }\n \n     // Construct the actual Container\n     ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n-        containerID, commonContainerSpec.getUser(), assignedCapability,\n+        commonContainerSpec.getUser(),\n         commonContainerSpec.getLocalResources(), myEnv, commands,\n         myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n         applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n      Configuration conf, Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Credentials credentials) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec \u003d\u003d null) {\n        commonContainerSpec \u003d createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, credentials);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    // Setup environment by cloning from common env.\n    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n    myEnv.putAll(env);\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n        commonContainerSpec.getUser(),\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-4043. Secret keys set in Credentials are not seen by tasks (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1304587 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/03/12 1:46 PM",
      "commitName": "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05",
      "commitAuthor": "Robert Joseph Evans",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-4043. Secret keys set in Credentials are not seen by tasks (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1304587 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/03/12 1:46 PM",
          "commitName": "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05",
          "commitAuthor": "Robert Joseph Evans",
          "commitDateOld": "19/03/12 1:31 PM",
          "commitNameOld": "9d8d02b68b5ffc18e8f9f00db1750a80d3fc9061",
          "commitAuthorOld": "Robert Joseph Evans",
          "daysBetweenCommits": 4.01,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,45 @@\n   static ContainerLaunchContext createContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs,\n       ContainerId containerID, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Resource assignedCapability, WrappedJvmID jvmID,\n       TaskAttemptListener taskAttemptListener,\n-      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n+      Credentials credentials) {\n \n     synchronized (commonContainerSpecLock) {\n       if (commonContainerSpec \u003d\u003d null) {\n         commonContainerSpec \u003d createCommonContainerLaunchContext(\n-            applicationACLs, conf, jobToken, oldJobId, fsTokens);\n+            applicationACLs, conf, jobToken, oldJobId, credentials);\n       }\n     }\n \n     // Fill in the fields needed per-container that are missing in the common\n     // spec.\n \n     // Setup environment by cloning from common env.\n     Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n     Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n     myEnv.putAll(env);\n     MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, jvmID);\n \n     // Duplicate the ByteBuffers for access by multiple containers.\n     Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n     for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                 .getServiceData().entrySet()) {\n       myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n     }\n \n     // Construct the actual Container\n     ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n         containerID, commonContainerSpec.getUser(), assignedCapability,\n         commonContainerSpec.getLocalResources(), myEnv, commands,\n         myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n         applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n      ContainerId containerID, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Resource assignedCapability, WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Credentials credentials) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec \u003d\u003d null) {\n        commonContainerSpec \u003d createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, credentials);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    // Setup environment by cloning from common env.\n    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n    myEnv.putAll(env);\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n        containerID, commonContainerSpec.getUser(), assignedCapability,\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {
            "oldValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e, containerID-ContainerId, conf-Configuration, jobToken-Token\u003cJobTokenIdentifier\u003e, remoteTask-Task, oldJobId-org.apache.hadoop.mapred.JobID(modifiers-final), assignedCapability-Resource, jvmID-WrappedJvmID, taskAttemptListener-TaskAttemptListener, fsTokens-Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e]",
            "newValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e, containerID-ContainerId, conf-Configuration, jobToken-Token\u003cJobTokenIdentifier\u003e, remoteTask-Task, oldJobId-org.apache.hadoop.mapred.JobID(modifiers-final), assignedCapability-Resource, jvmID-WrappedJvmID, taskAttemptListener-TaskAttemptListener, credentials-Credentials]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-4043. Secret keys set in Credentials are not seen by tasks (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1304587 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/03/12 1:46 PM",
          "commitName": "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05",
          "commitAuthor": "Robert Joseph Evans",
          "commitDateOld": "19/03/12 1:31 PM",
          "commitNameOld": "9d8d02b68b5ffc18e8f9f00db1750a80d3fc9061",
          "commitAuthorOld": "Robert Joseph Evans",
          "daysBetweenCommits": 4.01,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,45 @@\n   static ContainerLaunchContext createContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs,\n       ContainerId containerID, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Resource assignedCapability, WrappedJvmID jvmID,\n       TaskAttemptListener taskAttemptListener,\n-      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n+      Credentials credentials) {\n \n     synchronized (commonContainerSpecLock) {\n       if (commonContainerSpec \u003d\u003d null) {\n         commonContainerSpec \u003d createCommonContainerLaunchContext(\n-            applicationACLs, conf, jobToken, oldJobId, fsTokens);\n+            applicationACLs, conf, jobToken, oldJobId, credentials);\n       }\n     }\n \n     // Fill in the fields needed per-container that are missing in the common\n     // spec.\n \n     // Setup environment by cloning from common env.\n     Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n     Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n     myEnv.putAll(env);\n     MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, jvmID);\n \n     // Duplicate the ByteBuffers for access by multiple containers.\n     Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n     for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                 .getServiceData().entrySet()) {\n       myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n     }\n \n     // Construct the actual Container\n     ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n         containerID, commonContainerSpec.getUser(), assignedCapability,\n         commonContainerSpec.getLocalResources(), myEnv, commands,\n         myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n         applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n      ContainerId containerID, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Resource assignedCapability, WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Credentials credentials) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec \u003d\u003d null) {\n        commonContainerSpec \u003d createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, credentials);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    // Setup environment by cloning from common env.\n    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n    myEnv.putAll(env);\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n        containerID, commonContainerSpec.getUser(), assignedCapability,\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "0870734787d7005d85697549eab5b6479d97d453": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-3566. Fixed MR AM to construct CLC only once across all tasks. Contributed by Vinod K V.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227422 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/01/12 5:29 PM",
      "commitName": "0870734787d7005d85697549eab5b6479d97d453",
      "commitAuthor": "Arun Murthy",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-3566. Fixed MR AM to construct CLC only once across all tasks. Contributed by Vinod K V.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227422 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:29 PM",
          "commitName": "0870734787d7005d85697549eab5b6479d97d453",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "22/12/11 2:34 PM",
          "commitNameOld": "8fa0a3c737f27ff9d12fb657a7b22865754a5fd8",
          "commitAuthorOld": "Siddharth Seth",
          "daysBetweenCommits": 13.12,
          "commitsBetweenForRepo": 31,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,109 +1,45 @@\n-  private ContainerLaunchContext createContainerLaunchContext(\n-      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n+  static ContainerLaunchContext createContainerLaunchContext(\n+      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n+      ContainerId containerID, Configuration conf,\n+      Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n+      final org.apache.hadoop.mapred.JobID oldJobId,\n+      Resource assignedCapability, WrappedJvmID jvmID,\n+      TaskAttemptListener taskAttemptListener,\n+      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n \n-    // Application resources\n-    Map\u003cString, LocalResource\u003e localResources \u003d \n-        new HashMap\u003cString, LocalResource\u003e();\n-    \n-    // Application environment\n-    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n-\n-    // Service data\n-    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n-\n-    // Tokens\n-    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n-    try {\n-      FileSystem remoteFS \u003d FileSystem.get(conf);\n-\n-      // //////////// Set up JobJar to be localized properly on the remote NM.\n-      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n-        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n-              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n-                                               remoteFS.getWorkingDirectory());\n-        localResources.put(\n-            MRJobConfig.JOB_JAR,\n-            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n-                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n-        LOG.info(\"The job-jar file on the remote FS is \"\n-            + remoteJobJar.toUri().toASCIIString());\n-      } else {\n-        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n-        // mapreduce jar itself which is already on the classpath.\n-        LOG.info(\"Job jar is not present. \"\n-            + \"Not adding any jar to the list of resources.\");\n+    synchronized (commonContainerSpecLock) {\n+      if (commonContainerSpec \u003d\u003d null) {\n+        commonContainerSpec \u003d createCommonContainerLaunchContext(\n+            applicationACLs, conf, jobToken, oldJobId, fsTokens);\n       }\n-      // //////////// End of JobJar setup\n-\n-      // //////////// Set up JobConf to be localized properly on the remote NM.\n-      Path path \u003d\n-          MRApps.getStagingAreaDir(conf, UserGroupInformation\n-              .getCurrentUser().getShortUserName());\n-      Path remoteJobSubmitDir \u003d\n-          new Path(path, oldJobId.toString());\n-      Path remoteJobConfPath \u003d \n-          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n-      localResources.put(\n-          MRJobConfig.JOB_CONF_FILE,\n-          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n-              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n-      LOG.info(\"The job-conf file on the remote FS is \"\n-          + remoteJobConfPath.toUri().toASCIIString());\n-      // //////////// End of JobConf setup\n-\n-      // Setup DistributedCache\n-      MRApps.setupDistributedCache(conf, localResources);\n-\n-      // Setup up tokens\n-      Credentials taskCredentials \u003d new Credentials();\n-\n-      if (UserGroupInformation.isSecurityEnabled()) {\n-        // Add file-system tokens\n-        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n-          LOG.info(\"Putting fs-token for NM use for launching container : \"\n-              + token.toString());\n-          taskCredentials.addToken(token.getService(), token);\n-        }\n-      }\n-\n-      // LocalStorageToken is needed irrespective of whether security is enabled\n-      // or not.\n-      TokenCache.setJobToken(jobToken, taskCredentials);\n-\n-      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n-      LOG.info(\"Size of containertokens_dob is \"\n-          + taskCredentials.numberOfTokens());\n-      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n-      tokens \u003d \n-          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n-              containerTokens_dob.getLength());\n-\n-      // Add shuffle token\n-      LOG.info(\"Putting shuffle token in serviceData\");\n-      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n-          ShuffleHandler.serializeServiceData(jobToken));\n-\n-      Apps.addToEnvironment(\n-          environment,  \n-          Environment.CLASSPATH.name(), \n-          getInitialClasspath());\n-    } catch (IOException e) {\n-      throw new YarnException(e);\n     }\n \n-    // Setup environment\n-    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n+    // Fill in the fields needed per-container that are missing in the common\n+    // spec.\n+\n+    // Setup environment by cloning from common env.\n+    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n+    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n+    myEnv.putAll(env);\n+    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n-        taskAttemptListener.getAddress(), remoteTask,\n-        jvmID);\n-    \n+        taskAttemptListener.getAddress(), remoteTask, jvmID);\n+\n+    // Duplicate the ByteBuffers for access by multiple containers.\n+    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n+    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n+                .getServiceData().entrySet()) {\n+      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n+    }\n+\n     // Construct the actual Container\n-    ContainerLaunchContext container \u003d BuilderUtils\n-        .newContainerLaunchContext(containerID, conf\n-            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n-            environment, commands, serviceData, tokens, applicationACLs);\n+    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n+        containerID, commonContainerSpec.getUser(), assignedCapability,\n+        commonContainerSpec.getLocalResources(), myEnv, commands,\n+        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n+        applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n      ContainerId containerID, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Resource assignedCapability, WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec \u003d\u003d null) {\n        commonContainerSpec \u003d createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, fsTokens);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    // Setup environment by cloning from common env.\n    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n    myEnv.putAll(env);\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n        containerID, commonContainerSpec.getUser(), assignedCapability,\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {
            "oldValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e]",
            "newValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e, containerID-ContainerId, conf-Configuration, jobToken-Token\u003cJobTokenIdentifier\u003e, remoteTask-Task, oldJobId-org.apache.hadoop.mapred.JobID(modifiers-final), assignedCapability-Resource, jvmID-WrappedJvmID, taskAttemptListener-TaskAttemptListener, fsTokens-Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "MAPREDUCE-3566. Fixed MR AM to construct CLC only once across all tasks. Contributed by Vinod K V.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227422 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:29 PM",
          "commitName": "0870734787d7005d85697549eab5b6479d97d453",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "22/12/11 2:34 PM",
          "commitNameOld": "8fa0a3c737f27ff9d12fb657a7b22865754a5fd8",
          "commitAuthorOld": "Siddharth Seth",
          "daysBetweenCommits": 13.12,
          "commitsBetweenForRepo": 31,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,109 +1,45 @@\n-  private ContainerLaunchContext createContainerLaunchContext(\n-      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n+  static ContainerLaunchContext createContainerLaunchContext(\n+      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n+      ContainerId containerID, Configuration conf,\n+      Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n+      final org.apache.hadoop.mapred.JobID oldJobId,\n+      Resource assignedCapability, WrappedJvmID jvmID,\n+      TaskAttemptListener taskAttemptListener,\n+      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n \n-    // Application resources\n-    Map\u003cString, LocalResource\u003e localResources \u003d \n-        new HashMap\u003cString, LocalResource\u003e();\n-    \n-    // Application environment\n-    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n-\n-    // Service data\n-    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n-\n-    // Tokens\n-    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n-    try {\n-      FileSystem remoteFS \u003d FileSystem.get(conf);\n-\n-      // //////////// Set up JobJar to be localized properly on the remote NM.\n-      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n-        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n-              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n-                                               remoteFS.getWorkingDirectory());\n-        localResources.put(\n-            MRJobConfig.JOB_JAR,\n-            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n-                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n-        LOG.info(\"The job-jar file on the remote FS is \"\n-            + remoteJobJar.toUri().toASCIIString());\n-      } else {\n-        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n-        // mapreduce jar itself which is already on the classpath.\n-        LOG.info(\"Job jar is not present. \"\n-            + \"Not adding any jar to the list of resources.\");\n+    synchronized (commonContainerSpecLock) {\n+      if (commonContainerSpec \u003d\u003d null) {\n+        commonContainerSpec \u003d createCommonContainerLaunchContext(\n+            applicationACLs, conf, jobToken, oldJobId, fsTokens);\n       }\n-      // //////////// End of JobJar setup\n-\n-      // //////////// Set up JobConf to be localized properly on the remote NM.\n-      Path path \u003d\n-          MRApps.getStagingAreaDir(conf, UserGroupInformation\n-              .getCurrentUser().getShortUserName());\n-      Path remoteJobSubmitDir \u003d\n-          new Path(path, oldJobId.toString());\n-      Path remoteJobConfPath \u003d \n-          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n-      localResources.put(\n-          MRJobConfig.JOB_CONF_FILE,\n-          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n-              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n-      LOG.info(\"The job-conf file on the remote FS is \"\n-          + remoteJobConfPath.toUri().toASCIIString());\n-      // //////////// End of JobConf setup\n-\n-      // Setup DistributedCache\n-      MRApps.setupDistributedCache(conf, localResources);\n-\n-      // Setup up tokens\n-      Credentials taskCredentials \u003d new Credentials();\n-\n-      if (UserGroupInformation.isSecurityEnabled()) {\n-        // Add file-system tokens\n-        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n-          LOG.info(\"Putting fs-token for NM use for launching container : \"\n-              + token.toString());\n-          taskCredentials.addToken(token.getService(), token);\n-        }\n-      }\n-\n-      // LocalStorageToken is needed irrespective of whether security is enabled\n-      // or not.\n-      TokenCache.setJobToken(jobToken, taskCredentials);\n-\n-      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n-      LOG.info(\"Size of containertokens_dob is \"\n-          + taskCredentials.numberOfTokens());\n-      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n-      tokens \u003d \n-          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n-              containerTokens_dob.getLength());\n-\n-      // Add shuffle token\n-      LOG.info(\"Putting shuffle token in serviceData\");\n-      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n-          ShuffleHandler.serializeServiceData(jobToken));\n-\n-      Apps.addToEnvironment(\n-          environment,  \n-          Environment.CLASSPATH.name(), \n-          getInitialClasspath());\n-    } catch (IOException e) {\n-      throw new YarnException(e);\n     }\n \n-    // Setup environment\n-    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n+    // Fill in the fields needed per-container that are missing in the common\n+    // spec.\n+\n+    // Setup environment by cloning from common env.\n+    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n+    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n+    myEnv.putAll(env);\n+    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n-        taskAttemptListener.getAddress(), remoteTask,\n-        jvmID);\n-    \n+        taskAttemptListener.getAddress(), remoteTask, jvmID);\n+\n+    // Duplicate the ByteBuffers for access by multiple containers.\n+    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n+    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n+                .getServiceData().entrySet()) {\n+      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n+    }\n+\n     // Construct the actual Container\n-    ContainerLaunchContext container \u003d BuilderUtils\n-        .newContainerLaunchContext(containerID, conf\n-            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n-            environment, commands, serviceData, tokens, applicationACLs);\n+    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n+        containerID, commonContainerSpec.getUser(), assignedCapability,\n+        commonContainerSpec.getLocalResources(), myEnv, commands,\n+        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n+        applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n      ContainerId containerID, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Resource assignedCapability, WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec \u003d\u003d null) {\n        commonContainerSpec \u003d createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, fsTokens);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    // Setup environment by cloning from common env.\n    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n    myEnv.putAll(env);\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n        containerID, commonContainerSpec.getUser(), assignedCapability,\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-3566. Fixed MR AM to construct CLC only once across all tasks. Contributed by Vinod K V.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227422 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:29 PM",
          "commitName": "0870734787d7005d85697549eab5b6479d97d453",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "22/12/11 2:34 PM",
          "commitNameOld": "8fa0a3c737f27ff9d12fb657a7b22865754a5fd8",
          "commitAuthorOld": "Siddharth Seth",
          "daysBetweenCommits": 13.12,
          "commitsBetweenForRepo": 31,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,109 +1,45 @@\n-  private ContainerLaunchContext createContainerLaunchContext(\n-      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n+  static ContainerLaunchContext createContainerLaunchContext(\n+      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n+      ContainerId containerID, Configuration conf,\n+      Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n+      final org.apache.hadoop.mapred.JobID oldJobId,\n+      Resource assignedCapability, WrappedJvmID jvmID,\n+      TaskAttemptListener taskAttemptListener,\n+      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n \n-    // Application resources\n-    Map\u003cString, LocalResource\u003e localResources \u003d \n-        new HashMap\u003cString, LocalResource\u003e();\n-    \n-    // Application environment\n-    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n-\n-    // Service data\n-    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n-\n-    // Tokens\n-    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n-    try {\n-      FileSystem remoteFS \u003d FileSystem.get(conf);\n-\n-      // //////////// Set up JobJar to be localized properly on the remote NM.\n-      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n-        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n-              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n-                                               remoteFS.getWorkingDirectory());\n-        localResources.put(\n-            MRJobConfig.JOB_JAR,\n-            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n-                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n-        LOG.info(\"The job-jar file on the remote FS is \"\n-            + remoteJobJar.toUri().toASCIIString());\n-      } else {\n-        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n-        // mapreduce jar itself which is already on the classpath.\n-        LOG.info(\"Job jar is not present. \"\n-            + \"Not adding any jar to the list of resources.\");\n+    synchronized (commonContainerSpecLock) {\n+      if (commonContainerSpec \u003d\u003d null) {\n+        commonContainerSpec \u003d createCommonContainerLaunchContext(\n+            applicationACLs, conf, jobToken, oldJobId, fsTokens);\n       }\n-      // //////////// End of JobJar setup\n-\n-      // //////////// Set up JobConf to be localized properly on the remote NM.\n-      Path path \u003d\n-          MRApps.getStagingAreaDir(conf, UserGroupInformation\n-              .getCurrentUser().getShortUserName());\n-      Path remoteJobSubmitDir \u003d\n-          new Path(path, oldJobId.toString());\n-      Path remoteJobConfPath \u003d \n-          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n-      localResources.put(\n-          MRJobConfig.JOB_CONF_FILE,\n-          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n-              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n-      LOG.info(\"The job-conf file on the remote FS is \"\n-          + remoteJobConfPath.toUri().toASCIIString());\n-      // //////////// End of JobConf setup\n-\n-      // Setup DistributedCache\n-      MRApps.setupDistributedCache(conf, localResources);\n-\n-      // Setup up tokens\n-      Credentials taskCredentials \u003d new Credentials();\n-\n-      if (UserGroupInformation.isSecurityEnabled()) {\n-        // Add file-system tokens\n-        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n-          LOG.info(\"Putting fs-token for NM use for launching container : \"\n-              + token.toString());\n-          taskCredentials.addToken(token.getService(), token);\n-        }\n-      }\n-\n-      // LocalStorageToken is needed irrespective of whether security is enabled\n-      // or not.\n-      TokenCache.setJobToken(jobToken, taskCredentials);\n-\n-      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n-      LOG.info(\"Size of containertokens_dob is \"\n-          + taskCredentials.numberOfTokens());\n-      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n-      tokens \u003d \n-          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n-              containerTokens_dob.getLength());\n-\n-      // Add shuffle token\n-      LOG.info(\"Putting shuffle token in serviceData\");\n-      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n-          ShuffleHandler.serializeServiceData(jobToken));\n-\n-      Apps.addToEnvironment(\n-          environment,  \n-          Environment.CLASSPATH.name(), \n-          getInitialClasspath());\n-    } catch (IOException e) {\n-      throw new YarnException(e);\n     }\n \n-    // Setup environment\n-    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n+    // Fill in the fields needed per-container that are missing in the common\n+    // spec.\n+\n+    // Setup environment by cloning from common env.\n+    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n+    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n+    myEnv.putAll(env);\n+    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n-        taskAttemptListener.getAddress(), remoteTask,\n-        jvmID);\n-    \n+        taskAttemptListener.getAddress(), remoteTask, jvmID);\n+\n+    // Duplicate the ByteBuffers for access by multiple containers.\n+    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n+    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n+                .getServiceData().entrySet()) {\n+      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n+    }\n+\n     // Construct the actual Container\n-    ContainerLaunchContext container \u003d BuilderUtils\n-        .newContainerLaunchContext(containerID, conf\n-            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n-            environment, commands, serviceData, tokens, applicationACLs);\n+    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n+        containerID, commonContainerSpec.getUser(), assignedCapability,\n+        commonContainerSpec.getLocalResources(), myEnv, commands,\n+        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n+        applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs,\n      ContainerId containerID, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken, Task remoteTask,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Resource assignedCapability, WrappedJvmID jvmID,\n      TaskAttemptListener taskAttemptListener,\n      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n\n    synchronized (commonContainerSpecLock) {\n      if (commonContainerSpec \u003d\u003d null) {\n        commonContainerSpec \u003d createCommonContainerLaunchContext(\n            applicationACLs, conf, jobToken, oldJobId, fsTokens);\n      }\n    }\n\n    // Fill in the fields needed per-container that are missing in the common\n    // spec.\n\n    // Setup environment by cloning from common env.\n    Map\u003cString, String\u003e env \u003d commonContainerSpec.getEnvironment();\n    Map\u003cString, String\u003e myEnv \u003d new HashMap\u003cString, String\u003e(env.size());\n    myEnv.putAll(env);\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, jvmID);\n\n    // Duplicate the ByteBuffers for access by multiple containers.\n    Map\u003cString, ByteBuffer\u003e myServiceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n    for (Entry\u003cString, ByteBuffer\u003e entry : commonContainerSpec\n                .getServiceData().entrySet()) {\n      myServiceData.put(entry.getKey(), entry.getValue().duplicate());\n    }\n\n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils.newContainerLaunchContext(\n        containerID, commonContainerSpec.getUser(), assignedCapability,\n        commonContainerSpec.getLocalResources(), myEnv, commands,\n        myServiceData, commonContainerSpec.getContainerTokens().duplicate(),\n        applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "df2991c0cbc3f35c2640b93680667507c4f810dd": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-3104. Implemented Application-acls. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186748 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/10/11 4:45 AM",
      "commitName": "df2991c0cbc3f35c2640b93680667507c4f810dd",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-3104. Implemented Application-acls. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186748 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "20/10/11 4:45 AM",
          "commitName": "df2991c0cbc3f35c2640b93680667507c4f810dd",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "18/10/11 10:21 PM",
          "commitNameOld": "13e4562924a6cb3d16c262e0f595b2ffbf9e0546",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,114 +1,109 @@\n-  private ContainerLaunchContext createContainerLaunchContext() {\n+  private ContainerLaunchContext createContainerLaunchContext(\n+      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Setup environment\n     MapReduceChildJVM.setVMEnv(environment, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask,\n         jvmID);\n     \n     // Construct the actual Container\n-    ContainerLaunchContext container \u003d\n-        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n-    container.setContainerId(containerID);\n-    container.setUser(conf.get(MRJobConfig.USER_NAME));\n-    container.setResource(assignedCapability);\n-    container.setLocalResources(localResources);\n-    container.setEnvironment(environment);\n-    container.setCommands(commands);\n-    container.setServiceData(serviceData);\n-    container.setContainerTokens(tokens);\n-    \n+    ContainerLaunchContext container \u003d BuilderUtils\n+        .newContainerLaunchContext(containerID, conf\n+            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n+            environment, commands, serviceData, tokens, applicationACLs);\n+\n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Setup environment\n    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask,\n        jvmID);\n    \n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(containerID, conf\n            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n            environment, commands, serviceData, tokens, applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-3104. Implemented Application-acls. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186748 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "20/10/11 4:45 AM",
          "commitName": "df2991c0cbc3f35c2640b93680667507c4f810dd",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "18/10/11 10:21 PM",
          "commitNameOld": "13e4562924a6cb3d16c262e0f595b2ffbf9e0546",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,114 +1,109 @@\n-  private ContainerLaunchContext createContainerLaunchContext() {\n+  private ContainerLaunchContext createContainerLaunchContext(\n+      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Setup environment\n     MapReduceChildJVM.setVMEnv(environment, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask,\n         jvmID);\n     \n     // Construct the actual Container\n-    ContainerLaunchContext container \u003d\n-        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n-    container.setContainerId(containerID);\n-    container.setUser(conf.get(MRJobConfig.USER_NAME));\n-    container.setResource(assignedCapability);\n-    container.setLocalResources(localResources);\n-    container.setEnvironment(environment);\n-    container.setCommands(commands);\n-    container.setServiceData(serviceData);\n-    container.setContainerTokens(tokens);\n-    \n+    ContainerLaunchContext container \u003d BuilderUtils\n+        .newContainerLaunchContext(containerID, conf\n+            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n+            environment, commands, serviceData, tokens, applicationACLs);\n+\n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Setup environment\n    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask,\n        jvmID);\n    \n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(containerID, conf\n            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n            environment, commands, serviceData, tokens, applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "c1d90772b6e38bb4e4be7ed75cb5d34f3048ad7b": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3068. Added a whitelist of environment variables for containers from the NodeManager and set MALLOC_ARENA_MAX for all daemons and containers. Contributed by Chris Riccomini. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1185447 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/10/11 6:22 PM",
      "commitName": "c1d90772b6e38bb4e4be7ed75cb5d34f3048ad7b",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "17/10/11 8:17 AM",
      "commitNameOld": "a26b1672a85e97bea973cfcc7eab22b4cca01448",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 0.42,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,114 @@\n   private ContainerLaunchContext createContainerLaunchContext() {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n-      MRApps.addToEnvironment(\n+      Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Setup environment\n     MapReduceChildJVM.setVMEnv(environment, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask,\n         jvmID);\n     \n     // Construct the actual Container\n     ContainerLaunchContext container \u003d\n         recordFactory.newRecordInstance(ContainerLaunchContext.class);\n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME));\n     container.setResource(assignedCapability);\n     container.setLocalResources(localResources);\n     container.setEnvironment(environment);\n     container.setCommands(commands);\n     container.setServiceData(serviceData);\n     container.setContainerTokens(tokens);\n     \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Setup environment\n    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask,\n        jvmID);\n    \n    // Construct the actual Container\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    container.setLocalResources(localResources);\n    container.setEnvironment(environment);\n    container.setCommands(commands);\n    container.setServiceData(serviceData);\n    container.setContainerTokens(tokens);\n    \n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "d00b3c49f6fb3f6a617add6203c6b55f6c345940": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2880. Improved classpath-construction for mapreduce AM and containers. Contributed by Arun C Murthy.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1173783 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/09/11 11:28 AM",
      "commitName": "d00b3c49f6fb3f6a617add6203c6b55f6c345940",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "20/09/11 6:26 PM",
      "commitNameOld": "1d067c6e2b14e08943a46129f4ed521890d3ca22",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.71,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,125 +1,114 @@\n   private ContainerLaunchContext createContainerLaunchContext() {\n \n-    ContainerLaunchContext container \u003d\n-        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n-\n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n-    \n+\n+    // Service data\n+    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n+\n+    // Tokens\n+    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         localResources.put(\n-            MRConstants.JOB_JAR,\n+            MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n-          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n+          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n-          MRConstants.JOB_CONF_FILE,\n+          MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n-      MRApps.setupDistributedCache(conf, localResources, environment);\n+      MRApps.setupDistributedCache(conf, localResources);\n \n-      // Set local-resources and environment\n-      container.setLocalResources(localResources);\n-      container.setEnvironment(environment);\n-      \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n-      container.setContainerTokens(\n+      tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n-              containerTokens_dob.getLength()));\n+              containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n-      Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n-      container.setServiceData(serviceData);\n \n-      MRApps.addToClassPath(container.getEnvironment(), getInitialClasspath());\n+      MRApps.addToEnvironment(\n+          environment,  \n+          Environment.CLASSPATH.name(), \n+          getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n+\n+    // Setup environment\n+    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n+\n+    // Set up the launch command\n+    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n+        taskAttemptListener.getAddress(), remoteTask,\n+        jvmID);\n     \n-    container.setContainerId(containerID);\n-    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n-\n-    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n-    String containerLogDir \u003d\n-        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n-    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n-    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n-    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n-    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n-\n-    String localizedApplicationTokensFile \u003d\n-        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n-    classPaths.add(MRConstants.JOB_JAR);\n-    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n-    classPaths.add(workDir.toString()); // TODO\n-\n     // Construct the actual Container\n-    container.setCommands(MapReduceChildJVM.getVMCommand(\n-        taskAttemptListener.getAddress(), remoteTask, javaHome,\n-        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n-\n-    MapReduceChildJVM.setVMEnv(container.getEnvironment(), classPaths,\n-        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n-        localizedApplicationTokensFile);\n-\n-    // Construct the actual Container\n+    ContainerLaunchContext container \u003d\n+        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME));\n     container.setResource(assignedCapability);\n+    container.setLocalResources(localResources);\n+    container.setEnvironment(environment);\n+    container.setCommands(commands);\n+    container.setServiceData(serviceData);\n+    container.setContainerTokens(tokens);\n+    \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      MRApps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Setup environment\n    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask,\n        jvmID);\n    \n    // Construct the actual Container\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    container.setLocalResources(localResources);\n    container.setEnvironment(environment);\n    container.setCommands(commands);\n    container.setServiceData(serviceData);\n    container.setContainerTokens(tokens);\n    \n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "88b82a0f6687ce103817fbb460fd30d870f717a0": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2899. Replace major parts of ApplicationSubmissionContext with a ContainerLaunchContext (Arun Murthy via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1170459 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/09/11 12:26 AM",
      "commitName": "88b82a0f6687ce103817fbb460fd30d870f717a0",
      "commitAuthor": "Mahadev Konar",
      "commitDateOld": "13/09/11 11:12 AM",
      "commitNameOld": "53f921418d25cb232c7a0e1fa24c17bda729ac35",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.55,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,126 +1,125 @@\n   private ContainerLaunchContext createContainerLaunchContext() {\n \n     ContainerLaunchContext container \u003d\n         recordFactory.newRecordInstance(ContainerLaunchContext.class);\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n     \n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRConstants.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n       localResources.put(\n           MRConstants.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n-      \n       // Setup DistributedCache\n-      setupDistributedCache(remoteFS, conf, localResources, environment);\n+      MRApps.setupDistributedCache(conf, localResources, environment);\n \n       // Set local-resources and environment\n       container.setLocalResources(localResources);\n-      container.setEnv(environment);\n+      container.setEnvironment(environment);\n       \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       container.setContainerTokens(\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength()));\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n       container.setServiceData(serviceData);\n \n-      MRApps.addToClassPath(container.getEnv(), getInitialClasspath());\n+      MRApps.addToClassPath(container.getEnvironment(), getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n     \n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n \n     File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n     String containerLogDir \u003d\n         new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n     String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n     String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n     String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n     List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n \n     String localizedApplicationTokensFile \u003d\n         new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n     classPaths.add(MRConstants.JOB_JAR);\n     classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n     classPaths.add(workDir.toString()); // TODO\n \n     // Construct the actual Container\n     container.setCommands(MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, javaHome,\n         workDir.toString(), containerLogDir, childTmpDir, jvmID));\n \n-    MapReduceChildJVM.setVMEnv(container.getEnv(), classPaths,\n+    MapReduceChildJVM.setVMEnv(container.getEnvironment(), classPaths,\n         workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n         localizedApplicationTokensFile);\n \n     // Construct the actual Container\n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME));\n     container.setResource(assignedCapability);\n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n    \n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRConstants.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n      localResources.put(\n          MRConstants.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources, environment);\n\n      // Set local-resources and environment\n      container.setLocalResources(localResources);\n      container.setEnvironment(environment);\n      \n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      container.setContainerTokens(\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength()));\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n      container.setServiceData(serviceData);\n\n      MRApps.addToClassPath(container.getEnvironment(), getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n    \n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n\n    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n    String containerLogDir \u003d\n        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n\n    String localizedApplicationTokensFile \u003d\n        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n    classPaths.add(MRConstants.JOB_JAR);\n    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n    classPaths.add(workDir.toString()); // TODO\n\n    // Construct the actual Container\n    container.setCommands(MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, javaHome,\n        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n\n    MapReduceChildJVM.setVMEnv(container.getEnvironment(), classPaths,\n        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n        localizedApplicationTokensFile);\n\n    // Construct the actual Container\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "6165875dc6bf67d72fc3ce1d96dfc80ba312d4a1": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2896. Simplify all apis to in org.apache.hadoop.yarn.api.records.* to be get/set only. Added javadocs to all public records.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1169980 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/09/11 5:05 PM",
      "commitName": "6165875dc6bf67d72fc3ce1d96dfc80ba312d4a1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "10/09/11 11:21 PM",
      "commitNameOld": "8fb67650b146573c20ae010e28b1eca6e16433b3",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 1.74,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,126 @@\n   private ContainerLaunchContext createContainerLaunchContext() {\n \n     ContainerLaunchContext container \u003d\n         recordFactory.newRecordInstance(ContainerLaunchContext.class);\n \n+    // Application resources\n+    Map\u003cString, LocalResource\u003e localResources \u003d \n+        new HashMap\u003cString, LocalResource\u003e();\n+    \n+    // Application environment\n+    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n+    \n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n-        container.setLocalResource(\n+        localResources.put(\n             MRConstants.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n-      container.setLocalResource(\n+      localResources.put(\n           MRConstants.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n+      \n       // Setup DistributedCache\n-      setupDistributedCache(remoteFS, conf, container);\n+      setupDistributedCache(remoteFS, conf, localResources, environment);\n \n+      // Set local-resources and environment\n+      container.setLocalResources(localResources);\n+      container.setEnv(environment);\n+      \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       container.setContainerTokens(\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength()));\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n-      container\n-          .setServiceData(\n-              ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n-              ShuffleHandler.serializeServiceData(jobToken));\n+      Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n+      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n+          ShuffleHandler.serializeServiceData(jobToken));\n+      container.setServiceData(serviceData);\n \n-      MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n+      MRApps.addToClassPath(container.getEnv(), getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n     \n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n \n     File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n     String containerLogDir \u003d\n         new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n     String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n     String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n     String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n     List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n \n     String localizedApplicationTokensFile \u003d\n         new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n     classPaths.add(MRConstants.JOB_JAR);\n     classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n     classPaths.add(workDir.toString()); // TODO\n \n     // Construct the actual Container\n-    container.addAllCommands(MapReduceChildJVM.getVMCommand(\n+    container.setCommands(MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, javaHome,\n         workDir.toString(), containerLogDir, childTmpDir, jvmID));\n \n-    MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n+    MapReduceChildJVM.setVMEnv(container.getEnv(), classPaths,\n         workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n         localizedApplicationTokensFile);\n \n     // Construct the actual Container\n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME));\n     container.setResource(assignedCapability);\n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n    \n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRConstants.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n      localResources.put(\n          MRConstants.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      \n      // Setup DistributedCache\n      setupDistributedCache(remoteFS, conf, localResources, environment);\n\n      // Set local-resources and environment\n      container.setLocalResources(localResources);\n      container.setEnv(environment);\n      \n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      container.setContainerTokens(\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength()));\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n      container.setServiceData(serviceData);\n\n      MRApps.addToClassPath(container.getEnv(), getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n    \n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n\n    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n    String containerLogDir \u003d\n        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n\n    String localizedApplicationTokensFile \u003d\n        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n    classPaths.add(MRConstants.JOB_JAR);\n    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n    classPaths.add(workDir.toString()); // TODO\n\n    // Construct the actual Container\n    container.setCommands(MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, javaHome,\n        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n\n    MapReduceChildJVM.setVMEnv(container.getEnv(), classPaths,\n        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n        localizedApplicationTokensFile);\n\n    // Construct the actual Container\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "ade0f0560f729e50382c6992f713f29e2dd5b270": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2652. Enabled multiple NMs to be runnable on a single node by making shuffle service port to be truely configurable. Contributed by Robert Joseph Evans.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1163585 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/08/11 4:38 AM",
      "commitName": "ade0f0560f729e50382c6992f713f29e2dd5b270",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 6.48,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,117 +1,114 @@\n   private ContainerLaunchContext createContainerLaunchContext() {\n \n     ContainerLaunchContext container \u003d\n         recordFactory.newRecordInstance(ContainerLaunchContext.class);\n \n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         container.setLocalResource(\n             MRConstants.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n       container.setLocalResource(\n           MRConstants.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       setupDistributedCache(remoteFS, conf, container);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       container.setContainerTokens(\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength()));\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n-      DataOutputBuffer jobToken_dob \u003d new DataOutputBuffer();\n-      jobToken.write(jobToken_dob);\n       container\n           .setServiceData(\n               ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n-              ByteBuffer.wrap(jobToken_dob.getData(), 0,\n-                  jobToken_dob.getLength()));\n+              ShuffleHandler.serializeServiceData(jobToken));\n \n       MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n     \n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n \n     File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n     String containerLogDir \u003d\n         new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n     String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n     String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n     String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n     List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n \n     String localizedApplicationTokensFile \u003d\n         new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n     classPaths.add(MRConstants.JOB_JAR);\n     classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n     classPaths.add(workDir.toString()); // TODO\n \n     // Construct the actual Container\n     container.addAllCommands(MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, javaHome,\n         workDir.toString(), containerLogDir, childTmpDir, jvmID));\n \n     MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n         workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n         localizedApplicationTokensFile);\n \n     // Construct the actual Container\n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME));\n     container.setResource(assignedCapability);\n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        container.setLocalResource(\n            MRConstants.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n      container.setLocalResource(\n          MRConstants.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      setupDistributedCache(remoteFS, conf, container);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      container.setContainerTokens(\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength()));\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      container\n          .setServiceData(\n              ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n              ShuffleHandler.serializeServiceData(jobToken));\n\n      MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n    \n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n\n    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n    String containerLogDir \u003d\n        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n\n    String localizedApplicationTokensFile \u003d\n        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n    classPaths.add(MRConstants.JOB_JAR);\n    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n    classPaths.add(workDir.toString()); // TODO\n\n    // Construct the actual Container\n    container.addAllCommands(MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, javaHome,\n        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n\n    MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n        localizedApplicationTokensFile);\n\n    // Construct the actual Container\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        container.setLocalResource(\n            MRConstants.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n      container.setLocalResource(\n          MRConstants.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      setupDistributedCache(remoteFS, conf, container);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      container.setContainerTokens(\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength()));\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      DataOutputBuffer jobToken_dob \u003d new DataOutputBuffer();\n      jobToken.write(jobToken_dob);\n      container\n          .setServiceData(\n              ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n              ByteBuffer.wrap(jobToken_dob.getData(), 0,\n                  jobToken_dob.getLength()));\n\n      MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n    \n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n\n    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n    String containerLogDir \u003d\n        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n\n    String localizedApplicationTokensFile \u003d\n        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n    classPaths.add(MRConstants.JOB_JAR);\n    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n    classPaths.add(workDir.toString()); // TODO\n\n    // Construct the actual Container\n    container.addAllCommands(MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, javaHome,\n        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n\n    MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n        localizedApplicationTokensFile);\n\n    // Construct the actual Container\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,117 @@\n+  private ContainerLaunchContext createContainerLaunchContext() {\n+\n+    ContainerLaunchContext container \u003d\n+        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n+\n+    try {\n+      FileSystem remoteFS \u003d FileSystem.get(conf);\n+\n+      // //////////// Set up JobJar to be localized properly on the remote NM.\n+      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n+        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n+              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n+                                               remoteFS.getWorkingDirectory());\n+        container.setLocalResource(\n+            MRConstants.JOB_JAR,\n+            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n+                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n+        LOG.info(\"The job-jar file on the remote FS is \"\n+            + remoteJobJar.toUri().toASCIIString());\n+      } else {\n+        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n+        // mapreduce jar itself which is already on the classpath.\n+        LOG.info(\"Job jar is not present. \"\n+            + \"Not adding any jar to the list of resources.\");\n+      }\n+      // //////////// End of JobJar setup\n+\n+      // //////////// Set up JobConf to be localized properly on the remote NM.\n+      Path path \u003d\n+          MRApps.getStagingAreaDir(conf, UserGroupInformation\n+              .getCurrentUser().getShortUserName());\n+      Path remoteJobSubmitDir \u003d\n+          new Path(path, oldJobId.toString());\n+      Path remoteJobConfPath \u003d \n+          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n+      container.setLocalResource(\n+          MRConstants.JOB_CONF_FILE,\n+          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n+              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n+      LOG.info(\"The job-conf file on the remote FS is \"\n+          + remoteJobConfPath.toUri().toASCIIString());\n+      // //////////// End of JobConf setup\n+\n+      // Setup DistributedCache\n+      setupDistributedCache(remoteFS, conf, container);\n+\n+      // Setup up tokens\n+      Credentials taskCredentials \u003d new Credentials();\n+\n+      if (UserGroupInformation.isSecurityEnabled()) {\n+        // Add file-system tokens\n+        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n+          LOG.info(\"Putting fs-token for NM use for launching container : \"\n+              + token.toString());\n+          taskCredentials.addToken(token.getService(), token);\n+        }\n+      }\n+\n+      // LocalStorageToken is needed irrespective of whether security is enabled\n+      // or not.\n+      TokenCache.setJobToken(jobToken, taskCredentials);\n+\n+      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n+      LOG.info(\"Size of containertokens_dob is \"\n+          + taskCredentials.numberOfTokens());\n+      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n+      container.setContainerTokens(\n+          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n+              containerTokens_dob.getLength()));\n+\n+      // Add shuffle token\n+      LOG.info(\"Putting shuffle token in serviceData\");\n+      DataOutputBuffer jobToken_dob \u003d new DataOutputBuffer();\n+      jobToken.write(jobToken_dob);\n+      container\n+          .setServiceData(\n+              ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n+              ByteBuffer.wrap(jobToken_dob.getData(), 0,\n+                  jobToken_dob.getLength()));\n+\n+      MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n+    } catch (IOException e) {\n+      throw new YarnException(e);\n+    }\n+    \n+    container.setContainerId(containerID);\n+    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n+\n+    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n+    String containerLogDir \u003d\n+        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n+    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n+    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n+    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n+    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n+\n+    String localizedApplicationTokensFile \u003d\n+        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n+    classPaths.add(MRConstants.JOB_JAR);\n+    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n+    classPaths.add(workDir.toString()); // TODO\n+\n+    // Construct the actual Container\n+    container.addAllCommands(MapReduceChildJVM.getVMCommand(\n+        taskAttemptListener.getAddress(), remoteTask, javaHome,\n+        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n+\n+    MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n+        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n+        localizedApplicationTokensFile);\n+\n+    // Construct the actual Container\n+    container.setContainerId(containerID);\n+    container.setUser(conf.get(MRJobConfig.USER_NAME));\n+    container.setResource(assignedCapability);\n+    return container;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        container.setLocalResource(\n            MRConstants.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n      container.setLocalResource(\n          MRConstants.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      setupDistributedCache(remoteFS, conf, container);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      container.setContainerTokens(\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength()));\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      DataOutputBuffer jobToken_dob \u003d new DataOutputBuffer();\n      jobToken.write(jobToken_dob);\n      container\n          .setServiceData(\n              ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n              ByteBuffer.wrap(jobToken_dob.getData(), 0,\n                  jobToken_dob.getLength()));\n\n      MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n    \n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n\n    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n    String containerLogDir \u003d\n        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n\n    String localizedApplicationTokensFile \u003d\n        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n    classPaths.add(MRConstants.JOB_JAR);\n    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n    classPaths.add(workDir.toString()); // TODO\n\n    // Construct the actual Container\n    container.addAllCommands(MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, javaHome,\n        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n\n    MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n        localizedApplicationTokensFile);\n\n    // Construct the actual Container\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    return container;\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java"
    }
  }
}