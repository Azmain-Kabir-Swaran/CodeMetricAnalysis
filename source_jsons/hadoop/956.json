{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSStripedOutputStream.java",
  "functionName": "checkStreamerFailures",
  "functionId": "checkStreamerFailures___isNeedFlushAllPackets-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
  "functionStartLine": 657,
  "functionEndLine": 720,
  "numCommitsSeen": 73,
  "timeTaken": 3255,
  "changeHistory": [
    "db6252b6c3959220c6f985f940e2e731f99d8e30",
    "1d772dc5429bfffa015a1209e6f4a864505c871a",
    "df622cf4a32ee172ded6c4b3b97a1e49befc4f10",
    "ccd2ac60ecc5fccce56debf21a068e663c1d5f11",
    "5104077e1f431ad3675d0b1c5c3cf53936902d8e",
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd"
  ],
  "changeHistoryShort": {
    "db6252b6c3959220c6f985f940e2e731f99d8e30": "Ybodychange",
    "1d772dc5429bfffa015a1209e6f4a864505c871a": "Ybodychange",
    "df622cf4a32ee172ded6c4b3b97a1e49befc4f10": "Ymultichange(Yparameterchange,Ybodychange)",
    "ccd2ac60ecc5fccce56debf21a068e663c1d5f11": "Ybodychange",
    "5104077e1f431ad3675d0b1c5c3cf53936902d8e": "Ybodychange",
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd": "Ybodychange"
  },
  "changeHistoryDetails": {
    "db6252b6c3959220c6f985f940e2e731f99d8e30": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15210. EC : File write hanged when DN is shutdown by admin command. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "28/04/20 10:28 PM",
      "commitName": "db6252b6c3959220c6f985f940e2e731f99d8e30",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "15/03/20 8:14 AM",
      "commitNameOld": "1d772dc5429bfffa015a1209e6f4a864505c871a",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 44.59,
      "commitsBetweenForRepo": 143,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,64 @@\n   private void checkStreamerFailures(boolean isNeedFlushAllPackets)\n       throws IOException {\n     Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n     if (newFailed.size() \u003d\u003d 0) {\n       return;\n     }\n \n     if (isNeedFlushAllPackets) {\n       // for healthy streamers, wait till all of them have fetched the new block\n       // and flushed out all the enqueued packets.\n       flushAllInternals();\n     }\n     // recheck failed streamers again after the flush\n     newFailed \u003d checkStreamers();\n     while (newFailed.size() \u003e 0) {\n       failedStreamers.addAll(newFailed);\n       coordinator.clearFailureStates();\n       corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n \n       // mark all the healthy streamers as external error\n       Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n \n       // we have newly failed streamers, update block for pipeline\n       final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n \n       // wait till all the healthy streamers to\n       // 1) get the updated block info\n       // 2) create new block outputstream\n       newFailed \u003d waitCreatingStreamers(healthySet);\n       if (newFailed.size() + failedStreamers.size() \u003e\n           numAllBlocks - numDataBlocks) {\n         // The write has failed, Close all the streamers.\n         closeAllStreamers();\n         throw new IOException(\n             \"Data streamers failed while creating new block streams: \"\n                 + newFailed + \". There are not enough healthy streamers.\");\n       }\n       for (StripedDataStreamer failedStreamer : newFailed) {\n         assert !failedStreamer.isHealthy();\n       }\n \n       // TODO we can also succeed if all the failed streamers have not taken\n       // the updated block\n       if (newFailed.size() \u003d\u003d 0) {\n         // reset external error state of all the streamers\n         for (StripedDataStreamer streamer : healthySet) {\n           assert streamer.isHealthy();\n           streamer.getErrorState().reset();\n         }\n         updatePipeline(newBG);\n       }\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n       }\n+      //wait for get notify to failed stream\n+      if (newFailed.size() !\u003d 0) {\n+        try {\n+          Thread.sleep(datanodeRestartTimeout);\n+        } catch (InterruptedException e) {\n+          // Do nothing\n+        }\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void checkStreamerFailures(boolean isNeedFlushAllPackets)\n      throws IOException {\n    Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n    if (newFailed.size() \u003d\u003d 0) {\n      return;\n    }\n\n    if (isNeedFlushAllPackets) {\n      // for healthy streamers, wait till all of them have fetched the new block\n      // and flushed out all the enqueued packets.\n      flushAllInternals();\n    }\n    // recheck failed streamers again after the flush\n    newFailed \u003d checkStreamers();\n    while (newFailed.size() \u003e 0) {\n      failedStreamers.addAll(newFailed);\n      coordinator.clearFailureStates();\n      corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n\n      // mark all the healthy streamers as external error\n      Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n\n      // we have newly failed streamers, update block for pipeline\n      final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n\n      // wait till all the healthy streamers to\n      // 1) get the updated block info\n      // 2) create new block outputstream\n      newFailed \u003d waitCreatingStreamers(healthySet);\n      if (newFailed.size() + failedStreamers.size() \u003e\n          numAllBlocks - numDataBlocks) {\n        // The write has failed, Close all the streamers.\n        closeAllStreamers();\n        throw new IOException(\n            \"Data streamers failed while creating new block streams: \"\n                + newFailed + \". There are not enough healthy streamers.\");\n      }\n      for (StripedDataStreamer failedStreamer : newFailed) {\n        assert !failedStreamer.isHealthy();\n      }\n\n      // TODO we can also succeed if all the failed streamers have not taken\n      // the updated block\n      if (newFailed.size() \u003d\u003d 0) {\n        // reset external error state of all the streamers\n        for (StripedDataStreamer streamer : healthySet) {\n          assert streamer.isHealthy();\n          streamer.getErrorState().reset();\n        }\n        updatePipeline(newBG);\n      }\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n      }\n      //wait for get notify to failed stream\n      if (newFailed.size() !\u003d 0) {\n        try {\n          Thread.sleep(datanodeRestartTimeout);\n        } catch (InterruptedException e) {\n          // Do nothing\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "1d772dc5429bfffa015a1209e6f4a864505c871a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15211. EC: File write hangs during close in case of Exception during updatePipeline. Contributed by Ayush Saxena.\n",
      "commitDate": "15/03/20 8:14 AM",
      "commitName": "1d772dc5429bfffa015a1209e6f4a864505c871a",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "24/12/19 9:37 PM",
      "commitNameOld": "df622cf4a32ee172ded6c4b3b97a1e49befc4f10",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 81.4,
      "commitsBetweenForRepo": 272,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,56 @@\n   private void checkStreamerFailures(boolean isNeedFlushAllPackets)\n       throws IOException {\n     Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n     if (newFailed.size() \u003d\u003d 0) {\n       return;\n     }\n \n     if (isNeedFlushAllPackets) {\n       // for healthy streamers, wait till all of them have fetched the new block\n       // and flushed out all the enqueued packets.\n       flushAllInternals();\n     }\n     // recheck failed streamers again after the flush\n     newFailed \u003d checkStreamers();\n     while (newFailed.size() \u003e 0) {\n       failedStreamers.addAll(newFailed);\n       coordinator.clearFailureStates();\n       corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n \n       // mark all the healthy streamers as external error\n       Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n \n       // we have newly failed streamers, update block for pipeline\n       final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n \n       // wait till all the healthy streamers to\n       // 1) get the updated block info\n       // 2) create new block outputstream\n       newFailed \u003d waitCreatingStreamers(healthySet);\n       if (newFailed.size() + failedStreamers.size() \u003e\n           numAllBlocks - numDataBlocks) {\n+        // The write has failed, Close all the streamers.\n+        closeAllStreamers();\n         throw new IOException(\n             \"Data streamers failed while creating new block streams: \"\n                 + newFailed + \". There are not enough healthy streamers.\");\n       }\n       for (StripedDataStreamer failedStreamer : newFailed) {\n         assert !failedStreamer.isHealthy();\n       }\n \n       // TODO we can also succeed if all the failed streamers have not taken\n       // the updated block\n       if (newFailed.size() \u003d\u003d 0) {\n         // reset external error state of all the streamers\n         for (StripedDataStreamer streamer : healthySet) {\n           assert streamer.isHealthy();\n           streamer.getErrorState().reset();\n         }\n         updatePipeline(newBG);\n       }\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void checkStreamerFailures(boolean isNeedFlushAllPackets)\n      throws IOException {\n    Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n    if (newFailed.size() \u003d\u003d 0) {\n      return;\n    }\n\n    if (isNeedFlushAllPackets) {\n      // for healthy streamers, wait till all of them have fetched the new block\n      // and flushed out all the enqueued packets.\n      flushAllInternals();\n    }\n    // recheck failed streamers again after the flush\n    newFailed \u003d checkStreamers();\n    while (newFailed.size() \u003e 0) {\n      failedStreamers.addAll(newFailed);\n      coordinator.clearFailureStates();\n      corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n\n      // mark all the healthy streamers as external error\n      Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n\n      // we have newly failed streamers, update block for pipeline\n      final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n\n      // wait till all the healthy streamers to\n      // 1) get the updated block info\n      // 2) create new block outputstream\n      newFailed \u003d waitCreatingStreamers(healthySet);\n      if (newFailed.size() + failedStreamers.size() \u003e\n          numAllBlocks - numDataBlocks) {\n        // The write has failed, Close all the streamers.\n        closeAllStreamers();\n        throw new IOException(\n            \"Data streamers failed while creating new block streams: \"\n                + newFailed + \". There are not enough healthy streamers.\");\n      }\n      for (StripedDataStreamer failedStreamer : newFailed) {\n        assert !failedStreamer.isHealthy();\n      }\n\n      // TODO we can also succeed if all the failed streamers have not taken\n      // the updated block\n      if (newFailed.size() \u003d\u003d 0) {\n        // reset external error state of all the streamers\n        for (StripedDataStreamer streamer : healthySet) {\n          assert streamer.isHealthy();\n          streamer.getErrorState().reset();\n        }\n        updatePipeline(newBG);\n      }\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "df622cf4a32ee172ded6c4b3b97a1e49befc4f10": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-12999. When reach the end of the block group, it may not need to flush all the data packets(flushAllInternals) twice. Contributed by lufei and Fei Hui.\n",
      "commitDate": "24/12/19 9:37 PM",
      "commitName": "df622cf4a32ee172ded6c4b3b97a1e49befc4f10",
      "commitAuthor": "Ayush Saxena",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-12999. When reach the end of the block group, it may not need to flush all the data packets(flushAllInternals) twice. Contributed by lufei and Fei Hui.\n",
          "commitDate": "24/12/19 9:37 PM",
          "commitName": "df622cf4a32ee172ded6c4b3b97a1e49befc4f10",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "05/03/19 5:56 AM",
          "commitNameOld": "f940ab242da80a22bae95509d5c282d7e2f7ecdb",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 294.65,
          "commitsBetweenForRepo": 1993,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,54 @@\n-  private void checkStreamerFailures() throws IOException {\n+  private void checkStreamerFailures(boolean isNeedFlushAllPackets)\n+      throws IOException {\n     Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n     if (newFailed.size() \u003d\u003d 0) {\n       return;\n     }\n \n-    // for healthy streamers, wait till all of them have fetched the new block\n-    // and flushed out all the enqueued packets.\n-    flushAllInternals();\n+    if (isNeedFlushAllPackets) {\n+      // for healthy streamers, wait till all of them have fetched the new block\n+      // and flushed out all the enqueued packets.\n+      flushAllInternals();\n+    }\n     // recheck failed streamers again after the flush\n     newFailed \u003d checkStreamers();\n     while (newFailed.size() \u003e 0) {\n       failedStreamers.addAll(newFailed);\n       coordinator.clearFailureStates();\n       corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n \n       // mark all the healthy streamers as external error\n       Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n \n       // we have newly failed streamers, update block for pipeline\n       final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n \n       // wait till all the healthy streamers to\n       // 1) get the updated block info\n       // 2) create new block outputstream\n       newFailed \u003d waitCreatingStreamers(healthySet);\n       if (newFailed.size() + failedStreamers.size() \u003e\n           numAllBlocks - numDataBlocks) {\n         throw new IOException(\n             \"Data streamers failed while creating new block streams: \"\n                 + newFailed + \". There are not enough healthy streamers.\");\n       }\n       for (StripedDataStreamer failedStreamer : newFailed) {\n         assert !failedStreamer.isHealthy();\n       }\n \n       // TODO we can also succeed if all the failed streamers have not taken\n       // the updated block\n       if (newFailed.size() \u003d\u003d 0) {\n         // reset external error state of all the streamers\n         for (StripedDataStreamer streamer : healthySet) {\n           assert streamer.isHealthy();\n           streamer.getErrorState().reset();\n         }\n         updatePipeline(newBG);\n       }\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void checkStreamerFailures(boolean isNeedFlushAllPackets)\n      throws IOException {\n    Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n    if (newFailed.size() \u003d\u003d 0) {\n      return;\n    }\n\n    if (isNeedFlushAllPackets) {\n      // for healthy streamers, wait till all of them have fetched the new block\n      // and flushed out all the enqueued packets.\n      flushAllInternals();\n    }\n    // recheck failed streamers again after the flush\n    newFailed \u003d checkStreamers();\n    while (newFailed.size() \u003e 0) {\n      failedStreamers.addAll(newFailed);\n      coordinator.clearFailureStates();\n      corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n\n      // mark all the healthy streamers as external error\n      Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n\n      // we have newly failed streamers, update block for pipeline\n      final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n\n      // wait till all the healthy streamers to\n      // 1) get the updated block info\n      // 2) create new block outputstream\n      newFailed \u003d waitCreatingStreamers(healthySet);\n      if (newFailed.size() + failedStreamers.size() \u003e\n          numAllBlocks - numDataBlocks) {\n        throw new IOException(\n            \"Data streamers failed while creating new block streams: \"\n                + newFailed + \". There are not enough healthy streamers.\");\n      }\n      for (StripedDataStreamer failedStreamer : newFailed) {\n        assert !failedStreamer.isHealthy();\n      }\n\n      // TODO we can also succeed if all the failed streamers have not taken\n      // the updated block\n      if (newFailed.size() \u003d\u003d 0) {\n        // reset external error state of all the streamers\n        for (StripedDataStreamer streamer : healthySet) {\n          assert streamer.isHealthy();\n          streamer.getErrorState().reset();\n        }\n        updatePipeline(newBG);\n      }\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[isNeedFlushAllPackets-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-12999. When reach the end of the block group, it may not need to flush all the data packets(flushAllInternals) twice. Contributed by lufei and Fei Hui.\n",
          "commitDate": "24/12/19 9:37 PM",
          "commitName": "df622cf4a32ee172ded6c4b3b97a1e49befc4f10",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "05/03/19 5:56 AM",
          "commitNameOld": "f940ab242da80a22bae95509d5c282d7e2f7ecdb",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 294.65,
          "commitsBetweenForRepo": 1993,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,54 @@\n-  private void checkStreamerFailures() throws IOException {\n+  private void checkStreamerFailures(boolean isNeedFlushAllPackets)\n+      throws IOException {\n     Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n     if (newFailed.size() \u003d\u003d 0) {\n       return;\n     }\n \n-    // for healthy streamers, wait till all of them have fetched the new block\n-    // and flushed out all the enqueued packets.\n-    flushAllInternals();\n+    if (isNeedFlushAllPackets) {\n+      // for healthy streamers, wait till all of them have fetched the new block\n+      // and flushed out all the enqueued packets.\n+      flushAllInternals();\n+    }\n     // recheck failed streamers again after the flush\n     newFailed \u003d checkStreamers();\n     while (newFailed.size() \u003e 0) {\n       failedStreamers.addAll(newFailed);\n       coordinator.clearFailureStates();\n       corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n \n       // mark all the healthy streamers as external error\n       Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n \n       // we have newly failed streamers, update block for pipeline\n       final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n \n       // wait till all the healthy streamers to\n       // 1) get the updated block info\n       // 2) create new block outputstream\n       newFailed \u003d waitCreatingStreamers(healthySet);\n       if (newFailed.size() + failedStreamers.size() \u003e\n           numAllBlocks - numDataBlocks) {\n         throw new IOException(\n             \"Data streamers failed while creating new block streams: \"\n                 + newFailed + \". There are not enough healthy streamers.\");\n       }\n       for (StripedDataStreamer failedStreamer : newFailed) {\n         assert !failedStreamer.isHealthy();\n       }\n \n       // TODO we can also succeed if all the failed streamers have not taken\n       // the updated block\n       if (newFailed.size() \u003d\u003d 0) {\n         // reset external error state of all the streamers\n         for (StripedDataStreamer streamer : healthySet) {\n           assert streamer.isHealthy();\n           streamer.getErrorState().reset();\n         }\n         updatePipeline(newBG);\n       }\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void checkStreamerFailures(boolean isNeedFlushAllPackets)\n      throws IOException {\n    Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n    if (newFailed.size() \u003d\u003d 0) {\n      return;\n    }\n\n    if (isNeedFlushAllPackets) {\n      // for healthy streamers, wait till all of them have fetched the new block\n      // and flushed out all the enqueued packets.\n      flushAllInternals();\n    }\n    // recheck failed streamers again after the flush\n    newFailed \u003d checkStreamers();\n    while (newFailed.size() \u003e 0) {\n      failedStreamers.addAll(newFailed);\n      coordinator.clearFailureStates();\n      corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n\n      // mark all the healthy streamers as external error\n      Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n\n      // we have newly failed streamers, update block for pipeline\n      final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n\n      // wait till all the healthy streamers to\n      // 1) get the updated block info\n      // 2) create new block outputstream\n      newFailed \u003d waitCreatingStreamers(healthySet);\n      if (newFailed.size() + failedStreamers.size() \u003e\n          numAllBlocks - numDataBlocks) {\n        throw new IOException(\n            \"Data streamers failed while creating new block streams: \"\n                + newFailed + \". There are not enough healthy streamers.\");\n      }\n      for (StripedDataStreamer failedStreamer : newFailed) {\n        assert !failedStreamer.isHealthy();\n      }\n\n      // TODO we can also succeed if all the failed streamers have not taken\n      // the updated block\n      if (newFailed.size() \u003d\u003d 0) {\n        // reset external error state of all the streamers\n        for (StripedDataStreamer streamer : healthySet) {\n          assert streamer.isHealthy();\n          streamer.getErrorState().reset();\n        }\n        updatePipeline(newBG);\n      }\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "ccd2ac60ecc5fccce56debf21a068e663c1d5f11": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11882. Precisely calculate acked length of striped block groups in updatePipeline.\n",
      "commitDate": "05/09/17 2:16 PM",
      "commitName": "ccd2ac60ecc5fccce56debf21a068e663c1d5f11",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "05/09/17 2:46 AM",
      "commitNameOld": "5dba54596a1587e0ba5f9f02f40483e597b0df64",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 0.48,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   private void checkStreamerFailures() throws IOException {\n     Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n     if (newFailed.size() \u003d\u003d 0) {\n       return;\n     }\n \n     // for healthy streamers, wait till all of them have fetched the new block\n     // and flushed out all the enqueued packets.\n     flushAllInternals();\n     // recheck failed streamers again after the flush\n     newFailed \u003d checkStreamers();\n     while (newFailed.size() \u003e 0) {\n       failedStreamers.addAll(newFailed);\n       coordinator.clearFailureStates();\n       corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n \n       // mark all the healthy streamers as external error\n       Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n \n       // we have newly failed streamers, update block for pipeline\n       final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n \n       // wait till all the healthy streamers to\n       // 1) get the updated block info\n       // 2) create new block outputstream\n-      newFailed \u003d waitCreatingNewStreams(healthySet);\n+      newFailed \u003d waitCreatingStreamers(healthySet);\n       if (newFailed.size() + failedStreamers.size() \u003e\n           numAllBlocks - numDataBlocks) {\n         throw new IOException(\n             \"Data streamers failed while creating new block streams: \"\n                 + newFailed + \". There are not enough healthy streamers.\");\n       }\n       for (StripedDataStreamer failedStreamer : newFailed) {\n         assert !failedStreamer.isHealthy();\n       }\n \n       // TODO we can also succeed if all the failed streamers have not taken\n       // the updated block\n       if (newFailed.size() \u003d\u003d 0) {\n         // reset external error state of all the streamers\n         for (StripedDataStreamer streamer : healthySet) {\n           assert streamer.isHealthy();\n           streamer.getErrorState().reset();\n         }\n         updatePipeline(newBG);\n       }\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void checkStreamerFailures() throws IOException {\n    Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n    if (newFailed.size() \u003d\u003d 0) {\n      return;\n    }\n\n    // for healthy streamers, wait till all of them have fetched the new block\n    // and flushed out all the enqueued packets.\n    flushAllInternals();\n    // recheck failed streamers again after the flush\n    newFailed \u003d checkStreamers();\n    while (newFailed.size() \u003e 0) {\n      failedStreamers.addAll(newFailed);\n      coordinator.clearFailureStates();\n      corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n\n      // mark all the healthy streamers as external error\n      Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n\n      // we have newly failed streamers, update block for pipeline\n      final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n\n      // wait till all the healthy streamers to\n      // 1) get the updated block info\n      // 2) create new block outputstream\n      newFailed \u003d waitCreatingStreamers(healthySet);\n      if (newFailed.size() + failedStreamers.size() \u003e\n          numAllBlocks - numDataBlocks) {\n        throw new IOException(\n            \"Data streamers failed while creating new block streams: \"\n                + newFailed + \". There are not enough healthy streamers.\");\n      }\n      for (StripedDataStreamer failedStreamer : newFailed) {\n        assert !failedStreamer.isHealthy();\n      }\n\n      // TODO we can also succeed if all the failed streamers have not taken\n      // the updated block\n      if (newFailed.size() \u003d\u003d 0) {\n        // reset external error state of all the streamers\n        for (StripedDataStreamer streamer : healthySet) {\n          assert streamer.isHealthy();\n          streamer.getErrorState().reset();\n        }\n        updatePipeline(newBG);\n      }\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "5104077e1f431ad3675d0b1c5c3cf53936902d8e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9373. Erasure coding: friendly log information for write operations with some failed streamers. Contributed by Li Bo.\n\nChange-Id: Ie8ab4ae00e9ee0eb03c32a54bea26a3524308038\n",
      "commitDate": "17/12/15 1:05 PM",
      "commitName": "5104077e1f431ad3675d0b1c5c3cf53936902d8e",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "02/12/15 5:39 PM",
      "commitNameOld": "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 14.81,
      "commitsBetweenForRepo": 93,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,51 @@\n   private void checkStreamerFailures() throws IOException {\n     Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n     if (newFailed.size() \u003d\u003d 0) {\n       return;\n     }\n \n     // for healthy streamers, wait till all of them have fetched the new block\n     // and flushed out all the enqueued packets.\n     flushAllInternals();\n     // recheck failed streamers again after the flush\n     newFailed \u003d checkStreamers();\n     while (newFailed.size() \u003e 0) {\n       failedStreamers.addAll(newFailed);\n       coordinator.clearFailureStates();\n+      corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n \n       // mark all the healthy streamers as external error\n       Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n \n       // we have newly failed streamers, update block for pipeline\n       final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n \n       // wait till all the healthy streamers to\n       // 1) get the updated block info\n       // 2) create new block outputstream\n       newFailed \u003d waitCreatingNewStreams(healthySet);\n       if (newFailed.size() + failedStreamers.size() \u003e\n           numAllBlocks - numDataBlocks) {\n         throw new IOException(\n             \"Data streamers failed while creating new block streams: \"\n                 + newFailed + \". There are not enough healthy streamers.\");\n       }\n       for (StripedDataStreamer failedStreamer : newFailed) {\n         assert !failedStreamer.isHealthy();\n       }\n \n       // TODO we can also succeed if all the failed streamers have not taken\n       // the updated block\n       if (newFailed.size() \u003d\u003d 0) {\n         // reset external error state of all the streamers\n         for (StripedDataStreamer streamer : healthySet) {\n           assert streamer.isHealthy();\n           streamer.getErrorState().reset();\n         }\n         updatePipeline(newBG);\n       }\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void checkStreamerFailures() throws IOException {\n    Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n    if (newFailed.size() \u003d\u003d 0) {\n      return;\n    }\n\n    // for healthy streamers, wait till all of them have fetched the new block\n    // and flushed out all the enqueued packets.\n    flushAllInternals();\n    // recheck failed streamers again after the flush\n    newFailed \u003d checkStreamers();\n    while (newFailed.size() \u003e 0) {\n      failedStreamers.addAll(newFailed);\n      coordinator.clearFailureStates();\n      corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\n\n      // mark all the healthy streamers as external error\n      Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n\n      // we have newly failed streamers, update block for pipeline\n      final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n\n      // wait till all the healthy streamers to\n      // 1) get the updated block info\n      // 2) create new block outputstream\n      newFailed \u003d waitCreatingNewStreams(healthySet);\n      if (newFailed.size() + failedStreamers.size() \u003e\n          numAllBlocks - numDataBlocks) {\n        throw new IOException(\n            \"Data streamers failed while creating new block streams: \"\n                + newFailed + \". There are not enough healthy streamers.\");\n      }\n      for (StripedDataStreamer failedStreamer : newFailed) {\n        assert !failedStreamer.isHealthy();\n      }\n\n      // TODO we can also succeed if all the failed streamers have not taken\n      // the updated block\n      if (newFailed.size() \u003d\u003d 0) {\n        // reset external error state of all the streamers\n        for (StripedDataStreamer streamer : healthySet) {\n          assert streamer.isHealthy();\n          streamer.getErrorState().reset();\n        }\n        updatePipeline(newBG);\n      }\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9180. Update excluded DataNodes in DFSStripedOutputStream based on failures in data streamers. Contributed by Jing Zhao.\n",
      "commitDate": "06/10/15 10:56 AM",
      "commitName": "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 2.97,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,50 @@\n   private void checkStreamerFailures() throws IOException {\n     Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n-    if (newFailed.size() \u003e 0) {\n-      // for healthy streamers, wait till all of them have fetched the new block\n-      // and flushed out all the enqueued packets.\n-      flushAllInternals();\n+    if (newFailed.size() \u003d\u003d 0) {\n+      return;\n     }\n-    // get all the current failed streamers after the flush\n+\n+    // for healthy streamers, wait till all of them have fetched the new block\n+    // and flushed out all the enqueued packets.\n+    flushAllInternals();\n+    // recheck failed streamers again after the flush\n     newFailed \u003d checkStreamers();\n     while (newFailed.size() \u003e 0) {\n       failedStreamers.addAll(newFailed);\n       coordinator.clearFailureStates();\n \n       // mark all the healthy streamers as external error\n       Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n \n       // we have newly failed streamers, update block for pipeline\n       final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n \n       // wait till all the healthy streamers to\n       // 1) get the updated block info\n       // 2) create new block outputstream\n       newFailed \u003d waitCreatingNewStreams(healthySet);\n       if (newFailed.size() + failedStreamers.size() \u003e\n           numAllBlocks - numDataBlocks) {\n         throw new IOException(\n             \"Data streamers failed while creating new block streams: \"\n                 + newFailed + \". There are not enough healthy streamers.\");\n       }\n       for (StripedDataStreamer failedStreamer : newFailed) {\n         assert !failedStreamer.isHealthy();\n       }\n \n       // TODO we can also succeed if all the failed streamers have not taken\n       // the updated block\n       if (newFailed.size() \u003d\u003d 0) {\n         // reset external error state of all the streamers\n         for (StripedDataStreamer streamer : healthySet) {\n           assert streamer.isHealthy();\n           streamer.getErrorState().reset();\n         }\n         updatePipeline(newBG);\n       }\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void checkStreamerFailures() throws IOException {\n    Set\u003cStripedDataStreamer\u003e newFailed \u003d checkStreamers();\n    if (newFailed.size() \u003d\u003d 0) {\n      return;\n    }\n\n    // for healthy streamers, wait till all of them have fetched the new block\n    // and flushed out all the enqueued packets.\n    flushAllInternals();\n    // recheck failed streamers again after the flush\n    newFailed \u003d checkStreamers();\n    while (newFailed.size() \u003e 0) {\n      failedStreamers.addAll(newFailed);\n      coordinator.clearFailureStates();\n\n      // mark all the healthy streamers as external error\n      Set\u003cStripedDataStreamer\u003e healthySet \u003d markExternalErrorOnStreamers();\n\n      // we have newly failed streamers, update block for pipeline\n      final ExtendedBlock newBG \u003d updateBlockForPipeline(healthySet);\n\n      // wait till all the healthy streamers to\n      // 1) get the updated block info\n      // 2) create new block outputstream\n      newFailed \u003d waitCreatingNewStreams(healthySet);\n      if (newFailed.size() + failedStreamers.size() \u003e\n          numAllBlocks - numDataBlocks) {\n        throw new IOException(\n            \"Data streamers failed while creating new block streams: \"\n                + newFailed + \". There are not enough healthy streamers.\");\n      }\n      for (StripedDataStreamer failedStreamer : newFailed) {\n        assert !failedStreamer.isHealthy();\n      }\n\n      // TODO we can also succeed if all the failed streamers have not taken\n      // the updated block\n      if (newFailed.size() \u003d\u003d 0) {\n        // reset external error state of all the streamers\n        for (StripedDataStreamer streamer : healthySet) {\n          assert streamer.isHealthy();\n          streamer.getErrorState().reset();\n        }\n        updatePipeline(newBG);\n      }\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        coordinator.offerStreamerUpdateResult(i, newFailed.size() \u003d\u003d 0);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    }
  }
}