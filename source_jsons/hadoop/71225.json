{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CreateFileMapper.java",
  "functionName": "map",
  "functionId": "map___key-NullWritable__value-NullWritable__mapperContext-Mapper.Context",
  "sourceFilePath": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/CreateFileMapper.java",
  "functionStartLine": 101,
  "functionEndLine": 146,
  "numCommitsSeen": 3,
  "timeTaken": 367,
  "changeHistory": [
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91"
  ],
  "changeHistoryShort": {
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": "Yintroduced"
  },
  "changeHistoryDetails": {
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-12345 Add Dynamometer to hadoop-tools, a tool for scale testing the HDFS NameNode with real metadata and workloads. Contributed by Erik Krogen.\n",
      "commitDate": "25/06/19 8:07 AM",
      "commitName": "ab0b180ddb5d0775a2452d5eeb7badd252aadb91",
      "commitAuthor": "Erik Krogen",
      "diff": "@@ -0,0 +1,46 @@\n+  public void map(NullWritable key, NullWritable value,\n+      Mapper.Context mapperContext) throws IOException, InterruptedException {\n+    taskID \u003d mapperContext.getTaskAttemptID().getTaskID().getId();\n+    conf \u003d mapperContext.getConfiguration();\n+    String namenodeURI \u003d conf.get(WorkloadDriver.NN_URI);\n+    startTimestampMs \u003d conf.getLong(WorkloadDriver.START_TIMESTAMP_MS, -1);\n+    fileParentPath \u003d conf.get(FILE_PARENT_PATH_KEY, FILE_PARENT_PATH_DEFAULT);\n+    shouldDelete \u003d conf.getBoolean(SHOULD_DELETE_KEY, SHOULD_DELETE_DEFAULT);\n+    int durationMin \u003d conf.getInt(DURATION_MIN_KEY, -1);\n+    if (durationMin \u003c 0) {\n+      throw new IOException(\"Duration must be positive; got: \" + durationMin);\n+    }\n+    endTimeStampMs \u003d startTimestampMs\n+        + TimeUnit.MILLISECONDS.convert(durationMin, TimeUnit.MINUTES);\n+    fs \u003d FileSystem.get(URI.create(namenodeURI), conf);\n+    System.out.println(\"Start timestamp: \" + startTimestampMs);\n+\n+    long currentEpoch \u003d System.currentTimeMillis();\n+    long delay \u003d startTimestampMs - currentEpoch;\n+    if (delay \u003e 0) {\n+      System.out.println(\"Sleeping for \" + delay + \" ms\");\n+      Thread.sleep(delay);\n+    }\n+\n+    String mapperSpecifcPathPrefix \u003d fileParentPath + \"/mapper\" + taskID;\n+    System.out.println(\"Mapper path prefix: \" + mapperSpecifcPathPrefix);\n+    long numFilesCreated \u003d 0;\n+    Path path;\n+    final byte[] content \u003d {0x0};\n+    while (System.currentTimeMillis() \u003c endTimeStampMs) {\n+      path \u003d new Path(mapperSpecifcPathPrefix + \"/file\" + numFilesCreated);\n+      OutputStream out \u003d fs.create(path);\n+      out.write(content);\n+      out.close();\n+      numFilesCreated++;\n+      mapperContext.getCounter(CREATEFILECOUNTERS.NUMFILESCREATED)\n+          .increment(1L);\n+      if (numFilesCreated % 1000 \u003d\u003d 0) {\n+        mapperContext.progress();\n+        System.out.println(\"Number of files created: \" + numFilesCreated);\n+      }\n+      if (shouldDelete) {\n+        fs.delete(path, true);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void map(NullWritable key, NullWritable value,\n      Mapper.Context mapperContext) throws IOException, InterruptedException {\n    taskID \u003d mapperContext.getTaskAttemptID().getTaskID().getId();\n    conf \u003d mapperContext.getConfiguration();\n    String namenodeURI \u003d conf.get(WorkloadDriver.NN_URI);\n    startTimestampMs \u003d conf.getLong(WorkloadDriver.START_TIMESTAMP_MS, -1);\n    fileParentPath \u003d conf.get(FILE_PARENT_PATH_KEY, FILE_PARENT_PATH_DEFAULT);\n    shouldDelete \u003d conf.getBoolean(SHOULD_DELETE_KEY, SHOULD_DELETE_DEFAULT);\n    int durationMin \u003d conf.getInt(DURATION_MIN_KEY, -1);\n    if (durationMin \u003c 0) {\n      throw new IOException(\"Duration must be positive; got: \" + durationMin);\n    }\n    endTimeStampMs \u003d startTimestampMs\n        + TimeUnit.MILLISECONDS.convert(durationMin, TimeUnit.MINUTES);\n    fs \u003d FileSystem.get(URI.create(namenodeURI), conf);\n    System.out.println(\"Start timestamp: \" + startTimestampMs);\n\n    long currentEpoch \u003d System.currentTimeMillis();\n    long delay \u003d startTimestampMs - currentEpoch;\n    if (delay \u003e 0) {\n      System.out.println(\"Sleeping for \" + delay + \" ms\");\n      Thread.sleep(delay);\n    }\n\n    String mapperSpecifcPathPrefix \u003d fileParentPath + \"/mapper\" + taskID;\n    System.out.println(\"Mapper path prefix: \" + mapperSpecifcPathPrefix);\n    long numFilesCreated \u003d 0;\n    Path path;\n    final byte[] content \u003d {0x0};\n    while (System.currentTimeMillis() \u003c endTimeStampMs) {\n      path \u003d new Path(mapperSpecifcPathPrefix + \"/file\" + numFilesCreated);\n      OutputStream out \u003d fs.create(path);\n      out.write(content);\n      out.close();\n      numFilesCreated++;\n      mapperContext.getCounter(CREATEFILECOUNTERS.NUMFILESCREATED)\n          .increment(1L);\n      if (numFilesCreated % 1000 \u003d\u003d 0) {\n        mapperContext.progress();\n        System.out.println(\"Number of files created: \" + numFilesCreated);\n      }\n      if (shouldDelete) {\n        fs.delete(path, true);\n      }\n    }\n  }",
      "path": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/CreateFileMapper.java"
    }
  }
}