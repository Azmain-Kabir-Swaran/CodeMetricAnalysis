{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DynamicInputFormat.java",
  "functionName": "splitCopyListingIntoChunksWithShuffle",
  "functionId": "splitCopyListingIntoChunksWithShuffle___context-JobContext",
  "sourceFilePath": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java",
  "functionStartLine": 113,
  "functionEndLine": 180,
  "numCommitsSeen": 6,
  "timeTaken": 1046,
  "changeHistory": [
    "2868ca0328d908056745223fb38d9a90fd2811ba",
    "11be7334c4e04b1b3fe12d86f4646cc83c068b05",
    "03db13206f131d93347651513496e1b3fcff3dba",
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67"
  ],
  "changeHistoryShort": {
    "2868ca0328d908056745223fb38d9a90fd2811ba": "Ybodychange",
    "11be7334c4e04b1b3fe12d86f4646cc83c068b05": "Ybodychange",
    "03db13206f131d93347651513496e1b3fcff3dba": "Ybodychange",
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67": "Yintroduced"
  },
  "changeHistoryDetails": {
    "2868ca0328d908056745223fb38d9a90fd2811ba": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6451. DistCp has incorrect chunkFilePath for multiple jobs when strategy is dynamic. Contributed by Kuhu Shukla.\n",
      "commitDate": "30/10/15 12:56 PM",
      "commitName": "2868ca0328d908056745223fb38d9a90fd2811ba",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "03/03/15 1:06 AM",
      "commitNameOld": "9ae7f9eb7baeb244e1b95aabc93ad8124870b9a9",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 241.45,
      "commitsBetweenForRepo": 2078,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,68 @@\n   private List\u003cDynamicInputChunk\u003e splitCopyListingIntoChunksWithShuffle\n                                     (JobContext context) throws IOException {\n \n     final Configuration configuration \u003d context.getConfiguration();\n     int numRecords \u003d getNumberOfRecords(configuration);\n     int numMaps \u003d getNumMapTasks(configuration);\n     int maxChunksTolerable \u003d getMaxChunksTolerable(configuration);\n \n     // Number of chunks each map will process, on average.\n     int splitRatio \u003d getListingSplitRatio(configuration, numMaps, numRecords);\n     validateNumChunksUsing(splitRatio, numMaps, maxChunksTolerable);\n \n     int numEntriesPerChunk \u003d (int)Math.ceil((float)numRecords\n                                           /(splitRatio * numMaps));\n     DistCpUtils.publish(context.getConfiguration(),\n                         CONF_LABEL_NUM_ENTRIES_PER_CHUNK,\n                         numEntriesPerChunk);\n \n     final int nChunksTotal \u003d (int)Math.ceil((float)numRecords/numEntriesPerChunk);\n     int nChunksOpenAtOnce\n             \u003d Math.min(N_CHUNKS_OPEN_AT_ONCE_DEFAULT, nChunksTotal);\n \n     Path listingPath \u003d getListingFilePath(configuration);\n     SequenceFile.Reader reader\n             \u003d new SequenceFile.Reader(configuration,\n                                       SequenceFile.Reader.file(listingPath));\n \n     List\u003cDynamicInputChunk\u003e openChunks\n                   \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n     \n     List\u003cDynamicInputChunk\u003e chunksFinal \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n \n     CopyListingFileStatus fileStatus \u003d new CopyListingFileStatus();\n     Text relPath \u003d new Text();\n     int recordCounter \u003d 0;\n     int chunkCount \u003d 0;\n \n     try {\n \n       while (reader.next(relPath, fileStatus)) {\n         if (recordCounter % (nChunksOpenAtOnce*numEntriesPerChunk) \u003d\u003d 0) {\n           // All chunks full. Create new chunk-set.\n           closeAll(openChunks);\n           chunksFinal.addAll(openChunks);\n \n-          openChunks \u003d createChunks(\n-                  configuration, chunkCount, nChunksTotal, nChunksOpenAtOnce);\n+          openChunks \u003d createChunks(chunkCount, nChunksTotal,\n+              nChunksOpenAtOnce);\n \n           chunkCount +\u003d openChunks.size();\n \n           nChunksOpenAtOnce \u003d openChunks.size();\n           recordCounter \u003d 0;\n         }\n \n         // Shuffle into open chunks.\n         openChunks.get(recordCounter%nChunksOpenAtOnce).write(relPath, fileStatus);\n         ++recordCounter;\n       }\n \n     } finally {\n       closeAll(openChunks);\n       chunksFinal.addAll(openChunks);\n       IOUtils.closeStream(reader);\n     }\n \n     LOG.info(\"Number of dynamic-chunk-files created: \" + chunksFinal.size()); \n     return chunksFinal;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private List\u003cDynamicInputChunk\u003e splitCopyListingIntoChunksWithShuffle\n                                    (JobContext context) throws IOException {\n\n    final Configuration configuration \u003d context.getConfiguration();\n    int numRecords \u003d getNumberOfRecords(configuration);\n    int numMaps \u003d getNumMapTasks(configuration);\n    int maxChunksTolerable \u003d getMaxChunksTolerable(configuration);\n\n    // Number of chunks each map will process, on average.\n    int splitRatio \u003d getListingSplitRatio(configuration, numMaps, numRecords);\n    validateNumChunksUsing(splitRatio, numMaps, maxChunksTolerable);\n\n    int numEntriesPerChunk \u003d (int)Math.ceil((float)numRecords\n                                          /(splitRatio * numMaps));\n    DistCpUtils.publish(context.getConfiguration(),\n                        CONF_LABEL_NUM_ENTRIES_PER_CHUNK,\n                        numEntriesPerChunk);\n\n    final int nChunksTotal \u003d (int)Math.ceil((float)numRecords/numEntriesPerChunk);\n    int nChunksOpenAtOnce\n            \u003d Math.min(N_CHUNKS_OPEN_AT_ONCE_DEFAULT, nChunksTotal);\n\n    Path listingPath \u003d getListingFilePath(configuration);\n    SequenceFile.Reader reader\n            \u003d new SequenceFile.Reader(configuration,\n                                      SequenceFile.Reader.file(listingPath));\n\n    List\u003cDynamicInputChunk\u003e openChunks\n                  \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n    \n    List\u003cDynamicInputChunk\u003e chunksFinal \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n\n    CopyListingFileStatus fileStatus \u003d new CopyListingFileStatus();\n    Text relPath \u003d new Text();\n    int recordCounter \u003d 0;\n    int chunkCount \u003d 0;\n\n    try {\n\n      while (reader.next(relPath, fileStatus)) {\n        if (recordCounter % (nChunksOpenAtOnce*numEntriesPerChunk) \u003d\u003d 0) {\n          // All chunks full. Create new chunk-set.\n          closeAll(openChunks);\n          chunksFinal.addAll(openChunks);\n\n          openChunks \u003d createChunks(chunkCount, nChunksTotal,\n              nChunksOpenAtOnce);\n\n          chunkCount +\u003d openChunks.size();\n\n          nChunksOpenAtOnce \u003d openChunks.size();\n          recordCounter \u003d 0;\n        }\n\n        // Shuffle into open chunks.\n        openChunks.get(recordCounter%nChunksOpenAtOnce).write(relPath, fileStatus);\n        ++recordCounter;\n      }\n\n    } finally {\n      closeAll(openChunks);\n      chunksFinal.addAll(openChunks);\n      IOUtils.closeStream(reader);\n    }\n\n    LOG.info(\"Number of dynamic-chunk-files created: \" + chunksFinal.size()); \n    return chunksFinal;\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java",
      "extendedDetails": {}
    },
    "11be7334c4e04b1b3fe12d86f4646cc83c068b05": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5809. Enhance distcp to support preserving HDFS ACLs. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595283 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/05/14 11:25 AM",
      "commitName": "11be7334c4e04b1b3fe12d86f4646cc83c068b05",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "06/05/14 3:24 AM",
      "commitNameOld": "03db13206f131d93347651513496e1b3fcff3dba",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 10.33,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,68 @@\n   private List\u003cDynamicInputChunk\u003e splitCopyListingIntoChunksWithShuffle\n                                     (JobContext context) throws IOException {\n \n     final Configuration configuration \u003d context.getConfiguration();\n     int numRecords \u003d getNumberOfRecords(configuration);\n     int numMaps \u003d getNumMapTasks(configuration);\n     int maxChunksTolerable \u003d getMaxChunksTolerable(configuration);\n \n     // Number of chunks each map will process, on average.\n     int splitRatio \u003d getListingSplitRatio(configuration, numMaps, numRecords);\n     validateNumChunksUsing(splitRatio, numMaps, maxChunksTolerable);\n \n     int numEntriesPerChunk \u003d (int)Math.ceil((float)numRecords\n                                           /(splitRatio * numMaps));\n     DistCpUtils.publish(context.getConfiguration(),\n                         CONF_LABEL_NUM_ENTRIES_PER_CHUNK,\n                         numEntriesPerChunk);\n \n     final int nChunksTotal \u003d (int)Math.ceil((float)numRecords/numEntriesPerChunk);\n     int nChunksOpenAtOnce\n             \u003d Math.min(N_CHUNKS_OPEN_AT_ONCE_DEFAULT, nChunksTotal);\n \n     Path listingPath \u003d getListingFilePath(configuration);\n     SequenceFile.Reader reader\n             \u003d new SequenceFile.Reader(configuration,\n                                       SequenceFile.Reader.file(listingPath));\n \n     List\u003cDynamicInputChunk\u003e openChunks\n                   \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n     \n     List\u003cDynamicInputChunk\u003e chunksFinal \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n \n-    FileStatus fileStatus \u003d new FileStatus();\n+    CopyListingFileStatus fileStatus \u003d new CopyListingFileStatus();\n     Text relPath \u003d new Text();\n     int recordCounter \u003d 0;\n     int chunkCount \u003d 0;\n \n     try {\n \n       while (reader.next(relPath, fileStatus)) {\n         if (recordCounter % (nChunksOpenAtOnce*numEntriesPerChunk) \u003d\u003d 0) {\n           // All chunks full. Create new chunk-set.\n           closeAll(openChunks);\n           chunksFinal.addAll(openChunks);\n \n           openChunks \u003d createChunks(\n                   configuration, chunkCount, nChunksTotal, nChunksOpenAtOnce);\n \n           chunkCount +\u003d openChunks.size();\n \n           nChunksOpenAtOnce \u003d openChunks.size();\n           recordCounter \u003d 0;\n         }\n \n         // Shuffle into open chunks.\n         openChunks.get(recordCounter%nChunksOpenAtOnce).write(relPath, fileStatus);\n         ++recordCounter;\n       }\n \n     } finally {\n       closeAll(openChunks);\n       chunksFinal.addAll(openChunks);\n       IOUtils.closeStream(reader);\n     }\n \n     LOG.info(\"Number of dynamic-chunk-files created: \" + chunksFinal.size()); \n     return chunksFinal;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private List\u003cDynamicInputChunk\u003e splitCopyListingIntoChunksWithShuffle\n                                    (JobContext context) throws IOException {\n\n    final Configuration configuration \u003d context.getConfiguration();\n    int numRecords \u003d getNumberOfRecords(configuration);\n    int numMaps \u003d getNumMapTasks(configuration);\n    int maxChunksTolerable \u003d getMaxChunksTolerable(configuration);\n\n    // Number of chunks each map will process, on average.\n    int splitRatio \u003d getListingSplitRatio(configuration, numMaps, numRecords);\n    validateNumChunksUsing(splitRatio, numMaps, maxChunksTolerable);\n\n    int numEntriesPerChunk \u003d (int)Math.ceil((float)numRecords\n                                          /(splitRatio * numMaps));\n    DistCpUtils.publish(context.getConfiguration(),\n                        CONF_LABEL_NUM_ENTRIES_PER_CHUNK,\n                        numEntriesPerChunk);\n\n    final int nChunksTotal \u003d (int)Math.ceil((float)numRecords/numEntriesPerChunk);\n    int nChunksOpenAtOnce\n            \u003d Math.min(N_CHUNKS_OPEN_AT_ONCE_DEFAULT, nChunksTotal);\n\n    Path listingPath \u003d getListingFilePath(configuration);\n    SequenceFile.Reader reader\n            \u003d new SequenceFile.Reader(configuration,\n                                      SequenceFile.Reader.file(listingPath));\n\n    List\u003cDynamicInputChunk\u003e openChunks\n                  \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n    \n    List\u003cDynamicInputChunk\u003e chunksFinal \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n\n    CopyListingFileStatus fileStatus \u003d new CopyListingFileStatus();\n    Text relPath \u003d new Text();\n    int recordCounter \u003d 0;\n    int chunkCount \u003d 0;\n\n    try {\n\n      while (reader.next(relPath, fileStatus)) {\n        if (recordCounter % (nChunksOpenAtOnce*numEntriesPerChunk) \u003d\u003d 0) {\n          // All chunks full. Create new chunk-set.\n          closeAll(openChunks);\n          chunksFinal.addAll(openChunks);\n\n          openChunks \u003d createChunks(\n                  configuration, chunkCount, nChunksTotal, nChunksOpenAtOnce);\n\n          chunkCount +\u003d openChunks.size();\n\n          nChunksOpenAtOnce \u003d openChunks.size();\n          recordCounter \u003d 0;\n        }\n\n        // Shuffle into open chunks.\n        openChunks.get(recordCounter%nChunksOpenAtOnce).write(relPath, fileStatus);\n        ++recordCounter;\n      }\n\n    } finally {\n      closeAll(openChunks);\n      chunksFinal.addAll(openChunks);\n      IOUtils.closeStream(reader);\n    }\n\n    LOG.info(\"Number of dynamic-chunk-files created: \" + chunksFinal.size()); \n    return chunksFinal;\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java",
      "extendedDetails": {}
    },
    "03db13206f131d93347651513496e1b3fcff3dba": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5402. In DynamicInputFormat, change MAX_CHUNKS_TOLERABLE, MAX_CHUNKS_IDEAL, MIN_RECORDS_PER_CHUNK and SPLIT_RATIO to be configurable.  Contributed by Tsuyoshi OZAWA\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1592703 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/05/14 3:24 AM",
      "commitName": "03db13206f131d93347651513496e1b3fcff3dba",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "25/01/12 10:36 PM",
      "commitNameOld": "d06948002fb0cabf72cc0d46bf2fa67d45370f67",
      "commitAuthorOld": "Mahadev Konar",
      "daysBetweenCommits": 831.16,
      "commitsBetweenForRepo": 5265,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,68 @@\n   private List\u003cDynamicInputChunk\u003e splitCopyListingIntoChunksWithShuffle\n                                     (JobContext context) throws IOException {\n \n     final Configuration configuration \u003d context.getConfiguration();\n     int numRecords \u003d getNumberOfRecords(configuration);\n     int numMaps \u003d getNumMapTasks(configuration);\n+    int maxChunksTolerable \u003d getMaxChunksTolerable(configuration);\n+\n     // Number of chunks each map will process, on average.\n     int splitRatio \u003d getListingSplitRatio(configuration, numMaps, numRecords);\n-    validateNumChunksUsing(splitRatio, numMaps);\n+    validateNumChunksUsing(splitRatio, numMaps, maxChunksTolerable);\n \n     int numEntriesPerChunk \u003d (int)Math.ceil((float)numRecords\n                                           /(splitRatio * numMaps));\n     DistCpUtils.publish(context.getConfiguration(),\n                         CONF_LABEL_NUM_ENTRIES_PER_CHUNK,\n                         numEntriesPerChunk);\n \n     final int nChunksTotal \u003d (int)Math.ceil((float)numRecords/numEntriesPerChunk);\n     int nChunksOpenAtOnce\n             \u003d Math.min(N_CHUNKS_OPEN_AT_ONCE_DEFAULT, nChunksTotal);\n \n     Path listingPath \u003d getListingFilePath(configuration);\n     SequenceFile.Reader reader\n             \u003d new SequenceFile.Reader(configuration,\n                                       SequenceFile.Reader.file(listingPath));\n \n     List\u003cDynamicInputChunk\u003e openChunks\n                   \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n     \n     List\u003cDynamicInputChunk\u003e chunksFinal \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n \n     FileStatus fileStatus \u003d new FileStatus();\n     Text relPath \u003d new Text();\n     int recordCounter \u003d 0;\n     int chunkCount \u003d 0;\n \n     try {\n \n       while (reader.next(relPath, fileStatus)) {\n         if (recordCounter % (nChunksOpenAtOnce*numEntriesPerChunk) \u003d\u003d 0) {\n           // All chunks full. Create new chunk-set.\n           closeAll(openChunks);\n           chunksFinal.addAll(openChunks);\n \n           openChunks \u003d createChunks(\n                   configuration, chunkCount, nChunksTotal, nChunksOpenAtOnce);\n \n           chunkCount +\u003d openChunks.size();\n \n           nChunksOpenAtOnce \u003d openChunks.size();\n           recordCounter \u003d 0;\n         }\n \n         // Shuffle into open chunks.\n         openChunks.get(recordCounter%nChunksOpenAtOnce).write(relPath, fileStatus);\n         ++recordCounter;\n       }\n \n     } finally {\n       closeAll(openChunks);\n       chunksFinal.addAll(openChunks);\n       IOUtils.closeStream(reader);\n     }\n \n     LOG.info(\"Number of dynamic-chunk-files created: \" + chunksFinal.size()); \n     return chunksFinal;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private List\u003cDynamicInputChunk\u003e splitCopyListingIntoChunksWithShuffle\n                                    (JobContext context) throws IOException {\n\n    final Configuration configuration \u003d context.getConfiguration();\n    int numRecords \u003d getNumberOfRecords(configuration);\n    int numMaps \u003d getNumMapTasks(configuration);\n    int maxChunksTolerable \u003d getMaxChunksTolerable(configuration);\n\n    // Number of chunks each map will process, on average.\n    int splitRatio \u003d getListingSplitRatio(configuration, numMaps, numRecords);\n    validateNumChunksUsing(splitRatio, numMaps, maxChunksTolerable);\n\n    int numEntriesPerChunk \u003d (int)Math.ceil((float)numRecords\n                                          /(splitRatio * numMaps));\n    DistCpUtils.publish(context.getConfiguration(),\n                        CONF_LABEL_NUM_ENTRIES_PER_CHUNK,\n                        numEntriesPerChunk);\n\n    final int nChunksTotal \u003d (int)Math.ceil((float)numRecords/numEntriesPerChunk);\n    int nChunksOpenAtOnce\n            \u003d Math.min(N_CHUNKS_OPEN_AT_ONCE_DEFAULT, nChunksTotal);\n\n    Path listingPath \u003d getListingFilePath(configuration);\n    SequenceFile.Reader reader\n            \u003d new SequenceFile.Reader(configuration,\n                                      SequenceFile.Reader.file(listingPath));\n\n    List\u003cDynamicInputChunk\u003e openChunks\n                  \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n    \n    List\u003cDynamicInputChunk\u003e chunksFinal \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n\n    FileStatus fileStatus \u003d new FileStatus();\n    Text relPath \u003d new Text();\n    int recordCounter \u003d 0;\n    int chunkCount \u003d 0;\n\n    try {\n\n      while (reader.next(relPath, fileStatus)) {\n        if (recordCounter % (nChunksOpenAtOnce*numEntriesPerChunk) \u003d\u003d 0) {\n          // All chunks full. Create new chunk-set.\n          closeAll(openChunks);\n          chunksFinal.addAll(openChunks);\n\n          openChunks \u003d createChunks(\n                  configuration, chunkCount, nChunksTotal, nChunksOpenAtOnce);\n\n          chunkCount +\u003d openChunks.size();\n\n          nChunksOpenAtOnce \u003d openChunks.size();\n          recordCounter \u003d 0;\n        }\n\n        // Shuffle into open chunks.\n        openChunks.get(recordCounter%nChunksOpenAtOnce).write(relPath, fileStatus);\n        ++recordCounter;\n      }\n\n    } finally {\n      closeAll(openChunks);\n      chunksFinal.addAll(openChunks);\n      IOUtils.closeStream(reader);\n    }\n\n    LOG.info(\"Number of dynamic-chunk-files created: \" + chunksFinal.size()); \n    return chunksFinal;\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java",
      "extendedDetails": {}
    },
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1236045 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/12 10:36 PM",
      "commitName": "d06948002fb0cabf72cc0d46bf2fa67d45370f67",
      "commitAuthor": "Mahadev Konar",
      "diff": "@@ -0,0 +1,66 @@\n+  private List\u003cDynamicInputChunk\u003e splitCopyListingIntoChunksWithShuffle\n+                                    (JobContext context) throws IOException {\n+\n+    final Configuration configuration \u003d context.getConfiguration();\n+    int numRecords \u003d getNumberOfRecords(configuration);\n+    int numMaps \u003d getNumMapTasks(configuration);\n+    // Number of chunks each map will process, on average.\n+    int splitRatio \u003d getListingSplitRatio(configuration, numMaps, numRecords);\n+    validateNumChunksUsing(splitRatio, numMaps);\n+\n+    int numEntriesPerChunk \u003d (int)Math.ceil((float)numRecords\n+                                          /(splitRatio * numMaps));\n+    DistCpUtils.publish(context.getConfiguration(),\n+                        CONF_LABEL_NUM_ENTRIES_PER_CHUNK,\n+                        numEntriesPerChunk);\n+\n+    final int nChunksTotal \u003d (int)Math.ceil((float)numRecords/numEntriesPerChunk);\n+    int nChunksOpenAtOnce\n+            \u003d Math.min(N_CHUNKS_OPEN_AT_ONCE_DEFAULT, nChunksTotal);\n+\n+    Path listingPath \u003d getListingFilePath(configuration);\n+    SequenceFile.Reader reader\n+            \u003d new SequenceFile.Reader(configuration,\n+                                      SequenceFile.Reader.file(listingPath));\n+\n+    List\u003cDynamicInputChunk\u003e openChunks\n+                  \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n+    \n+    List\u003cDynamicInputChunk\u003e chunksFinal \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n+\n+    FileStatus fileStatus \u003d new FileStatus();\n+    Text relPath \u003d new Text();\n+    int recordCounter \u003d 0;\n+    int chunkCount \u003d 0;\n+\n+    try {\n+\n+      while (reader.next(relPath, fileStatus)) {\n+        if (recordCounter % (nChunksOpenAtOnce*numEntriesPerChunk) \u003d\u003d 0) {\n+          // All chunks full. Create new chunk-set.\n+          closeAll(openChunks);\n+          chunksFinal.addAll(openChunks);\n+\n+          openChunks \u003d createChunks(\n+                  configuration, chunkCount, nChunksTotal, nChunksOpenAtOnce);\n+\n+          chunkCount +\u003d openChunks.size();\n+\n+          nChunksOpenAtOnce \u003d openChunks.size();\n+          recordCounter \u003d 0;\n+        }\n+\n+        // Shuffle into open chunks.\n+        openChunks.get(recordCounter%nChunksOpenAtOnce).write(relPath, fileStatus);\n+        ++recordCounter;\n+      }\n+\n+    } finally {\n+      closeAll(openChunks);\n+      chunksFinal.addAll(openChunks);\n+      IOUtils.closeStream(reader);\n+    }\n+\n+    LOG.info(\"Number of dynamic-chunk-files created: \" + chunksFinal.size()); \n+    return chunksFinal;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private List\u003cDynamicInputChunk\u003e splitCopyListingIntoChunksWithShuffle\n                                    (JobContext context) throws IOException {\n\n    final Configuration configuration \u003d context.getConfiguration();\n    int numRecords \u003d getNumberOfRecords(configuration);\n    int numMaps \u003d getNumMapTasks(configuration);\n    // Number of chunks each map will process, on average.\n    int splitRatio \u003d getListingSplitRatio(configuration, numMaps, numRecords);\n    validateNumChunksUsing(splitRatio, numMaps);\n\n    int numEntriesPerChunk \u003d (int)Math.ceil((float)numRecords\n                                          /(splitRatio * numMaps));\n    DistCpUtils.publish(context.getConfiguration(),\n                        CONF_LABEL_NUM_ENTRIES_PER_CHUNK,\n                        numEntriesPerChunk);\n\n    final int nChunksTotal \u003d (int)Math.ceil((float)numRecords/numEntriesPerChunk);\n    int nChunksOpenAtOnce\n            \u003d Math.min(N_CHUNKS_OPEN_AT_ONCE_DEFAULT, nChunksTotal);\n\n    Path listingPath \u003d getListingFilePath(configuration);\n    SequenceFile.Reader reader\n            \u003d new SequenceFile.Reader(configuration,\n                                      SequenceFile.Reader.file(listingPath));\n\n    List\u003cDynamicInputChunk\u003e openChunks\n                  \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n    \n    List\u003cDynamicInputChunk\u003e chunksFinal \u003d new ArrayList\u003cDynamicInputChunk\u003e();\n\n    FileStatus fileStatus \u003d new FileStatus();\n    Text relPath \u003d new Text();\n    int recordCounter \u003d 0;\n    int chunkCount \u003d 0;\n\n    try {\n\n      while (reader.next(relPath, fileStatus)) {\n        if (recordCounter % (nChunksOpenAtOnce*numEntriesPerChunk) \u003d\u003d 0) {\n          // All chunks full. Create new chunk-set.\n          closeAll(openChunks);\n          chunksFinal.addAll(openChunks);\n\n          openChunks \u003d createChunks(\n                  configuration, chunkCount, nChunksTotal, nChunksOpenAtOnce);\n\n          chunkCount +\u003d openChunks.size();\n\n          nChunksOpenAtOnce \u003d openChunks.size();\n          recordCounter \u003d 0;\n        }\n\n        // Shuffle into open chunks.\n        openChunks.get(recordCounter%nChunksOpenAtOnce).write(relPath, fileStatus);\n        ++recordCounter;\n      }\n\n    } finally {\n      closeAll(openChunks);\n      chunksFinal.addAll(openChunks);\n      IOUtils.closeStream(reader);\n    }\n\n    LOG.info(\"Number of dynamic-chunk-files created: \" + chunksFinal.size()); \n    return chunksFinal;\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java"
    }
  }
}