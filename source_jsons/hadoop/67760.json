{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DynamicInputFormat.java",
  "functionName": "createSplits",
  "functionId": "createSplits___jobContext-JobContext__chunks-List__DynamicInputChunk__",
  "sourceFilePath": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java",
  "functionStartLine": 81,
  "functionEndLine": 102,
  "numCommitsSeen": 6,
  "timeTaken": 693,
  "changeHistory": [
    "03db13206f131d93347651513496e1b3fcff3dba",
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67"
  ],
  "changeHistoryShort": {
    "03db13206f131d93347651513496e1b3fcff3dba": "Ybodychange",
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67": "Yintroduced"
  },
  "changeHistoryDetails": {
    "03db13206f131d93347651513496e1b3fcff3dba": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5402. In DynamicInputFormat, change MAX_CHUNKS_TOLERABLE, MAX_CHUNKS_IDEAL, MIN_RECORDS_PER_CHUNK and SPLIT_RATIO to be configurable.  Contributed by Tsuyoshi OZAWA\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1592703 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/05/14 3:24 AM",
      "commitName": "03db13206f131d93347651513496e1b3fcff3dba",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "25/01/12 10:36 PM",
      "commitNameOld": "d06948002fb0cabf72cc0d46bf2fa67d45370f67",
      "commitAuthorOld": "Mahadev Konar",
      "daysBetweenCommits": 831.16,
      "commitsBetweenForRepo": 5265,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   private List\u003cInputSplit\u003e createSplits(JobContext jobContext,\n                                         List\u003cDynamicInputChunk\u003e chunks)\n           throws IOException {\n     int numMaps \u003d getNumMapTasks(jobContext.getConfiguration());\n \n     final int nSplits \u003d Math.min(numMaps, chunks.size());\n     List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e(nSplits);\n     \n     for (int i\u003d0; i\u003c nSplits; ++i) {\n       TaskID taskId \u003d new TaskID(jobContext.getJobID(), TaskType.MAP, i);\n       chunks.get(i).assignTo(taskId);\n       splits.add(new FileSplit(chunks.get(i).getPath(), 0,\n           // Setting non-zero length for FileSplit size, to avoid a possible\n           // future when 0-sized file-splits are considered \"empty\" and skipped\n           // over.\n-          MIN_RECORDS_PER_CHUNK,\n+          getMinRecordsPerChunk(jobContext.getConfiguration()),\n           null));\n     }\n     DistCpUtils.publish(jobContext.getConfiguration(),\n                         CONF_LABEL_NUM_SPLITS, splits.size());\n     return splits;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private List\u003cInputSplit\u003e createSplits(JobContext jobContext,\n                                        List\u003cDynamicInputChunk\u003e chunks)\n          throws IOException {\n    int numMaps \u003d getNumMapTasks(jobContext.getConfiguration());\n\n    final int nSplits \u003d Math.min(numMaps, chunks.size());\n    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e(nSplits);\n    \n    for (int i\u003d0; i\u003c nSplits; ++i) {\n      TaskID taskId \u003d new TaskID(jobContext.getJobID(), TaskType.MAP, i);\n      chunks.get(i).assignTo(taskId);\n      splits.add(new FileSplit(chunks.get(i).getPath(), 0,\n          // Setting non-zero length for FileSplit size, to avoid a possible\n          // future when 0-sized file-splits are considered \"empty\" and skipped\n          // over.\n          getMinRecordsPerChunk(jobContext.getConfiguration()),\n          null));\n    }\n    DistCpUtils.publish(jobContext.getConfiguration(),\n                        CONF_LABEL_NUM_SPLITS, splits.size());\n    return splits;\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java",
      "extendedDetails": {}
    },
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1236045 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/12 10:36 PM",
      "commitName": "d06948002fb0cabf72cc0d46bf2fa67d45370f67",
      "commitAuthor": "Mahadev Konar",
      "diff": "@@ -0,0 +1,22 @@\n+  private List\u003cInputSplit\u003e createSplits(JobContext jobContext,\n+                                        List\u003cDynamicInputChunk\u003e chunks)\n+          throws IOException {\n+    int numMaps \u003d getNumMapTasks(jobContext.getConfiguration());\n+\n+    final int nSplits \u003d Math.min(numMaps, chunks.size());\n+    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e(nSplits);\n+    \n+    for (int i\u003d0; i\u003c nSplits; ++i) {\n+      TaskID taskId \u003d new TaskID(jobContext.getJobID(), TaskType.MAP, i);\n+      chunks.get(i).assignTo(taskId);\n+      splits.add(new FileSplit(chunks.get(i).getPath(), 0,\n+          // Setting non-zero length for FileSplit size, to avoid a possible\n+          // future when 0-sized file-splits are considered \"empty\" and skipped\n+          // over.\n+          MIN_RECORDS_PER_CHUNK,\n+          null));\n+    }\n+    DistCpUtils.publish(jobContext.getConfiguration(),\n+                        CONF_LABEL_NUM_SPLITS, splits.size());\n+    return splits;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private List\u003cInputSplit\u003e createSplits(JobContext jobContext,\n                                        List\u003cDynamicInputChunk\u003e chunks)\n          throws IOException {\n    int numMaps \u003d getNumMapTasks(jobContext.getConfiguration());\n\n    final int nSplits \u003d Math.min(numMaps, chunks.size());\n    List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e(nSplits);\n    \n    for (int i\u003d0; i\u003c nSplits; ++i) {\n      TaskID taskId \u003d new TaskID(jobContext.getJobID(), TaskType.MAP, i);\n      chunks.get(i).assignTo(taskId);\n      splits.add(new FileSplit(chunks.get(i).getPath(), 0,\n          // Setting non-zero length for FileSplit size, to avoid a possible\n          // future when 0-sized file-splits are considered \"empty\" and skipped\n          // over.\n          MIN_RECORDS_PER_CHUNK,\n          null));\n    }\n    DistCpUtils.publish(jobContext.getConfiguration(),\n                        CONF_LABEL_NUM_SPLITS, splits.size());\n    return splits;\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java"
    }
  }
}