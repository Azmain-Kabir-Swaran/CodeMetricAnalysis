{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ImageLoaderCurrent.java",
  "functionName": "loadImage",
  "functionId": "loadImage___in-DataInputStream__v-ImageVisitor__skipBlocks-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageLoaderCurrent.java",
  "functionStartLine": 151,
  "functionEndLine": 235,
  "numCommitsSeen": 35,
  "timeTaken": 1989,
  "changeHistory": [
    "97f58955a6045b373ab73653bf26ab5922b00cf3"
  ],
  "changeHistoryShort": {
    "97f58955a6045b373ab73653bf26ab5922b00cf3": "Yintroduced"
  },
  "changeHistoryDetails": {
    "97f58955a6045b373ab73653bf26ab5922b00cf3": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-6293. Issues with OIV processing PB-based fsimages. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594439 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/05/14 6:15 PM",
      "commitName": "97f58955a6045b373ab73653bf26ab5922b00cf3",
      "commitAuthor": "Kihwal Lee",
      "diff": "@@ -0,0 +1,85 @@\n+  public void loadImage(DataInputStream in, ImageVisitor v,\n+      boolean skipBlocks) throws IOException {\n+    boolean done \u003d false;\n+    try {\n+      v.start();\n+      v.visitEnclosingElement(ImageElement.FS_IMAGE);\n+\n+      imageVersion \u003d in.readInt();\n+      if( !canLoadVersion(imageVersion))\n+        throw new IOException(\"Cannot process fslayout version \" + imageVersion);\n+      if (NameNodeLayoutVersion.supports(Feature.ADD_LAYOUT_FLAGS, imageVersion)) {\n+        LayoutFlags.read(in);\n+      }\n+\n+      v.visit(ImageElement.IMAGE_VERSION, imageVersion);\n+      v.visit(ImageElement.NAMESPACE_ID, in.readInt());\n+\n+      long numInodes \u003d in.readLong();\n+\n+      v.visit(ImageElement.GENERATION_STAMP, in.readLong());\n+\n+      if (NameNodeLayoutVersion.supports(Feature.SEQUENTIAL_BLOCK_ID, imageVersion)) {\n+        v.visit(ImageElement.GENERATION_STAMP_V2, in.readLong());\n+        v.visit(ImageElement.GENERATION_STAMP_V1_LIMIT, in.readLong());\n+        v.visit(ImageElement.LAST_ALLOCATED_BLOCK_ID, in.readLong());\n+      }\n+\n+      if (NameNodeLayoutVersion.supports(Feature.STORED_TXIDS, imageVersion)) {\n+        v.visit(ImageElement.TRANSACTION_ID, in.readLong());\n+      }\n+      \n+      if (NameNodeLayoutVersion.supports(Feature.ADD_INODE_ID, imageVersion)) {\n+        v.visit(ImageElement.LAST_INODE_ID, in.readLong());\n+      }\n+      \n+      boolean supportSnapshot \u003d NameNodeLayoutVersion.supports(Feature.SNAPSHOT,\n+          imageVersion);\n+      if (supportSnapshot) {\n+        v.visit(ImageElement.SNAPSHOT_COUNTER, in.readInt());\n+        int numSnapshots \u003d in.readInt();\n+        v.visit(ImageElement.NUM_SNAPSHOTS_TOTAL, numSnapshots);\n+        for (int i \u003d 0; i \u003c numSnapshots; i++) {\n+          processSnapshot(in, v);\n+        }\n+      }\n+      \n+      if (NameNodeLayoutVersion.supports(Feature.FSIMAGE_COMPRESSION, imageVersion)) {\n+        boolean isCompressed \u003d in.readBoolean();\n+        v.visit(ImageElement.IS_COMPRESSED, String.valueOf(isCompressed));\n+        if (isCompressed) {\n+          String codecClassName \u003d Text.readString(in);\n+          v.visit(ImageElement.COMPRESS_CODEC, codecClassName);\n+          CompressionCodecFactory codecFac \u003d new CompressionCodecFactory(\n+              new Configuration());\n+          CompressionCodec codec \u003d codecFac.getCodecByClassName(codecClassName);\n+          if (codec \u003d\u003d null) {\n+            throw new IOException(\"Image compression codec not supported: \"\n+                + codecClassName);\n+          }\n+          in \u003d new DataInputStream(codec.createInputStream(in));\n+        }\n+      }\n+      processINodes(in, v, numInodes, skipBlocks, supportSnapshot);\n+      subtreeMap.clear();\n+      dirNodeMap.clear();\n+\n+      processINodesUC(in, v, skipBlocks);\n+\n+      if (NameNodeLayoutVersion.supports(Feature.DELEGATION_TOKEN, imageVersion)) {\n+        processDelegationTokens(in, v);\n+      }\n+      \n+      if (NameNodeLayoutVersion.supports(Feature.CACHING, imageVersion)) {\n+        processCacheManagerState(in, v);\n+      }\n+      v.leaveEnclosingElement(); // FSImage\n+      done \u003d true;\n+    } finally {\n+      if (done) {\n+        v.finish();\n+      } else {\n+        v.finishAbnormally();\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void loadImage(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks) throws IOException {\n    boolean done \u003d false;\n    try {\n      v.start();\n      v.visitEnclosingElement(ImageElement.FS_IMAGE);\n\n      imageVersion \u003d in.readInt();\n      if( !canLoadVersion(imageVersion))\n        throw new IOException(\"Cannot process fslayout version \" + imageVersion);\n      if (NameNodeLayoutVersion.supports(Feature.ADD_LAYOUT_FLAGS, imageVersion)) {\n        LayoutFlags.read(in);\n      }\n\n      v.visit(ImageElement.IMAGE_VERSION, imageVersion);\n      v.visit(ImageElement.NAMESPACE_ID, in.readInt());\n\n      long numInodes \u003d in.readLong();\n\n      v.visit(ImageElement.GENERATION_STAMP, in.readLong());\n\n      if (NameNodeLayoutVersion.supports(Feature.SEQUENTIAL_BLOCK_ID, imageVersion)) {\n        v.visit(ImageElement.GENERATION_STAMP_V2, in.readLong());\n        v.visit(ImageElement.GENERATION_STAMP_V1_LIMIT, in.readLong());\n        v.visit(ImageElement.LAST_ALLOCATED_BLOCK_ID, in.readLong());\n      }\n\n      if (NameNodeLayoutVersion.supports(Feature.STORED_TXIDS, imageVersion)) {\n        v.visit(ImageElement.TRANSACTION_ID, in.readLong());\n      }\n      \n      if (NameNodeLayoutVersion.supports(Feature.ADD_INODE_ID, imageVersion)) {\n        v.visit(ImageElement.LAST_INODE_ID, in.readLong());\n      }\n      \n      boolean supportSnapshot \u003d NameNodeLayoutVersion.supports(Feature.SNAPSHOT,\n          imageVersion);\n      if (supportSnapshot) {\n        v.visit(ImageElement.SNAPSHOT_COUNTER, in.readInt());\n        int numSnapshots \u003d in.readInt();\n        v.visit(ImageElement.NUM_SNAPSHOTS_TOTAL, numSnapshots);\n        for (int i \u003d 0; i \u003c numSnapshots; i++) {\n          processSnapshot(in, v);\n        }\n      }\n      \n      if (NameNodeLayoutVersion.supports(Feature.FSIMAGE_COMPRESSION, imageVersion)) {\n        boolean isCompressed \u003d in.readBoolean();\n        v.visit(ImageElement.IS_COMPRESSED, String.valueOf(isCompressed));\n        if (isCompressed) {\n          String codecClassName \u003d Text.readString(in);\n          v.visit(ImageElement.COMPRESS_CODEC, codecClassName);\n          CompressionCodecFactory codecFac \u003d new CompressionCodecFactory(\n              new Configuration());\n          CompressionCodec codec \u003d codecFac.getCodecByClassName(codecClassName);\n          if (codec \u003d\u003d null) {\n            throw new IOException(\"Image compression codec not supported: \"\n                + codecClassName);\n          }\n          in \u003d new DataInputStream(codec.createInputStream(in));\n        }\n      }\n      processINodes(in, v, numInodes, skipBlocks, supportSnapshot);\n      subtreeMap.clear();\n      dirNodeMap.clear();\n\n      processINodesUC(in, v, skipBlocks);\n\n      if (NameNodeLayoutVersion.supports(Feature.DELEGATION_TOKEN, imageVersion)) {\n        processDelegationTokens(in, v);\n      }\n      \n      if (NameNodeLayoutVersion.supports(Feature.CACHING, imageVersion)) {\n        processCacheManagerState(in, v);\n      }\n      v.leaveEnclosingElement(); // FSImage\n      done \u003d true;\n    } finally {\n      if (done) {\n        v.finish();\n      } else {\n        v.finishAbnormally();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageLoaderCurrent.java"
    }
  }
}