{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FederationInterceptor.java",
  "functionName": "cacheAllocatedContainers",
  "functionId": "cacheAllocatedContainers___containers-List__Container____subClusterId-SubClusterId",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/FederationInterceptor.java",
  "functionStartLine": 1518,
  "functionEndLine": 1560,
  "numCommitsSeen": 21,
  "timeTaken": 2581,
  "changeHistory": [
    "3090922805699b8374a359e92323884a4177dc4e",
    "f1525825623a1307b5aa55c456b6afa3e0c61135",
    "670e8d4ec7e71fc3b054cd3b2826f869b649a788",
    "70b1a757f13b01a9192ea5fb0820ba7babfd974e",
    "bed1832c934fe4ba44efdcdc49fce06457dc3d4f"
  ],
  "changeHistoryShort": {
    "3090922805699b8374a359e92323884a4177dc4e": "Ybodychange",
    "f1525825623a1307b5aa55c456b6afa3e0c61135": "Ybodychange",
    "670e8d4ec7e71fc3b054cd3b2826f869b649a788": "Ybodychange",
    "70b1a757f13b01a9192ea5fb0820ba7babfd974e": "Ybodychange",
    "bed1832c934fe4ba44efdcdc49fce06457dc3d4f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3090922805699b8374a359e92323884a4177dc4e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8696. [AMRMProxy] FederationInterceptor upgrade: home sub-cluster heartbeat async. Contributed by Botong Huang.\n",
      "commitDate": "24/09/18 11:37 AM",
      "commitName": "3090922805699b8374a359e92323884a4177dc4e",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "12/09/18 11:46 AM",
      "commitNameOld": "02b9bfdf9e4bd0b3c05ca5fd75399dedcb656e09",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 11.99,
      "commitsBetweenForRepo": 142,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,43 @@\n   private void cacheAllocatedContainers(List\u003cContainer\u003e containers,\n       SubClusterId subClusterId) {\n     for (Container container : containers) {\n       LOG.debug(\"Adding container {}\", container);\n-      if (containerIdToSubClusterIdMap.containsKey(container.getId())) {\n+\n+      if (this.containerIdToSubClusterIdMap.containsKey(container.getId())) {\n         SubClusterId existingSubClusterId \u003d\n-            containerIdToSubClusterIdMap.get(container.getId());\n+            this.containerIdToSubClusterIdMap.get(container.getId());\n         if (existingSubClusterId.equals(subClusterId)) {\n           /*\n            * When RM fails over, the new RM master might send out the same\n            * container allocation more than once.\n            *\n            * It is also possible because of a recent NM restart with NM recovery\n            * enabled. We recover running containers from RM. But RM might not\n            * notified AM of some of these containers yet. When RM dose notify,\n            * we will already have these containers in the map.\n            *\n            * Either case, just warn and move on.\n            */\n           LOG.warn(\n               \"Duplicate containerID: {} found in the allocated containers\"\n                   + \" from same sub-cluster: {}, so ignoring.\",\n               container.getId(), subClusterId);\n         } else {\n           // The same container allocation from different sub-clusters,\n           // something is wrong.\n           // TODO: YARN-6667 if some subcluster RM is configured wrong, we\n           // should not fail the entire heartbeat.\n           throw new YarnRuntimeException(\n               \"Duplicate containerID found in the allocated containers. This\"\n                   + \" can happen if the RM epoch is not configured properly.\"\n                   + \" ContainerId: \" + container.getId().toString()\n                   + \" ApplicationId: \" + this.attemptId + \" From RM: \"\n                   + subClusterId\n                   + \" . Previous container was from sub-cluster: \"\n                   + existingSubClusterId);\n         }\n       }\n \n-      containerIdToSubClusterIdMap.put(container.getId(), subClusterId);\n+      this.containerIdToSubClusterIdMap.put(container.getId(), subClusterId);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void cacheAllocatedContainers(List\u003cContainer\u003e containers,\n      SubClusterId subClusterId) {\n    for (Container container : containers) {\n      LOG.debug(\"Adding container {}\", container);\n\n      if (this.containerIdToSubClusterIdMap.containsKey(container.getId())) {\n        SubClusterId existingSubClusterId \u003d\n            this.containerIdToSubClusterIdMap.get(container.getId());\n        if (existingSubClusterId.equals(subClusterId)) {\n          /*\n           * When RM fails over, the new RM master might send out the same\n           * container allocation more than once.\n           *\n           * It is also possible because of a recent NM restart with NM recovery\n           * enabled. We recover running containers from RM. But RM might not\n           * notified AM of some of these containers yet. When RM dose notify,\n           * we will already have these containers in the map.\n           *\n           * Either case, just warn and move on.\n           */\n          LOG.warn(\n              \"Duplicate containerID: {} found in the allocated containers\"\n                  + \" from same sub-cluster: {}, so ignoring.\",\n              container.getId(), subClusterId);\n        } else {\n          // The same container allocation from different sub-clusters,\n          // something is wrong.\n          // TODO: YARN-6667 if some subcluster RM is configured wrong, we\n          // should not fail the entire heartbeat.\n          throw new YarnRuntimeException(\n              \"Duplicate containerID found in the allocated containers. This\"\n                  + \" can happen if the RM epoch is not configured properly.\"\n                  + \" ContainerId: \" + container.getId().toString()\n                  + \" ApplicationId: \" + this.attemptId + \" From RM: \"\n                  + subClusterId\n                  + \" . Previous container was from sub-cluster: \"\n                  + existingSubClusterId);\n        }\n      }\n\n      this.containerIdToSubClusterIdMap.put(container.getId(), subClusterId);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/FederationInterceptor.java",
      "extendedDetails": {}
    },
    "f1525825623a1307b5aa55c456b6afa3e0c61135": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8705. Refactor the UAM heartbeat thread in preparation for YARN-8696. Contributed by Botong Huang.\n",
      "commitDate": "27/08/18 10:32 AM",
      "commitName": "f1525825623a1307b5aa55c456b6afa3e0c61135",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "20/08/18 12:22 PM",
      "commitNameOld": "8736fc39ac3b3de168d2c216f3d1c0edb48fb3f9",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 6.92,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,42 @@\n   private void cacheAllocatedContainers(List\u003cContainer\u003e containers,\n       SubClusterId subClusterId) {\n     for (Container container : containers) {\n       LOG.debug(\"Adding container {}\", container);\n       if (containerIdToSubClusterIdMap.containsKey(container.getId())) {\n         SubClusterId existingSubClusterId \u003d\n             containerIdToSubClusterIdMap.get(container.getId());\n         if (existingSubClusterId.equals(subClusterId)) {\n           /*\n            * When RM fails over, the new RM master might send out the same\n            * container allocation more than once.\n            *\n            * It is also possible because of a recent NM restart with NM recovery\n            * enabled. We recover running containers from RM. But RM might not\n            * notified AM of some of these containers yet. When RM dose notify,\n            * we will already have these containers in the map.\n            *\n            * Either case, just warn and move on.\n            */\n           LOG.warn(\n               \"Duplicate containerID: {} found in the allocated containers\"\n                   + \" from same sub-cluster: {}, so ignoring.\",\n               container.getId(), subClusterId);\n         } else {\n           // The same container allocation from different sub-clusters,\n           // something is wrong.\n           // TODO: YARN-6667 if some subcluster RM is configured wrong, we\n           // should not fail the entire heartbeat.\n           throw new YarnRuntimeException(\n               \"Duplicate containerID found in the allocated containers. This\"\n                   + \" can happen if the RM epoch is not configured properly.\"\n                   + \" ContainerId: \" + container.getId().toString()\n-                  + \" ApplicationId: \"\n-                  + getApplicationContext().getApplicationAttemptId()\n-                  + \" From RM: \" + subClusterId\n+                  + \" ApplicationId: \" + this.attemptId + \" From RM: \"\n+                  + subClusterId\n                   + \" . Previous container was from sub-cluster: \"\n                   + existingSubClusterId);\n         }\n       }\n \n       containerIdToSubClusterIdMap.put(container.getId(), subClusterId);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void cacheAllocatedContainers(List\u003cContainer\u003e containers,\n      SubClusterId subClusterId) {\n    for (Container container : containers) {\n      LOG.debug(\"Adding container {}\", container);\n      if (containerIdToSubClusterIdMap.containsKey(container.getId())) {\n        SubClusterId existingSubClusterId \u003d\n            containerIdToSubClusterIdMap.get(container.getId());\n        if (existingSubClusterId.equals(subClusterId)) {\n          /*\n           * When RM fails over, the new RM master might send out the same\n           * container allocation more than once.\n           *\n           * It is also possible because of a recent NM restart with NM recovery\n           * enabled. We recover running containers from RM. But RM might not\n           * notified AM of some of these containers yet. When RM dose notify,\n           * we will already have these containers in the map.\n           *\n           * Either case, just warn and move on.\n           */\n          LOG.warn(\n              \"Duplicate containerID: {} found in the allocated containers\"\n                  + \" from same sub-cluster: {}, so ignoring.\",\n              container.getId(), subClusterId);\n        } else {\n          // The same container allocation from different sub-clusters,\n          // something is wrong.\n          // TODO: YARN-6667 if some subcluster RM is configured wrong, we\n          // should not fail the entire heartbeat.\n          throw new YarnRuntimeException(\n              \"Duplicate containerID found in the allocated containers. This\"\n                  + \" can happen if the RM epoch is not configured properly.\"\n                  + \" ContainerId: \" + container.getId().toString()\n                  + \" ApplicationId: \" + this.attemptId + \" From RM: \"\n                  + subClusterId\n                  + \" . Previous container was from sub-cluster: \"\n                  + existingSubClusterId);\n        }\n      }\n\n      containerIdToSubClusterIdMap.put(container.getId(), subClusterId);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/FederationInterceptor.java",
      "extendedDetails": {}
    },
    "670e8d4ec7e71fc3b054cd3b2826f869b649a788": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6704. Add support for work preserving NM restart when FederationInterceptor is enabled in AMRMProxyService. (Botong Huang via Subru).\n",
      "commitDate": "08/12/17 3:39 PM",
      "commitName": "670e8d4ec7e71fc3b054cd3b2826f869b649a788",
      "commitAuthor": "Subru Krishnan",
      "commitDateOld": "17/11/17 5:39 PM",
      "commitNameOld": "d5f66888b8d767ee6706fab9950c194a1bf26d32",
      "commitAuthorOld": "Subru Krishnan",
      "daysBetweenCommits": 20.92,
      "commitsBetweenForRepo": 111,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,43 @@\n   private void cacheAllocatedContainers(List\u003cContainer\u003e containers,\n       SubClusterId subClusterId) {\n     for (Container container : containers) {\n+      LOG.debug(\"Adding container {}\", container);\n       if (containerIdToSubClusterIdMap.containsKey(container.getId())) {\n         SubClusterId existingSubClusterId \u003d\n             containerIdToSubClusterIdMap.get(container.getId());\n         if (existingSubClusterId.equals(subClusterId)) {\n-          // When RM fails over, the new RM master might send out the same\n-          // container allocation more than once. Just move on in this case.\n+          /*\n+           * When RM fails over, the new RM master might send out the same\n+           * container allocation more than once.\n+           *\n+           * It is also possible because of a recent NM restart with NM recovery\n+           * enabled. We recover running containers from RM. But RM might not\n+           * notified AM of some of these containers yet. When RM dose notify,\n+           * we will already have these containers in the map.\n+           *\n+           * Either case, just warn and move on.\n+           */\n           LOG.warn(\n               \"Duplicate containerID: {} found in the allocated containers\"\n                   + \" from same sub-cluster: {}, so ignoring.\",\n               container.getId(), subClusterId);\n         } else {\n           // The same container allocation from different sub-clusters,\n           // something is wrong.\n           // TODO: YARN-6667 if some subcluster RM is configured wrong, we\n           // should not fail the entire heartbeat.\n           throw new YarnRuntimeException(\n               \"Duplicate containerID found in the allocated containers. This\"\n                   + \" can happen if the RM epoch is not configured properly.\"\n                   + \" ContainerId: \" + container.getId().toString()\n                   + \" ApplicationId: \"\n                   + getApplicationContext().getApplicationAttemptId()\n                   + \" From RM: \" + subClusterId\n                   + \" . Previous container was from sub-cluster: \"\n                   + existingSubClusterId);\n         }\n       }\n \n       containerIdToSubClusterIdMap.put(container.getId(), subClusterId);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void cacheAllocatedContainers(List\u003cContainer\u003e containers,\n      SubClusterId subClusterId) {\n    for (Container container : containers) {\n      LOG.debug(\"Adding container {}\", container);\n      if (containerIdToSubClusterIdMap.containsKey(container.getId())) {\n        SubClusterId existingSubClusterId \u003d\n            containerIdToSubClusterIdMap.get(container.getId());\n        if (existingSubClusterId.equals(subClusterId)) {\n          /*\n           * When RM fails over, the new RM master might send out the same\n           * container allocation more than once.\n           *\n           * It is also possible because of a recent NM restart with NM recovery\n           * enabled. We recover running containers from RM. But RM might not\n           * notified AM of some of these containers yet. When RM dose notify,\n           * we will already have these containers in the map.\n           *\n           * Either case, just warn and move on.\n           */\n          LOG.warn(\n              \"Duplicate containerID: {} found in the allocated containers\"\n                  + \" from same sub-cluster: {}, so ignoring.\",\n              container.getId(), subClusterId);\n        } else {\n          // The same container allocation from different sub-clusters,\n          // something is wrong.\n          // TODO: YARN-6667 if some subcluster RM is configured wrong, we\n          // should not fail the entire heartbeat.\n          throw new YarnRuntimeException(\n              \"Duplicate containerID found in the allocated containers. This\"\n                  + \" can happen if the RM epoch is not configured properly.\"\n                  + \" ContainerId: \" + container.getId().toString()\n                  + \" ApplicationId: \"\n                  + getApplicationContext().getApplicationAttemptId()\n                  + \" From RM: \" + subClusterId\n                  + \" . Previous container was from sub-cluster: \"\n                  + existingSubClusterId);\n        }\n      }\n\n      containerIdToSubClusterIdMap.put(container.getId(), subClusterId);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/FederationInterceptor.java",
      "extendedDetails": {}
    },
    "70b1a757f13b01a9192ea5fb0820ba7babfd974e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6511. Federation: transparently spanning application across multiple sub-clusters. (Botong Huang via Subru).\n\n(cherry picked from commit 8c988d235eaf0972783985b1ab24680d029aea79)\n",
      "commitDate": "01/08/17 5:28 PM",
      "commitName": "70b1a757f13b01a9192ea5fb0820ba7babfd974e",
      "commitAuthor": "Subru Krishnan",
      "commitDateOld": "01/08/17 5:28 PM",
      "commitNameOld": "bed1832c934fe4ba44efdcdc49fce06457dc3d4f",
      "commitAuthorOld": "Subru Krishnan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   private void cacheAllocatedContainers(List\u003cContainer\u003e containers,\n       SubClusterId subClusterId) {\n     for (Container container : containers) {\n       if (containerIdToSubClusterIdMap.containsKey(container.getId())) {\n         SubClusterId existingSubClusterId \u003d\n             containerIdToSubClusterIdMap.get(container.getId());\n         if (existingSubClusterId.equals(subClusterId)) {\n           // When RM fails over, the new RM master might send out the same\n           // container allocation more than once. Just move on in this case.\n           LOG.warn(\n               \"Duplicate containerID: {} found in the allocated containers\"\n-                  + \" from same subcluster: {}, so ignoring.\",\n+                  + \" from same sub-cluster: {}, so ignoring.\",\n               container.getId(), subClusterId);\n         } else {\n-          // The same container allocation from different subclusters,\n+          // The same container allocation from different sub-clusters,\n           // something is wrong.\n           // TODO: YARN-6667 if some subcluster RM is configured wrong, we\n           // should not fail the entire heartbeat.\n           throw new YarnRuntimeException(\n               \"Duplicate containerID found in the allocated containers. This\"\n                   + \" can happen if the RM epoch is not configured properly.\"\n                   + \" ContainerId: \" + container.getId().toString()\n                   + \" ApplicationId: \"\n                   + getApplicationContext().getApplicationAttemptId()\n                   + \" From RM: \" + subClusterId\n-                  + \" . Previous container was from subcluster: \"\n+                  + \" . Previous container was from sub-cluster: \"\n                   + existingSubClusterId);\n         }\n       }\n \n       containerIdToSubClusterIdMap.put(container.getId(), subClusterId);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void cacheAllocatedContainers(List\u003cContainer\u003e containers,\n      SubClusterId subClusterId) {\n    for (Container container : containers) {\n      if (containerIdToSubClusterIdMap.containsKey(container.getId())) {\n        SubClusterId existingSubClusterId \u003d\n            containerIdToSubClusterIdMap.get(container.getId());\n        if (existingSubClusterId.equals(subClusterId)) {\n          // When RM fails over, the new RM master might send out the same\n          // container allocation more than once. Just move on in this case.\n          LOG.warn(\n              \"Duplicate containerID: {} found in the allocated containers\"\n                  + \" from same sub-cluster: {}, so ignoring.\",\n              container.getId(), subClusterId);\n        } else {\n          // The same container allocation from different sub-clusters,\n          // something is wrong.\n          // TODO: YARN-6667 if some subcluster RM is configured wrong, we\n          // should not fail the entire heartbeat.\n          throw new YarnRuntimeException(\n              \"Duplicate containerID found in the allocated containers. This\"\n                  + \" can happen if the RM epoch is not configured properly.\"\n                  + \" ContainerId: \" + container.getId().toString()\n                  + \" ApplicationId: \"\n                  + getApplicationContext().getApplicationAttemptId()\n                  + \" From RM: \" + subClusterId\n                  + \" . Previous container was from sub-cluster: \"\n                  + existingSubClusterId);\n        }\n      }\n\n      containerIdToSubClusterIdMap.put(container.getId(), subClusterId);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/FederationInterceptor.java",
      "extendedDetails": {}
    },
    "bed1832c934fe4ba44efdcdc49fce06457dc3d4f": {
      "type": "Yintroduced",
      "commitMessage": "YARN-3666. Federation Intercepting and propagating AM- home RM communications. (Botong Huang via Subru).\n\n(cherry picked from commit 2399eb8200609246cb623c74450ca4a2032063cc)\n",
      "commitDate": "01/08/17 5:28 PM",
      "commitName": "bed1832c934fe4ba44efdcdc49fce06457dc3d4f",
      "commitAuthor": "Subru Krishnan",
      "diff": "@@ -0,0 +1,33 @@\n+  private void cacheAllocatedContainers(List\u003cContainer\u003e containers,\n+      SubClusterId subClusterId) {\n+    for (Container container : containers) {\n+      if (containerIdToSubClusterIdMap.containsKey(container.getId())) {\n+        SubClusterId existingSubClusterId \u003d\n+            containerIdToSubClusterIdMap.get(container.getId());\n+        if (existingSubClusterId.equals(subClusterId)) {\n+          // When RM fails over, the new RM master might send out the same\n+          // container allocation more than once. Just move on in this case.\n+          LOG.warn(\n+              \"Duplicate containerID: {} found in the allocated containers\"\n+                  + \" from same subcluster: {}, so ignoring.\",\n+              container.getId(), subClusterId);\n+        } else {\n+          // The same container allocation from different subclusters,\n+          // something is wrong.\n+          // TODO: YARN-6667 if some subcluster RM is configured wrong, we\n+          // should not fail the entire heartbeat.\n+          throw new YarnRuntimeException(\n+              \"Duplicate containerID found in the allocated containers. This\"\n+                  + \" can happen if the RM epoch is not configured properly.\"\n+                  + \" ContainerId: \" + container.getId().toString()\n+                  + \" ApplicationId: \"\n+                  + getApplicationContext().getApplicationAttemptId()\n+                  + \" From RM: \" + subClusterId\n+                  + \" . Previous container was from subcluster: \"\n+                  + existingSubClusterId);\n+        }\n+      }\n+\n+      containerIdToSubClusterIdMap.put(container.getId(), subClusterId);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void cacheAllocatedContainers(List\u003cContainer\u003e containers,\n      SubClusterId subClusterId) {\n    for (Container container : containers) {\n      if (containerIdToSubClusterIdMap.containsKey(container.getId())) {\n        SubClusterId existingSubClusterId \u003d\n            containerIdToSubClusterIdMap.get(container.getId());\n        if (existingSubClusterId.equals(subClusterId)) {\n          // When RM fails over, the new RM master might send out the same\n          // container allocation more than once. Just move on in this case.\n          LOG.warn(\n              \"Duplicate containerID: {} found in the allocated containers\"\n                  + \" from same subcluster: {}, so ignoring.\",\n              container.getId(), subClusterId);\n        } else {\n          // The same container allocation from different subclusters,\n          // something is wrong.\n          // TODO: YARN-6667 if some subcluster RM is configured wrong, we\n          // should not fail the entire heartbeat.\n          throw new YarnRuntimeException(\n              \"Duplicate containerID found in the allocated containers. This\"\n                  + \" can happen if the RM epoch is not configured properly.\"\n                  + \" ContainerId: \" + container.getId().toString()\n                  + \" ApplicationId: \"\n                  + getApplicationContext().getApplicationAttemptId()\n                  + \" From RM: \" + subClusterId\n                  + \" . Previous container was from subcluster: \"\n                  + existingSubClusterId);\n        }\n      }\n\n      containerIdToSubClusterIdMap.put(container.getId(), subClusterId);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/FederationInterceptor.java"
    }
  }
}