{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "setupPipelineForAppendOrRecovery",
  "functionId": "setupPipelineForAppendOrRecovery",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 1469,
  "functionEndLine": 1482,
  "numCommitsSeen": 156,
  "timeTaken": 10595,
  "changeHistory": [
    "a3954ccab148bddc290cb96528e63ff19799bcc9",
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "8f378733423a5244461df79a92c00239514b8b93",
    "7947e5b53b9ac9524b535b0384c1c355b74723ff",
    "7fc50e2525b8b8fe36d92e283a68eeeb09c63d21",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
    "727331becc3902cb4e60ee04741e79703238e782",
    "552b4fb9f9a76b18605322c0b0e8072613d67773",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54",
    "57b28693ee295746c6d168d37dd05eaf7b601b87",
    "38a04a3042c5af455605bd3477358893700e2a9d",
    "ceea91c9cd8b2a18be13217894ccf1c17198de18",
    "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf",
    "f2f5cdb5554d294a29ebf465101c5607fd56e244",
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
    "be7dd8333a7e56e732171db0781786987de03195",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "a3954ccab148bddc290cb96528e63ff19799bcc9": "Ybodychange",
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "8f378733423a5244461df79a92c00239514b8b93": "Ybodychange",
    "7947e5b53b9ac9524b535b0384c1c355b74723ff": "Ybodychange",
    "7fc50e2525b8b8fe36d92e283a68eeeb09c63d21": "Ybodychange",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": "Ymultichange(Ymovefromfile,Ybodychange)",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": "Ybodychange",
    "727331becc3902cb4e60ee04741e79703238e782": "Ybodychange",
    "552b4fb9f9a76b18605322c0b0e8072613d67773": "Ybodychange",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": "Ybodychange",
    "57b28693ee295746c6d168d37dd05eaf7b601b87": "Ybodychange",
    "38a04a3042c5af455605bd3477358893700e2a9d": "Ybodychange",
    "ceea91c9cd8b2a18be13217894ccf1c17198de18": "Ybodychange",
    "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf": "Ybodychange",
    "f2f5cdb5554d294a29ebf465101c5607fd56e244": "Ybodychange",
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a3954ccab148bddc290cb96528e63ff19799bcc9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
      "commitDate": "05/05/17 12:01 PM",
      "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "15/02/17 10:44 AM",
      "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 79.01,
      "commitsBetweenForRepo": 465,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   private void setupPipelineForAppendOrRecovery() throws IOException {\n     // Check number of datanodes. Note that if there is no healthy datanode,\n     // this must be internal error because we mark external error in striped\n     // outputstream only when all the streamers are in the DATA_STREAMING stage\n     if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n       String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n           + src + \"\\\" - Aborting...\" + this;\n       LOG.warn(msg);\n       lastException.set(new IOException(msg));\n       streamerClosed \u003d true;\n       return;\n     }\n-    setupPipelineInternal(nodes, storageTypes);\n+    setupPipelineInternal(nodes, storageTypes, storageIDs);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void setupPipelineForAppendOrRecovery() throws IOException {\n    // Check number of datanodes. Note that if there is no healthy datanode,\n    // this must be internal error because we mark external error in striped\n    // outputstream only when all the streamers are in the DATA_STREAMING stage\n    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n      String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n          + src + \"\\\" - Aborting...\" + this;\n      LOG.warn(msg);\n      lastException.set(new IOException(msg));\n      streamerClosed \u003d true;\n      return;\n    }\n    setupPipelineInternal(nodes, storageTypes, storageIDs);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9180. Update excluded DataNodes in DFSStripedOutputStream based on failures in data streamers. Contributed by Jing Zhao.\n",
      "commitDate": "06/10/15 10:56 AM",
      "commitName": "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 2.97,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,14 @@\n   private void setupPipelineForAppendOrRecovery() throws IOException {\n-    // check number of datanodes\n+    // Check number of datanodes. Note that if there is no healthy datanode,\n+    // this must be internal error because we mark external error in striped\n+    // outputstream only when all the streamers are in the DATA_STREAMING stage\n     if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n       String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n-          + src + \"\\\" - Aborting...\";\n+          + src + \"\\\" - Aborting...\" + this;\n       LOG.warn(msg);\n       lastException.set(new IOException(msg));\n       streamerClosed \u003d true;\n       return;\n     }\n     setupPipelineInternal(nodes, storageTypes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void setupPipelineForAppendOrRecovery() throws IOException {\n    // Check number of datanodes. Note that if there is no healthy datanode,\n    // this must be internal error because we mark external error in striped\n    // outputstream only when all the streamers are in the DATA_STREAMING stage\n    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n      String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n          + src + \"\\\" - Aborting...\" + this;\n      LOG.warn(msg);\n      lastException.set(new IOException(msg));\n      streamerClosed \u003d true;\n      return;\n    }\n    setupPipelineInternal(nodes, storageTypes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean setupPipelineForAppendOrRecovery() throws IOException {\n    // check number of datanodes\n    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n      String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n          + src + \"\\\" - Aborting...\";\n      LOG.warn(msg);\n      lastException.set(new IOException(msg));\n      streamerClosed \u003d true;\n      return false;\n    }\n\n    boolean success \u003d false;\n    long newGS \u003d 0L;\n    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      if (!handleRestartingDatanode()) {\n        return false;\n      }\n\n      final boolean isRecovery \u003d errorState.hasError();\n      if (!handleBadDatanode()) {\n        return false;\n      }\n\n      handleDatanodeReplacement();\n\n      // get a new generation stamp and an access token\n      final LocatedBlock lb \u003d updateBlockForPipeline();\n      newGS \u003d lb.getBlock().getGenerationStamp();\n      accessToken \u003d lb.getBlockToken();\n\n      // set up the pipeline again with the remaining nodes\n      success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n\n      failPacket4Testing();\n\n      errorState.checkRestartingNodeDeadline(nodes);\n    } // while\n\n    if (success) {\n      block \u003d updatePipeline(newGS);\n    }\n    return false; // do not sleep, continue processing\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
      }
    },
    "8f378733423a5244461df79a92c00239514b8b93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8397. Refactor the error handling code in DataStreamer. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "15/05/15 4:14 PM",
      "commitName": "8f378733423a5244461df79a92c00239514b8b93",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "08/05/15 12:11 AM",
      "commitNameOld": "730f9930a48259f34e48404aee51e8d641cc3d36",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 7.67,
      "commitsBetweenForRepo": 95,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,163 +1,43 @@\n   private boolean setupPipelineForAppendOrRecovery() throws IOException {\n     // check number of datanodes\n     if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n       String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n           + src + \"\\\" - Aborting...\";\n       LOG.warn(msg);\n       lastException.set(new IOException(msg));\n       streamerClosed \u003d true;\n       return false;\n     }\n \n     boolean success \u003d false;\n     long newGS \u003d 0L;\n     while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n-      // Sleep before reconnect if a dn is restarting.\n-      // This process will be repeated until the deadline or the datanode\n-      // starts back up.\n-      if (restartingNodeIndex.get() \u003e\u003d 0) {\n-        // 4 seconds or the configured deadline period, whichever is shorter.\n-        // This is the retry interval and recovery will be retried in this\n-        // interval until timeout or success.\n-        long delay \u003d Math.min(dfsClient.getConf().getDatanodeRestartTimeout(),\n-            4000L);\n-        try {\n-          Thread.sleep(delay);\n-        } catch (InterruptedException ie) {\n-          lastException.set(new IOException(\"Interrupted while waiting for \" +\n-              \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n-          streamerClosed \u003d true;\n-          return false;\n-        }\n-      }\n-      boolean isRecovery \u003d hasError;\n-      // remove bad datanode from list of datanodes.\n-      // If errorIndex was not set (i.e. appends), then do not remove\n-      // any datanodes\n-      //\n-      if (errorIndex \u003e\u003d 0) {\n-        StringBuilder pipelineMsg \u003d new StringBuilder();\n-        for (int j \u003d 0; j \u003c nodes.length; j++) {\n-          pipelineMsg.append(nodes[j]);\n-          if (j \u003c nodes.length - 1) {\n-            pipelineMsg.append(\", \");\n-          }\n-        }\n-        if (nodes.length \u003c\u003d 1) {\n-          lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n-              + \" are bad. Aborting...\"));\n-          streamerClosed \u003d true;\n-          return false;\n-        }\n-        LOG.warn(\"Error Recovery for block \" + block +\n-            \" in pipeline \" + pipelineMsg +\n-            \": bad datanode \" + nodes[errorIndex]);\n-        failed.add(nodes[errorIndex]);\n-\n-        DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n-        arraycopy(nodes, newnodes, errorIndex);\n-\n-        final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n-        arraycopy(storageTypes, newStorageTypes, errorIndex);\n-\n-        final String[] newStorageIDs \u003d new String[newnodes.length];\n-        arraycopy(storageIDs, newStorageIDs, errorIndex);\n-\n-        setPipeline(newnodes, newStorageTypes, newStorageIDs);\n-\n-        // Just took care of a node error while waiting for a node restart\n-        if (restartingNodeIndex.get() \u003e\u003d 0) {\n-          // If the error came from a node further away than the restarting\n-          // node, the restart must have been complete.\n-          if (errorIndex \u003e restartingNodeIndex.get()) {\n-            restartingNodeIndex.set(-1);\n-          } else if (errorIndex \u003c restartingNodeIndex.get()) {\n-            // the node index has shifted.\n-            restartingNodeIndex.decrementAndGet();\n-          } else {\n-            // this shouldn\u0027t happen...\n-            assert false;\n-          }\n-        }\n-\n-        if (restartingNodeIndex.get() \u003d\u003d -1) {\n-          hasError \u003d false;\n-        }\n-        lastException.clear();\n-        errorIndex \u003d -1;\n+      if (!handleRestartingDatanode()) {\n+        return false;\n       }\n \n-      // Check if replace-datanode policy is satisfied.\n-      if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(),\n-          nodes, isAppend, isHflushed)) {\n-        try {\n-          addDatanode2ExistingPipeline();\n-        } catch(IOException ioe) {\n-          if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n-            throw ioe;\n-          }\n-          LOG.warn(\"Failed to replace datanode.\"\n-              + \" Continue with the remaining datanodes since \"\n-              + HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.BEST_EFFORT_KEY\n-              + \" is set to true.\", ioe);\n-        }\n+      final boolean isRecovery \u003d errorState.hasError();\n+      if (!handleBadDatanode()) {\n+        return false;\n       }\n \n+      handleDatanodeReplacement();\n+\n       // get a new generation stamp and an access token\n-      LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n+      final LocatedBlock lb \u003d updateBlockForPipeline();\n       newGS \u003d lb.getBlock().getGenerationStamp();\n       accessToken \u003d lb.getBlockToken();\n \n       // set up the pipeline again with the remaining nodes\n-      if (failPacket) { // for testing\n-        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n-        failPacket \u003d false;\n-        try {\n-          // Give DNs time to send in bad reports. In real situations,\n-          // good reports should follow bad ones, if client committed\n-          // with those nodes.\n-          Thread.sleep(2000);\n-        } catch (InterruptedException ie) {}\n-      } else {\n-        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n-      }\n+      success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n \n-      if (restartingNodeIndex.get() \u003e\u003d 0) {\n-        assert hasError \u003d\u003d true;\n-        // check errorIndex set above\n-        if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n-          // ignore, if came from the restarting node\n-          errorIndex \u003d -1;\n-        }\n-        // still within the deadline\n-        if (Time.monotonicNow() \u003c restartDeadline) {\n-          continue; // with in the deadline\n-        }\n-        // expired. declare the restarting node dead\n-        restartDeadline \u003d 0;\n-        int expiredNodeIndex \u003d restartingNodeIndex.get();\n-        restartingNodeIndex.set(-1);\n-        LOG.warn(\"Datanode did not restart in time: \" +\n-            nodes[expiredNodeIndex]);\n-        // Mark the restarting node as failed. If there is any other failed\n-        // node during the last pipeline construction attempt, it will not be\n-        // overwritten/dropped. In this case, the restarting node will get\n-        // excluded in the following attempt, if it still does not come up.\n-        if (errorIndex \u003d\u003d -1) {\n-          errorIndex \u003d expiredNodeIndex;\n-        }\n-        // From this point on, normal pipeline recovery applies.\n-      }\n+      failPacket4Testing();\n+\n+      errorState.checkRestartingNodeDeadline(nodes);\n     } // while\n \n     if (success) {\n-      // update pipeline at the namenode\n-      ExtendedBlock newBlock \u003d new ExtendedBlock(\n-          block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n-      dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n-          nodes, storageIDs);\n-      // update client side generation stamp\n-      block \u003d newBlock;\n+      block \u003d updatePipeline(newGS);\n     }\n     return false; // do not sleep, continue processing\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean setupPipelineForAppendOrRecovery() throws IOException {\n    // check number of datanodes\n    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n      String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n          + src + \"\\\" - Aborting...\";\n      LOG.warn(msg);\n      lastException.set(new IOException(msg));\n      streamerClosed \u003d true;\n      return false;\n    }\n\n    boolean success \u003d false;\n    long newGS \u003d 0L;\n    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      if (!handleRestartingDatanode()) {\n        return false;\n      }\n\n      final boolean isRecovery \u003d errorState.hasError();\n      if (!handleBadDatanode()) {\n        return false;\n      }\n\n      handleDatanodeReplacement();\n\n      // get a new generation stamp and an access token\n      final LocatedBlock lb \u003d updateBlockForPipeline();\n      newGS \u003d lb.getBlock().getGenerationStamp();\n      accessToken \u003d lb.getBlockToken();\n\n      // set up the pipeline again with the remaining nodes\n      success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n\n      failPacket4Testing();\n\n      errorState.checkRestartingNodeDeadline(nodes);\n    } // while\n\n    if (success) {\n      block \u003d updatePipeline(newGS);\n    }\n    return false; // do not sleep, continue processing\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "7947e5b53b9ac9524b535b0384c1c355b74723ff": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8283. DataStreamer cleanup and some minor improvement. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "29/04/15 10:41 AM",
      "commitName": "7947e5b53b9ac9524b535b0384c1c355b74723ff",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/04/15 12:21 AM",
      "commitNameOld": "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 5.43,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,163 +1,163 @@\n   private boolean setupPipelineForAppendOrRecovery() throws IOException {\n     // check number of datanodes\n     if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n       String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n           + src + \"\\\" - Aborting...\";\n-      DFSClient.LOG.warn(msg);\n-      setLastException(new IOException(msg));\n+      LOG.warn(msg);\n+      lastException.set(new IOException(msg));\n       streamerClosed \u003d true;\n       return false;\n     }\n \n     boolean success \u003d false;\n     long newGS \u003d 0L;\n     while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n       // Sleep before reconnect if a dn is restarting.\n       // This process will be repeated until the deadline or the datanode\n       // starts back up.\n       if (restartingNodeIndex.get() \u003e\u003d 0) {\n         // 4 seconds or the configured deadline period, whichever is shorter.\n         // This is the retry interval and recovery will be retried in this\n         // interval until timeout or success.\n         long delay \u003d Math.min(dfsClient.getConf().getDatanodeRestartTimeout(),\n             4000L);\n         try {\n           Thread.sleep(delay);\n         } catch (InterruptedException ie) {\n           lastException.set(new IOException(\"Interrupted while waiting for \" +\n               \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n           streamerClosed \u003d true;\n           return false;\n         }\n       }\n       boolean isRecovery \u003d hasError;\n       // remove bad datanode from list of datanodes.\n       // If errorIndex was not set (i.e. appends), then do not remove\n       // any datanodes\n       //\n       if (errorIndex \u003e\u003d 0) {\n         StringBuilder pipelineMsg \u003d new StringBuilder();\n         for (int j \u003d 0; j \u003c nodes.length; j++) {\n           pipelineMsg.append(nodes[j]);\n           if (j \u003c nodes.length - 1) {\n             pipelineMsg.append(\", \");\n           }\n         }\n         if (nodes.length \u003c\u003d 1) {\n           lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n               + \" are bad. Aborting...\"));\n           streamerClosed \u003d true;\n           return false;\n         }\n-        DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n+        LOG.warn(\"Error Recovery for block \" + block +\n             \" in pipeline \" + pipelineMsg +\n             \": bad datanode \" + nodes[errorIndex]);\n         failed.add(nodes[errorIndex]);\n \n         DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n         arraycopy(nodes, newnodes, errorIndex);\n \n         final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n         arraycopy(storageTypes, newStorageTypes, errorIndex);\n \n         final String[] newStorageIDs \u003d new String[newnodes.length];\n         arraycopy(storageIDs, newStorageIDs, errorIndex);\n \n         setPipeline(newnodes, newStorageTypes, newStorageIDs);\n \n         // Just took care of a node error while waiting for a node restart\n         if (restartingNodeIndex.get() \u003e\u003d 0) {\n           // If the error came from a node further away than the restarting\n           // node, the restart must have been complete.\n           if (errorIndex \u003e restartingNodeIndex.get()) {\n             restartingNodeIndex.set(-1);\n           } else if (errorIndex \u003c restartingNodeIndex.get()) {\n             // the node index has shifted.\n             restartingNodeIndex.decrementAndGet();\n           } else {\n             // this shouldn\u0027t happen...\n             assert false;\n           }\n         }\n \n         if (restartingNodeIndex.get() \u003d\u003d -1) {\n           hasError \u003d false;\n         }\n-        lastException.set(null);\n+        lastException.clear();\n         errorIndex \u003d -1;\n       }\n \n       // Check if replace-datanode policy is satisfied.\n       if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(),\n           nodes, isAppend, isHflushed)) {\n         try {\n           addDatanode2ExistingPipeline();\n         } catch(IOException ioe) {\n           if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n             throw ioe;\n           }\n-          DFSClient.LOG.warn(\"Failed to replace datanode.\"\n+          LOG.warn(\"Failed to replace datanode.\"\n               + \" Continue with the remaining datanodes since \"\n               + HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.BEST_EFFORT_KEY\n               + \" is set to true.\", ioe);\n         }\n       }\n \n       // get a new generation stamp and an access token\n       LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n       newGS \u003d lb.getBlock().getGenerationStamp();\n       accessToken \u003d lb.getBlockToken();\n \n       // set up the pipeline again with the remaining nodes\n       if (failPacket) { // for testing\n         success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n         failPacket \u003d false;\n         try {\n           // Give DNs time to send in bad reports. In real situations,\n           // good reports should follow bad ones, if client committed\n           // with those nodes.\n           Thread.sleep(2000);\n         } catch (InterruptedException ie) {}\n       } else {\n         success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n       }\n \n       if (restartingNodeIndex.get() \u003e\u003d 0) {\n         assert hasError \u003d\u003d true;\n         // check errorIndex set above\n         if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n           // ignore, if came from the restarting node\n           errorIndex \u003d -1;\n         }\n         // still within the deadline\n         if (Time.monotonicNow() \u003c restartDeadline) {\n           continue; // with in the deadline\n         }\n         // expired. declare the restarting node dead\n         restartDeadline \u003d 0;\n         int expiredNodeIndex \u003d restartingNodeIndex.get();\n         restartingNodeIndex.set(-1);\n-        DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n+        LOG.warn(\"Datanode did not restart in time: \" +\n             nodes[expiredNodeIndex]);\n         // Mark the restarting node as failed. If there is any other failed\n         // node during the last pipeline construction attempt, it will not be\n         // overwritten/dropped. In this case, the restarting node will get\n         // excluded in the following attempt, if it still does not come up.\n         if (errorIndex \u003d\u003d -1) {\n           errorIndex \u003d expiredNodeIndex;\n         }\n         // From this point on, normal pipeline recovery applies.\n       }\n     } // while\n \n     if (success) {\n       // update pipeline at the namenode\n       ExtendedBlock newBlock \u003d new ExtendedBlock(\n           block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n       dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n           nodes, storageIDs);\n       // update client side generation stamp\n       block \u003d newBlock;\n     }\n     return false; // do not sleep, continue processing\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean setupPipelineForAppendOrRecovery() throws IOException {\n    // check number of datanodes\n    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n      String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n          + src + \"\\\" - Aborting...\";\n      LOG.warn(msg);\n      lastException.set(new IOException(msg));\n      streamerClosed \u003d true;\n      return false;\n    }\n\n    boolean success \u003d false;\n    long newGS \u003d 0L;\n    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      // Sleep before reconnect if a dn is restarting.\n      // This process will be repeated until the deadline or the datanode\n      // starts back up.\n      if (restartingNodeIndex.get() \u003e\u003d 0) {\n        // 4 seconds or the configured deadline period, whichever is shorter.\n        // This is the retry interval and recovery will be retried in this\n        // interval until timeout or success.\n        long delay \u003d Math.min(dfsClient.getConf().getDatanodeRestartTimeout(),\n            4000L);\n        try {\n          Thread.sleep(delay);\n        } catch (InterruptedException ie) {\n          lastException.set(new IOException(\"Interrupted while waiting for \" +\n              \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n          streamerClosed \u003d true;\n          return false;\n        }\n      }\n      boolean isRecovery \u003d hasError;\n      // remove bad datanode from list of datanodes.\n      // If errorIndex was not set (i.e. appends), then do not remove\n      // any datanodes\n      //\n      if (errorIndex \u003e\u003d 0) {\n        StringBuilder pipelineMsg \u003d new StringBuilder();\n        for (int j \u003d 0; j \u003c nodes.length; j++) {\n          pipelineMsg.append(nodes[j]);\n          if (j \u003c nodes.length - 1) {\n            pipelineMsg.append(\", \");\n          }\n        }\n        if (nodes.length \u003c\u003d 1) {\n          lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n              + \" are bad. Aborting...\"));\n          streamerClosed \u003d true;\n          return false;\n        }\n        LOG.warn(\"Error Recovery for block \" + block +\n            \" in pipeline \" + pipelineMsg +\n            \": bad datanode \" + nodes[errorIndex]);\n        failed.add(nodes[errorIndex]);\n\n        DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n        arraycopy(nodes, newnodes, errorIndex);\n\n        final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n        arraycopy(storageTypes, newStorageTypes, errorIndex);\n\n        final String[] newStorageIDs \u003d new String[newnodes.length];\n        arraycopy(storageIDs, newStorageIDs, errorIndex);\n\n        setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n        // Just took care of a node error while waiting for a node restart\n        if (restartingNodeIndex.get() \u003e\u003d 0) {\n          // If the error came from a node further away than the restarting\n          // node, the restart must have been complete.\n          if (errorIndex \u003e restartingNodeIndex.get()) {\n            restartingNodeIndex.set(-1);\n          } else if (errorIndex \u003c restartingNodeIndex.get()) {\n            // the node index has shifted.\n            restartingNodeIndex.decrementAndGet();\n          } else {\n            // this shouldn\u0027t happen...\n            assert false;\n          }\n        }\n\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          hasError \u003d false;\n        }\n        lastException.clear();\n        errorIndex \u003d -1;\n      }\n\n      // Check if replace-datanode policy is satisfied.\n      if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(),\n          nodes, isAppend, isHflushed)) {\n        try {\n          addDatanode2ExistingPipeline();\n        } catch(IOException ioe) {\n          if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n            throw ioe;\n          }\n          LOG.warn(\"Failed to replace datanode.\"\n              + \" Continue with the remaining datanodes since \"\n              + HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.BEST_EFFORT_KEY\n              + \" is set to true.\", ioe);\n        }\n      }\n\n      // get a new generation stamp and an access token\n      LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n      newGS \u003d lb.getBlock().getGenerationStamp();\n      accessToken \u003d lb.getBlockToken();\n\n      // set up the pipeline again with the remaining nodes\n      if (failPacket) { // for testing\n        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n        failPacket \u003d false;\n        try {\n          // Give DNs time to send in bad reports. In real situations,\n          // good reports should follow bad ones, if client committed\n          // with those nodes.\n          Thread.sleep(2000);\n        } catch (InterruptedException ie) {}\n      } else {\n        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n      }\n\n      if (restartingNodeIndex.get() \u003e\u003d 0) {\n        assert hasError \u003d\u003d true;\n        // check errorIndex set above\n        if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n          // ignore, if came from the restarting node\n          errorIndex \u003d -1;\n        }\n        // still within the deadline\n        if (Time.monotonicNow() \u003c restartDeadline) {\n          continue; // with in the deadline\n        }\n        // expired. declare the restarting node dead\n        restartDeadline \u003d 0;\n        int expiredNodeIndex \u003d restartingNodeIndex.get();\n        restartingNodeIndex.set(-1);\n        LOG.warn(\"Datanode did not restart in time: \" +\n            nodes[expiredNodeIndex]);\n        // Mark the restarting node as failed. If there is any other failed\n        // node during the last pipeline construction attempt, it will not be\n        // overwritten/dropped. In this case, the restarting node will get\n        // excluded in the following attempt, if it still does not come up.\n        if (errorIndex \u003d\u003d -1) {\n          errorIndex \u003d expiredNodeIndex;\n        }\n        // From this point on, normal pipeline recovery applies.\n      }\n    } // while\n\n    if (success) {\n      // update pipeline at the namenode\n      ExtendedBlock newBlock \u003d new ExtendedBlock(\n          block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n      dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n          nodes, storageIDs);\n      // update client side generation stamp\n      block \u003d newBlock;\n    }\n    return false; // do not sleep, continue processing\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "7fc50e2525b8b8fe36d92e283a68eeeb09c63d21": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8083. Move dfs.client.write.* conf from DFSConfigKeys to HdfsClientConfigKeys.Write.\n",
      "commitDate": "13/04/15 11:43 AM",
      "commitName": "7fc50e2525b8b8fe36d92e283a68eeeb09c63d21",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "10/04/15 2:48 PM",
      "commitNameOld": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 2.87,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,163 +1,163 @@\n   private boolean setupPipelineForAppendOrRecovery() throws IOException {\n     // check number of datanodes\n     if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n       String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n           + src + \"\\\" - Aborting...\";\n       DFSClient.LOG.warn(msg);\n       setLastException(new IOException(msg));\n       streamerClosed \u003d true;\n       return false;\n     }\n \n     boolean success \u003d false;\n     long newGS \u003d 0L;\n     while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n       // Sleep before reconnect if a dn is restarting.\n       // This process will be repeated until the deadline or the datanode\n       // starts back up.\n       if (restartingNodeIndex.get() \u003e\u003d 0) {\n         // 4 seconds or the configured deadline period, whichever is shorter.\n         // This is the retry interval and recovery will be retried in this\n         // interval until timeout or success.\n         long delay \u003d Math.min(dfsClient.getConf().getDatanodeRestartTimeout(),\n             4000L);\n         try {\n           Thread.sleep(delay);\n         } catch (InterruptedException ie) {\n           lastException.set(new IOException(\"Interrupted while waiting for \" +\n               \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n           streamerClosed \u003d true;\n           return false;\n         }\n       }\n       boolean isRecovery \u003d hasError;\n       // remove bad datanode from list of datanodes.\n       // If errorIndex was not set (i.e. appends), then do not remove\n       // any datanodes\n       //\n       if (errorIndex \u003e\u003d 0) {\n         StringBuilder pipelineMsg \u003d new StringBuilder();\n         for (int j \u003d 0; j \u003c nodes.length; j++) {\n           pipelineMsg.append(nodes[j]);\n           if (j \u003c nodes.length - 1) {\n             pipelineMsg.append(\", \");\n           }\n         }\n         if (nodes.length \u003c\u003d 1) {\n           lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n               + \" are bad. Aborting...\"));\n           streamerClosed \u003d true;\n           return false;\n         }\n         DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n             \" in pipeline \" + pipelineMsg +\n             \": bad datanode \" + nodes[errorIndex]);\n         failed.add(nodes[errorIndex]);\n \n         DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n         arraycopy(nodes, newnodes, errorIndex);\n \n         final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n         arraycopy(storageTypes, newStorageTypes, errorIndex);\n \n         final String[] newStorageIDs \u003d new String[newnodes.length];\n         arraycopy(storageIDs, newStorageIDs, errorIndex);\n \n         setPipeline(newnodes, newStorageTypes, newStorageIDs);\n \n         // Just took care of a node error while waiting for a node restart\n         if (restartingNodeIndex.get() \u003e\u003d 0) {\n           // If the error came from a node further away than the restarting\n           // node, the restart must have been complete.\n           if (errorIndex \u003e restartingNodeIndex.get()) {\n             restartingNodeIndex.set(-1);\n           } else if (errorIndex \u003c restartingNodeIndex.get()) {\n             // the node index has shifted.\n             restartingNodeIndex.decrementAndGet();\n           } else {\n             // this shouldn\u0027t happen...\n             assert false;\n           }\n         }\n \n         if (restartingNodeIndex.get() \u003d\u003d -1) {\n           hasError \u003d false;\n         }\n         lastException.set(null);\n         errorIndex \u003d -1;\n       }\n \n       // Check if replace-datanode policy is satisfied.\n       if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(),\n           nodes, isAppend, isHflushed)) {\n         try {\n           addDatanode2ExistingPipeline();\n         } catch(IOException ioe) {\n           if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n             throw ioe;\n           }\n           DFSClient.LOG.warn(\"Failed to replace datanode.\"\n               + \" Continue with the remaining datanodes since \"\n-              + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n+              + HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.BEST_EFFORT_KEY\n               + \" is set to true.\", ioe);\n         }\n       }\n \n       // get a new generation stamp and an access token\n       LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n       newGS \u003d lb.getBlock().getGenerationStamp();\n       accessToken \u003d lb.getBlockToken();\n \n       // set up the pipeline again with the remaining nodes\n       if (failPacket) { // for testing\n         success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n         failPacket \u003d false;\n         try {\n           // Give DNs time to send in bad reports. In real situations,\n           // good reports should follow bad ones, if client committed\n           // with those nodes.\n           Thread.sleep(2000);\n         } catch (InterruptedException ie) {}\n       } else {\n         success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n       }\n \n       if (restartingNodeIndex.get() \u003e\u003d 0) {\n         assert hasError \u003d\u003d true;\n         // check errorIndex set above\n         if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n           // ignore, if came from the restarting node\n           errorIndex \u003d -1;\n         }\n         // still within the deadline\n         if (Time.monotonicNow() \u003c restartDeadline) {\n           continue; // with in the deadline\n         }\n         // expired. declare the restarting node dead\n         restartDeadline \u003d 0;\n         int expiredNodeIndex \u003d restartingNodeIndex.get();\n         restartingNodeIndex.set(-1);\n         DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n             nodes[expiredNodeIndex]);\n         // Mark the restarting node as failed. If there is any other failed\n         // node during the last pipeline construction attempt, it will not be\n         // overwritten/dropped. In this case, the restarting node will get\n         // excluded in the following attempt, if it still does not come up.\n         if (errorIndex \u003d\u003d -1) {\n           errorIndex \u003d expiredNodeIndex;\n         }\n         // From this point on, normal pipeline recovery applies.\n       }\n     } // while\n \n     if (success) {\n       // update pipeline at the namenode\n       ExtendedBlock newBlock \u003d new ExtendedBlock(\n           block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n       dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n           nodes, storageIDs);\n       // update client side generation stamp\n       block \u003d newBlock;\n     }\n     return false; // do not sleep, continue processing\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean setupPipelineForAppendOrRecovery() throws IOException {\n    // check number of datanodes\n    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n      String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n          + src + \"\\\" - Aborting...\";\n      DFSClient.LOG.warn(msg);\n      setLastException(new IOException(msg));\n      streamerClosed \u003d true;\n      return false;\n    }\n\n    boolean success \u003d false;\n    long newGS \u003d 0L;\n    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      // Sleep before reconnect if a dn is restarting.\n      // This process will be repeated until the deadline or the datanode\n      // starts back up.\n      if (restartingNodeIndex.get() \u003e\u003d 0) {\n        // 4 seconds or the configured deadline period, whichever is shorter.\n        // This is the retry interval and recovery will be retried in this\n        // interval until timeout or success.\n        long delay \u003d Math.min(dfsClient.getConf().getDatanodeRestartTimeout(),\n            4000L);\n        try {\n          Thread.sleep(delay);\n        } catch (InterruptedException ie) {\n          lastException.set(new IOException(\"Interrupted while waiting for \" +\n              \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n          streamerClosed \u003d true;\n          return false;\n        }\n      }\n      boolean isRecovery \u003d hasError;\n      // remove bad datanode from list of datanodes.\n      // If errorIndex was not set (i.e. appends), then do not remove\n      // any datanodes\n      //\n      if (errorIndex \u003e\u003d 0) {\n        StringBuilder pipelineMsg \u003d new StringBuilder();\n        for (int j \u003d 0; j \u003c nodes.length; j++) {\n          pipelineMsg.append(nodes[j]);\n          if (j \u003c nodes.length - 1) {\n            pipelineMsg.append(\", \");\n          }\n        }\n        if (nodes.length \u003c\u003d 1) {\n          lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n              + \" are bad. Aborting...\"));\n          streamerClosed \u003d true;\n          return false;\n        }\n        DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n            \" in pipeline \" + pipelineMsg +\n            \": bad datanode \" + nodes[errorIndex]);\n        failed.add(nodes[errorIndex]);\n\n        DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n        arraycopy(nodes, newnodes, errorIndex);\n\n        final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n        arraycopy(storageTypes, newStorageTypes, errorIndex);\n\n        final String[] newStorageIDs \u003d new String[newnodes.length];\n        arraycopy(storageIDs, newStorageIDs, errorIndex);\n\n        setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n        // Just took care of a node error while waiting for a node restart\n        if (restartingNodeIndex.get() \u003e\u003d 0) {\n          // If the error came from a node further away than the restarting\n          // node, the restart must have been complete.\n          if (errorIndex \u003e restartingNodeIndex.get()) {\n            restartingNodeIndex.set(-1);\n          } else if (errorIndex \u003c restartingNodeIndex.get()) {\n            // the node index has shifted.\n            restartingNodeIndex.decrementAndGet();\n          } else {\n            // this shouldn\u0027t happen...\n            assert false;\n          }\n        }\n\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          hasError \u003d false;\n        }\n        lastException.set(null);\n        errorIndex \u003d -1;\n      }\n\n      // Check if replace-datanode policy is satisfied.\n      if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(),\n          nodes, isAppend, isHflushed)) {\n        try {\n          addDatanode2ExistingPipeline();\n        } catch(IOException ioe) {\n          if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n            throw ioe;\n          }\n          DFSClient.LOG.warn(\"Failed to replace datanode.\"\n              + \" Continue with the remaining datanodes since \"\n              + HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.BEST_EFFORT_KEY\n              + \" is set to true.\", ioe);\n        }\n      }\n\n      // get a new generation stamp and an access token\n      LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n      newGS \u003d lb.getBlock().getGenerationStamp();\n      accessToken \u003d lb.getBlockToken();\n\n      // set up the pipeline again with the remaining nodes\n      if (failPacket) { // for testing\n        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n        failPacket \u003d false;\n        try {\n          // Give DNs time to send in bad reports. In real situations,\n          // good reports should follow bad ones, if client committed\n          // with those nodes.\n          Thread.sleep(2000);\n        } catch (InterruptedException ie) {}\n      } else {\n        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n      }\n\n      if (restartingNodeIndex.get() \u003e\u003d 0) {\n        assert hasError \u003d\u003d true;\n        // check errorIndex set above\n        if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n          // ignore, if came from the restarting node\n          errorIndex \u003d -1;\n        }\n        // still within the deadline\n        if (Time.monotonicNow() \u003c restartDeadline) {\n          continue; // with in the deadline\n        }\n        // expired. declare the restarting node dead\n        restartDeadline \u003d 0;\n        int expiredNodeIndex \u003d restartingNodeIndex.get();\n        restartingNodeIndex.set(-1);\n        DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n            nodes[expiredNodeIndex]);\n        // Mark the restarting node as failed. If there is any other failed\n        // node during the last pipeline construction attempt, it will not be\n        // overwritten/dropped. In this case, the restarting node will get\n        // excluded in the following attempt, if it still does not come up.\n        if (errorIndex \u003d\u003d -1) {\n          errorIndex \u003d expiredNodeIndex;\n        }\n        // From this point on, normal pipeline recovery applies.\n      }\n    } // while\n\n    if (success) {\n      // update pipeline at the namenode\n      ExtendedBlock newBlock \u003d new ExtendedBlock(\n          block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n      dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n          nodes, storageIDs);\n      // update client side generation stamp\n      block \u003d newBlock;\n    }\n    return false; // do not sleep, continue processing\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "07/04/15 1:59 PM",
      "commitNameOld": "571a1ce9d037d99e7c9042bcb77ae7a2c4daf6d3",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 3.03,
      "commitsBetweenForRepo": 48,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,163 +1,163 @@\n   private boolean setupPipelineForAppendOrRecovery() throws IOException {\n     // check number of datanodes\n     if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n       String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n           + src + \"\\\" - Aborting...\";\n       DFSClient.LOG.warn(msg);\n       setLastException(new IOException(msg));\n       streamerClosed \u003d true;\n       return false;\n     }\n \n     boolean success \u003d false;\n     long newGS \u003d 0L;\n     while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n       // Sleep before reconnect if a dn is restarting.\n       // This process will be repeated until the deadline or the datanode\n       // starts back up.\n       if (restartingNodeIndex.get() \u003e\u003d 0) {\n         // 4 seconds or the configured deadline period, whichever is shorter.\n         // This is the retry interval and recovery will be retried in this\n         // interval until timeout or success.\n-        long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n+        long delay \u003d Math.min(dfsClient.getConf().getDatanodeRestartTimeout(),\n             4000L);\n         try {\n           Thread.sleep(delay);\n         } catch (InterruptedException ie) {\n           lastException.set(new IOException(\"Interrupted while waiting for \" +\n               \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n           streamerClosed \u003d true;\n           return false;\n         }\n       }\n       boolean isRecovery \u003d hasError;\n       // remove bad datanode from list of datanodes.\n       // If errorIndex was not set (i.e. appends), then do not remove\n       // any datanodes\n       //\n       if (errorIndex \u003e\u003d 0) {\n         StringBuilder pipelineMsg \u003d new StringBuilder();\n         for (int j \u003d 0; j \u003c nodes.length; j++) {\n           pipelineMsg.append(nodes[j]);\n           if (j \u003c nodes.length - 1) {\n             pipelineMsg.append(\", \");\n           }\n         }\n         if (nodes.length \u003c\u003d 1) {\n           lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n               + \" are bad. Aborting...\"));\n           streamerClosed \u003d true;\n           return false;\n         }\n         DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n             \" in pipeline \" + pipelineMsg +\n             \": bad datanode \" + nodes[errorIndex]);\n         failed.add(nodes[errorIndex]);\n \n         DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n         arraycopy(nodes, newnodes, errorIndex);\n \n         final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n         arraycopy(storageTypes, newStorageTypes, errorIndex);\n \n         final String[] newStorageIDs \u003d new String[newnodes.length];\n         arraycopy(storageIDs, newStorageIDs, errorIndex);\n \n         setPipeline(newnodes, newStorageTypes, newStorageIDs);\n \n         // Just took care of a node error while waiting for a node restart\n         if (restartingNodeIndex.get() \u003e\u003d 0) {\n           // If the error came from a node further away than the restarting\n           // node, the restart must have been complete.\n           if (errorIndex \u003e restartingNodeIndex.get()) {\n             restartingNodeIndex.set(-1);\n           } else if (errorIndex \u003c restartingNodeIndex.get()) {\n             // the node index has shifted.\n             restartingNodeIndex.decrementAndGet();\n           } else {\n             // this shouldn\u0027t happen...\n             assert false;\n           }\n         }\n \n         if (restartingNodeIndex.get() \u003d\u003d -1) {\n           hasError \u003d false;\n         }\n         lastException.set(null);\n         errorIndex \u003d -1;\n       }\n \n       // Check if replace-datanode policy is satisfied.\n       if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(),\n           nodes, isAppend, isHflushed)) {\n         try {\n           addDatanode2ExistingPipeline();\n         } catch(IOException ioe) {\n           if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n             throw ioe;\n           }\n           DFSClient.LOG.warn(\"Failed to replace datanode.\"\n               + \" Continue with the remaining datanodes since \"\n               + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n               + \" is set to true.\", ioe);\n         }\n       }\n \n       // get a new generation stamp and an access token\n       LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n       newGS \u003d lb.getBlock().getGenerationStamp();\n       accessToken \u003d lb.getBlockToken();\n \n       // set up the pipeline again with the remaining nodes\n       if (failPacket) { // for testing\n         success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n         failPacket \u003d false;\n         try {\n           // Give DNs time to send in bad reports. In real situations,\n           // good reports should follow bad ones, if client committed\n           // with those nodes.\n           Thread.sleep(2000);\n         } catch (InterruptedException ie) {}\n       } else {\n         success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n       }\n \n       if (restartingNodeIndex.get() \u003e\u003d 0) {\n         assert hasError \u003d\u003d true;\n         // check errorIndex set above\n         if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n           // ignore, if came from the restarting node\n           errorIndex \u003d -1;\n         }\n         // still within the deadline\n         if (Time.monotonicNow() \u003c restartDeadline) {\n           continue; // with in the deadline\n         }\n         // expired. declare the restarting node dead\n         restartDeadline \u003d 0;\n         int expiredNodeIndex \u003d restartingNodeIndex.get();\n         restartingNodeIndex.set(-1);\n         DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n             nodes[expiredNodeIndex]);\n         // Mark the restarting node as failed. If there is any other failed\n         // node during the last pipeline construction attempt, it will not be\n         // overwritten/dropped. In this case, the restarting node will get\n         // excluded in the following attempt, if it still does not come up.\n         if (errorIndex \u003d\u003d -1) {\n           errorIndex \u003d expiredNodeIndex;\n         }\n         // From this point on, normal pipeline recovery applies.\n       }\n     } // while\n \n     if (success) {\n       // update pipeline at the namenode\n       ExtendedBlock newBlock \u003d new ExtendedBlock(\n           block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n       dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n           nodes, storageIDs);\n       // update client side generation stamp\n       block \u003d newBlock;\n     }\n     return false; // do not sleep, continue processing\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean setupPipelineForAppendOrRecovery() throws IOException {\n    // check number of datanodes\n    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n      String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n          + src + \"\\\" - Aborting...\";\n      DFSClient.LOG.warn(msg);\n      setLastException(new IOException(msg));\n      streamerClosed \u003d true;\n      return false;\n    }\n\n    boolean success \u003d false;\n    long newGS \u003d 0L;\n    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      // Sleep before reconnect if a dn is restarting.\n      // This process will be repeated until the deadline or the datanode\n      // starts back up.\n      if (restartingNodeIndex.get() \u003e\u003d 0) {\n        // 4 seconds or the configured deadline period, whichever is shorter.\n        // This is the retry interval and recovery will be retried in this\n        // interval until timeout or success.\n        long delay \u003d Math.min(dfsClient.getConf().getDatanodeRestartTimeout(),\n            4000L);\n        try {\n          Thread.sleep(delay);\n        } catch (InterruptedException ie) {\n          lastException.set(new IOException(\"Interrupted while waiting for \" +\n              \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n          streamerClosed \u003d true;\n          return false;\n        }\n      }\n      boolean isRecovery \u003d hasError;\n      // remove bad datanode from list of datanodes.\n      // If errorIndex was not set (i.e. appends), then do not remove\n      // any datanodes\n      //\n      if (errorIndex \u003e\u003d 0) {\n        StringBuilder pipelineMsg \u003d new StringBuilder();\n        for (int j \u003d 0; j \u003c nodes.length; j++) {\n          pipelineMsg.append(nodes[j]);\n          if (j \u003c nodes.length - 1) {\n            pipelineMsg.append(\", \");\n          }\n        }\n        if (nodes.length \u003c\u003d 1) {\n          lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n              + \" are bad. Aborting...\"));\n          streamerClosed \u003d true;\n          return false;\n        }\n        DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n            \" in pipeline \" + pipelineMsg +\n            \": bad datanode \" + nodes[errorIndex]);\n        failed.add(nodes[errorIndex]);\n\n        DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n        arraycopy(nodes, newnodes, errorIndex);\n\n        final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n        arraycopy(storageTypes, newStorageTypes, errorIndex);\n\n        final String[] newStorageIDs \u003d new String[newnodes.length];\n        arraycopy(storageIDs, newStorageIDs, errorIndex);\n\n        setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n        // Just took care of a node error while waiting for a node restart\n        if (restartingNodeIndex.get() \u003e\u003d 0) {\n          // If the error came from a node further away than the restarting\n          // node, the restart must have been complete.\n          if (errorIndex \u003e restartingNodeIndex.get()) {\n            restartingNodeIndex.set(-1);\n          } else if (errorIndex \u003c restartingNodeIndex.get()) {\n            // the node index has shifted.\n            restartingNodeIndex.decrementAndGet();\n          } else {\n            // this shouldn\u0027t happen...\n            assert false;\n          }\n        }\n\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          hasError \u003d false;\n        }\n        lastException.set(null);\n        errorIndex \u003d -1;\n      }\n\n      // Check if replace-datanode policy is satisfied.\n      if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(),\n          nodes, isAppend, isHflushed)) {\n        try {\n          addDatanode2ExistingPipeline();\n        } catch(IOException ioe) {\n          if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n            throw ioe;\n          }\n          DFSClient.LOG.warn(\"Failed to replace datanode.\"\n              + \" Continue with the remaining datanodes since \"\n              + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n              + \" is set to true.\", ioe);\n        }\n      }\n\n      // get a new generation stamp and an access token\n      LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n      newGS \u003d lb.getBlock().getGenerationStamp();\n      accessToken \u003d lb.getBlockToken();\n\n      // set up the pipeline again with the remaining nodes\n      if (failPacket) { // for testing\n        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n        failPacket \u003d false;\n        try {\n          // Give DNs time to send in bad reports. In real situations,\n          // good reports should follow bad ones, if client committed\n          // with those nodes.\n          Thread.sleep(2000);\n        } catch (InterruptedException ie) {}\n      } else {\n        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n      }\n\n      if (restartingNodeIndex.get() \u003e\u003d 0) {\n        assert hasError \u003d\u003d true;\n        // check errorIndex set above\n        if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n          // ignore, if came from the restarting node\n          errorIndex \u003d -1;\n        }\n        // still within the deadline\n        if (Time.monotonicNow() \u003c restartDeadline) {\n          continue; // with in the deadline\n        }\n        // expired. declare the restarting node dead\n        restartDeadline \u003d 0;\n        int expiredNodeIndex \u003d restartingNodeIndex.get();\n        restartingNodeIndex.set(-1);\n        DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n            nodes[expiredNodeIndex]);\n        // Mark the restarting node as failed. If there is any other failed\n        // node during the last pipeline construction attempt, it will not be\n        // overwritten/dropped. In this case, the restarting node will get\n        // excluded in the following attempt, if it still does not come up.\n        if (errorIndex \u003d\u003d -1) {\n          errorIndex \u003d expiredNodeIndex;\n        }\n        // From this point on, normal pipeline recovery applies.\n      }\n    } // while\n\n    if (success) {\n      // update pipeline at the namenode\n      ExtendedBlock newBlock \u003d new ExtendedBlock(\n          block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n      dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n          nodes, storageIDs);\n      // update client side generation stamp\n      block \u003d newBlock;\n    }\n    return false; // do not sleep, continue processing\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "24/03/15 11:06 AM",
      "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,163 +1,163 @@\n-    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n-      // check number of datanodes\n-      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n-        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n-            + src + \"\\\" - Aborting...\";\n-        DFSClient.LOG.warn(msg);\n-        setLastException(new IOException(msg));\n-        streamerClosed \u003d true;\n-        return false;\n+  private boolean setupPipelineForAppendOrRecovery() throws IOException {\n+    // check number of datanodes\n+    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n+      String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n+          + src + \"\\\" - Aborting...\";\n+      DFSClient.LOG.warn(msg);\n+      setLastException(new IOException(msg));\n+      streamerClosed \u003d true;\n+      return false;\n+    }\n+\n+    boolean success \u003d false;\n+    long newGS \u003d 0L;\n+    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n+      // Sleep before reconnect if a dn is restarting.\n+      // This process will be repeated until the deadline or the datanode\n+      // starts back up.\n+      if (restartingNodeIndex.get() \u003e\u003d 0) {\n+        // 4 seconds or the configured deadline period, whichever is shorter.\n+        // This is the retry interval and recovery will be retried in this\n+        // interval until timeout or success.\n+        long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n+            4000L);\n+        try {\n+          Thread.sleep(delay);\n+        } catch (InterruptedException ie) {\n+          lastException.set(new IOException(\"Interrupted while waiting for \" +\n+              \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n+          streamerClosed \u003d true;\n+          return false;\n+        }\n       }\n-      \n-      boolean success \u003d false;\n-      long newGS \u003d 0L;\n-      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n-        // Sleep before reconnect if a dn is restarting.\n-        // This process will be repeated until the deadline or the datanode\n-        // starts back up.\n-        if (restartingNodeIndex.get() \u003e\u003d 0) {\n-          // 4 seconds or the configured deadline period, whichever is shorter.\n-          // This is the retry interval and recovery will be retried in this\n-          // interval until timeout or success.\n-          long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n-              4000L);\n-          try {\n-            Thread.sleep(delay);\n-          } catch (InterruptedException ie) {\n-            lastException.set(new IOException(\"Interrupted while waiting for \" +\n-                \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n-            streamerClosed \u003d true;\n-            return false;\n+      boolean isRecovery \u003d hasError;\n+      // remove bad datanode from list of datanodes.\n+      // If errorIndex was not set (i.e. appends), then do not remove\n+      // any datanodes\n+      //\n+      if (errorIndex \u003e\u003d 0) {\n+        StringBuilder pipelineMsg \u003d new StringBuilder();\n+        for (int j \u003d 0; j \u003c nodes.length; j++) {\n+          pipelineMsg.append(nodes[j]);\n+          if (j \u003c nodes.length - 1) {\n+            pipelineMsg.append(\", \");\n           }\n         }\n-        boolean isRecovery \u003d hasError;\n-        // remove bad datanode from list of datanodes.\n-        // If errorIndex was not set (i.e. appends), then do not remove \n-        // any datanodes\n-        // \n-        if (errorIndex \u003e\u003d 0) {\n-          StringBuilder pipelineMsg \u003d new StringBuilder();\n-          for (int j \u003d 0; j \u003c nodes.length; j++) {\n-            pipelineMsg.append(nodes[j]);\n-            if (j \u003c nodes.length - 1) {\n-              pipelineMsg.append(\", \");\n-            }\n+        if (nodes.length \u003c\u003d 1) {\n+          lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n+              + \" are bad. Aborting...\"));\n+          streamerClosed \u003d true;\n+          return false;\n+        }\n+        DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n+            \" in pipeline \" + pipelineMsg +\n+            \": bad datanode \" + nodes[errorIndex]);\n+        failed.add(nodes[errorIndex]);\n+\n+        DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n+        arraycopy(nodes, newnodes, errorIndex);\n+\n+        final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n+        arraycopy(storageTypes, newStorageTypes, errorIndex);\n+\n+        final String[] newStorageIDs \u003d new String[newnodes.length];\n+        arraycopy(storageIDs, newStorageIDs, errorIndex);\n+\n+        setPipeline(newnodes, newStorageTypes, newStorageIDs);\n+\n+        // Just took care of a node error while waiting for a node restart\n+        if (restartingNodeIndex.get() \u003e\u003d 0) {\n+          // If the error came from a node further away than the restarting\n+          // node, the restart must have been complete.\n+          if (errorIndex \u003e restartingNodeIndex.get()) {\n+            restartingNodeIndex.set(-1);\n+          } else if (errorIndex \u003c restartingNodeIndex.get()) {\n+            // the node index has shifted.\n+            restartingNodeIndex.decrementAndGet();\n+          } else {\n+            // this shouldn\u0027t happen...\n+            assert false;\n           }\n-          if (nodes.length \u003c\u003d 1) {\n-            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n-                + \" are bad. Aborting...\"));\n-            streamerClosed \u003d true;\n-            return false;\n+        }\n+\n+        if (restartingNodeIndex.get() \u003d\u003d -1) {\n+          hasError \u003d false;\n+        }\n+        lastException.set(null);\n+        errorIndex \u003d -1;\n+      }\n+\n+      // Check if replace-datanode policy is satisfied.\n+      if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(),\n+          nodes, isAppend, isHflushed)) {\n+        try {\n+          addDatanode2ExistingPipeline();\n+        } catch(IOException ioe) {\n+          if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n+            throw ioe;\n           }\n-          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n-              \" in pipeline \" + pipelineMsg + \n-              \": bad datanode \" + nodes[errorIndex]);\n-          failed.add(nodes[errorIndex]);\n+          DFSClient.LOG.warn(\"Failed to replace datanode.\"\n+              + \" Continue with the remaining datanodes since \"\n+              + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n+              + \" is set to true.\", ioe);\n+        }\n+      }\n \n-          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n-          arraycopy(nodes, newnodes, errorIndex);\n+      // get a new generation stamp and an access token\n+      LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n+      newGS \u003d lb.getBlock().getGenerationStamp();\n+      accessToken \u003d lb.getBlockToken();\n \n-          final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n-          arraycopy(storageTypes, newStorageTypes, errorIndex);\n+      // set up the pipeline again with the remaining nodes\n+      if (failPacket) { // for testing\n+        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n+        failPacket \u003d false;\n+        try {\n+          // Give DNs time to send in bad reports. In real situations,\n+          // good reports should follow bad ones, if client committed\n+          // with those nodes.\n+          Thread.sleep(2000);\n+        } catch (InterruptedException ie) {}\n+      } else {\n+        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n+      }\n \n-          final String[] newStorageIDs \u003d new String[newnodes.length];\n-          arraycopy(storageIDs, newStorageIDs, errorIndex);\n-          \n-          setPipeline(newnodes, newStorageTypes, newStorageIDs);\n-\n-          // Just took care of a node error while waiting for a node restart\n-          if (restartingNodeIndex.get() \u003e\u003d 0) {\n-            // If the error came from a node further away than the restarting\n-            // node, the restart must have been complete.\n-            if (errorIndex \u003e restartingNodeIndex.get()) {\n-              restartingNodeIndex.set(-1);\n-            } else if (errorIndex \u003c restartingNodeIndex.get()) {\n-              // the node index has shifted.\n-              restartingNodeIndex.decrementAndGet();\n-            } else {\n-              // this shouldn\u0027t happen...\n-              assert false;\n-            }\n-          }\n-\n-          if (restartingNodeIndex.get() \u003d\u003d -1) {\n-            hasError \u003d false;\n-          }\n-          lastException.set(null);\n+      if (restartingNodeIndex.get() \u003e\u003d 0) {\n+        assert hasError \u003d\u003d true;\n+        // check errorIndex set above\n+        if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n+          // ignore, if came from the restarting node\n           errorIndex \u003d -1;\n         }\n-\n-        // Check if replace-datanode policy is satisfied.\n-        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n-            nodes, isAppend, isHflushed)) {\n-          try {\n-            addDatanode2ExistingPipeline();\n-          } catch(IOException ioe) {\n-            if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n-              throw ioe;\n-            }\n-            DFSClient.LOG.warn(\"Failed to replace datanode.\"\n-                + \" Continue with the remaining datanodes since \"\n-                + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n-                + \" is set to true.\", ioe);\n-          }\n+        // still within the deadline\n+        if (Time.monotonicNow() \u003c restartDeadline) {\n+          continue; // with in the deadline\n         }\n-\n-        // get a new generation stamp and an access token\n-        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n-        newGS \u003d lb.getBlock().getGenerationStamp();\n-        accessToken \u003d lb.getBlockToken();\n-        \n-        // set up the pipeline again with the remaining nodes\n-        if (failPacket) { // for testing\n-          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n-          failPacket \u003d false;\n-          try {\n-            // Give DNs time to send in bad reports. In real situations,\n-            // good reports should follow bad ones, if client committed\n-            // with those nodes.\n-            Thread.sleep(2000);\n-          } catch (InterruptedException ie) {}\n-        } else {\n-          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n+        // expired. declare the restarting node dead\n+        restartDeadline \u003d 0;\n+        int expiredNodeIndex \u003d restartingNodeIndex.get();\n+        restartingNodeIndex.set(-1);\n+        DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n+            nodes[expiredNodeIndex]);\n+        // Mark the restarting node as failed. If there is any other failed\n+        // node during the last pipeline construction attempt, it will not be\n+        // overwritten/dropped. In this case, the restarting node will get\n+        // excluded in the following attempt, if it still does not come up.\n+        if (errorIndex \u003d\u003d -1) {\n+          errorIndex \u003d expiredNodeIndex;\n         }\n-\n-        if (restartingNodeIndex.get() \u003e\u003d 0) {\n-          assert hasError \u003d\u003d true;\n-          // check errorIndex set above\n-          if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n-            // ignore, if came from the restarting node\n-            errorIndex \u003d -1;\n-          }\n-          // still within the deadline\n-          if (Time.monotonicNow() \u003c restartDeadline) {\n-            continue; // with in the deadline\n-          }\n-          // expired. declare the restarting node dead\n-          restartDeadline \u003d 0;\n-          int expiredNodeIndex \u003d restartingNodeIndex.get();\n-          restartingNodeIndex.set(-1);\n-          DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n-              nodes[expiredNodeIndex]);\n-          // Mark the restarting node as failed. If there is any other failed\n-          // node during the last pipeline construction attempt, it will not be\n-          // overwritten/dropped. In this case, the restarting node will get\n-          // excluded in the following attempt, if it still does not come up.\n-          if (errorIndex \u003d\u003d -1) {\n-            errorIndex \u003d expiredNodeIndex;\n-          }\n-          // From this point on, normal pipeline recovery applies.\n-        }\n-      } // while\n-\n-      if (success) {\n-        // update pipeline at the namenode\n-        ExtendedBlock newBlock \u003d new ExtendedBlock(\n-            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n-        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n-            nodes, storageIDs);\n-        // update client side generation stamp\n-        block \u003d newBlock;\n+        // From this point on, normal pipeline recovery applies.\n       }\n-      return false; // do not sleep, continue processing\n-    }\n\\ No newline at end of file\n+    } // while\n+\n+    if (success) {\n+      // update pipeline at the namenode\n+      ExtendedBlock newBlock \u003d new ExtendedBlock(\n+          block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n+      dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n+          nodes, storageIDs);\n+      // update client side generation stamp\n+      block \u003d newBlock;\n+    }\n+    return false; // do not sleep, continue processing\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean setupPipelineForAppendOrRecovery() throws IOException {\n    // check number of datanodes\n    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n      String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n          + src + \"\\\" - Aborting...\";\n      DFSClient.LOG.warn(msg);\n      setLastException(new IOException(msg));\n      streamerClosed \u003d true;\n      return false;\n    }\n\n    boolean success \u003d false;\n    long newGS \u003d 0L;\n    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      // Sleep before reconnect if a dn is restarting.\n      // This process will be repeated until the deadline or the datanode\n      // starts back up.\n      if (restartingNodeIndex.get() \u003e\u003d 0) {\n        // 4 seconds or the configured deadline period, whichever is shorter.\n        // This is the retry interval and recovery will be retried in this\n        // interval until timeout or success.\n        long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n            4000L);\n        try {\n          Thread.sleep(delay);\n        } catch (InterruptedException ie) {\n          lastException.set(new IOException(\"Interrupted while waiting for \" +\n              \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n          streamerClosed \u003d true;\n          return false;\n        }\n      }\n      boolean isRecovery \u003d hasError;\n      // remove bad datanode from list of datanodes.\n      // If errorIndex was not set (i.e. appends), then do not remove\n      // any datanodes\n      //\n      if (errorIndex \u003e\u003d 0) {\n        StringBuilder pipelineMsg \u003d new StringBuilder();\n        for (int j \u003d 0; j \u003c nodes.length; j++) {\n          pipelineMsg.append(nodes[j]);\n          if (j \u003c nodes.length - 1) {\n            pipelineMsg.append(\", \");\n          }\n        }\n        if (nodes.length \u003c\u003d 1) {\n          lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n              + \" are bad. Aborting...\"));\n          streamerClosed \u003d true;\n          return false;\n        }\n        DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n            \" in pipeline \" + pipelineMsg +\n            \": bad datanode \" + nodes[errorIndex]);\n        failed.add(nodes[errorIndex]);\n\n        DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n        arraycopy(nodes, newnodes, errorIndex);\n\n        final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n        arraycopy(storageTypes, newStorageTypes, errorIndex);\n\n        final String[] newStorageIDs \u003d new String[newnodes.length];\n        arraycopy(storageIDs, newStorageIDs, errorIndex);\n\n        setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n        // Just took care of a node error while waiting for a node restart\n        if (restartingNodeIndex.get() \u003e\u003d 0) {\n          // If the error came from a node further away than the restarting\n          // node, the restart must have been complete.\n          if (errorIndex \u003e restartingNodeIndex.get()) {\n            restartingNodeIndex.set(-1);\n          } else if (errorIndex \u003c restartingNodeIndex.get()) {\n            // the node index has shifted.\n            restartingNodeIndex.decrementAndGet();\n          } else {\n            // this shouldn\u0027t happen...\n            assert false;\n          }\n        }\n\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          hasError \u003d false;\n        }\n        lastException.set(null);\n        errorIndex \u003d -1;\n      }\n\n      // Check if replace-datanode policy is satisfied.\n      if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(),\n          nodes, isAppend, isHflushed)) {\n        try {\n          addDatanode2ExistingPipeline();\n        } catch(IOException ioe) {\n          if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n            throw ioe;\n          }\n          DFSClient.LOG.warn(\"Failed to replace datanode.\"\n              + \" Continue with the remaining datanodes since \"\n              + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n              + \" is set to true.\", ioe);\n        }\n      }\n\n      // get a new generation stamp and an access token\n      LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n      newGS \u003d lb.getBlock().getGenerationStamp();\n      accessToken \u003d lb.getBlockToken();\n\n      // set up the pipeline again with the remaining nodes\n      if (failPacket) { // for testing\n        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n        failPacket \u003d false;\n        try {\n          // Give DNs time to send in bad reports. In real situations,\n          // good reports should follow bad ones, if client committed\n          // with those nodes.\n          Thread.sleep(2000);\n        } catch (InterruptedException ie) {}\n      } else {\n        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n      }\n\n      if (restartingNodeIndex.get() \u003e\u003d 0) {\n        assert hasError \u003d\u003d true;\n        // check errorIndex set above\n        if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n          // ignore, if came from the restarting node\n          errorIndex \u003d -1;\n        }\n        // still within the deadline\n        if (Time.monotonicNow() \u003c restartDeadline) {\n          continue; // with in the deadline\n        }\n        // expired. declare the restarting node dead\n        restartDeadline \u003d 0;\n        int expiredNodeIndex \u003d restartingNodeIndex.get();\n        restartingNodeIndex.set(-1);\n        DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n            nodes[expiredNodeIndex]);\n        // Mark the restarting node as failed. If there is any other failed\n        // node during the last pipeline construction attempt, it will not be\n        // overwritten/dropped. In this case, the restarting node will get\n        // excluded in the following attempt, if it still does not come up.\n        if (errorIndex \u003d\u003d -1) {\n          errorIndex \u003d expiredNodeIndex;\n        }\n        // From this point on, normal pipeline recovery applies.\n      }\n    } // while\n\n    if (success) {\n      // update pipeline at the namenode\n      ExtendedBlock newBlock \u003d new ExtendedBlock(\n          block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n      dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n          nodes, storageIDs);\n      // update client side generation stamp\n      block \u003d newBlock;\n    }\n    return false; // do not sleep, continue processing\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
            "oldMethodName": "setupPipelineForAppendOrRecovery",
            "newMethodName": "setupPipelineForAppendOrRecovery"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,163 +1,163 @@\n-    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n-      // check number of datanodes\n-      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n-        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n-            + src + \"\\\" - Aborting...\";\n-        DFSClient.LOG.warn(msg);\n-        setLastException(new IOException(msg));\n-        streamerClosed \u003d true;\n-        return false;\n+  private boolean setupPipelineForAppendOrRecovery() throws IOException {\n+    // check number of datanodes\n+    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n+      String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n+          + src + \"\\\" - Aborting...\";\n+      DFSClient.LOG.warn(msg);\n+      setLastException(new IOException(msg));\n+      streamerClosed \u003d true;\n+      return false;\n+    }\n+\n+    boolean success \u003d false;\n+    long newGS \u003d 0L;\n+    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n+      // Sleep before reconnect if a dn is restarting.\n+      // This process will be repeated until the deadline or the datanode\n+      // starts back up.\n+      if (restartingNodeIndex.get() \u003e\u003d 0) {\n+        // 4 seconds or the configured deadline period, whichever is shorter.\n+        // This is the retry interval and recovery will be retried in this\n+        // interval until timeout or success.\n+        long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n+            4000L);\n+        try {\n+          Thread.sleep(delay);\n+        } catch (InterruptedException ie) {\n+          lastException.set(new IOException(\"Interrupted while waiting for \" +\n+              \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n+          streamerClosed \u003d true;\n+          return false;\n+        }\n       }\n-      \n-      boolean success \u003d false;\n-      long newGS \u003d 0L;\n-      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n-        // Sleep before reconnect if a dn is restarting.\n-        // This process will be repeated until the deadline or the datanode\n-        // starts back up.\n-        if (restartingNodeIndex.get() \u003e\u003d 0) {\n-          // 4 seconds or the configured deadline period, whichever is shorter.\n-          // This is the retry interval and recovery will be retried in this\n-          // interval until timeout or success.\n-          long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n-              4000L);\n-          try {\n-            Thread.sleep(delay);\n-          } catch (InterruptedException ie) {\n-            lastException.set(new IOException(\"Interrupted while waiting for \" +\n-                \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n-            streamerClosed \u003d true;\n-            return false;\n+      boolean isRecovery \u003d hasError;\n+      // remove bad datanode from list of datanodes.\n+      // If errorIndex was not set (i.e. appends), then do not remove\n+      // any datanodes\n+      //\n+      if (errorIndex \u003e\u003d 0) {\n+        StringBuilder pipelineMsg \u003d new StringBuilder();\n+        for (int j \u003d 0; j \u003c nodes.length; j++) {\n+          pipelineMsg.append(nodes[j]);\n+          if (j \u003c nodes.length - 1) {\n+            pipelineMsg.append(\", \");\n           }\n         }\n-        boolean isRecovery \u003d hasError;\n-        // remove bad datanode from list of datanodes.\n-        // If errorIndex was not set (i.e. appends), then do not remove \n-        // any datanodes\n-        // \n-        if (errorIndex \u003e\u003d 0) {\n-          StringBuilder pipelineMsg \u003d new StringBuilder();\n-          for (int j \u003d 0; j \u003c nodes.length; j++) {\n-            pipelineMsg.append(nodes[j]);\n-            if (j \u003c nodes.length - 1) {\n-              pipelineMsg.append(\", \");\n-            }\n+        if (nodes.length \u003c\u003d 1) {\n+          lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n+              + \" are bad. Aborting...\"));\n+          streamerClosed \u003d true;\n+          return false;\n+        }\n+        DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n+            \" in pipeline \" + pipelineMsg +\n+            \": bad datanode \" + nodes[errorIndex]);\n+        failed.add(nodes[errorIndex]);\n+\n+        DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n+        arraycopy(nodes, newnodes, errorIndex);\n+\n+        final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n+        arraycopy(storageTypes, newStorageTypes, errorIndex);\n+\n+        final String[] newStorageIDs \u003d new String[newnodes.length];\n+        arraycopy(storageIDs, newStorageIDs, errorIndex);\n+\n+        setPipeline(newnodes, newStorageTypes, newStorageIDs);\n+\n+        // Just took care of a node error while waiting for a node restart\n+        if (restartingNodeIndex.get() \u003e\u003d 0) {\n+          // If the error came from a node further away than the restarting\n+          // node, the restart must have been complete.\n+          if (errorIndex \u003e restartingNodeIndex.get()) {\n+            restartingNodeIndex.set(-1);\n+          } else if (errorIndex \u003c restartingNodeIndex.get()) {\n+            // the node index has shifted.\n+            restartingNodeIndex.decrementAndGet();\n+          } else {\n+            // this shouldn\u0027t happen...\n+            assert false;\n           }\n-          if (nodes.length \u003c\u003d 1) {\n-            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n-                + \" are bad. Aborting...\"));\n-            streamerClosed \u003d true;\n-            return false;\n+        }\n+\n+        if (restartingNodeIndex.get() \u003d\u003d -1) {\n+          hasError \u003d false;\n+        }\n+        lastException.set(null);\n+        errorIndex \u003d -1;\n+      }\n+\n+      // Check if replace-datanode policy is satisfied.\n+      if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(),\n+          nodes, isAppend, isHflushed)) {\n+        try {\n+          addDatanode2ExistingPipeline();\n+        } catch(IOException ioe) {\n+          if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n+            throw ioe;\n           }\n-          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n-              \" in pipeline \" + pipelineMsg + \n-              \": bad datanode \" + nodes[errorIndex]);\n-          failed.add(nodes[errorIndex]);\n+          DFSClient.LOG.warn(\"Failed to replace datanode.\"\n+              + \" Continue with the remaining datanodes since \"\n+              + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n+              + \" is set to true.\", ioe);\n+        }\n+      }\n \n-          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n-          arraycopy(nodes, newnodes, errorIndex);\n+      // get a new generation stamp and an access token\n+      LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n+      newGS \u003d lb.getBlock().getGenerationStamp();\n+      accessToken \u003d lb.getBlockToken();\n \n-          final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n-          arraycopy(storageTypes, newStorageTypes, errorIndex);\n+      // set up the pipeline again with the remaining nodes\n+      if (failPacket) { // for testing\n+        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n+        failPacket \u003d false;\n+        try {\n+          // Give DNs time to send in bad reports. In real situations,\n+          // good reports should follow bad ones, if client committed\n+          // with those nodes.\n+          Thread.sleep(2000);\n+        } catch (InterruptedException ie) {}\n+      } else {\n+        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n+      }\n \n-          final String[] newStorageIDs \u003d new String[newnodes.length];\n-          arraycopy(storageIDs, newStorageIDs, errorIndex);\n-          \n-          setPipeline(newnodes, newStorageTypes, newStorageIDs);\n-\n-          // Just took care of a node error while waiting for a node restart\n-          if (restartingNodeIndex.get() \u003e\u003d 0) {\n-            // If the error came from a node further away than the restarting\n-            // node, the restart must have been complete.\n-            if (errorIndex \u003e restartingNodeIndex.get()) {\n-              restartingNodeIndex.set(-1);\n-            } else if (errorIndex \u003c restartingNodeIndex.get()) {\n-              // the node index has shifted.\n-              restartingNodeIndex.decrementAndGet();\n-            } else {\n-              // this shouldn\u0027t happen...\n-              assert false;\n-            }\n-          }\n-\n-          if (restartingNodeIndex.get() \u003d\u003d -1) {\n-            hasError \u003d false;\n-          }\n-          lastException.set(null);\n+      if (restartingNodeIndex.get() \u003e\u003d 0) {\n+        assert hasError \u003d\u003d true;\n+        // check errorIndex set above\n+        if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n+          // ignore, if came from the restarting node\n           errorIndex \u003d -1;\n         }\n-\n-        // Check if replace-datanode policy is satisfied.\n-        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n-            nodes, isAppend, isHflushed)) {\n-          try {\n-            addDatanode2ExistingPipeline();\n-          } catch(IOException ioe) {\n-            if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n-              throw ioe;\n-            }\n-            DFSClient.LOG.warn(\"Failed to replace datanode.\"\n-                + \" Continue with the remaining datanodes since \"\n-                + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n-                + \" is set to true.\", ioe);\n-          }\n+        // still within the deadline\n+        if (Time.monotonicNow() \u003c restartDeadline) {\n+          continue; // with in the deadline\n         }\n-\n-        // get a new generation stamp and an access token\n-        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n-        newGS \u003d lb.getBlock().getGenerationStamp();\n-        accessToken \u003d lb.getBlockToken();\n-        \n-        // set up the pipeline again with the remaining nodes\n-        if (failPacket) { // for testing\n-          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n-          failPacket \u003d false;\n-          try {\n-            // Give DNs time to send in bad reports. In real situations,\n-            // good reports should follow bad ones, if client committed\n-            // with those nodes.\n-            Thread.sleep(2000);\n-          } catch (InterruptedException ie) {}\n-        } else {\n-          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n+        // expired. declare the restarting node dead\n+        restartDeadline \u003d 0;\n+        int expiredNodeIndex \u003d restartingNodeIndex.get();\n+        restartingNodeIndex.set(-1);\n+        DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n+            nodes[expiredNodeIndex]);\n+        // Mark the restarting node as failed. If there is any other failed\n+        // node during the last pipeline construction attempt, it will not be\n+        // overwritten/dropped. In this case, the restarting node will get\n+        // excluded in the following attempt, if it still does not come up.\n+        if (errorIndex \u003d\u003d -1) {\n+          errorIndex \u003d expiredNodeIndex;\n         }\n-\n-        if (restartingNodeIndex.get() \u003e\u003d 0) {\n-          assert hasError \u003d\u003d true;\n-          // check errorIndex set above\n-          if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n-            // ignore, if came from the restarting node\n-            errorIndex \u003d -1;\n-          }\n-          // still within the deadline\n-          if (Time.monotonicNow() \u003c restartDeadline) {\n-            continue; // with in the deadline\n-          }\n-          // expired. declare the restarting node dead\n-          restartDeadline \u003d 0;\n-          int expiredNodeIndex \u003d restartingNodeIndex.get();\n-          restartingNodeIndex.set(-1);\n-          DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n-              nodes[expiredNodeIndex]);\n-          // Mark the restarting node as failed. If there is any other failed\n-          // node during the last pipeline construction attempt, it will not be\n-          // overwritten/dropped. In this case, the restarting node will get\n-          // excluded in the following attempt, if it still does not come up.\n-          if (errorIndex \u003d\u003d -1) {\n-            errorIndex \u003d expiredNodeIndex;\n-          }\n-          // From this point on, normal pipeline recovery applies.\n-        }\n-      } // while\n-\n-      if (success) {\n-        // update pipeline at the namenode\n-        ExtendedBlock newBlock \u003d new ExtendedBlock(\n-            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n-        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n-            nodes, storageIDs);\n-        // update client side generation stamp\n-        block \u003d newBlock;\n+        // From this point on, normal pipeline recovery applies.\n       }\n-      return false; // do not sleep, continue processing\n-    }\n\\ No newline at end of file\n+    } // while\n+\n+    if (success) {\n+      // update pipeline at the namenode\n+      ExtendedBlock newBlock \u003d new ExtendedBlock(\n+          block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n+      dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n+          nodes, storageIDs);\n+      // update client side generation stamp\n+      block \u003d newBlock;\n+    }\n+    return false; // do not sleep, continue processing\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean setupPipelineForAppendOrRecovery() throws IOException {\n    // check number of datanodes\n    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n      String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n          + src + \"\\\" - Aborting...\";\n      DFSClient.LOG.warn(msg);\n      setLastException(new IOException(msg));\n      streamerClosed \u003d true;\n      return false;\n    }\n\n    boolean success \u003d false;\n    long newGS \u003d 0L;\n    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      // Sleep before reconnect if a dn is restarting.\n      // This process will be repeated until the deadline or the datanode\n      // starts back up.\n      if (restartingNodeIndex.get() \u003e\u003d 0) {\n        // 4 seconds or the configured deadline period, whichever is shorter.\n        // This is the retry interval and recovery will be retried in this\n        // interval until timeout or success.\n        long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n            4000L);\n        try {\n          Thread.sleep(delay);\n        } catch (InterruptedException ie) {\n          lastException.set(new IOException(\"Interrupted while waiting for \" +\n              \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n          streamerClosed \u003d true;\n          return false;\n        }\n      }\n      boolean isRecovery \u003d hasError;\n      // remove bad datanode from list of datanodes.\n      // If errorIndex was not set (i.e. appends), then do not remove\n      // any datanodes\n      //\n      if (errorIndex \u003e\u003d 0) {\n        StringBuilder pipelineMsg \u003d new StringBuilder();\n        for (int j \u003d 0; j \u003c nodes.length; j++) {\n          pipelineMsg.append(nodes[j]);\n          if (j \u003c nodes.length - 1) {\n            pipelineMsg.append(\", \");\n          }\n        }\n        if (nodes.length \u003c\u003d 1) {\n          lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n              + \" are bad. Aborting...\"));\n          streamerClosed \u003d true;\n          return false;\n        }\n        DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n            \" in pipeline \" + pipelineMsg +\n            \": bad datanode \" + nodes[errorIndex]);\n        failed.add(nodes[errorIndex]);\n\n        DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n        arraycopy(nodes, newnodes, errorIndex);\n\n        final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n        arraycopy(storageTypes, newStorageTypes, errorIndex);\n\n        final String[] newStorageIDs \u003d new String[newnodes.length];\n        arraycopy(storageIDs, newStorageIDs, errorIndex);\n\n        setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n        // Just took care of a node error while waiting for a node restart\n        if (restartingNodeIndex.get() \u003e\u003d 0) {\n          // If the error came from a node further away than the restarting\n          // node, the restart must have been complete.\n          if (errorIndex \u003e restartingNodeIndex.get()) {\n            restartingNodeIndex.set(-1);\n          } else if (errorIndex \u003c restartingNodeIndex.get()) {\n            // the node index has shifted.\n            restartingNodeIndex.decrementAndGet();\n          } else {\n            // this shouldn\u0027t happen...\n            assert false;\n          }\n        }\n\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          hasError \u003d false;\n        }\n        lastException.set(null);\n        errorIndex \u003d -1;\n      }\n\n      // Check if replace-datanode policy is satisfied.\n      if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(),\n          nodes, isAppend, isHflushed)) {\n        try {\n          addDatanode2ExistingPipeline();\n        } catch(IOException ioe) {\n          if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n            throw ioe;\n          }\n          DFSClient.LOG.warn(\"Failed to replace datanode.\"\n              + \" Continue with the remaining datanodes since \"\n              + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n              + \" is set to true.\", ioe);\n        }\n      }\n\n      // get a new generation stamp and an access token\n      LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n      newGS \u003d lb.getBlock().getGenerationStamp();\n      accessToken \u003d lb.getBlockToken();\n\n      // set up the pipeline again with the remaining nodes\n      if (failPacket) { // for testing\n        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n        failPacket \u003d false;\n        try {\n          // Give DNs time to send in bad reports. In real situations,\n          // good reports should follow bad ones, if client committed\n          // with those nodes.\n          Thread.sleep(2000);\n        } catch (InterruptedException ie) {}\n      } else {\n        success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n      }\n\n      if (restartingNodeIndex.get() \u003e\u003d 0) {\n        assert hasError \u003d\u003d true;\n        // check errorIndex set above\n        if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n          // ignore, if came from the restarting node\n          errorIndex \u003d -1;\n        }\n        // still within the deadline\n        if (Time.monotonicNow() \u003c restartDeadline) {\n          continue; // with in the deadline\n        }\n        // expired. declare the restarting node dead\n        restartDeadline \u003d 0;\n        int expiredNodeIndex \u003d restartingNodeIndex.get();\n        restartingNodeIndex.set(-1);\n        DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n            nodes[expiredNodeIndex]);\n        // Mark the restarting node as failed. If there is any other failed\n        // node during the last pipeline construction attempt, it will not be\n        // overwritten/dropped. In this case, the restarting node will get\n        // excluded in the following attempt, if it still does not come up.\n        if (errorIndex \u003d\u003d -1) {\n          errorIndex \u003d expiredNodeIndex;\n        }\n        // From this point on, normal pipeline recovery applies.\n      }\n    } // while\n\n    if (success) {\n      // update pipeline at the namenode\n      ExtendedBlock newBlock \u003d new ExtendedBlock(\n          block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n      dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n          nodes, storageIDs);\n      // update client side generation stamp\n      block \u003d newBlock;\n    }\n    return false; // do not sleep, continue processing\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {}
        }
      ]
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "20/03/15 9:12 AM",
      "commitNameOld": "15612313f578a5115f8d03885e9b0c8c376ed56e",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 0.12,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,163 +1,163 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n         // Sleep before reconnect if a dn is restarting.\n         // This process will be repeated until the deadline or the datanode\n         // starts back up.\n         if (restartingNodeIndex.get() \u003e\u003d 0) {\n           // 4 seconds or the configured deadline period, whichever is shorter.\n           // This is the retry interval and recovery will be retried in this\n           // interval until timeout or success.\n           long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n               4000L);\n           try {\n             Thread.sleep(delay);\n           } catch (InterruptedException ie) {\n             lastException.set(new IOException(\"Interrupted while waiting for \" +\n                 \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n             streamerClosed \u003d true;\n             return false;\n           }\n         }\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n             pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n             lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                 + \" are bad. Aborting...\"));\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n               \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n           arraycopy(nodes, newnodes, errorIndex);\n \n           final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n           arraycopy(storageTypes, newStorageTypes, errorIndex);\n \n           final String[] newStorageIDs \u003d new String[newnodes.length];\n           arraycopy(storageIDs, newStorageIDs, errorIndex);\n           \n           setPipeline(newnodes, newStorageTypes, newStorageIDs);\n \n           // Just took care of a node error while waiting for a node restart\n           if (restartingNodeIndex.get() \u003e\u003d 0) {\n             // If the error came from a node further away than the restarting\n             // node, the restart must have been complete.\n             if (errorIndex \u003e restartingNodeIndex.get()) {\n               restartingNodeIndex.set(-1);\n             } else if (errorIndex \u003c restartingNodeIndex.get()) {\n               // the node index has shifted.\n               restartingNodeIndex.decrementAndGet();\n             } else {\n               // this shouldn\u0027t happen...\n               assert false;\n             }\n           }\n \n           if (restartingNodeIndex.get() \u003d\u003d -1) {\n             hasError \u003d false;\n           }\n           lastException.set(null);\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n           try {\n             addDatanode2ExistingPipeline();\n           } catch(IOException ioe) {\n             if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n               throw ioe;\n             }\n             DFSClient.LOG.warn(\"Failed to replace datanode.\"\n                 + \" Continue with the remaining datanodes since \"\n                 + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n                 + \" is set to true.\", ioe);\n           }\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n         if (failPacket) { // for testing\n           success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n           failPacket \u003d false;\n           try {\n             // Give DNs time to send in bad reports. In real situations,\n             // good reports should follow bad ones, if client committed\n             // with those nodes.\n             Thread.sleep(2000);\n           } catch (InterruptedException ie) {}\n         } else {\n           success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n         }\n \n         if (restartingNodeIndex.get() \u003e\u003d 0) {\n           assert hasError \u003d\u003d true;\n           // check errorIndex set above\n           if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n             // ignore, if came from the restarting node\n             errorIndex \u003d -1;\n           }\n           // still within the deadline\n-          if (Time.now() \u003c restartDeadline) {\n+          if (Time.monotonicNow() \u003c restartDeadline) {\n             continue; // with in the deadline\n           }\n           // expired. declare the restarting node dead\n           restartDeadline \u003d 0;\n           int expiredNodeIndex \u003d restartingNodeIndex.get();\n           restartingNodeIndex.set(-1);\n           DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n               nodes[expiredNodeIndex]);\n           // Mark the restarting node as failed. If there is any other failed\n           // node during the last pipeline construction attempt, it will not be\n           // overwritten/dropped. In this case, the restarting node will get\n           // excluded in the following attempt, if it still does not come up.\n           if (errorIndex \u003d\u003d -1) {\n             errorIndex \u003d expiredNodeIndex;\n           }\n           // From this point on, normal pipeline recovery applies.\n         }\n       } // while\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n         dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n             nodes, storageIDs);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        // Sleep before reconnect if a dn is restarting.\n        // This process will be repeated until the deadline or the datanode\n        // starts back up.\n        if (restartingNodeIndex.get() \u003e\u003d 0) {\n          // 4 seconds or the configured deadline period, whichever is shorter.\n          // This is the retry interval and recovery will be retried in this\n          // interval until timeout or success.\n          long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n              4000L);\n          try {\n            Thread.sleep(delay);\n          } catch (InterruptedException ie) {\n            lastException.set(new IOException(\"Interrupted while waiting for \" +\n                \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n            streamerClosed \u003d true;\n            return false;\n          }\n        }\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\"));\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          arraycopy(nodes, newnodes, errorIndex);\n\n          final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n          arraycopy(storageTypes, newStorageTypes, errorIndex);\n\n          final String[] newStorageIDs \u003d new String[newnodes.length];\n          arraycopy(storageIDs, newStorageIDs, errorIndex);\n          \n          setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n          // Just took care of a node error while waiting for a node restart\n          if (restartingNodeIndex.get() \u003e\u003d 0) {\n            // If the error came from a node further away than the restarting\n            // node, the restart must have been complete.\n            if (errorIndex \u003e restartingNodeIndex.get()) {\n              restartingNodeIndex.set(-1);\n            } else if (errorIndex \u003c restartingNodeIndex.get()) {\n              // the node index has shifted.\n              restartingNodeIndex.decrementAndGet();\n            } else {\n              // this shouldn\u0027t happen...\n              assert false;\n            }\n          }\n\n          if (restartingNodeIndex.get() \u003d\u003d -1) {\n            hasError \u003d false;\n          }\n          lastException.set(null);\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          try {\n            addDatanode2ExistingPipeline();\n          } catch(IOException ioe) {\n            if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n              throw ioe;\n            }\n            DFSClient.LOG.warn(\"Failed to replace datanode.\"\n                + \" Continue with the remaining datanodes since \"\n                + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n                + \" is set to true.\", ioe);\n          }\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        if (failPacket) { // for testing\n          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n          failPacket \u003d false;\n          try {\n            // Give DNs time to send in bad reports. In real situations,\n            // good reports should follow bad ones, if client committed\n            // with those nodes.\n            Thread.sleep(2000);\n          } catch (InterruptedException ie) {}\n        } else {\n          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n        }\n\n        if (restartingNodeIndex.get() \u003e\u003d 0) {\n          assert hasError \u003d\u003d true;\n          // check errorIndex set above\n          if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n            // ignore, if came from the restarting node\n            errorIndex \u003d -1;\n          }\n          // still within the deadline\n          if (Time.monotonicNow() \u003c restartDeadline) {\n            continue; // with in the deadline\n          }\n          // expired. declare the restarting node dead\n          restartDeadline \u003d 0;\n          int expiredNodeIndex \u003d restartingNodeIndex.get();\n          restartingNodeIndex.set(-1);\n          DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n              nodes[expiredNodeIndex]);\n          // Mark the restarting node as failed. If there is any other failed\n          // node during the last pipeline construction attempt, it will not be\n          // overwritten/dropped. In this case, the restarting node will get\n          // excluded in the following attempt, if it still does not come up.\n          if (errorIndex \u003d\u003d -1) {\n            errorIndex \u003d expiredNodeIndex;\n          }\n          // From this point on, normal pipeline recovery applies.\n        }\n      } // while\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n            nodes, storageIDs);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.\n",
      "commitDate": "11/12/14 12:36 PM",
      "commitName": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "17/11/14 6:49 AM",
      "commitNameOld": "6783d17fcf5b25165767888f756a6b7802ab1371",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 24.24,
      "commitsBetweenForRepo": 165,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,163 +1,163 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n         // Sleep before reconnect if a dn is restarting.\n         // This process will be repeated until the deadline or the datanode\n         // starts back up.\n-        if (restartingNodeIndex \u003e\u003d 0) {\n+        if (restartingNodeIndex.get() \u003e\u003d 0) {\n           // 4 seconds or the configured deadline period, whichever is shorter.\n           // This is the retry interval and recovery will be retried in this\n           // interval until timeout or success.\n           long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n               4000L);\n           try {\n             Thread.sleep(delay);\n           } catch (InterruptedException ie) {\n             lastException.set(new IOException(\"Interrupted while waiting for \" +\n-                \"datanode to restart. \" + nodes[restartingNodeIndex]));\n+                \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n             streamerClosed \u003d true;\n             return false;\n           }\n         }\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n             pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n             lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                 + \" are bad. Aborting...\"));\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n               \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n           arraycopy(nodes, newnodes, errorIndex);\n \n           final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n           arraycopy(storageTypes, newStorageTypes, errorIndex);\n \n           final String[] newStorageIDs \u003d new String[newnodes.length];\n           arraycopy(storageIDs, newStorageIDs, errorIndex);\n           \n           setPipeline(newnodes, newStorageTypes, newStorageIDs);\n \n           // Just took care of a node error while waiting for a node restart\n-          if (restartingNodeIndex \u003e\u003d 0) {\n+          if (restartingNodeIndex.get() \u003e\u003d 0) {\n             // If the error came from a node further away than the restarting\n             // node, the restart must have been complete.\n-            if (errorIndex \u003e restartingNodeIndex) {\n-              restartingNodeIndex \u003d -1;\n-            } else if (errorIndex \u003c restartingNodeIndex) {\n+            if (errorIndex \u003e restartingNodeIndex.get()) {\n+              restartingNodeIndex.set(-1);\n+            } else if (errorIndex \u003c restartingNodeIndex.get()) {\n               // the node index has shifted.\n-              restartingNodeIndex--;\n+              restartingNodeIndex.decrementAndGet();\n             } else {\n               // this shouldn\u0027t happen...\n               assert false;\n             }\n           }\n \n-          if (restartingNodeIndex \u003d\u003d -1) {\n+          if (restartingNodeIndex.get() \u003d\u003d -1) {\n             hasError \u003d false;\n           }\n           lastException.set(null);\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n           try {\n             addDatanode2ExistingPipeline();\n           } catch(IOException ioe) {\n             if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n               throw ioe;\n             }\n             DFSClient.LOG.warn(\"Failed to replace datanode.\"\n                 + \" Continue with the remaining datanodes since \"\n                 + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n                 + \" is set to true.\", ioe);\n           }\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n         if (failPacket) { // for testing\n           success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n           failPacket \u003d false;\n           try {\n             // Give DNs time to send in bad reports. In real situations,\n             // good reports should follow bad ones, if client committed\n             // with those nodes.\n             Thread.sleep(2000);\n           } catch (InterruptedException ie) {}\n         } else {\n           success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n         }\n \n-        if (restartingNodeIndex \u003e\u003d 0) {\n+        if (restartingNodeIndex.get() \u003e\u003d 0) {\n           assert hasError \u003d\u003d true;\n           // check errorIndex set above\n-          if (errorIndex \u003d\u003d restartingNodeIndex) {\n+          if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n             // ignore, if came from the restarting node\n             errorIndex \u003d -1;\n           }\n           // still within the deadline\n           if (Time.now() \u003c restartDeadline) {\n             continue; // with in the deadline\n           }\n           // expired. declare the restarting node dead\n           restartDeadline \u003d 0;\n-          int expiredNodeIndex \u003d restartingNodeIndex;\n-          restartingNodeIndex \u003d -1;\n+          int expiredNodeIndex \u003d restartingNodeIndex.get();\n+          restartingNodeIndex.set(-1);\n           DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n               nodes[expiredNodeIndex]);\n           // Mark the restarting node as failed. If there is any other failed\n           // node during the last pipeline construction attempt, it will not be\n           // overwritten/dropped. In this case, the restarting node will get\n           // excluded in the following attempt, if it still does not come up.\n           if (errorIndex \u003d\u003d -1) {\n             errorIndex \u003d expiredNodeIndex;\n           }\n           // From this point on, normal pipeline recovery applies.\n         }\n       } // while\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n         dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n             nodes, storageIDs);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        // Sleep before reconnect if a dn is restarting.\n        // This process will be repeated until the deadline or the datanode\n        // starts back up.\n        if (restartingNodeIndex.get() \u003e\u003d 0) {\n          // 4 seconds or the configured deadline period, whichever is shorter.\n          // This is the retry interval and recovery will be retried in this\n          // interval until timeout or success.\n          long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n              4000L);\n          try {\n            Thread.sleep(delay);\n          } catch (InterruptedException ie) {\n            lastException.set(new IOException(\"Interrupted while waiting for \" +\n                \"datanode to restart. \" + nodes[restartingNodeIndex.get()]));\n            streamerClosed \u003d true;\n            return false;\n          }\n        }\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\"));\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          arraycopy(nodes, newnodes, errorIndex);\n\n          final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n          arraycopy(storageTypes, newStorageTypes, errorIndex);\n\n          final String[] newStorageIDs \u003d new String[newnodes.length];\n          arraycopy(storageIDs, newStorageIDs, errorIndex);\n          \n          setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n          // Just took care of a node error while waiting for a node restart\n          if (restartingNodeIndex.get() \u003e\u003d 0) {\n            // If the error came from a node further away than the restarting\n            // node, the restart must have been complete.\n            if (errorIndex \u003e restartingNodeIndex.get()) {\n              restartingNodeIndex.set(-1);\n            } else if (errorIndex \u003c restartingNodeIndex.get()) {\n              // the node index has shifted.\n              restartingNodeIndex.decrementAndGet();\n            } else {\n              // this shouldn\u0027t happen...\n              assert false;\n            }\n          }\n\n          if (restartingNodeIndex.get() \u003d\u003d -1) {\n            hasError \u003d false;\n          }\n          lastException.set(null);\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          try {\n            addDatanode2ExistingPipeline();\n          } catch(IOException ioe) {\n            if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n              throw ioe;\n            }\n            DFSClient.LOG.warn(\"Failed to replace datanode.\"\n                + \" Continue with the remaining datanodes since \"\n                + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n                + \" is set to true.\", ioe);\n          }\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        if (failPacket) { // for testing\n          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n          failPacket \u003d false;\n          try {\n            // Give DNs time to send in bad reports. In real situations,\n            // good reports should follow bad ones, if client committed\n            // with those nodes.\n            Thread.sleep(2000);\n          } catch (InterruptedException ie) {}\n        } else {\n          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n        }\n\n        if (restartingNodeIndex.get() \u003e\u003d 0) {\n          assert hasError \u003d\u003d true;\n          // check errorIndex set above\n          if (errorIndex \u003d\u003d restartingNodeIndex.get()) {\n            // ignore, if came from the restarting node\n            errorIndex \u003d -1;\n          }\n          // still within the deadline\n          if (Time.now() \u003c restartDeadline) {\n            continue; // with in the deadline\n          }\n          // expired. declare the restarting node dead\n          restartDeadline \u003d 0;\n          int expiredNodeIndex \u003d restartingNodeIndex.get();\n          restartingNodeIndex.set(-1);\n          DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n              nodes[expiredNodeIndex]);\n          // Mark the restarting node as failed. If there is any other failed\n          // node during the last pipeline construction attempt, it will not be\n          // overwritten/dropped. In this case, the restarting node will get\n          // excluded in the following attempt, if it still does not come up.\n          if (errorIndex \u003d\u003d -1) {\n            errorIndex \u003d expiredNodeIndex;\n          }\n          // From this point on, normal pipeline recovery applies.\n        }\n      } // while\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n            nodes, storageIDs);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "727331becc3902cb4e60ee04741e79703238e782": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4257. The ReplaceDatanodeOnFailure policies could have a forgiving option.  Contributed by szetszwo.\n",
      "commitDate": "02/09/14 3:14 PM",
      "commitName": "727331becc3902cb4e60ee04741e79703238e782",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "28/08/14 4:44 PM",
      "commitNameOld": "ab638e77b811d9592470f7d342cd11a66efbbf0d",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.94,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,153 +1,163 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n         // Sleep before reconnect if a dn is restarting.\n         // This process will be repeated until the deadline or the datanode\n         // starts back up.\n         if (restartingNodeIndex \u003e\u003d 0) {\n           // 4 seconds or the configured deadline period, whichever is shorter.\n           // This is the retry interval and recovery will be retried in this\n           // interval until timeout or success.\n           long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n               4000L);\n           try {\n             Thread.sleep(delay);\n           } catch (InterruptedException ie) {\n             lastException.set(new IOException(\"Interrupted while waiting for \" +\n                 \"datanode to restart. \" + nodes[restartingNodeIndex]));\n             streamerClosed \u003d true;\n             return false;\n           }\n         }\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n             pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n             lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                 + \" are bad. Aborting...\"));\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n               \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n           arraycopy(nodes, newnodes, errorIndex);\n \n           final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n           arraycopy(storageTypes, newStorageTypes, errorIndex);\n \n           final String[] newStorageIDs \u003d new String[newnodes.length];\n           arraycopy(storageIDs, newStorageIDs, errorIndex);\n           \n           setPipeline(newnodes, newStorageTypes, newStorageIDs);\n \n           // Just took care of a node error while waiting for a node restart\n           if (restartingNodeIndex \u003e\u003d 0) {\n             // If the error came from a node further away than the restarting\n             // node, the restart must have been complete.\n             if (errorIndex \u003e restartingNodeIndex) {\n               restartingNodeIndex \u003d -1;\n             } else if (errorIndex \u003c restartingNodeIndex) {\n               // the node index has shifted.\n               restartingNodeIndex--;\n             } else {\n               // this shouldn\u0027t happen...\n               assert false;\n             }\n           }\n \n           if (restartingNodeIndex \u003d\u003d -1) {\n             hasError \u003d false;\n           }\n           lastException.set(null);\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n-          addDatanode2ExistingPipeline();\n+          try {\n+            addDatanode2ExistingPipeline();\n+          } catch(IOException ioe) {\n+            if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n+              throw ioe;\n+            }\n+            DFSClient.LOG.warn(\"Failed to replace datanode.\"\n+                + \" Continue with the remaining datanodes since \"\n+                + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n+                + \" is set to true.\", ioe);\n+          }\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n         if (failPacket) { // for testing\n           success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n           failPacket \u003d false;\n           try {\n             // Give DNs time to send in bad reports. In real situations,\n             // good reports should follow bad ones, if client committed\n             // with those nodes.\n             Thread.sleep(2000);\n           } catch (InterruptedException ie) {}\n         } else {\n           success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n         }\n \n         if (restartingNodeIndex \u003e\u003d 0) {\n           assert hasError \u003d\u003d true;\n           // check errorIndex set above\n           if (errorIndex \u003d\u003d restartingNodeIndex) {\n             // ignore, if came from the restarting node\n             errorIndex \u003d -1;\n           }\n           // still within the deadline\n           if (Time.now() \u003c restartDeadline) {\n             continue; // with in the deadline\n           }\n           // expired. declare the restarting node dead\n           restartDeadline \u003d 0;\n           int expiredNodeIndex \u003d restartingNodeIndex;\n           restartingNodeIndex \u003d -1;\n           DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n               nodes[expiredNodeIndex]);\n           // Mark the restarting node as failed. If there is any other failed\n           // node during the last pipeline construction attempt, it will not be\n           // overwritten/dropped. In this case, the restarting node will get\n           // excluded in the following attempt, if it still does not come up.\n           if (errorIndex \u003d\u003d -1) {\n             errorIndex \u003d expiredNodeIndex;\n           }\n           // From this point on, normal pipeline recovery applies.\n         }\n       } // while\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n         dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n             nodes, storageIDs);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        // Sleep before reconnect if a dn is restarting.\n        // This process will be repeated until the deadline or the datanode\n        // starts back up.\n        if (restartingNodeIndex \u003e\u003d 0) {\n          // 4 seconds or the configured deadline period, whichever is shorter.\n          // This is the retry interval and recovery will be retried in this\n          // interval until timeout or success.\n          long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n              4000L);\n          try {\n            Thread.sleep(delay);\n          } catch (InterruptedException ie) {\n            lastException.set(new IOException(\"Interrupted while waiting for \" +\n                \"datanode to restart. \" + nodes[restartingNodeIndex]));\n            streamerClosed \u003d true;\n            return false;\n          }\n        }\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\"));\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          arraycopy(nodes, newnodes, errorIndex);\n\n          final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n          arraycopy(storageTypes, newStorageTypes, errorIndex);\n\n          final String[] newStorageIDs \u003d new String[newnodes.length];\n          arraycopy(storageIDs, newStorageIDs, errorIndex);\n          \n          setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n          // Just took care of a node error while waiting for a node restart\n          if (restartingNodeIndex \u003e\u003d 0) {\n            // If the error came from a node further away than the restarting\n            // node, the restart must have been complete.\n            if (errorIndex \u003e restartingNodeIndex) {\n              restartingNodeIndex \u003d -1;\n            } else if (errorIndex \u003c restartingNodeIndex) {\n              // the node index has shifted.\n              restartingNodeIndex--;\n            } else {\n              // this shouldn\u0027t happen...\n              assert false;\n            }\n          }\n\n          if (restartingNodeIndex \u003d\u003d -1) {\n            hasError \u003d false;\n          }\n          lastException.set(null);\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          try {\n            addDatanode2ExistingPipeline();\n          } catch(IOException ioe) {\n            if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\n              throw ioe;\n            }\n            DFSClient.LOG.warn(\"Failed to replace datanode.\"\n                + \" Continue with the remaining datanodes since \"\n                + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT_KEY\n                + \" is set to true.\", ioe);\n          }\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        if (failPacket) { // for testing\n          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n          failPacket \u003d false;\n          try {\n            // Give DNs time to send in bad reports. In real situations,\n            // good reports should follow bad ones, if client committed\n            // with those nodes.\n            Thread.sleep(2000);\n          } catch (InterruptedException ie) {}\n        } else {\n          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n        }\n\n        if (restartingNodeIndex \u003e\u003d 0) {\n          assert hasError \u003d\u003d true;\n          // check errorIndex set above\n          if (errorIndex \u003d\u003d restartingNodeIndex) {\n            // ignore, if came from the restarting node\n            errorIndex \u003d -1;\n          }\n          // still within the deadline\n          if (Time.now() \u003c restartDeadline) {\n            continue; // with in the deadline\n          }\n          // expired. declare the restarting node dead\n          restartDeadline \u003d 0;\n          int expiredNodeIndex \u003d restartingNodeIndex;\n          restartingNodeIndex \u003d -1;\n          DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n              nodes[expiredNodeIndex]);\n          // Mark the restarting node as failed. If there is any other failed\n          // node during the last pipeline construction attempt, it will not be\n          // overwritten/dropped. In this case, the restarting node will get\n          // excluded in the following attempt, if it still does not come up.\n          if (errorIndex \u003d\u003d -1) {\n            errorIndex \u003d expiredNodeIndex;\n          }\n          // From this point on, normal pipeline recovery applies.\n        }\n      } // while\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n            nodes, storageIDs);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "552b4fb9f9a76b18605322c0b0e8072613d67773": {
      "type": "Ybodychange",
      "commitMessage": "Merge from trunk to branch\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612928 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/07/14 12:26 PM",
      "commitName": "552b4fb9f9a76b18605322c0b0e8072613d67773",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "15/07/14 2:10 PM",
      "commitNameOld": "56c0bd4d37ab13b6cbcf860eda852da603ab2f62",
      "commitAuthorOld": "",
      "daysBetweenCommits": 7.93,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,154 +1,153 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n         // Sleep before reconnect if a dn is restarting.\n         // This process will be repeated until the deadline or the datanode\n         // starts back up.\n         if (restartingNodeIndex \u003e\u003d 0) {\n           // 4 seconds or the configured deadline period, whichever is shorter.\n           // This is the retry interval and recovery will be retried in this\n           // interval until timeout or success.\n           long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n               4000L);\n           try {\n             Thread.sleep(delay);\n           } catch (InterruptedException ie) {\n             lastException.set(new IOException(\"Interrupted while waiting for \" +\n                 \"datanode to restart. \" + nodes[restartingNodeIndex]));\n             streamerClosed \u003d true;\n             return false;\n           }\n         }\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n             pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n             lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                 + \" are bad. Aborting...\"));\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n               \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n-          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n-          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n-              newnodes.length-errorIndex);\n+          arraycopy(nodes, newnodes, errorIndex);\n+\n+          final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n+          arraycopy(storageTypes, newStorageTypes, errorIndex);\n \n           final String[] newStorageIDs \u003d new String[newnodes.length];\n-          System.arraycopy(storageIDs, 0, newStorageIDs, 0, errorIndex);\n-          System.arraycopy(storageIDs, errorIndex+1, newStorageIDs, errorIndex,\n-              newStorageIDs.length-errorIndex);\n+          arraycopy(storageIDs, newStorageIDs, errorIndex);\n           \n-          setPipeline(newnodes, newStorageIDs);\n+          setPipeline(newnodes, newStorageTypes, newStorageIDs);\n \n           // Just took care of a node error while waiting for a node restart\n           if (restartingNodeIndex \u003e\u003d 0) {\n             // If the error came from a node further away than the restarting\n             // node, the restart must have been complete.\n             if (errorIndex \u003e restartingNodeIndex) {\n               restartingNodeIndex \u003d -1;\n             } else if (errorIndex \u003c restartingNodeIndex) {\n               // the node index has shifted.\n               restartingNodeIndex--;\n             } else {\n               // this shouldn\u0027t happen...\n               assert false;\n             }\n           }\n \n           if (restartingNodeIndex \u003d\u003d -1) {\n             hasError \u003d false;\n           }\n           lastException.set(null);\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n           addDatanode2ExistingPipeline();\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n         if (failPacket) { // for testing\n-          success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n+          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n           failPacket \u003d false;\n           try {\n             // Give DNs time to send in bad reports. In real situations,\n             // good reports should follow bad ones, if client committed\n             // with those nodes.\n             Thread.sleep(2000);\n           } catch (InterruptedException ie) {}\n         } else {\n-          success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n+          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n         }\n \n         if (restartingNodeIndex \u003e\u003d 0) {\n           assert hasError \u003d\u003d true;\n           // check errorIndex set above\n           if (errorIndex \u003d\u003d restartingNodeIndex) {\n             // ignore, if came from the restarting node\n             errorIndex \u003d -1;\n           }\n           // still within the deadline\n           if (Time.now() \u003c restartDeadline) {\n             continue; // with in the deadline\n           }\n           // expired. declare the restarting node dead\n           restartDeadline \u003d 0;\n           int expiredNodeIndex \u003d restartingNodeIndex;\n           restartingNodeIndex \u003d -1;\n           DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n               nodes[expiredNodeIndex]);\n           // Mark the restarting node as failed. If there is any other failed\n           // node during the last pipeline construction attempt, it will not be\n           // overwritten/dropped. In this case, the restarting node will get\n           // excluded in the following attempt, if it still does not come up.\n           if (errorIndex \u003d\u003d -1) {\n             errorIndex \u003d expiredNodeIndex;\n           }\n           // From this point on, normal pipeline recovery applies.\n         }\n       } // while\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n         dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n             nodes, storageIDs);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        // Sleep before reconnect if a dn is restarting.\n        // This process will be repeated until the deadline or the datanode\n        // starts back up.\n        if (restartingNodeIndex \u003e\u003d 0) {\n          // 4 seconds or the configured deadline period, whichever is shorter.\n          // This is the retry interval and recovery will be retried in this\n          // interval until timeout or success.\n          long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n              4000L);\n          try {\n            Thread.sleep(delay);\n          } catch (InterruptedException ie) {\n            lastException.set(new IOException(\"Interrupted while waiting for \" +\n                \"datanode to restart. \" + nodes[restartingNodeIndex]));\n            streamerClosed \u003d true;\n            return false;\n          }\n        }\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\"));\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          arraycopy(nodes, newnodes, errorIndex);\n\n          final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n          arraycopy(storageTypes, newStorageTypes, errorIndex);\n\n          final String[] newStorageIDs \u003d new String[newnodes.length];\n          arraycopy(storageIDs, newStorageIDs, errorIndex);\n          \n          setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n          // Just took care of a node error while waiting for a node restart\n          if (restartingNodeIndex \u003e\u003d 0) {\n            // If the error came from a node further away than the restarting\n            // node, the restart must have been complete.\n            if (errorIndex \u003e restartingNodeIndex) {\n              restartingNodeIndex \u003d -1;\n            } else if (errorIndex \u003c restartingNodeIndex) {\n              // the node index has shifted.\n              restartingNodeIndex--;\n            } else {\n              // this shouldn\u0027t happen...\n              assert false;\n            }\n          }\n\n          if (restartingNodeIndex \u003d\u003d -1) {\n            hasError \u003d false;\n          }\n          lastException.set(null);\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        if (failPacket) { // for testing\n          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n          failPacket \u003d false;\n          try {\n            // Give DNs time to send in bad reports. In real situations,\n            // good reports should follow bad ones, if client committed\n            // with those nodes.\n            Thread.sleep(2000);\n          } catch (InterruptedException ie) {}\n        } else {\n          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n        }\n\n        if (restartingNodeIndex \u003e\u003d 0) {\n          assert hasError \u003d\u003d true;\n          // check errorIndex set above\n          if (errorIndex \u003d\u003d restartingNodeIndex) {\n            // ignore, if came from the restarting node\n            errorIndex \u003d -1;\n          }\n          // still within the deadline\n          if (Time.now() \u003c restartDeadline) {\n            continue; // with in the deadline\n          }\n          // expired. declare the restarting node dead\n          restartDeadline \u003d 0;\n          int expiredNodeIndex \u003d restartingNodeIndex;\n          restartingNodeIndex \u003d -1;\n          DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n              nodes[expiredNodeIndex]);\n          // Mark the restarting node as failed. If there is any other failed\n          // node during the last pipeline construction attempt, it will not be\n          // overwritten/dropped. In this case, the restarting node will get\n          // excluded in the following attempt, if it still does not come up.\n          if (errorIndex \u003d\u003d -1) {\n            errorIndex \u003d expiredNodeIndex;\n          }\n          // From this point on, normal pipeline recovery applies.\n        }\n      } // while\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n            nodes, storageIDs);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/14 12:41 AM",
      "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "14/07/14 11:10 AM",
      "commitNameOld": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 7.56,
      "commitsBetweenForRepo": 68,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,154 +1,153 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n         // Sleep before reconnect if a dn is restarting.\n         // This process will be repeated until the deadline or the datanode\n         // starts back up.\n         if (restartingNodeIndex \u003e\u003d 0) {\n           // 4 seconds or the configured deadline period, whichever is shorter.\n           // This is the retry interval and recovery will be retried in this\n           // interval until timeout or success.\n           long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n               4000L);\n           try {\n             Thread.sleep(delay);\n           } catch (InterruptedException ie) {\n             lastException.set(new IOException(\"Interrupted while waiting for \" +\n                 \"datanode to restart. \" + nodes[restartingNodeIndex]));\n             streamerClosed \u003d true;\n             return false;\n           }\n         }\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n             pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n             lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                 + \" are bad. Aborting...\"));\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n               \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n-          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n-          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n-              newnodes.length-errorIndex);\n+          arraycopy(nodes, newnodes, errorIndex);\n+\n+          final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n+          arraycopy(storageTypes, newStorageTypes, errorIndex);\n \n           final String[] newStorageIDs \u003d new String[newnodes.length];\n-          System.arraycopy(storageIDs, 0, newStorageIDs, 0, errorIndex);\n-          System.arraycopy(storageIDs, errorIndex+1, newStorageIDs, errorIndex,\n-              newStorageIDs.length-errorIndex);\n+          arraycopy(storageIDs, newStorageIDs, errorIndex);\n           \n-          setPipeline(newnodes, newStorageIDs);\n+          setPipeline(newnodes, newStorageTypes, newStorageIDs);\n \n           // Just took care of a node error while waiting for a node restart\n           if (restartingNodeIndex \u003e\u003d 0) {\n             // If the error came from a node further away than the restarting\n             // node, the restart must have been complete.\n             if (errorIndex \u003e restartingNodeIndex) {\n               restartingNodeIndex \u003d -1;\n             } else if (errorIndex \u003c restartingNodeIndex) {\n               // the node index has shifted.\n               restartingNodeIndex--;\n             } else {\n               // this shouldn\u0027t happen...\n               assert false;\n             }\n           }\n \n           if (restartingNodeIndex \u003d\u003d -1) {\n             hasError \u003d false;\n           }\n           lastException.set(null);\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n           addDatanode2ExistingPipeline();\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n         if (failPacket) { // for testing\n-          success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n+          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n           failPacket \u003d false;\n           try {\n             // Give DNs time to send in bad reports. In real situations,\n             // good reports should follow bad ones, if client committed\n             // with those nodes.\n             Thread.sleep(2000);\n           } catch (InterruptedException ie) {}\n         } else {\n-          success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n+          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n         }\n \n         if (restartingNodeIndex \u003e\u003d 0) {\n           assert hasError \u003d\u003d true;\n           // check errorIndex set above\n           if (errorIndex \u003d\u003d restartingNodeIndex) {\n             // ignore, if came from the restarting node\n             errorIndex \u003d -1;\n           }\n           // still within the deadline\n           if (Time.now() \u003c restartDeadline) {\n             continue; // with in the deadline\n           }\n           // expired. declare the restarting node dead\n           restartDeadline \u003d 0;\n           int expiredNodeIndex \u003d restartingNodeIndex;\n           restartingNodeIndex \u003d -1;\n           DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n               nodes[expiredNodeIndex]);\n           // Mark the restarting node as failed. If there is any other failed\n           // node during the last pipeline construction attempt, it will not be\n           // overwritten/dropped. In this case, the restarting node will get\n           // excluded in the following attempt, if it still does not come up.\n           if (errorIndex \u003d\u003d -1) {\n             errorIndex \u003d expiredNodeIndex;\n           }\n           // From this point on, normal pipeline recovery applies.\n         }\n       } // while\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n         dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n             nodes, storageIDs);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        // Sleep before reconnect if a dn is restarting.\n        // This process will be repeated until the deadline or the datanode\n        // starts back up.\n        if (restartingNodeIndex \u003e\u003d 0) {\n          // 4 seconds or the configured deadline period, whichever is shorter.\n          // This is the retry interval and recovery will be retried in this\n          // interval until timeout or success.\n          long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n              4000L);\n          try {\n            Thread.sleep(delay);\n          } catch (InterruptedException ie) {\n            lastException.set(new IOException(\"Interrupted while waiting for \" +\n                \"datanode to restart. \" + nodes[restartingNodeIndex]));\n            streamerClosed \u003d true;\n            return false;\n          }\n        }\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\"));\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          arraycopy(nodes, newnodes, errorIndex);\n\n          final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n          arraycopy(storageTypes, newStorageTypes, errorIndex);\n\n          final String[] newStorageIDs \u003d new String[newnodes.length];\n          arraycopy(storageIDs, newStorageIDs, errorIndex);\n          \n          setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n          // Just took care of a node error while waiting for a node restart\n          if (restartingNodeIndex \u003e\u003d 0) {\n            // If the error came from a node further away than the restarting\n            // node, the restart must have been complete.\n            if (errorIndex \u003e restartingNodeIndex) {\n              restartingNodeIndex \u003d -1;\n            } else if (errorIndex \u003c restartingNodeIndex) {\n              // the node index has shifted.\n              restartingNodeIndex--;\n            } else {\n              // this shouldn\u0027t happen...\n              assert false;\n            }\n          }\n\n          if (restartingNodeIndex \u003d\u003d -1) {\n            hasError \u003d false;\n          }\n          lastException.set(null);\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        if (failPacket) { // for testing\n          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n          failPacket \u003d false;\n          try {\n            // Give DNs time to send in bad reports. In real situations,\n            // good reports should follow bad ones, if client committed\n            // with those nodes.\n            Thread.sleep(2000);\n          } catch (InterruptedException ie) {}\n        } else {\n          success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n        }\n\n        if (restartingNodeIndex \u003e\u003d 0) {\n          assert hasError \u003d\u003d true;\n          // check errorIndex set above\n          if (errorIndex \u003d\u003d restartingNodeIndex) {\n            // ignore, if came from the restarting node\n            errorIndex \u003d -1;\n          }\n          // still within the deadline\n          if (Time.now() \u003c restartDeadline) {\n            continue; // with in the deadline\n          }\n          // expired. declare the restarting node dead\n          restartDeadline \u003d 0;\n          int expiredNodeIndex \u003d restartingNodeIndex;\n          restartingNodeIndex \u003d -1;\n          DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n              nodes[expiredNodeIndex]);\n          // Mark the restarting node as failed. If there is any other failed\n          // node during the last pipeline construction attempt, it will not be\n          // overwritten/dropped. In this case, the restarting node will get\n          // excluded in the following attempt, if it still does not come up.\n          if (errorIndex \u003d\u003d -1) {\n            errorIndex \u003d expiredNodeIndex;\n          }\n          // From this point on, normal pipeline recovery applies.\n        }\n      } // while\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n            nodes, storageIDs);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "57b28693ee295746c6d168d37dd05eaf7b601b87": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5924. Utilize OOB upgrade message processing for writes. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1571792 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/02/14 11:24 AM",
      "commitName": "57b28693ee295746c6d168d37dd05eaf7b601b87",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "24/02/14 6:16 PM",
      "commitNameOld": "440c3cd1050f2a871a73d44406c0013b6ff73f2e",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.71,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,154 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n+        // Sleep before reconnect if a dn is restarting.\n+        // This process will be repeated until the deadline or the datanode\n+        // starts back up.\n+        if (restartingNodeIndex \u003e\u003d 0) {\n+          // 4 seconds or the configured deadline period, whichever is shorter.\n+          // This is the retry interval and recovery will be retried in this\n+          // interval until timeout or success.\n+          long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n+              4000L);\n+          try {\n+            Thread.sleep(delay);\n+          } catch (InterruptedException ie) {\n+            lastException.set(new IOException(\"Interrupted while waiting for \" +\n+                \"datanode to restart. \" + nodes[restartingNodeIndex]));\n+            streamerClosed \u003d true;\n+            return false;\n+          }\n+        }\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n             pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n             lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                 + \" are bad. Aborting...\"));\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n               \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n           System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n           System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n               newnodes.length-errorIndex);\n \n           final String[] newStorageIDs \u003d new String[newnodes.length];\n           System.arraycopy(storageIDs, 0, newStorageIDs, 0, errorIndex);\n           System.arraycopy(storageIDs, errorIndex+1, newStorageIDs, errorIndex,\n               newStorageIDs.length-errorIndex);\n           \n           setPipeline(newnodes, newStorageIDs);\n \n-          hasError \u003d false;\n+          // Just took care of a node error while waiting for a node restart\n+          if (restartingNodeIndex \u003e\u003d 0) {\n+            // If the error came from a node further away than the restarting\n+            // node, the restart must have been complete.\n+            if (errorIndex \u003e restartingNodeIndex) {\n+              restartingNodeIndex \u003d -1;\n+            } else if (errorIndex \u003c restartingNodeIndex) {\n+              // the node index has shifted.\n+              restartingNodeIndex--;\n+            } else {\n+              // this shouldn\u0027t happen...\n+              assert false;\n+            }\n+          }\n+\n+          if (restartingNodeIndex \u003d\u003d -1) {\n+            hasError \u003d false;\n+          }\n           lastException.set(null);\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n           addDatanode2ExistingPipeline();\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n         if (failPacket) { // for testing\n           success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n           failPacket \u003d false;\n           try {\n             // Give DNs time to send in bad reports. In real situations,\n             // good reports should follow bad ones, if client committed\n             // with those nodes.\n             Thread.sleep(2000);\n           } catch (InterruptedException ie) {}\n         } else {\n           success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n         }\n-      }\n+\n+        if (restartingNodeIndex \u003e\u003d 0) {\n+          assert hasError \u003d\u003d true;\n+          // check errorIndex set above\n+          if (errorIndex \u003d\u003d restartingNodeIndex) {\n+            // ignore, if came from the restarting node\n+            errorIndex \u003d -1;\n+          }\n+          // still within the deadline\n+          if (Time.now() \u003c restartDeadline) {\n+            continue; // with in the deadline\n+          }\n+          // expired. declare the restarting node dead\n+          restartDeadline \u003d 0;\n+          int expiredNodeIndex \u003d restartingNodeIndex;\n+          restartingNodeIndex \u003d -1;\n+          DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n+              nodes[expiredNodeIndex]);\n+          // Mark the restarting node as failed. If there is any other failed\n+          // node during the last pipeline construction attempt, it will not be\n+          // overwritten/dropped. In this case, the restarting node will get\n+          // excluded in the following attempt, if it still does not come up.\n+          if (errorIndex \u003d\u003d -1) {\n+            errorIndex \u003d expiredNodeIndex;\n+          }\n+          // From this point on, normal pipeline recovery applies.\n+        }\n+      } // while\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n         dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n             nodes, storageIDs);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        // Sleep before reconnect if a dn is restarting.\n        // This process will be repeated until the deadline or the datanode\n        // starts back up.\n        if (restartingNodeIndex \u003e\u003d 0) {\n          // 4 seconds or the configured deadline period, whichever is shorter.\n          // This is the retry interval and recovery will be retried in this\n          // interval until timeout or success.\n          long delay \u003d Math.min(dfsClient.getConf().datanodeRestartTimeout,\n              4000L);\n          try {\n            Thread.sleep(delay);\n          } catch (InterruptedException ie) {\n            lastException.set(new IOException(\"Interrupted while waiting for \" +\n                \"datanode to restart. \" + nodes[restartingNodeIndex]));\n            streamerClosed \u003d true;\n            return false;\n          }\n        }\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\"));\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n\n          final String[] newStorageIDs \u003d new String[newnodes.length];\n          System.arraycopy(storageIDs, 0, newStorageIDs, 0, errorIndex);\n          System.arraycopy(storageIDs, errorIndex+1, newStorageIDs, errorIndex,\n              newStorageIDs.length-errorIndex);\n          \n          setPipeline(newnodes, newStorageIDs);\n\n          // Just took care of a node error while waiting for a node restart\n          if (restartingNodeIndex \u003e\u003d 0) {\n            // If the error came from a node further away than the restarting\n            // node, the restart must have been complete.\n            if (errorIndex \u003e restartingNodeIndex) {\n              restartingNodeIndex \u003d -1;\n            } else if (errorIndex \u003c restartingNodeIndex) {\n              // the node index has shifted.\n              restartingNodeIndex--;\n            } else {\n              // this shouldn\u0027t happen...\n              assert false;\n            }\n          }\n\n          if (restartingNodeIndex \u003d\u003d -1) {\n            hasError \u003d false;\n          }\n          lastException.set(null);\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        if (failPacket) { // for testing\n          success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n          failPacket \u003d false;\n          try {\n            // Give DNs time to send in bad reports. In real situations,\n            // good reports should follow bad ones, if client committed\n            // with those nodes.\n            Thread.sleep(2000);\n          } catch (InterruptedException ie) {}\n        } else {\n          success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n        }\n\n        if (restartingNodeIndex \u003e\u003d 0) {\n          assert hasError \u003d\u003d true;\n          // check errorIndex set above\n          if (errorIndex \u003d\u003d restartingNodeIndex) {\n            // ignore, if came from the restarting node\n            errorIndex \u003d -1;\n          }\n          // still within the deadline\n          if (Time.now() \u003c restartDeadline) {\n            continue; // with in the deadline\n          }\n          // expired. declare the restarting node dead\n          restartDeadline \u003d 0;\n          int expiredNodeIndex \u003d restartingNodeIndex;\n          restartingNodeIndex \u003d -1;\n          DFSClient.LOG.warn(\"Datanode did not restart in time: \" +\n              nodes[expiredNodeIndex]);\n          // Mark the restarting node as failed. If there is any other failed\n          // node during the last pipeline construction attempt, it will not be\n          // overwritten/dropped. In this case, the restarting node will get\n          // excluded in the following attempt, if it still does not come up.\n          if (errorIndex \u003d\u003d -1) {\n            errorIndex \u003d expiredNodeIndex;\n          }\n          // From this point on, normal pipeline recovery applies.\n        }\n      } // while\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n            nodes, storageIDs);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "38a04a3042c5af455605bd3477358893700e2a9d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5557. Write pipeline recovery for the last packet in the block may cause rejection of valid replicas. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1547173 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/12/13 12:18 PM",
      "commitName": "38a04a3042c5af455605bd3477358893700e2a9d",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "20/11/13 6:43 AM",
      "commitNameOld": "f26d2adbf98890cfe350c17241f5049b89a11e3c",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 12.23,
      "commitsBetweenForRepo": 54,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,84 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n             pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n             lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                 + \" are bad. Aborting...\"));\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n               \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n           System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n           System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n               newnodes.length-errorIndex);\n           nodes \u003d newnodes;\n           hasError \u003d false;\n           lastException.set(null);\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n           addDatanode2ExistingPipeline();\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n         if (failPacket) { // for testing\n-          success \u003d createBlockOutputStream(nodes, newGS-1, isRecovery);\n+          success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n           failPacket \u003d false;\n           try {\n             // Give DNs time to send in bad reports. In real situations,\n             // good reports should follow bad ones, if client committed\n             // with those nodes.\n             Thread.sleep(2000);\n           } catch (InterruptedException ie) {}\n         } else {\n           success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n         }\n       }\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n         dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\"));\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n          nodes \u003d newnodes;\n          hasError \u003d false;\n          lastException.set(null);\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        if (failPacket) { // for testing\n          success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n          failPacket \u003d false;\n          try {\n            // Give DNs time to send in bad reports. In real situations,\n            // good reports should follow bad ones, if client committed\n            // with those nodes.\n            Thread.sleep(2000);\n          } catch (InterruptedException ie) {}\n        } else {\n          success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n        }\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "ceea91c9cd8b2a18be13217894ccf1c17198de18": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5438. Flaws in block report processing can cause data loss. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1542054 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/11/13 12:11 PM",
      "commitName": "ceea91c9cd8b2a18be13217894ccf1c17198de18",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "25/10/13 11:43 AM",
      "commitNameOld": "5829029154b8e8e02bc6aeb45435046ca080bbe9",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 20.06,
      "commitsBetweenForRepo": 102,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,84 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n             pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n             lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                 + \" are bad. Aborting...\"));\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n               \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n           System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n           System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n               newnodes.length-errorIndex);\n           nodes \u003d newnodes;\n           hasError \u003d false;\n           lastException.set(null);\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n           addDatanode2ExistingPipeline();\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n-        success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n+        if (failPacket) { // for testing\n+          success \u003d createBlockOutputStream(nodes, newGS-1, isRecovery);\n+          failPacket \u003d false;\n+          try {\n+            // Give DNs time to send in bad reports. In real situations,\n+            // good reports should follow bad ones, if client committed\n+            // with those nodes.\n+            Thread.sleep(2000);\n+          } catch (InterruptedException ie) {}\n+        } else {\n+          success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n+        }\n       }\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n         dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\"));\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n          nodes \u003d newnodes;\n          hasError \u003d false;\n          lastException.set(null);\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        if (failPacket) { // for testing\n          success \u003d createBlockOutputStream(nodes, newGS-1, isRecovery);\n          failPacket \u003d false;\n          try {\n            // Give DNs time to send in bad reports. In real situations,\n            // good reports should follow bad ones, if client committed\n            // with those nodes.\n            Thread.sleep(2000);\n          } catch (InterruptedException ie) {}\n        } else {\n          success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n        }\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5466. Update storage IDs when the pipeline is updated. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1539203 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/11/13 4:25 PM",
      "commitName": "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "28/10/13 10:29 AM",
      "commitNameOld": "dc0b44a884700cda3665aa04b16d1e3474328e05",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 8.29,
      "commitsBetweenForRepo": 40,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,81 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n             pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n             lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                 + \" are bad. Aborting...\"));\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n               \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n           System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n           System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n               newnodes.length-errorIndex);\n-          nodes \u003d newnodes;\n+\n+          final String[] newStorageIDs \u003d new String[newnodes.length];\n+          System.arraycopy(storageIDs, 0, newStorageIDs, 0, errorIndex);\n+          System.arraycopy(storageIDs, errorIndex+1, newStorageIDs, errorIndex,\n+              newStorageIDs.length-errorIndex);\n+          \n+          setPipeline(newnodes, newStorageIDs);\n+\n           hasError \u003d false;\n           lastException.set(null);\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n           addDatanode2ExistingPipeline();\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n         success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n       }\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n         dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n             nodes, storageIDs);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\"));\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n\n          final String[] newStorageIDs \u003d new String[newnodes.length];\n          System.arraycopy(storageIDs, 0, newStorageIDs, 0, errorIndex);\n          System.arraycopy(storageIDs, errorIndex+1, newStorageIDs, errorIndex,\n              newStorageIDs.length-errorIndex);\n          \n          setPipeline(newnodes, newStorageIDs);\n\n          hasError \u003d false;\n          lastException.set(null);\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n            nodes, storageIDs);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "f2f5cdb5554d294a29ebf465101c5607fd56e244": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5335. Hive query failed with possible race in dfs output stream. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1531152 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/10/13 4:58 PM",
      "commitName": "f2f5cdb5554d294a29ebf465101c5607fd56e244",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "22/07/13 11:15 AM",
      "commitNameOld": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 80.24,
      "commitsBetweenForRepo": 499,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,73 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n             pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n-            lastException \u003d new IOException(\"All datanodes \" + pipelineMsg\n-                + \" are bad. Aborting...\");\n+            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n+                + \" are bad. Aborting...\"));\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n               \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n           System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n           System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n               newnodes.length-errorIndex);\n           nodes \u003d newnodes;\n           hasError \u003d false;\n-          lastException \u003d null;\n+          lastException.set(null);\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n           addDatanode2ExistingPipeline();\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n         success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n       }\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n         dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException.set(new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\"));\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n          nodes \u003d newnodes;\n          hasError \u003d false;\n          lastException.set(null);\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5009. Include storage information in the LocatedBlock.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1519691 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/09/13 7:03 AM",
      "commitName": "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "22/07/13 11:15 AM",
      "commitNameOld": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 42.83,
      "commitsBetweenForRepo": 230,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,74 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n             pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n             lastException \u003d new IOException(\"All datanodes \" + pipelineMsg\n                 + \" are bad. Aborting...\");\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n               \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n           System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n           System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n               newnodes.length-errorIndex);\n           nodes \u003d newnodes;\n           hasError \u003d false;\n           lastException \u003d null;\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n           addDatanode2ExistingPipeline();\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n         success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n       }\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n-        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n+        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n+            nodes, storageIDs);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException \u003d new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\");\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n          nodes \u003d newnodes;\n          hasError \u003d false;\n          lastException \u003d null;\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,\n            nodes, storageIDs);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "01/03/12 3:51 PM",
      "commitNameOld": "b2f67b47044a5cbb0c3aaac83299afba541aa771",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 30.93,
      "commitsBetweenForRepo": 182,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,73 @@\n     private boolean setupPipelineForAppendOrRecovery() throws IOException {\n       // check number of datanodes\n       if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n         String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n             + src + \"\\\" - Aborting...\";\n         DFSClient.LOG.warn(msg);\n         setLastException(new IOException(msg));\n         streamerClosed \u003d true;\n         return false;\n       }\n       \n       boolean success \u003d false;\n       long newGS \u003d 0L;\n       while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n         boolean isRecovery \u003d hasError;\n         // remove bad datanode from list of datanodes.\n         // If errorIndex was not set (i.e. appends), then do not remove \n         // any datanodes\n         // \n         if (errorIndex \u003e\u003d 0) {\n           StringBuilder pipelineMsg \u003d new StringBuilder();\n           for (int j \u003d 0; j \u003c nodes.length; j++) {\n-            pipelineMsg.append(nodes[j].getName());\n+            pipelineMsg.append(nodes[j]);\n             if (j \u003c nodes.length - 1) {\n               pipelineMsg.append(\", \");\n             }\n           }\n           if (nodes.length \u003c\u003d 1) {\n             lastException \u003d new IOException(\"All datanodes \" + pipelineMsg\n                 + \" are bad. Aborting...\");\n             streamerClosed \u003d true;\n             return false;\n           }\n           DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n               \" in pipeline \" + pipelineMsg + \n-              \": bad datanode \" + nodes[errorIndex].getName());\n+              \": bad datanode \" + nodes[errorIndex]);\n           failed.add(nodes[errorIndex]);\n \n           DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n           System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n           System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n               newnodes.length-errorIndex);\n           nodes \u003d newnodes;\n           hasError \u003d false;\n           lastException \u003d null;\n           errorIndex \u003d -1;\n         }\n \n         // Check if replace-datanode policy is satisfied.\n         if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n             nodes, isAppend, isHflushed)) {\n           addDatanode2ExistingPipeline();\n         }\n \n         // get a new generation stamp and an access token\n         LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n         newGS \u003d lb.getBlock().getGenerationStamp();\n         accessToken \u003d lb.getBlockToken();\n         \n         // set up the pipeline again with the remaining nodes\n         success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n       }\n \n       if (success) {\n         // update pipeline at the namenode\n         ExtendedBlock newBlock \u003d new ExtendedBlock(\n             block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n         dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n         // update client side generation stamp\n         block \u003d newBlock;\n       }\n       return false; // do not sleep, continue processing\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j]);\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException \u003d new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\");\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex]);\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n          nodes \u003d newnodes;\n          hasError \u003d false;\n          lastException \u003d null;\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j].getName());\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException \u003d new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\");\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex].getName());\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n          nodes \u003d newnodes;\n          hasError \u003d false;\n          lastException \u003d null;\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j].getName());\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException \u003d new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\");\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex].getName());\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n          nodes \u003d newnodes;\n          hasError \u003d false;\n          lastException \u003d null;\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,73 @@\n+    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n+      // check number of datanodes\n+      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n+        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n+            + src + \"\\\" - Aborting...\";\n+        DFSClient.LOG.warn(msg);\n+        setLastException(new IOException(msg));\n+        streamerClosed \u003d true;\n+        return false;\n+      }\n+      \n+      boolean success \u003d false;\n+      long newGS \u003d 0L;\n+      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n+        boolean isRecovery \u003d hasError;\n+        // remove bad datanode from list of datanodes.\n+        // If errorIndex was not set (i.e. appends), then do not remove \n+        // any datanodes\n+        // \n+        if (errorIndex \u003e\u003d 0) {\n+          StringBuilder pipelineMsg \u003d new StringBuilder();\n+          for (int j \u003d 0; j \u003c nodes.length; j++) {\n+            pipelineMsg.append(nodes[j].getName());\n+            if (j \u003c nodes.length - 1) {\n+              pipelineMsg.append(\", \");\n+            }\n+          }\n+          if (nodes.length \u003c\u003d 1) {\n+            lastException \u003d new IOException(\"All datanodes \" + pipelineMsg\n+                + \" are bad. Aborting...\");\n+            streamerClosed \u003d true;\n+            return false;\n+          }\n+          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n+              \" in pipeline \" + pipelineMsg + \n+              \": bad datanode \" + nodes[errorIndex].getName());\n+          failed.add(nodes[errorIndex]);\n+\n+          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n+          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n+          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n+              newnodes.length-errorIndex);\n+          nodes \u003d newnodes;\n+          hasError \u003d false;\n+          lastException \u003d null;\n+          errorIndex \u003d -1;\n+        }\n+\n+        // Check if replace-datanode policy is satisfied.\n+        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n+            nodes, isAppend, isHflushed)) {\n+          addDatanode2ExistingPipeline();\n+        }\n+\n+        // get a new generation stamp and an access token\n+        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n+        newGS \u003d lb.getBlock().getGenerationStamp();\n+        accessToken \u003d lb.getBlockToken();\n+        \n+        // set up the pipeline again with the remaining nodes\n+        success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n+      }\n+\n+      if (success) {\n+        // update pipeline at the namenode\n+        ExtendedBlock newBlock \u003d new ExtendedBlock(\n+            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n+        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n+        // update client side generation stamp\n+        block \u003d newBlock;\n+      }\n+      return false; // do not sleep, continue processing\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean setupPipelineForAppendOrRecovery() throws IOException {\n      // check number of datanodes\n      if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n        String msg \u003d \"Could not get block locations. \" + \"Source file \\\"\"\n            + src + \"\\\" - Aborting...\";\n        DFSClient.LOG.warn(msg);\n        setLastException(new IOException(msg));\n        streamerClosed \u003d true;\n        return false;\n      }\n      \n      boolean success \u003d false;\n      long newGS \u003d 0L;\n      while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n        boolean isRecovery \u003d hasError;\n        // remove bad datanode from list of datanodes.\n        // If errorIndex was not set (i.e. appends), then do not remove \n        // any datanodes\n        // \n        if (errorIndex \u003e\u003d 0) {\n          StringBuilder pipelineMsg \u003d new StringBuilder();\n          for (int j \u003d 0; j \u003c nodes.length; j++) {\n            pipelineMsg.append(nodes[j].getName());\n            if (j \u003c nodes.length - 1) {\n              pipelineMsg.append(\", \");\n            }\n          }\n          if (nodes.length \u003c\u003d 1) {\n            lastException \u003d new IOException(\"All datanodes \" + pipelineMsg\n                + \" are bad. Aborting...\");\n            streamerClosed \u003d true;\n            return false;\n          }\n          DFSClient.LOG.warn(\"Error Recovery for block \" + block +\n              \" in pipeline \" + pipelineMsg + \n              \": bad datanode \" + nodes[errorIndex].getName());\n          failed.add(nodes[errorIndex]);\n\n          DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);\n          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,\n              newnodes.length-errorIndex);\n          nodes \u003d newnodes;\n          hasError \u003d false;\n          lastException \u003d null;\n          errorIndex \u003d -1;\n        }\n\n        // Check if replace-datanode policy is satisfied.\n        if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(blockReplication,\n            nodes, isAppend, isHflushed)) {\n          addDatanode2ExistingPipeline();\n        }\n\n        // get a new generation stamp and an access token\n        LocatedBlock lb \u003d dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);\n        newGS \u003d lb.getBlock().getGenerationStamp();\n        accessToken \u003d lb.getBlockToken();\n        \n        // set up the pipeline again with the remaining nodes\n        success \u003d createBlockOutputStream(nodes, newGS, isRecovery);\n      }\n\n      if (success) {\n        // update pipeline at the namenode\n        ExtendedBlock newBlock \u003d new ExtendedBlock(\n            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);\n        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock, nodes);\n        // update client side generation stamp\n        block \u003d newBlock;\n      }\n      return false; // do not sleep, continue processing\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}