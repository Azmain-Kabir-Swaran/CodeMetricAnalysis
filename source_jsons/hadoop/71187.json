{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "SimulatedDataNodes.java",
  "functionName": "run",
  "functionId": "run___args-String[]",
  "sourceFilePath": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/SimulatedDataNodes.java",
  "functionStartLine": 91,
  "functionEndLine": 183,
  "numCommitsSeen": 3,
  "timeTaken": 1231,
  "changeHistory": [
    "63c295e29840587eb6eb4a0fa258c55002e3229a",
    "fc0656dd300f037cb8f97a4c1fac4bf942af3d0a",
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91"
  ],
  "changeHistoryShort": {
    "63c295e29840587eb6eb4a0fa258c55002e3229a": "Ybodychange",
    "fc0656dd300f037cb8f97a4c1fac4bf942af3d0a": "Ybodychange",
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": "Yintroduced"
  },
  "changeHistoryDetails": {
    "63c295e29840587eb6eb4a0fa258c55002e3229a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14755. [Dynamometer] Enhance compatibility of Dynamometer with branch-2 builds. Contributed by Takanobu Asanuma.\n",
      "commitDate": "22/08/19 9:57 AM",
      "commitName": "63c295e29840587eb6eb4a0fa258c55002e3229a",
      "commitAuthor": "Erik Krogen",
      "commitDateOld": "11/07/19 8:29 AM",
      "commitNameOld": "fc0656dd300f037cb8f97a4c1fac4bf942af3d0a",
      "commitAuthorOld": "Erik Krogen",
      "daysBetweenCommits": 42.06,
      "commitsBetweenForRepo": 386,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,93 @@\n   public int run(String[] args) throws Exception {\n     if (args.length \u003c 2) {\n       printUsageExit(\"Not enough arguments\");\n     }\n     String bpid \u003d args[0];\n     List\u003cPath\u003e blockListFiles \u003d new ArrayList\u003c\u003e();\n     for (int i \u003d 1; i \u003c args.length; i++) {\n       blockListFiles.add(new Path(args[i]));\n     }\n \n     URI defaultFS \u003d FileSystem.getDefaultUri(getConf());\n     if (!HdfsConstants.HDFS_URI_SCHEME.equals(defaultFS.getScheme())) {\n       printUsageExit(\n           \"Must specify an HDFS-based default FS! Got \u003c\" + defaultFS + \"\u003e\");\n     }\n     String nameNodeAdr \u003d defaultFS.getAuthority();\n     if (nameNodeAdr \u003d\u003d null) {\n       printUsageExit(\"No NameNode address and port in config\");\n     }\n     System.out.println(\"DataNodes will connect to NameNode at \" + nameNodeAdr);\n \n-    System.setProperty(MiniDFSCluster.PROP_TEST_BUILD_DATA,\n-        DataNode.getStorageLocations(getConf()).get(0).getUri().getPath());\n+    String loc \u003d DataNode.getStorageLocations(getConf()).get(0).toString();\n+    loc \u003d loc.substring(loc.indexOf(\"]\") + 1); // delete storage type\n+    String path \u003d new URI(loc).getPath();\n+    System.setProperty(MiniDFSCluster.PROP_TEST_BUILD_DATA, path);\n     SimulatedFSDataset.setFactory(getConf());\n     getConf().setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY,\n         STORAGE_CAPACITY);\n \n     UserGroupInformation.setConfiguration(getConf());\n     MiniDFSCluster mc \u003d new MiniDFSCluster();\n     try {\n       mc.formatDataNodeDirs();\n     } catch (IOException e) {\n       System.out.println(\"Error formatting DataNode dirs: \" + e);\n       throw new RuntimeException(\"Error formatting DataNode dirs\", e);\n     }\n \n     try {\n       System.out.println(\"Found \" + blockListFiles.size()\n           + \" block listing files; launching DataNodes accordingly.\");\n       mc.startDataNodes(getConf(), blockListFiles.size(), null, false,\n           StartupOption.REGULAR, null, null, null, null, false, true, true,\n           null);\n       long startTime \u003d Time.monotonicNow();\n       System.out.println(\"Waiting for DataNodes to connect to NameNode and \"\n           + \"init storage directories.\");\n       Set\u003cDataNode\u003e datanodesWithoutFSDataset \u003d new HashSet\u003c\u003e(\n           mc.getDataNodes());\n       while (!datanodesWithoutFSDataset.isEmpty()) {\n         datanodesWithoutFSDataset\n             .removeIf((dn) -\u003e DataNodeTestUtils.getFSDataset(dn) !\u003d null);\n         Thread.sleep(100);\n       }\n       System.out.println(\"Waited \" + (Time.monotonicNow() - startTime)\n           + \" ms for DataNode FSDatasets to be ready\");\n \n       for (int dnIndex \u003d 0; dnIndex \u003c blockListFiles.size(); dnIndex++) {\n         Path blockListFile \u003d blockListFiles.get(dnIndex);\n         try (FSDataInputStream fsdis \u003d\n             blockListFile.getFileSystem(getConf()).open(blockListFile);\n             BufferedReader reader \u003d new BufferedReader(\n                 new InputStreamReader(fsdis, StandardCharsets.UTF_8))) {\n           List\u003cBlock\u003e blockList \u003d new ArrayList\u003c\u003e();\n           int cnt \u003d 0;\n           for (String line \u003d reader.readLine(); line !\u003d null; line \u003d reader\n               .readLine()) {\n             // Format of the listing files is blockID,blockGenStamp,blockSize\n             String[] blockInfo \u003d line.split(\",\");\n             blockList.add(new Block(Long.parseLong(blockInfo[0]),\n                 Long.parseLong(blockInfo[2]), Long.parseLong(blockInfo[1])));\n             cnt++;\n           }\n           try {\n             mc.injectBlocks(dnIndex, blockList, bpid);\n           } catch (IOException ioe) {\n             System.out.printf(\"Error injecting blocks into DataNode %d for \"\n                     + \"block pool %s: %s%n\", dnIndex, bpid,\n                 ExceptionUtils.getStackTrace(ioe));\n           }\n           System.out.printf(\n               \"Injected %d blocks into DataNode %d for block pool %s%n\",\n               cnt, dnIndex, bpid);\n         }\n       }\n \n     } catch (IOException e) {\n       System.out.println(\n           \"Error creating DataNodes: \" + ExceptionUtils.getStackTrace(e));\n       return 1;\n     }\n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int run(String[] args) throws Exception {\n    if (args.length \u003c 2) {\n      printUsageExit(\"Not enough arguments\");\n    }\n    String bpid \u003d args[0];\n    List\u003cPath\u003e blockListFiles \u003d new ArrayList\u003c\u003e();\n    for (int i \u003d 1; i \u003c args.length; i++) {\n      blockListFiles.add(new Path(args[i]));\n    }\n\n    URI defaultFS \u003d FileSystem.getDefaultUri(getConf());\n    if (!HdfsConstants.HDFS_URI_SCHEME.equals(defaultFS.getScheme())) {\n      printUsageExit(\n          \"Must specify an HDFS-based default FS! Got \u003c\" + defaultFS + \"\u003e\");\n    }\n    String nameNodeAdr \u003d defaultFS.getAuthority();\n    if (nameNodeAdr \u003d\u003d null) {\n      printUsageExit(\"No NameNode address and port in config\");\n    }\n    System.out.println(\"DataNodes will connect to NameNode at \" + nameNodeAdr);\n\n    String loc \u003d DataNode.getStorageLocations(getConf()).get(0).toString();\n    loc \u003d loc.substring(loc.indexOf(\"]\") + 1); // delete storage type\n    String path \u003d new URI(loc).getPath();\n    System.setProperty(MiniDFSCluster.PROP_TEST_BUILD_DATA, path);\n    SimulatedFSDataset.setFactory(getConf());\n    getConf().setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY,\n        STORAGE_CAPACITY);\n\n    UserGroupInformation.setConfiguration(getConf());\n    MiniDFSCluster mc \u003d new MiniDFSCluster();\n    try {\n      mc.formatDataNodeDirs();\n    } catch (IOException e) {\n      System.out.println(\"Error formatting DataNode dirs: \" + e);\n      throw new RuntimeException(\"Error formatting DataNode dirs\", e);\n    }\n\n    try {\n      System.out.println(\"Found \" + blockListFiles.size()\n          + \" block listing files; launching DataNodes accordingly.\");\n      mc.startDataNodes(getConf(), blockListFiles.size(), null, false,\n          StartupOption.REGULAR, null, null, null, null, false, true, true,\n          null);\n      long startTime \u003d Time.monotonicNow();\n      System.out.println(\"Waiting for DataNodes to connect to NameNode and \"\n          + \"init storage directories.\");\n      Set\u003cDataNode\u003e datanodesWithoutFSDataset \u003d new HashSet\u003c\u003e(\n          mc.getDataNodes());\n      while (!datanodesWithoutFSDataset.isEmpty()) {\n        datanodesWithoutFSDataset\n            .removeIf((dn) -\u003e DataNodeTestUtils.getFSDataset(dn) !\u003d null);\n        Thread.sleep(100);\n      }\n      System.out.println(\"Waited \" + (Time.monotonicNow() - startTime)\n          + \" ms for DataNode FSDatasets to be ready\");\n\n      for (int dnIndex \u003d 0; dnIndex \u003c blockListFiles.size(); dnIndex++) {\n        Path blockListFile \u003d blockListFiles.get(dnIndex);\n        try (FSDataInputStream fsdis \u003d\n            blockListFile.getFileSystem(getConf()).open(blockListFile);\n            BufferedReader reader \u003d new BufferedReader(\n                new InputStreamReader(fsdis, StandardCharsets.UTF_8))) {\n          List\u003cBlock\u003e blockList \u003d new ArrayList\u003c\u003e();\n          int cnt \u003d 0;\n          for (String line \u003d reader.readLine(); line !\u003d null; line \u003d reader\n              .readLine()) {\n            // Format of the listing files is blockID,blockGenStamp,blockSize\n            String[] blockInfo \u003d line.split(\",\");\n            blockList.add(new Block(Long.parseLong(blockInfo[0]),\n                Long.parseLong(blockInfo[2]), Long.parseLong(blockInfo[1])));\n            cnt++;\n          }\n          try {\n            mc.injectBlocks(dnIndex, blockList, bpid);\n          } catch (IOException ioe) {\n            System.out.printf(\"Error injecting blocks into DataNode %d for \"\n                    + \"block pool %s: %s%n\", dnIndex, bpid,\n                ExceptionUtils.getStackTrace(ioe));\n          }\n          System.out.printf(\n              \"Injected %d blocks into DataNode %d for block pool %s%n\",\n              cnt, dnIndex, bpid);\n        }\n      }\n\n    } catch (IOException e) {\n      System.out.println(\n          \"Error creating DataNodes: \" + ExceptionUtils.getStackTrace(e));\n      return 1;\n    }\n    return 0;\n  }",
      "path": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/SimulatedDataNodes.java",
      "extendedDetails": {}
    },
    "fc0656dd300f037cb8f97a4c1fac4bf942af3d0a": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16418. [Dynamometer] Fix checkstyle and findbugs warnings. Contributed by Erik Krogen.\n",
      "commitDate": "11/07/19 8:29 AM",
      "commitName": "fc0656dd300f037cb8f97a4c1fac4bf942af3d0a",
      "commitAuthor": "Erik Krogen",
      "commitDateOld": "25/06/19 8:07 AM",
      "commitNameOld": "ab0b180ddb5d0775a2452d5eeb7badd252aadb91",
      "commitAuthorOld": "Erik Krogen",
      "daysBetweenCommits": 16.02,
      "commitsBetweenForRepo": 74,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,91 @@\n   public int run(String[] args) throws Exception {\n     if (args.length \u003c 2) {\n       printUsageExit(\"Not enough arguments\");\n     }\n     String bpid \u003d args[0];\n     List\u003cPath\u003e blockListFiles \u003d new ArrayList\u003c\u003e();\n     for (int i \u003d 1; i \u003c args.length; i++) {\n       blockListFiles.add(new Path(args[i]));\n     }\n \n     URI defaultFS \u003d FileSystem.getDefaultUri(getConf());\n     if (!HdfsConstants.HDFS_URI_SCHEME.equals(defaultFS.getScheme())) {\n       printUsageExit(\n           \"Must specify an HDFS-based default FS! Got \u003c\" + defaultFS + \"\u003e\");\n     }\n     String nameNodeAdr \u003d defaultFS.getAuthority();\n     if (nameNodeAdr \u003d\u003d null) {\n       printUsageExit(\"No NameNode address and port in config\");\n     }\n     System.out.println(\"DataNodes will connect to NameNode at \" + nameNodeAdr);\n \n     System.setProperty(MiniDFSCluster.PROP_TEST_BUILD_DATA,\n         DataNode.getStorageLocations(getConf()).get(0).getUri().getPath());\n     SimulatedFSDataset.setFactory(getConf());\n     getConf().setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY,\n         STORAGE_CAPACITY);\n \n     UserGroupInformation.setConfiguration(getConf());\n     MiniDFSCluster mc \u003d new MiniDFSCluster();\n     try {\n       mc.formatDataNodeDirs();\n     } catch (IOException e) {\n       System.out.println(\"Error formatting DataNode dirs: \" + e);\n-      System.exit(1);\n+      throw new RuntimeException(\"Error formatting DataNode dirs\", e);\n     }\n \n     try {\n       System.out.println(\"Found \" + blockListFiles.size()\n           + \" block listing files; launching DataNodes accordingly.\");\n       mc.startDataNodes(getConf(), blockListFiles.size(), null, false,\n           StartupOption.REGULAR, null, null, null, null, false, true, true,\n           null);\n       long startTime \u003d Time.monotonicNow();\n       System.out.println(\"Waiting for DataNodes to connect to NameNode and \"\n           + \"init storage directories.\");\n       Set\u003cDataNode\u003e datanodesWithoutFSDataset \u003d new HashSet\u003c\u003e(\n           mc.getDataNodes());\n       while (!datanodesWithoutFSDataset.isEmpty()) {\n         datanodesWithoutFSDataset\n             .removeIf((dn) -\u003e DataNodeTestUtils.getFSDataset(dn) !\u003d null);\n         Thread.sleep(100);\n       }\n       System.out.println(\"Waited \" + (Time.monotonicNow() - startTime)\n           + \" ms for DataNode FSDatasets to be ready\");\n \n       for (int dnIndex \u003d 0; dnIndex \u003c blockListFiles.size(); dnIndex++) {\n         Path blockListFile \u003d blockListFiles.get(dnIndex);\n-        try (FSDataInputStream fsdis \u003d blockListFile.getFileSystem(getConf())\n-            .open(blockListFile)) {\n-          BufferedReader reader \u003d new BufferedReader(\n-              new InputStreamReader(fsdis));\n+        try (FSDataInputStream fsdis \u003d\n+            blockListFile.getFileSystem(getConf()).open(blockListFile);\n+            BufferedReader reader \u003d new BufferedReader(\n+                new InputStreamReader(fsdis, StandardCharsets.UTF_8))) {\n           List\u003cBlock\u003e blockList \u003d new ArrayList\u003c\u003e();\n           int cnt \u003d 0;\n           for (String line \u003d reader.readLine(); line !\u003d null; line \u003d reader\n               .readLine()) {\n             // Format of the listing files is blockID,blockGenStamp,blockSize\n             String[] blockInfo \u003d line.split(\",\");\n             blockList.add(new Block(Long.parseLong(blockInfo[0]),\n                 Long.parseLong(blockInfo[2]), Long.parseLong(blockInfo[1])));\n             cnt++;\n           }\n           try {\n             mc.injectBlocks(dnIndex, blockList, bpid);\n           } catch (IOException ioe) {\n             System.out.printf(\"Error injecting blocks into DataNode %d for \"\n                     + \"block pool %s: %s%n\", dnIndex, bpid,\n                 ExceptionUtils.getStackTrace(ioe));\n           }\n           System.out.printf(\n               \"Injected %d blocks into DataNode %d for block pool %s%n\",\n               cnt, dnIndex, bpid);\n         }\n       }\n \n     } catch (IOException e) {\n       System.out.println(\n           \"Error creating DataNodes: \" + ExceptionUtils.getStackTrace(e));\n       return 1;\n     }\n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int run(String[] args) throws Exception {\n    if (args.length \u003c 2) {\n      printUsageExit(\"Not enough arguments\");\n    }\n    String bpid \u003d args[0];\n    List\u003cPath\u003e blockListFiles \u003d new ArrayList\u003c\u003e();\n    for (int i \u003d 1; i \u003c args.length; i++) {\n      blockListFiles.add(new Path(args[i]));\n    }\n\n    URI defaultFS \u003d FileSystem.getDefaultUri(getConf());\n    if (!HdfsConstants.HDFS_URI_SCHEME.equals(defaultFS.getScheme())) {\n      printUsageExit(\n          \"Must specify an HDFS-based default FS! Got \u003c\" + defaultFS + \"\u003e\");\n    }\n    String nameNodeAdr \u003d defaultFS.getAuthority();\n    if (nameNodeAdr \u003d\u003d null) {\n      printUsageExit(\"No NameNode address and port in config\");\n    }\n    System.out.println(\"DataNodes will connect to NameNode at \" + nameNodeAdr);\n\n    System.setProperty(MiniDFSCluster.PROP_TEST_BUILD_DATA,\n        DataNode.getStorageLocations(getConf()).get(0).getUri().getPath());\n    SimulatedFSDataset.setFactory(getConf());\n    getConf().setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY,\n        STORAGE_CAPACITY);\n\n    UserGroupInformation.setConfiguration(getConf());\n    MiniDFSCluster mc \u003d new MiniDFSCluster();\n    try {\n      mc.formatDataNodeDirs();\n    } catch (IOException e) {\n      System.out.println(\"Error formatting DataNode dirs: \" + e);\n      throw new RuntimeException(\"Error formatting DataNode dirs\", e);\n    }\n\n    try {\n      System.out.println(\"Found \" + blockListFiles.size()\n          + \" block listing files; launching DataNodes accordingly.\");\n      mc.startDataNodes(getConf(), blockListFiles.size(), null, false,\n          StartupOption.REGULAR, null, null, null, null, false, true, true,\n          null);\n      long startTime \u003d Time.monotonicNow();\n      System.out.println(\"Waiting for DataNodes to connect to NameNode and \"\n          + \"init storage directories.\");\n      Set\u003cDataNode\u003e datanodesWithoutFSDataset \u003d new HashSet\u003c\u003e(\n          mc.getDataNodes());\n      while (!datanodesWithoutFSDataset.isEmpty()) {\n        datanodesWithoutFSDataset\n            .removeIf((dn) -\u003e DataNodeTestUtils.getFSDataset(dn) !\u003d null);\n        Thread.sleep(100);\n      }\n      System.out.println(\"Waited \" + (Time.monotonicNow() - startTime)\n          + \" ms for DataNode FSDatasets to be ready\");\n\n      for (int dnIndex \u003d 0; dnIndex \u003c blockListFiles.size(); dnIndex++) {\n        Path blockListFile \u003d blockListFiles.get(dnIndex);\n        try (FSDataInputStream fsdis \u003d\n            blockListFile.getFileSystem(getConf()).open(blockListFile);\n            BufferedReader reader \u003d new BufferedReader(\n                new InputStreamReader(fsdis, StandardCharsets.UTF_8))) {\n          List\u003cBlock\u003e blockList \u003d new ArrayList\u003c\u003e();\n          int cnt \u003d 0;\n          for (String line \u003d reader.readLine(); line !\u003d null; line \u003d reader\n              .readLine()) {\n            // Format of the listing files is blockID,blockGenStamp,blockSize\n            String[] blockInfo \u003d line.split(\",\");\n            blockList.add(new Block(Long.parseLong(blockInfo[0]),\n                Long.parseLong(blockInfo[2]), Long.parseLong(blockInfo[1])));\n            cnt++;\n          }\n          try {\n            mc.injectBlocks(dnIndex, blockList, bpid);\n          } catch (IOException ioe) {\n            System.out.printf(\"Error injecting blocks into DataNode %d for \"\n                    + \"block pool %s: %s%n\", dnIndex, bpid,\n                ExceptionUtils.getStackTrace(ioe));\n          }\n          System.out.printf(\n              \"Injected %d blocks into DataNode %d for block pool %s%n\",\n              cnt, dnIndex, bpid);\n        }\n      }\n\n    } catch (IOException e) {\n      System.out.println(\n          \"Error creating DataNodes: \" + ExceptionUtils.getStackTrace(e));\n      return 1;\n    }\n    return 0;\n  }",
      "path": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/SimulatedDataNodes.java",
      "extendedDetails": {}
    },
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-12345 Add Dynamometer to hadoop-tools, a tool for scale testing the HDFS NameNode with real metadata and workloads. Contributed by Erik Krogen.\n",
      "commitDate": "25/06/19 8:07 AM",
      "commitName": "ab0b180ddb5d0775a2452d5eeb7badd252aadb91",
      "commitAuthor": "Erik Krogen",
      "diff": "@@ -0,0 +1,91 @@\n+  public int run(String[] args) throws Exception {\n+    if (args.length \u003c 2) {\n+      printUsageExit(\"Not enough arguments\");\n+    }\n+    String bpid \u003d args[0];\n+    List\u003cPath\u003e blockListFiles \u003d new ArrayList\u003c\u003e();\n+    for (int i \u003d 1; i \u003c args.length; i++) {\n+      blockListFiles.add(new Path(args[i]));\n+    }\n+\n+    URI defaultFS \u003d FileSystem.getDefaultUri(getConf());\n+    if (!HdfsConstants.HDFS_URI_SCHEME.equals(defaultFS.getScheme())) {\n+      printUsageExit(\n+          \"Must specify an HDFS-based default FS! Got \u003c\" + defaultFS + \"\u003e\");\n+    }\n+    String nameNodeAdr \u003d defaultFS.getAuthority();\n+    if (nameNodeAdr \u003d\u003d null) {\n+      printUsageExit(\"No NameNode address and port in config\");\n+    }\n+    System.out.println(\"DataNodes will connect to NameNode at \" + nameNodeAdr);\n+\n+    System.setProperty(MiniDFSCluster.PROP_TEST_BUILD_DATA,\n+        DataNode.getStorageLocations(getConf()).get(0).getUri().getPath());\n+    SimulatedFSDataset.setFactory(getConf());\n+    getConf().setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY,\n+        STORAGE_CAPACITY);\n+\n+    UserGroupInformation.setConfiguration(getConf());\n+    MiniDFSCluster mc \u003d new MiniDFSCluster();\n+    try {\n+      mc.formatDataNodeDirs();\n+    } catch (IOException e) {\n+      System.out.println(\"Error formatting DataNode dirs: \" + e);\n+      System.exit(1);\n+    }\n+\n+    try {\n+      System.out.println(\"Found \" + blockListFiles.size()\n+          + \" block listing files; launching DataNodes accordingly.\");\n+      mc.startDataNodes(getConf(), blockListFiles.size(), null, false,\n+          StartupOption.REGULAR, null, null, null, null, false, true, true,\n+          null);\n+      long startTime \u003d Time.monotonicNow();\n+      System.out.println(\"Waiting for DataNodes to connect to NameNode and \"\n+          + \"init storage directories.\");\n+      Set\u003cDataNode\u003e datanodesWithoutFSDataset \u003d new HashSet\u003c\u003e(\n+          mc.getDataNodes());\n+      while (!datanodesWithoutFSDataset.isEmpty()) {\n+        datanodesWithoutFSDataset\n+            .removeIf((dn) -\u003e DataNodeTestUtils.getFSDataset(dn) !\u003d null);\n+        Thread.sleep(100);\n+      }\n+      System.out.println(\"Waited \" + (Time.monotonicNow() - startTime)\n+          + \" ms for DataNode FSDatasets to be ready\");\n+\n+      for (int dnIndex \u003d 0; dnIndex \u003c blockListFiles.size(); dnIndex++) {\n+        Path blockListFile \u003d blockListFiles.get(dnIndex);\n+        try (FSDataInputStream fsdis \u003d blockListFile.getFileSystem(getConf())\n+            .open(blockListFile)) {\n+          BufferedReader reader \u003d new BufferedReader(\n+              new InputStreamReader(fsdis));\n+          List\u003cBlock\u003e blockList \u003d new ArrayList\u003c\u003e();\n+          int cnt \u003d 0;\n+          for (String line \u003d reader.readLine(); line !\u003d null; line \u003d reader\n+              .readLine()) {\n+            // Format of the listing files is blockID,blockGenStamp,blockSize\n+            String[] blockInfo \u003d line.split(\",\");\n+            blockList.add(new Block(Long.parseLong(blockInfo[0]),\n+                Long.parseLong(blockInfo[2]), Long.parseLong(blockInfo[1])));\n+            cnt++;\n+          }\n+          try {\n+            mc.injectBlocks(dnIndex, blockList, bpid);\n+          } catch (IOException ioe) {\n+            System.out.printf(\"Error injecting blocks into DataNode %d for \"\n+                    + \"block pool %s: %s%n\", dnIndex, bpid,\n+                ExceptionUtils.getStackTrace(ioe));\n+          }\n+          System.out.printf(\n+              \"Injected %d blocks into DataNode %d for block pool %s%n\",\n+              cnt, dnIndex, bpid);\n+        }\n+      }\n+\n+    } catch (IOException e) {\n+      System.out.println(\n+          \"Error creating DataNodes: \" + ExceptionUtils.getStackTrace(e));\n+      return 1;\n+    }\n+    return 0;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public int run(String[] args) throws Exception {\n    if (args.length \u003c 2) {\n      printUsageExit(\"Not enough arguments\");\n    }\n    String bpid \u003d args[0];\n    List\u003cPath\u003e blockListFiles \u003d new ArrayList\u003c\u003e();\n    for (int i \u003d 1; i \u003c args.length; i++) {\n      blockListFiles.add(new Path(args[i]));\n    }\n\n    URI defaultFS \u003d FileSystem.getDefaultUri(getConf());\n    if (!HdfsConstants.HDFS_URI_SCHEME.equals(defaultFS.getScheme())) {\n      printUsageExit(\n          \"Must specify an HDFS-based default FS! Got \u003c\" + defaultFS + \"\u003e\");\n    }\n    String nameNodeAdr \u003d defaultFS.getAuthority();\n    if (nameNodeAdr \u003d\u003d null) {\n      printUsageExit(\"No NameNode address and port in config\");\n    }\n    System.out.println(\"DataNodes will connect to NameNode at \" + nameNodeAdr);\n\n    System.setProperty(MiniDFSCluster.PROP_TEST_BUILD_DATA,\n        DataNode.getStorageLocations(getConf()).get(0).getUri().getPath());\n    SimulatedFSDataset.setFactory(getConf());\n    getConf().setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY,\n        STORAGE_CAPACITY);\n\n    UserGroupInformation.setConfiguration(getConf());\n    MiniDFSCluster mc \u003d new MiniDFSCluster();\n    try {\n      mc.formatDataNodeDirs();\n    } catch (IOException e) {\n      System.out.println(\"Error formatting DataNode dirs: \" + e);\n      System.exit(1);\n    }\n\n    try {\n      System.out.println(\"Found \" + blockListFiles.size()\n          + \" block listing files; launching DataNodes accordingly.\");\n      mc.startDataNodes(getConf(), blockListFiles.size(), null, false,\n          StartupOption.REGULAR, null, null, null, null, false, true, true,\n          null);\n      long startTime \u003d Time.monotonicNow();\n      System.out.println(\"Waiting for DataNodes to connect to NameNode and \"\n          + \"init storage directories.\");\n      Set\u003cDataNode\u003e datanodesWithoutFSDataset \u003d new HashSet\u003c\u003e(\n          mc.getDataNodes());\n      while (!datanodesWithoutFSDataset.isEmpty()) {\n        datanodesWithoutFSDataset\n            .removeIf((dn) -\u003e DataNodeTestUtils.getFSDataset(dn) !\u003d null);\n        Thread.sleep(100);\n      }\n      System.out.println(\"Waited \" + (Time.monotonicNow() - startTime)\n          + \" ms for DataNode FSDatasets to be ready\");\n\n      for (int dnIndex \u003d 0; dnIndex \u003c blockListFiles.size(); dnIndex++) {\n        Path blockListFile \u003d blockListFiles.get(dnIndex);\n        try (FSDataInputStream fsdis \u003d blockListFile.getFileSystem(getConf())\n            .open(blockListFile)) {\n          BufferedReader reader \u003d new BufferedReader(\n              new InputStreamReader(fsdis));\n          List\u003cBlock\u003e blockList \u003d new ArrayList\u003c\u003e();\n          int cnt \u003d 0;\n          for (String line \u003d reader.readLine(); line !\u003d null; line \u003d reader\n              .readLine()) {\n            // Format of the listing files is blockID,blockGenStamp,blockSize\n            String[] blockInfo \u003d line.split(\",\");\n            blockList.add(new Block(Long.parseLong(blockInfo[0]),\n                Long.parseLong(blockInfo[2]), Long.parseLong(blockInfo[1])));\n            cnt++;\n          }\n          try {\n            mc.injectBlocks(dnIndex, blockList, bpid);\n          } catch (IOException ioe) {\n            System.out.printf(\"Error injecting blocks into DataNode %d for \"\n                    + \"block pool %s: %s%n\", dnIndex, bpid,\n                ExceptionUtils.getStackTrace(ioe));\n          }\n          System.out.printf(\n              \"Injected %d blocks into DataNode %d for block pool %s%n\",\n              cnt, dnIndex, bpid);\n        }\n      }\n\n    } catch (IOException e) {\n      System.out.println(\n          \"Error creating DataNodes: \" + ExceptionUtils.getStackTrace(e));\n      return 1;\n    }\n    return 0;\n  }",
      "path": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/SimulatedDataNodes.java"
    }
  }
}