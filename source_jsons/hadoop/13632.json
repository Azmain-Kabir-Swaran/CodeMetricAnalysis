{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeAdminBackoffMonitor.java",
  "functionName": "moveBlocksToPending",
  "functionId": "moveBlocksToPending",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java",
  "functionStartLine": 461,
  "functionEndLine": 535,
  "numCommitsSeen": 1,
  "timeTaken": 941,
  "changeHistory": [
    "c93cb6790e0f1c64efd03d859f907a0522010894"
  ],
  "changeHistoryShort": {
    "c93cb6790e0f1c64efd03d859f907a0522010894": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c93cb6790e0f1c64efd03d859f907a0522010894": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-14854. Create improved decommission monitor implementation. Contributed by Stephen O\u0027Donnell.\n\nReviewed-by: Inigo Goiri \u003cinigoiri@apache.org\u003e\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "10/12/19 5:16 PM",
      "commitName": "c93cb6790e0f1c64efd03d859f907a0522010894",
      "commitAuthor": "Stephen O\u0027Donnell",
      "diff": "@@ -0,0 +1,75 @@\n+  private void moveBlocksToPending() {\n+    int blocksProcessed \u003d 0;\n+    int pendingCount \u003d getPendingCount();\n+    int yetToBeProcessed \u003d getYetToBeProcessedCount();\n+\n+    if (pendingCount \u003d\u003d 0 \u0026\u0026 yetToBeProcessed \u003d\u003d 0) {\n+      // There are no blocks to process so just return\n+      LOG.debug(\"There are no pending or blocks yet to be processed\");\n+      return;\n+    }\n+\n+    namesystem.writeLock();\n+    try {\n+      long repQueueSize \u003d blockManager.getLowRedundancyBlocksCount();\n+\n+      LOG.info(\"There are {} blocks pending replication and the limit is \"+\n+          \"{}. A further {} blocks are waiting to be processed. \"+\n+          \"The replication queue currently has {} blocks\",\n+          pendingCount, pendingRepLimit, yetToBeProcessed, repQueueSize);\n+\n+      if (pendingCount \u003e\u003d pendingRepLimit) {\n+        // Only add more blocks to the replication queue if we don\u0027t already\n+        // have too many pending\n+        return;\n+      }\n+\n+      // Create a \"Block Iterator\" for each node decommissioning or entering\n+      // maintenance. These iterators will be used \"round robined\" to add blocks\n+      // to the replication queue and PendingRep\n+      HashMap\u003cDatanodeDescriptor, Iterator\u003cBlockInfo\u003e\u003e\n+          iterators \u003d new HashMap\u003c\u003e();\n+      for (Map.Entry\u003cDatanodeDescriptor, HashMap\u003cBlockInfo, Integer\u003e\u003e e\n+          : outOfServiceNodeBlocks.entrySet()) {\n+        iterators.put(e.getKey(), e.getValue().keySet().iterator());\n+      }\n+\n+      // Now loop until we fill the pendingRep map with pendingRepLimit blocks\n+      // or run out of blocks to add.\n+      Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d\n+          Iterables.cycle(iterators.keySet()).iterator();\n+      while (nodeIter.hasNext()) {\n+        // Cycle through each node with blocks which still need processed\n+        DatanodeDescriptor dn \u003d nodeIter.next();\n+        Iterator\u003cBlockInfo\u003e blockIt \u003d iterators.get(dn);\n+        while (blockIt.hasNext()) {\n+          // Process the blocks for the node until we find one that needs\n+          // replication\n+          if (blocksProcessed \u003e\u003d blocksPerLock) {\n+            blocksProcessed \u003d 0;\n+            namesystem.writeUnlock();\n+            namesystem.writeLock();\n+          }\n+          blocksProcessed++;\n+          if (nextBlockAddedToPending(blockIt, dn)) {\n+            // Exit the inner \"block\" loop so an iterator for the next datanode\n+            // is used for the next block.\n+            pendingCount++;\n+            break;\n+          }\n+        }\n+        if (!blockIt.hasNext()) {\n+          // remove the iterator as there are no blocks left in it\n+          nodeIter.remove();\n+        }\n+        if (pendingCount \u003e\u003d pendingRepLimit) {\n+          // We have scheduled the limit of blocks for replication, so do\n+          // not add any more\n+          break;\n+        }\n+      }\n+    } finally {\n+      namesystem.writeUnlock();\n+    }\n+    LOG.debug(\"{} blocks are now pending replication\", pendingCount);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void moveBlocksToPending() {\n    int blocksProcessed \u003d 0;\n    int pendingCount \u003d getPendingCount();\n    int yetToBeProcessed \u003d getYetToBeProcessedCount();\n\n    if (pendingCount \u003d\u003d 0 \u0026\u0026 yetToBeProcessed \u003d\u003d 0) {\n      // There are no blocks to process so just return\n      LOG.debug(\"There are no pending or blocks yet to be processed\");\n      return;\n    }\n\n    namesystem.writeLock();\n    try {\n      long repQueueSize \u003d blockManager.getLowRedundancyBlocksCount();\n\n      LOG.info(\"There are {} blocks pending replication and the limit is \"+\n          \"{}. A further {} blocks are waiting to be processed. \"+\n          \"The replication queue currently has {} blocks\",\n          pendingCount, pendingRepLimit, yetToBeProcessed, repQueueSize);\n\n      if (pendingCount \u003e\u003d pendingRepLimit) {\n        // Only add more blocks to the replication queue if we don\u0027t already\n        // have too many pending\n        return;\n      }\n\n      // Create a \"Block Iterator\" for each node decommissioning or entering\n      // maintenance. These iterators will be used \"round robined\" to add blocks\n      // to the replication queue and PendingRep\n      HashMap\u003cDatanodeDescriptor, Iterator\u003cBlockInfo\u003e\u003e\n          iterators \u003d new HashMap\u003c\u003e();\n      for (Map.Entry\u003cDatanodeDescriptor, HashMap\u003cBlockInfo, Integer\u003e\u003e e\n          : outOfServiceNodeBlocks.entrySet()) {\n        iterators.put(e.getKey(), e.getValue().keySet().iterator());\n      }\n\n      // Now loop until we fill the pendingRep map with pendingRepLimit blocks\n      // or run out of blocks to add.\n      Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d\n          Iterables.cycle(iterators.keySet()).iterator();\n      while (nodeIter.hasNext()) {\n        // Cycle through each node with blocks which still need processed\n        DatanodeDescriptor dn \u003d nodeIter.next();\n        Iterator\u003cBlockInfo\u003e blockIt \u003d iterators.get(dn);\n        while (blockIt.hasNext()) {\n          // Process the blocks for the node until we find one that needs\n          // replication\n          if (blocksProcessed \u003e\u003d blocksPerLock) {\n            blocksProcessed \u003d 0;\n            namesystem.writeUnlock();\n            namesystem.writeLock();\n          }\n          blocksProcessed++;\n          if (nextBlockAddedToPending(blockIt, dn)) {\n            // Exit the inner \"block\" loop so an iterator for the next datanode\n            // is used for the next block.\n            pendingCount++;\n            break;\n          }\n        }\n        if (!blockIt.hasNext()) {\n          // remove the iterator as there are no blocks left in it\n          nodeIter.remove();\n        }\n        if (pendingCount \u003e\u003d pendingRepLimit) {\n          // We have scheduled the limit of blocks for replication, so do\n          // not add any more\n          break;\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n    LOG.debug(\"{} blocks are now pending replication\", pendingCount);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java"
    }
  }
}