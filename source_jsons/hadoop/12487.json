{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Dispatcher.java",
  "functionName": "getBlockList",
  "functionId": "getBlockList",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
  "functionStartLine": 797,
  "functionEndLine": 854,
  "numCommitsSeen": 110,
  "timeTaken": 9834,
  "changeHistory": [
    "88fba00caa8c8e26f70deb9be5b534e7482620a1",
    "c09dc258a8f64fab852bf6f26187163480dbee3c",
    "2bc0a4f299fbd8035e29f62ce9cd22e209a62805",
    "b56daff6a186599764b046248565918b894ec116",
    "673280df24f0228bf01777035ceeab8807da8c40",
    "e60673697d5046c29c52bbabdfe80506f99773e4",
    "c3cf331dc91e2beef2afeed11105084843b02858",
    "83b9933db3349e6a6faf23bce35c9d4ce3f7bcf2",
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
    "c802ca28fd2c9fd35a662b8dd6b675e2aaa515d1",
    "907fb15ee8c150e5ecc0560b7374441c57a84122",
    "7428aeca8666aeaf5f6682efbdb5349f44d1753e",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "88fba00caa8c8e26f70deb9be5b534e7482620a1": "Ybodychange",
    "c09dc258a8f64fab852bf6f26187163480dbee3c": "Ybodychange",
    "2bc0a4f299fbd8035e29f62ce9cd22e209a62805": "Ybodychange",
    "b56daff6a186599764b046248565918b894ec116": "Ybodychange",
    "673280df24f0228bf01777035ceeab8807da8c40": "Ybodychange",
    "e60673697d5046c29c52bbabdfe80506f99773e4": "Ybodychange",
    "c3cf331dc91e2beef2afeed11105084843b02858": "Ymultichange(Ymovefromfile,Ybodychange)",
    "83b9933db3349e6a6faf23bce35c9d4ce3f7bcf2": "Ybodychange",
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d": "Ybodychange",
    "c802ca28fd2c9fd35a662b8dd6b675e2aaa515d1": "Ybodychange",
    "907fb15ee8c150e5ecc0560b7374441c57a84122": "Ybodychange",
    "7428aeca8666aeaf5f6682efbdb5349f44d1753e": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "88fba00caa8c8e26f70deb9be5b534e7482620a1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13222. Update getBlocks method to take minBlockSize in RPC calls.  Contributed by Bharat Viswanadham\n",
      "commitDate": "07/03/18 11:27 AM",
      "commitName": "88fba00caa8c8e26f70deb9be5b534e7482620a1",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "21/07/17 7:14 AM",
      "commitNameOld": "8e3a992eccff26a7344c3f0e719898fa97706b8c",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 229.22,
      "commitsBetweenForRepo": 1696,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,58 @@\n     private long getBlockList() throws IOException {\n       final long size \u003d Math.min(getBlocksSize, blocksToReceive);\n       final BlocksWithLocations newBlksLocs \u003d\n-          nnc.getBlocks(getDatanodeInfo(), size);\n+          nnc.getBlocks(getDatanodeInfo(), size, getBlocksMinBlockSize);\n \n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\"getBlocks(\" + getDatanodeInfo() + \", \"\n             + StringUtils.TraditionalBinaryPrefix.long2String(size, \"B\", 2)\n             + \") returns \" + newBlksLocs.getBlocks().length + \" blocks.\");\n       }\n \n       long bytesReceived \u003d 0;\n       for (BlockWithLocations blkLocs : newBlksLocs.getBlocks()) {\n         // Skip small blocks.\n         if (blkLocs.getBlock().getNumBytes() \u003c getBlocksMinBlockSize) {\n           continue;\n         }\n \n         DBlock block;\n         if (blkLocs instanceof StripedBlockWithLocations) {\n           StripedBlockWithLocations sblkLocs \u003d\n               (StripedBlockWithLocations) blkLocs;\n           // approximate size\n           bytesReceived +\u003d sblkLocs.getBlock().getNumBytes() /\n               sblkLocs.getDataBlockNum();\n           block \u003d new DBlockStriped(sblkLocs.getBlock(), sblkLocs.getIndices(),\n               sblkLocs.getDataBlockNum(), sblkLocs.getCellSize());\n         } else {\n           bytesReceived +\u003d blkLocs.getBlock().getNumBytes();\n           block \u003d new DBlock(blkLocs.getBlock());\n         }\n \n         synchronized (globalBlocks) {\n           block \u003d globalBlocks.putIfAbsent(blkLocs.getBlock(), block);\n           synchronized (block) {\n             block.clearLocations();\n \n             // update locations\n             final String[] datanodeUuids \u003d blkLocs.getDatanodeUuids();\n             final StorageType[] storageTypes \u003d blkLocs.getStorageTypes();\n             for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n               final StorageGroup g \u003d storageGroupMap.get(\n                   datanodeUuids[i], storageTypes[i]);\n               if (g !\u003d null) { // not unknown\n                 block.addLocation(g);\n               }\n             }\n           }\n           if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             if (LOG.isTraceEnabled()) {\n               LOG.trace(\"Add \" + block + \" to \" + this);\n             }\n             srcBlocks.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      final long size \u003d Math.min(getBlocksSize, blocksToReceive);\n      final BlocksWithLocations newBlksLocs \u003d\n          nnc.getBlocks(getDatanodeInfo(), size, getBlocksMinBlockSize);\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"getBlocks(\" + getDatanodeInfo() + \", \"\n            + StringUtils.TraditionalBinaryPrefix.long2String(size, \"B\", 2)\n            + \") returns \" + newBlksLocs.getBlocks().length + \" blocks.\");\n      }\n\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blkLocs : newBlksLocs.getBlocks()) {\n        // Skip small blocks.\n        if (blkLocs.getBlock().getNumBytes() \u003c getBlocksMinBlockSize) {\n          continue;\n        }\n\n        DBlock block;\n        if (blkLocs instanceof StripedBlockWithLocations) {\n          StripedBlockWithLocations sblkLocs \u003d\n              (StripedBlockWithLocations) blkLocs;\n          // approximate size\n          bytesReceived +\u003d sblkLocs.getBlock().getNumBytes() /\n              sblkLocs.getDataBlockNum();\n          block \u003d new DBlockStriped(sblkLocs.getBlock(), sblkLocs.getIndices(),\n              sblkLocs.getDataBlockNum(), sblkLocs.getCellSize());\n        } else {\n          bytesReceived +\u003d blkLocs.getBlock().getNumBytes();\n          block \u003d new DBlock(blkLocs.getBlock());\n        }\n\n        synchronized (globalBlocks) {\n          block \u003d globalBlocks.putIfAbsent(blkLocs.getBlock(), block);\n          synchronized (block) {\n            block.clearLocations();\n\n            // update locations\n            final String[] datanodeUuids \u003d blkLocs.getDatanodeUuids();\n            final StorageType[] storageTypes \u003d blkLocs.getStorageTypes();\n            for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n              final StorageGroup g \u003d storageGroupMap.get(\n                  datanodeUuids[i], storageTypes[i]);\n              if (g !\u003d null) { // not unknown\n                block.addLocation(g);\n              }\n            }\n          }\n          if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Add \" + block + \" to \" + this);\n            }\n            srcBlocks.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "c09dc258a8f64fab852bf6f26187163480dbee3c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8882. Erasure Coding: Use datablocks, parityblocks and cell size from ErasureCodingPolicy. Contributed by Vinayakumar B.\n\nChange-Id: Ic56da0b426f47c63dac440aef6f5fc8554f6cf13\n",
      "commitDate": "23/09/15 1:34 PM",
      "commitName": "c09dc258a8f64fab852bf6f26187163480dbee3c",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "01/09/15 2:30 PM",
      "commitNameOld": "ab56fcdb1219d03713b408dd3a95d7405635254d",
      "commitAuthorOld": "",
      "daysBetweenCommits": 21.96,
      "commitsBetweenForRepo": 141,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,58 @@\n     private long getBlockList() throws IOException {\n       final long size \u003d Math.min(getBlocksSize, blocksToReceive);\n       final BlocksWithLocations newBlksLocs \u003d\n           nnc.getBlocks(getDatanodeInfo(), size);\n \n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\"getBlocks(\" + getDatanodeInfo() + \", \"\n             + StringUtils.TraditionalBinaryPrefix.long2String(size, \"B\", 2)\n             + \") returns \" + newBlksLocs.getBlocks().length + \" blocks.\");\n       }\n \n       long bytesReceived \u003d 0;\n       for (BlockWithLocations blkLocs : newBlksLocs.getBlocks()) {\n         // Skip small blocks.\n         if (blkLocs.getBlock().getNumBytes() \u003c getBlocksMinBlockSize) {\n           continue;\n         }\n \n         DBlock block;\n         if (blkLocs instanceof StripedBlockWithLocations) {\n           StripedBlockWithLocations sblkLocs \u003d\n               (StripedBlockWithLocations) blkLocs;\n           // approximate size\n           bytesReceived +\u003d sblkLocs.getBlock().getNumBytes() /\n               sblkLocs.getDataBlockNum();\n           block \u003d new DBlockStriped(sblkLocs.getBlock(), sblkLocs.getIndices(),\n-              sblkLocs.getDataBlockNum());\n-        } else{\n+              sblkLocs.getDataBlockNum(), sblkLocs.getCellSize());\n+        } else {\n           bytesReceived +\u003d blkLocs.getBlock().getNumBytes();\n           block \u003d new DBlock(blkLocs.getBlock());\n         }\n \n         synchronized (globalBlocks) {\n           block \u003d globalBlocks.putIfAbsent(blkLocs.getBlock(), block);\n           synchronized (block) {\n             block.clearLocations();\n \n             // update locations\n             final String[] datanodeUuids \u003d blkLocs.getDatanodeUuids();\n             final StorageType[] storageTypes \u003d blkLocs.getStorageTypes();\n             for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n               final StorageGroup g \u003d storageGroupMap.get(\n                   datanodeUuids[i], storageTypes[i]);\n               if (g !\u003d null) { // not unknown\n                 block.addLocation(g);\n               }\n             }\n           }\n           if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             if (LOG.isTraceEnabled()) {\n               LOG.trace(\"Add \" + block + \" to \" + this);\n             }\n             srcBlocks.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      final long size \u003d Math.min(getBlocksSize, blocksToReceive);\n      final BlocksWithLocations newBlksLocs \u003d\n          nnc.getBlocks(getDatanodeInfo(), size);\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"getBlocks(\" + getDatanodeInfo() + \", \"\n            + StringUtils.TraditionalBinaryPrefix.long2String(size, \"B\", 2)\n            + \") returns \" + newBlksLocs.getBlocks().length + \" blocks.\");\n      }\n\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blkLocs : newBlksLocs.getBlocks()) {\n        // Skip small blocks.\n        if (blkLocs.getBlock().getNumBytes() \u003c getBlocksMinBlockSize) {\n          continue;\n        }\n\n        DBlock block;\n        if (blkLocs instanceof StripedBlockWithLocations) {\n          StripedBlockWithLocations sblkLocs \u003d\n              (StripedBlockWithLocations) blkLocs;\n          // approximate size\n          bytesReceived +\u003d sblkLocs.getBlock().getNumBytes() /\n              sblkLocs.getDataBlockNum();\n          block \u003d new DBlockStriped(sblkLocs.getBlock(), sblkLocs.getIndices(),\n              sblkLocs.getDataBlockNum(), sblkLocs.getCellSize());\n        } else {\n          bytesReceived +\u003d blkLocs.getBlock().getNumBytes();\n          block \u003d new DBlock(blkLocs.getBlock());\n        }\n\n        synchronized (globalBlocks) {\n          block \u003d globalBlocks.putIfAbsent(blkLocs.getBlock(), block);\n          synchronized (block) {\n            block.clearLocations();\n\n            // update locations\n            final String[] datanodeUuids \u003d blkLocs.getDatanodeUuids();\n            final StorageType[] storageTypes \u003d blkLocs.getStorageTypes();\n            for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n              final StorageGroup g \u003d storageGroupMap.get(\n                  datanodeUuids[i], storageTypes[i]);\n              if (g !\u003d null) { // not unknown\n                block.addLocation(g);\n              }\n            }\n          }\n          if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Add \" + block + \" to \" + this);\n            }\n            srcBlocks.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "2bc0a4f299fbd8035e29f62ce9cd22e209a62805": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8824. Do not use small blocks for balancing the cluster.\n",
      "commitDate": "14/08/15 1:03 PM",
      "commitName": "2bc0a4f299fbd8035e29f62ce9cd22e209a62805",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "10/08/15 4:52 PM",
      "commitNameOld": "b56daff6a186599764b046248565918b894ec116",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 3.84,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,44 @@\n     private long getBlockList() throws IOException {\n-      final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n+      final long size \u003d Math.min(getBlocksSize, blocksToReceive);\n       final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanodeInfo(), size);\n+\n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\"getBlocks(\" + getDatanodeInfo() + \", \"\n             + StringUtils.TraditionalBinaryPrefix.long2String(size, \"B\", 2)\n             + \") returns \" + newBlocks.getBlocks().length + \" blocks.\");\n       }\n \n       long bytesReceived \u003d 0;\n       for (BlockWithLocations blk : newBlocks.getBlocks()) {\n+        // Skip small blocks.\n+        if (blk.getBlock().getNumBytes() \u003c getBlocksMinBlockSize) {\n+          continue;\n+        }\n+\n         bytesReceived +\u003d blk.getBlock().getNumBytes();\n         synchronized (globalBlocks) {\n           final DBlock block \u003d globalBlocks.get(blk.getBlock());\n           synchronized (block) {\n             block.clearLocations();\n \n             // update locations\n             final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n             final StorageType[] storageTypes \u003d blk.getStorageTypes();\n             for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n               final StorageGroup g \u003d storageGroupMap.get(\n                   datanodeUuids[i], storageTypes[i]);\n               if (g !\u003d null) { // not unknown\n                 block.addLocation(g);\n               }\n             }\n           }\n           if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             if (LOG.isTraceEnabled()) {\n               LOG.trace(\"Add \" + block + \" to \" + this);\n             }\n             srcBlocks.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      final long size \u003d Math.min(getBlocksSize, blocksToReceive);\n      final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanodeInfo(), size);\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"getBlocks(\" + getDatanodeInfo() + \", \"\n            + StringUtils.TraditionalBinaryPrefix.long2String(size, \"B\", 2)\n            + \") returns \" + newBlocks.getBlocks().length + \" blocks.\");\n      }\n\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks.getBlocks()) {\n        // Skip small blocks.\n        if (blk.getBlock().getNumBytes() \u003c getBlocksMinBlockSize) {\n          continue;\n        }\n\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        synchronized (globalBlocks) {\n          final DBlock block \u003d globalBlocks.get(blk.getBlock());\n          synchronized (block) {\n            block.clearLocations();\n\n            // update locations\n            final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n            final StorageType[] storageTypes \u003d blk.getStorageTypes();\n            for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n              final StorageGroup g \u003d storageGroupMap.get(\n                  datanodeUuids[i], storageTypes[i]);\n              if (g !\u003d null) { // not unknown\n                block.addLocation(g);\n              }\n            }\n          }\n          if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Add \" + block + \" to \" + this);\n            }\n            srcBlocks.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "b56daff6a186599764b046248565918b894ec116": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8818. Changes the global moveExecutor to per datanode executors and changes MAX_SIZE_TO_MOVE to be configurable.\n",
      "commitDate": "10/08/15 4:52 PM",
      "commitName": "b56daff6a186599764b046248565918b894ec116",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "13/07/15 3:12 PM",
      "commitNameOld": "9ef03a4c5bb5573eadc7d04e371c4af2dc6bae37",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 28.07,
      "commitsBetweenForRepo": 150,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,38 @@\n     private long getBlockList() throws IOException {\n       final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n       final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanodeInfo(), size);\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(\"getBlocks(\" + getDatanodeInfo() + \", \"\n+            + StringUtils.TraditionalBinaryPrefix.long2String(size, \"B\", 2)\n+            + \") returns \" + newBlocks.getBlocks().length + \" blocks.\");\n+      }\n \n       long bytesReceived \u003d 0;\n       for (BlockWithLocations blk : newBlocks.getBlocks()) {\n         bytesReceived +\u003d blk.getBlock().getNumBytes();\n         synchronized (globalBlocks) {\n           final DBlock block \u003d globalBlocks.get(blk.getBlock());\n           synchronized (block) {\n             block.clearLocations();\n \n             // update locations\n             final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n             final StorageType[] storageTypes \u003d blk.getStorageTypes();\n             for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n               final StorageGroup g \u003d storageGroupMap.get(\n                   datanodeUuids[i], storageTypes[i]);\n               if (g !\u003d null) { // not unknown\n                 block.addLocation(g);\n               }\n             }\n           }\n           if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n-            // filter bad candidates\n+            if (LOG.isTraceEnabled()) {\n+              LOG.trace(\"Add \" + block + \" to \" + this);\n+            }\n             srcBlocks.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n      final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanodeInfo(), size);\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"getBlocks(\" + getDatanodeInfo() + \", \"\n            + StringUtils.TraditionalBinaryPrefix.long2String(size, \"B\", 2)\n            + \") returns \" + newBlocks.getBlocks().length + \" blocks.\");\n      }\n\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks.getBlocks()) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        synchronized (globalBlocks) {\n          final DBlock block \u003d globalBlocks.get(blk.getBlock());\n          synchronized (block) {\n            block.clearLocations();\n\n            // update locations\n            final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n            final StorageType[] storageTypes \u003d blk.getStorageTypes();\n            for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n              final StorageGroup g \u003d storageGroupMap.get(\n                  datanodeUuids[i], storageTypes[i]);\n              if (g !\u003d null) { // not unknown\n                block.addLocation(g);\n              }\n            }\n          }\n          if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Add \" + block + \" to \" + this);\n            }\n            srcBlocks.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "673280df24f0228bf01777035ceeab8807da8c40": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7621. Erasure Coding: update the Balancer/Mover data migration logic. Contributed by Walter Su.\n",
      "commitDate": "03/06/15 11:51 AM",
      "commitName": "673280df24f0228bf01777035ceeab8807da8c40",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "05/05/15 3:41 PM",
      "commitNameOld": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 28.84,
      "commitsBetweenForRepo": 380,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,46 @@\n     private long getBlockList() throws IOException {\n       final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n-      final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanodeInfo(), size);\n+      final BlocksWithLocations newBlksLocs \u003d\n+          nnc.getBlocks(getDatanodeInfo(), size);\n \n       long bytesReceived \u003d 0;\n-      for (BlockWithLocations blk : newBlocks.getBlocks()) {\n-        bytesReceived +\u003d blk.getBlock().getNumBytes();\n+      for (BlockWithLocations blkLocs : newBlksLocs.getBlocks()) {\n+\n+        DBlock block;\n+        if (blkLocs instanceof StripedBlockWithLocations) {\n+          StripedBlockWithLocations sblkLocs \u003d\n+              (StripedBlockWithLocations) blkLocs;\n+          // approximate size\n+          bytesReceived +\u003d sblkLocs.getBlock().getNumBytes() /\n+              sblkLocs.getDataBlockNum();\n+          block \u003d new DBlockStriped(sblkLocs.getBlock(), sblkLocs.getIndices(),\n+              sblkLocs.getDataBlockNum());\n+        } else{\n+          bytesReceived +\u003d blkLocs.getBlock().getNumBytes();\n+          block \u003d new DBlock(blkLocs.getBlock());\n+        }\n+\n         synchronized (globalBlocks) {\n-          final DBlock block \u003d globalBlocks.get(blk.getBlock());\n+          block \u003d globalBlocks.putIfAbsent(blkLocs.getBlock(), block);\n           synchronized (block) {\n             block.clearLocations();\n \n             // update locations\n-            final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n-            final StorageType[] storageTypes \u003d blk.getStorageTypes();\n+            final String[] datanodeUuids \u003d blkLocs.getDatanodeUuids();\n+            final StorageType[] storageTypes \u003d blkLocs.getStorageTypes();\n             for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n               final StorageGroup g \u003d storageGroupMap.get(\n                   datanodeUuids[i], storageTypes[i]);\n               if (g !\u003d null) { // not unknown\n                 block.addLocation(g);\n               }\n             }\n           }\n           if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             // filter bad candidates\n             srcBlocks.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n      final BlocksWithLocations newBlksLocs \u003d\n          nnc.getBlocks(getDatanodeInfo(), size);\n\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blkLocs : newBlksLocs.getBlocks()) {\n\n        DBlock block;\n        if (blkLocs instanceof StripedBlockWithLocations) {\n          StripedBlockWithLocations sblkLocs \u003d\n              (StripedBlockWithLocations) blkLocs;\n          // approximate size\n          bytesReceived +\u003d sblkLocs.getBlock().getNumBytes() /\n              sblkLocs.getDataBlockNum();\n          block \u003d new DBlockStriped(sblkLocs.getBlock(), sblkLocs.getIndices(),\n              sblkLocs.getDataBlockNum());\n        } else{\n          bytesReceived +\u003d blkLocs.getBlock().getNumBytes();\n          block \u003d new DBlock(blkLocs.getBlock());\n        }\n\n        synchronized (globalBlocks) {\n          block \u003d globalBlocks.putIfAbsent(blkLocs.getBlock(), block);\n          synchronized (block) {\n            block.clearLocations();\n\n            // update locations\n            final String[] datanodeUuids \u003d blkLocs.getDatanodeUuids();\n            final StorageType[] storageTypes \u003d blkLocs.getStorageTypes();\n            for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n              final StorageGroup g \u003d storageGroupMap.get(\n                  datanodeUuids[i], storageTypes[i]);\n              if (g !\u003d null) { // not unknown\n                block.addLocation(g);\n              }\n            }\n          }\n          if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlocks.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "e60673697d5046c29c52bbabdfe80506f99773e4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6837. Code cleanup for Balancer and Dispatcher. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1617337 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/08/14 11:01 AM",
      "commitName": "e60673697d5046c29c52bbabdfe80506f99773e4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "08/08/14 2:33 PM",
      "commitNameOld": "c3cf331dc91e2beef2afeed11105084843b02858",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.85,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n     private long getBlockList() throws IOException {\n       final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n-      final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanode(), size);\n+      final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanodeInfo(), size);\n \n       long bytesReceived \u003d 0;\n       for (BlockWithLocations blk : newBlocks.getBlocks()) {\n         bytesReceived +\u003d blk.getBlock().getNumBytes();\n         synchronized (globalBlocks) {\n           final DBlock block \u003d globalBlocks.get(blk.getBlock());\n           synchronized (block) {\n             block.clearLocations();\n \n             // update locations\n             final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n             final StorageType[] storageTypes \u003d blk.getStorageTypes();\n             for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n-              final BalancerDatanode.StorageGroup g \u003d storageGroupMap.get(\n+              final StorageGroup g \u003d storageGroupMap.get(\n                   datanodeUuids[i], storageTypes[i]);\n               if (g !\u003d null) { // not unknown\n                 block.addLocation(g);\n               }\n             }\n           }\n           if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             // filter bad candidates\n             srcBlocks.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n      final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanodeInfo(), size);\n\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks.getBlocks()) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        synchronized (globalBlocks) {\n          final DBlock block \u003d globalBlocks.get(blk.getBlock());\n          synchronized (block) {\n            block.clearLocations();\n\n            // update locations\n            final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n            final StorageType[] storageTypes \u003d blk.getStorageTypes();\n            for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n              final StorageGroup g \u003d storageGroupMap.get(\n                  datanodeUuids[i], storageTypes[i]);\n              if (g !\u003d null) { // not unknown\n                block.addLocation(g);\n              }\n            }\n          }\n          if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlocks.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "c3cf331dc91e2beef2afeed11105084843b02858": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-6828. Separate block replica dispatching from Balancer. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616889 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/08/14 2:33 PM",
      "commitName": "c3cf331dc91e2beef2afeed11105084843b02858",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-6828. Separate block replica dispatching from Balancer. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616889 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/08/14 2:33 PM",
          "commitName": "c3cf331dc91e2beef2afeed11105084843b02858",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "08/08/14 2:22 PM",
          "commitNameOld": "05d1bf4157e6660610f11951845e59899260596e",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,38 +1,31 @@\n     private long getBlockList() throws IOException {\n       final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n-      final BlockWithLocations[] newBlocks \u003d nnc.getNamenode().getBlocks(\n-          getDatanode(), size).getBlocks();\n+      final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanode(), size);\n \n       long bytesReceived \u003d 0;\n-      for (BlockWithLocations blk : newBlocks) {\n+      for (BlockWithLocations blk : newBlocks.getBlocks()) {\n         bytesReceived +\u003d blk.getBlock().getNumBytes();\n-        BalancerBlock block;\n-        synchronized(globalBlockList) {\n-          block \u003d globalBlockList.get(blk.getBlock());\n-          if (block\u003d\u003dnull) {\n-            block \u003d new BalancerBlock(blk.getBlock());\n-            globalBlockList.put(blk.getBlock(), block);\n-          } else {\n-            block.clearLocations();\n-          }\n-        \n+        synchronized (globalBlocks) {\n+          final DBlock block \u003d globalBlocks.get(blk.getBlock());\n           synchronized (block) {\n+            block.clearLocations();\n+\n             // update locations\n             final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n             final StorageType[] storageTypes \u003d blk.getStorageTypes();\n             for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n               final BalancerDatanode.StorageGroup g \u003d storageGroupMap.get(\n                   datanodeUuids[i], storageTypes[i]);\n               if (g !\u003d null) { // not unknown\n                 block.addLocation(g);\n               }\n             }\n           }\n-          if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n+          if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             // filter bad candidates\n-            srcBlockList.add(block);\n+            srcBlocks.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private long getBlockList() throws IOException {\n      final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n      final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanode(), size);\n\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks.getBlocks()) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        synchronized (globalBlocks) {\n          final DBlock block \u003d globalBlocks.get(blk.getBlock());\n          synchronized (block) {\n            block.clearLocations();\n\n            // update locations\n            final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n            final StorageType[] storageTypes \u003d blk.getStorageTypes();\n            for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n              final BalancerDatanode.StorageGroup g \u003d storageGroupMap.get(\n                  datanodeUuids[i], storageTypes[i]);\n              if (g !\u003d null) { // not unknown\n                block.addLocation(g);\n              }\n            }\n          }\n          if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlocks.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
            "oldMethodName": "getBlockList",
            "newMethodName": "getBlockList"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6828. Separate block replica dispatching from Balancer. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616889 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/08/14 2:33 PM",
          "commitName": "c3cf331dc91e2beef2afeed11105084843b02858",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "08/08/14 2:22 PM",
          "commitNameOld": "05d1bf4157e6660610f11951845e59899260596e",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,38 +1,31 @@\n     private long getBlockList() throws IOException {\n       final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n-      final BlockWithLocations[] newBlocks \u003d nnc.getNamenode().getBlocks(\n-          getDatanode(), size).getBlocks();\n+      final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanode(), size);\n \n       long bytesReceived \u003d 0;\n-      for (BlockWithLocations blk : newBlocks) {\n+      for (BlockWithLocations blk : newBlocks.getBlocks()) {\n         bytesReceived +\u003d blk.getBlock().getNumBytes();\n-        BalancerBlock block;\n-        synchronized(globalBlockList) {\n-          block \u003d globalBlockList.get(blk.getBlock());\n-          if (block\u003d\u003dnull) {\n-            block \u003d new BalancerBlock(blk.getBlock());\n-            globalBlockList.put(blk.getBlock(), block);\n-          } else {\n-            block.clearLocations();\n-          }\n-        \n+        synchronized (globalBlocks) {\n+          final DBlock block \u003d globalBlocks.get(blk.getBlock());\n           synchronized (block) {\n+            block.clearLocations();\n+\n             // update locations\n             final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n             final StorageType[] storageTypes \u003d blk.getStorageTypes();\n             for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n               final BalancerDatanode.StorageGroup g \u003d storageGroupMap.get(\n                   datanodeUuids[i], storageTypes[i]);\n               if (g !\u003d null) { // not unknown\n                 block.addLocation(g);\n               }\n             }\n           }\n-          if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n+          if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             // filter bad candidates\n-            srcBlockList.add(block);\n+            srcBlocks.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private long getBlockList() throws IOException {\n      final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n      final BlocksWithLocations newBlocks \u003d nnc.getBlocks(getDatanode(), size);\n\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks.getBlocks()) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        synchronized (globalBlocks) {\n          final DBlock block \u003d globalBlocks.get(blk.getBlock());\n          synchronized (block) {\n            block.clearLocations();\n\n            // update locations\n            final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n            final StorageType[] storageTypes \u003d blk.getStorageTypes();\n            for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n              final BalancerDatanode.StorageGroup g \u003d storageGroupMap.get(\n                  datanodeUuids[i], storageTypes[i]);\n              if (g !\u003d null) { // not unknown\n                block.addLocation(g);\n              }\n            }\n          }\n          if (!srcBlocks.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlocks.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
          "extendedDetails": {}
        }
      ]
    },
    "83b9933db3349e6a6faf23bce35c9d4ce3f7bcf2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6809. Move Balancer\u0027s inner classes MovedBlocks and Matcher as to standalone classes and separates KeyManager from NameNodeConnector.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616422 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/14 12:18 AM",
      "commitName": "83b9933db3349e6a6faf23bce35c9d4ce3f7bcf2",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/08/14 7:12 AM",
      "commitNameOld": "7e12b1912f8cdbe6d88ac0b8eb71d7c4dc1bf78e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.71,
      "commitsBetweenForRepo": 41,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n     private long getBlockList() throws IOException {\n       final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n-      final BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(\n+      final BlockWithLocations[] newBlocks \u003d nnc.getNamenode().getBlocks(\n           getDatanode(), size).getBlocks();\n \n       long bytesReceived \u003d 0;\n       for (BlockWithLocations blk : newBlocks) {\n         bytesReceived +\u003d blk.getBlock().getNumBytes();\n         BalancerBlock block;\n         synchronized(globalBlockList) {\n           block \u003d globalBlockList.get(blk.getBlock());\n           if (block\u003d\u003dnull) {\n             block \u003d new BalancerBlock(blk.getBlock());\n             globalBlockList.put(blk.getBlock(), block);\n           } else {\n             block.clearLocations();\n           }\n         \n           synchronized (block) {\n             // update locations\n             final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n             final StorageType[] storageTypes \u003d blk.getStorageTypes();\n             for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n               final BalancerDatanode.StorageGroup g \u003d storageGroupMap.get(\n                   datanodeUuids[i], storageTypes[i]);\n               if (g !\u003d null) { // not unknown\n                 block.addLocation(g);\n               }\n             }\n           }\n           if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             // filter bad candidates\n             srcBlockList.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n      final BlockWithLocations[] newBlocks \u003d nnc.getNamenode().getBlocks(\n          getDatanode(), size).getBlocks();\n\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        BalancerBlock block;\n        synchronized(globalBlockList) {\n          block \u003d globalBlockList.get(blk.getBlock());\n          if (block\u003d\u003dnull) {\n            block \u003d new BalancerBlock(blk.getBlock());\n            globalBlockList.put(blk.getBlock(), block);\n          } else {\n            block.clearLocations();\n          }\n        \n          synchronized (block) {\n            // update locations\n            final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n            final StorageType[] storageTypes \u003d blk.getStorageTypes();\n            for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n              final BalancerDatanode.StorageGroup g \u003d storageGroupMap.get(\n                  datanodeUuids[i], storageTypes[i]);\n              if (g !\u003d null) { // not unknown\n                block.addLocation(g);\n              }\n            }\n          }\n          if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlockList.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6685. Balancer should preserve storage type of replicas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615015 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/07/14 6:05 PM",
      "commitName": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/07/14 11:02 PM",
      "commitNameOld": "b8b8f3f5e7214d6fcfc30e1b94ff097e52868f4f",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,38 @@\n     private long getBlockList() throws IOException {\n-      BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(datanode, \n-        Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive)).getBlocks();\n+      final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n+      final BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(\n+          getDatanode(), size).getBlocks();\n+\n       long bytesReceived \u003d 0;\n       for (BlockWithLocations blk : newBlocks) {\n         bytesReceived +\u003d blk.getBlock().getNumBytes();\n         BalancerBlock block;\n         synchronized(globalBlockList) {\n           block \u003d globalBlockList.get(blk.getBlock());\n           if (block\u003d\u003dnull) {\n             block \u003d new BalancerBlock(blk.getBlock());\n             globalBlockList.put(blk.getBlock(), block);\n           } else {\n             block.clearLocations();\n           }\n         \n           synchronized (block) {\n             // update locations\n-            for (String datanodeUuid : blk.getDatanodeUuids()) {\n-              final BalancerDatanode d \u003d datanodeMap.get(datanodeUuid);\n-              if (d !\u003d null) { // not an unknown datanode\n-                block.addLocation(d);\n+            final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n+            final StorageType[] storageTypes \u003d blk.getStorageTypes();\n+            for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n+              final BalancerDatanode.StorageGroup g \u003d storageGroupMap.get(\n+                  datanodeUuids[i], storageTypes[i]);\n+              if (g !\u003d null) { // not unknown\n+                block.addLocation(g);\n               }\n             }\n           }\n           if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             // filter bad candidates\n             srcBlockList.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      final long size \u003d Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive);\n      final BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(\n          getDatanode(), size).getBlocks();\n\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        BalancerBlock block;\n        synchronized(globalBlockList) {\n          block \u003d globalBlockList.get(blk.getBlock());\n          if (block\u003d\u003dnull) {\n            block \u003d new BalancerBlock(blk.getBlock());\n            globalBlockList.put(blk.getBlock(), block);\n          } else {\n            block.clearLocations();\n          }\n        \n          synchronized (block) {\n            // update locations\n            final String[] datanodeUuids \u003d blk.getDatanodeUuids();\n            final StorageType[] storageTypes \u003d blk.getStorageTypes();\n            for (int i \u003d 0; i \u003c datanodeUuids.length; i++) {\n              final BalancerDatanode.StorageGroup g \u003d storageGroupMap.get(\n                  datanodeUuids[i], storageTypes[i]);\n              if (g !\u003d null) { // not unknown\n                block.addLocation(g);\n              }\n            }\n          }\n          if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlockList.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "c802ca28fd2c9fd35a662b8dd6b675e2aaa515d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6364. Incorrect check for unknown datanode in Balancer. (Contributed by Benoy Antony)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601771 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/06/14 2:02 PM",
      "commitName": "c802ca28fd2c9fd35a662b8dd6b675e2aaa515d1",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "29/05/14 2:38 PM",
      "commitNameOld": "f40a36758bb183b23181fd59a99d9ab87a428166",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 11.98,
      "commitsBetweenForRepo": 60,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n     private long getBlockList() throws IOException {\n       BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(datanode, \n         Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive)).getBlocks();\n       long bytesReceived \u003d 0;\n       for (BlockWithLocations blk : newBlocks) {\n         bytesReceived +\u003d blk.getBlock().getNumBytes();\n         BalancerBlock block;\n         synchronized(globalBlockList) {\n           block \u003d globalBlockList.get(blk.getBlock());\n           if (block\u003d\u003dnull) {\n             block \u003d new BalancerBlock(blk.getBlock());\n             globalBlockList.put(blk.getBlock(), block);\n           } else {\n             block.clearLocations();\n           }\n         \n           synchronized (block) {\n             // update locations\n             for (String datanodeUuid : blk.getDatanodeUuids()) {\n               final BalancerDatanode d \u003d datanodeMap.get(datanodeUuid);\n-              if (datanode !\u003d null) { // not an unknown datanode\n+              if (d !\u003d null) { // not an unknown datanode\n                 block.addLocation(d);\n               }\n             }\n           }\n           if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             // filter bad candidates\n             srcBlockList.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(datanode, \n        Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive)).getBlocks();\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        BalancerBlock block;\n        synchronized(globalBlockList) {\n          block \u003d globalBlockList.get(blk.getBlock());\n          if (block\u003d\u003dnull) {\n            block \u003d new BalancerBlock(blk.getBlock());\n            globalBlockList.put(blk.getBlock(), block);\n          } else {\n            block.clearLocations();\n          }\n        \n          synchronized (block) {\n            // update locations\n            for (String datanodeUuid : blk.getDatanodeUuids()) {\n              final BalancerDatanode d \u003d datanodeMap.get(datanodeUuid);\n              if (d !\u003d null) { // not an unknown datanode\n                block.addLocation(d);\n              }\n            }\n          }\n          if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlockList.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "907fb15ee8c150e5ecc0560b7374441c57a84122": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5480. Update Balancer for HDFS-2832. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1540547 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/11/13 12:59 PM",
      "commitName": "907fb15ee8c150e5ecc0560b7374441c57a84122",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "17/10/13 11:54 AM",
      "commitNameOld": "f8d5755a69d7b4f230adbbfd88ea73df7a83b4f0",
      "commitAuthorOld": "",
      "daysBetweenCommits": 24.09,
      "commitsBetweenForRepo": 121,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n     private long getBlockList() throws IOException {\n       BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(datanode, \n         Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive)).getBlocks();\n       long bytesReceived \u003d 0;\n       for (BlockWithLocations blk : newBlocks) {\n         bytesReceived +\u003d blk.getBlock().getNumBytes();\n         BalancerBlock block;\n         synchronized(globalBlockList) {\n           block \u003d globalBlockList.get(blk.getBlock());\n           if (block\u003d\u003dnull) {\n             block \u003d new BalancerBlock(blk.getBlock());\n             globalBlockList.put(blk.getBlock(), block);\n           } else {\n             block.clearLocations();\n           }\n         \n           synchronized (block) {\n             // update locations\n-            for ( String storageID : blk.getStorageIDs() ) {\n-              BalancerDatanode datanode \u003d datanodes.get(storageID);\n+            for (String datanodeUuid : blk.getDatanodeUuids()) {\n+              final BalancerDatanode d \u003d datanodeMap.get(datanodeUuid);\n               if (datanode !\u003d null) { // not an unknown datanode\n-                block.addLocation(datanode);\n+                block.addLocation(d);\n               }\n             }\n           }\n           if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             // filter bad candidates\n             srcBlockList.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(datanode, \n        Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive)).getBlocks();\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        BalancerBlock block;\n        synchronized(globalBlockList) {\n          block \u003d globalBlockList.get(blk.getBlock());\n          if (block\u003d\u003dnull) {\n            block \u003d new BalancerBlock(blk.getBlock());\n            globalBlockList.put(blk.getBlock(), block);\n          } else {\n            block.clearLocations();\n          }\n        \n          synchronized (block) {\n            // update locations\n            for (String datanodeUuid : blk.getDatanodeUuids()) {\n              final BalancerDatanode d \u003d datanodeMap.get(datanodeUuid);\n              if (datanode !\u003d null) { // not an unknown datanode\n                block.addLocation(d);\n              }\n            }\n          }\n          if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlockList.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "7428aeca8666aeaf5f6682efbdb5349f44d1753e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3418. Rename BlockWithLocationsProto datanodeIDs field to storageIDs. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1338830 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/05/12 12:03 PM",
      "commitName": "7428aeca8666aeaf5f6682efbdb5349f44d1753e",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "15/05/12 9:01 AM",
      "commitNameOld": "e4df14f8f151413a8ec0972a21e31a0b51fe0fb0",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.13,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n     private long getBlockList() throws IOException {\n       BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(datanode, \n         Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive)).getBlocks();\n       long bytesReceived \u003d 0;\n       for (BlockWithLocations blk : newBlocks) {\n         bytesReceived +\u003d blk.getBlock().getNumBytes();\n         BalancerBlock block;\n         synchronized(globalBlockList) {\n           block \u003d globalBlockList.get(blk.getBlock());\n           if (block\u003d\u003dnull) {\n             block \u003d new BalancerBlock(blk.getBlock());\n             globalBlockList.put(blk.getBlock(), block);\n           } else {\n             block.clearLocations();\n           }\n         \n           synchronized (block) {\n             // update locations\n-            for ( String location : blk.getDatanodes() ) {\n-              BalancerDatanode datanode \u003d datanodes.get(location);\n+            for ( String storageID : blk.getStorageIDs() ) {\n+              BalancerDatanode datanode \u003d datanodes.get(storageID);\n               if (datanode !\u003d null) { // not an unknown datanode\n                 block.addLocation(datanode);\n               }\n             }\n           }\n           if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n             // filter bad candidates\n             srcBlockList.add(block);\n           }\n         }\n       }\n       return bytesReceived;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(datanode, \n        Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive)).getBlocks();\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        BalancerBlock block;\n        synchronized(globalBlockList) {\n          block \u003d globalBlockList.get(blk.getBlock());\n          if (block\u003d\u003dnull) {\n            block \u003d new BalancerBlock(blk.getBlock());\n            globalBlockList.put(blk.getBlock(), block);\n          } else {\n            block.clearLocations();\n          }\n        \n          synchronized (block) {\n            // update locations\n            for ( String storageID : blk.getStorageIDs() ) {\n              BalancerDatanode datanode \u003d datanodes.get(storageID);\n              if (datanode !\u003d null) { // not an unknown datanode\n                block.addLocation(datanode);\n              }\n            }\n          }\n          if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlockList.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private long getBlockList() throws IOException {\n      BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(datanode, \n        Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive)).getBlocks();\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        BalancerBlock block;\n        synchronized(globalBlockList) {\n          block \u003d globalBlockList.get(blk.getBlock());\n          if (block\u003d\u003dnull) {\n            block \u003d new BalancerBlock(blk.getBlock());\n            globalBlockList.put(blk.getBlock(), block);\n          } else {\n            block.clearLocations();\n          }\n        \n          synchronized (block) {\n            // update locations\n            for ( String location : blk.getDatanodes() ) {\n              BalancerDatanode datanode \u003d datanodes.get(location);\n              if (datanode !\u003d null) { // not an unknown datanode\n                block.addLocation(datanode);\n              }\n            }\n          }\n          if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlockList.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private long getBlockList() throws IOException {\n      BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(datanode, \n        Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive)).getBlocks();\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        BalancerBlock block;\n        synchronized(globalBlockList) {\n          block \u003d globalBlockList.get(blk.getBlock());\n          if (block\u003d\u003dnull) {\n            block \u003d new BalancerBlock(blk.getBlock());\n            globalBlockList.put(blk.getBlock(), block);\n          } else {\n            block.clearLocations();\n          }\n        \n          synchronized (block) {\n            // update locations\n            for ( String location : blk.getDatanodes() ) {\n              BalancerDatanode datanode \u003d datanodes.get(location);\n              if (datanode !\u003d null) { // not an unknown datanode\n                block.addLocation(datanode);\n              }\n            }\n          }\n          if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlockList.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,33 @@\n+    private long getBlockList() throws IOException {\n+      BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(datanode, \n+        Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive)).getBlocks();\n+      long bytesReceived \u003d 0;\n+      for (BlockWithLocations blk : newBlocks) {\n+        bytesReceived +\u003d blk.getBlock().getNumBytes();\n+        BalancerBlock block;\n+        synchronized(globalBlockList) {\n+          block \u003d globalBlockList.get(blk.getBlock());\n+          if (block\u003d\u003dnull) {\n+            block \u003d new BalancerBlock(blk.getBlock());\n+            globalBlockList.put(blk.getBlock(), block);\n+          } else {\n+            block.clearLocations();\n+          }\n+        \n+          synchronized (block) {\n+            // update locations\n+            for ( String location : blk.getDatanodes() ) {\n+              BalancerDatanode datanode \u003d datanodes.get(location);\n+              if (datanode !\u003d null) { // not an unknown datanode\n+                block.addLocation(datanode);\n+              }\n+            }\n+          }\n+          if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n+            // filter bad candidates\n+            srcBlockList.add(block);\n+          }\n+        }\n+      }\n+      return bytesReceived;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private long getBlockList() throws IOException {\n      BlockWithLocations[] newBlocks \u003d nnc.namenode.getBlocks(datanode, \n        Math.min(MAX_BLOCKS_SIZE_TO_FETCH, blocksToReceive)).getBlocks();\n      long bytesReceived \u003d 0;\n      for (BlockWithLocations blk : newBlocks) {\n        bytesReceived +\u003d blk.getBlock().getNumBytes();\n        BalancerBlock block;\n        synchronized(globalBlockList) {\n          block \u003d globalBlockList.get(blk.getBlock());\n          if (block\u003d\u003dnull) {\n            block \u003d new BalancerBlock(blk.getBlock());\n            globalBlockList.put(blk.getBlock(), block);\n          } else {\n            block.clearLocations();\n          }\n        \n          synchronized (block) {\n            // update locations\n            for ( String location : blk.getDatanodes() ) {\n              BalancerDatanode datanode \u003d datanodes.get(location);\n              if (datanode !\u003d null) { // not an unknown datanode\n                block.addLocation(datanode);\n              }\n            }\n          }\n          if (!srcBlockList.contains(block) \u0026\u0026 isGoodBlockCandidate(block)) {\n            // filter bad candidates\n            srcBlockList.add(block);\n          }\n        }\n      }\n      return bytesReceived;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java"
    }
  }
}