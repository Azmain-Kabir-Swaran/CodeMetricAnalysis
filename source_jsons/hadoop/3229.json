{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WebHdfsFileSystem.java",
  "functionName": "open",
  "functionId": "open___f-Path(modifiers-final)__bufferSize-int(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
  "functionStartLine": 1568,
  "functionEndLine": 1580,
  "numCommitsSeen": 196,
  "timeTaken": 5459,
  "changeHistory": [
    "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73",
    "687233f20d24c29041929dd0a99d963cec54b6df",
    "867048c3e4b20ece0039a876def129fa5eb9234f",
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
    "0c5128969522cf754010c32cdcbfcfa5ebe5b3b0",
    "6afe3e0d22caa2b0752d52ddf7794c25a66cc9c8",
    "bd21ddcb78350b311f271e233038b8ca78a65242",
    "6c3b59505b863f03629da52a1e9b886fe9b496d0"
  ],
  "changeHistoryShort": {
    "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73": "Ybodychange",
    "687233f20d24c29041929dd0a99d963cec54b6df": "Ybodychange",
    "867048c3e4b20ece0039a876def129fa5eb9234f": "Ymultichange(Yparameterchange,Ybodychange)",
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35": "Yfilerename",
    "0c5128969522cf754010c32cdcbfcfa5ebe5b3b0": "Ybodychange",
    "6afe3e0d22caa2b0752d52ddf7794c25a66cc9c8": "Ybodychange",
    "bd21ddcb78350b311f271e233038b8ca78a65242": "Ybodychange",
    "6c3b59505b863f03629da52a1e9b886fe9b496d0": "Yintroduced"
  },
  "changeHistoryDetails": {
    "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12574. Add CryptoInputStream to WebHdfsFileSystem read call. Contributed by Rushabh S Shah\n",
      "commitDate": "29/01/18 3:23 PM",
      "commitName": "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "04/12/17 10:40 AM",
      "commitNameOld": "404eab4dc0582e0384b93664ea6ee77ccd5eeebc",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 56.2,
      "commitsBetweenForRepo": 316,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,6 +1,13 @@\n   public FSDataInputStream open(final Path f, final int bufferSize\n   ) throws IOException {\n     statistics.incrementReadOps(1);\n     storageStatistics.incrementOpCounter(OpType.OPEN);\n-    return new FSDataInputStream(new WebHdfsInputStream(f, bufferSize));\n+    WebHdfsInputStream webfsInputStream \u003d\n+        new WebHdfsInputStream(f, bufferSize);\n+    if (webfsInputStream.getFileEncryptionInfo() \u003d\u003d null) {\n+      return new FSDataInputStream(webfsInputStream);\n+    } else {\n+      return new FSDataInputStream(\n+          webfsInputStream.createWrappedInputStream());\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FSDataInputStream open(final Path f, final int bufferSize\n  ) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.OPEN);\n    WebHdfsInputStream webfsInputStream \u003d\n        new WebHdfsInputStream(f, bufferSize);\n    if (webfsInputStream.getFileEncryptionInfo() \u003d\u003d null) {\n      return new FSDataInputStream(webfsInputStream);\n    } else {\n      return new FSDataInputStream(\n          webfsInputStream.createWrappedInputStream());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "687233f20d24c29041929dd0a99d963cec54b6df": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13065. Add a new interface for retrieving FS and FC Statistics (Mingliang Liu via cmccabe)\n",
      "commitDate": "11/05/16 1:45 PM",
      "commitName": "687233f20d24c29041929dd0a99d963cec54b6df",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "23/04/16 7:37 AM",
      "commitNameOld": "6fcde2e38da04cae3aad6b13cf442af211f71506",
      "commitAuthorOld": "Masatake Iwasaki",
      "daysBetweenCommits": 18.26,
      "commitsBetweenForRepo": 102,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,6 @@\n   public FSDataInputStream open(final Path f, final int bufferSize\n   ) throws IOException {\n     statistics.incrementReadOps(1);\n+    storageStatistics.incrementOpCounter(OpType.OPEN);\n     return new FSDataInputStream(new WebHdfsInputStream(f, bufferSize));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FSDataInputStream open(final Path f, final int bufferSize\n  ) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.OPEN);\n    return new FSDataInputStream(new WebHdfsInputStream(f, bufferSize));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "867048c3e4b20ece0039a876def129fa5eb9234f": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7163. WebHdfsFileSystem should retry reads according to the configured retry policy. Contributed by Eric Payne.\n",
      "commitDate": "22/12/15 12:08 PM",
      "commitName": "867048c3e4b20ece0039a876def129fa5eb9234f",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7163. WebHdfsFileSystem should retry reads according to the configured retry policy. Contributed by Eric Payne.\n",
          "commitDate": "22/12/15 12:08 PM",
          "commitName": "867048c3e4b20ece0039a876def129fa5eb9234f",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "11/12/15 10:59 AM",
          "commitNameOld": "576b569b6c97bd5f57e52efdabdf8c2fa996a524",
          "commitAuthorOld": "Allen Wittenauer",
          "daysBetweenCommits": 11.05,
          "commitsBetweenForRepo": 77,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,10 +1,5 @@\n-  public FSDataInputStream open(final Path f, final int buffersize\n+  public FSDataInputStream open(final Path f, final int bufferSize\n   ) throws IOException {\n     statistics.incrementReadOps(1);\n-    final HttpOpParam.Op op \u003d GetOpParam.Op.OPEN;\n-    // use a runner so the open can recover from an invalid token\n-    FsPathConnectionRunner runner \u003d\n-        new FsPathConnectionRunner(op, f, new BufferSizeParam(buffersize));\n-    return new FSDataInputStream(new OffsetUrlInputStream(\n-        new UnresolvedUrlOpener(runner), new OffsetUrlOpener(null)));\n+    return new FSDataInputStream(new WebHdfsInputStream(f, bufferSize));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public FSDataInputStream open(final Path f, final int bufferSize\n  ) throws IOException {\n    statistics.incrementReadOps(1);\n    return new FSDataInputStream(new WebHdfsInputStream(f, bufferSize));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
          "extendedDetails": {
            "oldValue": "[f-Path(modifiers-final), buffersize-int(modifiers-final)]",
            "newValue": "[f-Path(modifiers-final), bufferSize-int(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7163. WebHdfsFileSystem should retry reads according to the configured retry policy. Contributed by Eric Payne.\n",
          "commitDate": "22/12/15 12:08 PM",
          "commitName": "867048c3e4b20ece0039a876def129fa5eb9234f",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "11/12/15 10:59 AM",
          "commitNameOld": "576b569b6c97bd5f57e52efdabdf8c2fa996a524",
          "commitAuthorOld": "Allen Wittenauer",
          "daysBetweenCommits": 11.05,
          "commitsBetweenForRepo": 77,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,10 +1,5 @@\n-  public FSDataInputStream open(final Path f, final int buffersize\n+  public FSDataInputStream open(final Path f, final int bufferSize\n   ) throws IOException {\n     statistics.incrementReadOps(1);\n-    final HttpOpParam.Op op \u003d GetOpParam.Op.OPEN;\n-    // use a runner so the open can recover from an invalid token\n-    FsPathConnectionRunner runner \u003d\n-        new FsPathConnectionRunner(op, f, new BufferSizeParam(buffersize));\n-    return new FSDataInputStream(new OffsetUrlInputStream(\n-        new UnresolvedUrlOpener(runner), new OffsetUrlOpener(null)));\n+    return new FSDataInputStream(new WebHdfsInputStream(f, bufferSize));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public FSDataInputStream open(final Path f, final int bufferSize\n  ) throws IOException {\n    statistics.incrementReadOps(1);\n    return new FSDataInputStream(new WebHdfsInputStream(f, bufferSize));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8052. Move WebHdfsFileSystem into hadoop-hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "23/04/15 5:33 PM",
      "commitName": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "23/04/15 4:40 PM",
      "commitNameOld": "0b3f8957a87ada1a275c9904b211fdbdcefafb02",
      "commitAuthorOld": "Xuan",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public FSDataInputStream open(final Path f, final int buffersize\n      ) throws IOException {\n    statistics.incrementReadOps(1);\n    final HttpOpParam.Op op \u003d GetOpParam.Op.OPEN;\n    // use a runner so the open can recover from an invalid token\n    FsPathConnectionRunner runner \u003d\n        new FsPathConnectionRunner(op, f, new BufferSizeParam(buffersize));\n    return new FSDataInputStream(new OffsetUrlInputStream(\n        new UnresolvedUrlOpener(runner), new OffsetUrlOpener(null)));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java"
      }
    },
    "0c5128969522cf754010c32cdcbfcfa5ebe5b3b0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6222. Remove background token renewer from webhdfs. Contributed by Rushabh Shah and Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1604300 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/06/14 4:58 PM",
      "commitName": "0c5128969522cf754010c32cdcbfcfa5ebe5b3b0",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "19/06/14 4:06 PM",
      "commitNameOld": "46dc32e12568c5e254a3a2f2664095dc9de8bd55",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 1.04,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,10 @@\n   public FSDataInputStream open(final Path f, final int buffersize\n       ) throws IOException {\n     statistics.incrementReadOps(1);\n     final HttpOpParam.Op op \u003d GetOpParam.Op.OPEN;\n-    final URL url \u003d toUrl(op, f, new BufferSizeParam(buffersize));\n+    // use a runner so the open can recover from an invalid token\n+    FsPathConnectionRunner runner \u003d\n+        new FsPathConnectionRunner(op, f, new BufferSizeParam(buffersize));\n     return new FSDataInputStream(new OffsetUrlInputStream(\n-        new OffsetUrlOpener(url), new OffsetUrlOpener(null)));\n+        new UnresolvedUrlOpener(runner), new OffsetUrlOpener(null)));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FSDataInputStream open(final Path f, final int buffersize\n      ) throws IOException {\n    statistics.incrementReadOps(1);\n    final HttpOpParam.Op op \u003d GetOpParam.Op.OPEN;\n    // use a runner so the open can recover from an invalid token\n    FsPathConnectionRunner runner \u003d\n        new FsPathConnectionRunner(op, f, new BufferSizeParam(buffersize));\n    return new FSDataInputStream(new OffsetUrlInputStream(\n        new UnresolvedUrlOpener(runner), new OffsetUrlOpener(null)));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "6afe3e0d22caa2b0752d52ddf7794c25a66cc9c8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2527. WebHdfs: remove the use of \"Range\" header in Open; use ugi username if renewer parameter is null in GetDelegationToken; response OK when setting replication for non-files; rename GETFILEBLOCKLOCATIONS to GET_BLOCK_LOCATIONS and state that it is a private unstable API; replace isDirectory and isSymlink with enum {FILE, DIRECTORY, SYMLINK} in HdfsFileStatus JSON object. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1197329 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/11/11 3:34 PM",
      "commitName": "6afe3e0d22caa2b0752d52ddf7794c25a66cc9c8",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/11/11 7:08 PM",
      "commitNameOld": "bd21ddcb78350b311f271e233038b8ca78a65242",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 1.85,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,8 @@\n   public FSDataInputStream open(final Path f, final int buffersize\n       ) throws IOException {\n     statistics.incrementReadOps(1);\n     final HttpOpParam.Op op \u003d GetOpParam.Op.OPEN;\n     final URL url \u003d toUrl(op, f, new BufferSizeParam(buffersize));\n-    ByteRangeInputStream str \u003d getByteRangeInputStream(url);\n-    return new FSDataInputStream(str);\n+    return new FSDataInputStream(new OffsetUrlInputStream(\n+        new OffsetUrlOpener(url), new OffsetUrlOpener(null)));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FSDataInputStream open(final Path f, final int buffersize\n      ) throws IOException {\n    statistics.incrementReadOps(1);\n    final HttpOpParam.Op op \u003d GetOpParam.Op.OPEN;\n    final URL url \u003d toUrl(op, f, new BufferSizeParam(buffersize));\n    return new FSDataInputStream(new OffsetUrlInputStream(\n        new OffsetUrlOpener(url), new OffsetUrlOpener(null)));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "bd21ddcb78350b311f271e233038b8ca78a65242": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2416. distcp with a webhdfs uri on a secure cluster fails.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196434 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/11/11 7:08 PM",
      "commitName": "bd21ddcb78350b311f271e233038b8ca78a65242",
      "commitAuthor": "Jitendra Nath Pandey",
      "commitDateOld": "31/10/11 1:37 PM",
      "commitNameOld": "32cad9affe159ff7c6e4c7e31f57174967ef210a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.23,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,7 +1,8 @@\n   public FSDataInputStream open(final Path f, final int buffersize\n       ) throws IOException {\n     statistics.incrementReadOps(1);\n     final HttpOpParam.Op op \u003d GetOpParam.Op.OPEN;\n     final URL url \u003d toUrl(op, f, new BufferSizeParam(buffersize));\n-    return new FSDataInputStream(new ByteRangeInputStream(url));\n+    ByteRangeInputStream str \u003d getByteRangeInputStream(url);\n+    return new FSDataInputStream(str);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FSDataInputStream open(final Path f, final int buffersize\n      ) throws IOException {\n    statistics.incrementReadOps(1);\n    final HttpOpParam.Op op \u003d GetOpParam.Op.OPEN;\n    final URL url \u003d toUrl(op, f, new BufferSizeParam(buffersize));\n    ByteRangeInputStream str \u003d getByteRangeInputStream(url);\n    return new FSDataInputStream(str);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "6c3b59505b863f03629da52a1e9b886fe9b496d0": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2317. Support read access to HDFS in webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1170085 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/09/11 1:34 AM",
      "commitName": "6c3b59505b863f03629da52a1e9b886fe9b496d0",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,7 @@\n+  public FSDataInputStream open(final Path f, final int buffersize\n+      ) throws IOException {\n+    statistics.incrementReadOps(1);\n+    final HttpOpParam.Op op \u003d GetOpParam.Op.OPEN;\n+    final URL url \u003d toUrl(op, f, new BufferSizeParam(buffersize));\n+    return new FSDataInputStream(new ByteRangeInputStream(url));\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public FSDataInputStream open(final Path f, final int buffersize\n      ) throws IOException {\n    statistics.incrementReadOps(1);\n    final HttpOpParam.Op op \u003d GetOpParam.Op.OPEN;\n    final URL url \u003d toUrl(op, f, new BufferSizeParam(buffersize));\n    return new FSDataInputStream(new ByteRangeInputStream(url));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java"
    }
  }
}