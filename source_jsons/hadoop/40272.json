{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RegularContainerAllocator.java",
  "functionName": "canAssign",
  "functionId": "canAssign___schedulerKey-SchedulerRequestKey__node-FiCaSchedulerNode__type-NodeType__reservedContainer-RMContainer",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
  "functionStartLine": 303,
  "functionEndLine": 368,
  "numCommitsSeen": 232,
  "timeTaken": 14629,
  "changeHistory": [
    "2a0fa50f9d718d51036ccdf30c7d998362fe423b",
    "a957f1c60e1308d1d70a1803381994f59949c5f8",
    "7999318af12a75b35815461c601d4c25750e8340",
    "2977bc6a141041ef7579efc416e93fc55e0c2a1a",
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
    "796638d9bc86235b9f3e5d1a3a9a25bbf5c04d1c",
    "ba2313d6145a1234777938a747187373f4cd58d9",
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c",
    "520033b1cd81c76b38fcdcfcfeed16158db4bbba",
    "56d72dfdf1f33272f32794a02f3b08cfeee85343",
    "e1fdf62123625e4ba399af02f8aad500637d29d1",
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76",
    "9a10b4e773ac937b59b458343457bbbd686d7f1e",
    "5183e881097b37b723f07f4d6af06721a326bea1",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517"
  ],
  "changeHistoryShort": {
    "2a0fa50f9d718d51036ccdf30c7d998362fe423b": "Ybodychange",
    "a957f1c60e1308d1d70a1803381994f59949c5f8": "Ybodychange",
    "7999318af12a75b35815461c601d4c25750e8340": "Ybodychange",
    "2977bc6a141041ef7579efc416e93fc55e0c2a1a": "Ybodychange",
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828": "Ymultichange(Yparameterchange,Ybodychange)",
    "796638d9bc86235b9f3e5d1a3a9a25bbf5c04d1c": "Ybodychange",
    "ba2313d6145a1234777938a747187373f4cd58d9": "Ymultichange(Ymovefromfile,Ybodychange)",
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5": "Ybodychange",
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c": "Ybodychange",
    "520033b1cd81c76b38fcdcfcfeed16158db4bbba": "Ybodychange",
    "56d72dfdf1f33272f32794a02f3b08cfeee85343": "Ybodychange",
    "e1fdf62123625e4ba399af02f8aad500637d29d1": "Yfilerename",
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76": "Yparameterchange",
    "9a10b4e773ac937b59b458343457bbbd686d7f1e": "Ybodychange",
    "5183e881097b37b723f07f4d6af06721a326bea1": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yintroduced"
  },
  "changeHistoryDetails": {
    "2a0fa50f9d718d51036ccdf30c7d998362fe423b": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8193. YARN RM hangs abruptly (stops allocating resources) when running successive applications. (Zian Chen via wangda)\n\nChange-Id: Ia83dd2499ee9000b9e09ae5a932f21a13c0ddee6\n",
      "commitDate": "26/04/18 1:54 PM",
      "commitName": "2a0fa50f9d718d51036ccdf30c7d998362fe423b",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "31/01/18 1:30 AM",
      "commitNameOld": "38af23796971193fa529c3d08ffde8fcd6e607b6",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 85.48,
      "commitsBetweenForRepo": 1196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,66 @@\n   private boolean canAssign(SchedulerRequestKey schedulerKey,\n       FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n       // If there are no nodes in the cluster, return false.\n       if (rmContext.getScheduler().getNumClusterNodes() \u003d\u003d 0) {\n         return false;\n       }\n+\n+      int uniqLocationAsks \u003d 0;\n+      AppPlacementAllocator appPlacementAllocator \u003d\n+          application.getAppPlacementAllocator(schedulerKey);\n+      if (appPlacementAllocator !\u003d null) {\n+        uniqLocationAsks \u003d appPlacementAllocator.getUniqueLocationAsks();\n+      }\n       // If we have only ANY requests for this schedulerKey, we should not\n       // delay its scheduling.\n-      if (application.getAppPlacementAllocator(schedulerKey)\n-          .getUniqueLocationAsks() \u003d\u003d 1) {\n+      if (uniqLocationAsks \u003d\u003d 1) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       long missedOpportunities \u003d\n           application.getSchedulingOpportunities(schedulerKey);\n \n       // If rack locality additional delay parameter is enabled.\n       if (application.getCSLeafQueue().getRackLocalityAdditionalDelay() \u003e -1) {\n         return missedOpportunities \u003e getActualRackLocalityDelay();\n       } else {\n         long requiredContainers \u003d\n             application.getOutstandingAsksCount(schedulerKey);\n-        float localityWaitFactor \u003d getLocalityWaitFactor(schedulerKey,\n+        float localityWaitFactor \u003d getLocalityWaitFactor(uniqLocationAsks,\n             rmContext.getScheduler().getNumClusterNodes());\n         // Cap the delay by the number of nodes in the cluster.\n         return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n             (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n       }\n     }\n \n     // Check if we need containers on this rack\n     if (application.getOutstandingAsksCount(schedulerKey,\n         node.getRackName()) \u003c\u003d 0) {\n       return false;\n     }\n \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n       long missedOpportunities \u003d\n           application.getSchedulingOpportunities(schedulerKey);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       return application.getOutstandingAsksCount(schedulerKey,\n           node.getNodeName()) \u003e 0;\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean canAssign(SchedulerRequestKey schedulerKey,\n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n      // If there are no nodes in the cluster, return false.\n      if (rmContext.getScheduler().getNumClusterNodes() \u003d\u003d 0) {\n        return false;\n      }\n\n      int uniqLocationAsks \u003d 0;\n      AppPlacementAllocator appPlacementAllocator \u003d\n          application.getAppPlacementAllocator(schedulerKey);\n      if (appPlacementAllocator !\u003d null) {\n        uniqLocationAsks \u003d appPlacementAllocator.getUniqueLocationAsks();\n      }\n      // If we have only ANY requests for this schedulerKey, we should not\n      // delay its scheduling.\n      if (uniqLocationAsks \u003d\u003d 1) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n\n      // If rack locality additional delay parameter is enabled.\n      if (application.getCSLeafQueue().getRackLocalityAdditionalDelay() \u003e -1) {\n        return missedOpportunities \u003e getActualRackLocalityDelay();\n      } else {\n        long requiredContainers \u003d\n            application.getOutstandingAsksCount(schedulerKey);\n        float localityWaitFactor \u003d getLocalityWaitFactor(uniqLocationAsks,\n            rmContext.getScheduler().getNumClusterNodes());\n        // Cap the delay by the number of nodes in the cluster.\n        return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n            (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n      }\n    }\n\n    // Check if we need containers on this rack\n    if (application.getOutstandingAsksCount(schedulerKey,\n        node.getRackName()) \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      return application.getOutstandingAsksCount(schedulerKey,\n          node.getNodeName()) \u003e 0;\n    }\n\n    return false;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "a957f1c60e1308d1d70a1803381994f59949c5f8": {
      "type": "Ybodychange",
      "commitMessage": "YARN-7438. Additional changes to make SchedulingPlacementSet agnostic to ResourceRequest / placement algorithm. Contributed by Wangda Tan\n",
      "commitDate": "05/12/17 9:20 AM",
      "commitName": "a957f1c60e1308d1d70a1803381994f59949c5f8",
      "commitAuthor": "Sunil G",
      "commitDateOld": "09/11/17 1:01 PM",
      "commitNameOld": "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
      "commitAuthorOld": "Konstantinos Karanasos",
      "daysBetweenCommits": 25.85,
      "commitsBetweenForRepo": 121,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,60 @@\n   private boolean canAssign(SchedulerRequestKey schedulerKey,\n       FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n       // If there are no nodes in the cluster, return false.\n       if (rmContext.getScheduler().getNumClusterNodes() \u003d\u003d 0) {\n         return false;\n       }\n       // If we have only ANY requests for this schedulerKey, we should not\n       // delay its scheduling.\n-      if (application.getResourceRequests(schedulerKey).size() \u003d\u003d 1) {\n+      if (application.getAppPlacementAllocator(schedulerKey)\n+          .getUniqueLocationAsks() \u003d\u003d 1) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       long missedOpportunities \u003d\n           application.getSchedulingOpportunities(schedulerKey);\n \n       // If rack locality additional delay parameter is enabled.\n       if (application.getCSLeafQueue().getRackLocalityAdditionalDelay() \u003e -1) {\n         return missedOpportunities \u003e getActualRackLocalityDelay();\n       } else {\n         long requiredContainers \u003d\n             application.getOutstandingAsksCount(schedulerKey);\n         float localityWaitFactor \u003d getLocalityWaitFactor(schedulerKey,\n             rmContext.getScheduler().getNumClusterNodes());\n         // Cap the delay by the number of nodes in the cluster.\n         return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n             (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n       }\n     }\n \n     // Check if we need containers on this rack\n     if (application.getOutstandingAsksCount(schedulerKey,\n         node.getRackName()) \u003c\u003d 0) {\n       return false;\n     }\n \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n       long missedOpportunities \u003d\n           application.getSchedulingOpportunities(schedulerKey);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       return application.getOutstandingAsksCount(schedulerKey,\n           node.getNodeName()) \u003e 0;\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean canAssign(SchedulerRequestKey schedulerKey,\n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n      // If there are no nodes in the cluster, return false.\n      if (rmContext.getScheduler().getNumClusterNodes() \u003d\u003d 0) {\n        return false;\n      }\n      // If we have only ANY requests for this schedulerKey, we should not\n      // delay its scheduling.\n      if (application.getAppPlacementAllocator(schedulerKey)\n          .getUniqueLocationAsks() \u003d\u003d 1) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n\n      // If rack locality additional delay parameter is enabled.\n      if (application.getCSLeafQueue().getRackLocalityAdditionalDelay() \u003e -1) {\n        return missedOpportunities \u003e getActualRackLocalityDelay();\n      } else {\n        long requiredContainers \u003d\n            application.getOutstandingAsksCount(schedulerKey);\n        float localityWaitFactor \u003d getLocalityWaitFactor(schedulerKey,\n            rmContext.getScheduler().getNumClusterNodes());\n        // Cap the delay by the number of nodes in the cluster.\n        return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n            (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n      }\n    }\n\n    // Check if we need containers on this rack\n    if (application.getOutstandingAsksCount(schedulerKey,\n        node.getRackName()) \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      return application.getOutstandingAsksCount(schedulerKey,\n          node.getNodeName()) \u003e 0;\n    }\n\n    return false;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "7999318af12a75b35815461c601d4c25750e8340": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6344. Add parameter for rack locality delay in CapacityScheduler. (kkaranasos)\n",
      "commitDate": "10/04/17 3:34 PM",
      "commitName": "7999318af12a75b35815461c601d4c25750e8340",
      "commitAuthor": "Konstantinos Karanasos",
      "commitDateOld": "06/01/17 9:59 AM",
      "commitNameOld": "2977bc6a141041ef7579efc416e93fc55e0c2a1a",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 94.19,
      "commitsBetweenForRepo": 524,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,59 @@\n   private boolean canAssign(SchedulerRequestKey schedulerKey,\n       FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n+      // If there are no nodes in the cluster, return false.\n+      if (rmContext.getScheduler().getNumClusterNodes() \u003d\u003d 0) {\n+        return false;\n+      }\n+      // If we have only ANY requests for this schedulerKey, we should not\n+      // delay its scheduling.\n+      if (application.getResourceRequests(schedulerKey).size() \u003d\u003d 1) {\n+        return true;\n+      }\n \n       // \u0027Delay\u0027 off-switch\n       long missedOpportunities \u003d\n           application.getSchedulingOpportunities(schedulerKey);\n-      long requiredContainers \u003d application.getOutstandingAsksCount(\n-          schedulerKey);\n \n-      float localityWaitFactor \u003d\n-          getLocalityWaitFactor(schedulerKey, rmContext.getScheduler()\n-              .getNumClusterNodes());\n-      // Cap the delay by the number of nodes in the cluster. Under most\n-      // conditions this means we will consider each node in the cluster before\n-      // accepting an off-switch assignment.\n-      return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n-          (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n+      // If rack locality additional delay parameter is enabled.\n+      if (application.getCSLeafQueue().getRackLocalityAdditionalDelay() \u003e -1) {\n+        return missedOpportunities \u003e getActualRackLocalityDelay();\n+      } else {\n+        long requiredContainers \u003d\n+            application.getOutstandingAsksCount(schedulerKey);\n+        float localityWaitFactor \u003d getLocalityWaitFactor(schedulerKey,\n+            rmContext.getScheduler().getNumClusterNodes());\n+        // Cap the delay by the number of nodes in the cluster.\n+        return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n+            (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n+      }\n     }\n \n     // Check if we need containers on this rack\n-    if (application.getOutstandingAsksCount(schedulerKey, node.getRackName())\n-        \u003c\u003d 0) {\n+    if (application.getOutstandingAsksCount(schedulerKey,\n+        node.getRackName()) \u003c\u003d 0) {\n       return false;\n     }\n \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n       long missedOpportunities \u003d\n           application.getSchedulingOpportunities(schedulerKey);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       return application.getOutstandingAsksCount(schedulerKey,\n           node.getNodeName()) \u003e 0;\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean canAssign(SchedulerRequestKey schedulerKey,\n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n      // If there are no nodes in the cluster, return false.\n      if (rmContext.getScheduler().getNumClusterNodes() \u003d\u003d 0) {\n        return false;\n      }\n      // If we have only ANY requests for this schedulerKey, we should not\n      // delay its scheduling.\n      if (application.getResourceRequests(schedulerKey).size() \u003d\u003d 1) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n\n      // If rack locality additional delay parameter is enabled.\n      if (application.getCSLeafQueue().getRackLocalityAdditionalDelay() \u003e -1) {\n        return missedOpportunities \u003e getActualRackLocalityDelay();\n      } else {\n        long requiredContainers \u003d\n            application.getOutstandingAsksCount(schedulerKey);\n        float localityWaitFactor \u003d getLocalityWaitFactor(schedulerKey,\n            rmContext.getScheduler().getNumClusterNodes());\n        // Cap the delay by the number of nodes in the cluster.\n        return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n            (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n      }\n    }\n\n    // Check if we need containers on this rack\n    if (application.getOutstandingAsksCount(schedulerKey,\n        node.getRackName()) \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      return application.getOutstandingAsksCount(schedulerKey,\n          node.getNodeName()) \u003e 0;\n    }\n\n    return false;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "2977bc6a141041ef7579efc416e93fc55e0c2a1a": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6040. Introduce api independent PendingAsk to replace usage of ResourceRequest within Scheduler classes. (Wangda Tan via asuresh)\n",
      "commitDate": "06/01/17 9:59 AM",
      "commitName": "2977bc6a141041ef7579efc416e93fc55e0c2a1a",
      "commitAuthor": "Arun Suresh",
      "commitDateOld": "05/01/17 10:31 AM",
      "commitNameOld": "0a55bd841ec0f2eb89a0383f4c589526e8b138d4",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.98,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,48 @@\n   private boolean canAssign(SchedulerRequestKey schedulerKey,\n       FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n-      ResourceRequest offSwitchRequest \u003d\n-          application.getResourceRequest(schedulerKey, ResourceRequest.ANY);\n       long missedOpportunities \u003d\n           application.getSchedulingOpportunities(schedulerKey);\n-      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n+      long requiredContainers \u003d application.getOutstandingAsksCount(\n+          schedulerKey);\n \n       float localityWaitFactor \u003d\n           getLocalityWaitFactor(schedulerKey, rmContext.getScheduler()\n               .getNumClusterNodes());\n       // Cap the delay by the number of nodes in the cluster. Under most\n       // conditions this means we will consider each node in the cluster before\n       // accepting an off-switch assignment.\n       return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n           (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack\n-    ResourceRequest rackLocalRequest \u003d\n-        application.getResourceRequest(schedulerKey, node.getRackName());\n-    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n+    if (application.getOutstandingAsksCount(schedulerKey, node.getRackName())\n+        \u003c\u003d 0) {\n       return false;\n     }\n \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n       long missedOpportunities \u003d\n           application.getSchedulingOpportunities(schedulerKey);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n-      ResourceRequest nodeLocalRequest \u003d\n-          application.getResourceRequest(schedulerKey, node.getNodeName());\n-      if (nodeLocalRequest !\u003d null) {\n-        return nodeLocalRequest.getNumContainers() \u003e 0;\n-      }\n+      return application.getOutstandingAsksCount(schedulerKey,\n+          node.getNodeName()) \u003e 0;\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean canAssign(SchedulerRequestKey schedulerKey,\n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n      long requiredContainers \u003d application.getOutstandingAsksCount(\n          schedulerKey);\n\n      float localityWaitFactor \u003d\n          getLocalityWaitFactor(schedulerKey, rmContext.getScheduler()\n              .getNumClusterNodes());\n      // Cap the delay by the number of nodes in the cluster. Under most\n      // conditions this means we will consider each node in the cluster before\n      // accepting an off-switch assignment.\n      return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n          (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack\n    if (application.getOutstandingAsksCount(schedulerKey, node.getRackName())\n        \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      return application.getOutstandingAsksCount(schedulerKey,\n          node.getNodeName()) \u003e 0;\n    }\n\n    return false;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
      "commitDate": "26/07/16 2:54 PM",
      "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
      "commitAuthor": "Arun Suresh",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
          "commitDate": "26/07/16 2:54 PM",
          "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "07/06/16 3:06 PM",
          "commitNameOld": "620325e81696fca140195b74929ed9eda2d5eb16",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 48.99,
          "commitsBetweenForRepo": 441,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,53 @@\n-  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n-      NodeType type, RMContainer reservedContainer) {\n+  private boolean canAssign(SchedulerRequestKey schedulerKey,\n+      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d\n-          application.getResourceRequest(priority, ResourceRequest.ANY);\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n+          application.getResourceRequest(schedulerKey, ResourceRequest.ANY);\n+      long missedOpportunities \u003d\n+          application.getSchedulingOpportunities(schedulerKey);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers();\n \n       float localityWaitFactor \u003d\n-          getLocalityWaitFactor(priority, rmContext.getScheduler()\n+          getLocalityWaitFactor(schedulerKey, rmContext.getScheduler()\n               .getNumClusterNodes());\n-      // Cap the delay by the number of nodes in the cluster. Under most conditions\n-      // this means we will consider each node in the cluster before\n+      // Cap the delay by the number of nodes in the cluster. Under most\n+      // conditions this means we will consider each node in the cluster before\n       // accepting an off-switch assignment.\n       return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n           (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack\n     ResourceRequest rackLocalRequest \u003d\n-        application.getResourceRequest(priority, node.getRackName());\n+        application.getResourceRequest(schedulerKey, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n+      long missedOpportunities \u003d\n+          application.getSchedulingOpportunities(schedulerKey);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d\n-          application.getResourceRequest(priority, node.getNodeName());\n+          application.getResourceRequest(schedulerKey, node.getNodeName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean canAssign(SchedulerRequestKey schedulerKey,\n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d\n          application.getResourceRequest(schedulerKey, ResourceRequest.ANY);\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n\n      float localityWaitFactor \u003d\n          getLocalityWaitFactor(schedulerKey, rmContext.getScheduler()\n              .getNumClusterNodes());\n      // Cap the delay by the number of nodes in the cluster. Under most\n      // conditions this means we will consider each node in the cluster before\n      // accepting an off-switch assignment.\n      return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n          (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack\n    ResourceRequest rackLocalRequest \u003d\n        application.getResourceRequest(schedulerKey, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d\n          application.getResourceRequest(schedulerKey, node.getNodeName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[priority-Priority, node-FiCaSchedulerNode, type-NodeType, reservedContainer-RMContainer]",
            "newValue": "[schedulerKey-SchedulerRequestKey, node-FiCaSchedulerNode, type-NodeType, reservedContainer-RMContainer]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
          "commitDate": "26/07/16 2:54 PM",
          "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "07/06/16 3:06 PM",
          "commitNameOld": "620325e81696fca140195b74929ed9eda2d5eb16",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 48.99,
          "commitsBetweenForRepo": 441,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,53 @@\n-  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n-      NodeType type, RMContainer reservedContainer) {\n+  private boolean canAssign(SchedulerRequestKey schedulerKey,\n+      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d\n-          application.getResourceRequest(priority, ResourceRequest.ANY);\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n+          application.getResourceRequest(schedulerKey, ResourceRequest.ANY);\n+      long missedOpportunities \u003d\n+          application.getSchedulingOpportunities(schedulerKey);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers();\n \n       float localityWaitFactor \u003d\n-          getLocalityWaitFactor(priority, rmContext.getScheduler()\n+          getLocalityWaitFactor(schedulerKey, rmContext.getScheduler()\n               .getNumClusterNodes());\n-      // Cap the delay by the number of nodes in the cluster. Under most conditions\n-      // this means we will consider each node in the cluster before\n+      // Cap the delay by the number of nodes in the cluster. Under most\n+      // conditions this means we will consider each node in the cluster before\n       // accepting an off-switch assignment.\n       return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n           (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack\n     ResourceRequest rackLocalRequest \u003d\n-        application.getResourceRequest(priority, node.getRackName());\n+        application.getResourceRequest(schedulerKey, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n+      long missedOpportunities \u003d\n+          application.getSchedulingOpportunities(schedulerKey);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d\n-          application.getResourceRequest(priority, node.getNodeName());\n+          application.getResourceRequest(schedulerKey, node.getNodeName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean canAssign(SchedulerRequestKey schedulerKey,\n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d\n          application.getResourceRequest(schedulerKey, ResourceRequest.ANY);\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n\n      float localityWaitFactor \u003d\n          getLocalityWaitFactor(schedulerKey, rmContext.getScheduler()\n              .getNumClusterNodes());\n      // Cap the delay by the number of nodes in the cluster. Under most\n      // conditions this means we will consider each node in the cluster before\n      // accepting an off-switch assignment.\n      return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n          (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack\n    ResourceRequest rackLocalRequest \u003d\n        application.getResourceRequest(schedulerKey, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d\n          application.getSchedulingOpportunities(schedulerKey);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d\n          application.getResourceRequest(schedulerKey, node.getNodeName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "796638d9bc86235b9f3e5d1a3a9a25bbf5c04d1c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4287. Capacity Scheduler: Rack Locality improvement (Nathan Roberts via wangda)\n",
      "commitDate": "12/11/15 11:09 AM",
      "commitName": "796638d9bc86235b9f3e5d1a3a9a25bbf5c04d1c",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "23/09/15 1:29 PM",
      "commitNameOld": "89cab1ba5f0671f8ef30dbe7432079c18362b434",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 49.94,
      "commitsBetweenForRepo": 405,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,51 @@\n   private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n       NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d\n           application.getResourceRequest(priority, ResourceRequest.ANY);\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers();\n \n       float localityWaitFactor \u003d\n           getLocalityWaitFactor(priority, rmContext.getScheduler()\n               .getNumClusterNodes());\n-\n-      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n+      // Cap the delay by the number of nodes in the cluster. Under most conditions\n+      // this means we will consider each node in the cluster before\n+      // accepting an off-switch assignment.\n+      return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n+          (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack\n     ResourceRequest rackLocalRequest \u003d\n         application.getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d\n           application.getResourceRequest(priority, node.getNodeName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n      NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d\n          application.getResourceRequest(priority, ResourceRequest.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n\n      float localityWaitFactor \u003d\n          getLocalityWaitFactor(priority, rmContext.getScheduler()\n              .getNumClusterNodes());\n      // Cap the delay by the number of nodes in the cluster. Under most conditions\n      // this means we will consider each node in the cluster before\n      // accepting an off-switch assignment.\n      return (Math.min(rmContext.getScheduler().getNumClusterNodes(),\n          (requiredContainers * localityWaitFactor)) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack\n    ResourceRequest rackLocalRequest \u003d\n        application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d\n          application.getResourceRequest(priority, node.getNodeName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "ba2313d6145a1234777938a747187373f4cd58d9": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "YARN-3983. Refactored CapacityScheduleri#FiCaSchedulerApp to easier extend container allocation logic. Contributed by Wangda Tan\n",
      "commitDate": "05/08/15 1:47 PM",
      "commitName": "ba2313d6145a1234777938a747187373f4cd58d9",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "YARN-3983. Refactored CapacityScheduleri#FiCaSchedulerApp to easier extend container allocation logic. Contributed by Wangda Tan\n",
          "commitDate": "05/08/15 1:47 PM",
          "commitName": "ba2313d6145a1234777938a747187373f4cd58d9",
          "commitAuthor": "Jian He",
          "commitDateOld": "05/08/15 12:57 PM",
          "commitNameOld": "f271d377357ad680924d19f07e6c8315e7c89bae",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,48 @@\n   private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n       NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d\n-          getResourceRequest(priority, ResourceRequest.ANY);\n-      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n+          application.getResourceRequest(priority, ResourceRequest.ANY);\n+      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers();\n \n       float localityWaitFactor \u003d\n-          getLocalityWaitFactor(priority, scheduler.getNumClusterNodes());\n+          getLocalityWaitFactor(priority, rmContext.getScheduler()\n+              .getNumClusterNodes());\n \n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack\n     ResourceRequest rackLocalRequest \u003d\n-        getResourceRequest(priority, node.getRackName());\n+        application.getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n-      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n+      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d\n-          getResourceRequest(priority, node.getNodeName());\n+          application.getResourceRequest(priority, node.getNodeName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n      NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d\n          application.getResourceRequest(priority, ResourceRequest.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n\n      float localityWaitFactor \u003d\n          getLocalityWaitFactor(priority, rmContext.getScheduler()\n              .getNumClusterNodes());\n\n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack\n    ResourceRequest rackLocalRequest \u003d\n        application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d\n          application.getResourceRequest(priority, node.getNodeName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {
            "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
            "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
            "oldMethodName": "canAssign",
            "newMethodName": "canAssign"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3983. Refactored CapacityScheduleri#FiCaSchedulerApp to easier extend container allocation logic. Contributed by Wangda Tan\n",
          "commitDate": "05/08/15 1:47 PM",
          "commitName": "ba2313d6145a1234777938a747187373f4cd58d9",
          "commitAuthor": "Jian He",
          "commitDateOld": "05/08/15 12:57 PM",
          "commitNameOld": "f271d377357ad680924d19f07e6c8315e7c89bae",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,48 @@\n   private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n       NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d\n-          getResourceRequest(priority, ResourceRequest.ANY);\n-      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n+          application.getResourceRequest(priority, ResourceRequest.ANY);\n+      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers();\n \n       float localityWaitFactor \u003d\n-          getLocalityWaitFactor(priority, scheduler.getNumClusterNodes());\n+          getLocalityWaitFactor(priority, rmContext.getScheduler()\n+              .getNumClusterNodes());\n \n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack\n     ResourceRequest rackLocalRequest \u003d\n-        getResourceRequest(priority, node.getRackName());\n+        application.getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n-      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n+      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d\n-          getResourceRequest(priority, node.getNodeName());\n+          application.getResourceRequest(priority, node.getNodeName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n      NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d\n          application.getResourceRequest(priority, ResourceRequest.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n\n      float localityWaitFactor \u003d\n          getLocalityWaitFactor(priority, rmContext.getScheduler()\n              .getNumClusterNodes());\n\n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack\n    ResourceRequest rackLocalRequest \u003d\n        application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d\n          application.getResourceRequest(priority, node.getNodeName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
      "commitDate": "24/07/15 2:00 PM",
      "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,47 @@\n-  boolean canAssign(FiCaSchedulerApp application, Priority priority, \n-      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n+  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n+      NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n-      ResourceRequest offSwitchRequest \u003d \n-          application.getResourceRequest(priority, ResourceRequest.ANY);\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n-      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n-      \n-      float localityWaitFactor \u003d \n-        application.getLocalityWaitFactor(priority, \n-            scheduler.getNumClusterNodes());\n-      \n+      ResourceRequest offSwitchRequest \u003d\n+          getResourceRequest(priority, ResourceRequest.ANY);\n+      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n+      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n+\n+      float localityWaitFactor \u003d\n+          getLocalityWaitFactor(priority, scheduler.getNumClusterNodes());\n+\n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n-    // Check if we need containers on this rack \n-    ResourceRequest rackLocalRequest \u003d \n-      application.getResourceRequest(priority, node.getRackName());\n+    // Check if we need containers on this rack\n+    ResourceRequest rackLocalRequest \u003d\n+        getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n-      \n+\n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n+      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n-      ResourceRequest nodeLocalRequest \u003d \n-        application.getResourceRequest(priority, node.getNodeName());\n+      ResourceRequest nodeLocalRequest \u003d\n+          getResourceRequest(priority, node.getNodeName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n      NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d\n          getResourceRequest(priority, ResourceRequest.ANY);\n      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n\n      float localityWaitFactor \u003d\n          getLocalityWaitFactor(priority, scheduler.getNumClusterNodes());\n\n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack\n    ResourceRequest rackLocalRequest \u003d\n        getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d\n          getResourceRequest(priority, node.getNodeName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
            "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
            "oldMethodName": "canAssign",
            "newMethodName": "canAssign"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,47 @@\n-  boolean canAssign(FiCaSchedulerApp application, Priority priority, \n-      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n+  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n+      NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n-      ResourceRequest offSwitchRequest \u003d \n-          application.getResourceRequest(priority, ResourceRequest.ANY);\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n-      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n-      \n-      float localityWaitFactor \u003d \n-        application.getLocalityWaitFactor(priority, \n-            scheduler.getNumClusterNodes());\n-      \n+      ResourceRequest offSwitchRequest \u003d\n+          getResourceRequest(priority, ResourceRequest.ANY);\n+      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n+      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n+\n+      float localityWaitFactor \u003d\n+          getLocalityWaitFactor(priority, scheduler.getNumClusterNodes());\n+\n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n-    // Check if we need containers on this rack \n-    ResourceRequest rackLocalRequest \u003d \n-      application.getResourceRequest(priority, node.getRackName());\n+    // Check if we need containers on this rack\n+    ResourceRequest rackLocalRequest \u003d\n+        getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n-      \n+\n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n+      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n-      ResourceRequest nodeLocalRequest \u003d \n-        application.getResourceRequest(priority, node.getNodeName());\n+      ResourceRequest nodeLocalRequest \u003d\n+          getResourceRequest(priority, node.getNodeName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n      NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d\n          getResourceRequest(priority, ResourceRequest.ANY);\n      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n\n      float localityWaitFactor \u003d\n          getLocalityWaitFactor(priority, scheduler.getNumClusterNodes());\n\n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack\n    ResourceRequest rackLocalRequest \u003d\n        getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d\n          getResourceRequest(priority, node.getNodeName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,47 @@\n-  boolean canAssign(FiCaSchedulerApp application, Priority priority, \n-      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n+  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n+      NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n-      ResourceRequest offSwitchRequest \u003d \n-          application.getResourceRequest(priority, ResourceRequest.ANY);\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n-      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n-      \n-      float localityWaitFactor \u003d \n-        application.getLocalityWaitFactor(priority, \n-            scheduler.getNumClusterNodes());\n-      \n+      ResourceRequest offSwitchRequest \u003d\n+          getResourceRequest(priority, ResourceRequest.ANY);\n+      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n+      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n+\n+      float localityWaitFactor \u003d\n+          getLocalityWaitFactor(priority, scheduler.getNumClusterNodes());\n+\n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n-    // Check if we need containers on this rack \n-    ResourceRequest rackLocalRequest \u003d \n-      application.getResourceRequest(priority, node.getRackName());\n+    // Check if we need containers on this rack\n+    ResourceRequest rackLocalRequest \u003d\n+        getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n-      \n+\n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n+      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n-      ResourceRequest nodeLocalRequest \u003d \n-        application.getResourceRequest(priority, node.getNodeName());\n+      ResourceRequest nodeLocalRequest \u003d\n+          getResourceRequest(priority, node.getNodeName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n      NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d\n          getResourceRequest(priority, ResourceRequest.ANY);\n      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n\n      float localityWaitFactor \u003d\n          getLocalityWaitFactor(priority, scheduler.getNumClusterNodes());\n\n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack\n    ResourceRequest rackLocalRequest \u003d\n        getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d\n          getResourceRequest(priority, node.getNodeName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,47 @@\n-  boolean canAssign(FiCaSchedulerApp application, Priority priority, \n-      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n+  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n+      NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n-      ResourceRequest offSwitchRequest \u003d \n-          application.getResourceRequest(priority, ResourceRequest.ANY);\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n-      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n-      \n-      float localityWaitFactor \u003d \n-        application.getLocalityWaitFactor(priority, \n-            scheduler.getNumClusterNodes());\n-      \n+      ResourceRequest offSwitchRequest \u003d\n+          getResourceRequest(priority, ResourceRequest.ANY);\n+      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n+      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n+\n+      float localityWaitFactor \u003d\n+          getLocalityWaitFactor(priority, scheduler.getNumClusterNodes());\n+\n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n-    // Check if we need containers on this rack \n-    ResourceRequest rackLocalRequest \u003d \n-      application.getResourceRequest(priority, node.getRackName());\n+    // Check if we need containers on this rack\n+    ResourceRequest rackLocalRequest \u003d\n+        getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n-      \n+\n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n-      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n+      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n       return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n-      ResourceRequest nodeLocalRequest \u003d \n-        application.getResourceRequest(priority, node.getNodeName());\n+      ResourceRequest nodeLocalRequest \u003d\n+          getResourceRequest(priority, node.getNodeName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean canAssign(Priority priority, FiCaSchedulerNode node,\n      NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d\n          getResourceRequest(priority, ResourceRequest.ANY);\n      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers();\n\n      float localityWaitFactor \u003d\n          getLocalityWaitFactor(priority, scheduler.getNumClusterNodes());\n\n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack\n    ResourceRequest rackLocalRequest \u003d\n        getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n\n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d getSchedulingOpportunities(priority);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d\n          getResourceRequest(priority, node.getNodeName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "[application-FiCaSchedulerApp, priority-Priority, node-FiCaSchedulerNode, type-NodeType, reservedContainer-RMContainer]",
            "newValue": "[priority-Priority, node-FiCaSchedulerNode, type-NodeType, reservedContainer-RMContainer]"
          }
        }
      ]
    },
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3361. CapacityScheduler side changes to support non-exclusive node labels. Contributed by Wangda Tan\n",
      "commitDate": "14/04/15 11:45 AM",
      "commitName": "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
      "commitAuthor": "Jian He",
      "commitDateOld": "09/04/15 11:38 PM",
      "commitNameOld": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
      "commitAuthorOld": "Xuan",
      "daysBetweenCommits": 4.51,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,48 @@\n   boolean canAssign(FiCaSchedulerApp application, Priority priority, \n       FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d \n           application.getResourceRequest(priority, ResourceRequest.ANY);\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n       \n       float localityWaitFactor \u003d \n         application.getLocalityWaitFactor(priority, \n             scheduler.getNumClusterNodes());\n       \n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack \n     ResourceRequest rackLocalRequest \u003d \n       application.getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n       \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n-      return (\n-          Math.min(scheduler.getNumClusterNodes(), getNodeLocalityDelay()) \u003c \n-          missedOpportunities\n-          );\n+      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d \n         application.getResourceRequest(priority, node.getNodeName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean canAssign(FiCaSchedulerApp application, Priority priority, \n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d \n          application.getResourceRequest(priority, ResourceRequest.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor \u003d \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest \u003d \n      application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n      \n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      return getActualNodeLocalityDelay() \u003c missedOpportunities;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d \n        application.getResourceRequest(priority, node.getNodeName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1008. MiniYARNCluster with multiple nodemanagers, all nodes have same key for allocations. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1517563 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/08/13 8:39 AM",
      "commitName": "942e2ebaa54306ffc5b0ffb403e552764a40d58c",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "22/07/13 4:49 PM",
      "commitNameOld": "5b3bb05fbeb7ed4671f4d3a59677788f7fda43d0",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 34.66,
      "commitsBetweenForRepo": 214,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   boolean canAssign(FiCaSchedulerApp application, Priority priority, \n       FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d \n           application.getResourceRequest(priority, ResourceRequest.ANY);\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n       \n       float localityWaitFactor \u003d \n         application.getLocalityWaitFactor(priority, \n             scheduler.getNumClusterNodes());\n       \n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack \n     ResourceRequest rackLocalRequest \u003d \n       application.getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n       \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       return (\n           Math.min(scheduler.getNumClusterNodes(), getNodeLocalityDelay()) \u003c \n           missedOpportunities\n           );\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d \n-        application.getResourceRequest(priority, node.getHostName());\n+        application.getResourceRequest(priority, node.getNodeName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean canAssign(FiCaSchedulerApp application, Priority priority, \n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d \n          application.getResourceRequest(priority, ResourceRequest.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor \u003d \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest \u003d \n      application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n      \n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      return (\n          Math.min(scheduler.getNumClusterNodes(), getNodeLocalityDelay()) \u003c \n          missedOpportunities\n          );\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d \n        application.getResourceRequest(priority, node.getNodeName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "520033b1cd81c76b38fcdcfcfeed16158db4bbba": {
      "type": "Ybodychange",
      "commitMessage": "YARN-450. Define value for * in the scheduling protocol (Zhijie Shen via bikas)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1462271 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/03/13 12:44 PM",
      "commitName": "520033b1cd81c76b38fcdcfcfeed16158db4bbba",
      "commitAuthor": "Bikas Saha",
      "commitDateOld": "27/03/13 11:38 AM",
      "commitNameOld": "d0bbff6c32592cb5d49d7be8d8a7346788a9ba19",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 1.05,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   boolean canAssign(FiCaSchedulerApp application, Priority priority, \n       FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d \n-          application.getResourceRequest(priority, RMNode.ANY);\n+          application.getResourceRequest(priority, ResourceRequest.ANY);\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n       \n       float localityWaitFactor \u003d \n         application.getLocalityWaitFactor(priority, \n             scheduler.getNumClusterNodes());\n       \n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack \n     ResourceRequest rackLocalRequest \u003d \n       application.getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n       \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       // \u0027Delay\u0027 rack-local just a little bit...\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       return (\n           Math.min(scheduler.getNumClusterNodes(), getNodeLocalityDelay()) \u003c \n           missedOpportunities\n           );\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d \n         application.getResourceRequest(priority, node.getHostName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean canAssign(FiCaSchedulerApp application, Priority priority, \n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d \n          application.getResourceRequest(priority, ResourceRequest.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor \u003d \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest \u003d \n      application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n      \n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      return (\n          Math.min(scheduler.getNumClusterNodes(), getNodeLocalityDelay()) \u003c \n          missedOpportunities\n          );\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d \n        application.getResourceRequest(priority, node.getHostName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "56d72dfdf1f33272f32794a02f3b08cfeee85343": {
      "type": "Ybodychange",
      "commitMessage": "YARN-80. Add support for delaying rack-local containers in CapacityScheduler. Contributed by Arun C. Murthy. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1381872 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/09/12 10:10 PM",
      "commitName": "56d72dfdf1f33272f32794a02f3b08cfeee85343",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/12 7:18 PM",
      "commitNameOld": "ffd2e01604be814fa3db1dded7cd7cff26a79b1e",
      "commitAuthorOld": "Siddharth Seth",
      "daysBetweenCommits": 13.12,
      "commitsBetweenForRepo": 85,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,51 @@\n   boolean canAssign(FiCaSchedulerApp application, Priority priority, \n       FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d \n           application.getResourceRequest(priority, RMNode.ANY);\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n       \n       float localityWaitFactor \u003d \n         application.getLocalityWaitFactor(priority, \n             scheduler.getNumClusterNodes());\n       \n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack \n     ResourceRequest rackLocalRequest \u003d \n       application.getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n       \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n-      return true;\n+      // \u0027Delay\u0027 rack-local just a little bit...\n+      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n+      return (\n+          Math.min(scheduler.getNumClusterNodes(), getNodeLocalityDelay()) \u003c \n+          missedOpportunities\n+          );\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d \n         application.getResourceRequest(priority, node.getHostName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean canAssign(FiCaSchedulerApp application, Priority priority, \n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d \n          application.getResourceRequest(priority, RMNode.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor \u003d \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest \u003d \n      application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n      \n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      // \u0027Delay\u0027 rack-local just a little bit...\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      return (\n          Math.min(scheduler.getNumClusterNodes(), getNodeLocalityDelay()) \u003c \n          missedOpportunities\n          );\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d \n        application.getResourceRequest(priority, node.getHostName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "e1fdf62123625e4ba399af02f8aad500637d29d1": {
      "type": "Yfilerename",
      "commitMessage": "YARN-1. Promote YARN to be a sub-project of Apache Hadoop.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370666 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 10:22 PM",
      "commitName": "e1fdf62123625e4ba399af02f8aad500637d29d1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "07/08/12 7:53 PM",
      "commitNameOld": "34554d1e11ee1d5b564d7d9ed3e6d55931d72749",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  boolean canAssign(FiCaSchedulerApp application, Priority priority, \n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d \n          application.getResourceRequest(priority, RMNode.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor \u003d \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest \u003d \n      application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n      \n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      return true;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d \n        application.getResourceRequest(priority, node.getHostName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
      }
    },
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76": {
      "type": "Yparameterchange",
      "commitMessage": "MAPREDUCE-4440. Changed SchedulerApp and SchedulerNode to be a minimal interface to allow schedulers to maintain their own.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/07/12 6:43 PM",
      "commitName": "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "10/07/12 2:26 PM",
      "commitNameOld": "3bfb26ad3b5ac46f992a632541c97ca2bc897638",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 6.18,
      "commitsBetweenForRepo": 55,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n-  boolean canAssign(SchedulerApp application, Priority priority, \n-      SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n+  boolean canAssign(FiCaSchedulerApp application, Priority priority, \n+      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       if (reservedContainer !\u003d null) {\n         return true;\n       }\n \n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d \n           application.getResourceRequest(priority, RMNode.ANY);\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n       \n       float localityWaitFactor \u003d \n         application.getLocalityWaitFactor(priority, \n             scheduler.getNumClusterNodes());\n       \n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack \n     ResourceRequest rackLocalRequest \u003d \n       application.getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n       \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       return true;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d \n         application.getResourceRequest(priority, node.getHostName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean canAssign(FiCaSchedulerApp application, Priority priority, \n      FiCaSchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d \n          application.getResourceRequest(priority, RMNode.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor \u003d \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest \u003d \n      application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n      \n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      return true;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d \n        application.getResourceRequest(priority, node.getHostName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {
        "oldValue": "[application-SchedulerApp, priority-Priority, node-SchedulerNode, type-NodeType, reservedContainer-RMContainer]",
        "newValue": "[application-FiCaSchedulerApp, priority-Priority, node-FiCaSchedulerNode, type-NodeType, reservedContainer-RMContainer]"
      }
    },
    "9a10b4e773ac937b59b458343457bbbd686d7f1e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4144. Fix a NPE in the ResourceManager when handling node updates. (Contributed by Jason Lowe)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1325991 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/04/12 3:24 PM",
      "commitName": "9a10b4e773ac937b59b458343457bbbd686d7f1e",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "09/03/12 3:34 PM",
      "commitNameOld": "ed4c222d5c0aeb4a46a2dd8a6342c85e88f31d3b",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 34.95,
      "commitsBetweenForRepo": 257,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,46 @@\n   boolean canAssign(SchedulerApp application, Priority priority, \n       SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n-    // Reserved... \n-    if (reservedContainer !\u003d null) {\n-      return true;\n-    }\n-    \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n+      if (reservedContainer !\u003d null) {\n+        return true;\n+      }\n+\n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d \n           application.getResourceRequest(priority, RMNode.ANY);\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n       \n       float localityWaitFactor \u003d \n         application.getLocalityWaitFactor(priority, \n             scheduler.getNumClusterNodes());\n       \n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack \n     ResourceRequest rackLocalRequest \u003d \n       application.getResourceRequest(priority, node.getRackName());\n     if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n       return false;\n     }\n       \n     // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n       return true;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d \n         application.getResourceRequest(priority, node.getHostName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean canAssign(SchedulerApp application, Priority priority, \n      SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      if (reservedContainer !\u003d null) {\n        return true;\n      }\n\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d \n          application.getResourceRequest(priority, RMNode.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor \u003d \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest \u003d \n      application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n      \n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      return true;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d \n        application.getResourceRequest(priority, node.getHostName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "5183e881097b37b723f07f4d6af06721a326bea1": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3005. Fix both FifoScheduler and CapacityScheduler to correctly enforce locality constraints.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1170879 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/09/11 3:46 PM",
      "commitName": "5183e881097b37b723f07f4d6af06721a326bea1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "11/09/11 10:26 AM",
      "commitNameOld": "817ead65b99f465fc2dfa18072cf23cadf5f05d0",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 3.22,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,47 @@\n   boolean canAssign(SchedulerApp application, Priority priority, \n       SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n     // Reserved... \n     if (reservedContainer !\u003d null) {\n       return true;\n     }\n     \n     // Clearly we need containers for this application...\n     if (type \u003d\u003d NodeType.OFF_SWITCH) {\n       // \u0027Delay\u0027 off-switch\n       ResourceRequest offSwitchRequest \u003d \n           application.getResourceRequest(priority, RMNode.ANY);\n       long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n       long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n       \n       float localityWaitFactor \u003d \n         application.getLocalityWaitFactor(priority, \n             scheduler.getNumClusterNodes());\n       \n       return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n     }\n \n     // Check if we need containers on this rack \n     ResourceRequest rackLocalRequest \u003d \n       application.getResourceRequest(priority, node.getRackName());\n+    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n+      return false;\n+    }\n+      \n+    // If we are here, we do need containers on this rack for RACK_LOCAL req\n     if (type \u003d\u003d NodeType.RACK_LOCAL) {\n-      if (rackLocalRequest \u003d\u003d null) {\n-        return false;\n-      } else {\n-        return rackLocalRequest.getNumContainers() \u003e 0;      \n-      }\n+      return true;\n     }\n \n     // Check if we need containers on this host\n     if (type \u003d\u003d NodeType.NODE_LOCAL) {\n-      // First: Do we need containers on this rack?\n-      if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() \u003d\u003d 0) {\n-        return false;\n-      }\n-      \n       // Now check if we need containers on this host...\n       ResourceRequest nodeLocalRequest \u003d \n         application.getResourceRequest(priority, node.getHostName());\n       if (nodeLocalRequest !\u003d null) {\n         return nodeLocalRequest.getNumContainers() \u003e 0;\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean canAssign(SchedulerApp application, Priority priority, \n      SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Reserved... \n    if (reservedContainer !\u003d null) {\n      return true;\n    }\n    \n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d \n          application.getResourceRequest(priority, RMNode.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor \u003d \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest \u003d \n      application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalRequest \u003d\u003d null || rackLocalRequest.getNumContainers() \u003c\u003d 0) {\n      return false;\n    }\n      \n    // If we are here, we do need containers on this rack for RACK_LOCAL req\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      return true;\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d \n        application.getResourceRequest(priority, node.getHostName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  boolean canAssign(SchedulerApp application, Priority priority, \n      SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Reserved... \n    if (reservedContainer !\u003d null) {\n      return true;\n    }\n    \n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d \n          application.getResourceRequest(priority, RMNode.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor \u003d \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest \u003d \n      application.getResourceRequest(priority, node.getRackName());\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      if (rackLocalRequest \u003d\u003d null) {\n        return false;\n      } else {\n        return rackLocalRequest.getNumContainers() \u003e 0;      \n      }\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // First: Do we need containers on this rack?\n      if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() \u003d\u003d 0) {\n        return false;\n      }\n      \n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d \n        application.getResourceRequest(priority, node.getHostName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
        "newPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,51 @@\n+  boolean canAssign(SchedulerApp application, Priority priority, \n+      SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n+\n+    // Reserved... \n+    if (reservedContainer !\u003d null) {\n+      return true;\n+    }\n+    \n+    // Clearly we need containers for this application...\n+    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n+      // \u0027Delay\u0027 off-switch\n+      ResourceRequest offSwitchRequest \u003d \n+          application.getResourceRequest(priority, RMNode.ANY);\n+      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n+      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n+      \n+      float localityWaitFactor \u003d \n+        application.getLocalityWaitFactor(priority, \n+            scheduler.getNumClusterNodes());\n+      \n+      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n+    }\n+\n+    // Check if we need containers on this rack \n+    ResourceRequest rackLocalRequest \u003d \n+      application.getResourceRequest(priority, node.getRackName());\n+    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n+      if (rackLocalRequest \u003d\u003d null) {\n+        return false;\n+      } else {\n+        return rackLocalRequest.getNumContainers() \u003e 0;      \n+      }\n+    }\n+\n+    // Check if we need containers on this host\n+    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n+      // First: Do we need containers on this rack?\n+      if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() \u003d\u003d 0) {\n+        return false;\n+      }\n+      \n+      // Now check if we need containers on this host...\n+      ResourceRequest nodeLocalRequest \u003d \n+        application.getResourceRequest(priority, node.getHostName());\n+      if (nodeLocalRequest !\u003d null) {\n+        return nodeLocalRequest.getNumContainers() \u003e 0;\n+      }\n+    }\n+\n+    return false;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  boolean canAssign(SchedulerApp application, Priority priority, \n      SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n\n    // Reserved... \n    if (reservedContainer !\u003d null) {\n      return true;\n    }\n    \n    // Clearly we need containers for this application...\n    if (type \u003d\u003d NodeType.OFF_SWITCH) {\n      // \u0027Delay\u0027 off-switch\n      ResourceRequest offSwitchRequest \u003d \n          application.getResourceRequest(priority, RMNode.ANY);\n      long missedOpportunities \u003d application.getSchedulingOpportunities(priority);\n      long requiredContainers \u003d offSwitchRequest.getNumContainers(); \n      \n      float localityWaitFactor \u003d \n        application.getLocalityWaitFactor(priority, \n            scheduler.getNumClusterNodes());\n      \n      return ((requiredContainers * localityWaitFactor) \u003c missedOpportunities);\n    }\n\n    // Check if we need containers on this rack \n    ResourceRequest rackLocalRequest \u003d \n      application.getResourceRequest(priority, node.getRackName());\n    if (type \u003d\u003d NodeType.RACK_LOCAL) {\n      if (rackLocalRequest \u003d\u003d null) {\n        return false;\n      } else {\n        return rackLocalRequest.getNumContainers() \u003e 0;      \n      }\n    }\n\n    // Check if we need containers on this host\n    if (type \u003d\u003d NodeType.NODE_LOCAL) {\n      // First: Do we need containers on this rack?\n      if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() \u003d\u003d 0) {\n        return false;\n      }\n      \n      // Now check if we need containers on this host...\n      ResourceRequest nodeLocalRequest \u003d \n        application.getResourceRequest(priority, node.getHostName());\n      if (nodeLocalRequest !\u003d null) {\n        return nodeLocalRequest.getNumContainers() \u003e 0;\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
    }
  }
}