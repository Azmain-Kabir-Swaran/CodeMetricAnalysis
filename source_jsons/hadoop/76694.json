{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RawErasureEncoder.java",
  "functionName": "encode",
  "functionId": "encode___inputs-ByteBuffer[]__outputs-ByteBuffer[]",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java",
  "functionStartLine": 68,
  "functionEndLine": 99,
  "numCommitsSeen": 16,
  "timeTaken": 3789,
  "changeHistory": [
    "31ebccc96238136560f4210bdf6766fe18e0650c",
    "77202fa1035a54496d11d07472fbc399148ff630",
    "c52b407cbffc8693738b31c6cc4e71751efd70e8",
    "5eca6dece67620f990f3306b6caaf09f317b38f6",
    "29495cb8f6b940caa9964c39a290ef233ce1ec7c",
    "4ad484883f773c702a1874fc12816ef1a4a54136",
    "343c0e76fcd95ac739ca7cd6742c9d617e19fc37",
    "09c3a375bafa481e88d1317388a73c46950164c9",
    "e50bcea83d5f6b02ab03b06a3fbf1ed6b8ff4871"
  ],
  "changeHistoryShort": {
    "31ebccc96238136560f4210bdf6766fe18e0650c": "Yexceptionschange",
    "77202fa1035a54496d11d07472fbc399148ff630": "Ymultichange(Ymovefromfile,Ybodychange)",
    "c52b407cbffc8693738b31c6cc4e71751efd70e8": "Ybodychange",
    "5eca6dece67620f990f3306b6caaf09f317b38f6": "Ybodychange",
    "29495cb8f6b940caa9964c39a290ef233ce1ec7c": "Ybodychange",
    "4ad484883f773c702a1874fc12816ef1a4a54136": "Ybodychange",
    "343c0e76fcd95ac739ca7cd6742c9d617e19fc37": "Ybodychange",
    "09c3a375bafa481e88d1317388a73c46950164c9": "Ybodychange",
    "e50bcea83d5f6b02ab03b06a3fbf1ed6b8ff4871": "Yintroduced"
  },
  "changeHistoryDetails": {
    "31ebccc96238136560f4210bdf6766fe18e0650c": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-12613. Native EC coder should implement release() as idempotent function. (Lei (Eddy) Xu)\n",
      "commitDate": "16/10/17 7:44 PM",
      "commitName": "31ebccc96238136560f4210bdf6766fe18e0650c",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "26/05/16 10:23 PM",
      "commitNameOld": "77202fa1035a54496d11d07472fbc399148ff630",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 507.89,
      "commitsBetweenForRepo": 3419,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,32 @@\n-  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n+  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs)\n+      throws IOException {\n     ByteBufferEncodingState bbeState \u003d new ByteBufferEncodingState(\n         this, inputs, outputs);\n \n     boolean usingDirectBuffer \u003d bbeState.usingDirectBuffer;\n     int dataLen \u003d bbeState.encodeLength;\n     if (dataLen \u003d\u003d 0) {\n       return;\n     }\n \n     int[] inputPositions \u003d new int[inputs.length];\n     for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n       if (inputs[i] !\u003d null) {\n         inputPositions[i] \u003d inputs[i].position();\n       }\n     }\n \n     if (usingDirectBuffer) {\n       doEncode(bbeState);\n     } else {\n       ByteArrayEncodingState baeState \u003d bbeState.convertToByteArrayState();\n       doEncode(baeState);\n     }\n \n     for (int i \u003d 0; i \u003c inputs.length; i++) {\n       if (inputs[i] !\u003d null) {\n         // dataLen bytes consumed\n         inputs[i].position(inputPositions[i] + dataLen);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs)\n      throws IOException {\n    ByteBufferEncodingState bbeState \u003d new ByteBufferEncodingState(\n        this, inputs, outputs);\n\n    boolean usingDirectBuffer \u003d bbeState.usingDirectBuffer;\n    int dataLen \u003d bbeState.encodeLength;\n    if (dataLen \u003d\u003d 0) {\n      return;\n    }\n\n    int[] inputPositions \u003d new int[inputs.length];\n    for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n      if (inputs[i] !\u003d null) {\n        inputPositions[i] \u003d inputs[i].position();\n      }\n    }\n\n    if (usingDirectBuffer) {\n      doEncode(bbeState);\n    } else {\n      ByteArrayEncodingState baeState \u003d bbeState.convertToByteArrayState();\n      doEncode(baeState);\n    }\n\n    for (int i \u003d 0; i \u003c inputs.length; i++) {\n      if (inputs[i] !\u003d null) {\n        // dataLen bytes consumed\n        inputs[i].position(inputPositions[i] + dataLen);\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[IOException]"
      }
    },
    "77202fa1035a54496d11d07472fbc399148ff630": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
      "commitDate": "26/05/16 10:23 PM",
      "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
          "commitDate": "26/05/16 10:23 PM",
          "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "25/05/16 10:13 PM",
          "commitNameOld": "4f513a4a8ed73beec89b7b28c0cd056ee5f4ed0d",
          "commitAuthorOld": "Karthik Kambatla",
          "daysBetweenCommits": 1.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,31 @@\n   public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n-    checkParameters(inputs, outputs);\n+    ByteBufferEncodingState bbeState \u003d new ByteBufferEncodingState(\n+        this, inputs, outputs);\n \n-    boolean usingDirectBuffer \u003d inputs[0].isDirect();\n-    int dataLen \u003d inputs[0].remaining();\n+    boolean usingDirectBuffer \u003d bbeState.usingDirectBuffer;\n+    int dataLen \u003d bbeState.encodeLength;\n     if (dataLen \u003d\u003d 0) {\n       return;\n     }\n-    checkParameterBuffers(inputs, false, dataLen, usingDirectBuffer, false);\n-    checkParameterBuffers(outputs, false, dataLen, usingDirectBuffer, true);\n \n     int[] inputPositions \u003d new int[inputs.length];\n     for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n       if (inputs[i] !\u003d null) {\n         inputPositions[i] \u003d inputs[i].position();\n       }\n     }\n \n     if (usingDirectBuffer) {\n-      doEncode(inputs, outputs);\n+      doEncode(bbeState);\n     } else {\n-      int[] inputOffsets \u003d new int[inputs.length];\n-      int[] outputOffsets \u003d new int[outputs.length];\n-      byte[][] newInputs \u003d new byte[inputs.length][];\n-      byte[][] newOutputs \u003d new byte[outputs.length][];\n-\n-      ByteBuffer buffer;\n-      for (int i \u003d 0; i \u003c inputs.length; ++i) {\n-        buffer \u003d inputs[i];\n-        inputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n-        newInputs[i] \u003d buffer.array();\n-      }\n-\n-      for (int i \u003d 0; i \u003c outputs.length; ++i) {\n-        buffer \u003d outputs[i];\n-        outputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n-        newOutputs[i] \u003d buffer.array();\n-      }\n-\n-      doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n+      ByteArrayEncodingState baeState \u003d bbeState.convertToByteArrayState();\n+      doEncode(baeState);\n     }\n \n     for (int i \u003d 0; i \u003c inputs.length; i++) {\n       if (inputs[i] !\u003d null) {\n         // dataLen bytes consumed\n         inputs[i].position(inputPositions[i] + dataLen);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n    ByteBufferEncodingState bbeState \u003d new ByteBufferEncodingState(\n        this, inputs, outputs);\n\n    boolean usingDirectBuffer \u003d bbeState.usingDirectBuffer;\n    int dataLen \u003d bbeState.encodeLength;\n    if (dataLen \u003d\u003d 0) {\n      return;\n    }\n\n    int[] inputPositions \u003d new int[inputs.length];\n    for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n      if (inputs[i] !\u003d null) {\n        inputPositions[i] \u003d inputs[i].position();\n      }\n    }\n\n    if (usingDirectBuffer) {\n      doEncode(bbeState);\n    } else {\n      ByteArrayEncodingState baeState \u003d bbeState.convertToByteArrayState();\n      doEncode(baeState);\n    }\n\n    for (int i \u003d 0; i \u003c inputs.length; i++) {\n      if (inputs[i] !\u003d null) {\n        // dataLen bytes consumed\n        inputs[i].position(inputPositions[i] + dataLen);\n      }\n    }\n  }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java",
          "extendedDetails": {
            "oldPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractRawErasureEncoder.java",
            "newPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java",
            "oldMethodName": "encode",
            "newMethodName": "encode"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
          "commitDate": "26/05/16 10:23 PM",
          "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "25/05/16 10:13 PM",
          "commitNameOld": "4f513a4a8ed73beec89b7b28c0cd056ee5f4ed0d",
          "commitAuthorOld": "Karthik Kambatla",
          "daysBetweenCommits": 1.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,31 @@\n   public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n-    checkParameters(inputs, outputs);\n+    ByteBufferEncodingState bbeState \u003d new ByteBufferEncodingState(\n+        this, inputs, outputs);\n \n-    boolean usingDirectBuffer \u003d inputs[0].isDirect();\n-    int dataLen \u003d inputs[0].remaining();\n+    boolean usingDirectBuffer \u003d bbeState.usingDirectBuffer;\n+    int dataLen \u003d bbeState.encodeLength;\n     if (dataLen \u003d\u003d 0) {\n       return;\n     }\n-    checkParameterBuffers(inputs, false, dataLen, usingDirectBuffer, false);\n-    checkParameterBuffers(outputs, false, dataLen, usingDirectBuffer, true);\n \n     int[] inputPositions \u003d new int[inputs.length];\n     for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n       if (inputs[i] !\u003d null) {\n         inputPositions[i] \u003d inputs[i].position();\n       }\n     }\n \n     if (usingDirectBuffer) {\n-      doEncode(inputs, outputs);\n+      doEncode(bbeState);\n     } else {\n-      int[] inputOffsets \u003d new int[inputs.length];\n-      int[] outputOffsets \u003d new int[outputs.length];\n-      byte[][] newInputs \u003d new byte[inputs.length][];\n-      byte[][] newOutputs \u003d new byte[outputs.length][];\n-\n-      ByteBuffer buffer;\n-      for (int i \u003d 0; i \u003c inputs.length; ++i) {\n-        buffer \u003d inputs[i];\n-        inputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n-        newInputs[i] \u003d buffer.array();\n-      }\n-\n-      for (int i \u003d 0; i \u003c outputs.length; ++i) {\n-        buffer \u003d outputs[i];\n-        outputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n-        newOutputs[i] \u003d buffer.array();\n-      }\n-\n-      doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n+      ByteArrayEncodingState baeState \u003d bbeState.convertToByteArrayState();\n+      doEncode(baeState);\n     }\n \n     for (int i \u003d 0; i \u003c inputs.length; i++) {\n       if (inputs[i] !\u003d null) {\n         // dataLen bytes consumed\n         inputs[i].position(inputPositions[i] + dataLen);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n    ByteBufferEncodingState bbeState \u003d new ByteBufferEncodingState(\n        this, inputs, outputs);\n\n    boolean usingDirectBuffer \u003d bbeState.usingDirectBuffer;\n    int dataLen \u003d bbeState.encodeLength;\n    if (dataLen \u003d\u003d 0) {\n      return;\n    }\n\n    int[] inputPositions \u003d new int[inputs.length];\n    for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n      if (inputs[i] !\u003d null) {\n        inputPositions[i] \u003d inputs[i].position();\n      }\n    }\n\n    if (usingDirectBuffer) {\n      doEncode(bbeState);\n    } else {\n      ByteArrayEncodingState baeState \u003d bbeState.convertToByteArrayState();\n      doEncode(baeState);\n    }\n\n    for (int i \u003d 0; i \u003c inputs.length; i++) {\n      if (inputs[i] !\u003d null) {\n        // dataLen bytes consumed\n        inputs[i].position(inputPositions[i] + dataLen);\n      }\n    }\n  }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java",
          "extendedDetails": {}
        }
      ]
    },
    "c52b407cbffc8693738b31c6cc4e71751efd70e8": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12685. Input buffer position after encode/decode not consistent between different kinds of buffers. Contributed by Rui Li.\n\nChange-Id: I713c7b4e3cfae70c04b7e4b292ab53eae348d8d9\n",
      "commitDate": "05/01/16 4:32 PM",
      "commitName": "c52b407cbffc8693738b31c6cc4e71751efd70e8",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "29/10/15 12:04 AM",
      "commitNameOld": "5eca6dece67620f990f3306b6caaf09f317b38f6",
      "commitAuthorOld": "Walter Su",
      "daysBetweenCommits": 68.73,
      "commitsBetweenForRepo": 402,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,49 @@\n   public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n     checkParameters(inputs, outputs);\n \n     boolean usingDirectBuffer \u003d inputs[0].isDirect();\n     int dataLen \u003d inputs[0].remaining();\n     if (dataLen \u003d\u003d 0) {\n       return;\n     }\n     checkParameterBuffers(inputs, false, dataLen, usingDirectBuffer, false);\n     checkParameterBuffers(outputs, false, dataLen, usingDirectBuffer, true);\n \n+    int[] inputPositions \u003d new int[inputs.length];\n+    for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n+      if (inputs[i] !\u003d null) {\n+        inputPositions[i] \u003d inputs[i].position();\n+      }\n+    }\n+\n     if (usingDirectBuffer) {\n       doEncode(inputs, outputs);\n-      return;\n+    } else {\n+      int[] inputOffsets \u003d new int[inputs.length];\n+      int[] outputOffsets \u003d new int[outputs.length];\n+      byte[][] newInputs \u003d new byte[inputs.length][];\n+      byte[][] newOutputs \u003d new byte[outputs.length][];\n+\n+      ByteBuffer buffer;\n+      for (int i \u003d 0; i \u003c inputs.length; ++i) {\n+        buffer \u003d inputs[i];\n+        inputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n+        newInputs[i] \u003d buffer.array();\n+      }\n+\n+      for (int i \u003d 0; i \u003c outputs.length; ++i) {\n+        buffer \u003d outputs[i];\n+        outputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n+        newOutputs[i] \u003d buffer.array();\n+      }\n+\n+      doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n     }\n \n-    int[] inputOffsets \u003d new int[inputs.length];\n-    int[] outputOffsets \u003d new int[outputs.length];\n-    byte[][] newInputs \u003d new byte[inputs.length][];\n-    byte[][] newOutputs \u003d new byte[outputs.length][];\n-\n-    ByteBuffer buffer;\n-    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n-      buffer \u003d inputs[i];\n-      inputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n-      newInputs[i] \u003d buffer.array();\n-    }\n-\n-    for (int i \u003d 0; i \u003c outputs.length; ++i) {\n-      buffer \u003d outputs[i];\n-      outputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n-      newOutputs[i] \u003d buffer.array();\n-    }\n-\n-    doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n-\n-    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n-      buffer \u003d inputs[i];\n-      buffer.position(buffer.position() + dataLen); // dataLen bytes consumed\n+    for (int i \u003d 0; i \u003c inputs.length; i++) {\n+      if (inputs[i] !\u003d null) {\n+        // dataLen bytes consumed\n+        inputs[i].position(inputPositions[i] + dataLen);\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n    checkParameters(inputs, outputs);\n\n    boolean usingDirectBuffer \u003d inputs[0].isDirect();\n    int dataLen \u003d inputs[0].remaining();\n    if (dataLen \u003d\u003d 0) {\n      return;\n    }\n    checkParameterBuffers(inputs, false, dataLen, usingDirectBuffer, false);\n    checkParameterBuffers(outputs, false, dataLen, usingDirectBuffer, true);\n\n    int[] inputPositions \u003d new int[inputs.length];\n    for (int i \u003d 0; i \u003c inputPositions.length; i++) {\n      if (inputs[i] !\u003d null) {\n        inputPositions[i] \u003d inputs[i].position();\n      }\n    }\n\n    if (usingDirectBuffer) {\n      doEncode(inputs, outputs);\n    } else {\n      int[] inputOffsets \u003d new int[inputs.length];\n      int[] outputOffsets \u003d new int[outputs.length];\n      byte[][] newInputs \u003d new byte[inputs.length][];\n      byte[][] newOutputs \u003d new byte[outputs.length][];\n\n      ByteBuffer buffer;\n      for (int i \u003d 0; i \u003c inputs.length; ++i) {\n        buffer \u003d inputs[i];\n        inputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n        newInputs[i] \u003d buffer.array();\n      }\n\n      for (int i \u003d 0; i \u003c outputs.length; ++i) {\n        buffer \u003d outputs[i];\n        outputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n        newOutputs[i] \u003d buffer.array();\n      }\n\n      doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n    }\n\n    for (int i \u003d 0; i \u003c inputs.length; i++) {\n      if (inputs[i] !\u003d null) {\n        // dataLen bytes consumed\n        inputs[i].position(inputPositions[i] + dataLen);\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractRawErasureEncoder.java",
      "extendedDetails": {}
    },
    "5eca6dece67620f990f3306b6caaf09f317b38f6": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12327. Initialize output buffers with ZERO bytes in erasure coder. Contributed by Kai Zheng.\n",
      "commitDate": "29/10/15 12:04 AM",
      "commitName": "5eca6dece67620f990f3306b6caaf09f317b38f6",
      "commitAuthor": "Walter Su",
      "commitDateOld": "07/10/15 6:12 PM",
      "commitNameOld": "66e2cfa1a0285f2b4f62a4ffb4d5c1ee54f76156",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 21.24,
      "commitsBetweenForRepo": 199,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n     checkParameters(inputs, outputs);\n \n     boolean usingDirectBuffer \u003d inputs[0].isDirect();\n     int dataLen \u003d inputs[0].remaining();\n     if (dataLen \u003d\u003d 0) {\n       return;\n     }\n-    ensureLengthAndType(inputs, false, dataLen, usingDirectBuffer);\n-    ensureLengthAndType(outputs, false, dataLen, usingDirectBuffer);\n+    checkParameterBuffers(inputs, false, dataLen, usingDirectBuffer, false);\n+    checkParameterBuffers(outputs, false, dataLen, usingDirectBuffer, true);\n \n     if (usingDirectBuffer) {\n       doEncode(inputs, outputs);\n       return;\n     }\n \n     int[] inputOffsets \u003d new int[inputs.length];\n     int[] outputOffsets \u003d new int[outputs.length];\n     byte[][] newInputs \u003d new byte[inputs.length][];\n     byte[][] newOutputs \u003d new byte[outputs.length][];\n \n     ByteBuffer buffer;\n     for (int i \u003d 0; i \u003c inputs.length; ++i) {\n       buffer \u003d inputs[i];\n       inputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n       newInputs[i] \u003d buffer.array();\n     }\n \n     for (int i \u003d 0; i \u003c outputs.length; ++i) {\n       buffer \u003d outputs[i];\n       outputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n       newOutputs[i] \u003d buffer.array();\n     }\n \n     doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n \n     for (int i \u003d 0; i \u003c inputs.length; ++i) {\n       buffer \u003d inputs[i];\n       buffer.position(buffer.position() + dataLen); // dataLen bytes consumed\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n    checkParameters(inputs, outputs);\n\n    boolean usingDirectBuffer \u003d inputs[0].isDirect();\n    int dataLen \u003d inputs[0].remaining();\n    if (dataLen \u003d\u003d 0) {\n      return;\n    }\n    checkParameterBuffers(inputs, false, dataLen, usingDirectBuffer, false);\n    checkParameterBuffers(outputs, false, dataLen, usingDirectBuffer, true);\n\n    if (usingDirectBuffer) {\n      doEncode(inputs, outputs);\n      return;\n    }\n\n    int[] inputOffsets \u003d new int[inputs.length];\n    int[] outputOffsets \u003d new int[outputs.length];\n    byte[][] newInputs \u003d new byte[inputs.length][];\n    byte[][] newOutputs \u003d new byte[outputs.length][];\n\n    ByteBuffer buffer;\n    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n      buffer \u003d inputs[i];\n      inputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n      newInputs[i] \u003d buffer.array();\n    }\n\n    for (int i \u003d 0; i \u003c outputs.length; ++i) {\n      buffer \u003d outputs[i];\n      outputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n      newOutputs[i] \u003d buffer.array();\n    }\n\n    doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n\n    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n      buffer \u003d inputs[i];\n      buffer.position(buffer.position() + dataLen); // dataLen bytes consumed\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractRawErasureEncoder.java",
      "extendedDetails": {}
    },
    "29495cb8f6b940caa9964c39a290ef233ce1ec7c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12060. Fix ByteBuffer usage for raw erasure coders. Contributed by Kai Zheng.\n",
      "commitDate": "20/07/15 10:15 AM",
      "commitName": "29495cb8f6b940caa9964c39a290ef233ce1ec7c",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 12:07 PM",
      "commitNameOld": "4ad484883f773c702a1874fc12816ef1a4a54136",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 54.92,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,41 @@\n   public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n     checkParameters(inputs, outputs);\n+\n+    boolean usingDirectBuffer \u003d inputs[0].isDirect();\n     int dataLen \u003d inputs[0].remaining();\n     if (dataLen \u003d\u003d 0) {\n       return;\n     }\n-    ensureLength(inputs, false, dataLen);\n-    ensureLength(outputs, false, dataLen);\n+    ensureLengthAndType(inputs, false, dataLen, usingDirectBuffer);\n+    ensureLengthAndType(outputs, false, dataLen, usingDirectBuffer);\n \n-    boolean usingDirectBuffer \u003d inputs[0].isDirect();\n     if (usingDirectBuffer) {\n       doEncode(inputs, outputs);\n       return;\n     }\n \n     int[] inputOffsets \u003d new int[inputs.length];\n     int[] outputOffsets \u003d new int[outputs.length];\n     byte[][] newInputs \u003d new byte[inputs.length][];\n     byte[][] newOutputs \u003d new byte[outputs.length][];\n \n     ByteBuffer buffer;\n     for (int i \u003d 0; i \u003c inputs.length; ++i) {\n       buffer \u003d inputs[i];\n-      inputOffsets[i] \u003d buffer.position();\n+      inputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n       newInputs[i] \u003d buffer.array();\n     }\n \n     for (int i \u003d 0; i \u003c outputs.length; ++i) {\n       buffer \u003d outputs[i];\n-      outputOffsets[i] \u003d buffer.position();\n+      outputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n       newOutputs[i] \u003d buffer.array();\n     }\n \n     doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n \n     for (int i \u003d 0; i \u003c inputs.length; ++i) {\n       buffer \u003d inputs[i];\n       buffer.position(buffer.position() + dataLen); // dataLen bytes consumed\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n    checkParameters(inputs, outputs);\n\n    boolean usingDirectBuffer \u003d inputs[0].isDirect();\n    int dataLen \u003d inputs[0].remaining();\n    if (dataLen \u003d\u003d 0) {\n      return;\n    }\n    ensureLengthAndType(inputs, false, dataLen, usingDirectBuffer);\n    ensureLengthAndType(outputs, false, dataLen, usingDirectBuffer);\n\n    if (usingDirectBuffer) {\n      doEncode(inputs, outputs);\n      return;\n    }\n\n    int[] inputOffsets \u003d new int[inputs.length];\n    int[] outputOffsets \u003d new int[outputs.length];\n    byte[][] newInputs \u003d new byte[inputs.length][];\n    byte[][] newOutputs \u003d new byte[outputs.length][];\n\n    ByteBuffer buffer;\n    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n      buffer \u003d inputs[i];\n      inputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n      newInputs[i] \u003d buffer.array();\n    }\n\n    for (int i \u003d 0; i \u003c outputs.length; ++i) {\n      buffer \u003d outputs[i];\n      outputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n      newOutputs[i] \u003d buffer.array();\n    }\n\n    doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n\n    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n      buffer \u003d inputs[i];\n      buffer.position(buffer.position() + dataLen); // dataLen bytes consumed\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractRawErasureEncoder.java",
      "extendedDetails": {}
    },
    "4ad484883f773c702a1874fc12816ef1a4a54136": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11847 Enhance raw coder allowing to read least required inputs in decoding. Contributed by Kai Zheng\n",
      "commitDate": "26/05/15 12:07 PM",
      "commitName": "4ad484883f773c702a1874fc12816ef1a4a54136",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "26/05/15 12:07 PM",
      "commitNameOld": "b30e96bfb4b8ce5537671c97f0c9c56cd195bfdc",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n     checkParameters(inputs, outputs);\n     int dataLen \u003d inputs[0].remaining();\n     if (dataLen \u003d\u003d 0) {\n       return;\n     }\n-    ensureLength(inputs, dataLen);\n-    ensureLength(outputs, dataLen);\n+    ensureLength(inputs, false, dataLen);\n+    ensureLength(outputs, false, dataLen);\n \n     boolean usingDirectBuffer \u003d inputs[0].isDirect();\n     if (usingDirectBuffer) {\n       doEncode(inputs, outputs);\n       return;\n     }\n \n     int[] inputOffsets \u003d new int[inputs.length];\n     int[] outputOffsets \u003d new int[outputs.length];\n     byte[][] newInputs \u003d new byte[inputs.length][];\n     byte[][] newOutputs \u003d new byte[outputs.length][];\n \n     ByteBuffer buffer;\n     for (int i \u003d 0; i \u003c inputs.length; ++i) {\n       buffer \u003d inputs[i];\n       inputOffsets[i] \u003d buffer.position();\n       newInputs[i] \u003d buffer.array();\n     }\n \n     for (int i \u003d 0; i \u003c outputs.length; ++i) {\n       buffer \u003d outputs[i];\n       outputOffsets[i] \u003d buffer.position();\n       newOutputs[i] \u003d buffer.array();\n     }\n \n     doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n \n     for (int i \u003d 0; i \u003c inputs.length; ++i) {\n       buffer \u003d inputs[i];\n       buffer.position(buffer.position() + dataLen); // dataLen bytes consumed\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n    checkParameters(inputs, outputs);\n    int dataLen \u003d inputs[0].remaining();\n    if (dataLen \u003d\u003d 0) {\n      return;\n    }\n    ensureLength(inputs, false, dataLen);\n    ensureLength(outputs, false, dataLen);\n\n    boolean usingDirectBuffer \u003d inputs[0].isDirect();\n    if (usingDirectBuffer) {\n      doEncode(inputs, outputs);\n      return;\n    }\n\n    int[] inputOffsets \u003d new int[inputs.length];\n    int[] outputOffsets \u003d new int[outputs.length];\n    byte[][] newInputs \u003d new byte[inputs.length][];\n    byte[][] newOutputs \u003d new byte[outputs.length][];\n\n    ByteBuffer buffer;\n    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n      buffer \u003d inputs[i];\n      inputOffsets[i] \u003d buffer.position();\n      newInputs[i] \u003d buffer.array();\n    }\n\n    for (int i \u003d 0; i \u003c outputs.length; ++i) {\n      buffer \u003d outputs[i];\n      outputOffsets[i] \u003d buffer.position();\n      newOutputs[i] \u003d buffer.array();\n    }\n\n    doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n\n    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n      buffer \u003d inputs[i];\n      buffer.position(buffer.position() + dataLen); // dataLen bytes consumed\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractRawErasureEncoder.java",
      "extendedDetails": {}
    },
    "343c0e76fcd95ac739ca7cd6742c9d617e19fc37": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11938. Enhance ByteBuffer version encode/decode API of raw erasure coder. Contributed by Kai Zheng.\n",
      "commitDate": "26/05/15 12:02 PM",
      "commitName": "343c0e76fcd95ac739ca7cd6742c9d617e19fc37",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 12:02 PM",
      "commitNameOld": "09c3a375bafa481e88d1317388a73c46950164c9",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,40 @@\n   public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n     checkParameters(inputs, outputs);\n+    int dataLen \u003d inputs[0].remaining();\n+    if (dataLen \u003d\u003d 0) {\n+      return;\n+    }\n+    ensureLength(inputs, dataLen);\n+    ensureLength(outputs, dataLen);\n \n-    boolean hasArray \u003d inputs[0].hasArray();\n-    if (hasArray) {\n-      byte[][] newInputs \u003d toArrays(inputs);\n-      byte[][] newOutputs \u003d toArrays(outputs);\n-      doEncode(newInputs, newOutputs);\n-    } else {\n+    boolean usingDirectBuffer \u003d inputs[0].isDirect();\n+    if (usingDirectBuffer) {\n       doEncode(inputs, outputs);\n+      return;\n+    }\n+\n+    int[] inputOffsets \u003d new int[inputs.length];\n+    int[] outputOffsets \u003d new int[outputs.length];\n+    byte[][] newInputs \u003d new byte[inputs.length][];\n+    byte[][] newOutputs \u003d new byte[outputs.length][];\n+\n+    ByteBuffer buffer;\n+    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n+      buffer \u003d inputs[i];\n+      inputOffsets[i] \u003d buffer.position();\n+      newInputs[i] \u003d buffer.array();\n+    }\n+\n+    for (int i \u003d 0; i \u003c outputs.length; ++i) {\n+      buffer \u003d outputs[i];\n+      outputOffsets[i] \u003d buffer.position();\n+      newOutputs[i] \u003d buffer.array();\n+    }\n+\n+    doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n+\n+    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n+      buffer \u003d inputs[i];\n+      buffer.position(buffer.position() + dataLen); // dataLen bytes consumed\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n    checkParameters(inputs, outputs);\n    int dataLen \u003d inputs[0].remaining();\n    if (dataLen \u003d\u003d 0) {\n      return;\n    }\n    ensureLength(inputs, dataLen);\n    ensureLength(outputs, dataLen);\n\n    boolean usingDirectBuffer \u003d inputs[0].isDirect();\n    if (usingDirectBuffer) {\n      doEncode(inputs, outputs);\n      return;\n    }\n\n    int[] inputOffsets \u003d new int[inputs.length];\n    int[] outputOffsets \u003d new int[outputs.length];\n    byte[][] newInputs \u003d new byte[inputs.length][];\n    byte[][] newOutputs \u003d new byte[outputs.length][];\n\n    ByteBuffer buffer;\n    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n      buffer \u003d inputs[i];\n      inputOffsets[i] \u003d buffer.position();\n      newInputs[i] \u003d buffer.array();\n    }\n\n    for (int i \u003d 0; i \u003c outputs.length; ++i) {\n      buffer \u003d outputs[i];\n      outputOffsets[i] \u003d buffer.position();\n      newOutputs[i] \u003d buffer.array();\n    }\n\n    doEncode(newInputs, inputOffsets, dataLen, newOutputs, outputOffsets);\n\n    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n      buffer \u003d inputs[i];\n      buffer.position(buffer.position() + dataLen); // dataLen bytes consumed\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractRawErasureEncoder.java",
      "extendedDetails": {}
    },
    "09c3a375bafa481e88d1317388a73c46950164c9": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11920. Refactor some codes for erasure coders. Contributed by Kai Zheng.\n",
      "commitDate": "26/05/15 12:02 PM",
      "commitName": "09c3a375bafa481e88d1317388a73c46950164c9",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 11:03 AM",
      "commitNameOld": "e50bcea83d5f6b02ab03b06a3fbf1ed6b8ff4871",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 113,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,6 +1,12 @@\n   public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n-    assert (inputs.length \u003d\u003d getNumDataUnits());\n-    assert (outputs.length \u003d\u003d getNumParityUnits());\n+    checkParameters(inputs, outputs);\n \n-    doEncode(inputs, outputs);\n+    boolean hasArray \u003d inputs[0].hasArray();\n+    if (hasArray) {\n+      byte[][] newInputs \u003d toArrays(inputs);\n+      byte[][] newOutputs \u003d toArrays(outputs);\n+      doEncode(newInputs, newOutputs);\n+    } else {\n+      doEncode(inputs, outputs);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n    checkParameters(inputs, outputs);\n\n    boolean hasArray \u003d inputs[0].hasArray();\n    if (hasArray) {\n      byte[][] newInputs \u003d toArrays(inputs);\n      byte[][] newOutputs \u003d toArrays(outputs);\n      doEncode(newInputs, newOutputs);\n    } else {\n      doEncode(inputs, outputs);\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractRawErasureEncoder.java",
      "extendedDetails": {}
    },
    "e50bcea83d5f6b02ab03b06a3fbf1ed6b8ff4871": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-11514. Raw Erasure Coder API for concrete encoding and decoding (Kai Zheng via umamahesh)\n",
      "commitDate": "26/05/15 11:03 AM",
      "commitName": "e50bcea83d5f6b02ab03b06a3fbf1ed6b8ff4871",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,6 @@\n+  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n+    assert (inputs.length \u003d\u003d getNumDataUnits());\n+    assert (outputs.length \u003d\u003d getNumParityUnits());\n+\n+    doEncode(inputs, outputs);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void encode(ByteBuffer[] inputs, ByteBuffer[] outputs) {\n    assert (inputs.length \u003d\u003d getNumDataUnits());\n    assert (outputs.length \u003d\u003d getNumParityUnits());\n\n    doEncode(inputs, outputs);\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractRawErasureEncoder.java"
    }
  }
}