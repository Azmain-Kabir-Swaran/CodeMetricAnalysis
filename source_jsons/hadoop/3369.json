{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ByteRangeInputStream.java",
  "functionName": "openInputStream",
  "functionId": "openInputStream___startOffset-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java",
  "functionStartLine": 130,
  "functionEndLine": 165,
  "numCommitsSeen": 21,
  "timeTaken": 2500,
  "changeHistory": [
    "843ee8d59d8bacbca0d87ccf0790772e39d16138",
    "e91ccfad07ec5b5674a84009772dd31a82b4e4de",
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
    "68a79b0d3f189dfdbd3a3e2a0b906627db3eff8d",
    "87d87b04e6eb6f42c8998d7ddfdf60767a6e04e1",
    "cb787968c5deac3dd5d10291aae39c36656a1487",
    "556be2af92b68808aff71937d437ab9948164bb1",
    "e4eec269d91ae541a321ae2f28ff03310682b3fe",
    "920b8fac187de859307ae960b7abd456e23d87e6"
  ],
  "changeHistoryShort": {
    "843ee8d59d8bacbca0d87ccf0790772e39d16138": "Ybodychange",
    "e91ccfad07ec5b5674a84009772dd31a82b4e4de": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35": "Yfilerename",
    "68a79b0d3f189dfdbd3a3e2a0b906627db3eff8d": "Yfilerename",
    "87d87b04e6eb6f42c8998d7ddfdf60767a6e04e1": "Ybodychange",
    "cb787968c5deac3dd5d10291aae39c36656a1487": "Ybodychange",
    "556be2af92b68808aff71937d437ab9948164bb1": "Ybodychange",
    "e4eec269d91ae541a321ae2f28ff03310682b3fe": "Ybodychange",
    "920b8fac187de859307ae960b7abd456e23d87e6": "Yintroduced"
  },
  "changeHistoryDetails": {
    "843ee8d59d8bacbca0d87ccf0790772e39d16138": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12994. Specify PositionedReadable, add contract tests, fix problems. Contributed by Steve Loughran.\n",
      "commitDate": "08/04/16 1:36 PM",
      "commitName": "843ee8d59d8bacbca0d87ccf0790772e39d16138",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 188.08,
      "commitsBetweenForRepo": 1255,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,36 @@\n   protected InputStreamAndFileLength openInputStream(long startOffset)\n       throws IOException {\n+    if (startOffset \u003c 0) {\n+      throw new EOFException(\"Negative Position\");\n+    }\n     // Use the original url if no resolved url exists, eg. if\n     // it\u0027s the first time a request is made.\n     final boolean resolved \u003d resolvedURL.getURL() !\u003d null;\n     final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n \n     final HttpURLConnection connection \u003d opener.connect(startOffset, resolved);\n     resolvedURL.setURL(getResolvedUrl(connection));\n \n     InputStream in \u003d connection.getInputStream();\n     final Long length;\n     final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n     if (isChunkedTransferEncoding(headers)) {\n       // file length is not known\n       length \u003d null;\n     } else {\n       // for non-chunked transfer-encoding, get content-length\n       final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n       if (cl \u003d\u003d null) {\n         throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n             + headers);\n       }\n       final long streamlength \u003d Long.parseLong(cl);\n       length \u003d startOffset + streamlength;\n \n       // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n       // the reads so the transfer blocks until the server times out\n       in \u003d new BoundedInputStream(in, streamlength);\n     }\n \n     return new InputStreamAndFileLength(length, in);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected InputStreamAndFileLength openInputStream(long startOffset)\n      throws IOException {\n    if (startOffset \u003c 0) {\n      throw new EOFException(\"Negative Position\");\n    }\n    // Use the original url if no resolved url exists, eg. if\n    // it\u0027s the first time a request is made.\n    final boolean resolved \u003d resolvedURL.getURL() !\u003d null;\n    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n\n    final HttpURLConnection connection \u003d opener.connect(startOffset, resolved);\n    resolvedURL.setURL(getResolvedUrl(connection));\n\n    InputStream in \u003d connection.getInputStream();\n    final Long length;\n    final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n    if (isChunkedTransferEncoding(headers)) {\n      // file length is not known\n      length \u003d null;\n    } else {\n      // for non-chunked transfer-encoding, get content-length\n      final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n      if (cl \u003d\u003d null) {\n        throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n            + headers);\n      }\n      final long streamlength \u003d Long.parseLong(cl);\n      length \u003d startOffset + streamlength;\n\n      // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n      // the reads so the transfer blocks until the server times out\n      in \u003d new BoundedInputStream(in, streamlength);\n    }\n\n    return new InputStreamAndFileLength(length, in);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java",
      "extendedDetails": {}
    },
    "e91ccfad07ec5b5674a84009772dd31a82b4e4de": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-8797. WebHdfsFileSystem creates too many connections for pread. Contributed by Jing Zhao.\n",
      "commitDate": "22/07/15 5:42 PM",
      "commitName": "e91ccfad07ec5b5674a84009772dd31a82b4e4de",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8797. WebHdfsFileSystem creates too many connections for pread. Contributed by Jing Zhao.\n",
          "commitDate": "22/07/15 5:42 PM",
          "commitName": "e91ccfad07ec5b5674a84009772dd31a82b4e4de",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "23/04/15 5:33 PM",
          "commitNameOld": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 90.01,
          "commitsBetweenForRepo": 732,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,33 @@\n-  protected InputStream openInputStream() throws IOException {\n+  protected InputStreamAndFileLength openInputStream(long startOffset)\n+      throws IOException {\n     // Use the original url if no resolved url exists, eg. if\n     // it\u0027s the first time a request is made.\n     final boolean resolved \u003d resolvedURL.getURL() !\u003d null;\n     final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n \n-    final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n+    final HttpURLConnection connection \u003d opener.connect(startOffset, resolved);\n     resolvedURL.setURL(getResolvedUrl(connection));\n \n     InputStream in \u003d connection.getInputStream();\n+    final Long length;\n     final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n     if (isChunkedTransferEncoding(headers)) {\n       // file length is not known\n-      fileLength \u003d null;\n+      length \u003d null;\n     } else {\n       // for non-chunked transfer-encoding, get content-length\n       final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n       if (cl \u003d\u003d null) {\n         throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n             + headers);\n       }\n       final long streamlength \u003d Long.parseLong(cl);\n-      fileLength \u003d startPos + streamlength;\n+      length \u003d startOffset + streamlength;\n \n       // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n       // the reads so the transfer blocks until the server times out\n       in \u003d new BoundedInputStream(in, streamlength);\n     }\n \n-    return in;\n+    return new InputStreamAndFileLength(length, in);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected InputStreamAndFileLength openInputStream(long startOffset)\n      throws IOException {\n    // Use the original url if no resolved url exists, eg. if\n    // it\u0027s the first time a request is made.\n    final boolean resolved \u003d resolvedURL.getURL() !\u003d null;\n    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n\n    final HttpURLConnection connection \u003d opener.connect(startOffset, resolved);\n    resolvedURL.setURL(getResolvedUrl(connection));\n\n    InputStream in \u003d connection.getInputStream();\n    final Long length;\n    final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n    if (isChunkedTransferEncoding(headers)) {\n      // file length is not known\n      length \u003d null;\n    } else {\n      // for non-chunked transfer-encoding, get content-length\n      final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n      if (cl \u003d\u003d null) {\n        throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n            + headers);\n      }\n      final long streamlength \u003d Long.parseLong(cl);\n      length \u003d startOffset + streamlength;\n\n      // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n      // the reads so the transfer blocks until the server times out\n      in \u003d new BoundedInputStream(in, streamlength);\n    }\n\n    return new InputStreamAndFileLength(length, in);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[startOffset-long]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-8797. WebHdfsFileSystem creates too many connections for pread. Contributed by Jing Zhao.\n",
          "commitDate": "22/07/15 5:42 PM",
          "commitName": "e91ccfad07ec5b5674a84009772dd31a82b4e4de",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "23/04/15 5:33 PM",
          "commitNameOld": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 90.01,
          "commitsBetweenForRepo": 732,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,33 @@\n-  protected InputStream openInputStream() throws IOException {\n+  protected InputStreamAndFileLength openInputStream(long startOffset)\n+      throws IOException {\n     // Use the original url if no resolved url exists, eg. if\n     // it\u0027s the first time a request is made.\n     final boolean resolved \u003d resolvedURL.getURL() !\u003d null;\n     final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n \n-    final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n+    final HttpURLConnection connection \u003d opener.connect(startOffset, resolved);\n     resolvedURL.setURL(getResolvedUrl(connection));\n \n     InputStream in \u003d connection.getInputStream();\n+    final Long length;\n     final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n     if (isChunkedTransferEncoding(headers)) {\n       // file length is not known\n-      fileLength \u003d null;\n+      length \u003d null;\n     } else {\n       // for non-chunked transfer-encoding, get content-length\n       final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n       if (cl \u003d\u003d null) {\n         throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n             + headers);\n       }\n       final long streamlength \u003d Long.parseLong(cl);\n-      fileLength \u003d startPos + streamlength;\n+      length \u003d startOffset + streamlength;\n \n       // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n       // the reads so the transfer blocks until the server times out\n       in \u003d new BoundedInputStream(in, streamlength);\n     }\n \n-    return in;\n+    return new InputStreamAndFileLength(length, in);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected InputStreamAndFileLength openInputStream(long startOffset)\n      throws IOException {\n    // Use the original url if no resolved url exists, eg. if\n    // it\u0027s the first time a request is made.\n    final boolean resolved \u003d resolvedURL.getURL() !\u003d null;\n    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n\n    final HttpURLConnection connection \u003d opener.connect(startOffset, resolved);\n    resolvedURL.setURL(getResolvedUrl(connection));\n\n    InputStream in \u003d connection.getInputStream();\n    final Long length;\n    final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n    if (isChunkedTransferEncoding(headers)) {\n      // file length is not known\n      length \u003d null;\n    } else {\n      // for non-chunked transfer-encoding, get content-length\n      final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n      if (cl \u003d\u003d null) {\n        throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n            + headers);\n      }\n      final long streamlength \u003d Long.parseLong(cl);\n      length \u003d startOffset + streamlength;\n\n      // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n      // the reads so the transfer blocks until the server times out\n      in \u003d new BoundedInputStream(in, streamlength);\n    }\n\n    return new InputStreamAndFileLength(length, in);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java",
          "extendedDetails": {
            "oldValue": "InputStream",
            "newValue": "InputStreamAndFileLength"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8797. WebHdfsFileSystem creates too many connections for pread. Contributed by Jing Zhao.\n",
          "commitDate": "22/07/15 5:42 PM",
          "commitName": "e91ccfad07ec5b5674a84009772dd31a82b4e4de",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "23/04/15 5:33 PM",
          "commitNameOld": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 90.01,
          "commitsBetweenForRepo": 732,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,33 @@\n-  protected InputStream openInputStream() throws IOException {\n+  protected InputStreamAndFileLength openInputStream(long startOffset)\n+      throws IOException {\n     // Use the original url if no resolved url exists, eg. if\n     // it\u0027s the first time a request is made.\n     final boolean resolved \u003d resolvedURL.getURL() !\u003d null;\n     final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n \n-    final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n+    final HttpURLConnection connection \u003d opener.connect(startOffset, resolved);\n     resolvedURL.setURL(getResolvedUrl(connection));\n \n     InputStream in \u003d connection.getInputStream();\n+    final Long length;\n     final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n     if (isChunkedTransferEncoding(headers)) {\n       // file length is not known\n-      fileLength \u003d null;\n+      length \u003d null;\n     } else {\n       // for non-chunked transfer-encoding, get content-length\n       final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n       if (cl \u003d\u003d null) {\n         throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n             + headers);\n       }\n       final long streamlength \u003d Long.parseLong(cl);\n-      fileLength \u003d startPos + streamlength;\n+      length \u003d startOffset + streamlength;\n \n       // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n       // the reads so the transfer blocks until the server times out\n       in \u003d new BoundedInputStream(in, streamlength);\n     }\n \n-    return in;\n+    return new InputStreamAndFileLength(length, in);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected InputStreamAndFileLength openInputStream(long startOffset)\n      throws IOException {\n    // Use the original url if no resolved url exists, eg. if\n    // it\u0027s the first time a request is made.\n    final boolean resolved \u003d resolvedURL.getURL() !\u003d null;\n    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n\n    final HttpURLConnection connection \u003d opener.connect(startOffset, resolved);\n    resolvedURL.setURL(getResolvedUrl(connection));\n\n    InputStream in \u003d connection.getInputStream();\n    final Long length;\n    final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n    if (isChunkedTransferEncoding(headers)) {\n      // file length is not known\n      length \u003d null;\n    } else {\n      // for non-chunked transfer-encoding, get content-length\n      final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n      if (cl \u003d\u003d null) {\n        throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n            + headers);\n      }\n      final long streamlength \u003d Long.parseLong(cl);\n      length \u003d startOffset + streamlength;\n\n      // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n      // the reads so the transfer blocks until the server times out\n      in \u003d new BoundedInputStream(in, streamlength);\n    }\n\n    return new InputStreamAndFileLength(length, in);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8052. Move WebHdfsFileSystem into hadoop-hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "23/04/15 5:33 PM",
      "commitName": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "23/04/15 4:40 PM",
      "commitNameOld": "0b3f8957a87ada1a275c9904b211fdbdcefafb02",
      "commitAuthorOld": "Xuan",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected InputStream openInputStream() throws IOException {\n    // Use the original url if no resolved url exists, eg. if\n    // it\u0027s the first time a request is made.\n    final boolean resolved \u003d resolvedURL.getURL() !\u003d null;\n    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n\n    final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n    resolvedURL.setURL(getResolvedUrl(connection));\n\n    InputStream in \u003d connection.getInputStream();\n    final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n    if (isChunkedTransferEncoding(headers)) {\n      // file length is not known\n      fileLength \u003d null;\n    } else {\n      // for non-chunked transfer-encoding, get content-length\n      final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n      if (cl \u003d\u003d null) {\n        throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n            + headers);\n      }\n      final long streamlength \u003d Long.parseLong(cl);\n      fileLength \u003d startPos + streamlength;\n\n      // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n      // the reads so the transfer blocks until the server times out\n      in \u003d new BoundedInputStream(in, streamlength);\n    }\n\n    return in;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java"
      }
    },
    "68a79b0d3f189dfdbd3a3e2a0b906627db3eff8d": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-5436. Move HsFtpFileSystem and HFtpFileSystem into org.apache.hdfs.web. (Contributed by Haohui Mai)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1536921 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/10/13 3:44 PM",
      "commitName": "68a79b0d3f189dfdbd3a3e2a0b906627db3eff8d",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "29/10/13 2:11 PM",
      "commitNameOld": "7dd201c541c811069a898403cf28a50152a38737",
      "commitAuthorOld": "Bikas Saha",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   protected InputStream openInputStream() throws IOException {\n     // Use the original url if no resolved url exists, eg. if\n     // it\u0027s the first time a request is made.\n-    final boolean resolved \u003d resolvedURL.getURL() !\u003d null; \n+    final boolean resolved \u003d resolvedURL.getURL() !\u003d null;\n     final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n \n     final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n     resolvedURL.setURL(getResolvedUrl(connection));\n \n     InputStream in \u003d connection.getInputStream();\n     final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n     if (isChunkedTransferEncoding(headers)) {\n       // file length is not known\n       fileLength \u003d null;\n     } else {\n       // for non-chunked transfer-encoding, get content-length\n       final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n       if (cl \u003d\u003d null) {\n         throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n             + headers);\n       }\n       final long streamlength \u003d Long.parseLong(cl);\n       fileLength \u003d startPos + streamlength;\n \n       // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n       // the reads so the transfer blocks until the server times out\n       in \u003d new BoundedInputStream(in, streamlength);\n     }\n \n     return in;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected InputStream openInputStream() throws IOException {\n    // Use the original url if no resolved url exists, eg. if\n    // it\u0027s the first time a request is made.\n    final boolean resolved \u003d resolvedURL.getURL() !\u003d null;\n    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n\n    final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n    resolvedURL.setURL(getResolvedUrl(connection));\n\n    InputStream in \u003d connection.getInputStream();\n    final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n    if (isChunkedTransferEncoding(headers)) {\n      // file length is not known\n      fileLength \u003d null;\n    } else {\n      // for non-chunked transfer-encoding, get content-length\n      final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n      if (cl \u003d\u003d null) {\n        throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n            + headers);\n      }\n      final long streamlength \u003d Long.parseLong(cl);\n      fileLength \u003d startPos + streamlength;\n\n      // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n      // the reads so the transfer blocks until the server times out\n      in \u003d new BoundedInputStream(in, streamlength);\n    }\n\n    return in;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java"
      }
    },
    "87d87b04e6eb6f42c8998d7ddfdf60767a6e04e1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3788. ByteRangeInputStream should not expect HTTP Content-Length header when chunked transfer-encoding is used.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1374122 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/08/12 6:42 PM",
      "commitName": "87d87b04e6eb6f42c8998d7ddfdf60767a6e04e1",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "31/07/12 6:41 PM",
      "commitNameOld": "cb787968c5deac3dd5d10291aae39c36656a1487",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 16.0,
      "commitsBetweenForRepo": 76,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,31 @@\n   protected InputStream openInputStream() throws IOException {\n     // Use the original url if no resolved url exists, eg. if\n     // it\u0027s the first time a request is made.\n     final boolean resolved \u003d resolvedURL.getURL() !\u003d null; \n     final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n \n     final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n-    final String cl \u003d connection.getHeaderField(StreamFile.CONTENT_LENGTH);\n-    if (cl \u003d\u003d null) {\n-      throw new IOException(StreamFile.CONTENT_LENGTH+\" header is missing\");\n-    }\n-    final long streamlength \u003d Long.parseLong(cl);\n-    filelength \u003d startPos + streamlength;\n-    // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n-    // the reads so the transfer blocks until the server times out\n-    InputStream is \u003d\n-        new BoundedInputStream(connection.getInputStream(), streamlength);\n-\n     resolvedURL.setURL(getResolvedUrl(connection));\n-    \n-    return is;\n+\n+    InputStream in \u003d connection.getInputStream();\n+    final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n+    if (isChunkedTransferEncoding(headers)) {\n+      // file length is not known\n+      fileLength \u003d null;\n+    } else {\n+      // for non-chunked transfer-encoding, get content-length\n+      final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n+      if (cl \u003d\u003d null) {\n+        throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n+            + headers);\n+      }\n+      final long streamlength \u003d Long.parseLong(cl);\n+      fileLength \u003d startPos + streamlength;\n+\n+      // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n+      // the reads so the transfer blocks until the server times out\n+      in \u003d new BoundedInputStream(in, streamlength);\n+    }\n+\n+    return in;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected InputStream openInputStream() throws IOException {\n    // Use the original url if no resolved url exists, eg. if\n    // it\u0027s the first time a request is made.\n    final boolean resolved \u003d resolvedURL.getURL() !\u003d null; \n    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n\n    final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n    resolvedURL.setURL(getResolvedUrl(connection));\n\n    InputStream in \u003d connection.getInputStream();\n    final Map\u003cString, List\u003cString\u003e\u003e headers \u003d connection.getHeaderFields();\n    if (isChunkedTransferEncoding(headers)) {\n      // file length is not known\n      fileLength \u003d null;\n    } else {\n      // for non-chunked transfer-encoding, get content-length\n      final String cl \u003d connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\n      if (cl \u003d\u003d null) {\n        throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \"\n            + headers);\n      }\n      final long streamlength \u003d Long.parseLong(cl);\n      fileLength \u003d startPos + streamlength;\n\n      // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n      // the reads so the transfer blocks until the server times out\n      in \u003d new BoundedInputStream(in, streamlength);\n    }\n\n    return in;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java",
      "extendedDetails": {}
    },
    "cb787968c5deac3dd5d10291aae39c36656a1487": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3667.  Add retry support to WebHdfsFileSystem.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1367841 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/07/12 6:41 PM",
      "commitName": "cb787968c5deac3dd5d10291aae39c36656a1487",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/07/12 9:33 PM",
      "commitNameOld": "556be2af92b68808aff71937d437ab9948164bb1",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.88,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,22 @@\n   protected InputStream openInputStream() throws IOException {\n     // Use the original url if no resolved url exists, eg. if\n     // it\u0027s the first time a request is made.\n-    final URLOpener opener \u003d\n-      (resolvedURL.getURL() \u003d\u003d null) ? originalURL : resolvedURL;\n+    final boolean resolved \u003d resolvedURL.getURL() !\u003d null; \n+    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n \n-    final HttpURLConnection connection \u003d opener.openConnection(startPos);\n-    connection.connect();\n-    checkResponseCode(connection);\n-\n+    final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n     final String cl \u003d connection.getHeaderField(StreamFile.CONTENT_LENGTH);\n     if (cl \u003d\u003d null) {\n       throw new IOException(StreamFile.CONTENT_LENGTH+\" header is missing\");\n     }\n     final long streamlength \u003d Long.parseLong(cl);\n     filelength \u003d startPos + streamlength;\n     // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n     // the reads so the transfer blocks until the server times out\n     InputStream is \u003d\n         new BoundedInputStream(connection.getInputStream(), streamlength);\n \n     resolvedURL.setURL(getResolvedUrl(connection));\n     \n     return is;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected InputStream openInputStream() throws IOException {\n    // Use the original url if no resolved url exists, eg. if\n    // it\u0027s the first time a request is made.\n    final boolean resolved \u003d resolvedURL.getURL() !\u003d null; \n    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n\n    final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n    final String cl \u003d connection.getHeaderField(StreamFile.CONTENT_LENGTH);\n    if (cl \u003d\u003d null) {\n      throw new IOException(StreamFile.CONTENT_LENGTH+\" header is missing\");\n    }\n    final long streamlength \u003d Long.parseLong(cl);\n    filelength \u003d startPos + streamlength;\n    // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n    // the reads so the transfer blocks until the server times out\n    InputStream is \u003d\n        new BoundedInputStream(connection.getInputStream(), streamlength);\n\n    resolvedURL.setURL(getResolvedUrl(connection));\n    \n    return is;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java",
      "extendedDetails": {}
    },
    "556be2af92b68808aff71937d437ab9948164bb1": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c -1366601 for reverting HDFS-3667.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1367407 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/07/12 9:33 PM",
      "commitName": "556be2af92b68808aff71937d437ab9948164bb1",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "27/07/12 10:57 PM",
      "commitNameOld": "e4eec269d91ae541a321ae2f28ff03310682b3fe",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.94,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,25 @@\n   protected InputStream openInputStream() throws IOException {\n     // Use the original url if no resolved url exists, eg. if\n     // it\u0027s the first time a request is made.\n-    final boolean resolved \u003d resolvedURL.getURL() !\u003d null; \n-    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n+    final URLOpener opener \u003d\n+      (resolvedURL.getURL() \u003d\u003d null) ? originalURL : resolvedURL;\n \n-    final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n+    final HttpURLConnection connection \u003d opener.openConnection(startPos);\n+    connection.connect();\n+    checkResponseCode(connection);\n+\n     final String cl \u003d connection.getHeaderField(StreamFile.CONTENT_LENGTH);\n     if (cl \u003d\u003d null) {\n       throw new IOException(StreamFile.CONTENT_LENGTH+\" header is missing\");\n     }\n     final long streamlength \u003d Long.parseLong(cl);\n     filelength \u003d startPos + streamlength;\n     // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n     // the reads so the transfer blocks until the server times out\n     InputStream is \u003d\n         new BoundedInputStream(connection.getInputStream(), streamlength);\n \n     resolvedURL.setURL(getResolvedUrl(connection));\n     \n     return is;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected InputStream openInputStream() throws IOException {\n    // Use the original url if no resolved url exists, eg. if\n    // it\u0027s the first time a request is made.\n    final URLOpener opener \u003d\n      (resolvedURL.getURL() \u003d\u003d null) ? originalURL : resolvedURL;\n\n    final HttpURLConnection connection \u003d opener.openConnection(startPos);\n    connection.connect();\n    checkResponseCode(connection);\n\n    final String cl \u003d connection.getHeaderField(StreamFile.CONTENT_LENGTH);\n    if (cl \u003d\u003d null) {\n      throw new IOException(StreamFile.CONTENT_LENGTH+\" header is missing\");\n    }\n    final long streamlength \u003d Long.parseLong(cl);\n    filelength \u003d startPos + streamlength;\n    // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n    // the reads so the transfer blocks until the server times out\n    InputStream is \u003d\n        new BoundedInputStream(connection.getInputStream(), streamlength);\n\n    resolvedURL.setURL(getResolvedUrl(connection));\n    \n    return is;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java",
      "extendedDetails": {}
    },
    "e4eec269d91ae541a321ae2f28ff03310682b3fe": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3667.  Add retry support to WebHdfsFileSystem.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1366601 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/07/12 10:57 PM",
      "commitName": "e4eec269d91ae541a321ae2f28ff03310682b3fe",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "27/04/12 1:13 PM",
      "commitNameOld": "920b8fac187de859307ae960b7abd456e23d87e6",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 91.41,
      "commitsBetweenForRepo": 492,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,22 @@\n   protected InputStream openInputStream() throws IOException {\n     // Use the original url if no resolved url exists, eg. if\n     // it\u0027s the first time a request is made.\n-    final URLOpener opener \u003d\n-      (resolvedURL.getURL() \u003d\u003d null) ? originalURL : resolvedURL;\n+    final boolean resolved \u003d resolvedURL.getURL() !\u003d null; \n+    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n \n-    final HttpURLConnection connection \u003d opener.openConnection(startPos);\n-    connection.connect();\n-    checkResponseCode(connection);\n-\n+    final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n     final String cl \u003d connection.getHeaderField(StreamFile.CONTENT_LENGTH);\n     if (cl \u003d\u003d null) {\n       throw new IOException(StreamFile.CONTENT_LENGTH+\" header is missing\");\n     }\n     final long streamlength \u003d Long.parseLong(cl);\n     filelength \u003d startPos + streamlength;\n     // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n     // the reads so the transfer blocks until the server times out\n     InputStream is \u003d\n         new BoundedInputStream(connection.getInputStream(), streamlength);\n \n     resolvedURL.setURL(getResolvedUrl(connection));\n     \n     return is;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected InputStream openInputStream() throws IOException {\n    // Use the original url if no resolved url exists, eg. if\n    // it\u0027s the first time a request is made.\n    final boolean resolved \u003d resolvedURL.getURL() !\u003d null; \n    final URLOpener opener \u003d resolved? resolvedURL: originalURL;\n\n    final HttpURLConnection connection \u003d opener.connect(startPos, resolved);\n    final String cl \u003d connection.getHeaderField(StreamFile.CONTENT_LENGTH);\n    if (cl \u003d\u003d null) {\n      throw new IOException(StreamFile.CONTENT_LENGTH+\" header is missing\");\n    }\n    final long streamlength \u003d Long.parseLong(cl);\n    filelength \u003d startPos + streamlength;\n    // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n    // the reads so the transfer blocks until the server times out\n    InputStream is \u003d\n        new BoundedInputStream(connection.getInputStream(), streamlength);\n\n    resolvedURL.setURL(getResolvedUrl(connection));\n    \n    return is;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java",
      "extendedDetails": {}
    },
    "920b8fac187de859307ae960b7abd456e23d87e6": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3334. Fix ByteRangeInputStream stream leakage.  Contributed by Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1331570 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/04/12 1:13 PM",
      "commitName": "920b8fac187de859307ae960b7abd456e23d87e6",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,25 @@\n+  protected InputStream openInputStream() throws IOException {\n+    // Use the original url if no resolved url exists, eg. if\n+    // it\u0027s the first time a request is made.\n+    final URLOpener opener \u003d\n+      (resolvedURL.getURL() \u003d\u003d null) ? originalURL : resolvedURL;\n+\n+    final HttpURLConnection connection \u003d opener.openConnection(startPos);\n+    connection.connect();\n+    checkResponseCode(connection);\n+\n+    final String cl \u003d connection.getHeaderField(StreamFile.CONTENT_LENGTH);\n+    if (cl \u003d\u003d null) {\n+      throw new IOException(StreamFile.CONTENT_LENGTH+\" header is missing\");\n+    }\n+    final long streamlength \u003d Long.parseLong(cl);\n+    filelength \u003d startPos + streamlength;\n+    // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n+    // the reads so the transfer blocks until the server times out\n+    InputStream is \u003d\n+        new BoundedInputStream(connection.getInputStream(), streamlength);\n+\n+    resolvedURL.setURL(getResolvedUrl(connection));\n+    \n+    return is;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected InputStream openInputStream() throws IOException {\n    // Use the original url if no resolved url exists, eg. if\n    // it\u0027s the first time a request is made.\n    final URLOpener opener \u003d\n      (resolvedURL.getURL() \u003d\u003d null) ? originalURL : resolvedURL;\n\n    final HttpURLConnection connection \u003d opener.openConnection(startPos);\n    connection.connect();\n    checkResponseCode(connection);\n\n    final String cl \u003d connection.getHeaderField(StreamFile.CONTENT_LENGTH);\n    if (cl \u003d\u003d null) {\n      throw new IOException(StreamFile.CONTENT_LENGTH+\" header is missing\");\n    }\n    final long streamlength \u003d Long.parseLong(cl);\n    filelength \u003d startPos + streamlength;\n    // Java has a bug with \u003e2GB request streams.  It won\u0027t bounds check\n    // the reads so the transfer blocks until the server times out\n    InputStream is \u003d\n        new BoundedInputStream(connection.getInputStream(), streamlength);\n\n    resolvedURL.setURL(getResolvedUrl(connection));\n    \n    return is;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java"
    }
  }
}