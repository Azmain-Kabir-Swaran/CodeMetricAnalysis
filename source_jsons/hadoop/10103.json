{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataNodeMetricHelper.java",
  "functionName": "getMetrics",
  "functionId": "getMetrics___collector-MetricsCollector__beanClass-FSDatasetMBean__context-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetricHelper.java",
  "functionStartLine": 39,
  "functionEndLine": 77,
  "numCommitsSeen": 1,
  "timeTaken": 1684,
  "changeHistory": [
    "feb68cb5470dc3e6c16b6bc1549141613e360601"
  ],
  "changeHistoryShort": {
    "feb68cb5470dc3e6c16b6bc1549141613e360601": "Yintroduced"
  },
  "changeHistoryDetails": {
    "feb68cb5470dc3e6c16b6bc1549141613e360601": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8232. Missing datanode counters when using Metrics2 sink interface. Contributed by Anu Engineer.\n",
      "commitDate": "27/04/15 4:48 PM",
      "commitName": "feb68cb5470dc3e6c16b6bc1549141613e360601",
      "commitAuthor": "cnauroth",
      "diff": "@@ -0,0 +1,39 @@\n+  public static void getMetrics(MetricsCollector collector,\n+                                FSDatasetMBean beanClass, String context)\n+    throws IOException {\n+\n+    if (beanClass \u003d\u003d null) {\n+      throw new IOException(\"beanClass cannot be null\");\n+    }\n+\n+    String className \u003d beanClass.getClass().getName();\n+\n+    collector.addRecord(className)\n+      .setContext(context)\n+      .addGauge(Interns.info(\"Capacity\", \"Total storage capacity\"),\n+        beanClass.getCapacity())\n+      .addGauge(Interns.info(\"DfsUsed\", \"Total bytes used by dfs datanode\"),\n+        beanClass.getDfsUsed())\n+      .addGauge(Interns.info(\"Remaining\", \"Total bytes of free storage\"),\n+        beanClass.getRemaining())\n+      .add(new MetricsTag(Interns.info(\"StorageInfo\", \"Storage ID\"),\n+        beanClass.getStorageInfo()))\n+      .addGauge(Interns.info(\"NumFailedVolumes\", \"Number of failed Volumes\" +\n+        \" in the data Node\"), beanClass.getNumFailedVolumes())\n+      .addGauge(Interns.info(\"LastVolumeFailureDate\", \"Last Volume failure in\" +\n+        \" milliseconds from epoch\"), beanClass.getLastVolumeFailureDate())\n+      .addGauge(Interns.info(\"EstimatedCapacityLostTotal\", \"Total capacity lost\"\n+        + \" due to volume failure\"), beanClass.getEstimatedCapacityLostTotal())\n+      .addGauge(Interns.info(\"CacheUsed\", \"Datanode cache used in bytes\"),\n+        beanClass.getCacheUsed())\n+      .addGauge(Interns.info(\"CacheCapacity\", \"Datanode cache capacity\"),\n+        beanClass.getCacheCapacity())\n+      .addGauge(Interns.info(\"NumBlocksCached\", \"Datanode number\" +\n+        \" of blocks cached\"), beanClass.getNumBlocksCached())\n+      .addGauge(Interns.info(\"NumBlocksFailedToCache\", \"Datanode number of \" +\n+        \"blocks failed to cache\"), beanClass.getNumBlocksFailedToCache())\n+      .addGauge(Interns.info(\"NumBlocksFailedToUnCache\", \"Datanode number of\" +\n+          \" blocks failed in cache eviction\"),\n+        beanClass.getNumBlocksFailedToUncache());\n+\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static void getMetrics(MetricsCollector collector,\n                                FSDatasetMBean beanClass, String context)\n    throws IOException {\n\n    if (beanClass \u003d\u003d null) {\n      throw new IOException(\"beanClass cannot be null\");\n    }\n\n    String className \u003d beanClass.getClass().getName();\n\n    collector.addRecord(className)\n      .setContext(context)\n      .addGauge(Interns.info(\"Capacity\", \"Total storage capacity\"),\n        beanClass.getCapacity())\n      .addGauge(Interns.info(\"DfsUsed\", \"Total bytes used by dfs datanode\"),\n        beanClass.getDfsUsed())\n      .addGauge(Interns.info(\"Remaining\", \"Total bytes of free storage\"),\n        beanClass.getRemaining())\n      .add(new MetricsTag(Interns.info(\"StorageInfo\", \"Storage ID\"),\n        beanClass.getStorageInfo()))\n      .addGauge(Interns.info(\"NumFailedVolumes\", \"Number of failed Volumes\" +\n        \" in the data Node\"), beanClass.getNumFailedVolumes())\n      .addGauge(Interns.info(\"LastVolumeFailureDate\", \"Last Volume failure in\" +\n        \" milliseconds from epoch\"), beanClass.getLastVolumeFailureDate())\n      .addGauge(Interns.info(\"EstimatedCapacityLostTotal\", \"Total capacity lost\"\n        + \" due to volume failure\"), beanClass.getEstimatedCapacityLostTotal())\n      .addGauge(Interns.info(\"CacheUsed\", \"Datanode cache used in bytes\"),\n        beanClass.getCacheUsed())\n      .addGauge(Interns.info(\"CacheCapacity\", \"Datanode cache capacity\"),\n        beanClass.getCacheCapacity())\n      .addGauge(Interns.info(\"NumBlocksCached\", \"Datanode number\" +\n        \" of blocks cached\"), beanClass.getNumBlocksCached())\n      .addGauge(Interns.info(\"NumBlocksFailedToCache\", \"Datanode number of \" +\n        \"blocks failed to cache\"), beanClass.getNumBlocksFailedToCache())\n      .addGauge(Interns.info(\"NumBlocksFailedToUnCache\", \"Datanode number of\" +\n          \" blocks failed in cache eviction\"),\n        beanClass.getNumBlocksFailedToUncache());\n\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetricHelper.java"
    }
  }
}