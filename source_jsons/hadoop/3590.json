{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "getBlockReader",
  "functionId": "getBlockReader___targetBlock-LocatedBlock__offsetInBlock-long__length-long__targetAddr-InetSocketAddress__storageType-StorageType__datanode-DatanodeInfo",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 724,
  "functionEndLine": 753,
  "numCommitsSeen": 136,
  "timeTaken": 3038,
  "changeHistory": [
    "5d748bd056a32f2c6922514cd0c5b31d866a9919",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "439614b0c8a3df3d8b7967451c5331a0e034e13a"
  ],
  "changeHistoryShort": {
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "439614b0c8a3df3d8b7967451c5331a0e034e13a": "Yintroduced"
  },
  "changeHistoryDetails": {
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13702. Remove HTrace hooks from DFSClient to reduce CPU usage. Contributed by Todd Lipcon.\n",
      "commitDate": "02/07/18 3:11 AM",
      "commitName": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "02/07/18 3:02 AM",
      "commitNameOld": "6ba99741086170b83c38d3e7e715d9e8046a1e00",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,30 @@\n   protected BlockReader getBlockReader(LocatedBlock targetBlock,\n       long offsetInBlock, long length, InetSocketAddress targetAddr,\n       StorageType storageType, DatanodeInfo datanode) throws IOException {\n     ExtendedBlock blk \u003d targetBlock.getBlock();\n     Token\u003cBlockTokenIdentifier\u003e accessToken \u003d targetBlock.getBlockToken();\n     CachingStrategy curCachingStrategy;\n     boolean shortCircuitForbidden;\n     synchronized (infoLock) {\n       curCachingStrategy \u003d cachingStrategy;\n       shortCircuitForbidden \u003d shortCircuitForbidden();\n     }\n     return new BlockReaderFactory(dfsClient.getConf()).\n         setInetSocketAddress(targetAddr).\n         setRemotePeerFactory(dfsClient).\n         setDatanodeInfo(datanode).\n         setStorageType(storageType).\n         setFileName(src).\n         setBlock(blk).\n         setBlockToken(accessToken).\n         setStartOffset(offsetInBlock).\n         setVerifyChecksum(verifyChecksum).\n         setClientName(dfsClient.clientName).\n         setLength(length).\n         setCachingStrategy(curCachingStrategy).\n         setAllowShortCircuitLocalReads(!shortCircuitForbidden).\n         setClientCacheContext(dfsClient.getClientContext()).\n         setUserGroupInformation(dfsClient.ugi).\n         setConfiguration(dfsClient.getConfiguration()).\n-        setTracer(dfsClient.getTracer()).\n         build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected BlockReader getBlockReader(LocatedBlock targetBlock,\n      long offsetInBlock, long length, InetSocketAddress targetAddr,\n      StorageType storageType, DatanodeInfo datanode) throws IOException {\n    ExtendedBlock blk \u003d targetBlock.getBlock();\n    Token\u003cBlockTokenIdentifier\u003e accessToken \u003d targetBlock.getBlockToken();\n    CachingStrategy curCachingStrategy;\n    boolean shortCircuitForbidden;\n    synchronized (infoLock) {\n      curCachingStrategy \u003d cachingStrategy;\n      shortCircuitForbidden \u003d shortCircuitForbidden();\n    }\n    return new BlockReaderFactory(dfsClient.getConf()).\n        setInetSocketAddress(targetAddr).\n        setRemotePeerFactory(dfsClient).\n        setDatanodeInfo(datanode).\n        setStorageType(storageType).\n        setFileName(src).\n        setBlock(blk).\n        setBlockToken(accessToken).\n        setStartOffset(offsetInBlock).\n        setVerifyChecksum(verifyChecksum).\n        setClientName(dfsClient.clientName).\n        setLength(length).\n        setCachingStrategy(curCachingStrategy).\n        setAllowShortCircuitLocalReads(!shortCircuitForbidden).\n        setClientCacheContext(dfsClient.getClientContext()).\n        setUserGroupInformation(dfsClient.ugi).\n        setConfiguration(dfsClient.getConfiguration()).\n        build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/09/15 11:08 AM",
      "commitNameOld": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.86,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,31 @@\n   protected BlockReader getBlockReader(LocatedBlock targetBlock,\n       long offsetInBlock, long length, InetSocketAddress targetAddr,\n       StorageType storageType, DatanodeInfo datanode) throws IOException {\n     ExtendedBlock blk \u003d targetBlock.getBlock();\n     Token\u003cBlockTokenIdentifier\u003e accessToken \u003d targetBlock.getBlockToken();\n     CachingStrategy curCachingStrategy;\n     boolean shortCircuitForbidden;\n     synchronized (infoLock) {\n       curCachingStrategy \u003d cachingStrategy;\n       shortCircuitForbidden \u003d shortCircuitForbidden();\n     }\n     return new BlockReaderFactory(dfsClient.getConf()).\n         setInetSocketAddress(targetAddr).\n         setRemotePeerFactory(dfsClient).\n         setDatanodeInfo(datanode).\n         setStorageType(storageType).\n         setFileName(src).\n         setBlock(blk).\n         setBlockToken(accessToken).\n         setStartOffset(offsetInBlock).\n         setVerifyChecksum(verifyChecksum).\n         setClientName(dfsClient.clientName).\n         setLength(length).\n         setCachingStrategy(curCachingStrategy).\n         setAllowShortCircuitLocalReads(!shortCircuitForbidden).\n         setClientCacheContext(dfsClient.getClientContext()).\n         setUserGroupInformation(dfsClient.ugi).\n         setConfiguration(dfsClient.getConfiguration()).\n+        setTracer(dfsClient.getTracer()).\n         build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected BlockReader getBlockReader(LocatedBlock targetBlock,\n      long offsetInBlock, long length, InetSocketAddress targetAddr,\n      StorageType storageType, DatanodeInfo datanode) throws IOException {\n    ExtendedBlock blk \u003d targetBlock.getBlock();\n    Token\u003cBlockTokenIdentifier\u003e accessToken \u003d targetBlock.getBlockToken();\n    CachingStrategy curCachingStrategy;\n    boolean shortCircuitForbidden;\n    synchronized (infoLock) {\n      curCachingStrategy \u003d cachingStrategy;\n      shortCircuitForbidden \u003d shortCircuitForbidden();\n    }\n    return new BlockReaderFactory(dfsClient.getConf()).\n        setInetSocketAddress(targetAddr).\n        setRemotePeerFactory(dfsClient).\n        setDatanodeInfo(datanode).\n        setStorageType(storageType).\n        setFileName(src).\n        setBlock(blk).\n        setBlockToken(accessToken).\n        setStartOffset(offsetInBlock).\n        setVerifyChecksum(verifyChecksum).\n        setClientName(dfsClient.clientName).\n        setLength(length).\n        setCachingStrategy(curCachingStrategy).\n        setAllowShortCircuitLocalReads(!shortCircuitForbidden).\n        setClientCacheContext(dfsClient.getClientContext()).\n        setUserGroupInformation(dfsClient.ugi).\n        setConfiguration(dfsClient.getConfiguration()).\n        setTracer(dfsClient.getTracer()).\n        build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected BlockReader getBlockReader(LocatedBlock targetBlock,\n      long offsetInBlock, long length, InetSocketAddress targetAddr,\n      StorageType storageType, DatanodeInfo datanode) throws IOException {\n    ExtendedBlock blk \u003d targetBlock.getBlock();\n    Token\u003cBlockTokenIdentifier\u003e accessToken \u003d targetBlock.getBlockToken();\n    CachingStrategy curCachingStrategy;\n    boolean shortCircuitForbidden;\n    synchronized (infoLock) {\n      curCachingStrategy \u003d cachingStrategy;\n      shortCircuitForbidden \u003d shortCircuitForbidden();\n    }\n    return new BlockReaderFactory(dfsClient.getConf()).\n        setInetSocketAddress(targetAddr).\n        setRemotePeerFactory(dfsClient).\n        setDatanodeInfo(datanode).\n        setStorageType(storageType).\n        setFileName(src).\n        setBlock(blk).\n        setBlockToken(accessToken).\n        setStartOffset(offsetInBlock).\n        setVerifyChecksum(verifyChecksum).\n        setClientName(dfsClient.clientName).\n        setLength(length).\n        setCachingStrategy(curCachingStrategy).\n        setAllowShortCircuitLocalReads(!shortCircuitForbidden).\n        setClientCacheContext(dfsClient.getClientContext()).\n        setUserGroupInformation(dfsClient.ugi).\n        setConfiguration(dfsClient.getConfiguration()).\n        build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "439614b0c8a3df3d8b7967451c5331a0e034e13a": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8280. Code Cleanup in DFSInputStream. Contributed by Jing Zhao.\n",
      "commitDate": "28/04/15 6:11 PM",
      "commitName": "439614b0c8a3df3d8b7967451c5331a0e034e13a",
      "commitAuthor": "Haohui Mai",
      "diff": "@@ -0,0 +1,30 @@\n+  protected BlockReader getBlockReader(LocatedBlock targetBlock,\n+      long offsetInBlock, long length, InetSocketAddress targetAddr,\n+      StorageType storageType, DatanodeInfo datanode) throws IOException {\n+    ExtendedBlock blk \u003d targetBlock.getBlock();\n+    Token\u003cBlockTokenIdentifier\u003e accessToken \u003d targetBlock.getBlockToken();\n+    CachingStrategy curCachingStrategy;\n+    boolean shortCircuitForbidden;\n+    synchronized (infoLock) {\n+      curCachingStrategy \u003d cachingStrategy;\n+      shortCircuitForbidden \u003d shortCircuitForbidden();\n+    }\n+    return new BlockReaderFactory(dfsClient.getConf()).\n+        setInetSocketAddress(targetAddr).\n+        setRemotePeerFactory(dfsClient).\n+        setDatanodeInfo(datanode).\n+        setStorageType(storageType).\n+        setFileName(src).\n+        setBlock(blk).\n+        setBlockToken(accessToken).\n+        setStartOffset(offsetInBlock).\n+        setVerifyChecksum(verifyChecksum).\n+        setClientName(dfsClient.clientName).\n+        setLength(length).\n+        setCachingStrategy(curCachingStrategy).\n+        setAllowShortCircuitLocalReads(!shortCircuitForbidden).\n+        setClientCacheContext(dfsClient.getClientContext()).\n+        setUserGroupInformation(dfsClient.ugi).\n+        setConfiguration(dfsClient.getConfiguration()).\n+        build();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected BlockReader getBlockReader(LocatedBlock targetBlock,\n      long offsetInBlock, long length, InetSocketAddress targetAddr,\n      StorageType storageType, DatanodeInfo datanode) throws IOException {\n    ExtendedBlock blk \u003d targetBlock.getBlock();\n    Token\u003cBlockTokenIdentifier\u003e accessToken \u003d targetBlock.getBlockToken();\n    CachingStrategy curCachingStrategy;\n    boolean shortCircuitForbidden;\n    synchronized (infoLock) {\n      curCachingStrategy \u003d cachingStrategy;\n      shortCircuitForbidden \u003d shortCircuitForbidden();\n    }\n    return new BlockReaderFactory(dfsClient.getConf()).\n        setInetSocketAddress(targetAddr).\n        setRemotePeerFactory(dfsClient).\n        setDatanodeInfo(datanode).\n        setStorageType(storageType).\n        setFileName(src).\n        setBlock(blk).\n        setBlockToken(accessToken).\n        setStartOffset(offsetInBlock).\n        setVerifyChecksum(verifyChecksum).\n        setClientName(dfsClient.clientName).\n        setLength(length).\n        setCachingStrategy(curCachingStrategy).\n        setAllowShortCircuitLocalReads(!shortCircuitForbidden).\n        setClientCacheContext(dfsClient.getClientContext()).\n        setUserGroupInformation(dfsClient.ugi).\n        setConfiguration(dfsClient.getConfiguration()).\n        build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}