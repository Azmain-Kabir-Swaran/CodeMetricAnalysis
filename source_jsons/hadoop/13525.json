{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockInfoStriped.java",
  "functionName": "ensureCapacity",
  "functionId": "ensureCapacity___totalSize-int__keepOld-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.java",
  "functionStartLine": 192,
  "functionEndLine": 205,
  "numCommitsSeen": 30,
  "timeTaken": 1869,
  "changeHistory": [
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
    "ba9371492036983a9899398907ab41fe548f29b3"
  ],
  "changeHistoryShort": {
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": "Ybodychange",
    "ba9371492036983a9899398907ab41fe548f29b3": "Yintroduced"
  },
  "changeHistoryDetails": {
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9260. Improve the performance and GC friendliness of NameNode startup and full block reports (Staffan Friberg via cmccabe)\n",
      "commitDate": "02/02/16 11:23 AM",
      "commitName": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "21/12/15 10:47 PM",
      "commitNameOld": "70d6f201260086a3f12beaa317fede2a99639fef",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 42.52,
      "commitsBetweenForRepo": 247,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   private void ensureCapacity(int totalSize, boolean keepOld) {\n     if (getCapacity() \u003c totalSize) {\n-      Object[] old \u003d triplets;\n+      DatanodeStorageInfo[] old \u003d storages;\n       byte[] oldIndices \u003d indices;\n-      triplets \u003d new Object[totalSize * 3];\n+      storages \u003d new DatanodeStorageInfo[totalSize];\n       indices \u003d new byte[totalSize];\n       initIndices();\n \n       if (keepOld) {\n-        System.arraycopy(old, 0, triplets, 0, old.length);\n+        System.arraycopy(old, 0, storages, 0, old.length);\n         System.arraycopy(oldIndices, 0, indices, 0, oldIndices.length);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void ensureCapacity(int totalSize, boolean keepOld) {\n    if (getCapacity() \u003c totalSize) {\n      DatanodeStorageInfo[] old \u003d storages;\n      byte[] oldIndices \u003d indices;\n      storages \u003d new DatanodeStorageInfo[totalSize];\n      indices \u003d new byte[totalSize];\n      initIndices();\n\n      if (keepOld) {\n        System.arraycopy(old, 0, storages, 0, old.length);\n        System.arraycopy(oldIndices, 0, indices, 0, oldIndices.length);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.java",
      "extendedDetails": {}
    },
    "ba9371492036983a9899398907ab41fe548f29b3": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7716. Erasure Coding: extend BlockInfo to handle EC info. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "ba9371492036983a9899398907ab41fe548f29b3",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,14 @@\n+  private void ensureCapacity(int totalSize, boolean keepOld) {\n+    if (getCapacity() \u003c totalSize) {\n+      Object[] old \u003d triplets;\n+      byte[] oldIndices \u003d indices;\n+      triplets \u003d new Object[totalSize * 3];\n+      indices \u003d new byte[totalSize];\n+      initIndices();\n+\n+      if (keepOld) {\n+        System.arraycopy(old, 0, triplets, 0, old.length);\n+        System.arraycopy(oldIndices, 0, indices, 0, oldIndices.length);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void ensureCapacity(int totalSize, boolean keepOld) {\n    if (getCapacity() \u003c totalSize) {\n      Object[] old \u003d triplets;\n      byte[] oldIndices \u003d indices;\n      triplets \u003d new Object[totalSize * 3];\n      indices \u003d new byte[totalSize];\n      initIndices();\n\n      if (keepOld) {\n        System.arraycopy(old, 0, triplets, 0, old.length);\n        System.arraycopy(oldIndices, 0, indices, 0, oldIndices.length);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.java"
    }
  }
}