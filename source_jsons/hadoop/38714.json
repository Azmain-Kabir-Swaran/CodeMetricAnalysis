{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FiCaSchedulerApp.java",
  "functionName": "assignContainers",
  "functionId": "assignContainers___clusterResource-Resource__ps-CandidateNodeSet__FiCaSchedulerNode____currentResourceLimits-ResourceLimits__schedulingMode-SchedulingMode__reservedContainer-RMContainer",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
  "functionStartLine": 956,
  "functionEndLine": 968,
  "numCommitsSeen": 713,
  "timeTaken": 19133,
  "changeHistory": [
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
    "de3b4aac561258ad242a3c5ed1c919428893fd4c",
    "b8a30f2f170ffbd590e7366c3c944ab4919e40df",
    "e5003be907acef87c2770e3f2914953f62017b0e",
    "ba2313d6145a1234777938a747187373f4cd58d9",
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
    "189a63a719c63b67a1783a280bfc2f72dcb55277",
    "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5",
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
    "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
    "487374b7fe0c92fc7eb1406c568952722b5d5b15",
    "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
    "86358221fc85a7743052a0b4c1647353508bf308",
    "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb",
    "f2ea555ac6c06a3f2f6559731f48711fff05d3f1",
    "9c22065109a77681bc2534063eabe8692fbcb3cd",
    "424fd9494f144c035fdef8c533be51e2027ad8d9",
    "44b6261bfacddea88a3cf02d406f970bbbb98d04",
    "d0a5e43de73119e57d12f2ec89a9d1a192cde204",
    "1e513bfc68c8de2976e3340cb83b6763c5d16813",
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c",
    "a2c42330047bf955a6a585dcddf798920d4c8640",
    "ca8024673178fa1c80224b390dfba932921693d9",
    "453926397182078c65a4428eb5de5a90d6af6448",
    "90ba993bc72e374f99c44d0770f55aeaa8342f2d",
    "e1fdf62123625e4ba399af02f8aad500637d29d1",
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76",
    "126dd6adefeb00e4ba81ea137d63a8a76b75c3bd",
    "ffdf980b2056b2a1b31ccb19746f23c31f7d08ef",
    "f24dcb3449c77da665058427bc7fa480cad507fc",
    "1e6dfa7472ad78a252d05c8ebffe086d938b61fa",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517"
  ],
  "changeHistoryShort": {
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949": "Yparameterchange",
    "de3b4aac561258ad242a3c5ed1c919428893fd4c": "Ymultichange(Yparameterchange,Ybodychange)",
    "b8a30f2f170ffbd590e7366c3c944ab4919e40df": "Ybodychange",
    "e5003be907acef87c2770e3f2914953f62017b0e": "Ymultichange(Yparameterchange,Ybodychange)",
    "ba2313d6145a1234777938a747187373f4cd58d9": "Ybodychange",
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yrename,Yparameterchange)",
    "189a63a719c63b67a1783a280bfc2f72dcb55277": "Ymultichange(Yparameterchange,Ybodychange)",
    "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5": "Ybodychange",
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5": "Ymultichange(Yparameterchange,Ybodychange)",
    "afa5d4715a3aea2a6e93380b014c7bb8f0880383": "Ymultichange(Yreturntypechange,Ybodychange)",
    "487374b7fe0c92fc7eb1406c568952722b5d5b15": "Ymultichange(Yparameterchange,Ybodychange)",
    "e17e5ba9d7e2bd45ba6884f59f8045817594b284": "Ymultichange(Yparameterchange,Ybodychange)",
    "86358221fc85a7743052a0b4c1647353508bf308": "Ybodychange",
    "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb": "Ybodychange",
    "f2ea555ac6c06a3f2f6559731f48711fff05d3f1": "Ybodychange",
    "9c22065109a77681bc2534063eabe8692fbcb3cd": "Ymultichange(Yparameterchange,Ybodychange)",
    "424fd9494f144c035fdef8c533be51e2027ad8d9": "Ybodychange",
    "44b6261bfacddea88a3cf02d406f970bbbb98d04": "Ybodychange",
    "d0a5e43de73119e57d12f2ec89a9d1a192cde204": "Ybodychange",
    "1e513bfc68c8de2976e3340cb83b6763c5d16813": "Ybodychange",
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c": "Ybodychange",
    "a2c42330047bf955a6a585dcddf798920d4c8640": "Ybodychange",
    "ca8024673178fa1c80224b390dfba932921693d9": "Ybodychange",
    "453926397182078c65a4428eb5de5a90d6af6448": "Ybodychange",
    "90ba993bc72e374f99c44d0770f55aeaa8342f2d": "Ybodychange",
    "e1fdf62123625e4ba399af02f8aad500637d29d1": "Yfilerename",
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76": "Yparameterchange",
    "126dd6adefeb00e4ba81ea137d63a8a76b75c3bd": "Ybodychange",
    "ffdf980b2056b2a1b31ccb19746f23c31f7d08ef": "Ybodychange",
    "f24dcb3449c77da665058427bc7fa480cad507fc": "Ybodychange",
    "1e6dfa7472ad78a252d05c8ebffe086d938b61fa": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yintroduced"
  },
  "changeHistoryDetails": {
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949": {
      "type": "Yparameterchange",
      "commitMessage": "YARN-7437. Rename PlacementSet and SchedulingPlacementSet. (Wangda Tan via kkaranasos)\n",
      "commitDate": "09/11/17 1:01 PM",
      "commitName": "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
      "commitAuthor": "Konstantinos Karanasos",
      "commitDateOld": "17/09/17 9:20 PM",
      "commitNameOld": "e81596d06d226f1cfa44b2390ce3095ed4dee621",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 52.69,
      "commitsBetweenForRepo": 485,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,13 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n-      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n-      SchedulingMode schedulingMode, RMContainer reservedContainer) {\n+      CandidateNodeSet\u003cFiCaSchedulerNode\u003e ps,\n+      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode,\n+      RMContainer reservedContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pre-assignContainers for application \"\n           + getApplicationId());\n       showRequests();\n     }\n \n     return containerAllocator.assignContainers(clusterResource, ps,\n         schedulingMode, currentResourceLimits, reservedContainer);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e ps,\n      ResourceLimits currentResourceLimits, SchedulingMode schedulingMode,\n      RMContainer reservedContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    return containerAllocator.assignContainers(clusterResource, ps,\n        schedulingMode, currentResourceLimits, reservedContainer);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
      "extendedDetails": {
        "oldValue": "[clusterResource-Resource, ps-PlacementSet\u003cFiCaSchedulerNode\u003e, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode, reservedContainer-RMContainer]",
        "newValue": "[clusterResource-Resource, ps-CandidateNodeSet\u003cFiCaSchedulerNode\u003e, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode, reservedContainer-RMContainer]"
      }
    },
    "de3b4aac561258ad242a3c5ed1c919428893fd4c": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
      "commitDate": "07/11/16 10:14 AM",
      "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
          "commitDate": "07/11/16 10:14 AM",
          "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
          "commitAuthor": "Jian He",
          "commitDateOld": "31/10/16 3:18 PM",
          "commitNameOld": "90dd3a8148468ac37a3f2173ad8d45e38bfcb0c9",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 6.83,
          "commitsBetweenForRepo": 84,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,12 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n-      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n+      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode, RMContainer reservedContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pre-assignContainers for application \"\n           + getApplicationId());\n       showRequests();\n     }\n \n-    try {\n-      writeLock.lock();\n-      return containerAllocator.assignContainers(clusterResource, node,\n-          schedulingMode, currentResourceLimits, reservedContainer);\n-    } finally {\n-      writeLock.unlock();\n-    }\n+    return containerAllocator.assignContainers(clusterResource, ps,\n+        schedulingMode, currentResourceLimits, reservedContainer);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode, RMContainer reservedContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    return containerAllocator.assignContainers(clusterResource, ps,\n        schedulingMode, currentResourceLimits, reservedContainer);\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode, reservedContainer-RMContainer]",
            "newValue": "[clusterResource-Resource, ps-PlacementSet\u003cFiCaSchedulerNode\u003e, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode, reservedContainer-RMContainer]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
          "commitDate": "07/11/16 10:14 AM",
          "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
          "commitAuthor": "Jian He",
          "commitDateOld": "31/10/16 3:18 PM",
          "commitNameOld": "90dd3a8148468ac37a3f2173ad8d45e38bfcb0c9",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 6.83,
          "commitsBetweenForRepo": 84,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,12 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n-      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n+      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode, RMContainer reservedContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pre-assignContainers for application \"\n           + getApplicationId());\n       showRequests();\n     }\n \n-    try {\n-      writeLock.lock();\n-      return containerAllocator.assignContainers(clusterResource, node,\n-          schedulingMode, currentResourceLimits, reservedContainer);\n-    } finally {\n-      writeLock.unlock();\n-    }\n+    return containerAllocator.assignContainers(clusterResource, ps,\n+        schedulingMode, currentResourceLimits, reservedContainer);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      PlacementSet\u003cFiCaSchedulerNode\u003e ps, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode, RMContainer reservedContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    return containerAllocator.assignContainers(clusterResource, ps,\n        schedulingMode, currentResourceLimits, reservedContainer);\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {}
        }
      ]
    },
    "b8a30f2f170ffbd590e7366c3c944ab4919e40df": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3141. Improve locks in SchedulerApplicationAttempt/FSAppAttempt/FiCaSchedulerApp. Contributed by Wangda Tan\n",
      "commitDate": "19/09/16 2:08 AM",
      "commitName": "b8a30f2f170ffbd590e7366c3c944ab4919e40df",
      "commitAuthor": "Jian He",
      "commitDateOld": "02/09/16 3:32 AM",
      "commitNameOld": "05f5c0f631680cffc36a79550c351620615445db",
      "commitAuthorOld": "Varun Vasudev",
      "daysBetweenCommits": 16.94,
      "commitsBetweenForRepo": 79,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,17 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode, RMContainer reservedContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pre-assignContainers for application \"\n           + getApplicationId());\n       showRequests();\n     }\n \n-    synchronized (this) {\n+    try {\n+      writeLock.lock();\n       return containerAllocator.assignContainers(clusterResource, node,\n           schedulingMode, currentResourceLimits, reservedContainer);\n+    } finally {\n+      writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode, RMContainer reservedContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    try {\n      writeLock.lock();\n      return containerAllocator.assignContainers(clusterResource, node,\n          schedulingMode, currentResourceLimits, reservedContainer);\n    } finally {\n      writeLock.unlock();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
      "extendedDetails": {}
    },
    "e5003be907acef87c2770e3f2914953f62017b0e": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-4026. Refactored ContainerAllocator to accept a list of priorites rather than a single priority. Contributed by Wangda Tan\n",
      "commitDate": "12/08/15 3:07 PM",
      "commitName": "e5003be907acef87c2770e3f2914953f62017b0e",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-4026. Refactored ContainerAllocator to accept a list of priorites rather than a single priority. Contributed by Wangda Tan\n",
          "commitDate": "12/08/15 3:07 PM",
          "commitName": "e5003be907acef87c2770e3f2914953f62017b0e",
          "commitAuthor": "Jian He",
          "commitDateOld": "07/08/15 9:46 AM",
          "commitNameOld": "4bc42d76e7fa53cb268cab0f9fe1fd8d8dbb17fd",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 5.22,
          "commitsBetweenForRepo": 11,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,14 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n-      SchedulingMode schedulingMode) {\n+      SchedulingMode schedulingMode, RMContainer reservedContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pre-assignContainers for application \"\n           + getApplicationId());\n       showRequests();\n     }\n \n-    // Check if application needs more resource, skip if it doesn\u0027t need more.\n-    if (!hasPendingResourceRequest(rc,\n-        node.getPartition(), clusterResource, schedulingMode)) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n-            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n-            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n-      }\n-      return CSAssignment.SKIP_ASSIGNMENT;\n-    }\n-\n     synchronized (this) {\n-      // Schedule in priority order\n-      for (Priority priority : getPriorities()) {\n-        ContainerAllocation allocationResult \u003d\n-            containerAllocator.allocate(clusterResource, node,\n-                schedulingMode, currentResourceLimits, priority, null);\n-\n-        // If it\u0027s a skipped allocation\n-        AllocationState allocationState \u003d allocationResult.getAllocationState();\n-\n-        if (allocationState \u003d\u003d AllocationState.PRIORITY_SKIPPED) {\n-          continue;\n-        }\n-        return getCSAssignmentFromAllocateResult(clusterResource,\n-            allocationResult);\n-      }\n+      return containerAllocator.assignContainers(clusterResource, node,\n+          schedulingMode, currentResourceLimits, reservedContainer);\n     }\n-\n-    // We will reach here if we skipped all priorities of the app, so we will\n-    // skip the app.\n-    return CSAssignment.SKIP_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode, RMContainer reservedContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    synchronized (this) {\n      return containerAllocator.assignContainers(clusterResource, node,\n          schedulingMode, currentResourceLimits, reservedContainer);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode, reservedContainer-RMContainer]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-4026. Refactored ContainerAllocator to accept a list of priorites rather than a single priority. Contributed by Wangda Tan\n",
          "commitDate": "12/08/15 3:07 PM",
          "commitName": "e5003be907acef87c2770e3f2914953f62017b0e",
          "commitAuthor": "Jian He",
          "commitDateOld": "07/08/15 9:46 AM",
          "commitNameOld": "4bc42d76e7fa53cb268cab0f9fe1fd8d8dbb17fd",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 5.22,
          "commitsBetweenForRepo": 11,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,14 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n-      SchedulingMode schedulingMode) {\n+      SchedulingMode schedulingMode, RMContainer reservedContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pre-assignContainers for application \"\n           + getApplicationId());\n       showRequests();\n     }\n \n-    // Check if application needs more resource, skip if it doesn\u0027t need more.\n-    if (!hasPendingResourceRequest(rc,\n-        node.getPartition(), clusterResource, schedulingMode)) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n-            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n-            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n-      }\n-      return CSAssignment.SKIP_ASSIGNMENT;\n-    }\n-\n     synchronized (this) {\n-      // Schedule in priority order\n-      for (Priority priority : getPriorities()) {\n-        ContainerAllocation allocationResult \u003d\n-            containerAllocator.allocate(clusterResource, node,\n-                schedulingMode, currentResourceLimits, priority, null);\n-\n-        // If it\u0027s a skipped allocation\n-        AllocationState allocationState \u003d allocationResult.getAllocationState();\n-\n-        if (allocationState \u003d\u003d AllocationState.PRIORITY_SKIPPED) {\n-          continue;\n-        }\n-        return getCSAssignmentFromAllocateResult(clusterResource,\n-            allocationResult);\n-      }\n+      return containerAllocator.assignContainers(clusterResource, node,\n+          schedulingMode, currentResourceLimits, reservedContainer);\n     }\n-\n-    // We will reach here if we skipped all priorities of the app, so we will\n-    // skip the app.\n-    return CSAssignment.SKIP_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode, RMContainer reservedContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    synchronized (this) {\n      return containerAllocator.assignContainers(clusterResource, node,\n          schedulingMode, currentResourceLimits, reservedContainer);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {}
        }
      ]
    },
    "ba2313d6145a1234777938a747187373f4cd58d9": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3983. Refactored CapacityScheduleri#FiCaSchedulerApp to easier extend container allocation logic. Contributed by Wangda Tan\n",
      "commitDate": "05/08/15 1:47 PM",
      "commitName": "ba2313d6145a1234777938a747187373f4cd58d9",
      "commitAuthor": "Jian He",
      "commitDateOld": "24/07/15 2:00 PM",
      "commitNameOld": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 11.99,
      "commitsBetweenForRepo": 56,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,161 +1,42 @@\n   public CSAssignment assignContainers(Resource clusterResource,\n       FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n       SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pre-assignContainers for application \"\n           + getApplicationId());\n       showRequests();\n     }\n \n     // Check if application needs more resource, skip if it doesn\u0027t need more.\n     if (!hasPendingResourceRequest(rc,\n         node.getPartition(), clusterResource, schedulingMode)) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n             + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n             + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n       }\n-      return SKIP_ASSIGNMENT;\n+      return CSAssignment.SKIP_ASSIGNMENT;\n     }\n \n     synchronized (this) {\n-      // Check if this resource is on the blacklist\n-      if (SchedulerAppUtils.isBlacklisted(this, node, LOG)) {\n-        return SKIP_ASSIGNMENT;\n-      }\n-\n       // Schedule in priority order\n       for (Priority priority : getPriorities()) {\n-        ResourceRequest anyRequest \u003d\n-            getResourceRequest(priority, ResourceRequest.ANY);\n-        if (null \u003d\u003d anyRequest) {\n+        ContainerAllocation allocationResult \u003d\n+            containerAllocator.allocate(clusterResource, node,\n+                schedulingMode, currentResourceLimits, priority, null);\n+\n+        // If it\u0027s a skipped allocation\n+        AllocationState allocationState \u003d allocationResult.getAllocationState();\n+\n+        if (allocationState \u003d\u003d AllocationState.PRIORITY_SKIPPED) {\n           continue;\n         }\n-\n-        // Required resource\n-        Resource required \u003d anyRequest.getCapability();\n-\n-        // Do we need containers at this \u0027priority\u0027?\n-        if (getTotalRequiredResources(priority) \u003c\u003d 0) {\n-          continue;\n-        }\n-\n-        // AM container allocation doesn\u0027t support non-exclusive allocation to\n-        // avoid painful of preempt an AM container\n-        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n-\n-          RMAppAttempt rmAppAttempt \u003d\n-              rmContext.getRMApps()\n-                  .get(getApplicationId()).getCurrentAppAttempt();\n-          if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n-              \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n-                  + getApplicationAttemptId()\n-                  + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n-            }\n-            break;\n-          }\n-        }\n-\n-        // Is the node-label-expression of this offswitch resource request\n-        // matches the node\u0027s label?\n-        // If not match, jump to next priority.\n-        if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n-            anyRequest, node.getPartition(), schedulingMode)) {\n-          continue;\n-        }\n-\n-        if (!getCSLeafQueue().getReservationContinueLooking()) {\n-          if (!shouldAllocOrReserveNewContainer(priority, required)) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n-            }\n-            continue;\n-          }\n-        }\n-\n-        if (!checkHeadroom(clusterResource, currentResourceLimits, required,\n-            node)) {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"cannot allocate required resource\u003d\" + required\n-                + \" because of headroom\");\n-          }\n-          return NULL_ASSIGNMENT;\n-        }\n-\n-        // Inform the application it is about to get a scheduling opportunity\n-        addSchedulingOpportunity(priority);\n-\n-        // Increase missed-non-partitioned-resource-request-opportunity.\n-        // This is to make sure non-partitioned-resource-request will prefer\n-        // to be allocated to non-partitioned nodes\n-        int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n-        if (anyRequest.getNodeLabelExpression().equals(\n-            RMNodeLabelsManager.NO_LABEL)) {\n-          missedNonPartitionedRequestSchedulingOpportunity \u003d\n-              addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n-        }\n-\n-        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n-          // Before doing allocation, we need to check scheduling opportunity to\n-          // make sure : non-partitioned resource request should be scheduled to\n-          // non-partitioned partition first.\n-          if (missedNonPartitionedRequestSchedulingOpportunity \u003c rmContext\n-              .getScheduler().getNumClusterNodes()) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Skip app_attempt\u003d\"\n-                  + getApplicationAttemptId() + \" priority\u003d\"\n-                  + priority\n-                  + \" because missed-non-partitioned-resource-request\"\n-                  + \" opportunity under requred:\" + \" Now\u003d\"\n-                  + missedNonPartitionedRequestSchedulingOpportunity\n-                  + \" required\u003d\"\n-                  + rmContext.getScheduler().getNumClusterNodes());\n-            }\n-\n-            return SKIP_ASSIGNMENT;\n-          }\n-        }\n-\n-        // Try to schedule\n-        CSAssignment assignment \u003d\n-            assignContainersOnNode(clusterResource, node,\n-                priority, null, schedulingMode, currentResourceLimits);\n-\n-        // Did the application skip this node?\n-        if (assignment.getSkipped()) {\n-          // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n-          subtractSchedulingOpportunity(priority);\n-          continue;\n-        }\n-\n-        // Did we schedule or reserve a container?\n-        Resource assigned \u003d assignment.getResource();\n-        if (Resources.greaterThan(rc, clusterResource,\n-            assigned, Resources.none())) {\n-          // Don\u0027t reset scheduling opportunities for offswitch assignments\n-          // otherwise the app will be delayed for each non-local assignment.\n-          // This helps apps with many off-cluster requests schedule faster.\n-          if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Resetting scheduling opportunities\");\n-            }\n-            resetSchedulingOpportunities(priority);\n-          }\n-          // Non-exclusive scheduling opportunity is different: we need reset\n-          // it every time to make sure non-labeled resource request will be\n-          // most likely allocated on non-labeled nodes first.\n-          resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n-\n-          // Done\n-          return assignment;\n-        } else {\n-          // Do not assign out of order w.r.t priorities\n-          return SKIP_ASSIGNMENT;\n-        }\n+        return getCSAssignmentFromAllocateResult(clusterResource,\n+            allocationResult);\n       }\n     }\n \n-    return SKIP_ASSIGNMENT;\n+    // We will reach here if we skipped all priorities of the app, so we will\n+    // skip the app.\n+    return CSAssignment.SKIP_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    // Check if application needs more resource, skip if it doesn\u0027t need more.\n    if (!hasPendingResourceRequest(rc,\n        node.getPartition(), clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n      }\n      return CSAssignment.SKIP_ASSIGNMENT;\n    }\n\n    synchronized (this) {\n      // Schedule in priority order\n      for (Priority priority : getPriorities()) {\n        ContainerAllocation allocationResult \u003d\n            containerAllocator.allocate(clusterResource, node,\n                schedulingMode, currentResourceLimits, priority, null);\n\n        // If it\u0027s a skipped allocation\n        AllocationState allocationState \u003d allocationResult.getAllocationState();\n\n        if (allocationState \u003d\u003d AllocationState.PRIORITY_SKIPPED) {\n          continue;\n        }\n        return getCSAssignmentFromAllocateResult(clusterResource,\n            allocationResult);\n      }\n    }\n\n    // We will reach here if we skipped all priorities of the app, so we will\n    // skip the app.\n    return CSAssignment.SKIP_ASSIGNMENT;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
      "extendedDetails": {}
    },
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
      "commitDate": "24/07/15 2:00 PM",
      "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,159 +1,161 @@\n-  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      FiCaSchedulerApp application, Priority priority, \n-      ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode,\n-      ResourceLimits currentResoureLimits) {\n+  public CSAssignment assignContainers(Resource clusterResource,\n+      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n+      SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId()\n-        + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type);\n+      LOG.debug(\"pre-assignContainers for application \"\n+          + getApplicationId());\n+      showRequests();\n     }\n-    \n-    // check if the resource request can access the label\n-    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n-        node.getPartition(), schedulingMode)) {\n-      // this is a reserved container, but we cannot allocate it now according\n-      // to label not match. This can be caused by node label changed\n-      // We should un-reserve this container.\n-      if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n+\n+    // Check if application needs more resource, skip if it doesn\u0027t need more.\n+    if (!hasPendingResourceRequest(rc,\n+        node.getPartition(), clusterResource, schedulingMode)) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n+            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n       }\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-    \n-    Resource capability \u003d request.getCapability();\n-    Resource available \u003d node.getAvailableResource();\n-    Resource totalResource \u003d node.getTotalResource();\n-\n-    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n-        capability, totalResource)) {\n-      LOG.warn(\"Node : \" + node.getNodeID()\n-          + \" does not have sufficient resource for request : \" + request\n-          + \" node total capability : \" + node.getTotalResource());\n-      return new CSAssignment(Resources.none(), type);\n+      return SKIP_ASSIGNMENT;\n     }\n \n-    assert Resources.greaterThan(\n-        resourceCalculator, clusterResource, available, Resources.none());\n+    synchronized (this) {\n+      // Check if this resource is on the blacklist\n+      if (SchedulerAppUtils.isBlacklisted(this, node, LOG)) {\n+        return SKIP_ASSIGNMENT;\n+      }\n \n-    // Create the container if necessary\n-    Container container \u003d \n-        getContainer(rmContainer, application, node, capability, priority);\n-  \n-    // something went wrong getting/creating the container \n-    if (container \u003d\u003d null) {\n-      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-\n-    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n-        application, priority, capability);\n-\n-    // Can we allocate a container on this node?\n-    int availableContainers \u003d \n-        resourceCalculator.computeAvailableContainers(available, capability);\n-\n-    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n-        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n-\n-    if (availableContainers \u003e 0) {\n-      // Allocate...\n-\n-      // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n-        // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n-        // TODO, need change here when we want to support continuous reservation\n-        // looking for labeled partitions.\n-        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n-          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n-          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n-          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n-          // under the limit.\n-          Resource amountToUnreserve \u003d capability;\n-          if (needToUnreserve) {\n-            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n-          }\n-          boolean containerUnreserved \u003d\n-              findNodeToUnreserve(clusterResource, node, application, priority,\n-                  amountToUnreserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n-          // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n-          if (!containerUnreserved) {\n-            return new CSAssignment(Resources.none(), type);\n-          }\n+      // Schedule in priority order\n+      for (Priority priority : getPriorities()) {\n+        ResourceRequest anyRequest \u003d\n+            getResourceRequest(priority, ResourceRequest.ANY);\n+        if (null \u003d\u003d anyRequest) {\n+          continue;\n         }\n-      }\n \n-      // Inform the application\n-      RMContainer allocatedContainer \u003d \n-          application.allocate(type, node, priority, request, container);\n+        // Required resource\n+        Resource required \u003d anyRequest.getCapability();\n \n-      // Does the application need this resource?\n-      if (allocatedContainer \u003d\u003d null) {\n-        return new CSAssignment(Resources.none(), type);\n-      }\n+        // Do we need containers at this \u0027priority\u0027?\n+        if (getTotalRequiredResources(priority) \u003c\u003d 0) {\n+          continue;\n+        }\n \n-      // Inform the node\n-      node.allocateContainer(allocatedContainer);\n-            \n-      // Inform the ordering policy\n-      orderingPolicy.containerAllocated(application, allocatedContainer);\n+        // AM container allocation doesn\u0027t support non-exclusive allocation to\n+        // avoid painful of preempt an AM container\n+        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n \n-      LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" container\u003d\" + container + \n-          \" queue\u003d\" + this + \n-          \" clusterResource\u003d\" + clusterResource);\n-      createdContainer.setValue(allocatedContainer);\n-      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n-      assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getQueuePath());\n-      assignment.getAssignmentInformation().incrAllocations();\n-      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-        container.getResource());\n-      return assignment;\n-    } else {\n-      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n-      // if this was an already a reserved container, reserve it again\n-      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n-\n-        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n-          // we could possibly ignoring queue capacity or user limits when\n-          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n-          // one.\n-          if (needToUnreserve) {\n+          RMAppAttempt rmAppAttempt \u003d\n+              rmContext.getRMApps()\n+                  .get(getApplicationId()).getCurrentAppAttempt();\n+          if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n+              \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n             if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"we needed to unreserve to be able to allocate\");\n+              LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n+                  + getApplicationAttemptId()\n+                  + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n             }\n-            return new CSAssignment(Resources.none(), type);\n+            break;\n           }\n         }\n \n-        // Reserve by \u0027charging\u0027 in advance...\n-        reserve(application, priority, node, rmContainer, container);\n+        // Is the node-label-expression of this offswitch resource request\n+        // matches the node\u0027s label?\n+        // If not match, jump to next priority.\n+        if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n+            anyRequest, node.getPartition(), schedulingMode)) {\n+          continue;\n+        }\n \n-        LOG.info(\"Reserved container \" + \n-            \" application\u003d\" + application.getApplicationId() + \n-            \" resource\u003d\" + request.getCapability() + \n-            \" queue\u003d\" + this.toString() + \n-            \" usedCapacity\u003d\" + getUsedCapacity() + \n-            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n-            \" used\u003d\" + queueUsage.getUsed() +\n-            \" cluster\u003d\" + clusterResource);\n+        if (!getCSLeafQueue().getReservationContinueLooking()) {\n+          if (!shouldAllocOrReserveNewContainer(priority, required)) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n+            }\n+            continue;\n+          }\n+        }\n+\n+        if (!checkHeadroom(clusterResource, currentResourceLimits, required,\n+            node)) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"cannot allocate required resource\u003d\" + required\n+                + \" because of headroom\");\n+          }\n+          return NULL_ASSIGNMENT;\n+        }\n+\n+        // Inform the application it is about to get a scheduling opportunity\n+        addSchedulingOpportunity(priority);\n+\n+        // Increase missed-non-partitioned-resource-request-opportunity.\n+        // This is to make sure non-partitioned-resource-request will prefer\n+        // to be allocated to non-partitioned nodes\n+        int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n+        if (anyRequest.getNodeLabelExpression().equals(\n+            RMNodeLabelsManager.NO_LABEL)) {\n+          missedNonPartitionedRequestSchedulingOpportunity \u003d\n+              addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+        }\n+\n+        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n+          // Before doing allocation, we need to check scheduling opportunity to\n+          // make sure : non-partitioned resource request should be scheduled to\n+          // non-partitioned partition first.\n+          if (missedNonPartitionedRequestSchedulingOpportunity \u003c rmContext\n+              .getScheduler().getNumClusterNodes()) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Skip app_attempt\u003d\"\n+                  + getApplicationAttemptId() + \" priority\u003d\"\n+                  + priority\n+                  + \" because missed-non-partitioned-resource-request\"\n+                  + \" opportunity under requred:\" + \" Now\u003d\"\n+                  + missedNonPartitionedRequestSchedulingOpportunity\n+                  + \" required\u003d\"\n+                  + rmContext.getScheduler().getNumClusterNodes());\n+            }\n+\n+            return SKIP_ASSIGNMENT;\n+          }\n+        }\n+\n+        // Try to schedule\n         CSAssignment assignment \u003d\n-            new CSAssignment(request.getCapability(), type);\n-        assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getQueuePath());\n-        assignment.getAssignmentInformation().incrReservations();\n-        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n-          request.getCapability());\n-        return assignment;\n+            assignContainersOnNode(clusterResource, node,\n+                priority, null, schedulingMode, currentResourceLimits);\n+\n+        // Did the application skip this node?\n+        if (assignment.getSkipped()) {\n+          // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n+          subtractSchedulingOpportunity(priority);\n+          continue;\n+        }\n+\n+        // Did we schedule or reserve a container?\n+        Resource assigned \u003d assignment.getResource();\n+        if (Resources.greaterThan(rc, clusterResource,\n+            assigned, Resources.none())) {\n+          // Don\u0027t reset scheduling opportunities for offswitch assignments\n+          // otherwise the app will be delayed for each non-local assignment.\n+          // This helps apps with many off-cluster requests schedule faster.\n+          if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Resetting scheduling opportunities\");\n+            }\n+            resetSchedulingOpportunities(priority);\n+          }\n+          // Non-exclusive scheduling opportunity is different: we need reset\n+          // it every time to make sure non-labeled resource request will be\n+          // most likely allocated on non-labeled nodes first.\n+          resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+\n+          // Done\n+          return assignment;\n+        } else {\n+          // Do not assign out of order w.r.t priorities\n+          return SKIP_ASSIGNMENT;\n+        }\n       }\n-      return new CSAssignment(Resources.none(), type);\n     }\n+\n+    return SKIP_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    // Check if application needs more resource, skip if it doesn\u0027t need more.\n    if (!hasPendingResourceRequest(rc,\n        node.getPartition(), clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n      }\n      return SKIP_ASSIGNMENT;\n    }\n\n    synchronized (this) {\n      // Check if this resource is on the blacklist\n      if (SchedulerAppUtils.isBlacklisted(this, node, LOG)) {\n        return SKIP_ASSIGNMENT;\n      }\n\n      // Schedule in priority order\n      for (Priority priority : getPriorities()) {\n        ResourceRequest anyRequest \u003d\n            getResourceRequest(priority, ResourceRequest.ANY);\n        if (null \u003d\u003d anyRequest) {\n          continue;\n        }\n\n        // Required resource\n        Resource required \u003d anyRequest.getCapability();\n\n        // Do we need containers at this \u0027priority\u0027?\n        if (getTotalRequiredResources(priority) \u003c\u003d 0) {\n          continue;\n        }\n\n        // AM container allocation doesn\u0027t support non-exclusive allocation to\n        // avoid painful of preempt an AM container\n        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n\n          RMAppAttempt rmAppAttempt \u003d\n              rmContext.getRMApps()\n                  .get(getApplicationId()).getCurrentAppAttempt();\n          if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n              \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                  + getApplicationAttemptId()\n                  + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n            }\n            break;\n          }\n        }\n\n        // Is the node-label-expression of this offswitch resource request\n        // matches the node\u0027s label?\n        // If not match, jump to next priority.\n        if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n            anyRequest, node.getPartition(), schedulingMode)) {\n          continue;\n        }\n\n        if (!getCSLeafQueue().getReservationContinueLooking()) {\n          if (!shouldAllocOrReserveNewContainer(priority, required)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n            }\n            continue;\n          }\n        }\n\n        if (!checkHeadroom(clusterResource, currentResourceLimits, required,\n            node)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"cannot allocate required resource\u003d\" + required\n                + \" because of headroom\");\n          }\n          return NULL_ASSIGNMENT;\n        }\n\n        // Inform the application it is about to get a scheduling opportunity\n        addSchedulingOpportunity(priority);\n\n        // Increase missed-non-partitioned-resource-request-opportunity.\n        // This is to make sure non-partitioned-resource-request will prefer\n        // to be allocated to non-partitioned nodes\n        int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n        if (anyRequest.getNodeLabelExpression().equals(\n            RMNodeLabelsManager.NO_LABEL)) {\n          missedNonPartitionedRequestSchedulingOpportunity \u003d\n              addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n        }\n\n        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n          // Before doing allocation, we need to check scheduling opportunity to\n          // make sure : non-partitioned resource request should be scheduled to\n          // non-partitioned partition first.\n          if (missedNonPartitionedRequestSchedulingOpportunity \u003c rmContext\n              .getScheduler().getNumClusterNodes()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skip app_attempt\u003d\"\n                  + getApplicationAttemptId() + \" priority\u003d\"\n                  + priority\n                  + \" because missed-non-partitioned-resource-request\"\n                  + \" opportunity under requred:\" + \" Now\u003d\"\n                  + missedNonPartitionedRequestSchedulingOpportunity\n                  + \" required\u003d\"\n                  + rmContext.getScheduler().getNumClusterNodes());\n            }\n\n            return SKIP_ASSIGNMENT;\n          }\n        }\n\n        // Try to schedule\n        CSAssignment assignment \u003d\n            assignContainersOnNode(clusterResource, node,\n                priority, null, schedulingMode, currentResourceLimits);\n\n        // Did the application skip this node?\n        if (assignment.getSkipped()) {\n          // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n          subtractSchedulingOpportunity(priority);\n          continue;\n        }\n\n        // Did we schedule or reserve a container?\n        Resource assigned \u003d assignment.getResource();\n        if (Resources.greaterThan(rc, clusterResource,\n            assigned, Resources.none())) {\n          // Don\u0027t reset scheduling opportunities for offswitch assignments\n          // otherwise the app will be delayed for each non-local assignment.\n          // This helps apps with many off-cluster requests schedule faster.\n          if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Resetting scheduling opportunities\");\n            }\n            resetSchedulingOpportunities(priority);\n          }\n          // Non-exclusive scheduling opportunity is different: we need reset\n          // it every time to make sure non-labeled resource request will be\n          // most likely allocated on non-labeled nodes first.\n          resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n\n          // Done\n          return assignment;\n        } else {\n          // Do not assign out of order w.r.t priorities\n          return SKIP_ASSIGNMENT;\n        }\n      }\n    }\n\n    return SKIP_ASSIGNMENT;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
            "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
            "oldMethodName": "assignContainer",
            "newMethodName": "assignContainers"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,159 +1,161 @@\n-  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      FiCaSchedulerApp application, Priority priority, \n-      ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode,\n-      ResourceLimits currentResoureLimits) {\n+  public CSAssignment assignContainers(Resource clusterResource,\n+      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n+      SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId()\n-        + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type);\n+      LOG.debug(\"pre-assignContainers for application \"\n+          + getApplicationId());\n+      showRequests();\n     }\n-    \n-    // check if the resource request can access the label\n-    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n-        node.getPartition(), schedulingMode)) {\n-      // this is a reserved container, but we cannot allocate it now according\n-      // to label not match. This can be caused by node label changed\n-      // We should un-reserve this container.\n-      if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n+\n+    // Check if application needs more resource, skip if it doesn\u0027t need more.\n+    if (!hasPendingResourceRequest(rc,\n+        node.getPartition(), clusterResource, schedulingMode)) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n+            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n       }\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-    \n-    Resource capability \u003d request.getCapability();\n-    Resource available \u003d node.getAvailableResource();\n-    Resource totalResource \u003d node.getTotalResource();\n-\n-    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n-        capability, totalResource)) {\n-      LOG.warn(\"Node : \" + node.getNodeID()\n-          + \" does not have sufficient resource for request : \" + request\n-          + \" node total capability : \" + node.getTotalResource());\n-      return new CSAssignment(Resources.none(), type);\n+      return SKIP_ASSIGNMENT;\n     }\n \n-    assert Resources.greaterThan(\n-        resourceCalculator, clusterResource, available, Resources.none());\n+    synchronized (this) {\n+      // Check if this resource is on the blacklist\n+      if (SchedulerAppUtils.isBlacklisted(this, node, LOG)) {\n+        return SKIP_ASSIGNMENT;\n+      }\n \n-    // Create the container if necessary\n-    Container container \u003d \n-        getContainer(rmContainer, application, node, capability, priority);\n-  \n-    // something went wrong getting/creating the container \n-    if (container \u003d\u003d null) {\n-      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-\n-    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n-        application, priority, capability);\n-\n-    // Can we allocate a container on this node?\n-    int availableContainers \u003d \n-        resourceCalculator.computeAvailableContainers(available, capability);\n-\n-    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n-        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n-\n-    if (availableContainers \u003e 0) {\n-      // Allocate...\n-\n-      // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n-        // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n-        // TODO, need change here when we want to support continuous reservation\n-        // looking for labeled partitions.\n-        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n-          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n-          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n-          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n-          // under the limit.\n-          Resource amountToUnreserve \u003d capability;\n-          if (needToUnreserve) {\n-            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n-          }\n-          boolean containerUnreserved \u003d\n-              findNodeToUnreserve(clusterResource, node, application, priority,\n-                  amountToUnreserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n-          // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n-          if (!containerUnreserved) {\n-            return new CSAssignment(Resources.none(), type);\n-          }\n+      // Schedule in priority order\n+      for (Priority priority : getPriorities()) {\n+        ResourceRequest anyRequest \u003d\n+            getResourceRequest(priority, ResourceRequest.ANY);\n+        if (null \u003d\u003d anyRequest) {\n+          continue;\n         }\n-      }\n \n-      // Inform the application\n-      RMContainer allocatedContainer \u003d \n-          application.allocate(type, node, priority, request, container);\n+        // Required resource\n+        Resource required \u003d anyRequest.getCapability();\n \n-      // Does the application need this resource?\n-      if (allocatedContainer \u003d\u003d null) {\n-        return new CSAssignment(Resources.none(), type);\n-      }\n+        // Do we need containers at this \u0027priority\u0027?\n+        if (getTotalRequiredResources(priority) \u003c\u003d 0) {\n+          continue;\n+        }\n \n-      // Inform the node\n-      node.allocateContainer(allocatedContainer);\n-            \n-      // Inform the ordering policy\n-      orderingPolicy.containerAllocated(application, allocatedContainer);\n+        // AM container allocation doesn\u0027t support non-exclusive allocation to\n+        // avoid painful of preempt an AM container\n+        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n \n-      LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" container\u003d\" + container + \n-          \" queue\u003d\" + this + \n-          \" clusterResource\u003d\" + clusterResource);\n-      createdContainer.setValue(allocatedContainer);\n-      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n-      assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getQueuePath());\n-      assignment.getAssignmentInformation().incrAllocations();\n-      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-        container.getResource());\n-      return assignment;\n-    } else {\n-      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n-      // if this was an already a reserved container, reserve it again\n-      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n-\n-        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n-          // we could possibly ignoring queue capacity or user limits when\n-          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n-          // one.\n-          if (needToUnreserve) {\n+          RMAppAttempt rmAppAttempt \u003d\n+              rmContext.getRMApps()\n+                  .get(getApplicationId()).getCurrentAppAttempt();\n+          if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n+              \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n             if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"we needed to unreserve to be able to allocate\");\n+              LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n+                  + getApplicationAttemptId()\n+                  + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n             }\n-            return new CSAssignment(Resources.none(), type);\n+            break;\n           }\n         }\n \n-        // Reserve by \u0027charging\u0027 in advance...\n-        reserve(application, priority, node, rmContainer, container);\n+        // Is the node-label-expression of this offswitch resource request\n+        // matches the node\u0027s label?\n+        // If not match, jump to next priority.\n+        if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n+            anyRequest, node.getPartition(), schedulingMode)) {\n+          continue;\n+        }\n \n-        LOG.info(\"Reserved container \" + \n-            \" application\u003d\" + application.getApplicationId() + \n-            \" resource\u003d\" + request.getCapability() + \n-            \" queue\u003d\" + this.toString() + \n-            \" usedCapacity\u003d\" + getUsedCapacity() + \n-            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n-            \" used\u003d\" + queueUsage.getUsed() +\n-            \" cluster\u003d\" + clusterResource);\n+        if (!getCSLeafQueue().getReservationContinueLooking()) {\n+          if (!shouldAllocOrReserveNewContainer(priority, required)) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n+            }\n+            continue;\n+          }\n+        }\n+\n+        if (!checkHeadroom(clusterResource, currentResourceLimits, required,\n+            node)) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"cannot allocate required resource\u003d\" + required\n+                + \" because of headroom\");\n+          }\n+          return NULL_ASSIGNMENT;\n+        }\n+\n+        // Inform the application it is about to get a scheduling opportunity\n+        addSchedulingOpportunity(priority);\n+\n+        // Increase missed-non-partitioned-resource-request-opportunity.\n+        // This is to make sure non-partitioned-resource-request will prefer\n+        // to be allocated to non-partitioned nodes\n+        int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n+        if (anyRequest.getNodeLabelExpression().equals(\n+            RMNodeLabelsManager.NO_LABEL)) {\n+          missedNonPartitionedRequestSchedulingOpportunity \u003d\n+              addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+        }\n+\n+        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n+          // Before doing allocation, we need to check scheduling opportunity to\n+          // make sure : non-partitioned resource request should be scheduled to\n+          // non-partitioned partition first.\n+          if (missedNonPartitionedRequestSchedulingOpportunity \u003c rmContext\n+              .getScheduler().getNumClusterNodes()) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Skip app_attempt\u003d\"\n+                  + getApplicationAttemptId() + \" priority\u003d\"\n+                  + priority\n+                  + \" because missed-non-partitioned-resource-request\"\n+                  + \" opportunity under requred:\" + \" Now\u003d\"\n+                  + missedNonPartitionedRequestSchedulingOpportunity\n+                  + \" required\u003d\"\n+                  + rmContext.getScheduler().getNumClusterNodes());\n+            }\n+\n+            return SKIP_ASSIGNMENT;\n+          }\n+        }\n+\n+        // Try to schedule\n         CSAssignment assignment \u003d\n-            new CSAssignment(request.getCapability(), type);\n-        assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getQueuePath());\n-        assignment.getAssignmentInformation().incrReservations();\n-        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n-          request.getCapability());\n-        return assignment;\n+            assignContainersOnNode(clusterResource, node,\n+                priority, null, schedulingMode, currentResourceLimits);\n+\n+        // Did the application skip this node?\n+        if (assignment.getSkipped()) {\n+          // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n+          subtractSchedulingOpportunity(priority);\n+          continue;\n+        }\n+\n+        // Did we schedule or reserve a container?\n+        Resource assigned \u003d assignment.getResource();\n+        if (Resources.greaterThan(rc, clusterResource,\n+            assigned, Resources.none())) {\n+          // Don\u0027t reset scheduling opportunities for offswitch assignments\n+          // otherwise the app will be delayed for each non-local assignment.\n+          // This helps apps with many off-cluster requests schedule faster.\n+          if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Resetting scheduling opportunities\");\n+            }\n+            resetSchedulingOpportunities(priority);\n+          }\n+          // Non-exclusive scheduling opportunity is different: we need reset\n+          // it every time to make sure non-labeled resource request will be\n+          // most likely allocated on non-labeled nodes first.\n+          resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+\n+          // Done\n+          return assignment;\n+        } else {\n+          // Do not assign out of order w.r.t priorities\n+          return SKIP_ASSIGNMENT;\n+        }\n       }\n-      return new CSAssignment(Resources.none(), type);\n     }\n+\n+    return SKIP_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    // Check if application needs more resource, skip if it doesn\u0027t need more.\n    if (!hasPendingResourceRequest(rc,\n        node.getPartition(), clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n      }\n      return SKIP_ASSIGNMENT;\n    }\n\n    synchronized (this) {\n      // Check if this resource is on the blacklist\n      if (SchedulerAppUtils.isBlacklisted(this, node, LOG)) {\n        return SKIP_ASSIGNMENT;\n      }\n\n      // Schedule in priority order\n      for (Priority priority : getPriorities()) {\n        ResourceRequest anyRequest \u003d\n            getResourceRequest(priority, ResourceRequest.ANY);\n        if (null \u003d\u003d anyRequest) {\n          continue;\n        }\n\n        // Required resource\n        Resource required \u003d anyRequest.getCapability();\n\n        // Do we need containers at this \u0027priority\u0027?\n        if (getTotalRequiredResources(priority) \u003c\u003d 0) {\n          continue;\n        }\n\n        // AM container allocation doesn\u0027t support non-exclusive allocation to\n        // avoid painful of preempt an AM container\n        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n\n          RMAppAttempt rmAppAttempt \u003d\n              rmContext.getRMApps()\n                  .get(getApplicationId()).getCurrentAppAttempt();\n          if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n              \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                  + getApplicationAttemptId()\n                  + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n            }\n            break;\n          }\n        }\n\n        // Is the node-label-expression of this offswitch resource request\n        // matches the node\u0027s label?\n        // If not match, jump to next priority.\n        if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n            anyRequest, node.getPartition(), schedulingMode)) {\n          continue;\n        }\n\n        if (!getCSLeafQueue().getReservationContinueLooking()) {\n          if (!shouldAllocOrReserveNewContainer(priority, required)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n            }\n            continue;\n          }\n        }\n\n        if (!checkHeadroom(clusterResource, currentResourceLimits, required,\n            node)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"cannot allocate required resource\u003d\" + required\n                + \" because of headroom\");\n          }\n          return NULL_ASSIGNMENT;\n        }\n\n        // Inform the application it is about to get a scheduling opportunity\n        addSchedulingOpportunity(priority);\n\n        // Increase missed-non-partitioned-resource-request-opportunity.\n        // This is to make sure non-partitioned-resource-request will prefer\n        // to be allocated to non-partitioned nodes\n        int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n        if (anyRequest.getNodeLabelExpression().equals(\n            RMNodeLabelsManager.NO_LABEL)) {\n          missedNonPartitionedRequestSchedulingOpportunity \u003d\n              addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n        }\n\n        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n          // Before doing allocation, we need to check scheduling opportunity to\n          // make sure : non-partitioned resource request should be scheduled to\n          // non-partitioned partition first.\n          if (missedNonPartitionedRequestSchedulingOpportunity \u003c rmContext\n              .getScheduler().getNumClusterNodes()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skip app_attempt\u003d\"\n                  + getApplicationAttemptId() + \" priority\u003d\"\n                  + priority\n                  + \" because missed-non-partitioned-resource-request\"\n                  + \" opportunity under requred:\" + \" Now\u003d\"\n                  + missedNonPartitionedRequestSchedulingOpportunity\n                  + \" required\u003d\"\n                  + rmContext.getScheduler().getNumClusterNodes());\n            }\n\n            return SKIP_ASSIGNMENT;\n          }\n        }\n\n        // Try to schedule\n        CSAssignment assignment \u003d\n            assignContainersOnNode(clusterResource, node,\n                priority, null, schedulingMode, currentResourceLimits);\n\n        // Did the application skip this node?\n        if (assignment.getSkipped()) {\n          // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n          subtractSchedulingOpportunity(priority);\n          continue;\n        }\n\n        // Did we schedule or reserve a container?\n        Resource assigned \u003d assignment.getResource();\n        if (Resources.greaterThan(rc, clusterResource,\n            assigned, Resources.none())) {\n          // Don\u0027t reset scheduling opportunities for offswitch assignments\n          // otherwise the app will be delayed for each non-local assignment.\n          // This helps apps with many off-cluster requests schedule faster.\n          if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Resetting scheduling opportunities\");\n            }\n            resetSchedulingOpportunities(priority);\n          }\n          // Non-exclusive scheduling opportunity is different: we need reset\n          // it every time to make sure non-labeled resource request will be\n          // most likely allocated on non-labeled nodes first.\n          resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n\n          // Done\n          return assignment;\n        } else {\n          // Do not assign out of order w.r.t priorities\n          return SKIP_ASSIGNMENT;\n        }\n      }\n    }\n\n    return SKIP_ASSIGNMENT;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,159 +1,161 @@\n-  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      FiCaSchedulerApp application, Priority priority, \n-      ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode,\n-      ResourceLimits currentResoureLimits) {\n+  public CSAssignment assignContainers(Resource clusterResource,\n+      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n+      SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId()\n-        + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type);\n+      LOG.debug(\"pre-assignContainers for application \"\n+          + getApplicationId());\n+      showRequests();\n     }\n-    \n-    // check if the resource request can access the label\n-    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n-        node.getPartition(), schedulingMode)) {\n-      // this is a reserved container, but we cannot allocate it now according\n-      // to label not match. This can be caused by node label changed\n-      // We should un-reserve this container.\n-      if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n+\n+    // Check if application needs more resource, skip if it doesn\u0027t need more.\n+    if (!hasPendingResourceRequest(rc,\n+        node.getPartition(), clusterResource, schedulingMode)) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n+            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n       }\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-    \n-    Resource capability \u003d request.getCapability();\n-    Resource available \u003d node.getAvailableResource();\n-    Resource totalResource \u003d node.getTotalResource();\n-\n-    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n-        capability, totalResource)) {\n-      LOG.warn(\"Node : \" + node.getNodeID()\n-          + \" does not have sufficient resource for request : \" + request\n-          + \" node total capability : \" + node.getTotalResource());\n-      return new CSAssignment(Resources.none(), type);\n+      return SKIP_ASSIGNMENT;\n     }\n \n-    assert Resources.greaterThan(\n-        resourceCalculator, clusterResource, available, Resources.none());\n+    synchronized (this) {\n+      // Check if this resource is on the blacklist\n+      if (SchedulerAppUtils.isBlacklisted(this, node, LOG)) {\n+        return SKIP_ASSIGNMENT;\n+      }\n \n-    // Create the container if necessary\n-    Container container \u003d \n-        getContainer(rmContainer, application, node, capability, priority);\n-  \n-    // something went wrong getting/creating the container \n-    if (container \u003d\u003d null) {\n-      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-\n-    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n-        application, priority, capability);\n-\n-    // Can we allocate a container on this node?\n-    int availableContainers \u003d \n-        resourceCalculator.computeAvailableContainers(available, capability);\n-\n-    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n-        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n-\n-    if (availableContainers \u003e 0) {\n-      // Allocate...\n-\n-      // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n-        // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n-        // TODO, need change here when we want to support continuous reservation\n-        // looking for labeled partitions.\n-        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n-          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n-          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n-          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n-          // under the limit.\n-          Resource amountToUnreserve \u003d capability;\n-          if (needToUnreserve) {\n-            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n-          }\n-          boolean containerUnreserved \u003d\n-              findNodeToUnreserve(clusterResource, node, application, priority,\n-                  amountToUnreserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n-          // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n-          if (!containerUnreserved) {\n-            return new CSAssignment(Resources.none(), type);\n-          }\n+      // Schedule in priority order\n+      for (Priority priority : getPriorities()) {\n+        ResourceRequest anyRequest \u003d\n+            getResourceRequest(priority, ResourceRequest.ANY);\n+        if (null \u003d\u003d anyRequest) {\n+          continue;\n         }\n-      }\n \n-      // Inform the application\n-      RMContainer allocatedContainer \u003d \n-          application.allocate(type, node, priority, request, container);\n+        // Required resource\n+        Resource required \u003d anyRequest.getCapability();\n \n-      // Does the application need this resource?\n-      if (allocatedContainer \u003d\u003d null) {\n-        return new CSAssignment(Resources.none(), type);\n-      }\n+        // Do we need containers at this \u0027priority\u0027?\n+        if (getTotalRequiredResources(priority) \u003c\u003d 0) {\n+          continue;\n+        }\n \n-      // Inform the node\n-      node.allocateContainer(allocatedContainer);\n-            \n-      // Inform the ordering policy\n-      orderingPolicy.containerAllocated(application, allocatedContainer);\n+        // AM container allocation doesn\u0027t support non-exclusive allocation to\n+        // avoid painful of preempt an AM container\n+        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n \n-      LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" container\u003d\" + container + \n-          \" queue\u003d\" + this + \n-          \" clusterResource\u003d\" + clusterResource);\n-      createdContainer.setValue(allocatedContainer);\n-      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n-      assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getQueuePath());\n-      assignment.getAssignmentInformation().incrAllocations();\n-      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-        container.getResource());\n-      return assignment;\n-    } else {\n-      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n-      // if this was an already a reserved container, reserve it again\n-      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n-\n-        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n-          // we could possibly ignoring queue capacity or user limits when\n-          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n-          // one.\n-          if (needToUnreserve) {\n+          RMAppAttempt rmAppAttempt \u003d\n+              rmContext.getRMApps()\n+                  .get(getApplicationId()).getCurrentAppAttempt();\n+          if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n+              \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n             if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"we needed to unreserve to be able to allocate\");\n+              LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n+                  + getApplicationAttemptId()\n+                  + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n             }\n-            return new CSAssignment(Resources.none(), type);\n+            break;\n           }\n         }\n \n-        // Reserve by \u0027charging\u0027 in advance...\n-        reserve(application, priority, node, rmContainer, container);\n+        // Is the node-label-expression of this offswitch resource request\n+        // matches the node\u0027s label?\n+        // If not match, jump to next priority.\n+        if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n+            anyRequest, node.getPartition(), schedulingMode)) {\n+          continue;\n+        }\n \n-        LOG.info(\"Reserved container \" + \n-            \" application\u003d\" + application.getApplicationId() + \n-            \" resource\u003d\" + request.getCapability() + \n-            \" queue\u003d\" + this.toString() + \n-            \" usedCapacity\u003d\" + getUsedCapacity() + \n-            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n-            \" used\u003d\" + queueUsage.getUsed() +\n-            \" cluster\u003d\" + clusterResource);\n+        if (!getCSLeafQueue().getReservationContinueLooking()) {\n+          if (!shouldAllocOrReserveNewContainer(priority, required)) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n+            }\n+            continue;\n+          }\n+        }\n+\n+        if (!checkHeadroom(clusterResource, currentResourceLimits, required,\n+            node)) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"cannot allocate required resource\u003d\" + required\n+                + \" because of headroom\");\n+          }\n+          return NULL_ASSIGNMENT;\n+        }\n+\n+        // Inform the application it is about to get a scheduling opportunity\n+        addSchedulingOpportunity(priority);\n+\n+        // Increase missed-non-partitioned-resource-request-opportunity.\n+        // This is to make sure non-partitioned-resource-request will prefer\n+        // to be allocated to non-partitioned nodes\n+        int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n+        if (anyRequest.getNodeLabelExpression().equals(\n+            RMNodeLabelsManager.NO_LABEL)) {\n+          missedNonPartitionedRequestSchedulingOpportunity \u003d\n+              addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+        }\n+\n+        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n+          // Before doing allocation, we need to check scheduling opportunity to\n+          // make sure : non-partitioned resource request should be scheduled to\n+          // non-partitioned partition first.\n+          if (missedNonPartitionedRequestSchedulingOpportunity \u003c rmContext\n+              .getScheduler().getNumClusterNodes()) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Skip app_attempt\u003d\"\n+                  + getApplicationAttemptId() + \" priority\u003d\"\n+                  + priority\n+                  + \" because missed-non-partitioned-resource-request\"\n+                  + \" opportunity under requred:\" + \" Now\u003d\"\n+                  + missedNonPartitionedRequestSchedulingOpportunity\n+                  + \" required\u003d\"\n+                  + rmContext.getScheduler().getNumClusterNodes());\n+            }\n+\n+            return SKIP_ASSIGNMENT;\n+          }\n+        }\n+\n+        // Try to schedule\n         CSAssignment assignment \u003d\n-            new CSAssignment(request.getCapability(), type);\n-        assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getQueuePath());\n-        assignment.getAssignmentInformation().incrReservations();\n-        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n-          request.getCapability());\n-        return assignment;\n+            assignContainersOnNode(clusterResource, node,\n+                priority, null, schedulingMode, currentResourceLimits);\n+\n+        // Did the application skip this node?\n+        if (assignment.getSkipped()) {\n+          // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n+          subtractSchedulingOpportunity(priority);\n+          continue;\n+        }\n+\n+        // Did we schedule or reserve a container?\n+        Resource assigned \u003d assignment.getResource();\n+        if (Resources.greaterThan(rc, clusterResource,\n+            assigned, Resources.none())) {\n+          // Don\u0027t reset scheduling opportunities for offswitch assignments\n+          // otherwise the app will be delayed for each non-local assignment.\n+          // This helps apps with many off-cluster requests schedule faster.\n+          if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Resetting scheduling opportunities\");\n+            }\n+            resetSchedulingOpportunities(priority);\n+          }\n+          // Non-exclusive scheduling opportunity is different: we need reset\n+          // it every time to make sure non-labeled resource request will be\n+          // most likely allocated on non-labeled nodes first.\n+          resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+\n+          // Done\n+          return assignment;\n+        } else {\n+          // Do not assign out of order w.r.t priorities\n+          return SKIP_ASSIGNMENT;\n+        }\n       }\n-      return new CSAssignment(Resources.none(), type);\n     }\n+\n+    return SKIP_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    // Check if application needs more resource, skip if it doesn\u0027t need more.\n    if (!hasPendingResourceRequest(rc,\n        node.getPartition(), clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n      }\n      return SKIP_ASSIGNMENT;\n    }\n\n    synchronized (this) {\n      // Check if this resource is on the blacklist\n      if (SchedulerAppUtils.isBlacklisted(this, node, LOG)) {\n        return SKIP_ASSIGNMENT;\n      }\n\n      // Schedule in priority order\n      for (Priority priority : getPriorities()) {\n        ResourceRequest anyRequest \u003d\n            getResourceRequest(priority, ResourceRequest.ANY);\n        if (null \u003d\u003d anyRequest) {\n          continue;\n        }\n\n        // Required resource\n        Resource required \u003d anyRequest.getCapability();\n\n        // Do we need containers at this \u0027priority\u0027?\n        if (getTotalRequiredResources(priority) \u003c\u003d 0) {\n          continue;\n        }\n\n        // AM container allocation doesn\u0027t support non-exclusive allocation to\n        // avoid painful of preempt an AM container\n        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n\n          RMAppAttempt rmAppAttempt \u003d\n              rmContext.getRMApps()\n                  .get(getApplicationId()).getCurrentAppAttempt();\n          if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n              \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                  + getApplicationAttemptId()\n                  + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n            }\n            break;\n          }\n        }\n\n        // Is the node-label-expression of this offswitch resource request\n        // matches the node\u0027s label?\n        // If not match, jump to next priority.\n        if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n            anyRequest, node.getPartition(), schedulingMode)) {\n          continue;\n        }\n\n        if (!getCSLeafQueue().getReservationContinueLooking()) {\n          if (!shouldAllocOrReserveNewContainer(priority, required)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n            }\n            continue;\n          }\n        }\n\n        if (!checkHeadroom(clusterResource, currentResourceLimits, required,\n            node)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"cannot allocate required resource\u003d\" + required\n                + \" because of headroom\");\n          }\n          return NULL_ASSIGNMENT;\n        }\n\n        // Inform the application it is about to get a scheduling opportunity\n        addSchedulingOpportunity(priority);\n\n        // Increase missed-non-partitioned-resource-request-opportunity.\n        // This is to make sure non-partitioned-resource-request will prefer\n        // to be allocated to non-partitioned nodes\n        int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n        if (anyRequest.getNodeLabelExpression().equals(\n            RMNodeLabelsManager.NO_LABEL)) {\n          missedNonPartitionedRequestSchedulingOpportunity \u003d\n              addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n        }\n\n        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n          // Before doing allocation, we need to check scheduling opportunity to\n          // make sure : non-partitioned resource request should be scheduled to\n          // non-partitioned partition first.\n          if (missedNonPartitionedRequestSchedulingOpportunity \u003c rmContext\n              .getScheduler().getNumClusterNodes()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skip app_attempt\u003d\"\n                  + getApplicationAttemptId() + \" priority\u003d\"\n                  + priority\n                  + \" because missed-non-partitioned-resource-request\"\n                  + \" opportunity under requred:\" + \" Now\u003d\"\n                  + missedNonPartitionedRequestSchedulingOpportunity\n                  + \" required\u003d\"\n                  + rmContext.getScheduler().getNumClusterNodes());\n            }\n\n            return SKIP_ASSIGNMENT;\n          }\n        }\n\n        // Try to schedule\n        CSAssignment assignment \u003d\n            assignContainersOnNode(clusterResource, node,\n                priority, null, schedulingMode, currentResourceLimits);\n\n        // Did the application skip this node?\n        if (assignment.getSkipped()) {\n          // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n          subtractSchedulingOpportunity(priority);\n          continue;\n        }\n\n        // Did we schedule or reserve a container?\n        Resource assigned \u003d assignment.getResource();\n        if (Resources.greaterThan(rc, clusterResource,\n            assigned, Resources.none())) {\n          // Don\u0027t reset scheduling opportunities for offswitch assignments\n          // otherwise the app will be delayed for each non-local assignment.\n          // This helps apps with many off-cluster requests schedule faster.\n          if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Resetting scheduling opportunities\");\n            }\n            resetSchedulingOpportunities(priority);\n          }\n          // Non-exclusive scheduling opportunity is different: we need reset\n          // it every time to make sure non-labeled resource request will be\n          // most likely allocated on non-labeled nodes first.\n          resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n\n          // Done\n          return assignment;\n        } else {\n          // Do not assign out of order w.r.t priorities\n          return SKIP_ASSIGNMENT;\n        }\n      }\n    }\n\n    return SKIP_ASSIGNMENT;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,159 +1,161 @@\n-  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      FiCaSchedulerApp application, Priority priority, \n-      ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode,\n-      ResourceLimits currentResoureLimits) {\n+  public CSAssignment assignContainers(Resource clusterResource,\n+      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n+      SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId()\n-        + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type);\n+      LOG.debug(\"pre-assignContainers for application \"\n+          + getApplicationId());\n+      showRequests();\n     }\n-    \n-    // check if the resource request can access the label\n-    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n-        node.getPartition(), schedulingMode)) {\n-      // this is a reserved container, but we cannot allocate it now according\n-      // to label not match. This can be caused by node label changed\n-      // We should un-reserve this container.\n-      if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n+\n+    // Check if application needs more resource, skip if it doesn\u0027t need more.\n+    if (!hasPendingResourceRequest(rc,\n+        node.getPartition(), clusterResource, schedulingMode)) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n+            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n       }\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-    \n-    Resource capability \u003d request.getCapability();\n-    Resource available \u003d node.getAvailableResource();\n-    Resource totalResource \u003d node.getTotalResource();\n-\n-    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n-        capability, totalResource)) {\n-      LOG.warn(\"Node : \" + node.getNodeID()\n-          + \" does not have sufficient resource for request : \" + request\n-          + \" node total capability : \" + node.getTotalResource());\n-      return new CSAssignment(Resources.none(), type);\n+      return SKIP_ASSIGNMENT;\n     }\n \n-    assert Resources.greaterThan(\n-        resourceCalculator, clusterResource, available, Resources.none());\n+    synchronized (this) {\n+      // Check if this resource is on the blacklist\n+      if (SchedulerAppUtils.isBlacklisted(this, node, LOG)) {\n+        return SKIP_ASSIGNMENT;\n+      }\n \n-    // Create the container if necessary\n-    Container container \u003d \n-        getContainer(rmContainer, application, node, capability, priority);\n-  \n-    // something went wrong getting/creating the container \n-    if (container \u003d\u003d null) {\n-      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-\n-    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n-        application, priority, capability);\n-\n-    // Can we allocate a container on this node?\n-    int availableContainers \u003d \n-        resourceCalculator.computeAvailableContainers(available, capability);\n-\n-    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n-        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n-\n-    if (availableContainers \u003e 0) {\n-      // Allocate...\n-\n-      // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n-        // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n-        // TODO, need change here when we want to support continuous reservation\n-        // looking for labeled partitions.\n-        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n-          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n-          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n-          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n-          // under the limit.\n-          Resource amountToUnreserve \u003d capability;\n-          if (needToUnreserve) {\n-            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n-          }\n-          boolean containerUnreserved \u003d\n-              findNodeToUnreserve(clusterResource, node, application, priority,\n-                  amountToUnreserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n-          // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n-          if (!containerUnreserved) {\n-            return new CSAssignment(Resources.none(), type);\n-          }\n+      // Schedule in priority order\n+      for (Priority priority : getPriorities()) {\n+        ResourceRequest anyRequest \u003d\n+            getResourceRequest(priority, ResourceRequest.ANY);\n+        if (null \u003d\u003d anyRequest) {\n+          continue;\n         }\n-      }\n \n-      // Inform the application\n-      RMContainer allocatedContainer \u003d \n-          application.allocate(type, node, priority, request, container);\n+        // Required resource\n+        Resource required \u003d anyRequest.getCapability();\n \n-      // Does the application need this resource?\n-      if (allocatedContainer \u003d\u003d null) {\n-        return new CSAssignment(Resources.none(), type);\n-      }\n+        // Do we need containers at this \u0027priority\u0027?\n+        if (getTotalRequiredResources(priority) \u003c\u003d 0) {\n+          continue;\n+        }\n \n-      // Inform the node\n-      node.allocateContainer(allocatedContainer);\n-            \n-      // Inform the ordering policy\n-      orderingPolicy.containerAllocated(application, allocatedContainer);\n+        // AM container allocation doesn\u0027t support non-exclusive allocation to\n+        // avoid painful of preempt an AM container\n+        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n \n-      LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" container\u003d\" + container + \n-          \" queue\u003d\" + this + \n-          \" clusterResource\u003d\" + clusterResource);\n-      createdContainer.setValue(allocatedContainer);\n-      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n-      assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getQueuePath());\n-      assignment.getAssignmentInformation().incrAllocations();\n-      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-        container.getResource());\n-      return assignment;\n-    } else {\n-      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n-      // if this was an already a reserved container, reserve it again\n-      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n-\n-        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n-          // we could possibly ignoring queue capacity or user limits when\n-          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n-          // one.\n-          if (needToUnreserve) {\n+          RMAppAttempt rmAppAttempt \u003d\n+              rmContext.getRMApps()\n+                  .get(getApplicationId()).getCurrentAppAttempt();\n+          if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n+              \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n             if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"we needed to unreserve to be able to allocate\");\n+              LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n+                  + getApplicationAttemptId()\n+                  + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n             }\n-            return new CSAssignment(Resources.none(), type);\n+            break;\n           }\n         }\n \n-        // Reserve by \u0027charging\u0027 in advance...\n-        reserve(application, priority, node, rmContainer, container);\n+        // Is the node-label-expression of this offswitch resource request\n+        // matches the node\u0027s label?\n+        // If not match, jump to next priority.\n+        if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n+            anyRequest, node.getPartition(), schedulingMode)) {\n+          continue;\n+        }\n \n-        LOG.info(\"Reserved container \" + \n-            \" application\u003d\" + application.getApplicationId() + \n-            \" resource\u003d\" + request.getCapability() + \n-            \" queue\u003d\" + this.toString() + \n-            \" usedCapacity\u003d\" + getUsedCapacity() + \n-            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n-            \" used\u003d\" + queueUsage.getUsed() +\n-            \" cluster\u003d\" + clusterResource);\n+        if (!getCSLeafQueue().getReservationContinueLooking()) {\n+          if (!shouldAllocOrReserveNewContainer(priority, required)) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n+            }\n+            continue;\n+          }\n+        }\n+\n+        if (!checkHeadroom(clusterResource, currentResourceLimits, required,\n+            node)) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"cannot allocate required resource\u003d\" + required\n+                + \" because of headroom\");\n+          }\n+          return NULL_ASSIGNMENT;\n+        }\n+\n+        // Inform the application it is about to get a scheduling opportunity\n+        addSchedulingOpportunity(priority);\n+\n+        // Increase missed-non-partitioned-resource-request-opportunity.\n+        // This is to make sure non-partitioned-resource-request will prefer\n+        // to be allocated to non-partitioned nodes\n+        int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n+        if (anyRequest.getNodeLabelExpression().equals(\n+            RMNodeLabelsManager.NO_LABEL)) {\n+          missedNonPartitionedRequestSchedulingOpportunity \u003d\n+              addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+        }\n+\n+        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n+          // Before doing allocation, we need to check scheduling opportunity to\n+          // make sure : non-partitioned resource request should be scheduled to\n+          // non-partitioned partition first.\n+          if (missedNonPartitionedRequestSchedulingOpportunity \u003c rmContext\n+              .getScheduler().getNumClusterNodes()) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Skip app_attempt\u003d\"\n+                  + getApplicationAttemptId() + \" priority\u003d\"\n+                  + priority\n+                  + \" because missed-non-partitioned-resource-request\"\n+                  + \" opportunity under requred:\" + \" Now\u003d\"\n+                  + missedNonPartitionedRequestSchedulingOpportunity\n+                  + \" required\u003d\"\n+                  + rmContext.getScheduler().getNumClusterNodes());\n+            }\n+\n+            return SKIP_ASSIGNMENT;\n+          }\n+        }\n+\n+        // Try to schedule\n         CSAssignment assignment \u003d\n-            new CSAssignment(request.getCapability(), type);\n-        assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getQueuePath());\n-        assignment.getAssignmentInformation().incrReservations();\n-        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n-          request.getCapability());\n-        return assignment;\n+            assignContainersOnNode(clusterResource, node,\n+                priority, null, schedulingMode, currentResourceLimits);\n+\n+        // Did the application skip this node?\n+        if (assignment.getSkipped()) {\n+          // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n+          subtractSchedulingOpportunity(priority);\n+          continue;\n+        }\n+\n+        // Did we schedule or reserve a container?\n+        Resource assigned \u003d assignment.getResource();\n+        if (Resources.greaterThan(rc, clusterResource,\n+            assigned, Resources.none())) {\n+          // Don\u0027t reset scheduling opportunities for offswitch assignments\n+          // otherwise the app will be delayed for each non-local assignment.\n+          // This helps apps with many off-cluster requests schedule faster.\n+          if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Resetting scheduling opportunities\");\n+            }\n+            resetSchedulingOpportunities(priority);\n+          }\n+          // Non-exclusive scheduling opportunity is different: we need reset\n+          // it every time to make sure non-labeled resource request will be\n+          // most likely allocated on non-labeled nodes first.\n+          resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+\n+          // Done\n+          return assignment;\n+        } else {\n+          // Do not assign out of order w.r.t priorities\n+          return SKIP_ASSIGNMENT;\n+        }\n       }\n-      return new CSAssignment(Resources.none(), type);\n     }\n+\n+    return SKIP_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    // Check if application needs more resource, skip if it doesn\u0027t need more.\n    if (!hasPendingResourceRequest(rc,\n        node.getPartition(), clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n      }\n      return SKIP_ASSIGNMENT;\n    }\n\n    synchronized (this) {\n      // Check if this resource is on the blacklist\n      if (SchedulerAppUtils.isBlacklisted(this, node, LOG)) {\n        return SKIP_ASSIGNMENT;\n      }\n\n      // Schedule in priority order\n      for (Priority priority : getPriorities()) {\n        ResourceRequest anyRequest \u003d\n            getResourceRequest(priority, ResourceRequest.ANY);\n        if (null \u003d\u003d anyRequest) {\n          continue;\n        }\n\n        // Required resource\n        Resource required \u003d anyRequest.getCapability();\n\n        // Do we need containers at this \u0027priority\u0027?\n        if (getTotalRequiredResources(priority) \u003c\u003d 0) {\n          continue;\n        }\n\n        // AM container allocation doesn\u0027t support non-exclusive allocation to\n        // avoid painful of preempt an AM container\n        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n\n          RMAppAttempt rmAppAttempt \u003d\n              rmContext.getRMApps()\n                  .get(getApplicationId()).getCurrentAppAttempt();\n          if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n              \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                  + getApplicationAttemptId()\n                  + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n            }\n            break;\n          }\n        }\n\n        // Is the node-label-expression of this offswitch resource request\n        // matches the node\u0027s label?\n        // If not match, jump to next priority.\n        if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n            anyRequest, node.getPartition(), schedulingMode)) {\n          continue;\n        }\n\n        if (!getCSLeafQueue().getReservationContinueLooking()) {\n          if (!shouldAllocOrReserveNewContainer(priority, required)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n            }\n            continue;\n          }\n        }\n\n        if (!checkHeadroom(clusterResource, currentResourceLimits, required,\n            node)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"cannot allocate required resource\u003d\" + required\n                + \" because of headroom\");\n          }\n          return NULL_ASSIGNMENT;\n        }\n\n        // Inform the application it is about to get a scheduling opportunity\n        addSchedulingOpportunity(priority);\n\n        // Increase missed-non-partitioned-resource-request-opportunity.\n        // This is to make sure non-partitioned-resource-request will prefer\n        // to be allocated to non-partitioned nodes\n        int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n        if (anyRequest.getNodeLabelExpression().equals(\n            RMNodeLabelsManager.NO_LABEL)) {\n          missedNonPartitionedRequestSchedulingOpportunity \u003d\n              addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n        }\n\n        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n          // Before doing allocation, we need to check scheduling opportunity to\n          // make sure : non-partitioned resource request should be scheduled to\n          // non-partitioned partition first.\n          if (missedNonPartitionedRequestSchedulingOpportunity \u003c rmContext\n              .getScheduler().getNumClusterNodes()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skip app_attempt\u003d\"\n                  + getApplicationAttemptId() + \" priority\u003d\"\n                  + priority\n                  + \" because missed-non-partitioned-resource-request\"\n                  + \" opportunity under requred:\" + \" Now\u003d\"\n                  + missedNonPartitionedRequestSchedulingOpportunity\n                  + \" required\u003d\"\n                  + rmContext.getScheduler().getNumClusterNodes());\n            }\n\n            return SKIP_ASSIGNMENT;\n          }\n        }\n\n        // Try to schedule\n        CSAssignment assignment \u003d\n            assignContainersOnNode(clusterResource, node,\n                priority, null, schedulingMode, currentResourceLimits);\n\n        // Did the application skip this node?\n        if (assignment.getSkipped()) {\n          // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n          subtractSchedulingOpportunity(priority);\n          continue;\n        }\n\n        // Did we schedule or reserve a container?\n        Resource assigned \u003d assignment.getResource();\n        if (Resources.greaterThan(rc, clusterResource,\n            assigned, Resources.none())) {\n          // Don\u0027t reset scheduling opportunities for offswitch assignments\n          // otherwise the app will be delayed for each non-local assignment.\n          // This helps apps with many off-cluster requests schedule faster.\n          if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Resetting scheduling opportunities\");\n            }\n            resetSchedulingOpportunities(priority);\n          }\n          // Non-exclusive scheduling opportunity is different: we need reset\n          // it every time to make sure non-labeled resource request will be\n          // most likely allocated on non-labeled nodes first.\n          resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n\n          // Done\n          return assignment;\n        } else {\n          // Do not assign out of order w.r.t priorities\n          return SKIP_ASSIGNMENT;\n        }\n      }\n    }\n\n    return SKIP_ASSIGNMENT;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "assignContainer",
            "newValue": "assignContainers"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,159 +1,161 @@\n-  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      FiCaSchedulerApp application, Priority priority, \n-      ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode,\n-      ResourceLimits currentResoureLimits) {\n+  public CSAssignment assignContainers(Resource clusterResource,\n+      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n+      SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId()\n-        + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type);\n+      LOG.debug(\"pre-assignContainers for application \"\n+          + getApplicationId());\n+      showRequests();\n     }\n-    \n-    // check if the resource request can access the label\n-    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n-        node.getPartition(), schedulingMode)) {\n-      // this is a reserved container, but we cannot allocate it now according\n-      // to label not match. This can be caused by node label changed\n-      // We should un-reserve this container.\n-      if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n+\n+    // Check if application needs more resource, skip if it doesn\u0027t need more.\n+    if (!hasPendingResourceRequest(rc,\n+        node.getPartition(), clusterResource, schedulingMode)) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n+            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n+            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n       }\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-    \n-    Resource capability \u003d request.getCapability();\n-    Resource available \u003d node.getAvailableResource();\n-    Resource totalResource \u003d node.getTotalResource();\n-\n-    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n-        capability, totalResource)) {\n-      LOG.warn(\"Node : \" + node.getNodeID()\n-          + \" does not have sufficient resource for request : \" + request\n-          + \" node total capability : \" + node.getTotalResource());\n-      return new CSAssignment(Resources.none(), type);\n+      return SKIP_ASSIGNMENT;\n     }\n \n-    assert Resources.greaterThan(\n-        resourceCalculator, clusterResource, available, Resources.none());\n+    synchronized (this) {\n+      // Check if this resource is on the blacklist\n+      if (SchedulerAppUtils.isBlacklisted(this, node, LOG)) {\n+        return SKIP_ASSIGNMENT;\n+      }\n \n-    // Create the container if necessary\n-    Container container \u003d \n-        getContainer(rmContainer, application, node, capability, priority);\n-  \n-    // something went wrong getting/creating the container \n-    if (container \u003d\u003d null) {\n-      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-\n-    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n-        application, priority, capability);\n-\n-    // Can we allocate a container on this node?\n-    int availableContainers \u003d \n-        resourceCalculator.computeAvailableContainers(available, capability);\n-\n-    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n-        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n-\n-    if (availableContainers \u003e 0) {\n-      // Allocate...\n-\n-      // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n-        // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n-        // TODO, need change here when we want to support continuous reservation\n-        // looking for labeled partitions.\n-        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n-          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n-          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n-          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n-          // under the limit.\n-          Resource amountToUnreserve \u003d capability;\n-          if (needToUnreserve) {\n-            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n-          }\n-          boolean containerUnreserved \u003d\n-              findNodeToUnreserve(clusterResource, node, application, priority,\n-                  amountToUnreserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n-          // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n-          if (!containerUnreserved) {\n-            return new CSAssignment(Resources.none(), type);\n-          }\n+      // Schedule in priority order\n+      for (Priority priority : getPriorities()) {\n+        ResourceRequest anyRequest \u003d\n+            getResourceRequest(priority, ResourceRequest.ANY);\n+        if (null \u003d\u003d anyRequest) {\n+          continue;\n         }\n-      }\n \n-      // Inform the application\n-      RMContainer allocatedContainer \u003d \n-          application.allocate(type, node, priority, request, container);\n+        // Required resource\n+        Resource required \u003d anyRequest.getCapability();\n \n-      // Does the application need this resource?\n-      if (allocatedContainer \u003d\u003d null) {\n-        return new CSAssignment(Resources.none(), type);\n-      }\n+        // Do we need containers at this \u0027priority\u0027?\n+        if (getTotalRequiredResources(priority) \u003c\u003d 0) {\n+          continue;\n+        }\n \n-      // Inform the node\n-      node.allocateContainer(allocatedContainer);\n-            \n-      // Inform the ordering policy\n-      orderingPolicy.containerAllocated(application, allocatedContainer);\n+        // AM container allocation doesn\u0027t support non-exclusive allocation to\n+        // avoid painful of preempt an AM container\n+        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n \n-      LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" container\u003d\" + container + \n-          \" queue\u003d\" + this + \n-          \" clusterResource\u003d\" + clusterResource);\n-      createdContainer.setValue(allocatedContainer);\n-      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n-      assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getQueuePath());\n-      assignment.getAssignmentInformation().incrAllocations();\n-      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-        container.getResource());\n-      return assignment;\n-    } else {\n-      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n-      // if this was an already a reserved container, reserve it again\n-      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n-\n-        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n-          // we could possibly ignoring queue capacity or user limits when\n-          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n-          // one.\n-          if (needToUnreserve) {\n+          RMAppAttempt rmAppAttempt \u003d\n+              rmContext.getRMApps()\n+                  .get(getApplicationId()).getCurrentAppAttempt();\n+          if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n+              \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n             if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"we needed to unreserve to be able to allocate\");\n+              LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n+                  + getApplicationAttemptId()\n+                  + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n             }\n-            return new CSAssignment(Resources.none(), type);\n+            break;\n           }\n         }\n \n-        // Reserve by \u0027charging\u0027 in advance...\n-        reserve(application, priority, node, rmContainer, container);\n+        // Is the node-label-expression of this offswitch resource request\n+        // matches the node\u0027s label?\n+        // If not match, jump to next priority.\n+        if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n+            anyRequest, node.getPartition(), schedulingMode)) {\n+          continue;\n+        }\n \n-        LOG.info(\"Reserved container \" + \n-            \" application\u003d\" + application.getApplicationId() + \n-            \" resource\u003d\" + request.getCapability() + \n-            \" queue\u003d\" + this.toString() + \n-            \" usedCapacity\u003d\" + getUsedCapacity() + \n-            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n-            \" used\u003d\" + queueUsage.getUsed() +\n-            \" cluster\u003d\" + clusterResource);\n+        if (!getCSLeafQueue().getReservationContinueLooking()) {\n+          if (!shouldAllocOrReserveNewContainer(priority, required)) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n+            }\n+            continue;\n+          }\n+        }\n+\n+        if (!checkHeadroom(clusterResource, currentResourceLimits, required,\n+            node)) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"cannot allocate required resource\u003d\" + required\n+                + \" because of headroom\");\n+          }\n+          return NULL_ASSIGNMENT;\n+        }\n+\n+        // Inform the application it is about to get a scheduling opportunity\n+        addSchedulingOpportunity(priority);\n+\n+        // Increase missed-non-partitioned-resource-request-opportunity.\n+        // This is to make sure non-partitioned-resource-request will prefer\n+        // to be allocated to non-partitioned nodes\n+        int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n+        if (anyRequest.getNodeLabelExpression().equals(\n+            RMNodeLabelsManager.NO_LABEL)) {\n+          missedNonPartitionedRequestSchedulingOpportunity \u003d\n+              addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+        }\n+\n+        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n+          // Before doing allocation, we need to check scheduling opportunity to\n+          // make sure : non-partitioned resource request should be scheduled to\n+          // non-partitioned partition first.\n+          if (missedNonPartitionedRequestSchedulingOpportunity \u003c rmContext\n+              .getScheduler().getNumClusterNodes()) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Skip app_attempt\u003d\"\n+                  + getApplicationAttemptId() + \" priority\u003d\"\n+                  + priority\n+                  + \" because missed-non-partitioned-resource-request\"\n+                  + \" opportunity under requred:\" + \" Now\u003d\"\n+                  + missedNonPartitionedRequestSchedulingOpportunity\n+                  + \" required\u003d\"\n+                  + rmContext.getScheduler().getNumClusterNodes());\n+            }\n+\n+            return SKIP_ASSIGNMENT;\n+          }\n+        }\n+\n+        // Try to schedule\n         CSAssignment assignment \u003d\n-            new CSAssignment(request.getCapability(), type);\n-        assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getQueuePath());\n-        assignment.getAssignmentInformation().incrReservations();\n-        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n-          request.getCapability());\n-        return assignment;\n+            assignContainersOnNode(clusterResource, node,\n+                priority, null, schedulingMode, currentResourceLimits);\n+\n+        // Did the application skip this node?\n+        if (assignment.getSkipped()) {\n+          // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n+          subtractSchedulingOpportunity(priority);\n+          continue;\n+        }\n+\n+        // Did we schedule or reserve a container?\n+        Resource assigned \u003d assignment.getResource();\n+        if (Resources.greaterThan(rc, clusterResource,\n+            assigned, Resources.none())) {\n+          // Don\u0027t reset scheduling opportunities for offswitch assignments\n+          // otherwise the app will be delayed for each non-local assignment.\n+          // This helps apps with many off-cluster requests schedule faster.\n+          if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Resetting scheduling opportunities\");\n+            }\n+            resetSchedulingOpportunities(priority);\n+          }\n+          // Non-exclusive scheduling opportunity is different: we need reset\n+          // it every time to make sure non-labeled resource request will be\n+          // most likely allocated on non-labeled nodes first.\n+          resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+\n+          // Done\n+          return assignment;\n+        } else {\n+          // Do not assign out of order w.r.t priorities\n+          return SKIP_ASSIGNMENT;\n+        }\n       }\n-      return new CSAssignment(Resources.none(), type);\n     }\n+\n+    return SKIP_ASSIGNMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits,\n      SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pre-assignContainers for application \"\n          + getApplicationId());\n      showRequests();\n    }\n\n    // Check if application needs more resource, skip if it doesn\u0027t need more.\n    if (!hasPendingResourceRequest(rc,\n        node.getPartition(), clusterResource, schedulingMode)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Skip app_attempt\u003d\" + getApplicationAttemptId()\n            + \", because it doesn\u0027t need more resource, schedulingMode\u003d\"\n            + schedulingMode.name() + \" node-label\u003d\" + node.getPartition());\n      }\n      return SKIP_ASSIGNMENT;\n    }\n\n    synchronized (this) {\n      // Check if this resource is on the blacklist\n      if (SchedulerAppUtils.isBlacklisted(this, node, LOG)) {\n        return SKIP_ASSIGNMENT;\n      }\n\n      // Schedule in priority order\n      for (Priority priority : getPriorities()) {\n        ResourceRequest anyRequest \u003d\n            getResourceRequest(priority, ResourceRequest.ANY);\n        if (null \u003d\u003d anyRequest) {\n          continue;\n        }\n\n        // Required resource\n        Resource required \u003d anyRequest.getCapability();\n\n        // Do we need containers at this \u0027priority\u0027?\n        if (getTotalRequiredResources(priority) \u003c\u003d 0) {\n          continue;\n        }\n\n        // AM container allocation doesn\u0027t support non-exclusive allocation to\n        // avoid painful of preempt an AM container\n        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n\n          RMAppAttempt rmAppAttempt \u003d\n              rmContext.getRMApps()\n                  .get(getApplicationId()).getCurrentAppAttempt();\n          if (rmAppAttempt.getSubmissionContext().getUnmanagedAM() \u003d\u003d false\n              \u0026\u0026 null \u003d\u003d rmAppAttempt.getMasterContainer()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skip allocating AM container to app_attempt\u003d\"\n                  + getApplicationAttemptId()\n                  + \", don\u0027t allow to allocate AM container in non-exclusive mode\");\n            }\n            break;\n          }\n        }\n\n        // Is the node-label-expression of this offswitch resource request\n        // matches the node\u0027s label?\n        // If not match, jump to next priority.\n        if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n            anyRequest, node.getPartition(), schedulingMode)) {\n          continue;\n        }\n\n        if (!getCSLeafQueue().getReservationContinueLooking()) {\n          if (!shouldAllocOrReserveNewContainer(priority, required)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"doesn\u0027t need containers based on reservation algo!\");\n            }\n            continue;\n          }\n        }\n\n        if (!checkHeadroom(clusterResource, currentResourceLimits, required,\n            node)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"cannot allocate required resource\u003d\" + required\n                + \" because of headroom\");\n          }\n          return NULL_ASSIGNMENT;\n        }\n\n        // Inform the application it is about to get a scheduling opportunity\n        addSchedulingOpportunity(priority);\n\n        // Increase missed-non-partitioned-resource-request-opportunity.\n        // This is to make sure non-partitioned-resource-request will prefer\n        // to be allocated to non-partitioned nodes\n        int missedNonPartitionedRequestSchedulingOpportunity \u003d 0;\n        if (anyRequest.getNodeLabelExpression().equals(\n            RMNodeLabelsManager.NO_LABEL)) {\n          missedNonPartitionedRequestSchedulingOpportunity \u003d\n              addMissedNonPartitionedRequestSchedulingOpportunity(priority);\n        }\n\n        if (schedulingMode \u003d\u003d SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {\n          // Before doing allocation, we need to check scheduling opportunity to\n          // make sure : non-partitioned resource request should be scheduled to\n          // non-partitioned partition first.\n          if (missedNonPartitionedRequestSchedulingOpportunity \u003c rmContext\n              .getScheduler().getNumClusterNodes()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skip app_attempt\u003d\"\n                  + getApplicationAttemptId() + \" priority\u003d\"\n                  + priority\n                  + \" because missed-non-partitioned-resource-request\"\n                  + \" opportunity under requred:\" + \" Now\u003d\"\n                  + missedNonPartitionedRequestSchedulingOpportunity\n                  + \" required\u003d\"\n                  + rmContext.getScheduler().getNumClusterNodes());\n            }\n\n            return SKIP_ASSIGNMENT;\n          }\n        }\n\n        // Try to schedule\n        CSAssignment assignment \u003d\n            assignContainersOnNode(clusterResource, node,\n                priority, null, schedulingMode, currentResourceLimits);\n\n        // Did the application skip this node?\n        if (assignment.getSkipped()) {\n          // Don\u0027t count \u0027skipped nodes\u0027 as a scheduling opportunity!\n          subtractSchedulingOpportunity(priority);\n          continue;\n        }\n\n        // Did we schedule or reserve a container?\n        Resource assigned \u003d assignment.getResource();\n        if (Resources.greaterThan(rc, clusterResource,\n            assigned, Resources.none())) {\n          // Don\u0027t reset scheduling opportunities for offswitch assignments\n          // otherwise the app will be delayed for each non-local assignment.\n          // This helps apps with many off-cluster requests schedule faster.\n          if (assignment.getType() !\u003d NodeType.OFF_SWITCH) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Resetting scheduling opportunities\");\n            }\n            resetSchedulingOpportunities(priority);\n          }\n          // Non-exclusive scheduling opportunity is different: we need reset\n          // it every time to make sure non-labeled resource request will be\n          // most likely allocated on non-labeled nodes first.\n          resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n\n          // Done\n          return assignment;\n        } else {\n          // Do not assign out of order w.r.t priorities\n          return SKIP_ASSIGNMENT;\n        }\n      }\n    }\n\n    return SKIP_ASSIGNMENT;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject, schedulingMode-SchedulingMode, currentResoureLimits-ResourceLimits]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, currentResourceLimits-ResourceLimits, schedulingMode-SchedulingMode]"
          }
        }
      ]
    },
    "189a63a719c63b67a1783a280bfc2f72dcb55277": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3434. Interaction between reservations and userlimit can result in significant ULF violation\n",
      "commitDate": "23/04/15 7:39 AM",
      "commitName": "189a63a719c63b67a1783a280bfc2f72dcb55277",
      "commitAuthor": "tgraves",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3434. Interaction between reservations and userlimit can result in significant ULF violation\n",
          "commitDate": "23/04/15 7:39 AM",
          "commitName": "189a63a719c63b67a1783a280bfc2f72dcb55277",
          "commitAuthor": "tgraves",
          "commitDateOld": "21/04/15 8:06 PM",
          "commitNameOld": "bdd90110e6904b59746812d9a093924a65e72280",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.48,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,150 +1,159 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode) {\n+      MutableObject createdContainer, SchedulingMode schedulingMode,\n+      ResourceLimits currentResoureLimits) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n-    \n+\n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n+\n+    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n+        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n+\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue and its parents\u0027 resource limits\n+        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n-        Resource minimumUnreservedResource \u003d\n-            getMinimumResourceNeedUnreserved(capability);\n-        if (!shouldAllocOrReserveNewContainer\n-            || Resources.greaterThan(resourceCalculator, clusterResource,\n-                minimumUnreservedResource, Resources.none())) {\n+        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n+          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n+          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n+          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n+          // under the limit.\n+          Resource amountToUnreserve \u003d capability;\n+          if (needToUnreserve) {\n+            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n+          }\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n-                  capability, minimumUnreservedResource);\n+                  amountToUnreserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource,\n+          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n           if (!containerUnreserved) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n             \n       // Inform the ordering policy\n       orderingPolicy.containerAllocated(application, allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n         container.getId(), getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n-          // we could possibly ignoring parent queue capacity limits when\n-          // reservationsContinueLooking is set.\n-          // If we\u0027re trying to reserve a container here, not container will be\n-          // unreserved for reserving the new one. Check limits again before\n-          // reserve the new container\n-          if (!checkLimitsToReserve(clusterResource,\n-              application, capability, node.getPartition(), schedulingMode)) {\n+          // we could possibly ignoring queue capacity or user limits when\n+          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n+          // one.\n+          if (needToUnreserve) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"we needed to unreserve to be able to allocate\");\n+            }\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n           container.getId(), getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n\n    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n          // under the limit.\n          Resource amountToUnreserve \u003d capability;\n          if (needToUnreserve) {\n            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n          }\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  amountToUnreserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n            \n      // Inform the ordering policy\n      orderingPolicy.containerAllocated(application, allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n          // one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject, schedulingMode-SchedulingMode]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject, schedulingMode-SchedulingMode, currentResoureLimits-ResourceLimits]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3434. Interaction between reservations and userlimit can result in significant ULF violation\n",
          "commitDate": "23/04/15 7:39 AM",
          "commitName": "189a63a719c63b67a1783a280bfc2f72dcb55277",
          "commitAuthor": "tgraves",
          "commitDateOld": "21/04/15 8:06 PM",
          "commitNameOld": "bdd90110e6904b59746812d9a093924a65e72280",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.48,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,150 +1,159 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode) {\n+      MutableObject createdContainer, SchedulingMode schedulingMode,\n+      ResourceLimits currentResoureLimits) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n-    \n+\n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n+\n+    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n+        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n+\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue and its parents\u0027 resource limits\n+        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n-        Resource minimumUnreservedResource \u003d\n-            getMinimumResourceNeedUnreserved(capability);\n-        if (!shouldAllocOrReserveNewContainer\n-            || Resources.greaterThan(resourceCalculator, clusterResource,\n-                minimumUnreservedResource, Resources.none())) {\n+        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n+          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n+          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n+          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n+          // under the limit.\n+          Resource amountToUnreserve \u003d capability;\n+          if (needToUnreserve) {\n+            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n+          }\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n-                  capability, minimumUnreservedResource);\n+                  amountToUnreserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource,\n+          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n           if (!containerUnreserved) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n             \n       // Inform the ordering policy\n       orderingPolicy.containerAllocated(application, allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n         container.getId(), getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n-          // we could possibly ignoring parent queue capacity limits when\n-          // reservationsContinueLooking is set.\n-          // If we\u0027re trying to reserve a container here, not container will be\n-          // unreserved for reserving the new one. Check limits again before\n-          // reserve the new container\n-          if (!checkLimitsToReserve(clusterResource,\n-              application, capability, node.getPartition(), schedulingMode)) {\n+          // we could possibly ignoring queue capacity or user limits when\n+          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n+          // one.\n+          if (needToUnreserve) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"we needed to unreserve to be able to allocate\");\n+            }\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n           container.getId(), getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n\n    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n          // under the limit.\n          Resource amountToUnreserve \u003d capability;\n          if (needToUnreserve) {\n            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n          }\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  amountToUnreserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n            \n      // Inform the ordering policy\n      orderingPolicy.containerAllocated(application, allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n          // one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3463. Integrate OrderingPolicy Framework with CapacityScheduler. (Craig Welch via wangda)\n",
      "commitDate": "20/04/15 5:12 PM",
      "commitName": "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "17/04/15 1:36 PM",
      "commitNameOld": "d573f09fb93dbb711d504620af5d73840ea063a6",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 3.15,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,147 +1,150 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       MutableObject createdContainer, SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue and its parents\u0027 resource limits\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         Resource minimumUnreservedResource \u003d\n             getMinimumResourceNeedUnreserved(capability);\n         if (!shouldAllocOrReserveNewContainer\n             || Resources.greaterThan(resourceCalculator, clusterResource,\n                 minimumUnreservedResource, Resources.none())) {\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n                   capability, minimumUnreservedResource);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource,\n           if (!containerUnreserved) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n+            \n+      // Inform the ordering policy\n+      orderingPolicy.containerAllocated(application, allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n         container.getId(), getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring parent queue capacity limits when\n           // reservationsContinueLooking is set.\n           // If we\u0027re trying to reserve a container here, not container will be\n           // unreserved for reserving the new one. Check limits again before\n           // reserve the new container\n           if (!checkLimitsToReserve(clusterResource,\n               application, capability, node.getPartition(), schedulingMode)) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n           container.getId(), getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n            \n      // Inform the ordering policy\n      orderingPolicy.containerAllocated(application, allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource,\n              application, capability, node.getPartition(), schedulingMode)) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3361. CapacityScheduler side changes to support non-exclusive node labels. Contributed by Wangda Tan\n",
      "commitDate": "14/04/15 11:45 AM",
      "commitName": "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3361. CapacityScheduler side changes to support non-exclusive node labels. Contributed by Wangda Tan\n",
          "commitDate": "14/04/15 11:45 AM",
          "commitName": "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
          "commitAuthor": "Jian He",
          "commitDateOld": "09/04/15 11:38 PM",
          "commitNameOld": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
          "commitAuthorOld": "Xuan",
          "daysBetweenCommits": 4.51,
          "commitsBetweenForRepo": 30,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,148 +1,147 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer) {\n+      MutableObject createdContainer, SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n-    if (!SchedulerUtils.checkNodeLabelExpression(\n-        node.getLabels(),\n-        request.getNodeLabelExpression())) {\n+    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n+        node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue and its parents\u0027 resource limits\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         Resource minimumUnreservedResource \u003d\n             getMinimumResourceNeedUnreserved(capability);\n         if (!shouldAllocOrReserveNewContainer\n             || Resources.greaterThan(resourceCalculator, clusterResource,\n                 minimumUnreservedResource, Resources.none())) {\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n                   capability, minimumUnreservedResource);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource,\n           if (!containerUnreserved) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n         container.getId(), getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring parent queue capacity limits when\n           // reservationsContinueLooking is set.\n           // If we\u0027re trying to reserve a container here, not container will be\n           // unreserved for reserving the new one. Check limits again before\n           // reserve the new container\n-          if (!checkLimitsToReserve(clusterResource, \n-              application, capability)) {\n+          if (!checkLimitsToReserve(clusterResource,\n+              application, capability, node.getPartition(), schedulingMode)) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n           container.getId(), getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource,\n              application, capability, node.getPartition(), schedulingMode)) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject, schedulingMode-SchedulingMode]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3361. CapacityScheduler side changes to support non-exclusive node labels. Contributed by Wangda Tan\n",
          "commitDate": "14/04/15 11:45 AM",
          "commitName": "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
          "commitAuthor": "Jian He",
          "commitDateOld": "09/04/15 11:38 PM",
          "commitNameOld": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
          "commitAuthorOld": "Xuan",
          "daysBetweenCommits": 4.51,
          "commitsBetweenForRepo": 30,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,148 +1,147 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer) {\n+      MutableObject createdContainer, SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n-    if (!SchedulerUtils.checkNodeLabelExpression(\n-        node.getLabels(),\n-        request.getNodeLabelExpression())) {\n+    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n+        node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue and its parents\u0027 resource limits\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         Resource minimumUnreservedResource \u003d\n             getMinimumResourceNeedUnreserved(capability);\n         if (!shouldAllocOrReserveNewContainer\n             || Resources.greaterThan(resourceCalculator, clusterResource,\n                 minimumUnreservedResource, Resources.none())) {\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n                   capability, minimumUnreservedResource);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource,\n           if (!containerUnreserved) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n         container.getId(), getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring parent queue capacity limits when\n           // reservationsContinueLooking is set.\n           // If we\u0027re trying to reserve a container here, not container will be\n           // unreserved for reserving the new one. Check limits again before\n           // reserve the new container\n-          if (!checkLimitsToReserve(clusterResource, \n-              application, capability)) {\n+          if (!checkLimitsToReserve(clusterResource,\n+              application, capability, node.getPartition(), schedulingMode)) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n           container.getId(), getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource,\n              application, capability, node.getPartition(), schedulingMode)) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "afa5d4715a3aea2a6e93380b014c7bb8f0880383": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "YARN-3293. Track and display capacity scheduler health metrics in web\nUI. Contributed by Varun Vasudev\n",
      "commitDate": "09/04/15 11:38 PM",
      "commitName": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
      "commitAuthor": "Xuan",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "YARN-3293. Track and display capacity scheduler health metrics in web\nUI. Contributed by Varun Vasudev\n",
          "commitDate": "09/04/15 11:38 PM",
          "commitName": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
          "commitAuthor": "Xuan",
          "commitDateOld": "20/03/15 1:54 PM",
          "commitNameOld": "586348e4cbf197188057d6b843a6701cfffdaff3",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 20.41,
          "commitsBetweenForRepo": 184,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,136 +1,148 @@\n-  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n+  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n     \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue and its parents\u0027 resource limits\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         Resource minimumUnreservedResource \u003d\n             getMinimumResourceNeedUnreserved(capability);\n         if (!shouldAllocOrReserveNewContainer\n             || Resources.greaterThan(resourceCalculator, clusterResource,\n                 minimumUnreservedResource, Resources.none())) {\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n                   capability, minimumUnreservedResource);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource,\n           if (!containerUnreserved) {\n-            return Resources.none();\n+            return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n-        return Resources.none();\n+        return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n-      return container.getResource();\n+      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n+      assignment.getAssignmentInformation().addAllocationDetails(\n+        container.getId(), getQueuePath());\n+      assignment.getAssignmentInformation().incrAllocations();\n+      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n+        container.getResource());\n+      return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring parent queue capacity limits when\n           // reservationsContinueLooking is set.\n           // If we\u0027re trying to reserve a container here, not container will be\n           // unreserved for reserving the new one. Check limits again before\n           // reserve the new container\n           if (!checkLimitsToReserve(clusterResource, \n               application, capability)) {\n-            return Resources.none();\n+            return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n-\n-        return request.getCapability();\n+        CSAssignment assignment \u003d\n+            new CSAssignment(request.getCapability(), type);\n+        assignment.getAssignmentInformation().addReservationDetails(\n+          container.getId(), getQueuePath());\n+        assignment.getAssignmentInformation().incrReservations();\n+        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n+          request.getCapability());\n+        return assignment;\n       }\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource, \n              application, capability)) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "Resource",
            "newValue": "CSAssignment"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3293. Track and display capacity scheduler health metrics in web\nUI. Contributed by Varun Vasudev\n",
          "commitDate": "09/04/15 11:38 PM",
          "commitName": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
          "commitAuthor": "Xuan",
          "commitDateOld": "20/03/15 1:54 PM",
          "commitNameOld": "586348e4cbf197188057d6b843a6701cfffdaff3",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 20.41,
          "commitsBetweenForRepo": 184,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,136 +1,148 @@\n-  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n+  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n     \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue and its parents\u0027 resource limits\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         Resource minimumUnreservedResource \u003d\n             getMinimumResourceNeedUnreserved(capability);\n         if (!shouldAllocOrReserveNewContainer\n             || Resources.greaterThan(resourceCalculator, clusterResource,\n                 minimumUnreservedResource, Resources.none())) {\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n                   capability, minimumUnreservedResource);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource,\n           if (!containerUnreserved) {\n-            return Resources.none();\n+            return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n-        return Resources.none();\n+        return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n-      return container.getResource();\n+      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n+      assignment.getAssignmentInformation().addAllocationDetails(\n+        container.getId(), getQueuePath());\n+      assignment.getAssignmentInformation().incrAllocations();\n+      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n+        container.getResource());\n+      return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring parent queue capacity limits when\n           // reservationsContinueLooking is set.\n           // If we\u0027re trying to reserve a container here, not container will be\n           // unreserved for reserving the new one. Check limits again before\n           // reserve the new container\n           if (!checkLimitsToReserve(clusterResource, \n               application, capability)) {\n-            return Resources.none();\n+            return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n-\n-        return request.getCapability();\n+        CSAssignment assignment \u003d\n+            new CSAssignment(request.getCapability(), type);\n+        assignment.getAssignmentInformation().addReservationDetails(\n+          container.getId(), getQueuePath());\n+        assignment.getAssignmentInformation().incrReservations();\n+        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n+          request.getCapability());\n+        return assignment;\n       }\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource, \n              application, capability)) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "487374b7fe0c92fc7eb1406c568952722b5d5b15": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
      "commitDate": "17/03/15 10:24 AM",
      "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
          "commitDate": "17/03/15 10:24 AM",
          "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
          "commitAuthor": "Jian He",
          "commitDateOld": "03/03/15 11:49 AM",
          "commitNameOld": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 13.9,
          "commitsBetweenForRepo": 109,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,136 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      boolean needToUnreserve, MutableObject createdContainer) {\n+      MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type\n-        + \" needToUnreserve\u003d \" + needToUnreserve);\n+        + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n-    if (!Resources.fitsIn(capability, totalResource)) {\n+    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n+        capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n+\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n-\n-    // default to true since if reservation continue look feature isn\u0027t on\n-    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n-    boolean canAllocContainer \u003d true;\n-    if (this.reservationsContinueLooking) {\n-      // based on reservations can we allocate/reserve more or do we need\n-      // to unreserve one first\n-      canAllocContainer \u003d needContainers(application, priority, capability);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n-      }\n-    }\n+    \n+    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n+        application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking\n-          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n-        // need to unreserve some other container first\n-        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n-            priority, capability);\n-        if (!res) {\n-          return Resources.none();\n-        }\n-      } else {\n-        // we got here by possibly ignoring queue capacity limits. If the\n-        // parameter needToUnreserve is true it means we ignored one of those\n-        // limits in the chance we could unreserve. If we are here we aren\u0027t\n-        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n-        // limit.\n-        if (needToUnreserve) {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n+      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n+        // when reservationsContinueLooking is set, we may need to unreserve\n+        // some containers to meet this queue and its parents\u0027 resource limits\n+        // TODO, need change here when we want to support continuous reservation\n+        // looking for labeled partitions.\n+        Resource minimumUnreservedResource \u003d\n+            getMinimumResourceNeedUnreserved(capability);\n+        if (!shouldAllocOrReserveNewContainer\n+            || Resources.greaterThan(resourceCalculator, clusterResource,\n+                minimumUnreservedResource, Resources.none())) {\n+          boolean containerUnreserved \u003d\n+              findNodeToUnreserve(clusterResource, node, application, priority,\n+                  capability, minimumUnreservedResource);\n+          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n+          // container (That means we *have to* unreserve some resource to\n+          // continue)). If we failed to unreserve some resource,\n+          if (!containerUnreserved) {\n+            return Resources.none();\n           }\n-          return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n-      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n+      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n-        if (reservationsContinueLooking) {\n-          // we got here by possibly ignoring parent queue capacity limits. If\n-          // the parameter needToUnreserve is true it means we ignored one of\n-          // those limits in the chance we could unreserve. If we are here\n-          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n-          // anymore due to that parent limit\n-          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n-              needToUnreserve);\n-          if (!res) {\n+        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n+          // we could possibly ignoring parent queue capacity limits when\n+          // reservationsContinueLooking is set.\n+          // If we\u0027re trying to reserve a container here, not container will be\n+          // unreserved for reserving the new one. Check limits again before\n+          // reserve the new container\n+          if (!checkLimitsToReserve(clusterResource, \n+              application, capability)) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return Resources.none();\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource, \n              application, capability)) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, needToUnreserve-boolean, createdContainer-MutableObject]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
          "commitDate": "17/03/15 10:24 AM",
          "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
          "commitAuthor": "Jian He",
          "commitDateOld": "03/03/15 11:49 AM",
          "commitNameOld": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 13.9,
          "commitsBetweenForRepo": 109,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,136 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      boolean needToUnreserve, MutableObject createdContainer) {\n+      MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type\n-        + \" needToUnreserve\u003d \" + needToUnreserve);\n+        + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n-    if (!Resources.fitsIn(capability, totalResource)) {\n+    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n+        capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n+\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n-\n-    // default to true since if reservation continue look feature isn\u0027t on\n-    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n-    boolean canAllocContainer \u003d true;\n-    if (this.reservationsContinueLooking) {\n-      // based on reservations can we allocate/reserve more or do we need\n-      // to unreserve one first\n-      canAllocContainer \u003d needContainers(application, priority, capability);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n-      }\n-    }\n+    \n+    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n+        application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking\n-          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n-        // need to unreserve some other container first\n-        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n-            priority, capability);\n-        if (!res) {\n-          return Resources.none();\n-        }\n-      } else {\n-        // we got here by possibly ignoring queue capacity limits. If the\n-        // parameter needToUnreserve is true it means we ignored one of those\n-        // limits in the chance we could unreserve. If we are here we aren\u0027t\n-        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n-        // limit.\n-        if (needToUnreserve) {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n+      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n+        // when reservationsContinueLooking is set, we may need to unreserve\n+        // some containers to meet this queue and its parents\u0027 resource limits\n+        // TODO, need change here when we want to support continuous reservation\n+        // looking for labeled partitions.\n+        Resource minimumUnreservedResource \u003d\n+            getMinimumResourceNeedUnreserved(capability);\n+        if (!shouldAllocOrReserveNewContainer\n+            || Resources.greaterThan(resourceCalculator, clusterResource,\n+                minimumUnreservedResource, Resources.none())) {\n+          boolean containerUnreserved \u003d\n+              findNodeToUnreserve(clusterResource, node, application, priority,\n+                  capability, minimumUnreservedResource);\n+          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n+          // container (That means we *have to* unreserve some resource to\n+          // continue)). If we failed to unreserve some resource,\n+          if (!containerUnreserved) {\n+            return Resources.none();\n           }\n-          return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n-      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n+      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n-        if (reservationsContinueLooking) {\n-          // we got here by possibly ignoring parent queue capacity limits. If\n-          // the parameter needToUnreserve is true it means we ignored one of\n-          // those limits in the chance we could unreserve. If we are here\n-          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n-          // anymore due to that parent limit\n-          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n-              needToUnreserve);\n-          if (!res) {\n+        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n+          // we could possibly ignoring parent queue capacity limits when\n+          // reservationsContinueLooking is set.\n+          // If we\u0027re trying to reserve a container here, not container will be\n+          // unreserved for reserving the new one. Check limits again before\n+          // reserve the new container\n+          if (!checkLimitsToReserve(clusterResource, \n+              application, capability)) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return Resources.none();\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource, \n              application, capability)) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "e17e5ba9d7e2bd45ba6884f59f8045817594b284": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3272. Surface container locality info in RM web UI (Jian He via wangda)\n",
      "commitDate": "03/03/15 11:49 AM",
      "commitName": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
      "commitAuthor": "Wangda Tan",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3272. Surface container locality info in RM web UI (Jian He via wangda)\n",
          "commitDate": "03/03/15 11:49 AM",
          "commitName": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "02/03/15 5:52 PM",
          "commitNameOld": "14dd647c556016d351f425ee956ccf800ccb9ce2",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 0.75,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,145 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      boolean needToUnreserve) {\n+      boolean needToUnreserve, MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type\n         + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // default to true since if reservation continue look feature isn\u0027t on\n     // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n     boolean canAllocContainer \u003d true;\n     if (this.reservationsContinueLooking) {\n       // based on reservations can we allocate/reserve more or do we need\n       // to unreserve one first\n       canAllocContainer \u003d needContainers(application, priority, capability);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"can alloc container is: \" + canAllocContainer);\n       }\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking\n           \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n         // need to unreserve some other container first\n         boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n             priority, capability);\n         if (!res) {\n           return Resources.none();\n         }\n       } else {\n         // we got here by possibly ignoring queue capacity limits. If the\n         // parameter needToUnreserve is true it means we ignored one of those\n         // limits in the chance we could unreserve. If we are here we aren\u0027t\n         // trying to unreserve so we can\u0027t allocate anymore due to that parent\n         // limit.\n         if (needToUnreserve) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n           }\n           return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n-\n+      createdContainer.setValue(allocatedContainer);\n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n         if (reservationsContinueLooking) {\n           // we got here by possibly ignoring parent queue capacity limits. If\n           // the parameter needToUnreserve is true it means we ignored one of\n           // those limits in the chance we could unreserve. If we are here\n           // we aren\u0027t trying to unreserve so we can\u0027t allocate\n           // anymore due to that parent limit\n           boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n               needToUnreserve);\n           if (!res) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve, MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, needToUnreserve-boolean]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, needToUnreserve-boolean, createdContainer-MutableObject]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3272. Surface container locality info in RM web UI (Jian He via wangda)\n",
          "commitDate": "03/03/15 11:49 AM",
          "commitName": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "02/03/15 5:52 PM",
          "commitNameOld": "14dd647c556016d351f425ee956ccf800ccb9ce2",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 0.75,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,145 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      boolean needToUnreserve) {\n+      boolean needToUnreserve, MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type\n         + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // default to true since if reservation continue look feature isn\u0027t on\n     // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n     boolean canAllocContainer \u003d true;\n     if (this.reservationsContinueLooking) {\n       // based on reservations can we allocate/reserve more or do we need\n       // to unreserve one first\n       canAllocContainer \u003d needContainers(application, priority, capability);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"can alloc container is: \" + canAllocContainer);\n       }\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking\n           \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n         // need to unreserve some other container first\n         boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n             priority, capability);\n         if (!res) {\n           return Resources.none();\n         }\n       } else {\n         // we got here by possibly ignoring queue capacity limits. If the\n         // parameter needToUnreserve is true it means we ignored one of those\n         // limits in the chance we could unreserve. If we are here we aren\u0027t\n         // trying to unreserve so we can\u0027t allocate anymore due to that parent\n         // limit.\n         if (needToUnreserve) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n           }\n           return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n-\n+      createdContainer.setValue(allocatedContainer);\n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n         if (reservationsContinueLooking) {\n           // we got here by possibly ignoring parent queue capacity limits. If\n           // the parameter needToUnreserve is true it means we ignored one of\n           // those limits in the chance we could unreserve. If we are here\n           // we aren\u0027t trying to unreserve so we can\u0027t allocate\n           // anymore due to that parent limit\n           boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n               needToUnreserve);\n           if (!res) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve, MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "86358221fc85a7743052a0b4c1647353508bf308": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3099. Capacity Scheduler LeafQueue/ParentQueue should use ResourceUsage to track used-resources-by-label. Contributed by Wangda Tan\n",
      "commitDate": "30/01/15 3:15 PM",
      "commitName": "86358221fc85a7743052a0b4c1647353508bf308",
      "commitAuthor": "Jian He",
      "commitDateOld": "27/01/15 3:36 PM",
      "commitNameOld": "18741adf97f4fda5f8743318b59c440928e51297",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 2.99,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,145 +1,145 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       boolean needToUnreserve) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type\n         + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // default to true since if reservation continue look feature isn\u0027t on\n     // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n     boolean canAllocContainer \u003d true;\n     if (this.reservationsContinueLooking) {\n       // based on reservations can we allocate/reserve more or do we need\n       // to unreserve one first\n       canAllocContainer \u003d needContainers(application, priority, capability);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"can alloc container is: \" + canAllocContainer);\n       }\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking\n           \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n         // need to unreserve some other container first\n         boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n             priority, capability);\n         if (!res) {\n           return Resources.none();\n         }\n       } else {\n         // we got here by possibly ignoring queue capacity limits. If the\n         // parameter needToUnreserve is true it means we ignored one of those\n         // limits in the chance we could unreserve. If we are here we aren\u0027t\n         // trying to unreserve so we can\u0027t allocate anymore due to that parent\n         // limit.\n         if (needToUnreserve) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n           }\n           return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n         if (reservationsContinueLooking) {\n           // we got here by possibly ignoring parent queue capacity limits. If\n           // the parameter needToUnreserve is true it means we ignored one of\n           // those limits in the chance we could unreserve. If we are here\n           // we aren\u0027t trying to unreserve so we can\u0027t allocate\n           // anymore due to that parent limit\n           boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n               needToUnreserve);\n           if (!res) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n-            \" used\u003d\" + usedResources +\n+            \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2920. Changed CapacityScheduler to kill containers on nodes where node labels are changed. Contributed by  Wangda Tan\n",
      "commitDate": "22/12/14 4:51 PM",
      "commitName": "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb",
      "commitAuthor": "Jian He",
      "commitDateOld": "15/10/14 6:33 PM",
      "commitNameOld": "f2ea555ac6c06a3f2f6559731f48711fff05d3f1",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 67.97,
      "commitsBetweenForRepo": 558,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,145 +1,145 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       boolean needToUnreserve) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type\n         + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n-        labelManager.getLabelsOnNode(node.getNodeID()),\n+        node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // default to true since if reservation continue look feature isn\u0027t on\n     // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n     boolean canAllocContainer \u003d true;\n     if (this.reservationsContinueLooking) {\n       // based on reservations can we allocate/reserve more or do we need\n       // to unreserve one first\n       canAllocContainer \u003d needContainers(application, priority, capability);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"can alloc container is: \" + canAllocContainer);\n       }\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking\n           \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n         // need to unreserve some other container first\n         boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n             priority, capability);\n         if (!res) {\n           return Resources.none();\n         }\n       } else {\n         // we got here by possibly ignoring queue capacity limits. If the\n         // parameter needToUnreserve is true it means we ignored one of those\n         // limits in the chance we could unreserve. If we are here we aren\u0027t\n         // trying to unreserve so we can\u0027t allocate anymore due to that parent\n         // limit.\n         if (needToUnreserve) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n           }\n           return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n         if (reservationsContinueLooking) {\n           // we got here by possibly ignoring parent queue capacity limits. If\n           // the parameter needToUnreserve is true it means we ignored one of\n           // those limits in the chance we could unreserve. If we are here\n           // we aren\u0027t trying to unreserve so we can\u0027t allocate\n           // anymore due to that parent limit\n           boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n               needToUnreserve);\n           if (!res) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + usedResources +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + usedResources +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "f2ea555ac6c06a3f2f6559731f48711fff05d3f1": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2496. Enhanced Capacity Scheduler to have basic support for allocating resources based on node-labels. Contributed by Wangda Tan.\nYARN-2500. Ehnaced ResourceManager to support schedulers allocating resources based on node-labels. Contributed by Wangda Tan.\n",
      "commitDate": "15/10/14 6:33 PM",
      "commitName": "f2ea555ac6c06a3f2f6559731f48711fff05d3f1",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "07/10/14 1:45 PM",
      "commitNameOld": "30d56fdbb40d06c4e267d6c314c8c767a7adc6a3",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 8.2,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,131 +1,145 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       boolean needToUnreserve) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type\n         + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n+    \n+    // check if the resource request can access the label\n+    if (!SchedulerUtils.checkNodeLabelExpression(\n+        labelManager.getLabelsOnNode(node.getNodeID()),\n+        request.getNodeLabelExpression())) {\n+      // this is a reserved container, but we cannot allocate it now according\n+      // to label not match. This can be caused by node label changed\n+      // We should un-reserve this container.\n+      if (rmContainer !\u003d null) {\n+        unreserve(application, priority, node, rmContainer);\n+      }\n+      return Resources.none();\n+    }\n+    \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // default to true since if reservation continue look feature isn\u0027t on\n     // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n     boolean canAllocContainer \u003d true;\n     if (this.reservationsContinueLooking) {\n       // based on reservations can we allocate/reserve more or do we need\n       // to unreserve one first\n       canAllocContainer \u003d needContainers(application, priority, capability);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"can alloc container is: \" + canAllocContainer);\n       }\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking\n           \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n         // need to unreserve some other container first\n         boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n             priority, capability);\n         if (!res) {\n           return Resources.none();\n         }\n       } else {\n         // we got here by possibly ignoring queue capacity limits. If the\n         // parameter needToUnreserve is true it means we ignored one of those\n         // limits in the chance we could unreserve. If we are here we aren\u0027t\n         // trying to unreserve so we can\u0027t allocate anymore due to that parent\n         // limit.\n         if (needToUnreserve) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n           }\n           return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n         if (reservationsContinueLooking) {\n           // we got here by possibly ignoring parent queue capacity limits. If\n           // the parameter needToUnreserve is true it means we ignored one of\n           // those limits in the chance we could unreserve. If we are here\n           // we aren\u0027t trying to unreserve so we can\u0027t allocate\n           // anymore due to that parent limit\n           boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n               needToUnreserve);\n           if (!res) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + usedResources +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        labelManager.getLabelsOnNode(node.getNodeID()),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + usedResources +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "9c22065109a77681bc2534063eabe8692fbcb3cd": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-1769. CapacityScheduler: Improve reservations. Contributed by Thomas Graves\n",
      "commitDate": "29/09/14 7:12 AM",
      "commitName": "9c22065109a77681bc2534063eabe8692fbcb3cd",
      "commitAuthor": "Jason Lowe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-1769. CapacityScheduler: Improve reservations. Contributed by Thomas Graves\n",
          "commitDate": "29/09/14 7:12 AM",
          "commitName": "9c22065109a77681bc2534063eabe8692fbcb3cd",
          "commitAuthor": "Jason Lowe",
          "commitDateOld": "14/08/14 11:00 PM",
          "commitNameOld": "7360cec692be5dcc3377ae5082fe22870caac96b",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 45.34,
          "commitsBetweenForRepo": 409,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,76 +1,131 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n-      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n+      ResourceRequest request, NodeType type, RMContainer rmContainer,\n+      boolean needToUnreserve) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type);\n+        + \" request\u003d\" + request + \" type\u003d\" + type\n+        + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n+    // default to true since if reservation continue look feature isn\u0027t on\n+    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n+    boolean canAllocContainer \u003d true;\n+    if (this.reservationsContinueLooking) {\n+      // based on reservations can we allocate/reserve more or do we need\n+      // to unreserve one first\n+      canAllocContainer \u003d needContainers(application, priority, capability);\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n+      }\n+    }\n+\n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null){\n+      if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n+      } else if (this.reservationsContinueLooking\n+          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n+        // need to unreserve some other container first\n+        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n+            priority, capability);\n+        if (!res) {\n+          return Resources.none();\n+        }\n+      } else {\n+        // we got here by possibly ignoring queue capacity limits. If the\n+        // parameter needToUnreserve is true it means we ignored one of those\n+        // limits in the chance we could unreserve. If we are here we aren\u0027t\n+        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n+        // limit.\n+        if (needToUnreserve) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n+          }\n+          return Resources.none();\n+        }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n-      // Reserve by \u0027charging\u0027 in advance...\n-      reserve(application, priority, node, rmContainer, container);\n+      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n+      // if this was an already a reserved container, reserve it again\n+      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n-      LOG.info(\"Reserved container \" + \n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" resource\u003d\" + request.getCapability() + \n-          \" queue\u003d\" + this.toString() + \n-          \" node\u003d\" + node +\n-          \" clusterResource\u003d\" + clusterResource);\n+        if (reservationsContinueLooking) {\n+          // we got here by possibly ignoring parent queue capacity limits. If\n+          // the parameter needToUnreserve is true it means we ignored one of\n+          // those limits in the chance we could unreserve. If we are here\n+          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n+          // anymore due to that parent limit\n+          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n+              needToUnreserve);\n+          if (!res) {\n+            return Resources.none();\n+          }\n+        }\n \n-      return request.getCapability();\n+        // Reserve by \u0027charging\u0027 in advance...\n+        reserve(application, priority, node, rmContainer, container);\n+\n+        LOG.info(\"Reserved container \" + \n+            \" application\u003d\" + application.getApplicationId() + \n+            \" resource\u003d\" + request.getCapability() + \n+            \" queue\u003d\" + this.toString() + \n+            \" usedCapacity\u003d\" + getUsedCapacity() + \n+            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n+            \" used\u003d\" + usedResources +\n+            \" cluster\u003d\" + clusterResource);\n+\n+        return request.getCapability();\n+      }\n+      return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + usedResources +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, needToUnreserve-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-1769. CapacityScheduler: Improve reservations. Contributed by Thomas Graves\n",
          "commitDate": "29/09/14 7:12 AM",
          "commitName": "9c22065109a77681bc2534063eabe8692fbcb3cd",
          "commitAuthor": "Jason Lowe",
          "commitDateOld": "14/08/14 11:00 PM",
          "commitNameOld": "7360cec692be5dcc3377ae5082fe22870caac96b",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 45.34,
          "commitsBetweenForRepo": 409,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,76 +1,131 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n-      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n+      ResourceRequest request, NodeType type, RMContainer rmContainer,\n+      boolean needToUnreserve) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type);\n+        + \" request\u003d\" + request + \" type\u003d\" + type\n+        + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n+    // default to true since if reservation continue look feature isn\u0027t on\n+    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n+    boolean canAllocContainer \u003d true;\n+    if (this.reservationsContinueLooking) {\n+      // based on reservations can we allocate/reserve more or do we need\n+      // to unreserve one first\n+      canAllocContainer \u003d needContainers(application, priority, capability);\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n+      }\n+    }\n+\n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null){\n+      if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n+      } else if (this.reservationsContinueLooking\n+          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n+        // need to unreserve some other container first\n+        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n+            priority, capability);\n+        if (!res) {\n+          return Resources.none();\n+        }\n+      } else {\n+        // we got here by possibly ignoring queue capacity limits. If the\n+        // parameter needToUnreserve is true it means we ignored one of those\n+        // limits in the chance we could unreserve. If we are here we aren\u0027t\n+        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n+        // limit.\n+        if (needToUnreserve) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n+          }\n+          return Resources.none();\n+        }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n-      // Reserve by \u0027charging\u0027 in advance...\n-      reserve(application, priority, node, rmContainer, container);\n+      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n+      // if this was an already a reserved container, reserve it again\n+      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n-      LOG.info(\"Reserved container \" + \n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" resource\u003d\" + request.getCapability() + \n-          \" queue\u003d\" + this.toString() + \n-          \" node\u003d\" + node +\n-          \" clusterResource\u003d\" + clusterResource);\n+        if (reservationsContinueLooking) {\n+          // we got here by possibly ignoring parent queue capacity limits. If\n+          // the parameter needToUnreserve is true it means we ignored one of\n+          // those limits in the chance we could unreserve. If we are here\n+          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n+          // anymore due to that parent limit\n+          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n+              needToUnreserve);\n+          if (!res) {\n+            return Resources.none();\n+          }\n+        }\n \n-      return request.getCapability();\n+        // Reserve by \u0027charging\u0027 in advance...\n+        reserve(application, priority, node, rmContainer, container);\n+\n+        LOG.info(\"Reserved container \" + \n+            \" application\u003d\" + application.getApplicationId() + \n+            \" resource\u003d\" + request.getCapability() + \n+            \" queue\u003d\" + this.toString() + \n+            \" usedCapacity\u003d\" + getUsedCapacity() + \n+            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n+            \" used\u003d\" + usedResources +\n+            \" cluster\u003d\" + clusterResource);\n+\n+        return request.getCapability();\n+      }\n+      return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + usedResources +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "424fd9494f144c035fdef8c533be51e2027ad8d9": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1368. Added core functionality of recovering container state into schedulers after ResourceManager Restart so as to preserve running work in the cluster. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601303 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/06/14 8:09 PM",
      "commitName": "424fd9494f144c035fdef8c533be51e2027ad8d9",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "21/05/14 10:32 PM",
      "commitNameOld": "82f3454f5ac1f1c457e668e2cee12b4dcc800ee1",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 17.9,
      "commitsBetweenForRepo": 83,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,76 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n-      node.allocateContainer(application.getApplicationId(), \n-          allocatedContainer);\n+      node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" node\u003d\" + node +\n           \" clusterResource\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" node\u003d\" + node +\n          \" clusterResource\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "44b6261bfacddea88a3cf02d406f970bbbb98d04": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1892. Improved some logs in the scheduler. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1587717 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/04/14 1:37 PM",
      "commitName": "44b6261bfacddea88a3cf02d406f970bbbb98d04",
      "commitAuthor": "Zhijie Shen",
      "commitDateOld": "12/03/14 7:36 AM",
      "commitNameOld": "4ce0e4bf2e91278bbc33f4a1c44c7929627b5d6e",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 34.25,
      "commitsBetweenForRepo": 252,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,77 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId().getId()\n+        + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n-          \" application\u003d\" + application.getApplicationId() +\n+          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n-          \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n-          \" usedCapacity\u003d\" + getUsedCapacity() +\n-          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n-          \" used\u003d\" + usedResources + \n-          \" cluster\u003d\" + clusterResource);\n+          \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n-          \" application\u003d\" + application.getApplicationId() +\n+          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n-          \" usedCapacity\u003d\" + getUsedCapacity() +\n-          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n-          \" used\u003d\" + usedResources + \n-          \" cluster\u003d\" + clusterResource);\n+          \" node\u003d\" + node +\n+          \" clusterResource\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" node\u003d\" + node +\n          \" clusterResource\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "d0a5e43de73119e57d12f2ec89a9d1a192cde204": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1417. Modified RM to generate container-tokens not at creation time, but at allocation time so as to prevent RM\nfrom shelling out containers with expired tokens. Contributed by Omkar Vinit Joshi and Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1568060 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/02/14 2:02 PM",
      "commitName": "d0a5e43de73119e57d12f2ec89a9d1a192cde204",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "10/01/14 5:15 PM",
      "commitNameOld": "f677175f35f68bde9df72e648dffacbd31cfd620",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 33.87,
      "commitsBetweenForRepo": 224,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,83 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n-      Token containerToken \u003d\n-          createContainerToken(application, container);\n-      if (containerToken \u003d\u003d null) {\n-        // Something went wrong...\n-        return Resources.none();\n-      }\n-      container.setContainerToken(containerToken);\n-      \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "1e513bfc68c8de2976e3340cb83b6763c5d16813": {
      "type": "Ybodychange",
      "commitMessage": "YARN-957. Fixed a bug in CapacityScheduler because of which requests that need more than a node\u0027s total capability were incorrectly allocated on that node causing apps to hang. Contributed by Omkar Vinit Joshi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1520187 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/13 6:20 PM",
      "commitName": "1e513bfc68c8de2976e3340cb83b6763c5d16813",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "26/08/13 8:39 AM",
      "commitNameOld": "942e2ebaa54306ffc5b0ffb403e552764a40d58c",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 9.4,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,91 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n-\n     Resource available \u003d node.getAvailableResource();\n+    Resource totalResource \u003d node.getTotalResource();\n \n+    if (!Resources.fitsIn(capability, totalResource)) {\n+      LOG.warn(\"Node : \" + node.getNodeID()\n+          + \" does not have sufficient resource for request : \" + request\n+          + \" node total capability : \" + node.getTotalResource());\n+      return Resources.none();\n+    }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       Token containerToken \u003d\n           createContainerToken(application, container);\n       if (containerToken \u003d\u003d null) {\n         // Something went wrong...\n         return Resources.none();\n       }\n       container.setContainerToken(containerToken);\n       \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      Token containerToken \u003d\n          createContainerToken(application, container);\n      if (containerToken \u003d\u003d null) {\n        // Something went wrong...\n        return Resources.none();\n      }\n      container.setContainerToken(containerToken);\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1008. MiniYARNCluster with multiple nodemanagers, all nodes have same key for allocations. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1517563 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/08/13 8:39 AM",
      "commitName": "942e2ebaa54306ffc5b0ffb403e552764a40d58c",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "22/07/13 4:49 PM",
      "commitNameOld": "5b3bb05fbeb7ed4671f4d3a59677788f7fda43d0",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 34.66,
      "commitsBetweenForRepo": 214,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,85 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n+      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       Token containerToken \u003d\n           createContainerToken(application, container);\n       if (containerToken \u003d\u003d null) {\n         // Something went wrong...\n         return Resources.none();\n       }\n       container.setContainerToken(containerToken);\n       \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      Token containerToken \u003d\n          createContainerToken(application, container);\n      if (containerToken \u003d\u003d null) {\n        // Something went wrong...\n        return Resources.none();\n      }\n      container.setContainerToken(containerToken);\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "a2c42330047bf955a6a585dcddf798920d4c8640": {
      "type": "Ybodychange",
      "commitMessage": "YARN-717. Put object creation factories for Token in the class itself and remove useless derivations for specific tokens. Contributed by Jian He.\nMAPREDUCE-5289. Updated MR App to use Token directly after YARN-717. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1488616 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/06/13 2:43 PM",
      "commitName": "a2c42330047bf955a6a585dcddf798920d4c8640",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "29/05/13 9:59 PM",
      "commitNameOld": "b16c5638b5190c56f9d854d873589cb5c11c8b32",
      "commitAuthorOld": "Siddharth Seth",
      "daysBetweenCommits": 2.7,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,85 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n-      ContainerToken containerToken \u003d\n+      Token containerToken \u003d\n           createContainerToken(application, container);\n       if (containerToken \u003d\u003d null) {\n         // Something went wrong...\n         return Resources.none();\n       }\n       container.setContainerToken(containerToken);\n       \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      Token containerToken \u003d\n          createContainerToken(application, container);\n      if (containerToken \u003d\u003d null) {\n        // Something went wrong...\n        return Resources.none();\n      }\n      container.setContainerToken(containerToken);\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "ca8024673178fa1c80224b390dfba932921693d9": {
      "type": "Ybodychange",
      "commitMessage": "YARN-617. Made ContainerTokens to be used for validation at NodeManager also in unsecure mode to prevent AMs from faking resource requirements in unsecure mode. Contributed by Omkar Vinit Joshi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1483667 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/05/13 11:36 PM",
      "commitName": "ca8024673178fa1c80224b390dfba932921693d9",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "25/04/13 8:50 PM",
      "commitNameOld": "fbb55784d93e1a819daf55d936e864d344579cbf",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 21.12,
      "commitsBetweenForRepo": 137,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,88 +1,85 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n-      // Create container tokens in secure-mode\n-      if (UserGroupInformation.isSecurityEnabled()) {\n-        ContainerToken containerToken \u003d \n-            createContainerToken(application, container);\n-        if (containerToken \u003d\u003d null) {\n-          // Something went wrong...\n-          return Resources.none();\n-        }\n-        container.setContainerToken(containerToken);\n+      ContainerToken containerToken \u003d\n+          createContainerToken(application, container);\n+      if (containerToken \u003d\u003d null) {\n+        // Something went wrong...\n+        return Resources.none();\n       }\n+      container.setContainerToken(containerToken);\n       \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      ContainerToken containerToken \u003d\n          createContainerToken(application, container);\n      if (containerToken \u003d\u003d null) {\n        // Something went wrong...\n        return Resources.none();\n      }\n      container.setContainerToken(containerToken);\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "453926397182078c65a4428eb5de5a90d6af6448": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2. Enhanced CapacityScheduler to account for CPU alongwith memory for multi-dimensional resource scheduling. Contributed by Arun C. Murthy.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430682 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 9:08 PM",
      "commitName": "453926397182078c65a4428eb5de5a90d6af6448",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "07/11/12 1:56 PM",
      "commitNameOld": "fb5b96dfc324f999e8b3698288c110a1c3b71c30",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 62.3,
      "commitsBetweenForRepo": 257,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,88 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n-    assert (available.getMemory() \u003e  0);\n+    assert Resources.greaterThan(\n+        resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n+      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n-        available.getMemory() / capability.getMemory();         \n+        resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Create container tokens in secure-mode\n       if (UserGroupInformation.isSecurityEnabled()) {\n         ContainerToken containerToken \u003d \n             createContainerToken(application, container);\n         if (containerToken \u003d\u003d null) {\n           // Something went wrong...\n           return Resources.none();\n         }\n         container.setContainerToken(containerToken);\n       }\n       \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n+\n+      // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n-        // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Create container tokens in secure-mode\n      if (UserGroupInformation.isSecurityEnabled()) {\n        ContainerToken containerToken \u003d \n            createContainerToken(application, container);\n        if (containerToken \u003d\u003d null) {\n          // Something went wrong...\n          return Resources.none();\n        }\n        container.setContainerToken(containerToken);\n      }\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "90ba993bc72e374f99c44d0770f55aeaa8342f2d": {
      "type": "Ybodychange",
      "commitMessage": "YARN-180. Capacity scheduler - containers that get reserved create container token to early (acmurthy and bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1401703 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/10/12 7:16 AM",
      "commitName": "90ba993bc72e374f99c44d0770f55aeaa8342f2d",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "24/10/12 6:21 AM",
      "commitNameOld": "cc523683cfa76c1255667a3aedc48b08e5daabc7",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,85 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n+      // Create container tokens in secure-mode\n+      if (UserGroupInformation.isSecurityEnabled()) {\n+        ContainerToken containerToken \u003d \n+            createContainerToken(application, container);\n+        if (containerToken \u003d\u003d null) {\n+          // Something went wrong...\n+          return Resources.none();\n+        }\n+        container.setContainerToken(containerToken);\n+      }\n+      \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Create container tokens in secure-mode\n      if (UserGroupInformation.isSecurityEnabled()) {\n        ContainerToken containerToken \u003d \n            createContainerToken(application, container);\n        if (containerToken \u003d\u003d null) {\n          // Something went wrong...\n          return Resources.none();\n        }\n        container.setContainerToken(containerToken);\n      }\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "e1fdf62123625e4ba399af02f8aad500637d29d1": {
      "type": "Yfilerename",
      "commitMessage": "YARN-1. Promote YARN to be a sub-project of Apache Hadoop.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370666 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 10:22 PM",
      "commitName": "e1fdf62123625e4ba399af02f8aad500637d29d1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "07/08/12 7:53 PM",
      "commitNameOld": "34554d1e11ee1d5b564d7d9ed3e6d55931d72749",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
      }
    },
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76": {
      "type": "Yparameterchange",
      "commitMessage": "MAPREDUCE-4440. Changed SchedulerApp and SchedulerNode to be a minimal interface to allow schedulers to maintain their own.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/07/12 6:43 PM",
      "commitName": "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "10/07/12 2:26 PM",
      "commitNameOld": "3bfb26ad3b5ac46f992a632541c97ca2bc897638",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 6.18,
      "commitsBetweenForRepo": 55,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,74 @@\n-  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n-      SchedulerApp application, Priority priority, \n+  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n+      FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {
        "oldValue": "[clusterResource-Resource, node-SchedulerNode, application-SchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer]",
        "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer]"
      }
    },
    "126dd6adefeb00e4ba81ea137d63a8a76b75c3bd": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4295. RM crashes due to DNS issue (tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1352638 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/06/12 11:14 AM",
      "commitName": "126dd6adefeb00e4ba81ea137d63a8a76b75c3bd",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "21/05/12 12:15 PM",
      "commitNameOld": "d74bec2f883b562d377cc564ca86473c498a618a",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 30.96,
      "commitsBetweenForRepo": 129,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,74 @@\n   private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n       SchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n+  \n+    // something went wrong getting/creating the container \n+    if (container \u003d\u003d null) {\n+      return Resources.none();\n+    }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "ffdf980b2056b2a1b31ccb19746f23c31f7d08ef": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3816 capacity scheduler web ui bar graphs for used capacity wrong (tgraves via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1294808 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/02/12 12:06 PM",
      "commitName": "ffdf980b2056b2a1b31ccb19746f23c31f7d08ef",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "25/02/12 10:49 PM",
      "commitNameOld": "f3cc8911485385713395a04a5b292ae375ff83a3",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 2.55,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,69 @@\n   private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n       SchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n-          \" util\u003d\" + getUtilization() + \n+          \" usedCapacity\u003d\" + getUsedCapacity() +\n+          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n-          \" util\u003d\" + getUtilization() + \n+          \" usedCapacity\u003d\" + getUsedCapacity() +\n+          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "f24dcb3449c77da665058427bc7fa480cad507fc": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2907. Changed log level for various messages in ResourceManager from INFO to DEBUG. Contributed by Ravi Prakash.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179178 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/11 4:56 AM",
      "commitName": "f24dcb3449c77da665058427bc7fa480cad507fc",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "03/10/11 4:21 PM",
      "commitNameOld": "12743d2169f5a24a9b3be07c9e9dcc3f2f1001f0",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 1.52,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,67 @@\n   private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n       SchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n-      LOG.info(\"DEBUG --- assignContainers:\" +\n-          \" node\u003d\" + node.getHostName() + \n-          \" application\u003d\" + application.getApplicationId().getId() + \n-          \" priority\u003d\" + priority.getPriority() + \n-          \" request\u003d\" + request + \" type\u003d\" + type);\n+      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n+        + \" application\u003d\" + application.getApplicationId().getId()\n+        + \" priority\u003d\" + priority.getPriority()\n+        + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" util\u003d\" + getUtilization() + \n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" util\u003d\" + getUtilization() + \n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "1e6dfa7472ad78a252d05c8ebffe086d938b61fa": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2646. Fixed AMRMProtocol to return containers based on priority. Contributed by Sharad Agarwal and Arun C Murthy.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1175859 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/09/11 6:25 AM",
      "commitName": "1e6dfa7472ad78a252d05c8ebffe086d938b61fa",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "20/09/11 6:14 PM",
      "commitNameOld": "339b85b88ead760c6d4dc0f63a72780d6d5df8c2",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 5.51,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,68 @@\n   private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n       SchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.info(\"DEBUG --- assignContainers:\" +\n           \" node\u003d\" + node.getHostName() + \n           \" application\u003d\" + application.getApplicationId().getId() + \n           \" priority\u003d\" + priority.getPriority() + \n           \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n-        getContainer(rmContainer, application, node, capability);\n+        getContainer(rmContainer, application, node, capability, priority);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" util\u003d\" + getUtilization() + \n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" util\u003d\" + getUtilization() + \n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.info(\"DEBUG --- assignContainers:\" +\n          \" node\u003d\" + node.getHostName() + \n          \" application\u003d\" + application.getApplicationId().getId() + \n          \" priority\u003d\" + priority.getPriority() + \n          \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.info(\"DEBUG --- assignContainers:\" +\n          \" node\u003d\" + node.getHostName() + \n          \" application\u003d\" + application.getApplicationId().getId() + \n          \" priority\u003d\" + priority.getPriority() + \n          \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
        "newPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,68 @@\n+  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n+      SchedulerApp application, Priority priority, \n+      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n+    if (LOG.isDebugEnabled()) {\n+      LOG.info(\"DEBUG --- assignContainers:\" +\n+          \" node\u003d\" + node.getHostName() + \n+          \" application\u003d\" + application.getApplicationId().getId() + \n+          \" priority\u003d\" + priority.getPriority() + \n+          \" request\u003d\" + request + \" type\u003d\" + type);\n+    }\n+    Resource capability \u003d request.getCapability();\n+\n+    Resource available \u003d node.getAvailableResource();\n+\n+    assert (available.getMemory() \u003e  0);\n+\n+    // Create the container if necessary\n+    Container container \u003d \n+        getContainer(rmContainer, application, node, capability);\n+\n+    // Can we allocate a container on this node?\n+    int availableContainers \u003d \n+        available.getMemory() / capability.getMemory();         \n+    if (availableContainers \u003e 0) {\n+      // Allocate...\n+\n+      // Did we previously reserve containers at this \u0027priority\u0027?\n+      if (rmContainer !\u003d null){\n+        unreserve(application, priority, node, rmContainer);\n+      }\n+\n+      // Inform the application\n+      RMContainer allocatedContainer \u003d \n+          application.allocate(type, node, priority, request, container);\n+      if (allocatedContainer \u003d\u003d null) {\n+        // Did the application need this resource?\n+        return Resources.none();\n+      }\n+\n+      // Inform the node\n+      node.allocateContainer(application.getApplicationId(), \n+          allocatedContainer);\n+\n+      LOG.info(\"assignedContainer\" +\n+          \" application\u003d\" + application.getApplicationId() +\n+          \" container\u003d\" + container + \n+          \" containerId\u003d\" + container.getId() + \n+          \" queue\u003d\" + this + \n+          \" util\u003d\" + getUtilization() + \n+          \" used\u003d\" + usedResources + \n+          \" cluster\u003d\" + clusterResource);\n+\n+      return container.getResource();\n+    } else {\n+      // Reserve by \u0027charging\u0027 in advance...\n+      reserve(application, priority, node, rmContainer, container);\n+\n+      LOG.info(\"Reserved container \" + \n+          \" application\u003d\" + application.getApplicationId() +\n+          \" resource\u003d\" + request.getCapability() + \n+          \" queue\u003d\" + this.toString() + \n+          \" util\u003d\" + getUtilization() + \n+          \" used\u003d\" + usedResources + \n+          \" cluster\u003d\" + clusterResource);\n+\n+      return request.getCapability();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.info(\"DEBUG --- assignContainers:\" +\n          \" node\u003d\" + node.getHostName() + \n          \" application\u003d\" + application.getApplicationId().getId() + \n          \" priority\u003d\" + priority.getPriority() + \n          \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
    }
  }
}