{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PendingDataNodeMessages.java",
  "functionName": "removeAllMessagesForDatanode",
  "functionId": "removeAllMessagesForDatanode___dn-DatanodeDescriptor",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingDataNodeMessages.java",
  "functionStartLine": 79,
  "functionEndLine": 94,
  "numCommitsSeen": 8,
  "timeTaken": 1379,
  "changeHistory": [
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
    "e5d6fba47d9c6f4d984ca8295fcf5dee388e2241"
  ],
  "changeHistoryShort": {
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": "Ybodychange",
    "e5d6fba47d9c6f4d984ca8295fcf5dee388e2241": "Yintroduced"
  },
  "changeHistoryDetails": {
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/14 9:58 AM",
      "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "30/04/14 10:46 AM",
      "commitNameOld": "e5d6fba47d9c6f4d984ca8295fcf5dee388e2241",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 92.97,
      "commitsBetweenForRepo": 578,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   void removeAllMessagesForDatanode(DatanodeDescriptor dn) {\n     for (Map.Entry\u003cBlock, Queue\u003cReportedBlockInfo\u003e\u003e entry :\n         queueByBlockId.entrySet()) {\n       Queue\u003cReportedBlockInfo\u003e newQueue \u003d Lists.newLinkedList();\n       Queue\u003cReportedBlockInfo\u003e oldQueue \u003d entry.getValue();\n       while (!oldQueue.isEmpty()) {\n         ReportedBlockInfo rbi \u003d oldQueue.remove();\n-        if (!rbi.getNode().equals(dn)) {\n+        if (!rbi.getStorageInfo().getDatanodeDescriptor().equals(dn)) {\n           newQueue.add(rbi);\n         } else {\n           count--;\n         }\n       }\n       queueByBlockId.put(entry.getKey(), newQueue);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeAllMessagesForDatanode(DatanodeDescriptor dn) {\n    for (Map.Entry\u003cBlock, Queue\u003cReportedBlockInfo\u003e\u003e entry :\n        queueByBlockId.entrySet()) {\n      Queue\u003cReportedBlockInfo\u003e newQueue \u003d Lists.newLinkedList();\n      Queue\u003cReportedBlockInfo\u003e oldQueue \u003d entry.getValue();\n      while (!oldQueue.isEmpty()) {\n        ReportedBlockInfo rbi \u003d oldQueue.remove();\n        if (!rbi.getStorageInfo().getDatanodeDescriptor().equals(dn)) {\n          newQueue.add(rbi);\n        } else {\n          count--;\n        }\n      }\n      queueByBlockId.put(entry.getKey(), newQueue);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingDataNodeMessages.java",
      "extendedDetails": {}
    },
    "e5d6fba47d9c6f4d984ca8295fcf5dee388e2241": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-6289. HA failover can fail if there are pending DN messages for DNs which no longer exist. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1591413 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/04/14 10:46 AM",
      "commitName": "e5d6fba47d9c6f4d984ca8295fcf5dee388e2241",
      "commitAuthor": "Aaron Myers",
      "diff": "@@ -0,0 +1,16 @@\n+  void removeAllMessagesForDatanode(DatanodeDescriptor dn) {\n+    for (Map.Entry\u003cBlock, Queue\u003cReportedBlockInfo\u003e\u003e entry :\n+        queueByBlockId.entrySet()) {\n+      Queue\u003cReportedBlockInfo\u003e newQueue \u003d Lists.newLinkedList();\n+      Queue\u003cReportedBlockInfo\u003e oldQueue \u003d entry.getValue();\n+      while (!oldQueue.isEmpty()) {\n+        ReportedBlockInfo rbi \u003d oldQueue.remove();\n+        if (!rbi.getNode().equals(dn)) {\n+          newQueue.add(rbi);\n+        } else {\n+          count--;\n+        }\n+      }\n+      queueByBlockId.put(entry.getKey(), newQueue);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void removeAllMessagesForDatanode(DatanodeDescriptor dn) {\n    for (Map.Entry\u003cBlock, Queue\u003cReportedBlockInfo\u003e\u003e entry :\n        queueByBlockId.entrySet()) {\n      Queue\u003cReportedBlockInfo\u003e newQueue \u003d Lists.newLinkedList();\n      Queue\u003cReportedBlockInfo\u003e oldQueue \u003d entry.getValue();\n      while (!oldQueue.isEmpty()) {\n        ReportedBlockInfo rbi \u003d oldQueue.remove();\n        if (!rbi.getNode().equals(dn)) {\n          newQueue.add(rbi);\n        } else {\n          count--;\n        }\n      }\n      queueByBlockId.put(entry.getKey(), newQueue);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingDataNodeMessages.java"
    }
  }
}