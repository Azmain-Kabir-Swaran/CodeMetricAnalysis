{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "KafkaSink.java",
  "functionName": "putMetrics",
  "functionId": "putMetrics___record-MetricsRecord",
  "sourceFilePath": "hadoop-tools/hadoop-kafka/src/main/java/org/apache/hadoop/metrics2/sink/KafkaSink.java",
  "functionStartLine": 123,
  "functionEndLine": 182,
  "numCommitsSeen": 3,
  "timeTaken": 685,
  "changeHistory": [
    "8d202f12589356344e26b7ca15097eec46886055",
    "b59e434116a54464efa91c6e81dcd2eb94fdf1f8"
  ],
  "changeHistoryShort": {
    "8d202f12589356344e26b7ca15097eec46886055": "Ybodychange",
    "b59e434116a54464efa91c6e81dcd2eb94fdf1f8": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8d202f12589356344e26b7ca15097eec46886055": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13235. Use Date and Time API in KafkaSink.\n",
      "commitDate": "29/06/16 7:40 AM",
      "commitName": "8d202f12589356344e26b7ca15097eec46886055",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "05/10/15 2:02 PM",
      "commitNameOld": "b59e434116a54464efa91c6e81dcd2eb94fdf1f8",
      "commitAuthorOld": "Allen Wittenauer",
      "daysBetweenCommits": 267.73,
      "commitsBetweenForRepo": 1810,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,60 @@\n   public void putMetrics(MetricsRecord record) {\n \n     if (producer \u003d\u003d null) {\n       throw new MetricsException(\"Producer in KafkaSink is null!\");\n     }\n \n     // Create the json object.\n     StringBuilder jsonLines \u003d new StringBuilder();\n \n-    Long timestamp \u003d record.timestamp();\n-    Date currDate \u003d new Date(timestamp);\n-    SimpleDateFormat dateFormat \u003d new SimpleDateFormat(\"yyyy-MM-dd\");\n-    SimpleDateFormat timeFormat \u003d new SimpleDateFormat(\"hh:mm:ss\");\n-    String date \u003d dateFormat.format(currDate);\n-    String time \u003d timeFormat.format(currDate);\n+    long timestamp \u003d record.timestamp();\n+    Instant instant \u003d Instant.ofEpochMilli(timestamp);\n+    LocalDateTime ldt \u003d LocalDateTime.ofInstant(instant, zoneId);\n+    String date \u003d ldt.format(dateFormat);\n+    String time \u003d ldt.format(timeFormat);\n \n     // Collect datapoints and populate the json object.\n     jsonLines.append(\"{\\\"hostname\\\": \\\"\" + hostname);\n     jsonLines.append(\"\\\", \\\"timestamp\\\": \" + timestamp);\n     jsonLines.append(\", \\\"date\\\": \\\"\" + date);\n     jsonLines.append(\"\\\",\\\"time\\\": \\\"\" + time);\n     jsonLines.append(\"\\\",\\\"name\\\": \\\"\" + record.name() + \"\\\" \");\n     for (MetricsTag tag : record.tags()) {\n       jsonLines.append(\n           \", \\\"\" + tag.name().toString().replaceAll(\"[\\\\p{Cc}]\", \"\") + \"\\\": \");\n       jsonLines.append(\" \\\"\" + tag.value().toString() + \"\\\"\");\n     }\n     for (AbstractMetric metric : record.metrics()) {\n       jsonLines.append(\", \\\"\"\n           + metric.name().toString().replaceAll(\"[\\\\p{Cc}]\", \"\") + \"\\\": \");\n       jsonLines.append(\" \\\"\" + metric.value().toString() + \"\\\"\");\n     }\n     jsonLines.append(\"}\");\n     LOG.debug(\"kafka message: \" + jsonLines.toString());\n \n     // Create the record to be sent from the json.\n     ProducerRecord\u003cInteger, byte[]\u003e data \u003d new ProducerRecord\u003cInteger, byte[]\u003e(\n         topic, jsonLines.toString().getBytes(Charset.forName(\"UTF-8\")));\n \n     // Send the data to the Kafka broker. Here is an example of this data:\n     // {\"hostname\": \"...\", \"timestamp\": 1436913651516,\n     // \"date\": \"2015-6-14\",\"time\": \"22:40:51\",\"context\": \"yarn\",\"name\":\n     // \"QueueMetrics, \"running_0\": \"1\", \"running_60\": \"0\", \"running_300\": \"0\",\n     // \"running_1440\": \"0\", \"AppsSubmitted\": \"1\", \"AppsRunning\": \"1\",\n     // \"AppsPending\": \"0\", \"AppsCompleted\": \"0\", \"AppsKilled\": \"0\",\n     // \"AppsFailed\": \"0\", \"AllocatedMB\": \"134656\", \"AllocatedVCores\": \"132\",\n     // \"AllocatedContainers\": \"132\", \"AggregateContainersAllocated\": \"132\",\n     // \"AggregateContainersReleased\": \"0\", \"AvailableMB\": \"0\",\n     // \"AvailableVCores\": \"0\", \"PendingMB\": \"275456\", \"PendingVCores\": \"269\",\n     // \"PendingContainers\": \"269\", \"ReservedMB\": \"0\", \"ReservedVCores\": \"0\",\n     // \"ReservedContainers\": \"0\", \"ActiveUsers\": \"1\", \"ActiveApplications\": \"1\"}\n     Future\u003cRecordMetadata\u003e future \u003d producer.send(data);\n     jsonLines.setLength(0);\n     try {\n       future.get();\n     } catch (InterruptedException e) {\n       throw new MetricsException(\"Error sending data\", e);\n     } catch (ExecutionException e) {\n       throw new MetricsException(\"Error sending data\", e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void putMetrics(MetricsRecord record) {\n\n    if (producer \u003d\u003d null) {\n      throw new MetricsException(\"Producer in KafkaSink is null!\");\n    }\n\n    // Create the json object.\n    StringBuilder jsonLines \u003d new StringBuilder();\n\n    long timestamp \u003d record.timestamp();\n    Instant instant \u003d Instant.ofEpochMilli(timestamp);\n    LocalDateTime ldt \u003d LocalDateTime.ofInstant(instant, zoneId);\n    String date \u003d ldt.format(dateFormat);\n    String time \u003d ldt.format(timeFormat);\n\n    // Collect datapoints and populate the json object.\n    jsonLines.append(\"{\\\"hostname\\\": \\\"\" + hostname);\n    jsonLines.append(\"\\\", \\\"timestamp\\\": \" + timestamp);\n    jsonLines.append(\", \\\"date\\\": \\\"\" + date);\n    jsonLines.append(\"\\\",\\\"time\\\": \\\"\" + time);\n    jsonLines.append(\"\\\",\\\"name\\\": \\\"\" + record.name() + \"\\\" \");\n    for (MetricsTag tag : record.tags()) {\n      jsonLines.append(\n          \", \\\"\" + tag.name().toString().replaceAll(\"[\\\\p{Cc}]\", \"\") + \"\\\": \");\n      jsonLines.append(\" \\\"\" + tag.value().toString() + \"\\\"\");\n    }\n    for (AbstractMetric metric : record.metrics()) {\n      jsonLines.append(\", \\\"\"\n          + metric.name().toString().replaceAll(\"[\\\\p{Cc}]\", \"\") + \"\\\": \");\n      jsonLines.append(\" \\\"\" + metric.value().toString() + \"\\\"\");\n    }\n    jsonLines.append(\"}\");\n    LOG.debug(\"kafka message: \" + jsonLines.toString());\n\n    // Create the record to be sent from the json.\n    ProducerRecord\u003cInteger, byte[]\u003e data \u003d new ProducerRecord\u003cInteger, byte[]\u003e(\n        topic, jsonLines.toString().getBytes(Charset.forName(\"UTF-8\")));\n\n    // Send the data to the Kafka broker. Here is an example of this data:\n    // {\"hostname\": \"...\", \"timestamp\": 1436913651516,\n    // \"date\": \"2015-6-14\",\"time\": \"22:40:51\",\"context\": \"yarn\",\"name\":\n    // \"QueueMetrics, \"running_0\": \"1\", \"running_60\": \"0\", \"running_300\": \"0\",\n    // \"running_1440\": \"0\", \"AppsSubmitted\": \"1\", \"AppsRunning\": \"1\",\n    // \"AppsPending\": \"0\", \"AppsCompleted\": \"0\", \"AppsKilled\": \"0\",\n    // \"AppsFailed\": \"0\", \"AllocatedMB\": \"134656\", \"AllocatedVCores\": \"132\",\n    // \"AllocatedContainers\": \"132\", \"AggregateContainersAllocated\": \"132\",\n    // \"AggregateContainersReleased\": \"0\", \"AvailableMB\": \"0\",\n    // \"AvailableVCores\": \"0\", \"PendingMB\": \"275456\", \"PendingVCores\": \"269\",\n    // \"PendingContainers\": \"269\", \"ReservedMB\": \"0\", \"ReservedVCores\": \"0\",\n    // \"ReservedContainers\": \"0\", \"ActiveUsers\": \"1\", \"ActiveApplications\": \"1\"}\n    Future\u003cRecordMetadata\u003e future \u003d producer.send(data);\n    jsonLines.setLength(0);\n    try {\n      future.get();\n    } catch (InterruptedException e) {\n      throw new MetricsException(\"Error sending data\", e);\n    } catch (ExecutionException e) {\n      throw new MetricsException(\"Error sending data\", e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-kafka/src/main/java/org/apache/hadoop/metrics2/sink/KafkaSink.java",
      "extendedDetails": {}
    },
    "b59e434116a54464efa91c6e81dcd2eb94fdf1f8": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-10949. metrics2 sink plugin for Apache Kafka (Babak Behzad via aw)\n",
      "commitDate": "05/10/15 2:02 PM",
      "commitName": "b59e434116a54464efa91c6e81dcd2eb94fdf1f8",
      "commitAuthor": "Allen Wittenauer",
      "diff": "@@ -0,0 +1,61 @@\n+  public void putMetrics(MetricsRecord record) {\n+\n+    if (producer \u003d\u003d null) {\n+      throw new MetricsException(\"Producer in KafkaSink is null!\");\n+    }\n+\n+    // Create the json object.\n+    StringBuilder jsonLines \u003d new StringBuilder();\n+\n+    Long timestamp \u003d record.timestamp();\n+    Date currDate \u003d new Date(timestamp);\n+    SimpleDateFormat dateFormat \u003d new SimpleDateFormat(\"yyyy-MM-dd\");\n+    SimpleDateFormat timeFormat \u003d new SimpleDateFormat(\"hh:mm:ss\");\n+    String date \u003d dateFormat.format(currDate);\n+    String time \u003d timeFormat.format(currDate);\n+\n+    // Collect datapoints and populate the json object.\n+    jsonLines.append(\"{\\\"hostname\\\": \\\"\" + hostname);\n+    jsonLines.append(\"\\\", \\\"timestamp\\\": \" + timestamp);\n+    jsonLines.append(\", \\\"date\\\": \\\"\" + date);\n+    jsonLines.append(\"\\\",\\\"time\\\": \\\"\" + time);\n+    jsonLines.append(\"\\\",\\\"name\\\": \\\"\" + record.name() + \"\\\" \");\n+    for (MetricsTag tag : record.tags()) {\n+      jsonLines.append(\n+          \", \\\"\" + tag.name().toString().replaceAll(\"[\\\\p{Cc}]\", \"\") + \"\\\": \");\n+      jsonLines.append(\" \\\"\" + tag.value().toString() + \"\\\"\");\n+    }\n+    for (AbstractMetric metric : record.metrics()) {\n+      jsonLines.append(\", \\\"\"\n+          + metric.name().toString().replaceAll(\"[\\\\p{Cc}]\", \"\") + \"\\\": \");\n+      jsonLines.append(\" \\\"\" + metric.value().toString() + \"\\\"\");\n+    }\n+    jsonLines.append(\"}\");\n+    LOG.debug(\"kafka message: \" + jsonLines.toString());\n+\n+    // Create the record to be sent from the json.\n+    ProducerRecord\u003cInteger, byte[]\u003e data \u003d new ProducerRecord\u003cInteger, byte[]\u003e(\n+        topic, jsonLines.toString().getBytes(Charset.forName(\"UTF-8\")));\n+\n+    // Send the data to the Kafka broker. Here is an example of this data:\n+    // {\"hostname\": \"...\", \"timestamp\": 1436913651516,\n+    // \"date\": \"2015-6-14\",\"time\": \"22:40:51\",\"context\": \"yarn\",\"name\":\n+    // \"QueueMetrics, \"running_0\": \"1\", \"running_60\": \"0\", \"running_300\": \"0\",\n+    // \"running_1440\": \"0\", \"AppsSubmitted\": \"1\", \"AppsRunning\": \"1\",\n+    // \"AppsPending\": \"0\", \"AppsCompleted\": \"0\", \"AppsKilled\": \"0\",\n+    // \"AppsFailed\": \"0\", \"AllocatedMB\": \"134656\", \"AllocatedVCores\": \"132\",\n+    // \"AllocatedContainers\": \"132\", \"AggregateContainersAllocated\": \"132\",\n+    // \"AggregateContainersReleased\": \"0\", \"AvailableMB\": \"0\",\n+    // \"AvailableVCores\": \"0\", \"PendingMB\": \"275456\", \"PendingVCores\": \"269\",\n+    // \"PendingContainers\": \"269\", \"ReservedMB\": \"0\", \"ReservedVCores\": \"0\",\n+    // \"ReservedContainers\": \"0\", \"ActiveUsers\": \"1\", \"ActiveApplications\": \"1\"}\n+    Future\u003cRecordMetadata\u003e future \u003d producer.send(data);\n+    jsonLines.setLength(0);\n+    try {\n+      future.get();\n+    } catch (InterruptedException e) {\n+      throw new MetricsException(\"Error sending data\", e);\n+    } catch (ExecutionException e) {\n+      throw new MetricsException(\"Error sending data\", e);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void putMetrics(MetricsRecord record) {\n\n    if (producer \u003d\u003d null) {\n      throw new MetricsException(\"Producer in KafkaSink is null!\");\n    }\n\n    // Create the json object.\n    StringBuilder jsonLines \u003d new StringBuilder();\n\n    Long timestamp \u003d record.timestamp();\n    Date currDate \u003d new Date(timestamp);\n    SimpleDateFormat dateFormat \u003d new SimpleDateFormat(\"yyyy-MM-dd\");\n    SimpleDateFormat timeFormat \u003d new SimpleDateFormat(\"hh:mm:ss\");\n    String date \u003d dateFormat.format(currDate);\n    String time \u003d timeFormat.format(currDate);\n\n    // Collect datapoints and populate the json object.\n    jsonLines.append(\"{\\\"hostname\\\": \\\"\" + hostname);\n    jsonLines.append(\"\\\", \\\"timestamp\\\": \" + timestamp);\n    jsonLines.append(\", \\\"date\\\": \\\"\" + date);\n    jsonLines.append(\"\\\",\\\"time\\\": \\\"\" + time);\n    jsonLines.append(\"\\\",\\\"name\\\": \\\"\" + record.name() + \"\\\" \");\n    for (MetricsTag tag : record.tags()) {\n      jsonLines.append(\n          \", \\\"\" + tag.name().toString().replaceAll(\"[\\\\p{Cc}]\", \"\") + \"\\\": \");\n      jsonLines.append(\" \\\"\" + tag.value().toString() + \"\\\"\");\n    }\n    for (AbstractMetric metric : record.metrics()) {\n      jsonLines.append(\", \\\"\"\n          + metric.name().toString().replaceAll(\"[\\\\p{Cc}]\", \"\") + \"\\\": \");\n      jsonLines.append(\" \\\"\" + metric.value().toString() + \"\\\"\");\n    }\n    jsonLines.append(\"}\");\n    LOG.debug(\"kafka message: \" + jsonLines.toString());\n\n    // Create the record to be sent from the json.\n    ProducerRecord\u003cInteger, byte[]\u003e data \u003d new ProducerRecord\u003cInteger, byte[]\u003e(\n        topic, jsonLines.toString().getBytes(Charset.forName(\"UTF-8\")));\n\n    // Send the data to the Kafka broker. Here is an example of this data:\n    // {\"hostname\": \"...\", \"timestamp\": 1436913651516,\n    // \"date\": \"2015-6-14\",\"time\": \"22:40:51\",\"context\": \"yarn\",\"name\":\n    // \"QueueMetrics, \"running_0\": \"1\", \"running_60\": \"0\", \"running_300\": \"0\",\n    // \"running_1440\": \"0\", \"AppsSubmitted\": \"1\", \"AppsRunning\": \"1\",\n    // \"AppsPending\": \"0\", \"AppsCompleted\": \"0\", \"AppsKilled\": \"0\",\n    // \"AppsFailed\": \"0\", \"AllocatedMB\": \"134656\", \"AllocatedVCores\": \"132\",\n    // \"AllocatedContainers\": \"132\", \"AggregateContainersAllocated\": \"132\",\n    // \"AggregateContainersReleased\": \"0\", \"AvailableMB\": \"0\",\n    // \"AvailableVCores\": \"0\", \"PendingMB\": \"275456\", \"PendingVCores\": \"269\",\n    // \"PendingContainers\": \"269\", \"ReservedMB\": \"0\", \"ReservedVCores\": \"0\",\n    // \"ReservedContainers\": \"0\", \"ActiveUsers\": \"1\", \"ActiveApplications\": \"1\"}\n    Future\u003cRecordMetadata\u003e future \u003d producer.send(data);\n    jsonLines.setLength(0);\n    try {\n      future.get();\n    } catch (InterruptedException e) {\n      throw new MetricsException(\"Error sending data\", e);\n    } catch (ExecutionException e) {\n      throw new MetricsException(\"Error sending data\", e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-kafka/src/main/java/org/apache/hadoop/metrics2/sink/KafkaSink.java"
    }
  }
}