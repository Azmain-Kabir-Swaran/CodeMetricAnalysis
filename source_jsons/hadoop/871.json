{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSOutputStream.java",
  "functionName": "adjustChunkBoundary",
  "functionId": "adjustChunkBoundary",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
  "functionStartLine": 516,
  "functionEndLine": 528,
  "numCommitsSeen": 139,
  "timeTaken": 2474,
  "changeHistory": [
    "0eacd4c13be9bad0fbed9421a6539c64bbda4df1",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "efc510a570cf880e7df1b69932aa41932658ee51",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a"
  ],
  "changeHistoryShort": {
    "0eacd4c13be9bad0fbed9421a6539c64bbda4df1": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "efc510a570cf880e7df1b69932aa41932658ee51": "Ybodychange",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a": "Yintroduced"
  },
  "changeHistoryDetails": {
    "0eacd4c13be9bad0fbed9421a6539c64bbda4df1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11608. HDFS write crashed with block size greater than 2 GB. Contributed by Xiaobing Zhou.\n",
      "commitDate": "06/04/17 4:11 PM",
      "commitName": "0eacd4c13be9bad0fbed9421a6539c64bbda4df1",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "15/11/16 12:47 PM",
      "commitNameOld": "4fcea8a0c8019d6d9a5e6f315c83659938b93a40",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 142.1,
      "commitsBetweenForRepo": 772,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   protected void adjustChunkBoundary() {\n     if (getStreamer().getAppendChunk() \u0026\u0026\n         getStreamer().getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n       getStreamer().setAppendChunk(false);\n       resetChecksumBufSize();\n     }\n \n     if (!getStreamer().getAppendChunk()) {\n-      int psize \u003d Math.min((int)(blockSize- getStreamer().getBytesCurBlock()),\n-          dfsClient.getConf().getWritePacketSize());\n+      final int psize \u003d (int) Math\n+          .min(blockSize - getStreamer().getBytesCurBlock(), writePacketSize);\n       computePacketChunkSize(psize, bytesPerChecksum);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void adjustChunkBoundary() {\n    if (getStreamer().getAppendChunk() \u0026\u0026\n        getStreamer().getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n      getStreamer().setAppendChunk(false);\n      resetChecksumBufSize();\n    }\n\n    if (!getStreamer().getAppendChunk()) {\n      final int psize \u003d (int) Math\n          .min(blockSize - getStreamer().getBytesCurBlock(), writePacketSize);\n      computePacketChunkSize(psize, bytesPerChecksum);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void adjustChunkBoundary() {\n    if (getStreamer().getAppendChunk() \u0026\u0026\n        getStreamer().getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n      getStreamer().setAppendChunk(false);\n      resetChecksumBufSize();\n    }\n\n    if (!getStreamer().getAppendChunk()) {\n      int psize \u003d Math.min((int)(blockSize- getStreamer().getBytesCurBlock()),\n          dfsClient.getConf().getWritePacketSize());\n      computePacketChunkSize(psize, bytesPerChecksum);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "efc510a570cf880e7df1b69932aa41932658ee51": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8386. Improve synchronization of \u0027streamer\u0027 reference in DFSOutputStream. Contributed by Rakesh R.\n",
      "commitDate": "02/06/15 3:39 PM",
      "commitName": "efc510a570cf880e7df1b69932aa41932658ee51",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/04/15 7:27 PM",
      "commitNameOld": "98a61766286321468bf801a9f17a843d7eae8d9e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 32.84,
      "commitsBetweenForRepo": 337,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   protected void adjustChunkBoundary() {\n-    if (streamer.getAppendChunk() \u0026\u0026\n-        streamer.getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n-      streamer.setAppendChunk(false);\n+    if (getStreamer().getAppendChunk() \u0026\u0026\n+        getStreamer().getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n+      getStreamer().setAppendChunk(false);\n       resetChecksumBufSize();\n     }\n \n-    if (!streamer.getAppendChunk()) {\n-      int psize \u003d Math.min((int)(blockSize- streamer.getBytesCurBlock()),\n+    if (!getStreamer().getAppendChunk()) {\n+      int psize \u003d Math.min((int)(blockSize- getStreamer().getBytesCurBlock()),\n           dfsClient.getConf().getWritePacketSize());\n       computePacketChunkSize(psize, bytesPerChecksum);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void adjustChunkBoundary() {\n    if (getStreamer().getAppendChunk() \u0026\u0026\n        getStreamer().getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n      getStreamer().setAppendChunk(false);\n      resetChecksumBufSize();\n    }\n\n    if (!getStreamer().getAppendChunk()) {\n      int psize \u003d Math.min((int)(blockSize- getStreamer().getBytesCurBlock()),\n          dfsClient.getConf().getWritePacketSize());\n      computePacketChunkSize(psize, bytesPerChecksum);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "02/04/15 10:59 AM",
      "commitNameOld": "9ed43f2189fb4674b7379e8e995d53d4970d5c3a",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 8.16,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   protected void adjustChunkBoundary() {\n     if (streamer.getAppendChunk() \u0026\u0026\n         streamer.getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n       streamer.setAppendChunk(false);\n       resetChecksumBufSize();\n     }\n \n     if (!streamer.getAppendChunk()) {\n       int psize \u003d Math.min((int)(blockSize- streamer.getBytesCurBlock()),\n-          dfsClient.getConf().writePacketSize);\n+          dfsClient.getConf().getWritePacketSize());\n       computePacketChunkSize(psize, bytesPerChecksum);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void adjustChunkBoundary() {\n    if (streamer.getAppendChunk() \u0026\u0026\n        streamer.getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n      streamer.setAppendChunk(false);\n      resetChecksumBufSize();\n    }\n\n    if (!streamer.getAppendChunk()) {\n      int psize \u003d Math.min((int)(blockSize- streamer.getBytesCurBlock()),\n          dfsClient.getConf().getWritePacketSize());\n      computePacketChunkSize(psize, bytesPerChecksum);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7888. Change DFSOutputStream and DataStreamer for convenience of subclassing. Contributed by Li Bo\n",
      "commitDate": "02/04/15 10:59 AM",
      "commitName": "9ed43f2189fb4674b7379e8e995d53d4970d5c3a",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "diff": "@@ -0,0 +1,13 @@\n+  protected void adjustChunkBoundary() {\n+    if (streamer.getAppendChunk() \u0026\u0026\n+        streamer.getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n+      streamer.setAppendChunk(false);\n+      resetChecksumBufSize();\n+    }\n+\n+    if (!streamer.getAppendChunk()) {\n+      int psize \u003d Math.min((int)(blockSize- streamer.getBytesCurBlock()),\n+          dfsClient.getConf().writePacketSize);\n+      computePacketChunkSize(psize, bytesPerChecksum);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void adjustChunkBoundary() {\n    if (streamer.getAppendChunk() \u0026\u0026\n        streamer.getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n      streamer.setAppendChunk(false);\n      resetChecksumBufSize();\n    }\n\n    if (!streamer.getAppendChunk()) {\n      int psize \u003d Math.min((int)(blockSize- streamer.getBytesCurBlock()),\n          dfsClient.getConf().writePacketSize);\n      computePacketChunkSize(psize, bytesPerChecksum);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}