{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "MergeManagerImpl.java",
  "functionName": "merge",
  "functionId": "merge___inputs-List__InMemoryMapOutput__K,V____",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
  "functionStartLine": 444,
  "functionEndLine": 514,
  "numCommitsSeen": 28,
  "timeTaken": 11473,
  "changeHistory": [
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
    "df68c56267ca7dfbfee4b241bc84325d1760d12d",
    "14089f1e57e078cf20caed9db6f86de60773d704",
    "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
    "da4cab10990b3a352fc2c699f3b41c994ac55e95",
    "539153a6798a667d39f20972c5ae0936060e2cc1",
    "73fd247c7649919350ecfd16806af57ffe554649",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": "Ybodychange",
    "df68c56267ca7dfbfee4b241bc84325d1760d12d": "Ybodychange",
    "14089f1e57e078cf20caed9db6f86de60773d704": "Ybodychange",
    "0f430e53fde884f24b473043f0a7e2bffa98ebd3": "Ybodychange",
    "da4cab10990b3a352fc2c699f3b41c994ac55e95": "Ybodychange",
    "539153a6798a667d39f20972c5ae0936060e2cc1": "Ybodychange",
    "73fd247c7649919350ecfd16806af57ffe554649": "Ymultichange(Ymovefromfile,Yparameterchange)",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymovefromfile",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5890. Support for encrypting Intermediate data and spills in local filesystem. (asuresh via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1609597 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/07/14 5:43 PM",
      "commitName": "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "06/01/14 10:35 AM",
      "commitNameOld": "76238b9722539b5fd4773129ecc31b11bd8255ef",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 185.26,
      "commitsBetweenForRepo": 1309,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,72 +1,71 @@\n     public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n       if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n         return;\n       }\n       \n       //name this output file same as the name of the first file that is \n       //there in the current list of inmem files (this is guaranteed to\n       //be absent on the disk currently. So we don\u0027t overwrite a prev. \n       //created spill). Also we need to create the output file now since\n       //it is not guaranteed that this file will be present after merge\n       //is called (we delete empty files as soon as we see them\n       //in the merge method)\n \n       //figure out the mapId \n       TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n       TaskID mapTaskId \u003d mapId.getTaskID();\n \n       List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n       long mergeOutputSize \u003d \n         createInMemorySegments(inputs, inMemorySegments,0);\n       int noInMemorySegments \u003d inMemorySegments.size();\n \n       Path outputPath \u003d \n         mapOutputFile.getInputFileForWrite(mapTaskId,\n                                            mergeOutputSize).suffix(\n                                                Task.MERGED_OUTPUT_PREFIX);\n \n-      Writer\u003cK,V\u003e writer \u003d \n-        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n-                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n-                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n-                        codec, null);\n+      FSDataOutputStream out \u003d CryptoUtils.wrapIfNecessary(jobConf, rfs.create(outputPath));\n+      Writer\u003cK, V\u003e writer \u003d new Writer\u003cK, V\u003e(jobConf, out,\n+          (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n+          (Class\u003cV\u003e) jobConf.getMapOutputValueClass(), codec, null, true);\n \n       RawKeyValueIterator rIter \u003d null;\n       CompressAwarePath compressAwarePath;\n       try {\n         LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                  \" segments...\");\n         \n         rIter \u003d Merger.merge(jobConf, rfs,\n                              (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                              (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                              inMemorySegments, inMemorySegments.size(),\n                              new Path(reduceId.toString()),\n                              (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                              reporter, spilledRecordsCounter, null, null);\n         \n         if (null \u003d\u003d combinerClass) {\n           Merger.writeFile(rIter, writer, reporter, jobConf);\n         } else {\n           combineCollector.setWriter(writer);\n           combineAndSpill(rIter, reduceCombineInputCounter);\n         }\n         writer.close();\n         compressAwarePath \u003d new CompressAwarePath(outputPath,\n             writer.getRawLength(), writer.getCompressedLength());\n \n         LOG.info(reduceId +  \n             \" Merge of the \" + noInMemorySegments +\n             \" files in-memory complete.\" +\n             \" Local file is \" + outputPath + \" of size \" + \n             localFS.getFileStatus(outputPath).getLen());\n       } catch (IOException e) { \n         //make sure that we delete the ondisk file that we created \n         //earlier when we invoked cloneFileAttributes\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n       // Note the output of the merge\n       closeOnDiskFile(compressAwarePath);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n        return;\n      }\n      \n      //name this output file same as the name of the first file that is \n      //there in the current list of inmem files (this is guaranteed to\n      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n      //created spill). Also we need to create the output file now since\n      //it is not guaranteed that this file will be present after merge\n      //is called (we delete empty files as soon as we see them\n      //in the merge method)\n\n      //figure out the mapId \n      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n      TaskID mapTaskId \u003d mapId.getTaskID();\n\n      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n      long mergeOutputSize \u003d \n        createInMemorySegments(inputs, inMemorySegments,0);\n      int noInMemorySegments \u003d inMemorySegments.size();\n\n      Path outputPath \u003d \n        mapOutputFile.getInputFileForWrite(mapTaskId,\n                                           mergeOutputSize).suffix(\n                                               Task.MERGED_OUTPUT_PREFIX);\n\n      FSDataOutputStream out \u003d CryptoUtils.wrapIfNecessary(jobConf, rfs.create(outputPath));\n      Writer\u003cK, V\u003e writer \u003d new Writer\u003cK, V\u003e(jobConf, out,\n          (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n          (Class\u003cV\u003e) jobConf.getMapOutputValueClass(), codec, null, true);\n\n      RawKeyValueIterator rIter \u003d null;\n      CompressAwarePath compressAwarePath;\n      try {\n        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                 \" segments...\");\n        \n        rIter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                             inMemorySegments, inMemorySegments.size(),\n                             new Path(reduceId.toString()),\n                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                             reporter, spilledRecordsCounter, null, null);\n        \n        if (null \u003d\u003d combinerClass) {\n          Merger.writeFile(rIter, writer, reporter, jobConf);\n        } else {\n          combineCollector.setWriter(writer);\n          combineAndSpill(rIter, reduceCombineInputCounter);\n        }\n        writer.close();\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength(), writer.getCompressedLength());\n\n        LOG.info(reduceId +  \n            \" Merge of the \" + noInMemorySegments +\n            \" files in-memory complete.\" +\n            \" Local file is \" + outputPath + \" of size \" + \n            localFS.getFileStatus(outputPath).getLen());\n      } catch (IOException e) { \n        //make sure that we delete the ondisk file that we created \n        //earlier when we invoked cloneFileAttributes\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      // Note the output of the merge\n      closeOnDiskFile(compressAwarePath);\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "df68c56267ca7dfbfee4b241bc84325d1760d12d": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3685. Fix bugs in MergeManager to ensure compression codec is appropriately used and that on-disk segments are correctly sorted on file-size. Contributed by Anty Rao and Ravi Prakash.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1453365 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/03/13 7:02 AM",
      "commitName": "df68c56267ca7dfbfee4b241bc84325d1760d12d",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "27/02/13 2:40 AM",
      "commitNameOld": "14089f1e57e078cf20caed9db6f86de60773d704",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 7.18,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,72 +1,72 @@\n     public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n       if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n         return;\n       }\n       \n       //name this output file same as the name of the first file that is \n       //there in the current list of inmem files (this is guaranteed to\n       //be absent on the disk currently. So we don\u0027t overwrite a prev. \n       //created spill). Also we need to create the output file now since\n       //it is not guaranteed that this file will be present after merge\n       //is called (we delete empty files as soon as we see them\n       //in the merge method)\n \n       //figure out the mapId \n       TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n       TaskID mapTaskId \u003d mapId.getTaskID();\n \n       List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n       long mergeOutputSize \u003d \n         createInMemorySegments(inputs, inMemorySegments,0);\n       int noInMemorySegments \u003d inMemorySegments.size();\n \n       Path outputPath \u003d \n         mapOutputFile.getInputFileForWrite(mapTaskId,\n                                            mergeOutputSize).suffix(\n                                                Task.MERGED_OUTPUT_PREFIX);\n \n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n \n       RawKeyValueIterator rIter \u003d null;\n       CompressAwarePath compressAwarePath;\n       try {\n         LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                  \" segments...\");\n         \n         rIter \u003d Merger.merge(jobConf, rfs,\n                              (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                              (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                              inMemorySegments, inMemorySegments.size(),\n                              new Path(reduceId.toString()),\n                              (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                              reporter, spilledRecordsCounter, null, null);\n         \n         if (null \u003d\u003d combinerClass) {\n           Merger.writeFile(rIter, writer, reporter, jobConf);\n         } else {\n           combineCollector.setWriter(writer);\n           combineAndSpill(rIter, reduceCombineInputCounter);\n         }\n         writer.close();\n         compressAwarePath \u003d new CompressAwarePath(outputPath,\n-            writer.getRawLength());\n+            writer.getRawLength(), writer.getCompressedLength());\n \n         LOG.info(reduceId +  \n             \" Merge of the \" + noInMemorySegments +\n             \" files in-memory complete.\" +\n             \" Local file is \" + outputPath + \" of size \" + \n             localFS.getFileStatus(outputPath).getLen());\n       } catch (IOException e) { \n         //make sure that we delete the ondisk file that we created \n         //earlier when we invoked cloneFileAttributes\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n       // Note the output of the merge\n       closeOnDiskFile(compressAwarePath);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n        return;\n      }\n      \n      //name this output file same as the name of the first file that is \n      //there in the current list of inmem files (this is guaranteed to\n      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n      //created spill). Also we need to create the output file now since\n      //it is not guaranteed that this file will be present after merge\n      //is called (we delete empty files as soon as we see them\n      //in the merge method)\n\n      //figure out the mapId \n      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n      TaskID mapTaskId \u003d mapId.getTaskID();\n\n      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n      long mergeOutputSize \u003d \n        createInMemorySegments(inputs, inMemorySegments,0);\n      int noInMemorySegments \u003d inMemorySegments.size();\n\n      Path outputPath \u003d \n        mapOutputFile.getInputFileForWrite(mapTaskId,\n                                           mergeOutputSize).suffix(\n                                               Task.MERGED_OUTPUT_PREFIX);\n\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n\n      RawKeyValueIterator rIter \u003d null;\n      CompressAwarePath compressAwarePath;\n      try {\n        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                 \" segments...\");\n        \n        rIter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                             inMemorySegments, inMemorySegments.size(),\n                             new Path(reduceId.toString()),\n                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                             reporter, spilledRecordsCounter, null, null);\n        \n        if (null \u003d\u003d combinerClass) {\n          Merger.writeFile(rIter, writer, reporter, jobConf);\n        } else {\n          combineCollector.setWriter(writer);\n          combineAndSpill(rIter, reduceCombineInputCounter);\n        }\n        writer.close();\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength(), writer.getCompressedLength());\n\n        LOG.info(reduceId +  \n            \" Merge of the \" + noInMemorySegments +\n            \" files in-memory complete.\" +\n            \" Local file is \" + outputPath + \" of size \" + \n            localFS.getFileStatus(outputPath).getLen());\n      } catch (IOException e) { \n        //make sure that we delete the ondisk file that we created \n        //earlier when we invoked cloneFileAttributes\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      // Note the output of the merge\n      closeOnDiskFile(compressAwarePath);\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "14089f1e57e078cf20caed9db6f86de60773d704": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5008. Merger progress miscounts with respect to EOF_MARKER. Contributed by Sandy Ryza.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1450723 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/02/13 2:40 AM",
      "commitName": "14089f1e57e078cf20caed9db6f86de60773d704",
      "commitAuthor": "Thomas White",
      "commitDateOld": "29/01/13 11:38 AM",
      "commitNameOld": "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 28.63,
      "commitsBetweenForRepo": 101,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,72 +1,72 @@\n     public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n       if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n         return;\n       }\n       \n       //name this output file same as the name of the first file that is \n       //there in the current list of inmem files (this is guaranteed to\n       //be absent on the disk currently. So we don\u0027t overwrite a prev. \n       //created spill). Also we need to create the output file now since\n       //it is not guaranteed that this file will be present after merge\n       //is called (we delete empty files as soon as we see them\n       //in the merge method)\n \n       //figure out the mapId \n       TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n       TaskID mapTaskId \u003d mapId.getTaskID();\n \n       List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n       long mergeOutputSize \u003d \n         createInMemorySegments(inputs, inMemorySegments,0);\n       int noInMemorySegments \u003d inMemorySegments.size();\n \n       Path outputPath \u003d \n         mapOutputFile.getInputFileForWrite(mapTaskId,\n                                            mergeOutputSize).suffix(\n                                                Task.MERGED_OUTPUT_PREFIX);\n \n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n \n       RawKeyValueIterator rIter \u003d null;\n       CompressAwarePath compressAwarePath;\n       try {\n         LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                  \" segments...\");\n         \n         rIter \u003d Merger.merge(jobConf, rfs,\n                              (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                              (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                              inMemorySegments, inMemorySegments.size(),\n                              new Path(reduceId.toString()),\n                              (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                              reporter, spilledRecordsCounter, null, null);\n         \n         if (null \u003d\u003d combinerClass) {\n           Merger.writeFile(rIter, writer, reporter, jobConf);\n         } else {\n           combineCollector.setWriter(writer);\n           combineAndSpill(rIter, reduceCombineInputCounter);\n         }\n+        writer.close();\n         compressAwarePath \u003d new CompressAwarePath(outputPath,\n             writer.getRawLength());\n-        writer.close();\n \n         LOG.info(reduceId +  \n             \" Merge of the \" + noInMemorySegments +\n             \" files in-memory complete.\" +\n             \" Local file is \" + outputPath + \" of size \" + \n             localFS.getFileStatus(outputPath).getLen());\n       } catch (IOException e) { \n         //make sure that we delete the ondisk file that we created \n         //earlier when we invoked cloneFileAttributes\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n       // Note the output of the merge\n       closeOnDiskFile(compressAwarePath);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n        return;\n      }\n      \n      //name this output file same as the name of the first file that is \n      //there in the current list of inmem files (this is guaranteed to\n      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n      //created spill). Also we need to create the output file now since\n      //it is not guaranteed that this file will be present after merge\n      //is called (we delete empty files as soon as we see them\n      //in the merge method)\n\n      //figure out the mapId \n      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n      TaskID mapTaskId \u003d mapId.getTaskID();\n\n      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n      long mergeOutputSize \u003d \n        createInMemorySegments(inputs, inMemorySegments,0);\n      int noInMemorySegments \u003d inMemorySegments.size();\n\n      Path outputPath \u003d \n        mapOutputFile.getInputFileForWrite(mapTaskId,\n                                           mergeOutputSize).suffix(\n                                               Task.MERGED_OUTPUT_PREFIX);\n\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n\n      RawKeyValueIterator rIter \u003d null;\n      CompressAwarePath compressAwarePath;\n      try {\n        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                 \" segments...\");\n        \n        rIter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                             inMemorySegments, inMemorySegments.size(),\n                             new Path(reduceId.toString()),\n                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                             reporter, spilledRecordsCounter, null, null);\n        \n        if (null \u003d\u003d combinerClass) {\n          Merger.writeFile(rIter, writer, reporter, jobConf);\n        } else {\n          combineCollector.setWriter(writer);\n          combineAndSpill(rIter, reduceCombineInputCounter);\n        }\n        writer.close();\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength());\n\n        LOG.info(reduceId +  \n            \" Merge of the \" + noInMemorySegments +\n            \" files in-memory complete.\" +\n            \" Local file is \" + outputPath + \" of size \" + \n            localFS.getFileStatus(outputPath).getLen());\n      } catch (IOException e) { \n        //make sure that we delete the ondisk file that we created \n        //earlier when we invoked cloneFileAttributes\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      // Note the output of the merge\n      closeOnDiskFile(compressAwarePath);\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "0f430e53fde884f24b473043f0a7e2bffa98ebd3": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1440076 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/01/13 11:38 AM",
      "commitName": "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "28/01/13 10:58 AM",
      "commitNameOld": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 1.03,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,72 @@\n     public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n       if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n         return;\n       }\n       \n       //name this output file same as the name of the first file that is \n       //there in the current list of inmem files (this is guaranteed to\n       //be absent on the disk currently. So we don\u0027t overwrite a prev. \n       //created spill). Also we need to create the output file now since\n       //it is not guaranteed that this file will be present after merge\n       //is called (we delete empty files as soon as we see them\n       //in the merge method)\n \n       //figure out the mapId \n       TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n       TaskID mapTaskId \u003d mapId.getTaskID();\n \n       List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n       long mergeOutputSize \u003d \n         createInMemorySegments(inputs, inMemorySegments,0);\n       int noInMemorySegments \u003d inMemorySegments.size();\n \n       Path outputPath \u003d \n         mapOutputFile.getInputFileForWrite(mapTaskId,\n                                            mergeOutputSize).suffix(\n                                                Task.MERGED_OUTPUT_PREFIX);\n \n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n \n       RawKeyValueIterator rIter \u003d null;\n+      CompressAwarePath compressAwarePath;\n       try {\n         LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                  \" segments...\");\n         \n         rIter \u003d Merger.merge(jobConf, rfs,\n                              (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                              (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                              inMemorySegments, inMemorySegments.size(),\n                              new Path(reduceId.toString()),\n                              (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                              reporter, spilledRecordsCounter, null, null);\n         \n         if (null \u003d\u003d combinerClass) {\n           Merger.writeFile(rIter, writer, reporter, jobConf);\n         } else {\n           combineCollector.setWriter(writer);\n           combineAndSpill(rIter, reduceCombineInputCounter);\n         }\n+        compressAwarePath \u003d new CompressAwarePath(outputPath,\n+            writer.getRawLength());\n         writer.close();\n \n         LOG.info(reduceId +  \n             \" Merge of the \" + noInMemorySegments +\n             \" files in-memory complete.\" +\n             \" Local file is \" + outputPath + \" of size \" + \n             localFS.getFileStatus(outputPath).getLen());\n       } catch (IOException e) { \n         //make sure that we delete the ondisk file that we created \n         //earlier when we invoked cloneFileAttributes\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n       // Note the output of the merge\n-      closeOnDiskFile(outputPath);\n+      closeOnDiskFile(compressAwarePath);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n        return;\n      }\n      \n      //name this output file same as the name of the first file that is \n      //there in the current list of inmem files (this is guaranteed to\n      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n      //created spill). Also we need to create the output file now since\n      //it is not guaranteed that this file will be present after merge\n      //is called (we delete empty files as soon as we see them\n      //in the merge method)\n\n      //figure out the mapId \n      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n      TaskID mapTaskId \u003d mapId.getTaskID();\n\n      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n      long mergeOutputSize \u003d \n        createInMemorySegments(inputs, inMemorySegments,0);\n      int noInMemorySegments \u003d inMemorySegments.size();\n\n      Path outputPath \u003d \n        mapOutputFile.getInputFileForWrite(mapTaskId,\n                                           mergeOutputSize).suffix(\n                                               Task.MERGED_OUTPUT_PREFIX);\n\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n\n      RawKeyValueIterator rIter \u003d null;\n      CompressAwarePath compressAwarePath;\n      try {\n        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                 \" segments...\");\n        \n        rIter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                             inMemorySegments, inMemorySegments.size(),\n                             new Path(reduceId.toString()),\n                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                             reporter, spilledRecordsCounter, null, null);\n        \n        if (null \u003d\u003d combinerClass) {\n          Merger.writeFile(rIter, writer, reporter, jobConf);\n        } else {\n          combineCollector.setWriter(writer);\n          combineAndSpill(rIter, reduceCombineInputCounter);\n        }\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength());\n        writer.close();\n\n        LOG.info(reduceId +  \n            \" Merge of the \" + noInMemorySegments +\n            \" files in-memory complete.\" +\n            \" Local file is \" + outputPath + \" of size \" + \n            localFS.getFileStatus(outputPath).getLen());\n      } catch (IOException e) { \n        //make sure that we delete the ondisk file that we created \n        //earlier when we invoked cloneFileAttributes\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      // Note the output of the merge\n      closeOnDiskFile(compressAwarePath);\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "da4cab10990b3a352fc2c699f3b41c994ac55e95": {
      "type": "Ybodychange",
      "commitMessage": "Revering MAPREDUCE-2264\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1439561 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/01/13 10:58 AM",
      "commitName": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "24/01/13 4:25 PM",
      "commitNameOld": "539153a6798a667d39f20972c5ae0936060e2cc1",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 3.77,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,72 +1,69 @@\n     public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n       if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n         return;\n       }\n       \n       //name this output file same as the name of the first file that is \n       //there in the current list of inmem files (this is guaranteed to\n       //be absent on the disk currently. So we don\u0027t overwrite a prev. \n       //created spill). Also we need to create the output file now since\n       //it is not guaranteed that this file will be present after merge\n       //is called (we delete empty files as soon as we see them\n       //in the merge method)\n \n       //figure out the mapId \n       TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n       TaskID mapTaskId \u003d mapId.getTaskID();\n \n       List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n       long mergeOutputSize \u003d \n         createInMemorySegments(inputs, inMemorySegments,0);\n       int noInMemorySegments \u003d inMemorySegments.size();\n \n       Path outputPath \u003d \n         mapOutputFile.getInputFileForWrite(mapTaskId,\n                                            mergeOutputSize).suffix(\n                                                Task.MERGED_OUTPUT_PREFIX);\n \n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n \n       RawKeyValueIterator rIter \u003d null;\n-      CompressAwarePath compressAwarePath;\n       try {\n         LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                  \" segments...\");\n         \n         rIter \u003d Merger.merge(jobConf, rfs,\n                              (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                              (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                              inMemorySegments, inMemorySegments.size(),\n                              new Path(reduceId.toString()),\n                              (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                              reporter, spilledRecordsCounter, null, null);\n         \n         if (null \u003d\u003d combinerClass) {\n           Merger.writeFile(rIter, writer, reporter, jobConf);\n         } else {\n           combineCollector.setWriter(writer);\n           combineAndSpill(rIter, reduceCombineInputCounter);\n         }\n-        compressAwarePath \u003d new CompressAwarePath(outputPath,\n-            writer.getRawLength());\n         writer.close();\n \n         LOG.info(reduceId +  \n             \" Merge of the \" + noInMemorySegments +\n             \" files in-memory complete.\" +\n             \" Local file is \" + outputPath + \" of size \" + \n             localFS.getFileStatus(outputPath).getLen());\n       } catch (IOException e) { \n         //make sure that we delete the ondisk file that we created \n         //earlier when we invoked cloneFileAttributes\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n       // Note the output of the merge\n-      closeOnDiskFile(compressAwarePath);\n+      closeOnDiskFile(outputPath);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n        return;\n      }\n      \n      //name this output file same as the name of the first file that is \n      //there in the current list of inmem files (this is guaranteed to\n      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n      //created spill). Also we need to create the output file now since\n      //it is not guaranteed that this file will be present after merge\n      //is called (we delete empty files as soon as we see them\n      //in the merge method)\n\n      //figure out the mapId \n      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n      TaskID mapTaskId \u003d mapId.getTaskID();\n\n      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n      long mergeOutputSize \u003d \n        createInMemorySegments(inputs, inMemorySegments,0);\n      int noInMemorySegments \u003d inMemorySegments.size();\n\n      Path outputPath \u003d \n        mapOutputFile.getInputFileForWrite(mapTaskId,\n                                           mergeOutputSize).suffix(\n                                               Task.MERGED_OUTPUT_PREFIX);\n\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n\n      RawKeyValueIterator rIter \u003d null;\n      try {\n        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                 \" segments...\");\n        \n        rIter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                             inMemorySegments, inMemorySegments.size(),\n                             new Path(reduceId.toString()),\n                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                             reporter, spilledRecordsCounter, null, null);\n        \n        if (null \u003d\u003d combinerClass) {\n          Merger.writeFile(rIter, writer, reporter, jobConf);\n        } else {\n          combineCollector.setWriter(writer);\n          combineAndSpill(rIter, reduceCombineInputCounter);\n        }\n        writer.close();\n\n        LOG.info(reduceId +  \n            \" Merge of the \" + noInMemorySegments +\n            \" files in-memory complete.\" +\n            \" Local file is \" + outputPath + \" of size \" + \n            localFS.getFileStatus(outputPath).getLen());\n      } catch (IOException e) { \n        //make sure that we delete the ondisk file that we created \n        //earlier when we invoked cloneFileAttributes\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      // Note the output of the merge\n      closeOnDiskFile(outputPath);\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "539153a6798a667d39f20972c5ae0936060e2cc1": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1438277 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/01/13 4:25 PM",
      "commitName": "539153a6798a667d39f20972c5ae0936060e2cc1",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "22/01/13 6:10 AM",
      "commitNameOld": "73fd247c7649919350ecfd16806af57ffe554649",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 2.43,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,72 @@\n     public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n       if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n         return;\n       }\n       \n       //name this output file same as the name of the first file that is \n       //there in the current list of inmem files (this is guaranteed to\n       //be absent on the disk currently. So we don\u0027t overwrite a prev. \n       //created spill). Also we need to create the output file now since\n       //it is not guaranteed that this file will be present after merge\n       //is called (we delete empty files as soon as we see them\n       //in the merge method)\n \n       //figure out the mapId \n       TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n       TaskID mapTaskId \u003d mapId.getTaskID();\n \n       List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n       long mergeOutputSize \u003d \n         createInMemorySegments(inputs, inMemorySegments,0);\n       int noInMemorySegments \u003d inMemorySegments.size();\n \n       Path outputPath \u003d \n         mapOutputFile.getInputFileForWrite(mapTaskId,\n                                            mergeOutputSize).suffix(\n                                                Task.MERGED_OUTPUT_PREFIX);\n \n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n \n       RawKeyValueIterator rIter \u003d null;\n+      CompressAwarePath compressAwarePath;\n       try {\n         LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                  \" segments...\");\n         \n         rIter \u003d Merger.merge(jobConf, rfs,\n                              (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                              (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                              inMemorySegments, inMemorySegments.size(),\n                              new Path(reduceId.toString()),\n                              (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                              reporter, spilledRecordsCounter, null, null);\n         \n         if (null \u003d\u003d combinerClass) {\n           Merger.writeFile(rIter, writer, reporter, jobConf);\n         } else {\n           combineCollector.setWriter(writer);\n           combineAndSpill(rIter, reduceCombineInputCounter);\n         }\n+        compressAwarePath \u003d new CompressAwarePath(outputPath,\n+            writer.getRawLength());\n         writer.close();\n \n         LOG.info(reduceId +  \n             \" Merge of the \" + noInMemorySegments +\n             \" files in-memory complete.\" +\n             \" Local file is \" + outputPath + \" of size \" + \n             localFS.getFileStatus(outputPath).getLen());\n       } catch (IOException e) { \n         //make sure that we delete the ondisk file that we created \n         //earlier when we invoked cloneFileAttributes\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n       // Note the output of the merge\n-      closeOnDiskFile(outputPath);\n+      closeOnDiskFile(compressAwarePath);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n        return;\n      }\n      \n      //name this output file same as the name of the first file that is \n      //there in the current list of inmem files (this is guaranteed to\n      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n      //created spill). Also we need to create the output file now since\n      //it is not guaranteed that this file will be present after merge\n      //is called (we delete empty files as soon as we see them\n      //in the merge method)\n\n      //figure out the mapId \n      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n      TaskID mapTaskId \u003d mapId.getTaskID();\n\n      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n      long mergeOutputSize \u003d \n        createInMemorySegments(inputs, inMemorySegments,0);\n      int noInMemorySegments \u003d inMemorySegments.size();\n\n      Path outputPath \u003d \n        mapOutputFile.getInputFileForWrite(mapTaskId,\n                                           mergeOutputSize).suffix(\n                                               Task.MERGED_OUTPUT_PREFIX);\n\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n\n      RawKeyValueIterator rIter \u003d null;\n      CompressAwarePath compressAwarePath;\n      try {\n        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                 \" segments...\");\n        \n        rIter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                             inMemorySegments, inMemorySegments.size(),\n                             new Path(reduceId.toString()),\n                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                             reporter, spilledRecordsCounter, null, null);\n        \n        if (null \u003d\u003d combinerClass) {\n          Merger.writeFile(rIter, writer, reporter, jobConf);\n        } else {\n          combineCollector.setWriter(writer);\n          combineAndSpill(rIter, reduceCombineInputCounter);\n        }\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength());\n        writer.close();\n\n        LOG.info(reduceId +  \n            \" Merge of the \" + noInMemorySegments +\n            \" files in-memory complete.\" +\n            \" Local file is \" + outputPath + \" of size \" + \n            localFS.getFileStatus(outputPath).getLen());\n      } catch (IOException e) { \n        //make sure that we delete the ondisk file that we created \n        //earlier when we invoked cloneFileAttributes\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      // Note the output of the merge\n      closeOnDiskFile(compressAwarePath);\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "73fd247c7649919350ecfd16806af57ffe554649": {
      "type": "Ymultichange(Ymovefromfile,Yparameterchange)",
      "commitMessage": "MAPREDUCE-4808. Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations. (masokan via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1436936 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/01/13 6:10 AM",
      "commitName": "73fd247c7649919350ecfd16806af57ffe554649",
      "commitAuthor": "Alejandro Abdelnur",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "MAPREDUCE-4808. Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations. (masokan via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1436936 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/01/13 6:10 AM",
          "commitName": "73fd247c7649919350ecfd16806af57ffe554649",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "21/01/13 6:59 PM",
          "commitNameOld": "cfae13306ac0fb3f3c139d5ac511bf78cede1b77",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.47,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,69 +1,69 @@\n-    public void merge(List\u003cMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n+    public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n       if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n         return;\n       }\n       \n       //name this output file same as the name of the first file that is \n       //there in the current list of inmem files (this is guaranteed to\n       //be absent on the disk currently. So we don\u0027t overwrite a prev. \n       //created spill). Also we need to create the output file now since\n       //it is not guaranteed that this file will be present after merge\n       //is called (we delete empty files as soon as we see them\n       //in the merge method)\n \n       //figure out the mapId \n       TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n       TaskID mapTaskId \u003d mapId.getTaskID();\n \n       List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n       long mergeOutputSize \u003d \n         createInMemorySegments(inputs, inMemorySegments,0);\n       int noInMemorySegments \u003d inMemorySegments.size();\n \n       Path outputPath \u003d \n         mapOutputFile.getInputFileForWrite(mapTaskId,\n                                            mergeOutputSize).suffix(\n                                                Task.MERGED_OUTPUT_PREFIX);\n \n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n \n       RawKeyValueIterator rIter \u003d null;\n       try {\n         LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                  \" segments...\");\n         \n         rIter \u003d Merger.merge(jobConf, rfs,\n                              (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                              (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                              inMemorySegments, inMemorySegments.size(),\n                              new Path(reduceId.toString()),\n                              (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                              reporter, spilledRecordsCounter, null, null);\n         \n         if (null \u003d\u003d combinerClass) {\n           Merger.writeFile(rIter, writer, reporter, jobConf);\n         } else {\n           combineCollector.setWriter(writer);\n           combineAndSpill(rIter, reduceCombineInputCounter);\n         }\n         writer.close();\n \n         LOG.info(reduceId +  \n             \" Merge of the \" + noInMemorySegments +\n             \" files in-memory complete.\" +\n             \" Local file is \" + outputPath + \" of size \" + \n             localFS.getFileStatus(outputPath).getLen());\n       } catch (IOException e) { \n         //make sure that we delete the ondisk file that we created \n         //earlier when we invoked cloneFileAttributes\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n       // Note the output of the merge\n       closeOnDiskFile(outputPath);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n        return;\n      }\n      \n      //name this output file same as the name of the first file that is \n      //there in the current list of inmem files (this is guaranteed to\n      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n      //created spill). Also we need to create the output file now since\n      //it is not guaranteed that this file will be present after merge\n      //is called (we delete empty files as soon as we see them\n      //in the merge method)\n\n      //figure out the mapId \n      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n      TaskID mapTaskId \u003d mapId.getTaskID();\n\n      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n      long mergeOutputSize \u003d \n        createInMemorySegments(inputs, inMemorySegments,0);\n      int noInMemorySegments \u003d inMemorySegments.size();\n\n      Path outputPath \u003d \n        mapOutputFile.getInputFileForWrite(mapTaskId,\n                                           mergeOutputSize).suffix(\n                                               Task.MERGED_OUTPUT_PREFIX);\n\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n\n      RawKeyValueIterator rIter \u003d null;\n      try {\n        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                 \" segments...\");\n        \n        rIter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                             inMemorySegments, inMemorySegments.size(),\n                             new Path(reduceId.toString()),\n                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                             reporter, spilledRecordsCounter, null, null);\n        \n        if (null \u003d\u003d combinerClass) {\n          Merger.writeFile(rIter, writer, reporter, jobConf);\n        } else {\n          combineCollector.setWriter(writer);\n          combineAndSpill(rIter, reduceCombineInputCounter);\n        }\n        writer.close();\n\n        LOG.info(reduceId +  \n            \" Merge of the \" + noInMemorySegments +\n            \" files in-memory complete.\" +\n            \" Local file is \" + outputPath + \" of size \" + \n            localFS.getFileStatus(outputPath).getLen());\n      } catch (IOException e) { \n        //make sure that we delete the ondisk file that we created \n        //earlier when we invoked cloneFileAttributes\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      // Note the output of the merge\n      closeOnDiskFile(outputPath);\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {
            "oldPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
            "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
            "oldMethodName": "merge",
            "newMethodName": "merge"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-4808. Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations. (masokan via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1436936 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/01/13 6:10 AM",
          "commitName": "73fd247c7649919350ecfd16806af57ffe554649",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "21/01/13 6:59 PM",
          "commitNameOld": "cfae13306ac0fb3f3c139d5ac511bf78cede1b77",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.47,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,69 +1,69 @@\n-    public void merge(List\u003cMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n+    public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n       if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n         return;\n       }\n       \n       //name this output file same as the name of the first file that is \n       //there in the current list of inmem files (this is guaranteed to\n       //be absent on the disk currently. So we don\u0027t overwrite a prev. \n       //created spill). Also we need to create the output file now since\n       //it is not guaranteed that this file will be present after merge\n       //is called (we delete empty files as soon as we see them\n       //in the merge method)\n \n       //figure out the mapId \n       TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n       TaskID mapTaskId \u003d mapId.getTaskID();\n \n       List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n       long mergeOutputSize \u003d \n         createInMemorySegments(inputs, inMemorySegments,0);\n       int noInMemorySegments \u003d inMemorySegments.size();\n \n       Path outputPath \u003d \n         mapOutputFile.getInputFileForWrite(mapTaskId,\n                                            mergeOutputSize).suffix(\n                                                Task.MERGED_OUTPUT_PREFIX);\n \n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n \n       RawKeyValueIterator rIter \u003d null;\n       try {\n         LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                  \" segments...\");\n         \n         rIter \u003d Merger.merge(jobConf, rfs,\n                              (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                              (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                              inMemorySegments, inMemorySegments.size(),\n                              new Path(reduceId.toString()),\n                              (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                              reporter, spilledRecordsCounter, null, null);\n         \n         if (null \u003d\u003d combinerClass) {\n           Merger.writeFile(rIter, writer, reporter, jobConf);\n         } else {\n           combineCollector.setWriter(writer);\n           combineAndSpill(rIter, reduceCombineInputCounter);\n         }\n         writer.close();\n \n         LOG.info(reduceId +  \n             \" Merge of the \" + noInMemorySegments +\n             \" files in-memory complete.\" +\n             \" Local file is \" + outputPath + \" of size \" + \n             localFS.getFileStatus(outputPath).getLen());\n       } catch (IOException e) { \n         //make sure that we delete the ondisk file that we created \n         //earlier when we invoked cloneFileAttributes\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n       // Note the output of the merge\n       closeOnDiskFile(outputPath);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void merge(List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n        return;\n      }\n      \n      //name this output file same as the name of the first file that is \n      //there in the current list of inmem files (this is guaranteed to\n      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n      //created spill). Also we need to create the output file now since\n      //it is not guaranteed that this file will be present after merge\n      //is called (we delete empty files as soon as we see them\n      //in the merge method)\n\n      //figure out the mapId \n      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n      TaskID mapTaskId \u003d mapId.getTaskID();\n\n      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n      long mergeOutputSize \u003d \n        createInMemorySegments(inputs, inMemorySegments,0);\n      int noInMemorySegments \u003d inMemorySegments.size();\n\n      Path outputPath \u003d \n        mapOutputFile.getInputFileForWrite(mapTaskId,\n                                           mergeOutputSize).suffix(\n                                               Task.MERGED_OUTPUT_PREFIX);\n\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n\n      RawKeyValueIterator rIter \u003d null;\n      try {\n        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                 \" segments...\");\n        \n        rIter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                             inMemorySegments, inMemorySegments.size(),\n                             new Path(reduceId.toString()),\n                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                             reporter, spilledRecordsCounter, null, null);\n        \n        if (null \u003d\u003d combinerClass) {\n          Merger.writeFile(rIter, writer, reporter, jobConf);\n        } else {\n          combineCollector.setWriter(writer);\n          combineAndSpill(rIter, reduceCombineInputCounter);\n        }\n        writer.close();\n\n        LOG.info(reduceId +  \n            \" Merge of the \" + noInMemorySegments +\n            \" files in-memory complete.\" +\n            \" Local file is \" + outputPath + \" of size \" + \n            localFS.getFileStatus(outputPath).getLen());\n      } catch (IOException e) { \n        //make sure that we delete the ondisk file that we created \n        //earlier when we invoked cloneFileAttributes\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      // Note the output of the merge\n      closeOnDiskFile(outputPath);\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {
            "oldValue": "[inputs-List\u003cMapOutput\u003cK,V\u003e\u003e]",
            "newValue": "[inputs-List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e]"
          }
        }
      ]
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void merge(List\u003cMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n        return;\n      }\n      \n      //name this output file same as the name of the first file that is \n      //there in the current list of inmem files (this is guaranteed to\n      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n      //created spill). Also we need to create the output file now since\n      //it is not guaranteed that this file will be present after merge\n      //is called (we delete empty files as soon as we see them\n      //in the merge method)\n\n      //figure out the mapId \n      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n      TaskID mapTaskId \u003d mapId.getTaskID();\n\n      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n      long mergeOutputSize \u003d \n        createInMemorySegments(inputs, inMemorySegments,0);\n      int noInMemorySegments \u003d inMemorySegments.size();\n\n      Path outputPath \u003d \n        mapOutputFile.getInputFileForWrite(mapTaskId,\n                                           mergeOutputSize).suffix(\n                                               Task.MERGED_OUTPUT_PREFIX);\n\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n\n      RawKeyValueIterator rIter \u003d null;\n      try {\n        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                 \" segments...\");\n        \n        rIter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                             inMemorySegments, inMemorySegments.size(),\n                             new Path(reduceId.toString()),\n                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                             reporter, spilledRecordsCounter, null, null);\n        \n        if (null \u003d\u003d combinerClass) {\n          Merger.writeFile(rIter, writer, reporter, jobConf);\n        } else {\n          combineCollector.setWriter(writer);\n          combineAndSpill(rIter, reduceCombineInputCounter);\n        }\n        writer.close();\n\n        LOG.info(reduceId +  \n            \" Merge of the \" + noInMemorySegments +\n            \" files in-memory complete.\" +\n            \" Local file is \" + outputPath + \" of size \" + \n            localFS.getFileStatus(outputPath).getLen());\n      } catch (IOException e) { \n        //make sure that we delete the ondisk file that we created \n        //earlier when we invoked cloneFileAttributes\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      // Note the output of the merge\n      closeOnDiskFile(outputPath);\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void merge(List\u003cMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n        return;\n      }\n      \n      //name this output file same as the name of the first file that is \n      //there in the current list of inmem files (this is guaranteed to\n      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n      //created spill). Also we need to create the output file now since\n      //it is not guaranteed that this file will be present after merge\n      //is called (we delete empty files as soon as we see them\n      //in the merge method)\n\n      //figure out the mapId \n      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n      TaskID mapTaskId \u003d mapId.getTaskID();\n\n      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n      long mergeOutputSize \u003d \n        createInMemorySegments(inputs, inMemorySegments,0);\n      int noInMemorySegments \u003d inMemorySegments.size();\n\n      Path outputPath \u003d \n        mapOutputFile.getInputFileForWrite(mapTaskId,\n                                           mergeOutputSize).suffix(\n                                               Task.MERGED_OUTPUT_PREFIX);\n\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n\n      RawKeyValueIterator rIter \u003d null;\n      try {\n        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                 \" segments...\");\n        \n        rIter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                             inMemorySegments, inMemorySegments.size(),\n                             new Path(reduceId.toString()),\n                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                             reporter, spilledRecordsCounter, null, null);\n        \n        if (null \u003d\u003d combinerClass) {\n          Merger.writeFile(rIter, writer, reporter, jobConf);\n        } else {\n          combineCollector.setWriter(writer);\n          combineAndSpill(rIter, reduceCombineInputCounter);\n        }\n        writer.close();\n\n        LOG.info(reduceId +  \n            \" Merge of the \" + noInMemorySegments +\n            \" files in-memory complete.\" +\n            \" Local file is \" + outputPath + \" of size \" + \n            localFS.getFileStatus(outputPath).getLen());\n      } catch (IOException e) { \n        //make sure that we delete the ondisk file that we created \n        //earlier when we invoked cloneFileAttributes\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      // Note the output of the merge\n      closeOnDiskFile(outputPath);\n    }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
        "oldMethodName": "merge",
        "newMethodName": "merge"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,69 @@\n+    public void merge(List\u003cMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n+      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n+        return;\n+      }\n+      \n+      //name this output file same as the name of the first file that is \n+      //there in the current list of inmem files (this is guaranteed to\n+      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n+      //created spill). Also we need to create the output file now since\n+      //it is not guaranteed that this file will be present after merge\n+      //is called (we delete empty files as soon as we see them\n+      //in the merge method)\n+\n+      //figure out the mapId \n+      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n+      TaskID mapTaskId \u003d mapId.getTaskID();\n+\n+      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n+      long mergeOutputSize \u003d \n+        createInMemorySegments(inputs, inMemorySegments,0);\n+      int noInMemorySegments \u003d inMemorySegments.size();\n+\n+      Path outputPath \u003d \n+        mapOutputFile.getInputFileForWrite(mapTaskId,\n+                                           mergeOutputSize).suffix(\n+                                               Task.MERGED_OUTPUT_PREFIX);\n+\n+      Writer\u003cK,V\u003e writer \u003d \n+        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n+                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n+                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n+                        codec, null);\n+\n+      RawKeyValueIterator rIter \u003d null;\n+      try {\n+        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n+                 \" segments...\");\n+        \n+        rIter \u003d Merger.merge(jobConf, rfs,\n+                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n+                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n+                             inMemorySegments, inMemorySegments.size(),\n+                             new Path(reduceId.toString()),\n+                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n+                             reporter, spilledRecordsCounter, null, null);\n+        \n+        if (null \u003d\u003d combinerClass) {\n+          Merger.writeFile(rIter, writer, reporter, jobConf);\n+        } else {\n+          combineCollector.setWriter(writer);\n+          combineAndSpill(rIter, reduceCombineInputCounter);\n+        }\n+        writer.close();\n+\n+        LOG.info(reduceId +  \n+            \" Merge of the \" + noInMemorySegments +\n+            \" files in-memory complete.\" +\n+            \" Local file is \" + outputPath + \" of size \" + \n+            localFS.getFileStatus(outputPath).getLen());\n+      } catch (IOException e) { \n+        //make sure that we delete the ondisk file that we created \n+        //earlier when we invoked cloneFileAttributes\n+        localFS.delete(outputPath, true);\n+        throw e;\n+      }\n+\n+      // Note the output of the merge\n+      closeOnDiskFile(outputPath);\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public void merge(List\u003cMapOutput\u003cK,V\u003e\u003e inputs) throws IOException {\n      if (inputs \u003d\u003d null || inputs.size() \u003d\u003d 0) {\n        return;\n      }\n      \n      //name this output file same as the name of the first file that is \n      //there in the current list of inmem files (this is guaranteed to\n      //be absent on the disk currently. So we don\u0027t overwrite a prev. \n      //created spill). Also we need to create the output file now since\n      //it is not guaranteed that this file will be present after merge\n      //is called (we delete empty files as soon as we see them\n      //in the merge method)\n\n      //figure out the mapId \n      TaskAttemptID mapId \u003d inputs.get(0).getMapId();\n      TaskID mapTaskId \u003d mapId.getTaskID();\n\n      List\u003cSegment\u003cK, V\u003e\u003e inMemorySegments \u003d new ArrayList\u003cSegment\u003cK, V\u003e\u003e();\n      long mergeOutputSize \u003d \n        createInMemorySegments(inputs, inMemorySegments,0);\n      int noInMemorySegments \u003d inMemorySegments.size();\n\n      Path outputPath \u003d \n        mapOutputFile.getInputFileForWrite(mapTaskId,\n                                           mergeOutputSize).suffix(\n                                               Task.MERGED_OUTPUT_PREFIX);\n\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath,\n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n\n      RawKeyValueIterator rIter \u003d null;\n      try {\n        LOG.info(\"Initiating in-memory merge with \" + noInMemorySegments + \n                 \" segments...\");\n        \n        rIter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e)jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e)jobConf.getMapOutputValueClass(),\n                             inMemorySegments, inMemorySegments.size(),\n                             new Path(reduceId.toString()),\n                             (RawComparator\u003cK\u003e)jobConf.getOutputKeyComparator(),\n                             reporter, spilledRecordsCounter, null, null);\n        \n        if (null \u003d\u003d combinerClass) {\n          Merger.writeFile(rIter, writer, reporter, jobConf);\n        } else {\n          combineCollector.setWriter(writer);\n          combineAndSpill(rIter, reduceCombineInputCounter);\n        }\n        writer.close();\n\n        LOG.info(reduceId +  \n            \" Merge of the \" + noInMemorySegments +\n            \" files in-memory complete.\" +\n            \" Local file is \" + outputPath + \" of size \" + \n            localFS.getFileStatus(outputPath).getLen());\n      } catch (IOException e) { \n        //make sure that we delete the ondisk file that we created \n        //earlier when we invoked cloneFileAttributes\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      // Note the output of the merge\n      closeOnDiskFile(outputPath);\n    }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java"
    }
  }
}