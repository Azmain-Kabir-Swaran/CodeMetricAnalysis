{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "scanAndCompactStorages",
  "functionId": "scanAndCompactStorages",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 4979,
  "functionEndLine": 5030,
  "numCommitsSeen": 477,
  "timeTaken": 8898,
  "changeHistory": [
    "30728aced4a6b05394b3fc8c613f39fade9cf3c2",
    "1fbb662c7092d08a540acff7e92715693412e486",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a"
  ],
  "changeHistoryShort": {
    "30728aced4a6b05394b3fc8c613f39fade9cf3c2": "Ybodychange",
    "1fbb662c7092d08a540acff7e92715693412e486": "Ybodychange",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": "Yintroduced"
  },
  "changeHistoryDetails": {
    "30728aced4a6b05394b3fc8c613f39fade9cf3c2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13692. StorageInfoDefragmenter floods log when compacting StorageInfo TreeSet. Contributed by Bharat Viswanadham.\n",
      "commitDate": "21/06/18 7:50 PM",
      "commitName": "30728aced4a6b05394b3fc8c613f39fade9cf3c2",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "05/04/18 9:59 AM",
      "commitNameOld": "d737bf99d44ce34cd01baad716d23df269267c95",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 77.41,
      "commitsBetweenForRepo": 1080,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,52 @@\n     private void scanAndCompactStorages() throws InterruptedException {\n       ArrayList\u003cString\u003e datanodesAndStorages \u003d new ArrayList\u003c\u003e();\n       for (DatanodeDescriptor node\n           : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n         for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n           try {\n             namesystem.readLock();\n             double ratio \u003d storage.treeSetFillRatio();\n             if (ratio \u003c storageInfoDefragmentRatio) {\n               datanodesAndStorages.add(node.getDatanodeUuid());\n               datanodesAndStorages.add(storage.getStorageID());\n             }\n-            LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n+            LOG.debug(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n                      storage.getStorageID(), ratio,\n                      (ratio \u003c storageInfoDefragmentRatio)\n                      ? \" (queued for defragmentation)\" : \"\");\n           } finally {\n             namesystem.readUnlock();\n           }\n         }\n       }\n       if (!datanodesAndStorages.isEmpty()) {\n         for (int i \u003d 0; i \u003c datanodesAndStorages.size(); i +\u003d 2) {\n           namesystem.writeLock();\n           try {\n             final DatanodeDescriptor dn \u003d datanodeManager.\n                 getDatanode(datanodesAndStorages.get(i));\n             if (dn \u003d\u003d null) {\n               continue;\n             }\n             final DatanodeStorageInfo storage \u003d dn.\n                 getStorageInfo(datanodesAndStorages.get(i + 1));\n             if (storage !\u003d null) {\n               boolean aborted \u003d\n                   !storage.treeSetCompact(storageInfoDefragmentTimeout);\n               if (aborted) {\n                 // Compaction timed out, reset iterator to continue with\n                 // the same storage next iteration.\n                 i -\u003d 2;\n               }\n               LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n                        storage.getStorageID(), storage.treeSetFillRatio(),\n                        aborted ? \" (aborted)\" : \"\");\n             }\n           } finally {\n             namesystem.writeUnlock();\n           }\n           // Wait between each iteration\n           Thread.sleep(1000);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void scanAndCompactStorages() throws InterruptedException {\n      ArrayList\u003cString\u003e datanodesAndStorages \u003d new ArrayList\u003c\u003e();\n      for (DatanodeDescriptor node\n          : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n        for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n          try {\n            namesystem.readLock();\n            double ratio \u003d storage.treeSetFillRatio();\n            if (ratio \u003c storageInfoDefragmentRatio) {\n              datanodesAndStorages.add(node.getDatanodeUuid());\n              datanodesAndStorages.add(storage.getStorageID());\n            }\n            LOG.debug(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n                     storage.getStorageID(), ratio,\n                     (ratio \u003c storageInfoDefragmentRatio)\n                     ? \" (queued for defragmentation)\" : \"\");\n          } finally {\n            namesystem.readUnlock();\n          }\n        }\n      }\n      if (!datanodesAndStorages.isEmpty()) {\n        for (int i \u003d 0; i \u003c datanodesAndStorages.size(); i +\u003d 2) {\n          namesystem.writeLock();\n          try {\n            final DatanodeDescriptor dn \u003d datanodeManager.\n                getDatanode(datanodesAndStorages.get(i));\n            if (dn \u003d\u003d null) {\n              continue;\n            }\n            final DatanodeStorageInfo storage \u003d dn.\n                getStorageInfo(datanodesAndStorages.get(i + 1));\n            if (storage !\u003d null) {\n              boolean aborted \u003d\n                  !storage.treeSetCompact(storageInfoDefragmentTimeout);\n              if (aborted) {\n                // Compaction timed out, reset iterator to continue with\n                // the same storage next iteration.\n                i -\u003d 2;\n              }\n              LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n                       storage.getStorageID(), storage.treeSetFillRatio(),\n                       aborted ? \" (aborted)\" : \"\");\n            }\n          } finally {\n            namesystem.writeUnlock();\n          }\n          // Wait between each iteration\n          Thread.sleep(1000);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "1fbb662c7092d08a540acff7e92715693412e486": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12363. Possible NPE in BlockManager$StorageInfoDefragmenter#scanAndCompactStorages. Contributed by Xiao Chen\n",
      "commitDate": "31/08/17 10:36 PM",
      "commitName": "1fbb662c7092d08a540acff7e92715693412e486",
      "commitAuthor": "Mingliang Liu",
      "commitDateOld": "08/08/17 11:44 PM",
      "commitNameOld": "9a3c2379ef24cdca5153abf4b63fde1131ff8989",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 22.95,
      "commitsBetweenForRepo": 198,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,52 @@\n     private void scanAndCompactStorages() throws InterruptedException {\n       ArrayList\u003cString\u003e datanodesAndStorages \u003d new ArrayList\u003c\u003e();\n       for (DatanodeDescriptor node\n           : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n         for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n           try {\n             namesystem.readLock();\n             double ratio \u003d storage.treeSetFillRatio();\n             if (ratio \u003c storageInfoDefragmentRatio) {\n               datanodesAndStorages.add(node.getDatanodeUuid());\n               datanodesAndStorages.add(storage.getStorageID());\n             }\n             LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n                      storage.getStorageID(), ratio,\n                      (ratio \u003c storageInfoDefragmentRatio)\n                      ? \" (queued for defragmentation)\" : \"\");\n           } finally {\n             namesystem.readUnlock();\n           }\n         }\n       }\n       if (!datanodesAndStorages.isEmpty()) {\n         for (int i \u003d 0; i \u003c datanodesAndStorages.size(); i +\u003d 2) {\n           namesystem.writeLock();\n           try {\n-            DatanodeStorageInfo storage \u003d datanodeManager.\n-                getDatanode(datanodesAndStorages.get(i)).\n+            final DatanodeDescriptor dn \u003d datanodeManager.\n+                getDatanode(datanodesAndStorages.get(i));\n+            if (dn \u003d\u003d null) {\n+              continue;\n+            }\n+            final DatanodeStorageInfo storage \u003d dn.\n                 getStorageInfo(datanodesAndStorages.get(i + 1));\n             if (storage !\u003d null) {\n               boolean aborted \u003d\n                   !storage.treeSetCompact(storageInfoDefragmentTimeout);\n               if (aborted) {\n                 // Compaction timed out, reset iterator to continue with\n                 // the same storage next iteration.\n                 i -\u003d 2;\n               }\n               LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n                        storage.getStorageID(), storage.treeSetFillRatio(),\n                        aborted ? \" (aborted)\" : \"\");\n             }\n           } finally {\n             namesystem.writeUnlock();\n           }\n           // Wait between each iteration\n           Thread.sleep(1000);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void scanAndCompactStorages() throws InterruptedException {\n      ArrayList\u003cString\u003e datanodesAndStorages \u003d new ArrayList\u003c\u003e();\n      for (DatanodeDescriptor node\n          : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n        for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n          try {\n            namesystem.readLock();\n            double ratio \u003d storage.treeSetFillRatio();\n            if (ratio \u003c storageInfoDefragmentRatio) {\n              datanodesAndStorages.add(node.getDatanodeUuid());\n              datanodesAndStorages.add(storage.getStorageID());\n            }\n            LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n                     storage.getStorageID(), ratio,\n                     (ratio \u003c storageInfoDefragmentRatio)\n                     ? \" (queued for defragmentation)\" : \"\");\n          } finally {\n            namesystem.readUnlock();\n          }\n        }\n      }\n      if (!datanodesAndStorages.isEmpty()) {\n        for (int i \u003d 0; i \u003c datanodesAndStorages.size(); i +\u003d 2) {\n          namesystem.writeLock();\n          try {\n            final DatanodeDescriptor dn \u003d datanodeManager.\n                getDatanode(datanodesAndStorages.get(i));\n            if (dn \u003d\u003d null) {\n              continue;\n            }\n            final DatanodeStorageInfo storage \u003d dn.\n                getStorageInfo(datanodesAndStorages.get(i + 1));\n            if (storage !\u003d null) {\n              boolean aborted \u003d\n                  !storage.treeSetCompact(storageInfoDefragmentTimeout);\n              if (aborted) {\n                // Compaction timed out, reset iterator to continue with\n                // the same storage next iteration.\n                i -\u003d 2;\n              }\n              LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n                       storage.getStorageID(), storage.treeSetFillRatio(),\n                       aborted ? \" (aborted)\" : \"\");\n            }\n          } finally {\n            namesystem.writeUnlock();\n          }\n          // Wait between each iteration\n          Thread.sleep(1000);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9260. Improve the performance and GC friendliness of NameNode startup and full block reports (Staffan Friberg via cmccabe)\n",
      "commitDate": "02/02/16 11:23 AM",
      "commitName": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
      "commitAuthor": "Colin Patrick Mccabe",
      "diff": "@@ -0,0 +1,48 @@\n+    private void scanAndCompactStorages() throws InterruptedException {\n+      ArrayList\u003cString\u003e datanodesAndStorages \u003d new ArrayList\u003c\u003e();\n+      for (DatanodeDescriptor node\n+          : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n+        for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n+          try {\n+            namesystem.readLock();\n+            double ratio \u003d storage.treeSetFillRatio();\n+            if (ratio \u003c storageInfoDefragmentRatio) {\n+              datanodesAndStorages.add(node.getDatanodeUuid());\n+              datanodesAndStorages.add(storage.getStorageID());\n+            }\n+            LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n+                     storage.getStorageID(), ratio,\n+                     (ratio \u003c storageInfoDefragmentRatio)\n+                     ? \" (queued for defragmentation)\" : \"\");\n+          } finally {\n+            namesystem.readUnlock();\n+          }\n+        }\n+      }\n+      if (!datanodesAndStorages.isEmpty()) {\n+        for (int i \u003d 0; i \u003c datanodesAndStorages.size(); i +\u003d 2) {\n+          namesystem.writeLock();\n+          try {\n+            DatanodeStorageInfo storage \u003d datanodeManager.\n+                getDatanode(datanodesAndStorages.get(i)).\n+                getStorageInfo(datanodesAndStorages.get(i + 1));\n+            if (storage !\u003d null) {\n+              boolean aborted \u003d\n+                  !storage.treeSetCompact(storageInfoDefragmentTimeout);\n+              if (aborted) {\n+                // Compaction timed out, reset iterator to continue with\n+                // the same storage next iteration.\n+                i -\u003d 2;\n+              }\n+              LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n+                       storage.getStorageID(), storage.treeSetFillRatio(),\n+                       aborted ? \" (aborted)\" : \"\");\n+            }\n+          } finally {\n+            namesystem.writeUnlock();\n+          }\n+          // Wait between each iteration\n+          Thread.sleep(1000);\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void scanAndCompactStorages() throws InterruptedException {\n      ArrayList\u003cString\u003e datanodesAndStorages \u003d new ArrayList\u003c\u003e();\n      for (DatanodeDescriptor node\n          : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n        for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n          try {\n            namesystem.readLock();\n            double ratio \u003d storage.treeSetFillRatio();\n            if (ratio \u003c storageInfoDefragmentRatio) {\n              datanodesAndStorages.add(node.getDatanodeUuid());\n              datanodesAndStorages.add(storage.getStorageID());\n            }\n            LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n                     storage.getStorageID(), ratio,\n                     (ratio \u003c storageInfoDefragmentRatio)\n                     ? \" (queued for defragmentation)\" : \"\");\n          } finally {\n            namesystem.readUnlock();\n          }\n        }\n      }\n      if (!datanodesAndStorages.isEmpty()) {\n        for (int i \u003d 0; i \u003c datanodesAndStorages.size(); i +\u003d 2) {\n          namesystem.writeLock();\n          try {\n            DatanodeStorageInfo storage \u003d datanodeManager.\n                getDatanode(datanodesAndStorages.get(i)).\n                getStorageInfo(datanodesAndStorages.get(i + 1));\n            if (storage !\u003d null) {\n              boolean aborted \u003d\n                  !storage.treeSetCompact(storageInfoDefragmentTimeout);\n              if (aborted) {\n                // Compaction timed out, reset iterator to continue with\n                // the same storage next iteration.\n                i -\u003d 2;\n              }\n              LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n                       storage.getStorageID(), storage.treeSetFillRatio(),\n                       aborted ? \" (aborted)\" : \"\");\n            }\n          } finally {\n            namesystem.writeUnlock();\n          }\n          // Wait between each iteration\n          Thread.sleep(1000);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    }
  }
}