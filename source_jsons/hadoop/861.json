{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSOutputStream.java",
  "functionName": "adjustPacketChunkSize",
  "functionId": "adjustPacketChunkSize___stat-HdfsFileStatus",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
  "functionStartLine": 357,
  "functionEndLine": 391,
  "numCommitsSeen": 139,
  "timeTaken": 2656,
  "changeHistory": [
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "efc510a570cf880e7df1b69932aa41932658ee51",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66"
  ],
  "changeHistoryShort": {
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "efc510a570cf880e7df1b69932aa41932658ee51": "Ybodychange",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": "Yintroduced"
  },
  "changeHistoryDetails": {
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void adjustPacketChunkSize(HdfsFileStatus stat) throws IOException{\n\n    long usedInLastBlock \u003d stat.getLen() % blockSize;\n    int freeInLastBlock \u003d (int)(blockSize - usedInLastBlock);\n\n    // calculate the amount of free space in the pre-existing\n    // last crc chunk\n    int usedInCksum \u003d (int)(stat.getLen() % bytesPerChecksum);\n    int freeInCksum \u003d bytesPerChecksum - usedInCksum;\n\n    // if there is space in the last block, then we have to\n    // append to that block\n    if (freeInLastBlock \u003d\u003d blockSize) {\n      throw new IOException(\"The last block for file \" +\n          src + \" is full.\");\n    }\n\n    if (usedInCksum \u003e 0 \u0026\u0026 freeInCksum \u003e 0) {\n      // if there is space in the last partial chunk, then\n      // setup in such a way that the next packet will have only\n      // one chunk that fills up the partial chunk.\n      //\n      computePacketChunkSize(0, freeInCksum);\n      setChecksumBufSize(freeInCksum);\n      getStreamer().setAppendChunk(true);\n    } else {\n      // if the remaining space in the block is smaller than\n      // that expected size of of a packet, then create\n      // smaller size packet.\n      //\n      computePacketChunkSize(\n          Math.min(dfsClient.getConf().getWritePacketSize(), freeInLastBlock),\n          bytesPerChecksum);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "efc510a570cf880e7df1b69932aa41932658ee51": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8386. Improve synchronization of \u0027streamer\u0027 reference in DFSOutputStream. Contributed by Rakesh R.\n",
      "commitDate": "02/06/15 3:39 PM",
      "commitName": "efc510a570cf880e7df1b69932aa41932658ee51",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/04/15 7:27 PM",
      "commitNameOld": "98a61766286321468bf801a9f17a843d7eae8d9e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 32.84,
      "commitsBetweenForRepo": 337,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,35 @@\n   private void adjustPacketChunkSize(HdfsFileStatus stat) throws IOException{\n \n     long usedInLastBlock \u003d stat.getLen() % blockSize;\n     int freeInLastBlock \u003d (int)(blockSize - usedInLastBlock);\n \n     // calculate the amount of free space in the pre-existing\n     // last crc chunk\n     int usedInCksum \u003d (int)(stat.getLen() % bytesPerChecksum);\n     int freeInCksum \u003d bytesPerChecksum - usedInCksum;\n \n     // if there is space in the last block, then we have to\n     // append to that block\n     if (freeInLastBlock \u003d\u003d blockSize) {\n       throw new IOException(\"The last block for file \" +\n           src + \" is full.\");\n     }\n \n     if (usedInCksum \u003e 0 \u0026\u0026 freeInCksum \u003e 0) {\n       // if there is space in the last partial chunk, then\n       // setup in such a way that the next packet will have only\n       // one chunk that fills up the partial chunk.\n       //\n       computePacketChunkSize(0, freeInCksum);\n       setChecksumBufSize(freeInCksum);\n-      streamer.setAppendChunk(true);\n+      getStreamer().setAppendChunk(true);\n     } else {\n       // if the remaining space in the block is smaller than\n       // that expected size of of a packet, then create\n       // smaller size packet.\n       //\n       computePacketChunkSize(\n           Math.min(dfsClient.getConf().getWritePacketSize(), freeInLastBlock),\n           bytesPerChecksum);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void adjustPacketChunkSize(HdfsFileStatus stat) throws IOException{\n\n    long usedInLastBlock \u003d stat.getLen() % blockSize;\n    int freeInLastBlock \u003d (int)(blockSize - usedInLastBlock);\n\n    // calculate the amount of free space in the pre-existing\n    // last crc chunk\n    int usedInCksum \u003d (int)(stat.getLen() % bytesPerChecksum);\n    int freeInCksum \u003d bytesPerChecksum - usedInCksum;\n\n    // if there is space in the last block, then we have to\n    // append to that block\n    if (freeInLastBlock \u003d\u003d blockSize) {\n      throw new IOException(\"The last block for file \" +\n          src + \" is full.\");\n    }\n\n    if (usedInCksum \u003e 0 \u0026\u0026 freeInCksum \u003e 0) {\n      // if there is space in the last partial chunk, then\n      // setup in such a way that the next packet will have only\n      // one chunk that fills up the partial chunk.\n      //\n      computePacketChunkSize(0, freeInCksum);\n      setChecksumBufSize(freeInCksum);\n      getStreamer().setAppendChunk(true);\n    } else {\n      // if the remaining space in the block is smaller than\n      // that expected size of of a packet, then create\n      // smaller size packet.\n      //\n      computePacketChunkSize(\n          Math.min(dfsClient.getConf().getWritePacketSize(), freeInLastBlock),\n          bytesPerChecksum);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "02/04/15 10:59 AM",
      "commitNameOld": "9ed43f2189fb4674b7379e8e995d53d4970d5c3a",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 8.16,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,35 @@\n   private void adjustPacketChunkSize(HdfsFileStatus stat) throws IOException{\n \n     long usedInLastBlock \u003d stat.getLen() % blockSize;\n     int freeInLastBlock \u003d (int)(blockSize - usedInLastBlock);\n \n     // calculate the amount of free space in the pre-existing\n     // last crc chunk\n     int usedInCksum \u003d (int)(stat.getLen() % bytesPerChecksum);\n     int freeInCksum \u003d bytesPerChecksum - usedInCksum;\n \n     // if there is space in the last block, then we have to\n     // append to that block\n     if (freeInLastBlock \u003d\u003d blockSize) {\n       throw new IOException(\"The last block for file \" +\n           src + \" is full.\");\n     }\n \n     if (usedInCksum \u003e 0 \u0026\u0026 freeInCksum \u003e 0) {\n       // if there is space in the last partial chunk, then\n       // setup in such a way that the next packet will have only\n       // one chunk that fills up the partial chunk.\n       //\n       computePacketChunkSize(0, freeInCksum);\n       setChecksumBufSize(freeInCksum);\n       streamer.setAppendChunk(true);\n     } else {\n       // if the remaining space in the block is smaller than\n       // that expected size of of a packet, then create\n       // smaller size packet.\n       //\n-      computePacketChunkSize(Math.min(dfsClient.getConf().writePacketSize, freeInLastBlock),\n+      computePacketChunkSize(\n+          Math.min(dfsClient.getConf().getWritePacketSize(), freeInLastBlock),\n           bytesPerChecksum);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void adjustPacketChunkSize(HdfsFileStatus stat) throws IOException{\n\n    long usedInLastBlock \u003d stat.getLen() % blockSize;\n    int freeInLastBlock \u003d (int)(blockSize - usedInLastBlock);\n\n    // calculate the amount of free space in the pre-existing\n    // last crc chunk\n    int usedInCksum \u003d (int)(stat.getLen() % bytesPerChecksum);\n    int freeInCksum \u003d bytesPerChecksum - usedInCksum;\n\n    // if there is space in the last block, then we have to\n    // append to that block\n    if (freeInLastBlock \u003d\u003d blockSize) {\n      throw new IOException(\"The last block for file \" +\n          src + \" is full.\");\n    }\n\n    if (usedInCksum \u003e 0 \u0026\u0026 freeInCksum \u003e 0) {\n      // if there is space in the last partial chunk, then\n      // setup in such a way that the next packet will have only\n      // one chunk that fills up the partial chunk.\n      //\n      computePacketChunkSize(0, freeInCksum);\n      setChecksumBufSize(freeInCksum);\n      streamer.setAppendChunk(true);\n    } else {\n      // if the remaining space in the block is smaller than\n      // that expected size of of a packet, then create\n      // smaller size packet.\n      //\n      computePacketChunkSize(\n          Math.min(dfsClient.getConf().getWritePacketSize(), freeInLastBlock),\n          bytesPerChecksum);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "24/03/15 11:06 AM",
      "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,34 @@\n+  private void adjustPacketChunkSize(HdfsFileStatus stat) throws IOException{\n+\n+    long usedInLastBlock \u003d stat.getLen() % blockSize;\n+    int freeInLastBlock \u003d (int)(blockSize - usedInLastBlock);\n+\n+    // calculate the amount of free space in the pre-existing\n+    // last crc chunk\n+    int usedInCksum \u003d (int)(stat.getLen() % bytesPerChecksum);\n+    int freeInCksum \u003d bytesPerChecksum - usedInCksum;\n+\n+    // if there is space in the last block, then we have to\n+    // append to that block\n+    if (freeInLastBlock \u003d\u003d blockSize) {\n+      throw new IOException(\"The last block for file \" +\n+          src + \" is full.\");\n+    }\n+\n+    if (usedInCksum \u003e 0 \u0026\u0026 freeInCksum \u003e 0) {\n+      // if there is space in the last partial chunk, then\n+      // setup in such a way that the next packet will have only\n+      // one chunk that fills up the partial chunk.\n+      //\n+      computePacketChunkSize(0, freeInCksum);\n+      setChecksumBufSize(freeInCksum);\n+      streamer.setAppendChunk(true);\n+    } else {\n+      // if the remaining space in the block is smaller than\n+      // that expected size of of a packet, then create\n+      // smaller size packet.\n+      //\n+      computePacketChunkSize(Math.min(dfsClient.getConf().writePacketSize, freeInLastBlock),\n+          bytesPerChecksum);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void adjustPacketChunkSize(HdfsFileStatus stat) throws IOException{\n\n    long usedInLastBlock \u003d stat.getLen() % blockSize;\n    int freeInLastBlock \u003d (int)(blockSize - usedInLastBlock);\n\n    // calculate the amount of free space in the pre-existing\n    // last crc chunk\n    int usedInCksum \u003d (int)(stat.getLen() % bytesPerChecksum);\n    int freeInCksum \u003d bytesPerChecksum - usedInCksum;\n\n    // if there is space in the last block, then we have to\n    // append to that block\n    if (freeInLastBlock \u003d\u003d blockSize) {\n      throw new IOException(\"The last block for file \" +\n          src + \" is full.\");\n    }\n\n    if (usedInCksum \u003e 0 \u0026\u0026 freeInCksum \u003e 0) {\n      // if there is space in the last partial chunk, then\n      // setup in such a way that the next packet will have only\n      // one chunk that fills up the partial chunk.\n      //\n      computePacketChunkSize(0, freeInCksum);\n      setChecksumBufSize(freeInCksum);\n      streamer.setAppendChunk(true);\n    } else {\n      // if the remaining space in the block is smaller than\n      // that expected size of of a packet, then create\n      // smaller size packet.\n      //\n      computePacketChunkSize(Math.min(dfsClient.getConf().writePacketSize, freeInLastBlock),\n          bytesPerChecksum);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}