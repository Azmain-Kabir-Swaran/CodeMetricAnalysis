{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeDescriptor.java",
  "functionName": "updateStorage",
  "functionId": "updateStorage___s-DatanodeStorage",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
  "functionStartLine": 988,
  "functionEndLine": 1030,
  "numCommitsSeen": 118,
  "timeTaken": 3282,
  "changeHistory": [
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f",
    "2e7b7e2cda67eba4c03e0a2c7892d868d235b0cf",
    "ef3c3a832c2f0c1e5ccdda2ff8ef84902912955f",
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b",
    "97acde2d33967f7f870f7dfe96c6b558e6fe324b"
  ],
  "changeHistoryShort": {
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f": "Ybodychange",
    "2e7b7e2cda67eba4c03e0a2c7892d868d235b0cf": "Ybodychange",
    "ef3c3a832c2f0c1e5ccdda2ff8ef84902912955f": "Ybodychange",
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b": "Ybodychange",
    "97acde2d33967f7f870f7dfe96c6b558e6fe324b": "Ymultichange(Ymodifierchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11530. Use HDFS specific network topology to choose datanode in BlockPlacementPolicyDefault. Contributed by Yiqun Lin and Chen Liang.\n",
      "commitDate": "04/05/17 8:54 PM",
      "commitName": "97c2e576c91c2316c2b52bfc948bae9bff8ca49f",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "17/04/17 4:56 PM",
      "commitNameOld": "8dfcd95d580bb090af7f40af0a57061518c18c8c",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 17.17,
      "commitsBetweenForRepo": 103,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,43 @@\n   DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n     synchronized (storageMap) {\n       DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n+      DFSTopologyNodeImpl parent \u003d null;\n+      if (getParent() instanceof DFSTopologyNodeImpl) {\n+        parent \u003d (DFSTopologyNodeImpl) getParent();\n+      }\n+\n       if (storage \u003d\u003d null) {\n         LOG.info(\"Adding new storage ID {} for DN {}\", s.getStorageID(),\n             getXferAddr());\n+        StorageType type \u003d s.getStorageType();\n+        if (!hasStorageType(type) \u0026\u0026 parent !\u003d null) {\n+          // we are about to add a type this node currently does not have,\n+          // inform the parent that a new type is added to this datanode\n+          parent.childAddStorage(getName(), s.getStorageType());\n+        }\n         storage \u003d new DatanodeStorageInfo(this, s);\n         storageMap.put(s.getStorageID(), storage);\n       } else if (storage.getState() !\u003d s.getState() ||\n                  storage.getStorageType() !\u003d s.getStorageType()) {\n         // For backwards compatibility, make sure that the type and\n         // state are updated. Some reports from older datanodes do\n         // not include these fields so we may have assumed defaults.\n+        StorageType oldType \u003d storage.getStorageType();\n+        StorageType newType \u003d s.getStorageType();\n+        if (oldType !\u003d newType \u0026\u0026 !hasStorageType(newType) \u0026\u0026 parent !\u003d null) {\n+          // we are about to add a type this node currently does not have\n+          // inform the parent that a new type is added to this datanode\n+          // if old \u003d\u003d new, nothing\u0027s changed. don\u0027t bother\n+          parent.childAddStorage(getName(), newType);\n+        }\n         storage.updateFromStorage(s);\n         storageMap.put(storage.getStorageID(), storage);\n+        if (oldType !\u003d newType \u0026\u0026 !hasStorageType(oldType) \u0026\u0026 parent !\u003d null) {\n+          // there is no more old type storage on this datanode, inform parent\n+          // about this change.\n+          parent.childRemoveStorage(getName(), oldType);\n+        }\n       }\n       return storage;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n    synchronized (storageMap) {\n      DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n      DFSTopologyNodeImpl parent \u003d null;\n      if (getParent() instanceof DFSTopologyNodeImpl) {\n        parent \u003d (DFSTopologyNodeImpl) getParent();\n      }\n\n      if (storage \u003d\u003d null) {\n        LOG.info(\"Adding new storage ID {} for DN {}\", s.getStorageID(),\n            getXferAddr());\n        StorageType type \u003d s.getStorageType();\n        if (!hasStorageType(type) \u0026\u0026 parent !\u003d null) {\n          // we are about to add a type this node currently does not have,\n          // inform the parent that a new type is added to this datanode\n          parent.childAddStorage(getName(), s.getStorageType());\n        }\n        storage \u003d new DatanodeStorageInfo(this, s);\n        storageMap.put(s.getStorageID(), storage);\n      } else if (storage.getState() !\u003d s.getState() ||\n                 storage.getStorageType() !\u003d s.getStorageType()) {\n        // For backwards compatibility, make sure that the type and\n        // state are updated. Some reports from older datanodes do\n        // not include these fields so we may have assumed defaults.\n        StorageType oldType \u003d storage.getStorageType();\n        StorageType newType \u003d s.getStorageType();\n        if (oldType !\u003d newType \u0026\u0026 !hasStorageType(newType) \u0026\u0026 parent !\u003d null) {\n          // we are about to add a type this node currently does not have\n          // inform the parent that a new type is added to this datanode\n          // if old \u003d\u003d new, nothing\u0027s changed. don\u0027t bother\n          parent.childAddStorage(getName(), newType);\n        }\n        storage.updateFromStorage(s);\n        storageMap.put(storage.getStorageID(), storage);\n        if (oldType !\u003d newType \u0026\u0026 !hasStorageType(oldType) \u0026\u0026 parent !\u003d null) {\n          // there is no more old type storage on this datanode, inform parent\n          // about this change.\n          parent.childRemoveStorage(getName(), oldType);\n        }\n      }\n      return storage;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    },
    "2e7b7e2cda67eba4c03e0a2c7892d868d235b0cf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8713. Convert DatanodeDescriptor to use SLF4J logging.\n",
      "commitDate": "17/08/15 10:17 AM",
      "commitName": "2e7b7e2cda67eba4c03e0a2c7892d868d235b0cf",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "06/08/15 10:21 AM",
      "commitNameOld": "f4c523b69ba55b1fd35e8995c3011a9f546ac835",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 11.0,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n     synchronized (storageMap) {\n       DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n       if (storage \u003d\u003d null) {\n-        LOG.info(\"Adding new storage ID \" + s.getStorageID() +\n-                 \" for DN \" + getXferAddr());\n+        LOG.info(\"Adding new storage ID {} for DN {}\", s.getStorageID(),\n+            getXferAddr());\n         storage \u003d new DatanodeStorageInfo(this, s);\n         storageMap.put(s.getStorageID(), storage);\n       } else if (storage.getState() !\u003d s.getState() ||\n                  storage.getStorageType() !\u003d s.getStorageType()) {\n         // For backwards compatibility, make sure that the type and\n         // state are updated. Some reports from older datanodes do\n         // not include these fields so we may have assumed defaults.\n         storage.updateFromStorage(s);\n         storageMap.put(storage.getStorageID(), storage);\n       }\n       return storage;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n    synchronized (storageMap) {\n      DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n      if (storage \u003d\u003d null) {\n        LOG.info(\"Adding new storage ID {} for DN {}\", s.getStorageID(),\n            getXferAddr());\n        storage \u003d new DatanodeStorageInfo(this, s);\n        storageMap.put(s.getStorageID(), storage);\n      } else if (storage.getState() !\u003d s.getState() ||\n                 storage.getStorageType() !\u003d s.getStorageType()) {\n        // For backwards compatibility, make sure that the type and\n        // state are updated. Some reports from older datanodes do\n        // not include these fields so we may have assumed defaults.\n        storage.updateFromStorage(s);\n        storageMap.put(storage.getStorageID(), storage);\n      }\n      return storage;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    },
    "ef3c3a832c2f0c1e5ccdda2ff8ef84902912955f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7596. NameNode should prune dead storages from storageMap. Contributed by Arpit Agarwal.\n",
      "commitDate": "10/01/15 9:18 AM",
      "commitName": "ef3c3a832c2f0c1e5ccdda2ff8ef84902912955f",
      "commitAuthor": "cnauroth",
      "commitDateOld": "15/10/14 8:44 PM",
      "commitNameOld": "41980c56d3c01d7a0ddc7deea2d89b7f28026722",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 86.57,
      "commitsBetweenForRepo": 625,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,19 @@\n   DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n     synchronized (storageMap) {\n       DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n       if (storage \u003d\u003d null) {\n         LOG.info(\"Adding new storage ID \" + s.getStorageID() +\n                  \" for DN \" + getXferAddr());\n         storage \u003d new DatanodeStorageInfo(this, s);\n         storageMap.put(s.getStorageID(), storage);\n       } else if (storage.getState() !\u003d s.getState() ||\n                  storage.getStorageType() !\u003d s.getStorageType()) {\n         // For backwards compatibility, make sure that the type and\n         // state are updated. Some reports from older datanodes do\n         // not include these fields so we may have assumed defaults.\n-        // This check can be removed in the next major release after\n-        // 2.4.\n         storage.updateFromStorage(s);\n         storageMap.put(storage.getStorageID(), storage);\n       }\n       return storage;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n    synchronized (storageMap) {\n      DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n      if (storage \u003d\u003d null) {\n        LOG.info(\"Adding new storage ID \" + s.getStorageID() +\n                 \" for DN \" + getXferAddr());\n        storage \u003d new DatanodeStorageInfo(this, s);\n        storageMap.put(s.getStorageID(), storage);\n      } else if (storage.getState() !\u003d s.getState() ||\n                 storage.getStorageType() !\u003d s.getStorageType()) {\n        // For backwards compatibility, make sure that the type and\n        // state are updated. Some reports from older datanodes do\n        // not include these fields so we may have assumed defaults.\n        storage.updateFromStorage(s);\n        storageMap.put(storage.getStorageID(), storage);\n      }\n      return storage;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    },
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6094. The same block can be counted twice towards safe mode threshold. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1578478 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/03/14 10:37 AM",
      "commitName": "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "31/01/14 1:00 PM",
      "commitNameOld": "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 44.86,
      "commitsBetweenForRepo": 401,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,21 @@\n   DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n     synchronized (storageMap) {\n       DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n       if (storage \u003d\u003d null) {\n         LOG.info(\"Adding new storage ID \" + s.getStorageID() +\n                  \" for DN \" + getXferAddr());\n         storage \u003d new DatanodeStorageInfo(this, s);\n         storageMap.put(s.getStorageID(), storage);\n+      } else if (storage.getState() !\u003d s.getState() ||\n+                 storage.getStorageType() !\u003d s.getStorageType()) {\n+        // For backwards compatibility, make sure that the type and\n+        // state are updated. Some reports from older datanodes do\n+        // not include these fields so we may have assumed defaults.\n+        // This check can be removed in the next major release after\n+        // 2.4.\n+        storage.updateFromStorage(s);\n+        storageMap.put(storage.getStorageID(), storage);\n       }\n       return storage;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n    synchronized (storageMap) {\n      DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n      if (storage \u003d\u003d null) {\n        LOG.info(\"Adding new storage ID \" + s.getStorageID() +\n                 \" for DN \" + getXferAddr());\n        storage \u003d new DatanodeStorageInfo(this, s);\n        storageMap.put(s.getStorageID(), storage);\n      } else if (storage.getState() !\u003d s.getState() ||\n                 storage.getStorageType() !\u003d s.getStorageType()) {\n        // For backwards compatibility, make sure that the type and\n        // state are updated. Some reports from older datanodes do\n        // not include these fields so we may have assumed defaults.\n        // This check can be removed in the next major release after\n        // 2.4.\n        storage.updateFromStorage(s);\n        storageMap.put(storage.getStorageID(), storage);\n      }\n      return storage;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    },
    "97acde2d33967f7f870f7dfe96c6b558e6fe324b": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-5542. Fix TODO and clean up the code in HDFS-2832. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1544664 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/11/13 12:07 PM",
      "commitName": "97acde2d33967f7f870f7dfe96c6b558e6fe324b",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5542. Fix TODO and clean up the code in HDFS-2832. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1544664 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/11/13 12:07 PM",
          "commitName": "97acde2d33967f7f870f7dfe96c6b558e6fe324b",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "14/11/13 3:56 PM",
          "commitNameOld": "132a8ff7c710dfffd4ea18a3f365726784050ade",
          "commitAuthorOld": "",
          "daysBetweenCommits": 7.84,
          "commitsBetweenForRepo": 49,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,12 @@\n-  public DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n+  DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n     synchronized (storageMap) {\n       DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n       if (storage \u003d\u003d null) {\n         LOG.info(\"Adding new storage ID \" + s.getStorageID() +\n                  \" for DN \" + getXferAddr());\n         storage \u003d new DatanodeStorageInfo(this, s);\n         storageMap.put(s.getStorageID(), storage);\n-      } else {\n-        storage.setState(s.getState());\n       }\n       return storage;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n    synchronized (storageMap) {\n      DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n      if (storage \u003d\u003d null) {\n        LOG.info(\"Adding new storage ID \" + s.getStorageID() +\n                 \" for DN \" + getXferAddr());\n        storage \u003d new DatanodeStorageInfo(this, s);\n        storageMap.put(s.getStorageID(), storage);\n      }\n      return storage;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5542. Fix TODO and clean up the code in HDFS-2832. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1544664 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/11/13 12:07 PM",
          "commitName": "97acde2d33967f7f870f7dfe96c6b558e6fe324b",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "14/11/13 3:56 PM",
          "commitNameOld": "132a8ff7c710dfffd4ea18a3f365726784050ade",
          "commitAuthorOld": "",
          "daysBetweenCommits": 7.84,
          "commitsBetweenForRepo": 49,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,12 @@\n-  public DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n+  DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n     synchronized (storageMap) {\n       DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n       if (storage \u003d\u003d null) {\n         LOG.info(\"Adding new storage ID \" + s.getStorageID() +\n                  \" for DN \" + getXferAddr());\n         storage \u003d new DatanodeStorageInfo(this, s);\n         storageMap.put(s.getStorageID(), storage);\n-      } else {\n-        storage.setState(s.getState());\n       }\n       return storage;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeStorageInfo updateStorage(DatanodeStorage s) {\n    synchronized (storageMap) {\n      DatanodeStorageInfo storage \u003d storageMap.get(s.getStorageID());\n      if (storage \u003d\u003d null) {\n        LOG.info(\"Adding new storage ID \" + s.getStorageID() +\n                 \" for DN \" + getXferAddr());\n        storage \u003d new DatanodeStorageInfo(this, s);\n        storageMap.put(s.getStorageID(), storage);\n      }\n      return storage;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}