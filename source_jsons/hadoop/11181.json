{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WebHdfsHandler.java",
  "functionName": "onCreate",
  "functionId": "onCreate___ctx-ChannelHandlerContext",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
  "functionStartLine": 192,
  "functionEndLine": 231,
  "numCommitsSeen": 19,
  "timeTaken": 2525,
  "changeHistory": [
    "f0d5382ff3e31a47d13e4cb6c3a244cca82b17ce",
    "bf74dbf80dc9379d669779a598950908adffb8a7",
    "4b0f55b6ea1665e2118fd573f72a6fcd1fce20d6",
    "30e342a5d32be5efffeb472cce76d4ed43642608",
    "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4"
  ],
  "changeHistoryShort": {
    "f0d5382ff3e31a47d13e4cb6c3a244cca82b17ce": "Ybodychange",
    "bf74dbf80dc9379d669779a598950908adffb8a7": "Ybodychange",
    "4b0f55b6ea1665e2118fd573f72a6fcd1fce20d6": "Ybodychange",
    "30e342a5d32be5efffeb472cce76d4ed43642608": "Ybodychange",
    "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f0d5382ff3e31a47d13e4cb6c3a244cca82b17ce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6962. ACL inheritance conflicts with umaskmode. Contributed by Chris Nauroth.\n",
      "commitDate": "06/09/16 11:02 AM",
      "commitName": "f0d5382ff3e31a47d13e4cb6c3a244cca82b17ce",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "02/08/16 1:07 AM",
      "commitNameOld": "a5fb298e56220a35d61b8d2bda716d8fb8ef8bb7",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 35.41,
      "commitsBetweenForRepo": 241,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,40 @@\n   private void onCreate(ChannelHandlerContext ctx)\n     throws IOException, URISyntaxException {\n     writeContinueHeader(ctx);\n \n     final String nnId \u003d params.namenodeId();\n     final int bufferSize \u003d params.bufferSize();\n     final short replication \u003d params.replication();\n     final long blockSize \u003d params.blockSize();\n-    final FsPermission permission \u003d params.permission();\n+    final FsPermission unmaskedPermission \u003d params.unmaskedPermission();\n+    final FsPermission permission \u003d unmaskedPermission \u003d\u003d null ?\n+        params.permission() :\n+        FsCreateModes.create(params.permission(), unmaskedPermission);\n     final boolean createParent \u003d params.createParent();\n \n     EnumSet\u003cCreateFlag\u003e flags \u003d params.createFlag();\n     if (flags.equals(EMPTY_CREATE_FLAG)) {\n       flags \u003d params.overwrite() ?\n           EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n           : EnumSet.of(CreateFlag.CREATE);\n     } else {\n       if(params.overwrite()) {\n         flags.add(CreateFlag.OVERWRITE);\n       }\n     }\n \n     final DFSClient dfsClient \u003d newDfsClient(nnId, confForCreate);\n     OutputStream out \u003d dfsClient.createWrappedOutputStream(dfsClient.create(\n         path, permission, flags, createParent, replication, blockSize, null,\n         bufferSize, null), null);\n \n     resp \u003d new DefaultHttpResponse(HTTP_1_1, CREATED);\n \n     final URI uri \u003d new URI(HDFS_URI_SCHEME, nnId, path, null, null);\n     resp.headers().set(LOCATION, uri.toString());\n     resp.headers().set(CONTENT_LENGTH, 0);\n     resp.headers().set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n \n     ctx.pipeline().replace(this, HdfsWriter.class.getSimpleName(),\n       new HdfsWriter(dfsClient, out, resp));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void onCreate(ChannelHandlerContext ctx)\n    throws IOException, URISyntaxException {\n    writeContinueHeader(ctx);\n\n    final String nnId \u003d params.namenodeId();\n    final int bufferSize \u003d params.bufferSize();\n    final short replication \u003d params.replication();\n    final long blockSize \u003d params.blockSize();\n    final FsPermission unmaskedPermission \u003d params.unmaskedPermission();\n    final FsPermission permission \u003d unmaskedPermission \u003d\u003d null ?\n        params.permission() :\n        FsCreateModes.create(params.permission(), unmaskedPermission);\n    final boolean createParent \u003d params.createParent();\n\n    EnumSet\u003cCreateFlag\u003e flags \u003d params.createFlag();\n    if (flags.equals(EMPTY_CREATE_FLAG)) {\n      flags \u003d params.overwrite() ?\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n          : EnumSet.of(CreateFlag.CREATE);\n    } else {\n      if(params.overwrite()) {\n        flags.add(CreateFlag.OVERWRITE);\n      }\n    }\n\n    final DFSClient dfsClient \u003d newDfsClient(nnId, confForCreate);\n    OutputStream out \u003d dfsClient.createWrappedOutputStream(dfsClient.create(\n        path, permission, flags, createParent, replication, blockSize, null,\n        bufferSize, null), null);\n\n    resp \u003d new DefaultHttpResponse(HTTP_1_1, CREATED);\n\n    final URI uri \u003d new URI(HDFS_URI_SCHEME, nnId, path, null, null);\n    resp.headers().set(LOCATION, uri.toString());\n    resp.headers().set(CONTENT_LENGTH, 0);\n    resp.headers().set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n\n    ctx.pipeline().replace(this, HdfsWriter.class.getSimpleName(),\n      new HdfsWriter(dfsClient, out, resp));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
      "extendedDetails": {}
    },
    "bf74dbf80dc9379d669779a598950908adffb8a7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7959. WebHdfs logging is missing on Datanode (Kihwal Lee via sjlee)\n",
      "commitDate": "24/06/16 2:44 PM",
      "commitName": "bf74dbf80dc9379d669779a598950908adffb8a7",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "23/05/16 3:52 PM",
      "commitNameOld": "4b0f55b6ea1665e2118fd573f72a6fcd1fce20d6",
      "commitAuthorOld": "Allen Wittenauer",
      "daysBetweenCommits": 31.95,
      "commitsBetweenForRepo": 255,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n   private void onCreate(ChannelHandlerContext ctx)\n     throws IOException, URISyntaxException {\n     writeContinueHeader(ctx);\n \n     final String nnId \u003d params.namenodeId();\n     final int bufferSize \u003d params.bufferSize();\n     final short replication \u003d params.replication();\n     final long blockSize \u003d params.blockSize();\n     final FsPermission permission \u003d params.permission();\n     final boolean createParent \u003d params.createParent();\n \n     EnumSet\u003cCreateFlag\u003e flags \u003d params.createFlag();\n     if (flags.equals(EMPTY_CREATE_FLAG)) {\n       flags \u003d params.overwrite() ?\n           EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n           : EnumSet.of(CreateFlag.CREATE);\n     } else {\n       if(params.overwrite()) {\n         flags.add(CreateFlag.OVERWRITE);\n       }\n     }\n \n     final DFSClient dfsClient \u003d newDfsClient(nnId, confForCreate);\n     OutputStream out \u003d dfsClient.createWrappedOutputStream(dfsClient.create(\n         path, permission, flags, createParent, replication, blockSize, null,\n         bufferSize, null), null);\n \n-    DefaultHttpResponse resp \u003d new DefaultHttpResponse(HTTP_1_1, CREATED);\n+    resp \u003d new DefaultHttpResponse(HTTP_1_1, CREATED);\n \n     final URI uri \u003d new URI(HDFS_URI_SCHEME, nnId, path, null, null);\n     resp.headers().set(LOCATION, uri.toString());\n     resp.headers().set(CONTENT_LENGTH, 0);\n     resp.headers().set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n \n     ctx.pipeline().replace(this, HdfsWriter.class.getSimpleName(),\n       new HdfsWriter(dfsClient, out, resp));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void onCreate(ChannelHandlerContext ctx)\n    throws IOException, URISyntaxException {\n    writeContinueHeader(ctx);\n\n    final String nnId \u003d params.namenodeId();\n    final int bufferSize \u003d params.bufferSize();\n    final short replication \u003d params.replication();\n    final long blockSize \u003d params.blockSize();\n    final FsPermission permission \u003d params.permission();\n    final boolean createParent \u003d params.createParent();\n\n    EnumSet\u003cCreateFlag\u003e flags \u003d params.createFlag();\n    if (flags.equals(EMPTY_CREATE_FLAG)) {\n      flags \u003d params.overwrite() ?\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n          : EnumSet.of(CreateFlag.CREATE);\n    } else {\n      if(params.overwrite()) {\n        flags.add(CreateFlag.OVERWRITE);\n      }\n    }\n\n    final DFSClient dfsClient \u003d newDfsClient(nnId, confForCreate);\n    OutputStream out \u003d dfsClient.createWrappedOutputStream(dfsClient.create(\n        path, permission, flags, createParent, replication, blockSize, null,\n        bufferSize, null), null);\n\n    resp \u003d new DefaultHttpResponse(HTTP_1_1, CREATED);\n\n    final URI uri \u003d new URI(HDFS_URI_SCHEME, nnId, path, null, null);\n    resp.headers().set(LOCATION, uri.toString());\n    resp.headers().set(CONTENT_LENGTH, 0);\n    resp.headers().set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n\n    ctx.pipeline().replace(this, HdfsWriter.class.getSimpleName(),\n      new HdfsWriter(dfsClient, out, resp));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
      "extendedDetails": {}
    },
    "4b0f55b6ea1665e2118fd573f72a6fcd1fce20d6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7766. Add a flag to WebHDFS op\u003dCREATE to not respond with a 307 redirect (Ravi Prakash via aw)\n",
      "commitDate": "23/05/16 3:52 PM",
      "commitName": "4b0f55b6ea1665e2118fd573f72a6fcd1fce20d6",
      "commitAuthor": "Allen Wittenauer",
      "commitDateOld": "04/11/15 10:21 AM",
      "commitNameOld": "88beb46cf6e6fd3e51f73a411a2750de7595e326",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 201.19,
      "commitsBetweenForRepo": 1265,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,37 @@\n   private void onCreate(ChannelHandlerContext ctx)\n     throws IOException, URISyntaxException {\n     writeContinueHeader(ctx);\n \n     final String nnId \u003d params.namenodeId();\n     final int bufferSize \u003d params.bufferSize();\n     final short replication \u003d params.replication();\n     final long blockSize \u003d params.blockSize();\n     final FsPermission permission \u003d params.permission();\n     final boolean createParent \u003d params.createParent();\n \n     EnumSet\u003cCreateFlag\u003e flags \u003d params.createFlag();\n     if (flags.equals(EMPTY_CREATE_FLAG)) {\n       flags \u003d params.overwrite() ?\n           EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n           : EnumSet.of(CreateFlag.CREATE);\n     } else {\n       if(params.overwrite()) {\n         flags.add(CreateFlag.OVERWRITE);\n       }\n     }\n \n     final DFSClient dfsClient \u003d newDfsClient(nnId, confForCreate);\n     OutputStream out \u003d dfsClient.createWrappedOutputStream(dfsClient.create(\n         path, permission, flags, createParent, replication, blockSize, null,\n         bufferSize, null), null);\n \n     DefaultHttpResponse resp \u003d new DefaultHttpResponse(HTTP_1_1, CREATED);\n \n     final URI uri \u003d new URI(HDFS_URI_SCHEME, nnId, path, null, null);\n     resp.headers().set(LOCATION, uri.toString());\n     resp.headers().set(CONTENT_LENGTH, 0);\n+    resp.headers().set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n+\n     ctx.pipeline().replace(this, HdfsWriter.class.getSimpleName(),\n       new HdfsWriter(dfsClient, out, resp));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void onCreate(ChannelHandlerContext ctx)\n    throws IOException, URISyntaxException {\n    writeContinueHeader(ctx);\n\n    final String nnId \u003d params.namenodeId();\n    final int bufferSize \u003d params.bufferSize();\n    final short replication \u003d params.replication();\n    final long blockSize \u003d params.blockSize();\n    final FsPermission permission \u003d params.permission();\n    final boolean createParent \u003d params.createParent();\n\n    EnumSet\u003cCreateFlag\u003e flags \u003d params.createFlag();\n    if (flags.equals(EMPTY_CREATE_FLAG)) {\n      flags \u003d params.overwrite() ?\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n          : EnumSet.of(CreateFlag.CREATE);\n    } else {\n      if(params.overwrite()) {\n        flags.add(CreateFlag.OVERWRITE);\n      }\n    }\n\n    final DFSClient dfsClient \u003d newDfsClient(nnId, confForCreate);\n    OutputStream out \u003d dfsClient.createWrappedOutputStream(dfsClient.create(\n        path, permission, flags, createParent, replication, blockSize, null,\n        bufferSize, null), null);\n\n    DefaultHttpResponse resp \u003d new DefaultHttpResponse(HTTP_1_1, CREATED);\n\n    final URI uri \u003d new URI(HDFS_URI_SCHEME, nnId, path, null, null);\n    resp.headers().set(LOCATION, uri.toString());\n    resp.headers().set(CONTENT_LENGTH, 0);\n    resp.headers().set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n\n    ctx.pipeline().replace(this, HdfsWriter.class.getSimpleName(),\n      new HdfsWriter(dfsClient, out, resp));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
      "extendedDetails": {}
    },
    "30e342a5d32be5efffeb472cce76d4ed43642608": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8435. Support CreateFlag in WebHDFS. Contributed by Jakob Homan\n",
      "commitDate": "18/08/15 5:32 PM",
      "commitName": "30e342a5d32be5efffeb472cce76d4ed43642608",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "24/05/15 10:30 PM",
      "commitNameOld": "ada233b7cd7db39e609bb57e487fee8cec59cd48",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 85.79,
      "commitsBetweenForRepo": 537,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,35 @@\n   private void onCreate(ChannelHandlerContext ctx)\n     throws IOException, URISyntaxException {\n     writeContinueHeader(ctx);\n \n     final String nnId \u003d params.namenodeId();\n     final int bufferSize \u003d params.bufferSize();\n     final short replication \u003d params.replication();\n     final long blockSize \u003d params.blockSize();\n     final FsPermission permission \u003d params.permission();\n+    final boolean createParent \u003d params.createParent();\n \n-    EnumSet\u003cCreateFlag\u003e flags \u003d params.overwrite() ?\n-      EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n-        : EnumSet.of(CreateFlag.CREATE);\n+    EnumSet\u003cCreateFlag\u003e flags \u003d params.createFlag();\n+    if (flags.equals(EMPTY_CREATE_FLAG)) {\n+      flags \u003d params.overwrite() ?\n+          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n+          : EnumSet.of(CreateFlag.CREATE);\n+    } else {\n+      if(params.overwrite()) {\n+        flags.add(CreateFlag.OVERWRITE);\n+      }\n+    }\n \n     final DFSClient dfsClient \u003d newDfsClient(nnId, confForCreate);\n     OutputStream out \u003d dfsClient.createWrappedOutputStream(dfsClient.create(\n-      path, permission, flags, replication,\n-      blockSize, null, bufferSize, null), null);\n+        path, permission, flags, createParent, replication, blockSize, null,\n+        bufferSize, null), null);\n+\n     DefaultHttpResponse resp \u003d new DefaultHttpResponse(HTTP_1_1, CREATED);\n \n     final URI uri \u003d new URI(HDFS_URI_SCHEME, nnId, path, null, null);\n     resp.headers().set(LOCATION, uri.toString());\n     resp.headers().set(CONTENT_LENGTH, 0);\n     ctx.pipeline().replace(this, HdfsWriter.class.getSimpleName(),\n       new HdfsWriter(dfsClient, out, resp));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void onCreate(ChannelHandlerContext ctx)\n    throws IOException, URISyntaxException {\n    writeContinueHeader(ctx);\n\n    final String nnId \u003d params.namenodeId();\n    final int bufferSize \u003d params.bufferSize();\n    final short replication \u003d params.replication();\n    final long blockSize \u003d params.blockSize();\n    final FsPermission permission \u003d params.permission();\n    final boolean createParent \u003d params.createParent();\n\n    EnumSet\u003cCreateFlag\u003e flags \u003d params.createFlag();\n    if (flags.equals(EMPTY_CREATE_FLAG)) {\n      flags \u003d params.overwrite() ?\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n          : EnumSet.of(CreateFlag.CREATE);\n    } else {\n      if(params.overwrite()) {\n        flags.add(CreateFlag.OVERWRITE);\n      }\n    }\n\n    final DFSClient dfsClient \u003d newDfsClient(nnId, confForCreate);\n    OutputStream out \u003d dfsClient.createWrappedOutputStream(dfsClient.create(\n        path, permission, flags, createParent, replication, blockSize, null,\n        bufferSize, null), null);\n\n    DefaultHttpResponse resp \u003d new DefaultHttpResponse(HTTP_1_1, CREATED);\n\n    final URI uri \u003d new URI(HDFS_URI_SCHEME, nnId, path, null, null);\n    resp.headers().set(LOCATION, uri.toString());\n    resp.headers().set(CONTENT_LENGTH, 0);\n    ctx.pipeline().replace(this, HdfsWriter.class.getSimpleName(),\n      new HdfsWriter(dfsClient, out, resp));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
      "extendedDetails": {}
    },
    "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7279. Use netty to implement DatanodeWebHdfsMethods. Contributed by Haohui Mai.\n",
      "commitDate": "17/11/14 11:42 AM",
      "commitName": "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4",
      "commitAuthor": "Haohui Mai",
      "diff": "@@ -0,0 +1,26 @@\n+  private void onCreate(ChannelHandlerContext ctx)\n+    throws IOException, URISyntaxException {\n+    writeContinueHeader(ctx);\n+\n+    final String nnId \u003d params.namenodeId();\n+    final int bufferSize \u003d params.bufferSize();\n+    final short replication \u003d params.replication();\n+    final long blockSize \u003d params.blockSize();\n+    final FsPermission permission \u003d params.permission();\n+\n+    EnumSet\u003cCreateFlag\u003e flags \u003d params.overwrite() ?\n+      EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n+        : EnumSet.of(CreateFlag.CREATE);\n+\n+    final DFSClient dfsClient \u003d newDfsClient(nnId, confForCreate);\n+    OutputStream out \u003d dfsClient.createWrappedOutputStream(dfsClient.create(\n+      path, permission, flags, replication,\n+      blockSize, null, bufferSize, null), null);\n+    DefaultHttpResponse resp \u003d new DefaultHttpResponse(HTTP_1_1, CREATED);\n+\n+    final URI uri \u003d new URI(HDFS_URI_SCHEME, nnId, path, null, null);\n+    resp.headers().set(LOCATION, uri.toString());\n+    resp.headers().set(CONTENT_LENGTH, 0);\n+    ctx.pipeline().replace(this, HdfsWriter.class.getSimpleName(),\n+      new HdfsWriter(dfsClient, out, resp));\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void onCreate(ChannelHandlerContext ctx)\n    throws IOException, URISyntaxException {\n    writeContinueHeader(ctx);\n\n    final String nnId \u003d params.namenodeId();\n    final int bufferSize \u003d params.bufferSize();\n    final short replication \u003d params.replication();\n    final long blockSize \u003d params.blockSize();\n    final FsPermission permission \u003d params.permission();\n\n    EnumSet\u003cCreateFlag\u003e flags \u003d params.overwrite() ?\n      EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n        : EnumSet.of(CreateFlag.CREATE);\n\n    final DFSClient dfsClient \u003d newDfsClient(nnId, confForCreate);\n    OutputStream out \u003d dfsClient.createWrappedOutputStream(dfsClient.create(\n      path, permission, flags, replication,\n      blockSize, null, bufferSize, null), null);\n    DefaultHttpResponse resp \u003d new DefaultHttpResponse(HTTP_1_1, CREATED);\n\n    final URI uri \u003d new URI(HDFS_URI_SCHEME, nnId, path, null, null);\n    resp.headers().set(LOCATION, uri.toString());\n    resp.headers().set(CONTENT_LENGTH, 0);\n    ctx.pipeline().replace(this, HdfsWriter.class.getSimpleName(),\n      new HdfsWriter(dfsClient, out, resp));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java"
    }
  }
}