{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DynamoDBMetadataStore.java",
  "functionName": "prune",
  "functionId": "prune___pruneMode-PruneMode__cutoff-long__keyPrefix-String",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
  "functionStartLine": 1585,
  "functionEndLine": 1594,
  "numCommitsSeen": 73,
  "timeTaken": 6365,
  "changeHistory": [
    "49df83899543586bbcaf80f01399ade031cf68b0",
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
    "e02eb24e0a9139418120027b694492e0738df20a",
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
    "d7c0a08a1c077752918a8cf1b4f1900ce2721899",
    "d7232857d8d1e10cdac171acdc931187e45fd6be",
    "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a",
    "621b43e254afaff708cd6fc4698b29628f6abc33"
  ],
  "changeHistoryShort": {
    "49df83899543586bbcaf80f01399ade031cf68b0": "Ymultichange(Yreturntypechange,Ybodychange)",
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0": "Ybodychange",
    "e02eb24e0a9139418120027b694492e0738df20a": "Ybodychange",
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c": "Ymultichange(Yparameterchange,Ybodychange)",
    "d7c0a08a1c077752918a8cf1b4f1900ce2721899": "Ybodychange",
    "d7232857d8d1e10cdac171acdc931187e45fd6be": "Ybodychange",
    "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a": "Ymultichange(Yparameterchange,Ybodychange)",
    "621b43e254afaff708cd6fc4698b29628f6abc33": "Yintroduced"
  },
  "changeHistoryDetails": {
    "49df83899543586bbcaf80f01399ade031cf68b0": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
      "commitDate": "10/01/20 3:11 AM",
      "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
          "commitDate": "10/01/20 3:11 AM",
          "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "26/11/19 7:36 AM",
          "commitNameOld": "ea25f4de236611d388e14a710ebe5d6872c421b6",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 44.82,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,10 +1,10 @@\n-  public void prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n+  public long prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n       throws IOException {\n     LOG.debug(\"Prune {} under {} with age {}\",\n         pruneMode \u003d\u003d PruneMode.ALL_BY_MODTIME\n             ? \"files and tombstones\" : \"tombstones\",\n         keyPrefix, cutoff);\n     final ItemCollection\u003cScanOutcome\u003e items \u003d\n         expiredFiles(pruneMode, cutoff, keyPrefix);\n-    innerPrune(keyPrefix, items);\n+    return innerPrune(pruneMode, cutoff, keyPrefix, items);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n      throws IOException {\n    LOG.debug(\"Prune {} under {} with age {}\",\n        pruneMode \u003d\u003d PruneMode.ALL_BY_MODTIME\n            ? \"files and tombstones\" : \"tombstones\",\n        keyPrefix, cutoff);\n    final ItemCollection\u003cScanOutcome\u003e items \u003d\n        expiredFiles(pruneMode, cutoff, keyPrefix);\n    return innerPrune(pruneMode, cutoff, keyPrefix, items);\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "long"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
          "commitDate": "10/01/20 3:11 AM",
          "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "26/11/19 7:36 AM",
          "commitNameOld": "ea25f4de236611d388e14a710ebe5d6872c421b6",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 44.82,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,10 +1,10 @@\n-  public void prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n+  public long prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n       throws IOException {\n     LOG.debug(\"Prune {} under {} with age {}\",\n         pruneMode \u003d\u003d PruneMode.ALL_BY_MODTIME\n             ? \"files and tombstones\" : \"tombstones\",\n         keyPrefix, cutoff);\n     final ItemCollection\u003cScanOutcome\u003e items \u003d\n         expiredFiles(pruneMode, cutoff, keyPrefix);\n-    innerPrune(keyPrefix, items);\n+    return innerPrune(pruneMode, cutoff, keyPrefix, items);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n      throws IOException {\n    LOG.debug(\"Prune {} under {} with age {}\",\n        pruneMode \u003d\u003d PruneMode.ALL_BY_MODTIME\n            ? \"files and tombstones\" : \"tombstones\",\n        keyPrefix, cutoff);\n    final ItemCollection\u003cScanOutcome\u003e items \u003d\n        expiredFiles(pruneMode, cutoff, keyPrefix);\n    return innerPrune(pruneMode, cutoff, keyPrefix, items);\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16384: S3A: Avoid inconsistencies between DDB and S3.\n\nContributed by Steve Loughran\n\nContains\n\n- HADOOP-16397. Hadoop S3Guard Prune command to support a -tombstone option.\n- HADOOP-16406. ITestDynamoDBMetadataStore.testProvisionTable times out intermittently\n\nThis patch doesn\u0027t fix the underlying problem but it\n\n* changes some tests to clean up better\n* does a lot more in logging operations in against DDB, if enabled\n* adds an entry point to dump the state of the metastore and s3 tables (precursor to fsck)\n* adds a purge entry point to help clean up after a test run has got a store into a mess\n* s3guard prune command adds -tombstone option to only clear tombstones\n\nThe outcome is that tests should pass consistently and if problems occur we have better diagnostics.\n\nChange-Id: I3eca3f5529d7f6fec398c0ff0472919f08f054eb\n",
      "commitDate": "12/07/19 5:02 AM",
      "commitName": "b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "08/07/19 10:27 AM",
      "commitNameOld": "de6b7bc67ace7744adb0320ee7de79cf28259d2d",
      "commitAuthorOld": "Sean Mackrory",
      "daysBetweenCommits": 3.77,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,7 +1,10 @@\n   public void prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n       throws IOException {\n-    LOG.debug(\"Prune files under {} with age {}\", keyPrefix, cutoff);\n+    LOG.debug(\"Prune {} under {} with age {}\",\n+        pruneMode \u003d\u003d PruneMode.ALL_BY_MODTIME\n+            ? \"files and tombstones\" : \"tombstones\",\n+        keyPrefix, cutoff);\n     final ItemCollection\u003cScanOutcome\u003e items \u003d\n         expiredFiles(pruneMode, cutoff, keyPrefix);\n     innerPrune(keyPrefix, items);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n      throws IOException {\n    LOG.debug(\"Prune {} under {} with age {}\",\n        pruneMode \u003d\u003d PruneMode.ALL_BY_MODTIME\n            ? \"files and tombstones\" : \"tombstones\",\n        keyPrefix, cutoff);\n    final ItemCollection\u003cScanOutcome\u003e items \u003d\n        expiredFiles(pruneMode, cutoff, keyPrefix);\n    innerPrune(keyPrefix, items);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
      "extendedDetails": {}
    },
    "e02eb24e0a9139418120027b694492e0738df20a": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
      "commitDate": "20/06/19 1:56 AM",
      "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "16/06/19 9:05 AM",
      "commitNameOld": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
      "commitAuthorOld": "Gabor Bota",
      "daysBetweenCommits": 3.7,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,6 +1,7 @@\n   public void prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n       throws IOException {\n+    LOG.debug(\"Prune files under {} with age {}\", keyPrefix, cutoff);\n     final ItemCollection\u003cScanOutcome\u003e items \u003d\n         expiredFiles(pruneMode, cutoff, keyPrefix);\n-    innerPrune(items);\n+    innerPrune(keyPrefix, items);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n      throws IOException {\n    LOG.debug(\"Prune files under {} with age {}\", keyPrefix, cutoff);\n    final ItemCollection\u003cScanOutcome\u003e items \u003d\n        expiredFiles(pruneMode, cutoff, keyPrefix);\n    innerPrune(keyPrefix, items);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
      "extendedDetails": {}
    },
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-16279. S3Guard: Implement time-based (TTL) expiry for entries (and tombstones).\n\nContributed by Gabor Bota.\n\nChange-Id: I73a2d2861901dedfe7a0e783b310fbb95e7c1af9\n",
      "commitDate": "16/06/19 9:05 AM",
      "commitName": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
      "commitAuthor": "Gabor Bota",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-16279. S3Guard: Implement time-based (TTL) expiry for entries (and tombstones).\n\nContributed by Gabor Bota.\n\nChange-Id: I73a2d2861901dedfe7a0e783b310fbb95e7c1af9\n",
          "commitDate": "16/06/19 9:05 AM",
          "commitName": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
          "commitAuthor": "Gabor Bota",
          "commitDateOld": "07/06/19 10:26 AM",
          "commitNameOld": "4e38dafde4dce8cd8c368783a291e830f06e1def",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 8.94,
          "commitsBetweenForRepo": 50,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,6 @@\n-  public void prune(long modTime, String keyPrefix) throws IOException {\n-    int itemCount \u003d 0;\n-    try {\n-      Collection\u003cPath\u003e deletionBatch \u003d\n-          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n-      long delay \u003d conf.getTimeDuration(\n-          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n-          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n-          TimeUnit.MILLISECONDS);\n-      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n-      for (Item item : expiredFiles(modTime, keyPrefix)) {\n-        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n-            .itemToPathMetadata(item, username);\n-        Path path \u003d md.getFileStatus().getPath();\n-        deletionBatch.add(path);\n-\n-        // add parent path of what we remove\n-        Path parentPath \u003d path.getParent();\n-        if (parentPath !\u003d null) {\n-          parentPathSet.add(parentPath);\n-        }\n-\n-        itemCount++;\n-        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n-          Thread.sleep(delay);\n-          processBatchWriteRequest(pathToKey(deletionBatch), null);\n-\n-          // set authoritative false for each pruned dir listing\n-          removeAuthoritativeDirFlag(parentPathSet);\n-          parentPathSet.clear();\n-\n-          deletionBatch.clear();\n-        }\n-      }\n-      // final batch of deletes\n-      if (!deletionBatch.isEmpty()) {\n-        Thread.sleep(delay);\n-        processBatchWriteRequest(pathToKey(deletionBatch), null);\n-\n-        // set authoritative false for each pruned dir listing\n-        removeAuthoritativeDirFlag(parentPathSet);\n-        parentPathSet.clear();\n-      }\n-    } catch (InterruptedException e) {\n-      Thread.currentThread().interrupt();\n-      throw new InterruptedIOException(\"Pruning was interrupted\");\n-    }\n-    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n-        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n+  public void prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n+      throws IOException {\n+    final ItemCollection\u003cScanOutcome\u003e items \u003d\n+        expiredFiles(pruneMode, cutoff, keyPrefix);\n+    innerPrune(items);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n      throws IOException {\n    final ItemCollection\u003cScanOutcome\u003e items \u003d\n        expiredFiles(pruneMode, cutoff, keyPrefix);\n    innerPrune(items);\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {
            "oldValue": "[modTime-long, keyPrefix-String]",
            "newValue": "[pruneMode-PruneMode, cutoff-long, keyPrefix-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-16279. S3Guard: Implement time-based (TTL) expiry for entries (and tombstones).\n\nContributed by Gabor Bota.\n\nChange-Id: I73a2d2861901dedfe7a0e783b310fbb95e7c1af9\n",
          "commitDate": "16/06/19 9:05 AM",
          "commitName": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
          "commitAuthor": "Gabor Bota",
          "commitDateOld": "07/06/19 10:26 AM",
          "commitNameOld": "4e38dafde4dce8cd8c368783a291e830f06e1def",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 8.94,
          "commitsBetweenForRepo": 50,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,6 @@\n-  public void prune(long modTime, String keyPrefix) throws IOException {\n-    int itemCount \u003d 0;\n-    try {\n-      Collection\u003cPath\u003e deletionBatch \u003d\n-          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n-      long delay \u003d conf.getTimeDuration(\n-          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n-          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n-          TimeUnit.MILLISECONDS);\n-      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n-      for (Item item : expiredFiles(modTime, keyPrefix)) {\n-        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n-            .itemToPathMetadata(item, username);\n-        Path path \u003d md.getFileStatus().getPath();\n-        deletionBatch.add(path);\n-\n-        // add parent path of what we remove\n-        Path parentPath \u003d path.getParent();\n-        if (parentPath !\u003d null) {\n-          parentPathSet.add(parentPath);\n-        }\n-\n-        itemCount++;\n-        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n-          Thread.sleep(delay);\n-          processBatchWriteRequest(pathToKey(deletionBatch), null);\n-\n-          // set authoritative false for each pruned dir listing\n-          removeAuthoritativeDirFlag(parentPathSet);\n-          parentPathSet.clear();\n-\n-          deletionBatch.clear();\n-        }\n-      }\n-      // final batch of deletes\n-      if (!deletionBatch.isEmpty()) {\n-        Thread.sleep(delay);\n-        processBatchWriteRequest(pathToKey(deletionBatch), null);\n-\n-        // set authoritative false for each pruned dir listing\n-        removeAuthoritativeDirFlag(parentPathSet);\n-        parentPathSet.clear();\n-      }\n-    } catch (InterruptedException e) {\n-      Thread.currentThread().interrupt();\n-      throw new InterruptedIOException(\"Pruning was interrupted\");\n-    }\n-    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n-        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n+  public void prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n+      throws IOException {\n+    final ItemCollection\u003cScanOutcome\u003e items \u003d\n+        expiredFiles(pruneMode, cutoff, keyPrefix);\n+    innerPrune(items);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void prune(PruneMode pruneMode, long cutoff, String keyPrefix)\n      throws IOException {\n    final ItemCollection\u003cScanOutcome\u003e items \u003d\n        expiredFiles(pruneMode, cutoff, keyPrefix);\n    innerPrune(items);\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "d7c0a08a1c077752918a8cf1b4f1900ce2721899": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15426 Make S3guard client resilient to DDB throttle events and network failures (Contributed by Steve Loughran)\n",
      "commitDate": "12/09/18 9:04 PM",
      "commitName": "d7c0a08a1c077752918a8cf1b4f1900ce2721899",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "12/09/18 4:36 PM",
      "commitNameOld": "d32a8d5d582725eb724b78f27310ad1efd33ed2a",
      "commitAuthorOld": "Aaron Fabbri",
      "daysBetweenCommits": 0.19,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,50 @@\n   public void prune(long modTime, String keyPrefix) throws IOException {\n     int itemCount \u003d 0;\n     try {\n       Collection\u003cPath\u003e deletionBatch \u003d\n           new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n-      int delay \u003d conf.getInt(S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n-          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT);\n-      Set\u003cPath\u003e parentPathSet \u003d  new HashSet\u003c\u003e();\n+      long delay \u003d conf.getTimeDuration(\n+          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n+          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n+          TimeUnit.MILLISECONDS);\n+      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n       for (Item item : expiredFiles(modTime, keyPrefix)) {\n         DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n             .itemToPathMetadata(item, username);\n         Path path \u003d md.getFileStatus().getPath();\n         deletionBatch.add(path);\n \n         // add parent path of what we remove\n         Path parentPath \u003d path.getParent();\n         if (parentPath !\u003d null) {\n           parentPathSet.add(parentPath);\n         }\n \n         itemCount++;\n         if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n           Thread.sleep(delay);\n           processBatchWriteRequest(pathToKey(deletionBatch), null);\n \n           // set authoritative false for each pruned dir listing\n           removeAuthoritativeDirFlag(parentPathSet);\n           parentPathSet.clear();\n \n           deletionBatch.clear();\n         }\n       }\n-      if (deletionBatch.size() \u003e 0) {\n+      // final batch of deletes\n+      if (!deletionBatch.isEmpty()) {\n         Thread.sleep(delay);\n         processBatchWriteRequest(pathToKey(deletionBatch), null);\n \n         // set authoritative false for each pruned dir listing\n         removeAuthoritativeDirFlag(parentPathSet);\n         parentPathSet.clear();\n       }\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\"Pruning was interrupted\");\n     }\n     LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n         S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void prune(long modTime, String keyPrefix) throws IOException {\n    int itemCount \u003d 0;\n    try {\n      Collection\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      long delay \u003d conf.getTimeDuration(\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n          TimeUnit.MILLISECONDS);\n      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n      for (Item item : expiredFiles(modTime, keyPrefix)) {\n        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        deletionBatch.add(path);\n\n        // add parent path of what we remove\n        Path parentPath \u003d path.getParent();\n        if (parentPath !\u003d null) {\n          parentPathSet.add(parentPath);\n        }\n\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          Thread.sleep(delay);\n          processBatchWriteRequest(pathToKey(deletionBatch), null);\n\n          // set authoritative false for each pruned dir listing\n          removeAuthoritativeDirFlag(parentPathSet);\n          parentPathSet.clear();\n\n          deletionBatch.clear();\n        }\n      }\n      // final batch of deletes\n      if (!deletionBatch.isEmpty()) {\n        Thread.sleep(delay);\n        processBatchWriteRequest(pathToKey(deletionBatch), null);\n\n        // set authoritative false for each pruned dir listing\n        removeAuthoritativeDirFlag(parentPathSet);\n        parentPathSet.clear();\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
      "extendedDetails": {}
    },
    "d7232857d8d1e10cdac171acdc931187e45fd6be": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14154 Persist isAuthoritative bit in DynamoDBMetaStore (Contributed by Gabor Bota)\n",
      "commitDate": "17/08/18 10:15 AM",
      "commitName": "d7232857d8d1e10cdac171acdc931187e45fd6be",
      "commitAuthor": "Aaron Fabbri",
      "commitDateOld": "08/08/18 10:57 PM",
      "commitNameOld": "da9a39eed138210de29b59b90c449b28da1c04f9",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 8.47,
      "commitsBetweenForRepo": 126,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,47 @@\n   public void prune(long modTime, String keyPrefix) throws IOException {\n     int itemCount \u003d 0;\n     try {\n       Collection\u003cPath\u003e deletionBatch \u003d\n           new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n       int delay \u003d conf.getInt(S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT);\n+      Set\u003cPath\u003e parentPathSet \u003d  new HashSet\u003c\u003e();\n       for (Item item : expiredFiles(modTime, keyPrefix)) {\n-        PathMetadata md \u003d PathMetadataDynamoDBTranslation\n+        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n             .itemToPathMetadata(item, username);\n         Path path \u003d md.getFileStatus().getPath();\n         deletionBatch.add(path);\n+\n+        // add parent path of what we remove\n+        Path parentPath \u003d path.getParent();\n+        if (parentPath !\u003d null) {\n+          parentPathSet.add(parentPath);\n+        }\n+\n         itemCount++;\n         if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n           Thread.sleep(delay);\n           processBatchWriteRequest(pathToKey(deletionBatch), null);\n+\n+          // set authoritative false for each pruned dir listing\n+          removeAuthoritativeDirFlag(parentPathSet);\n+          parentPathSet.clear();\n+\n           deletionBatch.clear();\n         }\n       }\n       if (deletionBatch.size() \u003e 0) {\n         Thread.sleep(delay);\n         processBatchWriteRequest(pathToKey(deletionBatch), null);\n+\n+        // set authoritative false for each pruned dir listing\n+        removeAuthoritativeDirFlag(parentPathSet);\n+        parentPathSet.clear();\n       }\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\"Pruning was interrupted\");\n     }\n     LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n         S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void prune(long modTime, String keyPrefix) throws IOException {\n    int itemCount \u003d 0;\n    try {\n      Collection\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      int delay \u003d conf.getInt(S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT);\n      Set\u003cPath\u003e parentPathSet \u003d  new HashSet\u003c\u003e();\n      for (Item item : expiredFiles(modTime, keyPrefix)) {\n        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        deletionBatch.add(path);\n\n        // add parent path of what we remove\n        Path parentPath \u003d path.getParent();\n        if (parentPath !\u003d null) {\n          parentPathSet.add(parentPath);\n        }\n\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          Thread.sleep(delay);\n          processBatchWriteRequest(pathToKey(deletionBatch), null);\n\n          // set authoritative false for each pruned dir listing\n          removeAuthoritativeDirFlag(parentPathSet);\n          parentPathSet.clear();\n\n          deletionBatch.clear();\n        }\n      }\n      if (deletionBatch.size() \u003e 0) {\n        Thread.sleep(delay);\n        processBatchWriteRequest(pathToKey(deletionBatch), null);\n\n        // set authoritative false for each pruned dir listing\n        removeAuthoritativeDirFlag(parentPathSet);\n        parentPathSet.clear();\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
      "extendedDetails": {}
    },
    "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-14759 S3GuardTool prune to prune specific bucket entries. Contributed by Gabor Bota.\n",
      "commitDate": "05/04/18 8:23 PM",
      "commitName": "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a",
      "commitAuthor": "Aaron Fabbri",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-14759 S3GuardTool prune to prune specific bucket entries. Contributed by Gabor Bota.\n",
          "commitDate": "05/04/18 8:23 PM",
          "commitName": "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a",
          "commitAuthor": "Aaron Fabbri",
          "commitDateOld": "05/03/18 6:06 AM",
          "commitNameOld": "8110d6a0d59e7dc2ddb25fa424fab188c5e9ce35",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 31.55,
          "commitsBetweenForRepo": 341,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,30 +1,30 @@\n-  public void prune(long modTime) throws IOException {\n+  public void prune(long modTime, String keyPrefix) throws IOException {\n     int itemCount \u003d 0;\n     try {\n       Collection\u003cPath\u003e deletionBatch \u003d\n           new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n       int delay \u003d conf.getInt(S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT);\n-      for (Item item : expiredFiles(modTime)) {\n+      for (Item item : expiredFiles(modTime, keyPrefix)) {\n         PathMetadata md \u003d PathMetadataDynamoDBTranslation\n             .itemToPathMetadata(item, username);\n         Path path \u003d md.getFileStatus().getPath();\n         deletionBatch.add(path);\n         itemCount++;\n         if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n           Thread.sleep(delay);\n           processBatchWriteRequest(pathToKey(deletionBatch), null);\n           deletionBatch.clear();\n         }\n       }\n       if (deletionBatch.size() \u003e 0) {\n         Thread.sleep(delay);\n         processBatchWriteRequest(pathToKey(deletionBatch), null);\n       }\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\"Pruning was interrupted\");\n     }\n     LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n         S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void prune(long modTime, String keyPrefix) throws IOException {\n    int itemCount \u003d 0;\n    try {\n      Collection\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      int delay \u003d conf.getInt(S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT);\n      for (Item item : expiredFiles(modTime, keyPrefix)) {\n        PathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        deletionBatch.add(path);\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          Thread.sleep(delay);\n          processBatchWriteRequest(pathToKey(deletionBatch), null);\n          deletionBatch.clear();\n        }\n      }\n      if (deletionBatch.size() \u003e 0) {\n        Thread.sleep(delay);\n        processBatchWriteRequest(pathToKey(deletionBatch), null);\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {
            "oldValue": "[modTime-long]",
            "newValue": "[modTime-long, keyPrefix-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-14759 S3GuardTool prune to prune specific bucket entries. Contributed by Gabor Bota.\n",
          "commitDate": "05/04/18 8:23 PM",
          "commitName": "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a",
          "commitAuthor": "Aaron Fabbri",
          "commitDateOld": "05/03/18 6:06 AM",
          "commitNameOld": "8110d6a0d59e7dc2ddb25fa424fab188c5e9ce35",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 31.55,
          "commitsBetweenForRepo": 341,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,30 +1,30 @@\n-  public void prune(long modTime) throws IOException {\n+  public void prune(long modTime, String keyPrefix) throws IOException {\n     int itemCount \u003d 0;\n     try {\n       Collection\u003cPath\u003e deletionBatch \u003d\n           new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n       int delay \u003d conf.getInt(S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT);\n-      for (Item item : expiredFiles(modTime)) {\n+      for (Item item : expiredFiles(modTime, keyPrefix)) {\n         PathMetadata md \u003d PathMetadataDynamoDBTranslation\n             .itemToPathMetadata(item, username);\n         Path path \u003d md.getFileStatus().getPath();\n         deletionBatch.add(path);\n         itemCount++;\n         if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n           Thread.sleep(delay);\n           processBatchWriteRequest(pathToKey(deletionBatch), null);\n           deletionBatch.clear();\n         }\n       }\n       if (deletionBatch.size() \u003e 0) {\n         Thread.sleep(delay);\n         processBatchWriteRequest(pathToKey(deletionBatch), null);\n       }\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\"Pruning was interrupted\");\n     }\n     LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n         S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void prune(long modTime, String keyPrefix) throws IOException {\n    int itemCount \u003d 0;\n    try {\n      Collection\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      int delay \u003d conf.getInt(S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT);\n      for (Item item : expiredFiles(modTime, keyPrefix)) {\n        PathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        deletionBatch.add(path);\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          Thread.sleep(delay);\n          processBatchWriteRequest(pathToKey(deletionBatch), null);\n          deletionBatch.clear();\n        }\n      }\n      if (deletionBatch.size() \u003e 0) {\n        Thread.sleep(delay);\n        processBatchWriteRequest(pathToKey(deletionBatch), null);\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "621b43e254afaff708cd6fc4698b29628f6abc33": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-13345 HS3Guard: Improved Consistency for S3A.\nContributed by: Chris Nauroth, Aaron Fabbri, Mingliang Liu, Lei (Eddy) Xu,\nSean Mackrory, Steve Loughran and others.\n",
      "commitDate": "01/09/17 6:13 AM",
      "commitName": "621b43e254afaff708cd6fc4698b29628f6abc33",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,30 @@\n+  public void prune(long modTime) throws IOException {\n+    int itemCount \u003d 0;\n+    try {\n+      Collection\u003cPath\u003e deletionBatch \u003d\n+          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n+      int delay \u003d conf.getInt(S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n+          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT);\n+      for (Item item : expiredFiles(modTime)) {\n+        PathMetadata md \u003d PathMetadataDynamoDBTranslation\n+            .itemToPathMetadata(item, username);\n+        Path path \u003d md.getFileStatus().getPath();\n+        deletionBatch.add(path);\n+        itemCount++;\n+        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n+          Thread.sleep(delay);\n+          processBatchWriteRequest(pathToKey(deletionBatch), null);\n+          deletionBatch.clear();\n+        }\n+      }\n+      if (deletionBatch.size() \u003e 0) {\n+        Thread.sleep(delay);\n+        processBatchWriteRequest(pathToKey(deletionBatch), null);\n+      }\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      throw new InterruptedIOException(\"Pruning was interrupted\");\n+    }\n+    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n+        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void prune(long modTime) throws IOException {\n    int itemCount \u003d 0;\n    try {\n      Collection\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      int delay \u003d conf.getInt(S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT);\n      for (Item item : expiredFiles(modTime)) {\n        PathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        deletionBatch.add(path);\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          Thread.sleep(delay);\n          processBatchWriteRequest(pathToKey(deletionBatch), null);\n          deletionBatch.clear();\n        }\n      }\n      if (deletionBatch.size() \u003e 0) {\n        Thread.sleep(delay);\n        processBatchWriteRequest(pathToKey(deletionBatch), null);\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java"
    }
  }
}