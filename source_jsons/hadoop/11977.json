{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "moveBlock",
  "functionId": "moveBlock___block-ExtendedBlock__replicaInfo-ReplicaInfo__volumeRef-FsVolumeReference",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 1053,
  "functionEndLine": 1060,
  "numCommitsSeen": 197,
  "timeTaken": 7759,
  "changeHistory": [
    "d907fdc3cdf1a9173e97389166c22689cb51e72a",
    "aa45faf0b20c922b0d147ece9fa01fb95a5b0dec",
    "69afa26f19adad4c630a307c274130eb8b697141",
    "1543d0f5be6a02ad00e7a33e35d78af8516043e3",
    "96b12662ea76e3ded4ef13944fc8df206cfb4613",
    "86c9862bec0248d671e657aa56094a2919b8ac14",
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
    "7820737cfa178d9de1bcbb1e99b9677d70901914"
  ],
  "changeHistoryShort": {
    "d907fdc3cdf1a9173e97389166c22689cb51e72a": "Ymodifierchange",
    "aa45faf0b20c922b0d147ece9fa01fb95a5b0dec": "Ybodychange",
    "69afa26f19adad4c630a307c274130eb8b697141": "Ybodychange",
    "1543d0f5be6a02ad00e7a33e35d78af8516043e3": "Ybodychange",
    "96b12662ea76e3ded4ef13944fc8df206cfb4613": "Ybodychange",
    "86c9862bec0248d671e657aa56094a2919b8ac14": "Ybodychange",
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c": "Ybodychange",
    "7820737cfa178d9de1bcbb1e99b9677d70901914": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d907fdc3cdf1a9173e97389166c22689cb51e72a": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-13439. Add test case for read block operation when it is moved. Contributed by Ajay Kumar.\n",
      "commitDate": "16/04/18 2:16 PM",
      "commitName": "d907fdc3cdf1a9173e97389166c22689cb51e72a",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "02/02/18 5:18 PM",
      "commitNameOld": "2021f4bdce3b27c46edaad198f0007a26a8a1391",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 72.83,
      "commitsBetweenForRepo": 579,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,8 @@\n-  private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n+  ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n       FsVolumeReference volumeRef) throws IOException {\n     ReplicaInfo newReplicaInfo \u003d copyReplicaToVolume(block, replicaInfo,\n         volumeRef);\n     finalizeNewReplica(newReplicaInfo, block);\n     removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n      FsVolumeReference volumeRef) throws IOException {\n    ReplicaInfo newReplicaInfo \u003d copyReplicaToVolume(block, replicaInfo,\n        volumeRef);\n    finalizeNewReplica(newReplicaInfo, block);\n    removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[]"
      }
    },
    "aa45faf0b20c922b0d147ece9fa01fb95a5b0dec": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12942. Synchronization issue in FSDataSetImpl#moveBlock. Contributed by Ajay Kumar.\n",
      "commitDate": "01/02/18 6:03 PM",
      "commitName": "aa45faf0b20c922b0d147ece9fa01fb95a5b0dec",
      "commitAuthor": "Anu Engineer",
      "commitDateOld": "19/01/18 5:51 PM",
      "commitNameOld": "62c9e7fa99da1b1c8af222436102b8dea02fcde8",
      "commitAuthorOld": "Chen Liang",
      "daysBetweenCommits": 13.01,
      "commitsBetweenForRepo": 101,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,8 @@\n   private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n-                                FsVolumeReference volumeRef) throws\n-      IOException {\n-\n-    FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n-    // Copy files to temp dir first\n-    ReplicaInfo newReplicaInfo \u003d targetVolume.moveBlockToTmpLocation(block,\n-        replicaInfo, smallBufferSize, conf);\n-\n-    // Finalize the copied files\n-    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n-    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n-      // Increment numBlocks here as this block moved without knowing to BPS\n-      FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n-      volume.incrNumBlocks(block.getBlockPoolId());\n-    }\n-\n+      FsVolumeReference volumeRef) throws IOException {\n+    ReplicaInfo newReplicaInfo \u003d copyReplicaToVolume(block, replicaInfo,\n+        volumeRef);\n+    finalizeNewReplica(newReplicaInfo, block);\n     removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n      FsVolumeReference volumeRef) throws IOException {\n    ReplicaInfo newReplicaInfo \u003d copyReplicaToVolume(block, replicaInfo,\n        volumeRef);\n    finalizeNewReplica(newReplicaInfo, block);\n    removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "69afa26f19adad4c630a307c274130eb8b697141": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12157. Do fsyncDirectory(..) outside of FSDataset lock. Contributed by inayakumar B.\n",
      "commitDate": "09/08/17 7:03 AM",
      "commitName": "69afa26f19adad4c630a307c274130eb8b697141",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "01/08/17 6:34 PM",
      "commitNameOld": "6814324c332a7d780f3b844fd6f1c62db2f6c88e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 7.52,
      "commitsBetweenForRepo": 54,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,20 @@\n   private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n                                 FsVolumeReference volumeRef) throws\n       IOException {\n \n     FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n     // Copy files to temp dir first\n     ReplicaInfo newReplicaInfo \u003d targetVolume.moveBlockToTmpLocation(block,\n         replicaInfo, smallBufferSize, conf);\n \n     // Finalize the copied files\n-    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo,\n-        false);\n+    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       // Increment numBlocks here as this block moved without knowing to BPS\n       FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n       volume.incrNumBlocks(block.getBlockPoolId());\n     }\n \n     removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n                                FsVolumeReference volumeRef) throws\n      IOException {\n\n    FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n    // Copy files to temp dir first\n    ReplicaInfo newReplicaInfo \u003d targetVolume.moveBlockToTmpLocation(block,\n        replicaInfo, smallBufferSize, conf);\n\n    // Finalize the copied files\n    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      // Increment numBlocks here as this block moved without knowing to BPS\n      FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n      volume.incrNumBlocks(block.getBlockPoolId());\n    }\n\n    removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "1543d0f5be6a02ad00e7a33e35d78af8516043e3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5042. Completed files lost after power failure. Contributed by Vinayakumar B.\n",
      "commitDate": "31/05/17 8:55 AM",
      "commitName": "1543d0f5be6a02ad00e7a33e35d78af8516043e3",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "25/05/17 11:05 AM",
      "commitNameOld": "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 5.91,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,21 @@\n   private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n                                 FsVolumeReference volumeRef) throws\n       IOException {\n \n     FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n     // Copy files to temp dir first\n     ReplicaInfo newReplicaInfo \u003d targetVolume.moveBlockToTmpLocation(block,\n         replicaInfo, smallBufferSize, conf);\n \n     // Finalize the copied files\n-    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n+    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo,\n+        false);\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       // Increment numBlocks here as this block moved without knowing to BPS\n       FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n       volume.incrNumBlocks(block.getBlockPoolId());\n     }\n \n     removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n                                FsVolumeReference volumeRef) throws\n      IOException {\n\n    FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n    // Copy files to temp dir first\n    ReplicaInfo newReplicaInfo \u003d targetVolume.moveBlockToTmpLocation(block,\n        replicaInfo, smallBufferSize, conf);\n\n    // Finalize the copied files\n    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo,\n        false);\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      // Increment numBlocks here as this block moved without knowing to BPS\n      FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n      volume.incrNumBlocks(block.getBlockPoolId());\n    }\n\n    removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "96b12662ea76e3ded4ef13944fc8df206cfb4613": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10637. Modifications to remove the assumption that FsVolumes are backed by java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "10/10/16 3:30 PM",
      "commitName": "96b12662ea76e3ded4ef13944fc8df206cfb4613",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "30/09/16 11:11 PM",
      "commitNameOld": "fe9ebe20ab113567f0777c11cb48ce0d3ce587a8",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 9.68,
      "commitsBetweenForRepo": 64,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,20 @@\n   private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n                                 FsVolumeReference volumeRef) throws\n       IOException {\n \n     FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n     // Copy files to temp dir first\n-    File[] blockFiles \u003d copyBlockFiles(block.getBlockId(),\n-        block.getGenerationStamp(), replicaInfo,\n-        targetVolume.getTmpDir(block.getBlockPoolId()),\n-        replicaInfo.isOnTransientStorage(), smallBufferSize, conf);\n+    ReplicaInfo newReplicaInfo \u003d targetVolume.moveBlockToTmpLocation(block,\n+        replicaInfo, smallBufferSize, conf);\n \n-    ReplicaInfo newReplicaInfo \u003d new ReplicaBuilder(ReplicaState.TEMPORARY)\n-        .setBlockId(replicaInfo.getBlockId())\n-        .setGenerationStamp(replicaInfo.getGenerationStamp())\n-        .setFsVolume(targetVolume)\n-        .setDirectoryToUse(blockFiles[0].getParentFile())\n-        .setBytesToReserve(0)\n-        .build();\n-    newReplicaInfo.setNumBytes(blockFiles[1].length());\n     // Finalize the copied files\n     newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       // Increment numBlocks here as this block moved without knowing to BPS\n       FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n-      volume.getBlockPoolSlice(block.getBlockPoolId()).incrNumBlocks();\n+      volume.incrNumBlocks(block.getBlockPoolId());\n     }\n \n     removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n                                FsVolumeReference volumeRef) throws\n      IOException {\n\n    FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n    // Copy files to temp dir first\n    ReplicaInfo newReplicaInfo \u003d targetVolume.moveBlockToTmpLocation(block,\n        replicaInfo, smallBufferSize, conf);\n\n    // Finalize the copied files\n    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      // Increment numBlocks here as this block moved without knowing to BPS\n      FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n      volume.incrNumBlocks(block.getBlockPoolId());\n    }\n\n    removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "86c9862bec0248d671e657aa56094a2919b8ac14": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "13/09/16 12:54 PM",
      "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "10/09/16 6:22 PM",
      "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.77,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,30 @@\n   private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n                                 FsVolumeReference volumeRef) throws\n       IOException {\n-    File oldBlockFile \u003d replicaInfo.getBlockFile();\n-    File oldMetaFile \u003d replicaInfo.getMetaFile();\n+\n     FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n     // Copy files to temp dir first\n     File[] blockFiles \u003d copyBlockFiles(block.getBlockId(),\n-        block.getGenerationStamp(), oldMetaFile, oldBlockFile,\n+        block.getGenerationStamp(), replicaInfo,\n         targetVolume.getTmpDir(block.getBlockPoolId()),\n         replicaInfo.isOnTransientStorage(), smallBufferSize, conf);\n \n-    ReplicaInfo newReplicaInfo \u003d new ReplicaInPipeline(\n-        replicaInfo.getBlockId(), replicaInfo.getGenerationStamp(),\n-        targetVolume, blockFiles[0].getParentFile(), 0);\n+    ReplicaInfo newReplicaInfo \u003d new ReplicaBuilder(ReplicaState.TEMPORARY)\n+        .setBlockId(replicaInfo.getBlockId())\n+        .setGenerationStamp(replicaInfo.getGenerationStamp())\n+        .setFsVolume(targetVolume)\n+        .setDirectoryToUse(blockFiles[0].getParentFile())\n+        .setBytesToReserve(0)\n+        .build();\n     newReplicaInfo.setNumBytes(blockFiles[1].length());\n     // Finalize the copied files\n     newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       // Increment numBlocks here as this block moved without knowing to BPS\n       FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n       volume.getBlockPoolSlice(block.getBlockPoolId()).incrNumBlocks();\n     }\n \n-    removeOldReplica(replicaInfo, newReplicaInfo, oldBlockFile, oldMetaFile,\n-        oldBlockFile.length(), oldMetaFile.length(), block.getBlockPoolId());\n+    removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n                                FsVolumeReference volumeRef) throws\n      IOException {\n\n    FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n    // Copy files to temp dir first\n    File[] blockFiles \u003d copyBlockFiles(block.getBlockId(),\n        block.getGenerationStamp(), replicaInfo,\n        targetVolume.getTmpDir(block.getBlockPoolId()),\n        replicaInfo.isOnTransientStorage(), smallBufferSize, conf);\n\n    ReplicaInfo newReplicaInfo \u003d new ReplicaBuilder(ReplicaState.TEMPORARY)\n        .setBlockId(replicaInfo.getBlockId())\n        .setGenerationStamp(replicaInfo.getGenerationStamp())\n        .setFsVolume(targetVolume)\n        .setDirectoryToUse(blockFiles[0].getParentFile())\n        .setBytesToReserve(0)\n        .build();\n    newReplicaInfo.setNumBytes(blockFiles[1].length());\n    // Finalize the copied files\n    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      // Increment numBlocks here as this block moved without knowing to BPS\n      FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n      volume.getBlockPoolSlice(block.getBlockPoolId()).incrNumBlocks();\n    }\n\n    removeOldReplica(replicaInfo, newReplicaInfo, block.getBlockPoolId());\n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10682. Replace FsDatasetImpl object lock with a separate lock object. (Chen Liang)\n",
      "commitDate": "08/08/16 12:02 PM",
      "commitName": "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "08/07/16 7:40 PM",
      "commitNameOld": "da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 30.68,
      "commitsBetweenForRepo": 320,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n                                 FsVolumeReference volumeRef) throws\n       IOException {\n     File oldBlockFile \u003d replicaInfo.getBlockFile();\n     File oldMetaFile \u003d replicaInfo.getMetaFile();\n     FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n     // Copy files to temp dir first\n     File[] blockFiles \u003d copyBlockFiles(block.getBlockId(),\n         block.getGenerationStamp(), oldMetaFile, oldBlockFile,\n         targetVolume.getTmpDir(block.getBlockPoolId()),\n         replicaInfo.isOnTransientStorage(), smallBufferSize, conf);\n \n     ReplicaInfo newReplicaInfo \u003d new ReplicaInPipeline(\n         replicaInfo.getBlockId(), replicaInfo.getGenerationStamp(),\n         targetVolume, blockFiles[0].getParentFile(), 0);\n     newReplicaInfo.setNumBytes(blockFiles[1].length());\n     // Finalize the copied files\n     newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n-    synchronized (this) {\n+    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       // Increment numBlocks here as this block moved without knowing to BPS\n       FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n       volume.getBlockPoolSlice(block.getBlockPoolId()).incrNumBlocks();\n     }\n \n     removeOldReplica(replicaInfo, newReplicaInfo, oldBlockFile, oldMetaFile,\n         oldBlockFile.length(), oldMetaFile.length(), block.getBlockPoolId());\n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n                                FsVolumeReference volumeRef) throws\n      IOException {\n    File oldBlockFile \u003d replicaInfo.getBlockFile();\n    File oldMetaFile \u003d replicaInfo.getMetaFile();\n    FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n    // Copy files to temp dir first\n    File[] blockFiles \u003d copyBlockFiles(block.getBlockId(),\n        block.getGenerationStamp(), oldMetaFile, oldBlockFile,\n        targetVolume.getTmpDir(block.getBlockPoolId()),\n        replicaInfo.isOnTransientStorage(), smallBufferSize, conf);\n\n    ReplicaInfo newReplicaInfo \u003d new ReplicaInPipeline(\n        replicaInfo.getBlockId(), replicaInfo.getGenerationStamp(),\n        targetVolume, blockFiles[0].getParentFile(), 0);\n    newReplicaInfo.setNumBytes(blockFiles[1].length());\n    // Finalize the copied files\n    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      // Increment numBlocks here as this block moved without knowing to BPS\n      FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n      volume.getBlockPoolSlice(block.getBlockPoolId()).incrNumBlocks();\n    }\n\n    removeOldReplica(replicaInfo, newReplicaInfo, oldBlockFile, oldMetaFile,\n        oldBlockFile.length(), oldMetaFile.length(), block.getBlockPoolId());\n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "7820737cfa178d9de1bcbb1e99b9677d70901914": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9735. DiskBalancer : Refactor moveBlockAcrossStorage to be used by disk balancer. Contributed by Anu Engineer.\n",
      "commitDate": "23/06/16 6:20 PM",
      "commitName": "7820737cfa178d9de1bcbb1e99b9677d70901914",
      "commitAuthor": "Anu Engineer",
      "diff": "@@ -0,0 +1,28 @@\n+  private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n+                                FsVolumeReference volumeRef) throws\n+      IOException {\n+    File oldBlockFile \u003d replicaInfo.getBlockFile();\n+    File oldMetaFile \u003d replicaInfo.getMetaFile();\n+    FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n+    // Copy files to temp dir first\n+    File[] blockFiles \u003d copyBlockFiles(block.getBlockId(),\n+        block.getGenerationStamp(), oldMetaFile, oldBlockFile,\n+        targetVolume.getTmpDir(block.getBlockPoolId()),\n+        replicaInfo.isOnTransientStorage(), smallBufferSize, conf);\n+\n+    ReplicaInfo newReplicaInfo \u003d new ReplicaInPipeline(\n+        replicaInfo.getBlockId(), replicaInfo.getGenerationStamp(),\n+        targetVolume, blockFiles[0].getParentFile(), 0);\n+    newReplicaInfo.setNumBytes(blockFiles[1].length());\n+    // Finalize the copied files\n+    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n+    synchronized (this) {\n+      // Increment numBlocks here as this block moved without knowing to BPS\n+      FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n+      volume.getBlockPoolSlice(block.getBlockPoolId()).incrNumBlocks();\n+    }\n+\n+    removeOldReplica(replicaInfo, newReplicaInfo, oldBlockFile, oldMetaFile,\n+        oldBlockFile.length(), oldMetaFile.length(), block.getBlockPoolId());\n+    return newReplicaInfo;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private ReplicaInfo moveBlock(ExtendedBlock block, ReplicaInfo replicaInfo,\n                                FsVolumeReference volumeRef) throws\n      IOException {\n    File oldBlockFile \u003d replicaInfo.getBlockFile();\n    File oldMetaFile \u003d replicaInfo.getMetaFile();\n    FsVolumeImpl targetVolume \u003d (FsVolumeImpl) volumeRef.getVolume();\n    // Copy files to temp dir first\n    File[] blockFiles \u003d copyBlockFiles(block.getBlockId(),\n        block.getGenerationStamp(), oldMetaFile, oldBlockFile,\n        targetVolume.getTmpDir(block.getBlockPoolId()),\n        replicaInfo.isOnTransientStorage(), smallBufferSize, conf);\n\n    ReplicaInfo newReplicaInfo \u003d new ReplicaInPipeline(\n        replicaInfo.getBlockId(), replicaInfo.getGenerationStamp(),\n        targetVolume, blockFiles[0].getParentFile(), 0);\n    newReplicaInfo.setNumBytes(blockFiles[1].length());\n    // Finalize the copied files\n    newReplicaInfo \u003d finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n    synchronized (this) {\n      // Increment numBlocks here as this block moved without knowing to BPS\n      FsVolumeImpl volume \u003d (FsVolumeImpl) newReplicaInfo.getVolume();\n      volume.getBlockPoolSlice(block.getBlockPoolId()).incrNumBlocks();\n    }\n\n    removeOldReplica(replicaInfo, newReplicaInfo, oldBlockFile, oldMetaFile,\n        oldBlockFile.length(), oldMetaFile.length(), block.getBlockPoolId());\n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java"
    }
  }
}