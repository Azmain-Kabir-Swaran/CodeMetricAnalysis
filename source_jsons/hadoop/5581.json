{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "INodeFile.java",
  "functionName": "collectBlocksBeyondMax",
  "functionId": "collectBlocksBeyondMax___max-long(modifiers-final)__collectedBlocks-BlocksMapUpdateInfo(modifiers-final)__toRetain-Set__BlockInfo__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
  "functionStartLine": 1108,
  "functionEndLine": 1138,
  "numCommitsSeen": 385,
  "timeTaken": 11500,
  "changeHistory": [
    "972782d9568e0849484c027f27c1638ba50ec56e",
    "47f4c54106ebb234a7d3dc71320aa584ecba161a",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f",
    "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
    "3f82484218d5694e62ddcb23376d0e4e332aa8b8",
    "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
    "8df119da214babde03e73243c7ca4cfe6d0ca562",
    "afe77ce53d3cf203690aa419e377f26cbd45a96e",
    "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
    "b71d3868908a49c1b2e353afea795a76dfb20f7d",
    "397835acdf66cf48ebdbc256aa15b6660181c339",
    "25116c26fd9b998025fa28666ae45ab03a995d91",
    "8a577a16f96437d87ad764dedbdc67d4c184b8d9",
    "deaf979d4122a0a0e4ae0557abbb7f17d18a9380",
    "719279ea8a510ba8d04174ac85ad42fa991725a2"
  ],
  "changeHistoryShort": {
    "972782d9568e0849484c027f27c1638ba50ec56e": "Ymultichange(Yparameterchange,Ybodychange)",
    "47f4c54106ebb234a7d3dc71320aa584ecba161a": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": "Ybodychange",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": "Ybodychange",
    "7e9358feb326d48b8c4f00249e7af5023cebd2e2": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
    "3f82484218d5694e62ddcb23376d0e4e332aa8b8": "Ybodychange",
    "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a": "Ymultichange(Yfilerename,Ybodychange,Yparameterchange)",
    "8df119da214babde03e73243c7ca4cfe6d0ca562": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "afe77ce53d3cf203690aa419e377f26cbd45a96e": "Ybodychange",
    "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
    "b71d3868908a49c1b2e353afea795a76dfb20f7d": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "397835acdf66cf48ebdbc256aa15b6660181c339": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "25116c26fd9b998025fa28666ae45ab03a995d91": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "8a577a16f96437d87ad764dedbdc67d4c184b8d9": "Ymultichange(Yparameterchange,Ybodychange)",
    "deaf979d4122a0a0e4ae0557abbb7f17d18a9380": "Ybodychange",
    "719279ea8a510ba8d04174ac85ad42fa991725a2": "Yintroduced"
  },
  "changeHistoryDetails": {
    "972782d9568e0849484c027f27c1638ba50ec56e": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9754. Avoid unnecessary getBlockCollection calls in BlockManager. Contributed by Jing Zhao.\n",
      "commitDate": "12/02/16 11:07 AM",
      "commitName": "972782d9568e0849484c027f27c1638ba50ec56e",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9754. Avoid unnecessary getBlockCollection calls in BlockManager. Contributed by Jing Zhao.\n",
          "commitDate": "12/02/16 11:07 AM",
          "commitName": "972782d9568e0849484c027f27c1638ba50ec56e",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "01/02/16 1:56 PM",
          "commitNameOld": "34ab50ea92370cc7440a8f7649286b148c2fde65",
          "commitAuthorOld": "Yongjun Zhang",
          "daysBetweenCommits": 10.88,
          "commitsBetweenForRepo": 93,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,31 @@\n   public long collectBlocksBeyondMax(final long max,\n-      final BlocksMapUpdateInfo collectedBlocks) {\n+      final BlocksMapUpdateInfo collectedBlocks, Set\u003cBlockInfo\u003e toRetain) {\n     final BlockInfo[] oldBlocks \u003d getBlocks();\n     if (oldBlocks \u003d\u003d null) {\n       return 0;\n     }\n     // find the minimum n such that the size of the first n blocks \u003e max\n     int n \u003d 0;\n     long size \u003d 0;\n     for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n       size +\u003d oldBlocks[n].getNumBytes();\n     }\n     if (n \u003e\u003d oldBlocks.length) {\n       return size;\n     }\n \n     // starting from block n, the data is beyond max.\n     // resize the array.\n     truncateBlocksTo(n);\n \n     // collect the blocks beyond max\n     if (collectedBlocks !\u003d null) {\n       for(; n \u003c oldBlocks.length; n++) {\n-        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n+        final BlockInfo del \u003d oldBlocks[n];\n+        if (toRetain \u003d\u003d null || !toRetain.contains(del)) {\n+          collectedBlocks.addDeleteBlock(del);\n+        }\n       }\n     }\n     return size;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks, Set\u003cBlockInfo\u003e toRetain) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null) {\n      return 0;\n    }\n    // find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length) {\n      return size;\n    }\n\n    // starting from block n, the data is beyond max.\n    // resize the array.\n    truncateBlocksTo(n);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        final BlockInfo del \u003d oldBlocks[n];\n        if (toRetain \u003d\u003d null || !toRetain.contains(del)) {\n          collectedBlocks.addDeleteBlock(del);\n        }\n      }\n    }\n    return size;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {
            "oldValue": "[max-long(modifiers-final), collectedBlocks-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[max-long(modifiers-final), collectedBlocks-BlocksMapUpdateInfo(modifiers-final), toRetain-Set\u003cBlockInfo\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9754. Avoid unnecessary getBlockCollection calls in BlockManager. Contributed by Jing Zhao.\n",
          "commitDate": "12/02/16 11:07 AM",
          "commitName": "972782d9568e0849484c027f27c1638ba50ec56e",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "01/02/16 1:56 PM",
          "commitNameOld": "34ab50ea92370cc7440a8f7649286b148c2fde65",
          "commitAuthorOld": "Yongjun Zhang",
          "daysBetweenCommits": 10.88,
          "commitsBetweenForRepo": 93,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,31 @@\n   public long collectBlocksBeyondMax(final long max,\n-      final BlocksMapUpdateInfo collectedBlocks) {\n+      final BlocksMapUpdateInfo collectedBlocks, Set\u003cBlockInfo\u003e toRetain) {\n     final BlockInfo[] oldBlocks \u003d getBlocks();\n     if (oldBlocks \u003d\u003d null) {\n       return 0;\n     }\n     // find the minimum n such that the size of the first n blocks \u003e max\n     int n \u003d 0;\n     long size \u003d 0;\n     for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n       size +\u003d oldBlocks[n].getNumBytes();\n     }\n     if (n \u003e\u003d oldBlocks.length) {\n       return size;\n     }\n \n     // starting from block n, the data is beyond max.\n     // resize the array.\n     truncateBlocksTo(n);\n \n     // collect the blocks beyond max\n     if (collectedBlocks !\u003d null) {\n       for(; n \u003c oldBlocks.length; n++) {\n-        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n+        final BlockInfo del \u003d oldBlocks[n];\n+        if (toRetain \u003d\u003d null || !toRetain.contains(del)) {\n+          collectedBlocks.addDeleteBlock(del);\n+        }\n       }\n     }\n     return size;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks, Set\u003cBlockInfo\u003e toRetain) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null) {\n      return 0;\n    }\n    // find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length) {\n      return size;\n    }\n\n    // starting from block n, the data is beyond max.\n    // resize the array.\n    truncateBlocksTo(n);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        final BlockInfo del \u003d oldBlocks[n];\n        if (toRetain \u003d\u003d null || !toRetain.contains(del)) {\n          collectedBlocks.addDeleteBlock(del);\n        }\n      }\n    }\n    return size;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {}
        }
      ]
    },
    "47f4c54106ebb234a7d3dc71320aa584ecba161a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8751. Remove setBlocks API from INodeFile and misc code cleanup. Contributed by Zhe Zhang\n",
      "commitDate": "10/07/15 2:15 PM",
      "commitName": "47f4c54106ebb234a7d3dc71320aa584ecba161a",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "17/06/15 8:05 AM",
      "commitNameOld": "6e3fcffe291faec40fa9214f4880a35a952836c4",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 23.26,
      "commitsBetweenForRepo": 160,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,28 @@\n   public long collectBlocksBeyondMax(final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n     final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks \u003d\u003d null)\n+    if (oldBlocks \u003d\u003d null) {\n       return 0;\n+    }\n     // find the minimum n such that the size of the first n blocks \u003e max\n     int n \u003d 0;\n     long size \u003d 0;\n     for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n       size +\u003d oldBlocks[n].getNumBytes();\n     }\n-    if (n \u003e\u003d oldBlocks.length)\n+    if (n \u003e\u003d oldBlocks.length) {\n       return size;\n+    }\n \n     // starting from block n, the data is beyond max.\n     // resize the array.\n     truncateBlocksTo(n);\n \n     // collect the blocks beyond max\n     if (collectedBlocks !\u003d null) {\n       for(; n \u003c oldBlocks.length; n++) {\n         collectedBlocks.addDeleteBlock(oldBlocks[n]);\n       }\n     }\n     return size;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null) {\n      return 0;\n    }\n    // find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length) {\n      return size;\n    }\n\n    // starting from block n, the data is beyond max.\n    // resize the array.\n    truncateBlocksTo(n);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n      }\n    }\n    return size;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "13/05/15 9:50 PM",
      "commitNameOld": "b2c85db86c9a62b0a03ee87547265077f664970a",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 13.74,
      "commitsBetweenForRepo": 96,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   public long collectBlocksBeyondMax(final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n-    final BlockInfoContiguous[] oldBlocks \u003d getBlocks();\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n     if (oldBlocks \u003d\u003d null)\n       return 0;\n     // find the minimum n such that the size of the first n blocks \u003e max\n     int n \u003d 0;\n     long size \u003d 0;\n     for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n       size +\u003d oldBlocks[n].getNumBytes();\n     }\n     if (n \u003e\u003d oldBlocks.length)\n       return size;\n \n     // starting from block n, the data is beyond max.\n     // resize the array.\n     truncateBlocksTo(n);\n \n     // collect the blocks beyond max\n     if (collectedBlocks !\u003d null) {\n       for(; n \u003c oldBlocks.length; n++) {\n         collectedBlocks.addDeleteBlock(oldBlocks[n]);\n       }\n     }\n     return size;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null)\n      return 0;\n    // find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length)\n      return size;\n\n    // starting from block n, the data is beyond max.\n    // resize the array.\n    truncateBlocksTo(n);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n      }\n    }\n    return size;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7749. Erasure Coding: Add striped block support in INodeFile. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:03 AM",
      "commitNameOld": "bc2833b1c91e107d090619d755c584f6eae82327",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,27 @@\n   public long collectBlocksBeyondMax(final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n-    final BlockInfoContiguous[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks \u003d\u003d null)\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks \u003d\u003d null) {\n       return 0;\n+    }\n     // find the minimum n such that the size of the first n blocks \u003e max\n     int n \u003d 0;\n     long size \u003d 0;\n     for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n       size +\u003d oldBlocks[n].getNumBytes();\n     }\n     if (n \u003e\u003d oldBlocks.length)\n       return size;\n \n     // starting from block n, the data is beyond max.\n     // resize the array.\n     truncateBlocksTo(n);\n \n     // collect the blocks beyond max\n     if (collectedBlocks !\u003d null) {\n       for(; n \u003c oldBlocks.length; n++) {\n         collectedBlocks.addDeleteBlock(oldBlocks[n]);\n       }\n     }\n     return size;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null) {\n      return 0;\n    }\n    // find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length)\n      return size;\n\n    // starting from block n, the data is beyond max.\n    // resize the array.\n    truncateBlocksTo(n);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n      }\n    }\n    return size;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/02/15 4:32 PM",
      "commitNameOld": "8cb473124c1cf1c6f68ead7bde06558ebf7ce47e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 5.8,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   public long collectBlocksBeyondMax(final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    final BlockInfoContiguous[] oldBlocks \u003d getBlocks();\n     if (oldBlocks \u003d\u003d null)\n       return 0;\n     // find the minimum n such that the size of the first n blocks \u003e max\n     int n \u003d 0;\n     long size \u003d 0;\n     for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n       size +\u003d oldBlocks[n].getNumBytes();\n     }\n     if (n \u003e\u003d oldBlocks.length)\n       return size;\n \n     // starting from block n, the data is beyond max.\n     // resize the array.\n     truncateBlocksTo(n);\n \n     // collect the blocks beyond max\n     if (collectedBlocks !\u003d null) {\n       for(; n \u003c oldBlocks.length; n++) {\n         collectedBlocks.addDeleteBlock(oldBlocks[n]);\n       }\n     }\n     return size;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfoContiguous[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null)\n      return 0;\n    // find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length)\n      return size;\n\n    // starting from block n, the data is beyond max.\n    // resize the array.\n    truncateBlocksTo(n);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n      }\n    }\n    return size;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7056. Snapshot support for truncate. Contributed by Konstantin Shvachko and Plamen Jeliazkov.",
      "commitDate": "13/01/15 12:24 AM",
      "commitName": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "12/01/15 10:50 PM",
      "commitNameOld": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
      "commitAuthorOld": "Plamen Jeliazkov",
      "daysBetweenCommits": 0.07,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,26 @@\n   public long collectBlocksBeyondMax(final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n     final BlockInfo[] oldBlocks \u003d getBlocks();\n     if (oldBlocks \u003d\u003d null)\n       return 0;\n-    //find the minimum n such that the size of the first n blocks \u003e max\n+    // find the minimum n such that the size of the first n blocks \u003e max\n     int n \u003d 0;\n     long size \u003d 0;\n     for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n       size +\u003d oldBlocks[n].getNumBytes();\n     }\n     if (n \u003e\u003d oldBlocks.length)\n       return size;\n \n     // starting from block n, the data is beyond max.\n-    // resize the array.  \n-    final BlockInfo[] newBlocks;\n-    if (n \u003d\u003d 0) {\n-      newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n-    } else {\n-      newBlocks \u003d new BlockInfo[n];\n-      System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-    }\n-    // set new blocks\n-    setBlocks(newBlocks);\n+    // resize the array.\n+    truncateBlocksTo(n);\n \n     // collect the blocks beyond max\n     if (collectedBlocks !\u003d null) {\n       for(; n \u003c oldBlocks.length; n++) {\n         collectedBlocks.addDeleteBlock(oldBlocks[n]);\n       }\n     }\n     return size;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null)\n      return 0;\n    // find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length)\n      return size;\n\n    // starting from block n, the data is beyond max.\n    // resize the array.\n    truncateBlocksTo(n);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n      }\n    }\n    return size;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "7e9358feb326d48b8c4f00249e7af5023cebd2e2": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-3107. Introduce truncate. Contributed by Plamen Jeliazkov.",
      "commitDate": "12/01/15 10:50 PM",
      "commitName": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
      "commitAuthor": "Plamen Jeliazkov",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-3107. Introduce truncate. Contributed by Plamen Jeliazkov.",
          "commitDate": "12/01/15 10:50 PM",
          "commitName": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
          "commitAuthor": "Plamen Jeliazkov",
          "commitDateOld": "12/01/15 9:22 PM",
          "commitNameOld": "c4cba6165a3afbf4f1f8ff6b7f11286772d70d6f",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.06,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,34 @@\n-  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n+  public long collectBlocksBeyondMax(final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n-    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        \n-        // set new blocks\n-        file.setBlocks(newBlocks);\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks \u003d\u003d null)\n+      return 0;\n+    //find the minimum n such that the size of the first n blocks \u003e max\n+    int n \u003d 0;\n+    long size \u003d 0;\n+    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+      size +\u003d oldBlocks[n].getNumBytes();\n+    }\n+    if (n \u003e\u003d oldBlocks.length)\n+      return size;\n \n-        // collect the blocks beyond max.  \n-        if (collectedBlocks !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n-          }\n-        }\n+    // starting from block n, the data is beyond max.\n+    // resize the array.  \n+    final BlockInfo[] newBlocks;\n+    if (n \u003d\u003d 0) {\n+      newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n+    } else {\n+      newBlocks \u003d new BlockInfo[n];\n+      System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+    }\n+    // set new blocks\n+    setBlocks(newBlocks);\n+\n+    // collect the blocks beyond max\n+    if (collectedBlocks !\u003d null) {\n+      for(; n \u003c oldBlocks.length; n++) {\n+        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n       }\n     }\n+    return size;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null)\n      return 0;\n    //find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length)\n      return size;\n\n    // starting from block n, the data is beyond max.\n    // resize the array.  \n    final BlockInfo[] newBlocks;\n    if (n \u003d\u003d 0) {\n      newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n    } else {\n      newBlocks \u003d new BlockInfo[n];\n      System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n    }\n    // set new blocks\n    setBlocks(newBlocks);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n      }\n    }\n    return size;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
            "oldMethodName": "collectBlocksBeyondMax",
            "newMethodName": "collectBlocksBeyondMax"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-3107. Introduce truncate. Contributed by Plamen Jeliazkov.",
          "commitDate": "12/01/15 10:50 PM",
          "commitName": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
          "commitAuthor": "Plamen Jeliazkov",
          "commitDateOld": "12/01/15 9:22 PM",
          "commitNameOld": "c4cba6165a3afbf4f1f8ff6b7f11286772d70d6f",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.06,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,34 @@\n-  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n+  public long collectBlocksBeyondMax(final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n-    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        \n-        // set new blocks\n-        file.setBlocks(newBlocks);\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks \u003d\u003d null)\n+      return 0;\n+    //find the minimum n such that the size of the first n blocks \u003e max\n+    int n \u003d 0;\n+    long size \u003d 0;\n+    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+      size +\u003d oldBlocks[n].getNumBytes();\n+    }\n+    if (n \u003e\u003d oldBlocks.length)\n+      return size;\n \n-        // collect the blocks beyond max.  \n-        if (collectedBlocks !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n-          }\n-        }\n+    // starting from block n, the data is beyond max.\n+    // resize the array.  \n+    final BlockInfo[] newBlocks;\n+    if (n \u003d\u003d 0) {\n+      newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n+    } else {\n+      newBlocks \u003d new BlockInfo[n];\n+      System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+    }\n+    // set new blocks\n+    setBlocks(newBlocks);\n+\n+    // collect the blocks beyond max\n+    if (collectedBlocks !\u003d null) {\n+      for(; n \u003c oldBlocks.length; n++) {\n+        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n       }\n     }\n+    return size;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null)\n      return 0;\n    //find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length)\n      return size;\n\n    // starting from block n, the data is beyond max.\n    // resize the array.  \n    final BlockInfo[] newBlocks;\n    if (n \u003d\u003d 0) {\n      newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n    } else {\n      newBlocks \u003d new BlockInfo[n];\n      System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n    }\n    // set new blocks\n    setBlocks(newBlocks);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n      }\n    }\n    return size;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "long"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-3107. Introduce truncate. Contributed by Plamen Jeliazkov.",
          "commitDate": "12/01/15 10:50 PM",
          "commitName": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
          "commitAuthor": "Plamen Jeliazkov",
          "commitDateOld": "12/01/15 9:22 PM",
          "commitNameOld": "c4cba6165a3afbf4f1f8ff6b7f11286772d70d6f",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.06,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,34 @@\n-  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n+  public long collectBlocksBeyondMax(final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n-    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        \n-        // set new blocks\n-        file.setBlocks(newBlocks);\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks \u003d\u003d null)\n+      return 0;\n+    //find the minimum n such that the size of the first n blocks \u003e max\n+    int n \u003d 0;\n+    long size \u003d 0;\n+    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+      size +\u003d oldBlocks[n].getNumBytes();\n+    }\n+    if (n \u003e\u003d oldBlocks.length)\n+      return size;\n \n-        // collect the blocks beyond max.  \n-        if (collectedBlocks !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n-          }\n-        }\n+    // starting from block n, the data is beyond max.\n+    // resize the array.  \n+    final BlockInfo[] newBlocks;\n+    if (n \u003d\u003d 0) {\n+      newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n+    } else {\n+      newBlocks \u003d new BlockInfo[n];\n+      System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+    }\n+    // set new blocks\n+    setBlocks(newBlocks);\n+\n+    // collect the blocks beyond max\n+    if (collectedBlocks !\u003d null) {\n+      for(; n \u003c oldBlocks.length; n++) {\n+        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n       }\n     }\n+    return size;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null)\n      return 0;\n    //find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length)\n      return size;\n\n    // starting from block n, the data is beyond max.\n    // resize the array.  \n    final BlockInfo[] newBlocks;\n    if (n \u003d\u003d 0) {\n      newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n    } else {\n      newBlocks \u003d new BlockInfo[n];\n      System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n    }\n    // set new blocks\n    setBlocks(newBlocks);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n      }\n    }\n    return size;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3107. Introduce truncate. Contributed by Plamen Jeliazkov.",
          "commitDate": "12/01/15 10:50 PM",
          "commitName": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
          "commitAuthor": "Plamen Jeliazkov",
          "commitDateOld": "12/01/15 9:22 PM",
          "commitNameOld": "c4cba6165a3afbf4f1f8ff6b7f11286772d70d6f",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.06,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,34 @@\n-  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n+  public long collectBlocksBeyondMax(final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n-    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        \n-        // set new blocks\n-        file.setBlocks(newBlocks);\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks \u003d\u003d null)\n+      return 0;\n+    //find the minimum n such that the size of the first n blocks \u003e max\n+    int n \u003d 0;\n+    long size \u003d 0;\n+    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+      size +\u003d oldBlocks[n].getNumBytes();\n+    }\n+    if (n \u003e\u003d oldBlocks.length)\n+      return size;\n \n-        // collect the blocks beyond max.  \n-        if (collectedBlocks !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n-          }\n-        }\n+    // starting from block n, the data is beyond max.\n+    // resize the array.  \n+    final BlockInfo[] newBlocks;\n+    if (n \u003d\u003d 0) {\n+      newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n+    } else {\n+      newBlocks \u003d new BlockInfo[n];\n+      System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+    }\n+    // set new blocks\n+    setBlocks(newBlocks);\n+\n+    // collect the blocks beyond max\n+    if (collectedBlocks !\u003d null) {\n+      for(; n \u003c oldBlocks.length; n++) {\n+        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n       }\n     }\n+    return size;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null)\n      return 0;\n    //find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length)\n      return size;\n\n    // starting from block n, the data is beyond max.\n    // resize the array.  \n    final BlockInfo[] newBlocks;\n    if (n \u003d\u003d 0) {\n      newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n    } else {\n      newBlocks \u003d new BlockInfo[n];\n      System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n    }\n    // set new blocks\n    setBlocks(newBlocks);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n      }\n    }\n    return size;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-3107. Introduce truncate. Contributed by Plamen Jeliazkov.",
          "commitDate": "12/01/15 10:50 PM",
          "commitName": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
          "commitAuthor": "Plamen Jeliazkov",
          "commitDateOld": "12/01/15 9:22 PM",
          "commitNameOld": "c4cba6165a3afbf4f1f8ff6b7f11286772d70d6f",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.06,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,34 @@\n-  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n+  public long collectBlocksBeyondMax(final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n-    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        \n-        // set new blocks\n-        file.setBlocks(newBlocks);\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks \u003d\u003d null)\n+      return 0;\n+    //find the minimum n such that the size of the first n blocks \u003e max\n+    int n \u003d 0;\n+    long size \u003d 0;\n+    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+      size +\u003d oldBlocks[n].getNumBytes();\n+    }\n+    if (n \u003e\u003d oldBlocks.length)\n+      return size;\n \n-        // collect the blocks beyond max.  \n-        if (collectedBlocks !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n-          }\n-        }\n+    // starting from block n, the data is beyond max.\n+    // resize the array.  \n+    final BlockInfo[] newBlocks;\n+    if (n \u003d\u003d 0) {\n+      newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n+    } else {\n+      newBlocks \u003d new BlockInfo[n];\n+      System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+    }\n+    // set new blocks\n+    setBlocks(newBlocks);\n+\n+    // collect the blocks beyond max\n+    if (collectedBlocks !\u003d null) {\n+      for(; n \u003c oldBlocks.length; n++) {\n+        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n       }\n     }\n+    return size;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks \u003d\u003d null)\n      return 0;\n    //find the minimum n such that the size of the first n blocks \u003e max\n    int n \u003d 0;\n    long size \u003d 0;\n    for(; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n      size +\u003d oldBlocks[n].getNumBytes();\n    }\n    if (n \u003e\u003d oldBlocks.length)\n      return size;\n\n    // starting from block n, the data is beyond max.\n    // resize the array.  \n    final BlockInfo[] newBlocks;\n    if (n \u003d\u003d 0) {\n      newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n    } else {\n      newBlocks \u003d new BlockInfo[n];\n      System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n    }\n    // set new blocks\n    setBlocks(newBlocks);\n\n    // collect the blocks beyond max\n    if (collectedBlocks !\u003d null) {\n      for(; n \u003c oldBlocks.length; n++) {\n        collectedBlocks.addDeleteBlock(oldBlocks[n]);\n      }\n    }\n    return size;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {
            "oldValue": "[file-INodeFile(modifiers-final), max-long(modifiers-final), collectedBlocks-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[max-long(modifiers-final), collectedBlocks-BlocksMapUpdateInfo(modifiers-final)]"
          }
        }
      ]
    },
    "3f82484218d5694e62ddcb23376d0e4e332aa8b8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6563. NameNode cannot save fsimage in certain circumstances when snapshots are in use. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603712 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/06/14 10:17 PM",
      "commitName": "3f82484218d5694e62ddcb23376d0e4e332aa8b8",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "07/01/14 12:52 PM",
      "commitNameOld": "70cff9e2f0c8f78c1dc54a064182971bb2106795",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 162.35,
      "commitsBetweenForRepo": 1151,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   private void collectBlocksBeyondMax(final INodeFile file, final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n     final BlockInfo[] oldBlocks \u003d file.getBlocks();\n     if (oldBlocks !\u003d null) {\n       //find the minimum n such that the size of the first n blocks \u003e max\n       int n \u003d 0;\n       for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n         size +\u003d oldBlocks[n].getNumBytes();\n       }\n       \n       // starting from block n, the data is beyond max.\n       if (n \u003c oldBlocks.length) {\n         // resize the array.  \n         final BlockInfo[] newBlocks;\n         if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n+          newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n         } else {\n           newBlocks \u003d new BlockInfo[n];\n           System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         \n         // set new blocks\n         file.setBlocks(newBlocks);\n \n         // collect the blocks beyond max.  \n         if (collectedBlocks !\u003d null) {\n           for(; n \u003c oldBlocks.length; n++) {\n             collectedBlocks.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d BlockInfo.EMPTY_ARRAY;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        \n        // set new blocks\n        file.setBlocks(newBlocks);\n\n        // collect the blocks beyond max.  \n        if (collectedBlocks !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
      "extendedDetails": {}
    },
    "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a": {
      "type": "Ymultichange(Yfilerename,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-5554. Flatten INodeFile hierarchy: Replace INodeFileWithSnapshot with FileWithSnapshotFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548796 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/12/13 10:17 PM",
      "commitName": "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-5554. Flatten INodeFile hierarchy: Replace INodeFileWithSnapshot with FileWithSnapshotFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548796 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/13 10:17 PM",
          "commitName": "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "06/12/13 4:11 PM",
          "commitNameOld": "7f059104d293614f3250bd1408874e97f659c92b",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.25,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,33 @@\n-  private void collectBlocksBeyondMax(final long max,\n+  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n     if (oldBlocks !\u003d null) {\n       //find the minimum n such that the size of the first n blocks \u003e max\n       int n \u003d 0;\n       for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n         size +\u003d oldBlocks[n].getNumBytes();\n       }\n       \n       // starting from block n, the data is beyond max.\n       if (n \u003c oldBlocks.length) {\n         // resize the array.  \n         final BlockInfo[] newBlocks;\n         if (n \u003d\u003d 0) {\n           newBlocks \u003d null;\n         } else {\n           newBlocks \u003d new BlockInfo[n];\n           System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         \n         // set new blocks\n-        setBlocks(newBlocks);\n+        file.setBlocks(newBlocks);\n \n         // collect the blocks beyond max.  \n         if (collectedBlocks !\u003d null) {\n           for(; n \u003c oldBlocks.length; n++) {\n             collectedBlocks.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        \n        // set new blocks\n        file.setBlocks(newBlocks);\n\n        // collect the blocks beyond max.  \n        if (collectedBlocks !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5554. Flatten INodeFile hierarchy: Replace INodeFileWithSnapshot with FileWithSnapshotFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548796 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/13 10:17 PM",
          "commitName": "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "06/12/13 4:11 PM",
          "commitNameOld": "7f059104d293614f3250bd1408874e97f659c92b",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.25,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,33 @@\n-  private void collectBlocksBeyondMax(final long max,\n+  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n     if (oldBlocks !\u003d null) {\n       //find the minimum n such that the size of the first n blocks \u003e max\n       int n \u003d 0;\n       for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n         size +\u003d oldBlocks[n].getNumBytes();\n       }\n       \n       // starting from block n, the data is beyond max.\n       if (n \u003c oldBlocks.length) {\n         // resize the array.  \n         final BlockInfo[] newBlocks;\n         if (n \u003d\u003d 0) {\n           newBlocks \u003d null;\n         } else {\n           newBlocks \u003d new BlockInfo[n];\n           System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         \n         // set new blocks\n-        setBlocks(newBlocks);\n+        file.setBlocks(newBlocks);\n \n         // collect the blocks beyond max.  \n         if (collectedBlocks !\u003d null) {\n           for(; n \u003c oldBlocks.length; n++) {\n             collectedBlocks.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        \n        // set new blocks\n        file.setBlocks(newBlocks);\n\n        // collect the blocks beyond max.  \n        if (collectedBlocks !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5554. Flatten INodeFile hierarchy: Replace INodeFileWithSnapshot with FileWithSnapshotFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548796 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/13 10:17 PM",
          "commitName": "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "06/12/13 4:11 PM",
          "commitNameOld": "7f059104d293614f3250bd1408874e97f659c92b",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.25,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,33 @@\n-  private void collectBlocksBeyondMax(final long max,\n+  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n       final BlocksMapUpdateInfo collectedBlocks) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n     if (oldBlocks !\u003d null) {\n       //find the minimum n such that the size of the first n blocks \u003e max\n       int n \u003d 0;\n       for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n         size +\u003d oldBlocks[n].getNumBytes();\n       }\n       \n       // starting from block n, the data is beyond max.\n       if (n \u003c oldBlocks.length) {\n         // resize the array.  \n         final BlockInfo[] newBlocks;\n         if (n \u003d\u003d 0) {\n           newBlocks \u003d null;\n         } else {\n           newBlocks \u003d new BlockInfo[n];\n           System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         \n         // set new blocks\n-        setBlocks(newBlocks);\n+        file.setBlocks(newBlocks);\n \n         // collect the blocks beyond max.  \n         if (collectedBlocks !\u003d null) {\n           for(; n \u003c oldBlocks.length; n++) {\n             collectedBlocks.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMax(final INodeFile file, final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d file.getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        \n        // set new blocks\n        file.setBlocks(newBlocks);\n\n        // collect the blocks beyond max.  \n        if (collectedBlocks !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {
            "oldValue": "[max-long(modifiers-final), collectedBlocks-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[file-INodeFile(modifiers-final), max-long(modifiers-final), collectedBlocks-BlocksMapUpdateInfo(modifiers-final)]"
          }
        }
      ]
    },
    "8df119da214babde03e73243c7ca4cfe6d0ca562": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-5537. Remove FileWithSnapshot interface.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546184 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 12:32 PM",
      "commitName": "8df119da214babde03e73243c7ca4cfe6d0ca562",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-5537. Remove FileWithSnapshot interface.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546184 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 12:32 PM",
          "commitName": "8df119da214babde03e73243c7ca4cfe6d0ca562",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "27/11/13 10:20 AM",
          "commitNameOld": "2214871d916fdcae62aa51afbb5fd571f2808745",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,33 @@\n-    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n-        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n-      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n+  private void collectBlocksBeyondMax(final long max,\n+      final BlocksMapUpdateInfo collectedBlocks) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          \n-          // set new blocks\n-          file.asINodeFile().setBlocks(newBlocks);\n+        // set new blocks\n+        setBlocks(newBlocks);\n \n-          // collect the blocks beyond max.  \n-          if (collectedBlocks !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n-            }\n+        // collect the blocks beyond max.  \n+        if (collectedBlocks !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n-    }\n\\ No newline at end of file\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        \n        // set new blocks\n        setBlocks(newBlocks);\n\n        // collect the blocks beyond max.  \n        if (collectedBlocks !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
            "oldMethodName": "collectBlocksBeyondMax",
            "newMethodName": "collectBlocksBeyondMax"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5537. Remove FileWithSnapshot interface.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546184 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 12:32 PM",
          "commitName": "8df119da214babde03e73243c7ca4cfe6d0ca562",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "27/11/13 10:20 AM",
          "commitNameOld": "2214871d916fdcae62aa51afbb5fd571f2808745",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,33 @@\n-    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n-        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n-      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n+  private void collectBlocksBeyondMax(final long max,\n+      final BlocksMapUpdateInfo collectedBlocks) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          \n-          // set new blocks\n-          file.asINodeFile().setBlocks(newBlocks);\n+        // set new blocks\n+        setBlocks(newBlocks);\n \n-          // collect the blocks beyond max.  \n-          if (collectedBlocks !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n-            }\n+        // collect the blocks beyond max.  \n+        if (collectedBlocks !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n-    }\n\\ No newline at end of file\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        \n        // set new blocks\n        setBlocks(newBlocks);\n\n        // collect the blocks beyond max.  \n        if (collectedBlocks !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[private, static]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5537. Remove FileWithSnapshot interface.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546184 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 12:32 PM",
          "commitName": "8df119da214babde03e73243c7ca4cfe6d0ca562",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "27/11/13 10:20 AM",
          "commitNameOld": "2214871d916fdcae62aa51afbb5fd571f2808745",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,33 @@\n-    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n-        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n-      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n+  private void collectBlocksBeyondMax(final long max,\n+      final BlocksMapUpdateInfo collectedBlocks) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          \n-          // set new blocks\n-          file.asINodeFile().setBlocks(newBlocks);\n+        // set new blocks\n+        setBlocks(newBlocks);\n \n-          // collect the blocks beyond max.  \n-          if (collectedBlocks !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n-            }\n+        // collect the blocks beyond max.  \n+        if (collectedBlocks !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n-    }\n\\ No newline at end of file\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        \n        // set new blocks\n        setBlocks(newBlocks);\n\n        // collect the blocks beyond max.  \n        if (collectedBlocks !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5537. Remove FileWithSnapshot interface.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546184 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 12:32 PM",
          "commitName": "8df119da214babde03e73243c7ca4cfe6d0ca562",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "27/11/13 10:20 AM",
          "commitNameOld": "2214871d916fdcae62aa51afbb5fd571f2808745",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,33 @@\n-    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n-        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n-      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n+  private void collectBlocksBeyondMax(final long max,\n+      final BlocksMapUpdateInfo collectedBlocks) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          \n-          // set new blocks\n-          file.asINodeFile().setBlocks(newBlocks);\n+        // set new blocks\n+        setBlocks(newBlocks);\n \n-          // collect the blocks beyond max.  \n-          if (collectedBlocks !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n-            }\n+        // collect the blocks beyond max.  \n+        if (collectedBlocks !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n-    }\n\\ No newline at end of file\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMax(final long max,\n      final BlocksMapUpdateInfo collectedBlocks) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        \n        // set new blocks\n        setBlocks(newBlocks);\n\n        // collect the blocks beyond max.  \n        if (collectedBlocks !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            collectedBlocks.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[file-FileWithSnapshot(modifiers-final), max-long(modifiers-final), collectedBlocks-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[max-long(modifiers-final), collectedBlocks-BlocksMapUpdateInfo(modifiers-final)]"
          }
        }
      ]
    },
    "afe77ce53d3cf203690aa419e377f26cbd45a96e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4480. Eliminate the file snapshot circular linked list.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1444280 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/02/13 3:19 PM",
      "commitName": "afe77ce53d3cf203690aa419e377f26cbd45a96e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "07/02/13 6:18 PM",
      "commitNameOld": "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.88,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,33 @@\n     private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n         final long max, final BlocksMapUpdateInfo collectedBlocks) {\n       final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n       if (oldBlocks !\u003d null) {\n         //find the minimum n such that the size of the first n blocks \u003e max\n         int n \u003d 0;\n         for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n           size +\u003d oldBlocks[n].getNumBytes();\n         }\n-\n-        // collect update blocks\n-        final FileWithSnapshot next \u003d file.getNext();\n-        if (next !\u003d null \u0026\u0026 next !\u003d file \u0026\u0026 file.isEverythingDeleted() \u0026\u0026 collectedBlocks !\u003d null) {\n-          final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-              file.asINodeFile(), next.asINodeFile());\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            collectedBlocks.addUpdateBlock(oldBlocks[i], entry);\n-          }\n-        }\n         \n         // starting from block n, the data is beyond max.\n         if (n \u003c oldBlocks.length) {\n           // resize the array.  \n           final BlockInfo[] newBlocks;\n           if (n \u003d\u003d 0) {\n             newBlocks \u003d null;\n           } else {\n             newBlocks \u003d new BlockInfo[n];\n             System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n           }\n           \n           // set new blocks\n           file.asINodeFile().setBlocks(newBlocks);\n-          for(FileWithSnapshot i \u003d next; i !\u003d null \u0026\u0026 i !\u003d file; i \u003d i.getNext()) {\n-            i.asINodeFile().setBlocks(newBlocks);\n-          }\n \n           // collect the blocks beyond max.  \n           if (collectedBlocks !\u003d null) {\n             for(; n \u003c oldBlocks.length; n++) {\n               collectedBlocks.addDeleteBlock(oldBlocks[n]);\n             }\n           }\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          \n          // set new blocks\n          file.asINodeFile().setBlocks(newBlocks);\n\n          // collect the blocks beyond max.  \n          if (collectedBlocks !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
      "extendedDetails": {}
    },
    "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-4446. Support file snapshots with diff lists.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1443825 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/02/13 6:18 PM",
      "commitName": "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-4446. Support file snapshots with diff lists.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1443825 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/02/13 6:18 PM",
          "commitName": "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "24/01/13 1:33 PM",
          "commitNameOld": "bb80f2fb29d6f58d9c35f4a1fd88c99517f43e16",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 14.2,
          "commitsBetweenForRepo": 46,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,46 @@\n-    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n-            final long max, final BlocksMapUpdateInfo info) {\n+    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n+        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n       final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n       if (oldBlocks !\u003d null) {\n         //find the minimum n such that the size of the first n blocks \u003e max\n         int n \u003d 0;\n         for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n           size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // Replace the INode for all the remaining blocks in blocksMap\n+        // collect update blocks\n         final FileWithSnapshot next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file.asINodeFile(), next.asINodeFile());\n-        if (info !\u003d null) {\n+        if (next !\u003d null \u0026\u0026 next !\u003d file \u0026\u0026 file.isEverythingDeleted() \u0026\u0026 collectedBlocks !\u003d null) {\n+          final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+              file.asINodeFile(), next.asINodeFile());\n           for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n+            collectedBlocks.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n         \n         // starting from block n, the data is beyond max.\n         if (n \u003c oldBlocks.length) {\n           // resize the array.  \n           final BlockInfo[] newBlocks;\n           if (n \u003d\u003d 0) {\n             newBlocks \u003d null;\n           } else {\n             newBlocks \u003d new BlockInfo[n];\n             System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n           }\n-          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+          \n+          // set new blocks\n+          file.asINodeFile().setBlocks(newBlocks);\n+          for(FileWithSnapshot i \u003d next; i !\u003d null \u0026\u0026 i !\u003d file; i \u003d i.getNext()) {\n             i.asINodeFile().setBlocks(newBlocks);\n           }\n \n           // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n+          if (collectedBlocks !\u003d null) {\n             for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n+              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n             }\n           }\n         }\n-        file.asINodeFile().setBlocks(null);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // collect update blocks\n        final FileWithSnapshot next \u003d file.getNext();\n        if (next !\u003d null \u0026\u0026 next !\u003d file \u0026\u0026 file.isEverythingDeleted() \u0026\u0026 collectedBlocks !\u003d null) {\n          final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n              file.asINodeFile(), next.asINodeFile());\n          for (int i \u003d 0; i \u003c n; i++) {\n            collectedBlocks.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          \n          // set new blocks\n          file.asINodeFile().setBlocks(newBlocks);\n          for(FileWithSnapshot i \u003d next; i !\u003d null \u0026\u0026 i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (collectedBlocks !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "collectBlocksBeyondMaxAndClear",
            "newValue": "collectBlocksBeyondMax"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4446. Support file snapshots with diff lists.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1443825 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/02/13 6:18 PM",
          "commitName": "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "24/01/13 1:33 PM",
          "commitNameOld": "bb80f2fb29d6f58d9c35f4a1fd88c99517f43e16",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 14.2,
          "commitsBetweenForRepo": 46,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,46 @@\n-    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n-            final long max, final BlocksMapUpdateInfo info) {\n+    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n+        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n       final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n       if (oldBlocks !\u003d null) {\n         //find the minimum n such that the size of the first n blocks \u003e max\n         int n \u003d 0;\n         for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n           size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // Replace the INode for all the remaining blocks in blocksMap\n+        // collect update blocks\n         final FileWithSnapshot next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file.asINodeFile(), next.asINodeFile());\n-        if (info !\u003d null) {\n+        if (next !\u003d null \u0026\u0026 next !\u003d file \u0026\u0026 file.isEverythingDeleted() \u0026\u0026 collectedBlocks !\u003d null) {\n+          final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+              file.asINodeFile(), next.asINodeFile());\n           for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n+            collectedBlocks.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n         \n         // starting from block n, the data is beyond max.\n         if (n \u003c oldBlocks.length) {\n           // resize the array.  \n           final BlockInfo[] newBlocks;\n           if (n \u003d\u003d 0) {\n             newBlocks \u003d null;\n           } else {\n             newBlocks \u003d new BlockInfo[n];\n             System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n           }\n-          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+          \n+          // set new blocks\n+          file.asINodeFile().setBlocks(newBlocks);\n+          for(FileWithSnapshot i \u003d next; i !\u003d null \u0026\u0026 i !\u003d file; i \u003d i.getNext()) {\n             i.asINodeFile().setBlocks(newBlocks);\n           }\n \n           // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n+          if (collectedBlocks !\u003d null) {\n             for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n+              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n             }\n           }\n         }\n-        file.asINodeFile().setBlocks(null);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // collect update blocks\n        final FileWithSnapshot next \u003d file.getNext();\n        if (next !\u003d null \u0026\u0026 next !\u003d file \u0026\u0026 file.isEverythingDeleted() \u0026\u0026 collectedBlocks !\u003d null) {\n          final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n              file.asINodeFile(), next.asINodeFile());\n          for (int i \u003d 0; i \u003c n; i++) {\n            collectedBlocks.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          \n          // set new blocks\n          file.asINodeFile().setBlocks(newBlocks);\n          for(FileWithSnapshot i \u003d next; i !\u003d null \u0026\u0026 i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (collectedBlocks !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[file-FileWithSnapshot(modifiers-final), max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[file-FileWithSnapshot(modifiers-final), max-long(modifiers-final), collectedBlocks-BlocksMapUpdateInfo(modifiers-final)]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-4446. Support file snapshots with diff lists.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1443825 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/02/13 6:18 PM",
          "commitName": "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "24/01/13 1:33 PM",
          "commitNameOld": "bb80f2fb29d6f58d9c35f4a1fd88c99517f43e16",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 14.2,
          "commitsBetweenForRepo": 46,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,46 @@\n-    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n-            final long max, final BlocksMapUpdateInfo info) {\n+    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n+        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n       final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n       if (oldBlocks !\u003d null) {\n         //find the minimum n such that the size of the first n blocks \u003e max\n         int n \u003d 0;\n         for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n           size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // Replace the INode for all the remaining blocks in blocksMap\n+        // collect update blocks\n         final FileWithSnapshot next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file.asINodeFile(), next.asINodeFile());\n-        if (info !\u003d null) {\n+        if (next !\u003d null \u0026\u0026 next !\u003d file \u0026\u0026 file.isEverythingDeleted() \u0026\u0026 collectedBlocks !\u003d null) {\n+          final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+              file.asINodeFile(), next.asINodeFile());\n           for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n+            collectedBlocks.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n         \n         // starting from block n, the data is beyond max.\n         if (n \u003c oldBlocks.length) {\n           // resize the array.  \n           final BlockInfo[] newBlocks;\n           if (n \u003d\u003d 0) {\n             newBlocks \u003d null;\n           } else {\n             newBlocks \u003d new BlockInfo[n];\n             System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n           }\n-          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+          \n+          // set new blocks\n+          file.asINodeFile().setBlocks(newBlocks);\n+          for(FileWithSnapshot i \u003d next; i !\u003d null \u0026\u0026 i !\u003d file; i \u003d i.getNext()) {\n             i.asINodeFile().setBlocks(newBlocks);\n           }\n \n           // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n+          if (collectedBlocks !\u003d null) {\n             for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n+              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n             }\n           }\n         }\n-        file.asINodeFile().setBlocks(null);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // collect update blocks\n        final FileWithSnapshot next \u003d file.getNext();\n        if (next !\u003d null \u0026\u0026 next !\u003d file \u0026\u0026 file.isEverythingDeleted() \u0026\u0026 collectedBlocks !\u003d null) {\n          final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n              file.asINodeFile(), next.asINodeFile());\n          for (int i \u003d 0; i \u003c n; i++) {\n            collectedBlocks.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          \n          // set new blocks\n          file.asINodeFile().setBlocks(newBlocks);\n          for(FileWithSnapshot i \u003d next; i !\u003d null \u0026\u0026 i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (collectedBlocks !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[static]",
            "newValue": "[private, static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4446. Support file snapshots with diff lists.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1443825 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/02/13 6:18 PM",
          "commitName": "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "24/01/13 1:33 PM",
          "commitNameOld": "bb80f2fb29d6f58d9c35f4a1fd88c99517f43e16",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 14.2,
          "commitsBetweenForRepo": 46,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,46 @@\n-    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n-            final long max, final BlocksMapUpdateInfo info) {\n+    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n+        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n       final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n       if (oldBlocks !\u003d null) {\n         //find the minimum n such that the size of the first n blocks \u003e max\n         int n \u003d 0;\n         for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n           size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // Replace the INode for all the remaining blocks in blocksMap\n+        // collect update blocks\n         final FileWithSnapshot next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file.asINodeFile(), next.asINodeFile());\n-        if (info !\u003d null) {\n+        if (next !\u003d null \u0026\u0026 next !\u003d file \u0026\u0026 file.isEverythingDeleted() \u0026\u0026 collectedBlocks !\u003d null) {\n+          final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+              file.asINodeFile(), next.asINodeFile());\n           for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n+            collectedBlocks.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n         \n         // starting from block n, the data is beyond max.\n         if (n \u003c oldBlocks.length) {\n           // resize the array.  \n           final BlockInfo[] newBlocks;\n           if (n \u003d\u003d 0) {\n             newBlocks \u003d null;\n           } else {\n             newBlocks \u003d new BlockInfo[n];\n             System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n           }\n-          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+          \n+          // set new blocks\n+          file.asINodeFile().setBlocks(newBlocks);\n+          for(FileWithSnapshot i \u003d next; i !\u003d null \u0026\u0026 i !\u003d file; i \u003d i.getNext()) {\n             i.asINodeFile().setBlocks(newBlocks);\n           }\n \n           // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n+          if (collectedBlocks !\u003d null) {\n             for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n+              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n             }\n           }\n         }\n-        file.asINodeFile().setBlocks(null);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private static void collectBlocksBeyondMax(final FileWithSnapshot file,\n        final long max, final BlocksMapUpdateInfo collectedBlocks) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // collect update blocks\n        final FileWithSnapshot next \u003d file.getNext();\n        if (next !\u003d null \u0026\u0026 next !\u003d file \u0026\u0026 file.isEverythingDeleted() \u0026\u0026 collectedBlocks !\u003d null) {\n          final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n              file.asINodeFile(), next.asINodeFile());\n          for (int i \u003d 0; i \u003c n; i++) {\n            collectedBlocks.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          \n          // set new blocks\n          file.asINodeFile().setBlocks(newBlocks);\n          for(FileWithSnapshot i \u003d next; i !\u003d null \u0026\u0026 i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (collectedBlocks !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              collectedBlocks.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {}
        }
      ]
    },
    "b71d3868908a49c1b2e353afea795a76dfb20f7d": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-4098. Add FileWithSnapshot, INodeFileUnderConstructionWithSnapshot and INodeFileUnderConstructionSnapshot for supporting append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1434966 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/01/13 3:38 PM",
      "commitName": "b71d3868908a49c1b2e353afea795a76dfb20f7d",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-4098. Add FileWithSnapshot, INodeFileUnderConstructionWithSnapshot and INodeFileUnderConstructionSnapshot for supporting append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1434966 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/01/13 3:38 PM",
          "commitName": "b71d3868908a49c1b2e353afea795a76dfb20f7d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/01/13 4:43 AM",
          "commitNameOld": "7856221d4a4701565bb21259d839c8c402e72f49",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final FileWithSnapshot next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file.asINodeFile(), next.asINodeFile());\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.asINodeFile().setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.asINodeFile().setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final FileWithSnapshot next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file.asINodeFile(), next.asINodeFile());\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
            "oldMethodName": "collectBlocksBeyondMaxAndClear",
            "newMethodName": "collectBlocksBeyondMaxAndClear"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-4098. Add FileWithSnapshot, INodeFileUnderConstructionWithSnapshot and INodeFileUnderConstructionSnapshot for supporting append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1434966 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/01/13 3:38 PM",
          "commitName": "b71d3868908a49c1b2e353afea795a76dfb20f7d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/01/13 4:43 AM",
          "commitNameOld": "7856221d4a4701565bb21259d839c8c402e72f49",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final FileWithSnapshot next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file.asINodeFile(), next.asINodeFile());\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.asINodeFile().setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.asINodeFile().setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final FileWithSnapshot next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file.asINodeFile(), next.asINodeFile());\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4098. Add FileWithSnapshot, INodeFileUnderConstructionWithSnapshot and INodeFileUnderConstructionSnapshot for supporting append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1434966 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/01/13 3:38 PM",
          "commitName": "b71d3868908a49c1b2e353afea795a76dfb20f7d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/01/13 4:43 AM",
          "commitNameOld": "7856221d4a4701565bb21259d839c8c402e72f49",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final FileWithSnapshot next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file.asINodeFile(), next.asINodeFile());\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.asINodeFile().setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.asINodeFile().setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final FileWithSnapshot next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file.asINodeFile(), next.asINodeFile());\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4098. Add FileWithSnapshot, INodeFileUnderConstructionWithSnapshot and INodeFileUnderConstructionSnapshot for supporting append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1434966 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/01/13 3:38 PM",
          "commitName": "b71d3868908a49c1b2e353afea795a76dfb20f7d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/01/13 4:43 AM",
          "commitNameOld": "7856221d4a4701565bb21259d839c8c402e72f49",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final FileWithSnapshot next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file.asINodeFile(), next.asINodeFile());\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.asINodeFile().setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.asINodeFile().setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final FileWithSnapshot next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file.asINodeFile(), next.asINodeFile());\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[file-FileWithSnapshot(modifiers-final), max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]"
          }
        }
      ]
    },
    "397835acdf66cf48ebdbc256aa15b6660181c339": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "svn merge -c -1432788 for reverting HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1433284 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/01/13 8:33 PM",
      "commitName": "397835acdf66cf48ebdbc256aa15b6660181c339",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "svn merge -c -1432788 for reverting HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1433284 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/01/13 8:33 PM",
          "commitName": "397835acdf66cf48ebdbc256aa15b6660181c339",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/01/13 12:40 AM",
          "commitNameOld": "686e13db2fdb1cb7b8d0cc55a677b25df420156d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.83,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,43 @@\n-        void collectBlocksBeyondMaxAndClear(final F file,\n-            final long max, final BlocksMapUpdateInfo info) {\n-      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n-        }\n-\n-        // Replace the INode for all the remaining blocks in blocksMap\n-        final N next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file, next);\n-        if (info !\u003d null) {\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n-          }\n-        }\n-        \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-            i.setBlocks(newBlocks);\n-          }\n-\n-          // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n-            }\n-          }\n-        }\n-        file.setBlocks(null);\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n       }\n-    }\n\\ No newline at end of file\n+\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+        }\n+        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n+          i.setBlocks(newBlocks);\n+        }\n+\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            info.addDeleteBlock(oldBlocks[n]);\n+          }\n+        }\n+      }\n+      setBlocks(null);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
            "oldMethodName": "collectBlocksBeyondMaxAndClear",
            "newMethodName": "collectBlocksBeyondMaxAndClear"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "svn merge -c -1432788 for reverting HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1433284 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/01/13 8:33 PM",
          "commitName": "397835acdf66cf48ebdbc256aa15b6660181c339",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/01/13 12:40 AM",
          "commitNameOld": "686e13db2fdb1cb7b8d0cc55a677b25df420156d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.83,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,43 @@\n-        void collectBlocksBeyondMaxAndClear(final F file,\n-            final long max, final BlocksMapUpdateInfo info) {\n-      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n-        }\n-\n-        // Replace the INode for all the remaining blocks in blocksMap\n-        final N next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file, next);\n-        if (info !\u003d null) {\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n-          }\n-        }\n-        \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-            i.setBlocks(newBlocks);\n-          }\n-\n-          // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n-            }\n-          }\n-        }\n-        file.setBlocks(null);\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n       }\n-    }\n\\ No newline at end of file\n+\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+        }\n+        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n+          i.setBlocks(newBlocks);\n+        }\n+\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            info.addDeleteBlock(oldBlocks[n]);\n+          }\n+        }\n+      }\n+      setBlocks(null);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {
            "oldValue": "[static]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "svn merge -c -1432788 for reverting HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1433284 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/01/13 8:33 PM",
          "commitName": "397835acdf66cf48ebdbc256aa15b6660181c339",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/01/13 12:40 AM",
          "commitNameOld": "686e13db2fdb1cb7b8d0cc55a677b25df420156d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.83,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,43 @@\n-        void collectBlocksBeyondMaxAndClear(final F file,\n-            final long max, final BlocksMapUpdateInfo info) {\n-      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n-        }\n-\n-        // Replace the INode for all the remaining blocks in blocksMap\n-        final N next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file, next);\n-        if (info !\u003d null) {\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n-          }\n-        }\n-        \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-            i.setBlocks(newBlocks);\n-          }\n-\n-          // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n-            }\n-          }\n-        }\n-        file.setBlocks(null);\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n       }\n-    }\n\\ No newline at end of file\n+\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+        }\n+        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n+          i.setBlocks(newBlocks);\n+        }\n+\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            info.addDeleteBlock(oldBlocks[n]);\n+          }\n+        }\n+      }\n+      setBlocks(null);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "svn merge -c -1432788 for reverting HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1433284 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/01/13 8:33 PM",
          "commitName": "397835acdf66cf48ebdbc256aa15b6660181c339",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/01/13 12:40 AM",
          "commitNameOld": "686e13db2fdb1cb7b8d0cc55a677b25df420156d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.83,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,43 @@\n-        void collectBlocksBeyondMaxAndClear(final F file,\n-            final long max, final BlocksMapUpdateInfo info) {\n-      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n-        }\n-\n-        // Replace the INode for all the remaining blocks in blocksMap\n-        final N next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file, next);\n-        if (info !\u003d null) {\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n-          }\n-        }\n-        \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-            i.setBlocks(newBlocks);\n-          }\n-\n-          // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n-            }\n-          }\n-        }\n-        file.setBlocks(null);\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n       }\n-    }\n\\ No newline at end of file\n+\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+        }\n+        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n+          i.setBlocks(newBlocks);\n+        }\n+\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            info.addDeleteBlock(oldBlocks[n]);\n+          }\n+        }\n+      }\n+      setBlocks(null);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {
            "oldValue": "[file-F(modifiers-final), max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]"
          }
        }
      ]
    },
    "25116c26fd9b998025fa28666ae45ab03a995d91": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1432788 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/01/13 6:30 PM",
      "commitName": "25116c26fd9b998025fa28666ae45ab03a995d91",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1432788 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/01/13 6:30 PM",
          "commitName": "25116c26fd9b998025fa28666ae45ab03a995d91",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "13/01/13 3:29 AM",
          "commitNameOld": "bc0aff27a4b781b3af9603251b7e09b43e66368c",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.63,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+        void collectBlocksBeyondMaxAndClear(final F file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final N next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file, next);\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "        void collectBlocksBeyondMaxAndClear(final F file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final N next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file, next);\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
            "oldMethodName": "collectBlocksBeyondMaxAndClear",
            "newMethodName": "collectBlocksBeyondMaxAndClear"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1432788 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/01/13 6:30 PM",
          "commitName": "25116c26fd9b998025fa28666ae45ab03a995d91",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "13/01/13 3:29 AM",
          "commitNameOld": "bc0aff27a4b781b3af9603251b7e09b43e66368c",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.63,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+        void collectBlocksBeyondMaxAndClear(final F file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final N next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file, next);\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "        void collectBlocksBeyondMaxAndClear(final F file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final N next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file, next);\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1432788 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/01/13 6:30 PM",
          "commitName": "25116c26fd9b998025fa28666ae45ab03a995d91",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "13/01/13 3:29 AM",
          "commitNameOld": "bc0aff27a4b781b3af9603251b7e09b43e66368c",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.63,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+        void collectBlocksBeyondMaxAndClear(final F file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final N next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file, next);\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "        void collectBlocksBeyondMaxAndClear(final F file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final N next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file, next);\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1432788 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/01/13 6:30 PM",
          "commitName": "25116c26fd9b998025fa28666ae45ab03a995d91",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "13/01/13 3:29 AM",
          "commitNameOld": "bc0aff27a4b781b3af9603251b7e09b43e66368c",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.63,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+        void collectBlocksBeyondMaxAndClear(final F file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final N next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file, next);\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "        void collectBlocksBeyondMaxAndClear(final F file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final N next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file, next);\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
          "extendedDetails": {
            "oldValue": "[max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[file-F(modifiers-final), max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]"
          }
        }
      ]
    },
    "8a577a16f96437d87ad764dedbdc67d4c184b8d9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4150.  Update the inode in the block map when a snapshotted file or a snapshot file is deleted. Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1406763 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/11/12 11:30 AM",
      "commitName": "8a577a16f96437d87ad764dedbdc67d4c184b8d9",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4150.  Update the inode in the block map when a snapshotted file or a snapshot file is deleted. Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1406763 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/11/12 11:30 AM",
          "commitName": "8a577a16f96437d87ad764dedbdc67d4c184b8d9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "04/11/12 2:00 PM",
          "commitNameOld": "deaf979d4122a0a0e4ae0557abbb7f17d18a9380",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.9,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,43 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n     final BlockInfo[] oldBlocks \u003d getBlocks();\n     if (oldBlocks !\u003d null) {\n       //find the minimum n such that the size of the first n blocks \u003e max\n       int n \u003d 0;\n       for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n         size +\u003d oldBlocks[n].getNumBytes();\n       }\n \n-      //starting from block n, the data is beyond max.\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n       if (n \u003c oldBlocks.length) {\n-        //resize the array.  \n+        // resize the array.  \n         final BlockInfo[] newBlocks;\n         if (n \u003d\u003d 0) {\n           newBlocks \u003d null;\n         } else {\n           newBlocks \u003d new BlockInfo[n];\n           System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n           i.setBlocks(newBlocks);\n         }\n \n-        //collect the blocks beyond max.  \n-        if (v !\u003d null) {\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n           for(; n \u003c oldBlocks.length; n++) {\n-            v.add(oldBlocks[n]);\n+            info.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n       setBlocks(null);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {
            "oldValue": "[max-long(modifiers-final), v-List\u003cBlock\u003e(modifiers-final)]",
            "newValue": "[max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4150.  Update the inode in the block map when a snapshotted file or a snapshot file is deleted. Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1406763 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/11/12 11:30 AM",
          "commitName": "8a577a16f96437d87ad764dedbdc67d4c184b8d9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "04/11/12 2:00 PM",
          "commitNameOld": "deaf979d4122a0a0e4ae0557abbb7f17d18a9380",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.9,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,43 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n     final BlockInfo[] oldBlocks \u003d getBlocks();\n     if (oldBlocks !\u003d null) {\n       //find the minimum n such that the size of the first n blocks \u003e max\n       int n \u003d 0;\n       for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n         size +\u003d oldBlocks[n].getNumBytes();\n       }\n \n-      //starting from block n, the data is beyond max.\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n       if (n \u003c oldBlocks.length) {\n-        //resize the array.  \n+        // resize the array.  \n         final BlockInfo[] newBlocks;\n         if (n \u003d\u003d 0) {\n           newBlocks \u003d null;\n         } else {\n           newBlocks \u003d new BlockInfo[n];\n           System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n           i.setBlocks(newBlocks);\n         }\n \n-        //collect the blocks beyond max.  \n-        if (v !\u003d null) {\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n           for(; n \u003c oldBlocks.length; n++) {\n-            v.add(oldBlocks[n]);\n+            info.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n       setBlocks(null);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {}
        }
      ]
    },
    "deaf979d4122a0a0e4ae0557abbb7f17d18a9380": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4146. Use getter and setter in INodeFileWithLink to access blocks and initialize root directory as snapshottable.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1405648 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/11/12 2:00 PM",
      "commitName": "deaf979d4122a0a0e4ae0557abbb7f17d18a9380",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "02/11/12 6:31 PM",
      "commitNameOld": "e5a7b3d4307951f0574c2341178e3cd95e69f7b7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.85,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,33 @@\n   private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n-    if (blocks !\u003d null) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n       //find the minimum n such that the size of the first n blocks \u003e max\n       int n \u003d 0;\n-      for(long size \u003d 0; n \u003c blocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d blocks[n].getNumBytes();\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n       }\n \n-      //starting from block[n], the data is beyond max.\n-      if (n \u003c blocks.length) {\n+      //starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n         //resize the array.  \n         final BlockInfo[] newBlocks;\n         if (n \u003d\u003d 0) {\n           newBlocks \u003d null;\n         } else {\n           newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(blocks, 0, newBlocks, 0, n);\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.blocks \u003d newBlocks;\n+          i.setBlocks(newBlocks);\n         }\n \n         //collect the blocks beyond max.  \n         if (v !\u003d null) {\n-          for(; n \u003c blocks.length; n++) {\n-            v.add(blocks[n]);\n+          for(; n \u003c oldBlocks.length; n++) {\n+            v.add(oldBlocks[n]);\n           }\n         }\n       }\n-      blocks \u003d null;\n+      setBlocks(null);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      //starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        //resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        //collect the blocks beyond max.  \n        if (v !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            v.add(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
      "extendedDetails": {}
    },
    "719279ea8a510ba8d04174ac85ad42fa991725a2": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4092. Update file deletion logic for snapshot so that the current inode is removed from the circular linked list; and if some blocks at the end of the block list no longer belong to any other inode, collect them and update the block list.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1402287 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/10/12 12:32 PM",
      "commitName": "719279ea8a510ba8d04174ac85ad42fa991725a2",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,32 @@\n+  private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n+    if (blocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c blocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d blocks[n].getNumBytes();\n+      }\n+\n+      //starting from block[n], the data is beyond max.\n+      if (n \u003c blocks.length) {\n+        //resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(blocks, 0, newBlocks, 0, n);\n+        }\n+        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n+          i.blocks \u003d newBlocks;\n+        }\n+\n+        //collect the blocks beyond max.  \n+        if (v !\u003d null) {\n+          for(; n \u003c blocks.length; n++) {\n+            v.add(blocks[n]);\n+          }\n+        }\n+      }\n+      blocks \u003d null;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n    if (blocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c blocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d blocks[n].getNumBytes();\n      }\n\n      //starting from block[n], the data is beyond max.\n      if (n \u003c blocks.length) {\n        //resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(blocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.blocks \u003d newBlocks;\n        }\n\n        //collect the blocks beyond max.  \n        if (v !\u003d null) {\n          for(; n \u003c blocks.length; n++) {\n            v.add(blocks[n]);\n          }\n        }\n      }\n      blocks \u003d null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java"
    }
  }
}