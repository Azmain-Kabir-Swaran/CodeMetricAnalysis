{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "TransferFsImage.java",
  "functionName": "doGetUrl",
  "functionId": "doGetUrl___url-URL__localPaths-List__File____dstStorage-Storage__getChecksum-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java",
  "functionStartLine": 439,
  "functionEndLine": 443,
  "numCommitsSeen": 45,
  "timeTaken": 3137,
  "changeHistory": [
    "13d4bcfe3535a2df79c2a56e7578716d15497ff4",
    "7ec609b28989303fe0cc36812f225028b0251b32",
    "94a1632fcb677fda6f4d812614026417f1d0a360",
    "dbd22b23c2d68b97b4da47215897906f06f978e3",
    "0f595915a388305edbb3ce928415571811d304e8",
    "d8a23834614581a292aad214dddcbcc4bbe86d27",
    "4909821aa97f5c1343db5c2d9b200853eb05ec77",
    "31142aa8927568b7901eb48d80ba04d3e0080f60",
    "4a5ba3b7bd2360fd9605863630b477d362874e1e",
    "8dd3148e734fa9d1db761ce65410fdc49c0fe1d5"
  ],
  "changeHistoryShort": {
    "13d4bcfe3535a2df79c2a56e7578716d15497ff4": "Ybodychange",
    "7ec609b28989303fe0cc36812f225028b0251b32": "Ybodychange",
    "94a1632fcb677fda6f4d812614026417f1d0a360": "Ybodychange",
    "dbd22b23c2d68b97b4da47215897906f06f978e3": "Ybodychange",
    "0f595915a388305edbb3ce928415571811d304e8": "Ybodychange",
    "d8a23834614581a292aad214dddcbcc4bbe86d27": "Ybodychange",
    "4909821aa97f5c1343db5c2d9b200853eb05ec77": "Ybodychange",
    "31142aa8927568b7901eb48d80ba04d3e0080f60": "Ybodychange",
    "4a5ba3b7bd2360fd9605863630b477d362874e1e": "Ybodychange",
    "8dd3148e734fa9d1db761ce65410fdc49c0fe1d5": "Yintroduced"
  },
  "changeHistoryDetails": {
    "13d4bcfe3535a2df79c2a56e7578716d15497ff4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4025. QJM: Sychronize past log segments to JNs that missed them. Contributed by Hanisha Koneru.\n",
      "commitDate": "22/02/17 4:33 PM",
      "commitName": "13d4bcfe3535a2df79c2a56e7578716d15497ff4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "09/01/17 6:05 PM",
      "commitNameOld": "7ec609b28989303fe0cc36812f225028b0251b32",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 43.94,
      "commitsBetweenForRepo": 221,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,4 +1,5 @@\n   public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n       Storage dstStorage, boolean getChecksum) throws IOException {\n-    return Util.doGetUrl(url, localPaths, dstStorage, getChecksum, timeout);\n+    return Util.doGetUrl(url, localPaths, dstStorage, getChecksum, timeout,\n+        null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n      Storage dstStorage, boolean getChecksum) throws IOException {\n    return Util.doGetUrl(url, localPaths, dstStorage, getChecksum, timeout,\n        null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java",
      "extendedDetails": {}
    },
    "7ec609b28989303fe0cc36812f225028b0251b32": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11273. Move TransferFsImage#doGetUrl function to a Util class. Contributed by Hanisha Koneru.\n",
      "commitDate": "09/01/17 6:05 PM",
      "commitName": "7ec609b28989303fe0cc36812f225028b0251b32",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/10/16 4:09 PM",
      "commitNameOld": "5877f20f9c3f6f0afa505715e9a2ee312475af17",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 74.12,
      "commitsBetweenForRepo": 452,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,4 @@\n   public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n       Storage dstStorage, boolean getChecksum) throws IOException {\n-    HttpURLConnection connection;\n-    try {\n-      connection \u003d (HttpURLConnection)\n-        connectionFactory.openConnection(url, isSpnegoEnabled);\n-    } catch (AuthenticationException e) {\n-      throw new IOException(e);\n-    }\n-\n-    setTimeout(connection);\n-\n-    if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n-      throw new HttpGetFailedException(\n-          \"Image transfer servlet at \" + url +\n-          \" failed with status code \" + connection.getResponseCode() +\n-          \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n-          connection);\n-    }\n-    \n-    long advertisedSize;\n-    String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n-    if (contentLength !\u003d null) {\n-      advertisedSize \u003d Long.parseLong(contentLength);\n-    } else {\n-      throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n-                            \"by the namenode when trying to fetch \" + url);\n-    }\n-    MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n-    String fsImageName \u003d connection\n-        .getHeaderField(ImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n-    InputStream stream \u003d connection.getInputStream();\n-\n-    return receiveFile(url.toExternalForm(), localPaths, dstStorage,\n-        getChecksum, advertisedSize, advertisedDigest, fsImageName, stream,\n-        null);\n+    return Util.doGetUrl(url, localPaths, dstStorage, getChecksum, timeout);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n      Storage dstStorage, boolean getChecksum) throws IOException {\n    return Util.doGetUrl(url, localPaths, dstStorage, getChecksum, timeout);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java",
      "extendedDetails": {}
    },
    "94a1632fcb677fda6f4d812614026417f1d0a360": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3405. Checkpointing should use HTTP POST or PUT instead of GET-GET to send merged fsimages. Contributed by Vinayakumar B.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1575611 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/03/14 1:25 PM",
      "commitName": "94a1632fcb677fda6f4d812614026417f1d0a360",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "08/03/14 1:15 PM",
      "commitNameOld": "dbd22b23c2d68b97b4da47215897906f06f978e3",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,144 +1,37 @@\n   public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n       Storage dstStorage, boolean getChecksum) throws IOException {\n-    long startTime \u003d Time.monotonicNow();\n     HttpURLConnection connection;\n     try {\n       connection \u003d (HttpURLConnection)\n         connectionFactory.openConnection(url, isSpnegoEnabled);\n     } catch (AuthenticationException e) {\n       throw new IOException(e);\n     }\n \n-    if (timeout \u003c\u003d 0) {\n-      Configuration conf \u003d new HdfsConfiguration();\n-      timeout \u003d conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY,\n-          DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);\n-    }\n-\n-    if (timeout \u003e 0) {\n-      connection.setConnectTimeout(timeout);\n-      connection.setReadTimeout(timeout);\n-    }\n+    setTimeout(connection);\n \n     if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n       throw new HttpGetFailedException(\n           \"Image transfer servlet at \" + url +\n           \" failed with status code \" + connection.getResponseCode() +\n           \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n           connection);\n     }\n     \n     long advertisedSize;\n     String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n     if (contentLength !\u003d null) {\n       advertisedSize \u003d Long.parseLong(contentLength);\n     } else {\n       throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                             \"by the namenode when trying to fetch \" + url);\n     }\n-    \n-    if (localPaths !\u003d null) {\n-      String fsImageName \u003d connection.getHeaderField(\n-          GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n-      // If the local paths refer to directories, use the server-provided header\n-      // as the filename within that directory\n-      List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n-      for (File localPath : localPaths) {\n-        if (localPath.isDirectory()) {\n-          if (fsImageName \u003d\u003d null) {\n-            throw new IOException(\"No filename header provided by server\");\n-          }\n-          newLocalPaths.add(new File(localPath, fsImageName));\n-        } else {\n-          newLocalPaths.add(localPath);\n-        }\n-      }\n-      localPaths \u003d newLocalPaths;\n-    }\n-    \n     MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n-\n-    long received \u003d 0;\n+    String fsImageName \u003d connection\n+        .getHeaderField(ImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n     InputStream stream \u003d connection.getInputStream();\n-    MessageDigest digester \u003d null;\n-    if (getChecksum) {\n-      digester \u003d MD5Hash.getDigester();\n-      stream \u003d new DigestInputStream(stream, digester);\n-    }\n-    boolean finishedReceiving \u003d false;\n \n-    List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n-\n-    try {\n-      if (localPaths !\u003d null) {\n-        for (File f : localPaths) {\n-          try {\n-            if (f.exists()) {\n-              LOG.warn(\"Overwriting existing file \" + f\n-                  + \" with file downloaded from \" + url);\n-            }\n-            outputStreams.add(new FileOutputStream(f));\n-          } catch (IOException ioe) {\n-            LOG.warn(\"Unable to download file \" + f, ioe);\n-            // This will be null if we\u0027re downloading the fsimage to a file\n-            // outside of an NNStorage directory.\n-            if (dstStorage !\u003d null \u0026\u0026\n-                (dstStorage instanceof StorageErrorReporter)) {\n-              ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n-            }\n-          }\n-        }\n-        \n-        if (outputStreams.isEmpty()) {\n-          throw new IOException(\n-              \"Unable to download to any storage directory\");\n-        }\n-      }\n-      \n-      int num \u003d 1;\n-      byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n-      while (num \u003e 0) {\n-        num \u003d stream.read(buf);\n-        if (num \u003e 0) {\n-          received +\u003d num;\n-          for (FileOutputStream fos : outputStreams) {\n-            fos.write(buf, 0, num);\n-          }\n-        }\n-      }\n-      finishedReceiving \u003d true;\n-    } finally {\n-      stream.close();\n-      for (FileOutputStream fos : outputStreams) {\n-        fos.getChannel().force(true);\n-        fos.close();\n-      }\n-      if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n-        // only throw this exception if we think we read all of it on our end\n-        // -- otherwise a client-side IOException would be masked by this\n-        // exception that makes it look like a server-side problem!\n-        throw new IOException(\"File \" + url + \" received length \" + received +\n-                              \" is not of the advertised size \" +\n-                              advertisedSize);\n-      }\n-    }\n-    double xferSec \u003d Math.max(\n-        ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n-    long xferKb \u003d received / 1024;\n-    LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n-        xferSec, xferKb / xferSec));\n-\n-    if (digester !\u003d null) {\n-      MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n-      \n-      if (advertisedDigest !\u003d null \u0026\u0026\n-          !computedDigest.equals(advertisedDigest)) {\n-        throw new IOException(\"File \" + url + \" computed digest \" +\n-            computedDigest + \" does not match advertised digest \" + \n-            advertisedDigest);\n-      }\n-      return computedDigest;\n-    } else {\n-      return null;\n-    }    \n+    return receiveFile(url.toExternalForm(), localPaths, dstStorage,\n+        getChecksum, advertisedSize, advertisedDigest, fsImageName, stream,\n+        null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n      Storage dstStorage, boolean getChecksum) throws IOException {\n    HttpURLConnection connection;\n    try {\n      connection \u003d (HttpURLConnection)\n        connectionFactory.openConnection(url, isSpnegoEnabled);\n    } catch (AuthenticationException e) {\n      throw new IOException(e);\n    }\n\n    setTimeout(connection);\n\n    if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n      throw new HttpGetFailedException(\n          \"Image transfer servlet at \" + url +\n          \" failed with status code \" + connection.getResponseCode() +\n          \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n          connection);\n    }\n    \n    long advertisedSize;\n    String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n    if (contentLength !\u003d null) {\n      advertisedSize \u003d Long.parseLong(contentLength);\n    } else {\n      throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                            \"by the namenode when trying to fetch \" + url);\n    }\n    MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n    String fsImageName \u003d connection\n        .getHeaderField(ImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n    InputStream stream \u003d connection.getInputStream();\n\n    return receiveFile(url.toExternalForm(), localPaths, dstStorage,\n        getChecksum, advertisedSize, advertisedDigest, fsImageName, stream,\n        null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java",
      "extendedDetails": {}
    },
    "dbd22b23c2d68b97b4da47215897906f06f978e3": {
      "type": "Ybodychange",
      "commitMessage": "Revert HDFS-3405 for recommit with correct renamed files\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1575610 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/03/14 1:15 PM",
      "commitName": "dbd22b23c2d68b97b4da47215897906f06f978e3",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "07/03/14 4:39 PM",
      "commitNameOld": "0f595915a388305edbb3ce928415571811d304e8",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.86,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,144 @@\n   public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n       Storage dstStorage, boolean getChecksum) throws IOException {\n+    long startTime \u003d Time.monotonicNow();\n     HttpURLConnection connection;\n     try {\n       connection \u003d (HttpURLConnection)\n         connectionFactory.openConnection(url, isSpnegoEnabled);\n     } catch (AuthenticationException e) {\n       throw new IOException(e);\n     }\n \n-    setTimeout(connection);\n+    if (timeout \u003c\u003d 0) {\n+      Configuration conf \u003d new HdfsConfiguration();\n+      timeout \u003d conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY,\n+          DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);\n+    }\n+\n+    if (timeout \u003e 0) {\n+      connection.setConnectTimeout(timeout);\n+      connection.setReadTimeout(timeout);\n+    }\n \n     if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n       throw new HttpGetFailedException(\n           \"Image transfer servlet at \" + url +\n           \" failed with status code \" + connection.getResponseCode() +\n           \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n           connection);\n     }\n     \n     long advertisedSize;\n     String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n     if (contentLength !\u003d null) {\n       advertisedSize \u003d Long.parseLong(contentLength);\n     } else {\n       throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                             \"by the namenode when trying to fetch \" + url);\n     }\n+    \n+    if (localPaths !\u003d null) {\n+      String fsImageName \u003d connection.getHeaderField(\n+          GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n+      // If the local paths refer to directories, use the server-provided header\n+      // as the filename within that directory\n+      List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n+      for (File localPath : localPaths) {\n+        if (localPath.isDirectory()) {\n+          if (fsImageName \u003d\u003d null) {\n+            throw new IOException(\"No filename header provided by server\");\n+          }\n+          newLocalPaths.add(new File(localPath, fsImageName));\n+        } else {\n+          newLocalPaths.add(localPath);\n+        }\n+      }\n+      localPaths \u003d newLocalPaths;\n+    }\n+    \n     MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n-    String fsImageName \u003d connection\n-        .getHeaderField(ImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n-    InputStream stream \u003d connection.getInputStream();\n \n-    return receiveFile(url.toExternalForm(), localPaths, dstStorage,\n-        getChecksum, advertisedSize, advertisedDigest, fsImageName, stream,\n-        null);\n+    long received \u003d 0;\n+    InputStream stream \u003d connection.getInputStream();\n+    MessageDigest digester \u003d null;\n+    if (getChecksum) {\n+      digester \u003d MD5Hash.getDigester();\n+      stream \u003d new DigestInputStream(stream, digester);\n+    }\n+    boolean finishedReceiving \u003d false;\n+\n+    List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n+\n+    try {\n+      if (localPaths !\u003d null) {\n+        for (File f : localPaths) {\n+          try {\n+            if (f.exists()) {\n+              LOG.warn(\"Overwriting existing file \" + f\n+                  + \" with file downloaded from \" + url);\n+            }\n+            outputStreams.add(new FileOutputStream(f));\n+          } catch (IOException ioe) {\n+            LOG.warn(\"Unable to download file \" + f, ioe);\n+            // This will be null if we\u0027re downloading the fsimage to a file\n+            // outside of an NNStorage directory.\n+            if (dstStorage !\u003d null \u0026\u0026\n+                (dstStorage instanceof StorageErrorReporter)) {\n+              ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n+            }\n+          }\n+        }\n+        \n+        if (outputStreams.isEmpty()) {\n+          throw new IOException(\n+              \"Unable to download to any storage directory\");\n+        }\n+      }\n+      \n+      int num \u003d 1;\n+      byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n+      while (num \u003e 0) {\n+        num \u003d stream.read(buf);\n+        if (num \u003e 0) {\n+          received +\u003d num;\n+          for (FileOutputStream fos : outputStreams) {\n+            fos.write(buf, 0, num);\n+          }\n+        }\n+      }\n+      finishedReceiving \u003d true;\n+    } finally {\n+      stream.close();\n+      for (FileOutputStream fos : outputStreams) {\n+        fos.getChannel().force(true);\n+        fos.close();\n+      }\n+      if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n+        // only throw this exception if we think we read all of it on our end\n+        // -- otherwise a client-side IOException would be masked by this\n+        // exception that makes it look like a server-side problem!\n+        throw new IOException(\"File \" + url + \" received length \" + received +\n+                              \" is not of the advertised size \" +\n+                              advertisedSize);\n+      }\n+    }\n+    double xferSec \u003d Math.max(\n+        ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n+    long xferKb \u003d received / 1024;\n+    LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n+        xferSec, xferKb / xferSec));\n+\n+    if (digester !\u003d null) {\n+      MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n+      \n+      if (advertisedDigest !\u003d null \u0026\u0026\n+          !computedDigest.equals(advertisedDigest)) {\n+        throw new IOException(\"File \" + url + \" computed digest \" +\n+            computedDigest + \" does not match advertised digest \" + \n+            advertisedDigest);\n+      }\n+      return computedDigest;\n+    } else {\n+      return null;\n+    }    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n      Storage dstStorage, boolean getChecksum) throws IOException {\n    long startTime \u003d Time.monotonicNow();\n    HttpURLConnection connection;\n    try {\n      connection \u003d (HttpURLConnection)\n        connectionFactory.openConnection(url, isSpnegoEnabled);\n    } catch (AuthenticationException e) {\n      throw new IOException(e);\n    }\n\n    if (timeout \u003c\u003d 0) {\n      Configuration conf \u003d new HdfsConfiguration();\n      timeout \u003d conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY,\n          DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);\n    }\n\n    if (timeout \u003e 0) {\n      connection.setConnectTimeout(timeout);\n      connection.setReadTimeout(timeout);\n    }\n\n    if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n      throw new HttpGetFailedException(\n          \"Image transfer servlet at \" + url +\n          \" failed with status code \" + connection.getResponseCode() +\n          \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n          connection);\n    }\n    \n    long advertisedSize;\n    String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n    if (contentLength !\u003d null) {\n      advertisedSize \u003d Long.parseLong(contentLength);\n    } else {\n      throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                            \"by the namenode when trying to fetch \" + url);\n    }\n    \n    if (localPaths !\u003d null) {\n      String fsImageName \u003d connection.getHeaderField(\n          GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n      // If the local paths refer to directories, use the server-provided header\n      // as the filename within that directory\n      List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n      for (File localPath : localPaths) {\n        if (localPath.isDirectory()) {\n          if (fsImageName \u003d\u003d null) {\n            throw new IOException(\"No filename header provided by server\");\n          }\n          newLocalPaths.add(new File(localPath, fsImageName));\n        } else {\n          newLocalPaths.add(localPath);\n        }\n      }\n      localPaths \u003d newLocalPaths;\n    }\n    \n    MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n\n    long received \u003d 0;\n    InputStream stream \u003d connection.getInputStream();\n    MessageDigest digester \u003d null;\n    if (getChecksum) {\n      digester \u003d MD5Hash.getDigester();\n      stream \u003d new DigestInputStream(stream, digester);\n    }\n    boolean finishedReceiving \u003d false;\n\n    List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n\n    try {\n      if (localPaths !\u003d null) {\n        for (File f : localPaths) {\n          try {\n            if (f.exists()) {\n              LOG.warn(\"Overwriting existing file \" + f\n                  + \" with file downloaded from \" + url);\n            }\n            outputStreams.add(new FileOutputStream(f));\n          } catch (IOException ioe) {\n            LOG.warn(\"Unable to download file \" + f, ioe);\n            // This will be null if we\u0027re downloading the fsimage to a file\n            // outside of an NNStorage directory.\n            if (dstStorage !\u003d null \u0026\u0026\n                (dstStorage instanceof StorageErrorReporter)) {\n              ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n            }\n          }\n        }\n        \n        if (outputStreams.isEmpty()) {\n          throw new IOException(\n              \"Unable to download to any storage directory\");\n        }\n      }\n      \n      int num \u003d 1;\n      byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n      while (num \u003e 0) {\n        num \u003d stream.read(buf);\n        if (num \u003e 0) {\n          received +\u003d num;\n          for (FileOutputStream fos : outputStreams) {\n            fos.write(buf, 0, num);\n          }\n        }\n      }\n      finishedReceiving \u003d true;\n    } finally {\n      stream.close();\n      for (FileOutputStream fos : outputStreams) {\n        fos.getChannel().force(true);\n        fos.close();\n      }\n      if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n        // only throw this exception if we think we read all of it on our end\n        // -- otherwise a client-side IOException would be masked by this\n        // exception that makes it look like a server-side problem!\n        throw new IOException(\"File \" + url + \" received length \" + received +\n                              \" is not of the advertised size \" +\n                              advertisedSize);\n      }\n    }\n    double xferSec \u003d Math.max(\n        ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n    long xferKb \u003d received / 1024;\n    LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n        xferSec, xferKb / xferSec));\n\n    if (digester !\u003d null) {\n      MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n      \n      if (advertisedDigest !\u003d null \u0026\u0026\n          !computedDigest.equals(advertisedDigest)) {\n        throw new IOException(\"File \" + url + \" computed digest \" +\n            computedDigest + \" does not match advertised digest \" + \n            advertisedDigest);\n      }\n      return computedDigest;\n    } else {\n      return null;\n    }    \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java",
      "extendedDetails": {}
    },
    "0f595915a388305edbb3ce928415571811d304e8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3405. Checkpointing should use HTTP POST or PUT instead of GET-GET to send merged fsimages. Contributed by Vinayakumar B.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1575457 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/03/14 4:39 PM",
      "commitName": "0f595915a388305edbb3ce928415571811d304e8",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/02/14 5:21 PM",
      "commitNameOld": "e9a17c8ce0656a4e5d47401ca22a575c5f5f66db",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 7.97,
      "commitsBetweenForRepo": 73,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,144 +1,37 @@\n   public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n       Storage dstStorage, boolean getChecksum) throws IOException {\n-    long startTime \u003d Time.monotonicNow();\n     HttpURLConnection connection;\n     try {\n       connection \u003d (HttpURLConnection)\n         connectionFactory.openConnection(url, isSpnegoEnabled);\n     } catch (AuthenticationException e) {\n       throw new IOException(e);\n     }\n \n-    if (timeout \u003c\u003d 0) {\n-      Configuration conf \u003d new HdfsConfiguration();\n-      timeout \u003d conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY,\n-          DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);\n-    }\n-\n-    if (timeout \u003e 0) {\n-      connection.setConnectTimeout(timeout);\n-      connection.setReadTimeout(timeout);\n-    }\n+    setTimeout(connection);\n \n     if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n       throw new HttpGetFailedException(\n           \"Image transfer servlet at \" + url +\n           \" failed with status code \" + connection.getResponseCode() +\n           \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n           connection);\n     }\n     \n     long advertisedSize;\n     String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n     if (contentLength !\u003d null) {\n       advertisedSize \u003d Long.parseLong(contentLength);\n     } else {\n       throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                             \"by the namenode when trying to fetch \" + url);\n     }\n-    \n-    if (localPaths !\u003d null) {\n-      String fsImageName \u003d connection.getHeaderField(\n-          GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n-      // If the local paths refer to directories, use the server-provided header\n-      // as the filename within that directory\n-      List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n-      for (File localPath : localPaths) {\n-        if (localPath.isDirectory()) {\n-          if (fsImageName \u003d\u003d null) {\n-            throw new IOException(\"No filename header provided by server\");\n-          }\n-          newLocalPaths.add(new File(localPath, fsImageName));\n-        } else {\n-          newLocalPaths.add(localPath);\n-        }\n-      }\n-      localPaths \u003d newLocalPaths;\n-    }\n-    \n     MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n-\n-    long received \u003d 0;\n+    String fsImageName \u003d connection\n+        .getHeaderField(ImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n     InputStream stream \u003d connection.getInputStream();\n-    MessageDigest digester \u003d null;\n-    if (getChecksum) {\n-      digester \u003d MD5Hash.getDigester();\n-      stream \u003d new DigestInputStream(stream, digester);\n-    }\n-    boolean finishedReceiving \u003d false;\n \n-    List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n-\n-    try {\n-      if (localPaths !\u003d null) {\n-        for (File f : localPaths) {\n-          try {\n-            if (f.exists()) {\n-              LOG.warn(\"Overwriting existing file \" + f\n-                  + \" with file downloaded from \" + url);\n-            }\n-            outputStreams.add(new FileOutputStream(f));\n-          } catch (IOException ioe) {\n-            LOG.warn(\"Unable to download file \" + f, ioe);\n-            // This will be null if we\u0027re downloading the fsimage to a file\n-            // outside of an NNStorage directory.\n-            if (dstStorage !\u003d null \u0026\u0026\n-                (dstStorage instanceof StorageErrorReporter)) {\n-              ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n-            }\n-          }\n-        }\n-        \n-        if (outputStreams.isEmpty()) {\n-          throw new IOException(\n-              \"Unable to download to any storage directory\");\n-        }\n-      }\n-      \n-      int num \u003d 1;\n-      byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n-      while (num \u003e 0) {\n-        num \u003d stream.read(buf);\n-        if (num \u003e 0) {\n-          received +\u003d num;\n-          for (FileOutputStream fos : outputStreams) {\n-            fos.write(buf, 0, num);\n-          }\n-        }\n-      }\n-      finishedReceiving \u003d true;\n-    } finally {\n-      stream.close();\n-      for (FileOutputStream fos : outputStreams) {\n-        fos.getChannel().force(true);\n-        fos.close();\n-      }\n-      if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n-        // only throw this exception if we think we read all of it on our end\n-        // -- otherwise a client-side IOException would be masked by this\n-        // exception that makes it look like a server-side problem!\n-        throw new IOException(\"File \" + url + \" received length \" + received +\n-                              \" is not of the advertised size \" +\n-                              advertisedSize);\n-      }\n-    }\n-    double xferSec \u003d Math.max(\n-        ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n-    long xferKb \u003d received / 1024;\n-    LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n-        xferSec, xferKb / xferSec));\n-\n-    if (digester !\u003d null) {\n-      MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n-      \n-      if (advertisedDigest !\u003d null \u0026\u0026\n-          !computedDigest.equals(advertisedDigest)) {\n-        throw new IOException(\"File \" + url + \" computed digest \" +\n-            computedDigest + \" does not match advertised digest \" + \n-            advertisedDigest);\n-      }\n-      return computedDigest;\n-    } else {\n-      return null;\n-    }    \n+    return receiveFile(url.toExternalForm(), localPaths, dstStorage,\n+        getChecksum, advertisedSize, advertisedDigest, fsImageName, stream,\n+        null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n      Storage dstStorage, boolean getChecksum) throws IOException {\n    HttpURLConnection connection;\n    try {\n      connection \u003d (HttpURLConnection)\n        connectionFactory.openConnection(url, isSpnegoEnabled);\n    } catch (AuthenticationException e) {\n      throw new IOException(e);\n    }\n\n    setTimeout(connection);\n\n    if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n      throw new HttpGetFailedException(\n          \"Image transfer servlet at \" + url +\n          \" failed with status code \" + connection.getResponseCode() +\n          \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n          connection);\n    }\n    \n    long advertisedSize;\n    String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n    if (contentLength !\u003d null) {\n      advertisedSize \u003d Long.parseLong(contentLength);\n    } else {\n      throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                            \"by the namenode when trying to fetch \" + url);\n    }\n    MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n    String fsImageName \u003d connection\n        .getHeaderField(ImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n    InputStream stream \u003d connection.getInputStream();\n\n    return receiveFile(url.toExternalForm(), localPaths, dstStorage,\n        getChecksum, advertisedSize, advertisedDigest, fsImageName, stream,\n        null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java",
      "extendedDetails": {}
    },
    "d8a23834614581a292aad214dddcbcc4bbe86d27": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5538. URLConnectionFactory should pick up the SSL related configuration by default. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1545491 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/11/13 5:16 PM",
      "commitName": "d8a23834614581a292aad214dddcbcc4bbe86d27",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "13/05/13 10:47 AM",
      "commitNameOld": "09593530fb6ccb93fd123f9497b93f7ec733210f",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 196.31,
      "commitsBetweenForRepo": 1217,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,139 +1,144 @@\n   public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n       Storage dstStorage, boolean getChecksum) throws IOException {\n     long startTime \u003d Time.monotonicNow();\n-    HttpURLConnection connection \u003d (HttpURLConnection)\n-      SecurityUtil.openSecureHttpConnection(url);\n+    HttpURLConnection connection;\n+    try {\n+      connection \u003d (HttpURLConnection)\n+        connectionFactory.openConnection(url, isSpnegoEnabled);\n+    } catch (AuthenticationException e) {\n+      throw new IOException(e);\n+    }\n \n     if (timeout \u003c\u003d 0) {\n       Configuration conf \u003d new HdfsConfiguration();\n       timeout \u003d conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY,\n           DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);\n     }\n \n     if (timeout \u003e 0) {\n       connection.setConnectTimeout(timeout);\n       connection.setReadTimeout(timeout);\n     }\n \n     if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n       throw new HttpGetFailedException(\n           \"Image transfer servlet at \" + url +\n           \" failed with status code \" + connection.getResponseCode() +\n           \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n           connection);\n     }\n     \n     long advertisedSize;\n     String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n     if (contentLength !\u003d null) {\n       advertisedSize \u003d Long.parseLong(contentLength);\n     } else {\n       throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                             \"by the namenode when trying to fetch \" + url);\n     }\n     \n     if (localPaths !\u003d null) {\n       String fsImageName \u003d connection.getHeaderField(\n           GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n       // If the local paths refer to directories, use the server-provided header\n       // as the filename within that directory\n       List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n       for (File localPath : localPaths) {\n         if (localPath.isDirectory()) {\n           if (fsImageName \u003d\u003d null) {\n             throw new IOException(\"No filename header provided by server\");\n           }\n           newLocalPaths.add(new File(localPath, fsImageName));\n         } else {\n           newLocalPaths.add(localPath);\n         }\n       }\n       localPaths \u003d newLocalPaths;\n     }\n     \n     MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n \n     long received \u003d 0;\n     InputStream stream \u003d connection.getInputStream();\n     MessageDigest digester \u003d null;\n     if (getChecksum) {\n       digester \u003d MD5Hash.getDigester();\n       stream \u003d new DigestInputStream(stream, digester);\n     }\n     boolean finishedReceiving \u003d false;\n \n     List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n \n     try {\n       if (localPaths !\u003d null) {\n         for (File f : localPaths) {\n           try {\n             if (f.exists()) {\n               LOG.warn(\"Overwriting existing file \" + f\n                   + \" with file downloaded from \" + url);\n             }\n             outputStreams.add(new FileOutputStream(f));\n           } catch (IOException ioe) {\n             LOG.warn(\"Unable to download file \" + f, ioe);\n             // This will be null if we\u0027re downloading the fsimage to a file\n             // outside of an NNStorage directory.\n             if (dstStorage !\u003d null \u0026\u0026\n                 (dstStorage instanceof StorageErrorReporter)) {\n               ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n             }\n           }\n         }\n         \n         if (outputStreams.isEmpty()) {\n           throw new IOException(\n               \"Unable to download to any storage directory\");\n         }\n       }\n       \n       int num \u003d 1;\n       byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n       while (num \u003e 0) {\n         num \u003d stream.read(buf);\n         if (num \u003e 0) {\n           received +\u003d num;\n           for (FileOutputStream fos : outputStreams) {\n             fos.write(buf, 0, num);\n           }\n         }\n       }\n       finishedReceiving \u003d true;\n     } finally {\n       stream.close();\n       for (FileOutputStream fos : outputStreams) {\n         fos.getChannel().force(true);\n         fos.close();\n       }\n       if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n         // only throw this exception if we think we read all of it on our end\n         // -- otherwise a client-side IOException would be masked by this\n         // exception that makes it look like a server-side problem!\n         throw new IOException(\"File \" + url + \" received length \" + received +\n                               \" is not of the advertised size \" +\n                               advertisedSize);\n       }\n     }\n     double xferSec \u003d Math.max(\n         ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n     long xferKb \u003d received / 1024;\n     LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n         xferSec, xferKb / xferSec));\n \n     if (digester !\u003d null) {\n       MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n       \n       if (advertisedDigest !\u003d null \u0026\u0026\n           !computedDigest.equals(advertisedDigest)) {\n         throw new IOException(\"File \" + url + \" computed digest \" +\n             computedDigest + \" does not match advertised digest \" + \n             advertisedDigest);\n       }\n       return computedDigest;\n     } else {\n       return null;\n     }    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n      Storage dstStorage, boolean getChecksum) throws IOException {\n    long startTime \u003d Time.monotonicNow();\n    HttpURLConnection connection;\n    try {\n      connection \u003d (HttpURLConnection)\n        connectionFactory.openConnection(url, isSpnegoEnabled);\n    } catch (AuthenticationException e) {\n      throw new IOException(e);\n    }\n\n    if (timeout \u003c\u003d 0) {\n      Configuration conf \u003d new HdfsConfiguration();\n      timeout \u003d conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY,\n          DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);\n    }\n\n    if (timeout \u003e 0) {\n      connection.setConnectTimeout(timeout);\n      connection.setReadTimeout(timeout);\n    }\n\n    if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n      throw new HttpGetFailedException(\n          \"Image transfer servlet at \" + url +\n          \" failed with status code \" + connection.getResponseCode() +\n          \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n          connection);\n    }\n    \n    long advertisedSize;\n    String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n    if (contentLength !\u003d null) {\n      advertisedSize \u003d Long.parseLong(contentLength);\n    } else {\n      throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                            \"by the namenode when trying to fetch \" + url);\n    }\n    \n    if (localPaths !\u003d null) {\n      String fsImageName \u003d connection.getHeaderField(\n          GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n      // If the local paths refer to directories, use the server-provided header\n      // as the filename within that directory\n      List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n      for (File localPath : localPaths) {\n        if (localPath.isDirectory()) {\n          if (fsImageName \u003d\u003d null) {\n            throw new IOException(\"No filename header provided by server\");\n          }\n          newLocalPaths.add(new File(localPath, fsImageName));\n        } else {\n          newLocalPaths.add(localPath);\n        }\n      }\n      localPaths \u003d newLocalPaths;\n    }\n    \n    MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n\n    long received \u003d 0;\n    InputStream stream \u003d connection.getInputStream();\n    MessageDigest digester \u003d null;\n    if (getChecksum) {\n      digester \u003d MD5Hash.getDigester();\n      stream \u003d new DigestInputStream(stream, digester);\n    }\n    boolean finishedReceiving \u003d false;\n\n    List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n\n    try {\n      if (localPaths !\u003d null) {\n        for (File f : localPaths) {\n          try {\n            if (f.exists()) {\n              LOG.warn(\"Overwriting existing file \" + f\n                  + \" with file downloaded from \" + url);\n            }\n            outputStreams.add(new FileOutputStream(f));\n          } catch (IOException ioe) {\n            LOG.warn(\"Unable to download file \" + f, ioe);\n            // This will be null if we\u0027re downloading the fsimage to a file\n            // outside of an NNStorage directory.\n            if (dstStorage !\u003d null \u0026\u0026\n                (dstStorage instanceof StorageErrorReporter)) {\n              ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n            }\n          }\n        }\n        \n        if (outputStreams.isEmpty()) {\n          throw new IOException(\n              \"Unable to download to any storage directory\");\n        }\n      }\n      \n      int num \u003d 1;\n      byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n      while (num \u003e 0) {\n        num \u003d stream.read(buf);\n        if (num \u003e 0) {\n          received +\u003d num;\n          for (FileOutputStream fos : outputStreams) {\n            fos.write(buf, 0, num);\n          }\n        }\n      }\n      finishedReceiving \u003d true;\n    } finally {\n      stream.close();\n      for (FileOutputStream fos : outputStreams) {\n        fos.getChannel().force(true);\n        fos.close();\n      }\n      if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n        // only throw this exception if we think we read all of it on our end\n        // -- otherwise a client-side IOException would be masked by this\n        // exception that makes it look like a server-side problem!\n        throw new IOException(\"File \" + url + \" received length \" + received +\n                              \" is not of the advertised size \" +\n                              advertisedSize);\n      }\n    }\n    double xferSec \u003d Math.max(\n        ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n    long xferKb \u003d received / 1024;\n    LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n        xferSec, xferKb / xferSec));\n\n    if (digester !\u003d null) {\n      MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n      \n      if (advertisedDigest !\u003d null \u0026\u0026\n          !computedDigest.equals(advertisedDigest)) {\n        throw new IOException(\"File \" + url + \" computed digest \" +\n            computedDigest + \" does not match advertised digest \" + \n            advertisedDigest);\n      }\n      return computedDigest;\n    } else {\n      return null;\n    }    \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java",
      "extendedDetails": {}
    },
    "4909821aa97f5c1343db5c2d9b200853eb05ec77": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4569. Small image transfer related cleanups. Contributed by Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1454233 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/03/13 7:37 PM",
      "commitName": "4909821aa97f5c1343db5c2d9b200853eb05ec77",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "04/09/12 9:53 PM",
      "commitNameOld": "31142aa8927568b7901eb48d80ba04d3e0080f60",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 183.95,
      "commitsBetweenForRepo": 893,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,140 +1,139 @@\n   public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n       Storage dstStorage, boolean getChecksum) throws IOException {\n     long startTime \u003d Time.monotonicNow();\n     HttpURLConnection connection \u003d (HttpURLConnection)\n       SecurityUtil.openSecureHttpConnection(url);\n \n     if (timeout \u003c\u003d 0) {\n-      // Set the ping interval as timeout\n       Configuration conf \u003d new HdfsConfiguration();\n       timeout \u003d conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY,\n           DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);\n     }\n \n     if (timeout \u003e 0) {\n       connection.setConnectTimeout(timeout);\n       connection.setReadTimeout(timeout);\n     }\n \n     if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n       throw new HttpGetFailedException(\n           \"Image transfer servlet at \" + url +\n           \" failed with status code \" + connection.getResponseCode() +\n           \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n           connection);\n     }\n     \n     long advertisedSize;\n     String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n     if (contentLength !\u003d null) {\n       advertisedSize \u003d Long.parseLong(contentLength);\n     } else {\n       throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                             \"by the namenode when trying to fetch \" + url);\n     }\n     \n     if (localPaths !\u003d null) {\n       String fsImageName \u003d connection.getHeaderField(\n           GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n       // If the local paths refer to directories, use the server-provided header\n       // as the filename within that directory\n       List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n       for (File localPath : localPaths) {\n         if (localPath.isDirectory()) {\n           if (fsImageName \u003d\u003d null) {\n             throw new IOException(\"No filename header provided by server\");\n           }\n           newLocalPaths.add(new File(localPath, fsImageName));\n         } else {\n           newLocalPaths.add(localPath);\n         }\n       }\n       localPaths \u003d newLocalPaths;\n     }\n     \n     MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n \n     long received \u003d 0;\n     InputStream stream \u003d connection.getInputStream();\n     MessageDigest digester \u003d null;\n     if (getChecksum) {\n       digester \u003d MD5Hash.getDigester();\n       stream \u003d new DigestInputStream(stream, digester);\n     }\n     boolean finishedReceiving \u003d false;\n \n     List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n \n     try {\n       if (localPaths !\u003d null) {\n         for (File f : localPaths) {\n           try {\n             if (f.exists()) {\n               LOG.warn(\"Overwriting existing file \" + f\n                   + \" with file downloaded from \" + url);\n             }\n             outputStreams.add(new FileOutputStream(f));\n           } catch (IOException ioe) {\n             LOG.warn(\"Unable to download file \" + f, ioe);\n             // This will be null if we\u0027re downloading the fsimage to a file\n             // outside of an NNStorage directory.\n             if (dstStorage !\u003d null \u0026\u0026\n                 (dstStorage instanceof StorageErrorReporter)) {\n               ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n             }\n           }\n         }\n         \n         if (outputStreams.isEmpty()) {\n           throw new IOException(\n               \"Unable to download to any storage directory\");\n         }\n       }\n       \n       int num \u003d 1;\n       byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n       while (num \u003e 0) {\n         num \u003d stream.read(buf);\n         if (num \u003e 0) {\n           received +\u003d num;\n           for (FileOutputStream fos : outputStreams) {\n             fos.write(buf, 0, num);\n           }\n         }\n       }\n       finishedReceiving \u003d true;\n     } finally {\n       stream.close();\n       for (FileOutputStream fos : outputStreams) {\n         fos.getChannel().force(true);\n         fos.close();\n       }\n       if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n         // only throw this exception if we think we read all of it on our end\n         // -- otherwise a client-side IOException would be masked by this\n         // exception that makes it look like a server-side problem!\n         throw new IOException(\"File \" + url + \" received length \" + received +\n                               \" is not of the advertised size \" +\n                               advertisedSize);\n       }\n     }\n     double xferSec \u003d Math.max(\n         ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n     long xferKb \u003d received / 1024;\n     LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n         xferSec, xferKb / xferSec));\n \n     if (digester !\u003d null) {\n       MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n       \n       if (advertisedDigest !\u003d null \u0026\u0026\n           !computedDigest.equals(advertisedDigest)) {\n         throw new IOException(\"File \" + url + \" computed digest \" +\n             computedDigest + \" does not match advertised digest \" + \n             advertisedDigest);\n       }\n       return computedDigest;\n     } else {\n       return null;\n     }    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n      Storage dstStorage, boolean getChecksum) throws IOException {\n    long startTime \u003d Time.monotonicNow();\n    HttpURLConnection connection \u003d (HttpURLConnection)\n      SecurityUtil.openSecureHttpConnection(url);\n\n    if (timeout \u003c\u003d 0) {\n      Configuration conf \u003d new HdfsConfiguration();\n      timeout \u003d conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY,\n          DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);\n    }\n\n    if (timeout \u003e 0) {\n      connection.setConnectTimeout(timeout);\n      connection.setReadTimeout(timeout);\n    }\n\n    if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n      throw new HttpGetFailedException(\n          \"Image transfer servlet at \" + url +\n          \" failed with status code \" + connection.getResponseCode() +\n          \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n          connection);\n    }\n    \n    long advertisedSize;\n    String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n    if (contentLength !\u003d null) {\n      advertisedSize \u003d Long.parseLong(contentLength);\n    } else {\n      throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                            \"by the namenode when trying to fetch \" + url);\n    }\n    \n    if (localPaths !\u003d null) {\n      String fsImageName \u003d connection.getHeaderField(\n          GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n      // If the local paths refer to directories, use the server-provided header\n      // as the filename within that directory\n      List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n      for (File localPath : localPaths) {\n        if (localPath.isDirectory()) {\n          if (fsImageName \u003d\u003d null) {\n            throw new IOException(\"No filename header provided by server\");\n          }\n          newLocalPaths.add(new File(localPath, fsImageName));\n        } else {\n          newLocalPaths.add(localPath);\n        }\n      }\n      localPaths \u003d newLocalPaths;\n    }\n    \n    MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n\n    long received \u003d 0;\n    InputStream stream \u003d connection.getInputStream();\n    MessageDigest digester \u003d null;\n    if (getChecksum) {\n      digester \u003d MD5Hash.getDigester();\n      stream \u003d new DigestInputStream(stream, digester);\n    }\n    boolean finishedReceiving \u003d false;\n\n    List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n\n    try {\n      if (localPaths !\u003d null) {\n        for (File f : localPaths) {\n          try {\n            if (f.exists()) {\n              LOG.warn(\"Overwriting existing file \" + f\n                  + \" with file downloaded from \" + url);\n            }\n            outputStreams.add(new FileOutputStream(f));\n          } catch (IOException ioe) {\n            LOG.warn(\"Unable to download file \" + f, ioe);\n            // This will be null if we\u0027re downloading the fsimage to a file\n            // outside of an NNStorage directory.\n            if (dstStorage !\u003d null \u0026\u0026\n                (dstStorage instanceof StorageErrorReporter)) {\n              ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n            }\n          }\n        }\n        \n        if (outputStreams.isEmpty()) {\n          throw new IOException(\n              \"Unable to download to any storage directory\");\n        }\n      }\n      \n      int num \u003d 1;\n      byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n      while (num \u003e 0) {\n        num \u003d stream.read(buf);\n        if (num \u003e 0) {\n          received +\u003d num;\n          for (FileOutputStream fos : outputStreams) {\n            fos.write(buf, 0, num);\n          }\n        }\n      }\n      finishedReceiving \u003d true;\n    } finally {\n      stream.close();\n      for (FileOutputStream fos : outputStreams) {\n        fos.getChannel().force(true);\n        fos.close();\n      }\n      if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n        // only throw this exception if we think we read all of it on our end\n        // -- otherwise a client-side IOException would be masked by this\n        // exception that makes it look like a server-side problem!\n        throw new IOException(\"File \" + url + \" received length \" + received +\n                              \" is not of the advertised size \" +\n                              advertisedSize);\n      }\n    }\n    double xferSec \u003d Math.max(\n        ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n    long xferKb \u003d received / 1024;\n    LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n        xferSec, xferKb / xferSec));\n\n    if (digester !\u003d null) {\n      MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n      \n      if (advertisedDigest !\u003d null \u0026\u0026\n          !computedDigest.equals(advertisedDigest)) {\n        throw new IOException(\"File \" + url + \" computed digest \" +\n            computedDigest + \" does not match advertised digest \" + \n            advertisedDigest);\n      }\n      return computedDigest;\n    } else {\n      return null;\n    }    \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java",
      "extendedDetails": {}
    },
    "31142aa8927568b7901eb48d80ba04d3e0080f60": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1490. TransferFSImage should timeout. Contributed by Dmytro Molkov and Vinay.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1380988 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/12 9:53 PM",
      "commitName": "31142aa8927568b7901eb48d80ba04d3e0080f60",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "09/08/12 3:52 PM",
      "commitNameOld": "4bca22005f48f426b9bc7cf36d435ead470a2590",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 26.25,
      "commitsBetweenForRepo": 159,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,128 +1,140 @@\n   public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n       Storage dstStorage, boolean getChecksum) throws IOException {\n     long startTime \u003d Time.monotonicNow();\n     HttpURLConnection connection \u003d (HttpURLConnection)\n       SecurityUtil.openSecureHttpConnection(url);\n \n+    if (timeout \u003c\u003d 0) {\n+      // Set the ping interval as timeout\n+      Configuration conf \u003d new HdfsConfiguration();\n+      timeout \u003d conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY,\n+          DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);\n+    }\n+\n+    if (timeout \u003e 0) {\n+      connection.setConnectTimeout(timeout);\n+      connection.setReadTimeout(timeout);\n+    }\n+\n     if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n       throw new HttpGetFailedException(\n           \"Image transfer servlet at \" + url +\n           \" failed with status code \" + connection.getResponseCode() +\n           \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n           connection);\n     }\n     \n     long advertisedSize;\n     String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n     if (contentLength !\u003d null) {\n       advertisedSize \u003d Long.parseLong(contentLength);\n     } else {\n       throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                             \"by the namenode when trying to fetch \" + url);\n     }\n     \n     if (localPaths !\u003d null) {\n       String fsImageName \u003d connection.getHeaderField(\n           GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n       // If the local paths refer to directories, use the server-provided header\n       // as the filename within that directory\n       List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n       for (File localPath : localPaths) {\n         if (localPath.isDirectory()) {\n           if (fsImageName \u003d\u003d null) {\n             throw new IOException(\"No filename header provided by server\");\n           }\n           newLocalPaths.add(new File(localPath, fsImageName));\n         } else {\n           newLocalPaths.add(localPath);\n         }\n       }\n       localPaths \u003d newLocalPaths;\n     }\n     \n     MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n \n     long received \u003d 0;\n     InputStream stream \u003d connection.getInputStream();\n     MessageDigest digester \u003d null;\n     if (getChecksum) {\n       digester \u003d MD5Hash.getDigester();\n       stream \u003d new DigestInputStream(stream, digester);\n     }\n     boolean finishedReceiving \u003d false;\n \n     List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n \n     try {\n       if (localPaths !\u003d null) {\n         for (File f : localPaths) {\n           try {\n             if (f.exists()) {\n               LOG.warn(\"Overwriting existing file \" + f\n                   + \" with file downloaded from \" + url);\n             }\n             outputStreams.add(new FileOutputStream(f));\n           } catch (IOException ioe) {\n             LOG.warn(\"Unable to download file \" + f, ioe);\n             // This will be null if we\u0027re downloading the fsimage to a file\n             // outside of an NNStorage directory.\n             if (dstStorage !\u003d null \u0026\u0026\n                 (dstStorage instanceof StorageErrorReporter)) {\n               ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n             }\n           }\n         }\n         \n         if (outputStreams.isEmpty()) {\n           throw new IOException(\n               \"Unable to download to any storage directory\");\n         }\n       }\n       \n       int num \u003d 1;\n       byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n       while (num \u003e 0) {\n         num \u003d stream.read(buf);\n         if (num \u003e 0) {\n           received +\u003d num;\n           for (FileOutputStream fos : outputStreams) {\n             fos.write(buf, 0, num);\n           }\n         }\n       }\n       finishedReceiving \u003d true;\n     } finally {\n       stream.close();\n       for (FileOutputStream fos : outputStreams) {\n         fos.getChannel().force(true);\n         fos.close();\n       }\n       if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n         // only throw this exception if we think we read all of it on our end\n         // -- otherwise a client-side IOException would be masked by this\n         // exception that makes it look like a server-side problem!\n         throw new IOException(\"File \" + url + \" received length \" + received +\n                               \" is not of the advertised size \" +\n                               advertisedSize);\n       }\n     }\n     double xferSec \u003d Math.max(\n         ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n     long xferKb \u003d received / 1024;\n     LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n         xferSec, xferKb / xferSec));\n \n     if (digester !\u003d null) {\n       MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n       \n       if (advertisedDigest !\u003d null \u0026\u0026\n           !computedDigest.equals(advertisedDigest)) {\n         throw new IOException(\"File \" + url + \" computed digest \" +\n             computedDigest + \" does not match advertised digest \" + \n             advertisedDigest);\n       }\n       return computedDigest;\n     } else {\n       return null;\n     }    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n      Storage dstStorage, boolean getChecksum) throws IOException {\n    long startTime \u003d Time.monotonicNow();\n    HttpURLConnection connection \u003d (HttpURLConnection)\n      SecurityUtil.openSecureHttpConnection(url);\n\n    if (timeout \u003c\u003d 0) {\n      // Set the ping interval as timeout\n      Configuration conf \u003d new HdfsConfiguration();\n      timeout \u003d conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY,\n          DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);\n    }\n\n    if (timeout \u003e 0) {\n      connection.setConnectTimeout(timeout);\n      connection.setReadTimeout(timeout);\n    }\n\n    if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n      throw new HttpGetFailedException(\n          \"Image transfer servlet at \" + url +\n          \" failed with status code \" + connection.getResponseCode() +\n          \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n          connection);\n    }\n    \n    long advertisedSize;\n    String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n    if (contentLength !\u003d null) {\n      advertisedSize \u003d Long.parseLong(contentLength);\n    } else {\n      throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                            \"by the namenode when trying to fetch \" + url);\n    }\n    \n    if (localPaths !\u003d null) {\n      String fsImageName \u003d connection.getHeaderField(\n          GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n      // If the local paths refer to directories, use the server-provided header\n      // as the filename within that directory\n      List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n      for (File localPath : localPaths) {\n        if (localPath.isDirectory()) {\n          if (fsImageName \u003d\u003d null) {\n            throw new IOException(\"No filename header provided by server\");\n          }\n          newLocalPaths.add(new File(localPath, fsImageName));\n        } else {\n          newLocalPaths.add(localPath);\n        }\n      }\n      localPaths \u003d newLocalPaths;\n    }\n    \n    MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n\n    long received \u003d 0;\n    InputStream stream \u003d connection.getInputStream();\n    MessageDigest digester \u003d null;\n    if (getChecksum) {\n      digester \u003d MD5Hash.getDigester();\n      stream \u003d new DigestInputStream(stream, digester);\n    }\n    boolean finishedReceiving \u003d false;\n\n    List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n\n    try {\n      if (localPaths !\u003d null) {\n        for (File f : localPaths) {\n          try {\n            if (f.exists()) {\n              LOG.warn(\"Overwriting existing file \" + f\n                  + \" with file downloaded from \" + url);\n            }\n            outputStreams.add(new FileOutputStream(f));\n          } catch (IOException ioe) {\n            LOG.warn(\"Unable to download file \" + f, ioe);\n            // This will be null if we\u0027re downloading the fsimage to a file\n            // outside of an NNStorage directory.\n            if (dstStorage !\u003d null \u0026\u0026\n                (dstStorage instanceof StorageErrorReporter)) {\n              ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n            }\n          }\n        }\n        \n        if (outputStreams.isEmpty()) {\n          throw new IOException(\n              \"Unable to download to any storage directory\");\n        }\n      }\n      \n      int num \u003d 1;\n      byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n      while (num \u003e 0) {\n        num \u003d stream.read(buf);\n        if (num \u003e 0) {\n          received +\u003d num;\n          for (FileOutputStream fos : outputStreams) {\n            fos.write(buf, 0, num);\n          }\n        }\n      }\n      finishedReceiving \u003d true;\n    } finally {\n      stream.close();\n      for (FileOutputStream fos : outputStreams) {\n        fos.getChannel().force(true);\n        fos.close();\n      }\n      if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n        // only throw this exception if we think we read all of it on our end\n        // -- otherwise a client-side IOException would be masked by this\n        // exception that makes it look like a server-side problem!\n        throw new IOException(\"File \" + url + \" received length \" + received +\n                              \" is not of the advertised size \" +\n                              advertisedSize);\n      }\n    }\n    double xferSec \u003d Math.max(\n        ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n    long xferKb \u003d received / 1024;\n    LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n        xferSec, xferKb / xferSec));\n\n    if (digester !\u003d null) {\n      MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n      \n      if (advertisedDigest !\u003d null \u0026\u0026\n          !computedDigest.equals(advertisedDigest)) {\n        throw new IOException(\"File \" + url + \" computed digest \" +\n            computedDigest + \" does not match advertised digest \" + \n            advertisedDigest);\n      }\n      return computedDigest;\n    } else {\n      return null;\n    }    \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java",
      "extendedDetails": {}
    },
    "4a5ba3b7bd2360fd9605863630b477d362874e1e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3641. Move server Util time methods to common and use now instead of System#currentTimeMillis. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1360858 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/07/12 12:01 PM",
      "commitName": "4a5ba3b7bd2360fd9605863630b477d362874e1e",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "03/07/12 1:55 PM",
      "commitNameOld": "3728d16160118a4b6e632a59fb1e2e0795ca6595",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 8.92,
      "commitsBetweenForRepo": 57,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,128 +1,128 @@\n   public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n       Storage dstStorage, boolean getChecksum) throws IOException {\n-    long startTime \u003d Util.monotonicNow();\n+    long startTime \u003d Time.monotonicNow();\n     HttpURLConnection connection \u003d (HttpURLConnection)\n       SecurityUtil.openSecureHttpConnection(url);\n \n     if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n       throw new HttpGetFailedException(\n           \"Image transfer servlet at \" + url +\n           \" failed with status code \" + connection.getResponseCode() +\n           \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n           connection);\n     }\n     \n     long advertisedSize;\n     String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n     if (contentLength !\u003d null) {\n       advertisedSize \u003d Long.parseLong(contentLength);\n     } else {\n       throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                             \"by the namenode when trying to fetch \" + url);\n     }\n     \n     if (localPaths !\u003d null) {\n       String fsImageName \u003d connection.getHeaderField(\n           GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n       // If the local paths refer to directories, use the server-provided header\n       // as the filename within that directory\n       List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n       for (File localPath : localPaths) {\n         if (localPath.isDirectory()) {\n           if (fsImageName \u003d\u003d null) {\n             throw new IOException(\"No filename header provided by server\");\n           }\n           newLocalPaths.add(new File(localPath, fsImageName));\n         } else {\n           newLocalPaths.add(localPath);\n         }\n       }\n       localPaths \u003d newLocalPaths;\n     }\n     \n     MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n \n     long received \u003d 0;\n     InputStream stream \u003d connection.getInputStream();\n     MessageDigest digester \u003d null;\n     if (getChecksum) {\n       digester \u003d MD5Hash.getDigester();\n       stream \u003d new DigestInputStream(stream, digester);\n     }\n     boolean finishedReceiving \u003d false;\n \n     List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n \n     try {\n       if (localPaths !\u003d null) {\n         for (File f : localPaths) {\n           try {\n             if (f.exists()) {\n               LOG.warn(\"Overwriting existing file \" + f\n                   + \" with file downloaded from \" + url);\n             }\n             outputStreams.add(new FileOutputStream(f));\n           } catch (IOException ioe) {\n             LOG.warn(\"Unable to download file \" + f, ioe);\n             // This will be null if we\u0027re downloading the fsimage to a file\n             // outside of an NNStorage directory.\n             if (dstStorage !\u003d null \u0026\u0026\n                 (dstStorage instanceof StorageErrorReporter)) {\n               ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n             }\n           }\n         }\n         \n         if (outputStreams.isEmpty()) {\n           throw new IOException(\n               \"Unable to download to any storage directory\");\n         }\n       }\n       \n       int num \u003d 1;\n       byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n       while (num \u003e 0) {\n         num \u003d stream.read(buf);\n         if (num \u003e 0) {\n           received +\u003d num;\n           for (FileOutputStream fos : outputStreams) {\n             fos.write(buf, 0, num);\n           }\n         }\n       }\n       finishedReceiving \u003d true;\n     } finally {\n       stream.close();\n       for (FileOutputStream fos : outputStreams) {\n         fos.getChannel().force(true);\n         fos.close();\n       }\n       if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n         // only throw this exception if we think we read all of it on our end\n         // -- otherwise a client-side IOException would be masked by this\n         // exception that makes it look like a server-side problem!\n         throw new IOException(\"File \" + url + \" received length \" + received +\n                               \" is not of the advertised size \" +\n                               advertisedSize);\n       }\n     }\n     double xferSec \u003d Math.max(\n-        ((float)(Util.monotonicNow() - startTime)) / 1000.0, 0.001);\n+        ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n     long xferKb \u003d received / 1024;\n     LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n         xferSec, xferKb / xferSec));\n \n     if (digester !\u003d null) {\n       MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n       \n       if (advertisedDigest !\u003d null \u0026\u0026\n           !computedDigest.equals(advertisedDigest)) {\n         throw new IOException(\"File \" + url + \" computed digest \" +\n             computedDigest + \" does not match advertised digest \" + \n             advertisedDigest);\n       }\n       return computedDigest;\n     } else {\n       return null;\n     }    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n      Storage dstStorage, boolean getChecksum) throws IOException {\n    long startTime \u003d Time.monotonicNow();\n    HttpURLConnection connection \u003d (HttpURLConnection)\n      SecurityUtil.openSecureHttpConnection(url);\n\n    if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n      throw new HttpGetFailedException(\n          \"Image transfer servlet at \" + url +\n          \" failed with status code \" + connection.getResponseCode() +\n          \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n          connection);\n    }\n    \n    long advertisedSize;\n    String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n    if (contentLength !\u003d null) {\n      advertisedSize \u003d Long.parseLong(contentLength);\n    } else {\n      throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                            \"by the namenode when trying to fetch \" + url);\n    }\n    \n    if (localPaths !\u003d null) {\n      String fsImageName \u003d connection.getHeaderField(\n          GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n      // If the local paths refer to directories, use the server-provided header\n      // as the filename within that directory\n      List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n      for (File localPath : localPaths) {\n        if (localPath.isDirectory()) {\n          if (fsImageName \u003d\u003d null) {\n            throw new IOException(\"No filename header provided by server\");\n          }\n          newLocalPaths.add(new File(localPath, fsImageName));\n        } else {\n          newLocalPaths.add(localPath);\n        }\n      }\n      localPaths \u003d newLocalPaths;\n    }\n    \n    MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n\n    long received \u003d 0;\n    InputStream stream \u003d connection.getInputStream();\n    MessageDigest digester \u003d null;\n    if (getChecksum) {\n      digester \u003d MD5Hash.getDigester();\n      stream \u003d new DigestInputStream(stream, digester);\n    }\n    boolean finishedReceiving \u003d false;\n\n    List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n\n    try {\n      if (localPaths !\u003d null) {\n        for (File f : localPaths) {\n          try {\n            if (f.exists()) {\n              LOG.warn(\"Overwriting existing file \" + f\n                  + \" with file downloaded from \" + url);\n            }\n            outputStreams.add(new FileOutputStream(f));\n          } catch (IOException ioe) {\n            LOG.warn(\"Unable to download file \" + f, ioe);\n            // This will be null if we\u0027re downloading the fsimage to a file\n            // outside of an NNStorage directory.\n            if (dstStorage !\u003d null \u0026\u0026\n                (dstStorage instanceof StorageErrorReporter)) {\n              ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n            }\n          }\n        }\n        \n        if (outputStreams.isEmpty()) {\n          throw new IOException(\n              \"Unable to download to any storage directory\");\n        }\n      }\n      \n      int num \u003d 1;\n      byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n      while (num \u003e 0) {\n        num \u003d stream.read(buf);\n        if (num \u003e 0) {\n          received +\u003d num;\n          for (FileOutputStream fos : outputStreams) {\n            fos.write(buf, 0, num);\n          }\n        }\n      }\n      finishedReceiving \u003d true;\n    } finally {\n      stream.close();\n      for (FileOutputStream fos : outputStreams) {\n        fos.getChannel().force(true);\n        fos.close();\n      }\n      if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n        // only throw this exception if we think we read all of it on our end\n        // -- otherwise a client-side IOException would be masked by this\n        // exception that makes it look like a server-side problem!\n        throw new IOException(\"File \" + url + \" received length \" + received +\n                              \" is not of the advertised size \" +\n                              advertisedSize);\n      }\n    }\n    double xferSec \u003d Math.max(\n        ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n    long xferKb \u003d received / 1024;\n    LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n        xferSec, xferKb / xferSec));\n\n    if (digester !\u003d null) {\n      MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n      \n      if (advertisedDigest !\u003d null \u0026\u0026\n          !computedDigest.equals(advertisedDigest)) {\n        throw new IOException(\"File \" + url + \" computed digest \" +\n            computedDigest + \" does not match advertised digest \" + \n            advertisedDigest);\n      }\n      return computedDigest;\n    } else {\n      return null;\n    }    \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java",
      "extendedDetails": {}
    },
    "8dd3148e734fa9d1db761ce65410fdc49c0fe1d5": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3190. Simple refactors in existing NN code to assist QuorumJournalManager extension. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1356525 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/07/12 4:59 PM",
      "commitName": "8dd3148e734fa9d1db761ce65410fdc49c0fe1d5",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,128 @@\n+  public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n+      Storage dstStorage, boolean getChecksum) throws IOException {\n+    long startTime \u003d Util.monotonicNow();\n+    HttpURLConnection connection \u003d (HttpURLConnection)\n+      SecurityUtil.openSecureHttpConnection(url);\n+\n+    if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n+      throw new HttpGetFailedException(\n+          \"Image transfer servlet at \" + url +\n+          \" failed with status code \" + connection.getResponseCode() +\n+          \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n+          connection);\n+    }\n+    \n+    long advertisedSize;\n+    String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n+    if (contentLength !\u003d null) {\n+      advertisedSize \u003d Long.parseLong(contentLength);\n+    } else {\n+      throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n+                            \"by the namenode when trying to fetch \" + url);\n+    }\n+    \n+    if (localPaths !\u003d null) {\n+      String fsImageName \u003d connection.getHeaderField(\n+          GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n+      // If the local paths refer to directories, use the server-provided header\n+      // as the filename within that directory\n+      List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n+      for (File localPath : localPaths) {\n+        if (localPath.isDirectory()) {\n+          if (fsImageName \u003d\u003d null) {\n+            throw new IOException(\"No filename header provided by server\");\n+          }\n+          newLocalPaths.add(new File(localPath, fsImageName));\n+        } else {\n+          newLocalPaths.add(localPath);\n+        }\n+      }\n+      localPaths \u003d newLocalPaths;\n+    }\n+    \n+    MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n+\n+    long received \u003d 0;\n+    InputStream stream \u003d connection.getInputStream();\n+    MessageDigest digester \u003d null;\n+    if (getChecksum) {\n+      digester \u003d MD5Hash.getDigester();\n+      stream \u003d new DigestInputStream(stream, digester);\n+    }\n+    boolean finishedReceiving \u003d false;\n+\n+    List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n+\n+    try {\n+      if (localPaths !\u003d null) {\n+        for (File f : localPaths) {\n+          try {\n+            if (f.exists()) {\n+              LOG.warn(\"Overwriting existing file \" + f\n+                  + \" with file downloaded from \" + url);\n+            }\n+            outputStreams.add(new FileOutputStream(f));\n+          } catch (IOException ioe) {\n+            LOG.warn(\"Unable to download file \" + f, ioe);\n+            // This will be null if we\u0027re downloading the fsimage to a file\n+            // outside of an NNStorage directory.\n+            if (dstStorage !\u003d null \u0026\u0026\n+                (dstStorage instanceof StorageErrorReporter)) {\n+              ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n+            }\n+          }\n+        }\n+        \n+        if (outputStreams.isEmpty()) {\n+          throw new IOException(\n+              \"Unable to download to any storage directory\");\n+        }\n+      }\n+      \n+      int num \u003d 1;\n+      byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n+      while (num \u003e 0) {\n+        num \u003d stream.read(buf);\n+        if (num \u003e 0) {\n+          received +\u003d num;\n+          for (FileOutputStream fos : outputStreams) {\n+            fos.write(buf, 0, num);\n+          }\n+        }\n+      }\n+      finishedReceiving \u003d true;\n+    } finally {\n+      stream.close();\n+      for (FileOutputStream fos : outputStreams) {\n+        fos.getChannel().force(true);\n+        fos.close();\n+      }\n+      if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n+        // only throw this exception if we think we read all of it on our end\n+        // -- otherwise a client-side IOException would be masked by this\n+        // exception that makes it look like a server-side problem!\n+        throw new IOException(\"File \" + url + \" received length \" + received +\n+                              \" is not of the advertised size \" +\n+                              advertisedSize);\n+      }\n+    }\n+    double xferSec \u003d Math.max(\n+        ((float)(Util.monotonicNow() - startTime)) / 1000.0, 0.001);\n+    long xferKb \u003d received / 1024;\n+    LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n+        xferSec, xferKb / xferSec));\n+\n+    if (digester !\u003d null) {\n+      MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n+      \n+      if (advertisedDigest !\u003d null \u0026\u0026\n+          !computedDigest.equals(advertisedDigest)) {\n+        throw new IOException(\"File \" + url + \" computed digest \" +\n+            computedDigest + \" does not match advertised digest \" + \n+            advertisedDigest);\n+      }\n+      return computedDigest;\n+    } else {\n+      return null;\n+    }    \n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static MD5Hash doGetUrl(URL url, List\u003cFile\u003e localPaths,\n      Storage dstStorage, boolean getChecksum) throws IOException {\n    long startTime \u003d Util.monotonicNow();\n    HttpURLConnection connection \u003d (HttpURLConnection)\n      SecurityUtil.openSecureHttpConnection(url);\n\n    if (connection.getResponseCode() !\u003d HttpURLConnection.HTTP_OK) {\n      throw new HttpGetFailedException(\n          \"Image transfer servlet at \" + url +\n          \" failed with status code \" + connection.getResponseCode() +\n          \"\\nResponse message:\\n\" + connection.getResponseMessage(),\n          connection);\n    }\n    \n    long advertisedSize;\n    String contentLength \u003d connection.getHeaderField(CONTENT_LENGTH);\n    if (contentLength !\u003d null) {\n      advertisedSize \u003d Long.parseLong(contentLength);\n    } else {\n      throw new IOException(CONTENT_LENGTH + \" header is not provided \" +\n                            \"by the namenode when trying to fetch \" + url);\n    }\n    \n    if (localPaths !\u003d null) {\n      String fsImageName \u003d connection.getHeaderField(\n          GetImageServlet.HADOOP_IMAGE_EDITS_HEADER);\n      // If the local paths refer to directories, use the server-provided header\n      // as the filename within that directory\n      List\u003cFile\u003e newLocalPaths \u003d new ArrayList\u003cFile\u003e();\n      for (File localPath : localPaths) {\n        if (localPath.isDirectory()) {\n          if (fsImageName \u003d\u003d null) {\n            throw new IOException(\"No filename header provided by server\");\n          }\n          newLocalPaths.add(new File(localPath, fsImageName));\n        } else {\n          newLocalPaths.add(localPath);\n        }\n      }\n      localPaths \u003d newLocalPaths;\n    }\n    \n    MD5Hash advertisedDigest \u003d parseMD5Header(connection);\n\n    long received \u003d 0;\n    InputStream stream \u003d connection.getInputStream();\n    MessageDigest digester \u003d null;\n    if (getChecksum) {\n      digester \u003d MD5Hash.getDigester();\n      stream \u003d new DigestInputStream(stream, digester);\n    }\n    boolean finishedReceiving \u003d false;\n\n    List\u003cFileOutputStream\u003e outputStreams \u003d Lists.newArrayList();\n\n    try {\n      if (localPaths !\u003d null) {\n        for (File f : localPaths) {\n          try {\n            if (f.exists()) {\n              LOG.warn(\"Overwriting existing file \" + f\n                  + \" with file downloaded from \" + url);\n            }\n            outputStreams.add(new FileOutputStream(f));\n          } catch (IOException ioe) {\n            LOG.warn(\"Unable to download file \" + f, ioe);\n            // This will be null if we\u0027re downloading the fsimage to a file\n            // outside of an NNStorage directory.\n            if (dstStorage !\u003d null \u0026\u0026\n                (dstStorage instanceof StorageErrorReporter)) {\n              ((StorageErrorReporter)dstStorage).reportErrorOnFile(f);\n            }\n          }\n        }\n        \n        if (outputStreams.isEmpty()) {\n          throw new IOException(\n              \"Unable to download to any storage directory\");\n        }\n      }\n      \n      int num \u003d 1;\n      byte[] buf \u003d new byte[HdfsConstants.IO_FILE_BUFFER_SIZE];\n      while (num \u003e 0) {\n        num \u003d stream.read(buf);\n        if (num \u003e 0) {\n          received +\u003d num;\n          for (FileOutputStream fos : outputStreams) {\n            fos.write(buf, 0, num);\n          }\n        }\n      }\n      finishedReceiving \u003d true;\n    } finally {\n      stream.close();\n      for (FileOutputStream fos : outputStreams) {\n        fos.getChannel().force(true);\n        fos.close();\n      }\n      if (finishedReceiving \u0026\u0026 received !\u003d advertisedSize) {\n        // only throw this exception if we think we read all of it on our end\n        // -- otherwise a client-side IOException would be masked by this\n        // exception that makes it look like a server-side problem!\n        throw new IOException(\"File \" + url + \" received length \" + received +\n                              \" is not of the advertised size \" +\n                              advertisedSize);\n      }\n    }\n    double xferSec \u003d Math.max(\n        ((float)(Util.monotonicNow() - startTime)) / 1000.0, 0.001);\n    long xferKb \u003d received / 1024;\n    LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n        xferSec, xferKb / xferSec));\n\n    if (digester !\u003d null) {\n      MD5Hash computedDigest \u003d new MD5Hash(digester.digest());\n      \n      if (advertisedDigest !\u003d null \u0026\u0026\n          !computedDigest.equals(advertisedDigest)) {\n        throw new IOException(\"File \" + url + \" computed digest \" +\n            computedDigest + \" does not match advertised digest \" + \n            advertisedDigest);\n      }\n      return computedDigest;\n    } else {\n      return null;\n    }    \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java"
    }
  }
}