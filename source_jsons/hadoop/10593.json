{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockRecoveryWorker.java",
  "functionName": "truncatePartialBlock",
  "functionId": "truncatePartialBlock___rurList-List__BlockRecord____safeLength-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java",
  "functionStartLine": 487,
  "functionEndLine": 513,
  "numCommitsSeen": 15,
  "timeTaken": 1393,
  "changeHistory": [
    "61ab0440f7eaff0f631cbae0378403912f88d7ad"
  ],
  "changeHistoryShort": {
    "61ab0440f7eaff0f631cbae0378403912f88d7ad": "Yintroduced"
  },
  "changeHistoryDetails": {
    "61ab0440f7eaff0f631cbae0378403912f88d7ad": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9173. Erasure Coding: Lease recovery for striped file. Contributed by Walter Su and Jing Zhao.\n\nChange-Id: I51703a61c9d8454f883028f3f6acb5729fde1b15\n",
      "commitDate": "18/12/15 3:57 PM",
      "commitName": "61ab0440f7eaff0f631cbae0378403912f88d7ad",
      "commitAuthor": "Zhe Zhang",
      "diff": "@@ -0,0 +1,27 @@\n+    private void truncatePartialBlock(List\u003cBlockRecord\u003e rurList,\n+        long safeLength) throws IOException {\n+      int cellSize \u003d ecPolicy.getCellSize();\n+      int dataBlkNum \u003d ecPolicy.getNumDataUnits();\n+      List\u003cDatanodeID\u003e failedList \u003d new ArrayList\u003c\u003e();\n+      for (BlockRecord r : rurList) {\n+        int blockIndex \u003d (int) (r.rInfo.getBlockId() \u0026 BLOCK_GROUP_INDEX_MASK);\n+        long newSize \u003d getInternalBlockLength(safeLength, cellSize, dataBlkNum,\n+            blockIndex);\n+        try {\n+          r.updateReplicaUnderRecovery(bpid, recoveryId, r.rInfo.getBlockId(),\n+              newSize);\n+        } catch (IOException e) {\n+          InterDatanodeProtocol.LOG.warn(\"Failed to updateBlock (newblock\u003d\"\n+              + \", datanode\u003d\" + r.id + \")\", e);\n+          failedList.add(r.id);\n+        }\n+      }\n+\n+      // If any of the data-nodes failed, the recovery fails, because\n+      // we never know the actual state of the replica on failed data-nodes.\n+      // The recovery should be started over.\n+      if (!failedList.isEmpty()) {\n+        throw new IOException(\"Cannot recover \" + block\n+            + \", the following datanodes failed: \" + failedList);\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void truncatePartialBlock(List\u003cBlockRecord\u003e rurList,\n        long safeLength) throws IOException {\n      int cellSize \u003d ecPolicy.getCellSize();\n      int dataBlkNum \u003d ecPolicy.getNumDataUnits();\n      List\u003cDatanodeID\u003e failedList \u003d new ArrayList\u003c\u003e();\n      for (BlockRecord r : rurList) {\n        int blockIndex \u003d (int) (r.rInfo.getBlockId() \u0026 BLOCK_GROUP_INDEX_MASK);\n        long newSize \u003d getInternalBlockLength(safeLength, cellSize, dataBlkNum,\n            blockIndex);\n        try {\n          r.updateReplicaUnderRecovery(bpid, recoveryId, r.rInfo.getBlockId(),\n              newSize);\n        } catch (IOException e) {\n          InterDatanodeProtocol.LOG.warn(\"Failed to updateBlock (newblock\u003d\"\n              + \", datanode\u003d\" + r.id + \")\", e);\n          failedList.add(r.id);\n        }\n      }\n\n      // If any of the data-nodes failed, the recovery fails, because\n      // we never know the actual state of the replica on failed data-nodes.\n      // The recovery should be started over.\n      if (!failedList.isEmpty()) {\n        throw new IOException(\"Cannot recover \" + block\n            + \", the following datanodes failed: \" + failedList);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java"
    }
  }
}