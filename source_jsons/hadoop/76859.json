{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RSLegacyRawDecoder.java",
  "functionName": "doDecode",
  "functionId": "doDecode___decodingState-ByteBufferDecodingState",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java",
  "functionStartLine": 168,
  "functionEndLine": 217,
  "numCommitsSeen": 15,
  "timeTaken": 4327,
  "changeHistory": [
    "a22fe02fba66280a8e994282e9ead23d9e20669a",
    "8fbb57fbd903a838684fa87cf15767d13695e4ed",
    "77202fa1035a54496d11d07472fbc399148ff630",
    "efdc0070d880c7e1b778e0029a1b827ca962ce70",
    "c89a14a8a4fe58f01f0cba643f2bc203e1a8701e",
    "5eca6dece67620f990f3306b6caaf09f317b38f6",
    "4ad484883f773c702a1874fc12816ef1a4a54136",
    "343c0e76fcd95ac739ca7cd6742c9d617e19fc37",
    "17f7cdc04764524c091bb0e9eb43399f88ac0e6b",
    "dae27f6dd14ac3ed0b9821a3c5239569b13f6adf"
  ],
  "changeHistoryShort": {
    "a22fe02fba66280a8e994282e9ead23d9e20669a": "Yfilerename",
    "8fbb57fbd903a838684fa87cf15767d13695e4ed": "Ybodychange",
    "77202fa1035a54496d11d07472fbc399148ff630": "Ymultichange(Yparameterchange,Ybodychange)",
    "efdc0070d880c7e1b778e0029a1b827ca962ce70": "Yfilerename",
    "c89a14a8a4fe58f01f0cba643f2bc203e1a8701e": "Ybodychange",
    "5eca6dece67620f990f3306b6caaf09f317b38f6": "Ybodychange",
    "4ad484883f773c702a1874fc12816ef1a4a54136": "Ybodychange",
    "343c0e76fcd95ac739ca7cd6742c9d617e19fc37": "Ybodychange",
    "17f7cdc04764524c091bb0e9eb43399f88ac0e6b": "Yfilerename",
    "dae27f6dd14ac3ed0b9821a3c5239569b13f6adf": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a22fe02fba66280a8e994282e9ead23d9e20669a": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-14261. Some refactoring work for erasure coding raw coder. Contributed by Lin Zeng.\n",
      "commitDate": "21/04/17 11:35 AM",
      "commitName": "a22fe02fba66280a8e994282e9ead23d9e20669a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "20/04/17 10:33 PM",
      "commitNameOld": "b0803388fc5ec03b774aa003f52232deb8db6f69",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 0.54,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void doDecode(ByteBufferDecodingState decodingState) {\n    int dataLen \u003d decodingState.decodeLength;\n    CoderUtil.resetOutputBuffers(decodingState.outputs, dataLen);\n\n    /**\n     * As passed parameters are friendly to callers but not to the underlying\n     * implementations, so we have to adjust them before calling doDecodeImpl.\n     */\n\n    int[] erasedOrNotToReadIndexes \u003d\n        CoderUtil.getNullIndexes(decodingState.inputs);\n\n    ByteBuffer[] directBuffers \u003d new ByteBuffer[getNumParityUnits()];\n    ByteBuffer[] adjustedDirectBufferOutputsParameter \u003d\n        new ByteBuffer[getNumParityUnits()];\n\n    // Use the caller passed buffers in erasedIndexes positions\n    for (int outputIdx \u003d 0, i \u003d 0;\n         i \u003c decodingState.erasedIndexes.length; i++) {\n      boolean found \u003d false;\n      for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n        // If this index is one requested by the caller via erasedIndexes, then\n        // we use the passed output buffer to avoid copying data thereafter.\n        if (decodingState.erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n          found \u003d true;\n          adjustedDirectBufferOutputsParameter[j] \u003d CoderUtil.resetBuffer(\n              decodingState.outputs[outputIdx++], dataLen);\n        }\n      }\n      if (!found) {\n        throw new HadoopIllegalArgumentException(\n            \"Inputs not fully corresponding to erasedIndexes in null places\");\n      }\n    }\n    // Use shared buffers for other positions (not set yet)\n    for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n        ByteBuffer buffer \u003d checkGetDirectBuffer(\n            directBuffers, bufferIdx, dataLen);\n        buffer.position(0);\n        buffer.limit(dataLen);\n        adjustedDirectBufferOutputsParameter[i] \u003d\n            CoderUtil.resetBuffer(buffer, dataLen);\n        bufferIdx++;\n      }\n    }\n\n    doDecodeImpl(decodingState.inputs, erasedOrNotToReadIndexes,\n        adjustedDirectBufferOutputsParameter);\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java",
      "extendedDetails": {
        "oldPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoderLegacy.java",
        "newPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java"
      }
    },
    "8fbb57fbd903a838684fa87cf15767d13695e4ed": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11588. Benchmark framework and test for erasure coders. Contributed by Rui Li\n",
      "commitDate": "12/08/16 12:05 AM",
      "commitName": "8fbb57fbd903a838684fa87cf15767d13695e4ed",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "26/05/16 10:23 PM",
      "commitNameOld": "77202fa1035a54496d11d07472fbc399148ff630",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 77.07,
      "commitsBetweenForRepo": 638,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,50 @@\n   protected void doDecode(ByteBufferDecodingState decodingState) {\n     int dataLen \u003d decodingState.decodeLength;\n     CoderUtil.resetOutputBuffers(decodingState.outputs, dataLen);\n \n     /**\n      * As passed parameters are friendly to callers but not to the underlying\n      * implementations, so we have to adjust them before calling doDecodeImpl.\n      */\n \n     int[] erasedOrNotToReadIndexes \u003d\n         CoderUtil.getNullIndexes(decodingState.inputs);\n \n-    // Prepare for adjustedDirectBufferOutputsParameter\n+    ByteBuffer[] directBuffers \u003d new ByteBuffer[getNumParityUnits()];\n+    ByteBuffer[] adjustedDirectBufferOutputsParameter \u003d\n+        new ByteBuffer[getNumParityUnits()];\n \n-    // First reset the positions needed this time\n-    for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n-      adjustedDirectBufferOutputsParameter[i] \u003d null;\n-    }\n     // Use the caller passed buffers in erasedIndexes positions\n     for (int outputIdx \u003d 0, i \u003d 0;\n          i \u003c decodingState.erasedIndexes.length; i++) {\n       boolean found \u003d false;\n       for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n         // If this index is one requested by the caller via erasedIndexes, then\n         // we use the passed output buffer to avoid copying data thereafter.\n         if (decodingState.erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n           found \u003d true;\n           adjustedDirectBufferOutputsParameter[j] \u003d CoderUtil.resetBuffer(\n               decodingState.outputs[outputIdx++], dataLen);\n         }\n       }\n       if (!found) {\n         throw new HadoopIllegalArgumentException(\n             \"Inputs not fully corresponding to erasedIndexes in null places\");\n       }\n     }\n     // Use shared buffers for other positions (not set yet)\n     for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n       if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n-        ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n+        ByteBuffer buffer \u003d checkGetDirectBuffer(\n+            directBuffers, bufferIdx, dataLen);\n         buffer.position(0);\n         buffer.limit(dataLen);\n         adjustedDirectBufferOutputsParameter[i] \u003d\n             CoderUtil.resetBuffer(buffer, dataLen);\n         bufferIdx++;\n       }\n     }\n \n     doDecodeImpl(decodingState.inputs, erasedOrNotToReadIndexes,\n         adjustedDirectBufferOutputsParameter);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(ByteBufferDecodingState decodingState) {\n    int dataLen \u003d decodingState.decodeLength;\n    CoderUtil.resetOutputBuffers(decodingState.outputs, dataLen);\n\n    /**\n     * As passed parameters are friendly to callers but not to the underlying\n     * implementations, so we have to adjust them before calling doDecodeImpl.\n     */\n\n    int[] erasedOrNotToReadIndexes \u003d\n        CoderUtil.getNullIndexes(decodingState.inputs);\n\n    ByteBuffer[] directBuffers \u003d new ByteBuffer[getNumParityUnits()];\n    ByteBuffer[] adjustedDirectBufferOutputsParameter \u003d\n        new ByteBuffer[getNumParityUnits()];\n\n    // Use the caller passed buffers in erasedIndexes positions\n    for (int outputIdx \u003d 0, i \u003d 0;\n         i \u003c decodingState.erasedIndexes.length; i++) {\n      boolean found \u003d false;\n      for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n        // If this index is one requested by the caller via erasedIndexes, then\n        // we use the passed output buffer to avoid copying data thereafter.\n        if (decodingState.erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n          found \u003d true;\n          adjustedDirectBufferOutputsParameter[j] \u003d CoderUtil.resetBuffer(\n              decodingState.outputs[outputIdx++], dataLen);\n        }\n      }\n      if (!found) {\n        throw new HadoopIllegalArgumentException(\n            \"Inputs not fully corresponding to erasedIndexes in null places\");\n      }\n    }\n    // Use shared buffers for other positions (not set yet)\n    for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n        ByteBuffer buffer \u003d checkGetDirectBuffer(\n            directBuffers, bufferIdx, dataLen);\n        buffer.position(0);\n        buffer.limit(dataLen);\n        adjustedDirectBufferOutputsParameter[i] \u003d\n            CoderUtil.resetBuffer(buffer, dataLen);\n        bufferIdx++;\n      }\n    }\n\n    doDecodeImpl(decodingState.inputs, erasedOrNotToReadIndexes,\n        adjustedDirectBufferOutputsParameter);\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoderLegacy.java",
      "extendedDetails": {}
    },
    "77202fa1035a54496d11d07472fbc399148ff630": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
      "commitDate": "26/05/16 10:23 PM",
      "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
          "commitDate": "26/05/16 10:23 PM",
          "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "24/02/16 2:29 PM",
          "commitNameOld": "efdc0070d880c7e1b778e0029a1b827ca962ce70",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 92.29,
          "commitsBetweenForRepo": 576,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,51 @@\n-  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n-                          ByteBuffer[] outputs) {\n-    ByteBuffer validInput \u003d CoderUtil.findFirstValidInput(inputs);\n-    int dataLen \u003d validInput.remaining();\n+  protected void doDecode(ByteBufferDecodingState decodingState) {\n+    int dataLen \u003d decodingState.decodeLength;\n+    CoderUtil.resetOutputBuffers(decodingState.outputs, dataLen);\n \n     /**\n      * As passed parameters are friendly to callers but not to the underlying\n      * implementations, so we have to adjust them before calling doDecodeImpl.\n      */\n \n     int[] erasedOrNotToReadIndexes \u003d\n-        CoderUtil.getErasedOrNotToReadIndexes(inputs);\n+        CoderUtil.getNullIndexes(decodingState.inputs);\n \n     // Prepare for adjustedDirectBufferOutputsParameter\n \n     // First reset the positions needed this time\n     for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n       adjustedDirectBufferOutputsParameter[i] \u003d null;\n     }\n     // Use the caller passed buffers in erasedIndexes positions\n-    for (int outputIdx \u003d 0, i \u003d 0; i \u003c erasedIndexes.length; i++) {\n+    for (int outputIdx \u003d 0, i \u003d 0;\n+         i \u003c decodingState.erasedIndexes.length; i++) {\n       boolean found \u003d false;\n       for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n         // If this index is one requested by the caller via erasedIndexes, then\n         // we use the passed output buffer to avoid copying data thereafter.\n-        if (erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n+        if (decodingState.erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n           found \u003d true;\n-          adjustedDirectBufferOutputsParameter[j] \u003d\n-              resetBuffer(outputs[outputIdx++], dataLen);\n+          adjustedDirectBufferOutputsParameter[j] \u003d CoderUtil.resetBuffer(\n+              decodingState.outputs[outputIdx++], dataLen);\n         }\n       }\n       if (!found) {\n         throw new HadoopIllegalArgumentException(\n             \"Inputs not fully corresponding to erasedIndexes in null places\");\n       }\n     }\n     // Use shared buffers for other positions (not set yet)\n     for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n       if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n         ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n         buffer.position(0);\n         buffer.limit(dataLen);\n-        adjustedDirectBufferOutputsParameter[i] \u003d resetBuffer(buffer, dataLen);\n+        adjustedDirectBufferOutputsParameter[i] \u003d\n+            CoderUtil.resetBuffer(buffer, dataLen);\n         bufferIdx++;\n       }\n     }\n \n-    doDecodeImpl(inputs, erasedOrNotToReadIndexes,\n+    doDecodeImpl(decodingState.inputs, erasedOrNotToReadIndexes,\n         adjustedDirectBufferOutputsParameter);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void doDecode(ByteBufferDecodingState decodingState) {\n    int dataLen \u003d decodingState.decodeLength;\n    CoderUtil.resetOutputBuffers(decodingState.outputs, dataLen);\n\n    /**\n     * As passed parameters are friendly to callers but not to the underlying\n     * implementations, so we have to adjust them before calling doDecodeImpl.\n     */\n\n    int[] erasedOrNotToReadIndexes \u003d\n        CoderUtil.getNullIndexes(decodingState.inputs);\n\n    // Prepare for adjustedDirectBufferOutputsParameter\n\n    // First reset the positions needed this time\n    for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      adjustedDirectBufferOutputsParameter[i] \u003d null;\n    }\n    // Use the caller passed buffers in erasedIndexes positions\n    for (int outputIdx \u003d 0, i \u003d 0;\n         i \u003c decodingState.erasedIndexes.length; i++) {\n      boolean found \u003d false;\n      for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n        // If this index is one requested by the caller via erasedIndexes, then\n        // we use the passed output buffer to avoid copying data thereafter.\n        if (decodingState.erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n          found \u003d true;\n          adjustedDirectBufferOutputsParameter[j] \u003d CoderUtil.resetBuffer(\n              decodingState.outputs[outputIdx++], dataLen);\n        }\n      }\n      if (!found) {\n        throw new HadoopIllegalArgumentException(\n            \"Inputs not fully corresponding to erasedIndexes in null places\");\n      }\n    }\n    // Use shared buffers for other positions (not set yet)\n    for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n        ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n        buffer.position(0);\n        buffer.limit(dataLen);\n        adjustedDirectBufferOutputsParameter[i] \u003d\n            CoderUtil.resetBuffer(buffer, dataLen);\n        bufferIdx++;\n      }\n    }\n\n    doDecodeImpl(decodingState.inputs, erasedOrNotToReadIndexes,\n        adjustedDirectBufferOutputsParameter);\n  }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoderLegacy.java",
          "extendedDetails": {
            "oldValue": "[inputs-ByteBuffer[], erasedIndexes-int[], outputs-ByteBuffer[]]",
            "newValue": "[decodingState-ByteBufferDecodingState]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
          "commitDate": "26/05/16 10:23 PM",
          "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "24/02/16 2:29 PM",
          "commitNameOld": "efdc0070d880c7e1b778e0029a1b827ca962ce70",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 92.29,
          "commitsBetweenForRepo": 576,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,51 @@\n-  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n-                          ByteBuffer[] outputs) {\n-    ByteBuffer validInput \u003d CoderUtil.findFirstValidInput(inputs);\n-    int dataLen \u003d validInput.remaining();\n+  protected void doDecode(ByteBufferDecodingState decodingState) {\n+    int dataLen \u003d decodingState.decodeLength;\n+    CoderUtil.resetOutputBuffers(decodingState.outputs, dataLen);\n \n     /**\n      * As passed parameters are friendly to callers but not to the underlying\n      * implementations, so we have to adjust them before calling doDecodeImpl.\n      */\n \n     int[] erasedOrNotToReadIndexes \u003d\n-        CoderUtil.getErasedOrNotToReadIndexes(inputs);\n+        CoderUtil.getNullIndexes(decodingState.inputs);\n \n     // Prepare for adjustedDirectBufferOutputsParameter\n \n     // First reset the positions needed this time\n     for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n       adjustedDirectBufferOutputsParameter[i] \u003d null;\n     }\n     // Use the caller passed buffers in erasedIndexes positions\n-    for (int outputIdx \u003d 0, i \u003d 0; i \u003c erasedIndexes.length; i++) {\n+    for (int outputIdx \u003d 0, i \u003d 0;\n+         i \u003c decodingState.erasedIndexes.length; i++) {\n       boolean found \u003d false;\n       for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n         // If this index is one requested by the caller via erasedIndexes, then\n         // we use the passed output buffer to avoid copying data thereafter.\n-        if (erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n+        if (decodingState.erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n           found \u003d true;\n-          adjustedDirectBufferOutputsParameter[j] \u003d\n-              resetBuffer(outputs[outputIdx++], dataLen);\n+          adjustedDirectBufferOutputsParameter[j] \u003d CoderUtil.resetBuffer(\n+              decodingState.outputs[outputIdx++], dataLen);\n         }\n       }\n       if (!found) {\n         throw new HadoopIllegalArgumentException(\n             \"Inputs not fully corresponding to erasedIndexes in null places\");\n       }\n     }\n     // Use shared buffers for other positions (not set yet)\n     for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n       if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n         ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n         buffer.position(0);\n         buffer.limit(dataLen);\n-        adjustedDirectBufferOutputsParameter[i] \u003d resetBuffer(buffer, dataLen);\n+        adjustedDirectBufferOutputsParameter[i] \u003d\n+            CoderUtil.resetBuffer(buffer, dataLen);\n         bufferIdx++;\n       }\n     }\n \n-    doDecodeImpl(inputs, erasedOrNotToReadIndexes,\n+    doDecodeImpl(decodingState.inputs, erasedOrNotToReadIndexes,\n         adjustedDirectBufferOutputsParameter);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void doDecode(ByteBufferDecodingState decodingState) {\n    int dataLen \u003d decodingState.decodeLength;\n    CoderUtil.resetOutputBuffers(decodingState.outputs, dataLen);\n\n    /**\n     * As passed parameters are friendly to callers but not to the underlying\n     * implementations, so we have to adjust them before calling doDecodeImpl.\n     */\n\n    int[] erasedOrNotToReadIndexes \u003d\n        CoderUtil.getNullIndexes(decodingState.inputs);\n\n    // Prepare for adjustedDirectBufferOutputsParameter\n\n    // First reset the positions needed this time\n    for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      adjustedDirectBufferOutputsParameter[i] \u003d null;\n    }\n    // Use the caller passed buffers in erasedIndexes positions\n    for (int outputIdx \u003d 0, i \u003d 0;\n         i \u003c decodingState.erasedIndexes.length; i++) {\n      boolean found \u003d false;\n      for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n        // If this index is one requested by the caller via erasedIndexes, then\n        // we use the passed output buffer to avoid copying data thereafter.\n        if (decodingState.erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n          found \u003d true;\n          adjustedDirectBufferOutputsParameter[j] \u003d CoderUtil.resetBuffer(\n              decodingState.outputs[outputIdx++], dataLen);\n        }\n      }\n      if (!found) {\n        throw new HadoopIllegalArgumentException(\n            \"Inputs not fully corresponding to erasedIndexes in null places\");\n      }\n    }\n    // Use shared buffers for other positions (not set yet)\n    for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n        ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n        buffer.position(0);\n        buffer.limit(dataLen);\n        adjustedDirectBufferOutputsParameter[i] \u003d\n            CoderUtil.resetBuffer(buffer, dataLen);\n        bufferIdx++;\n      }\n    }\n\n    doDecodeImpl(decodingState.inputs, erasedOrNotToReadIndexes,\n        adjustedDirectBufferOutputsParameter);\n  }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoderLegacy.java",
          "extendedDetails": {}
        }
      ]
    },
    "efdc0070d880c7e1b778e0029a1b827ca962ce70": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-12808. Rename the RS coder from HDFS-RAID as legacy. Contributed by Rui Li.\n\nChange-Id: Icb64eb60833fe0139aadb6da9aa666f664defc0e\n",
      "commitDate": "24/02/16 2:29 PM",
      "commitName": "efdc0070d880c7e1b778e0029a1b827ca962ce70",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "24/02/16 1:55 PM",
      "commitNameOld": "d6b181c6faa56e43c9f05d2cc860a0aeb940fd90",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    ByteBuffer validInput \u003d CoderUtil.findFirstValidInput(inputs);\n    int dataLen \u003d validInput.remaining();\n\n    /**\n     * As passed parameters are friendly to callers but not to the underlying\n     * implementations, so we have to adjust them before calling doDecodeImpl.\n     */\n\n    int[] erasedOrNotToReadIndexes \u003d\n        CoderUtil.getErasedOrNotToReadIndexes(inputs);\n\n    // Prepare for adjustedDirectBufferOutputsParameter\n\n    // First reset the positions needed this time\n    for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      adjustedDirectBufferOutputsParameter[i] \u003d null;\n    }\n    // Use the caller passed buffers in erasedIndexes positions\n    for (int outputIdx \u003d 0, i \u003d 0; i \u003c erasedIndexes.length; i++) {\n      boolean found \u003d false;\n      for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n        // If this index is one requested by the caller via erasedIndexes, then\n        // we use the passed output buffer to avoid copying data thereafter.\n        if (erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n          found \u003d true;\n          adjustedDirectBufferOutputsParameter[j] \u003d\n              resetBuffer(outputs[outputIdx++], dataLen);\n        }\n      }\n      if (!found) {\n        throw new HadoopIllegalArgumentException(\n            \"Inputs not fully corresponding to erasedIndexes in null places\");\n      }\n    }\n    // Use shared buffers for other positions (not set yet)\n    for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n        ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n        buffer.position(0);\n        buffer.limit(dataLen);\n        adjustedDirectBufferOutputsParameter[i] \u003d resetBuffer(buffer, dataLen);\n        bufferIdx++;\n      }\n    }\n\n    doDecodeImpl(inputs, erasedOrNotToReadIndexes,\n        adjustedDirectBufferOutputsParameter);\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoderLegacy.java",
      "extendedDetails": {
        "oldPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java",
        "newPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoderLegacy.java"
      }
    },
    "c89a14a8a4fe58f01f0cba643f2bc203e1a8701e": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12041. Implement another Reed-Solomon coder in pure Java. Contributed by Kai Zheng.\n\nChange-Id: I35ff2e498d4f988c9a064f74374f7c7258b7a6b7\n",
      "commitDate": "03/02/16 3:05 PM",
      "commitName": "c89a14a8a4fe58f01f0cba643f2bc203e1a8701e",
      "commitAuthor": "zhezhang",
      "commitDateOld": "29/10/15 12:04 AM",
      "commitNameOld": "5eca6dece67620f990f3306b6caaf09f317b38f6",
      "commitAuthorOld": "Walter Su",
      "daysBetweenCommits": 97.67,
      "commitsBetweenForRepo": 623,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,50 @@\n   protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                           ByteBuffer[] outputs) {\n-    ByteBuffer validInput \u003d findFirstValidInput(inputs);\n+    ByteBuffer validInput \u003d CoderUtil.findFirstValidInput(inputs);\n     int dataLen \u003d validInput.remaining();\n \n     /**\n      * As passed parameters are friendly to callers but not to the underlying\n      * implementations, so we have to adjust them before calling doDecodeImpl.\n      */\n \n-    int[] erasedOrNotToReadIndexes \u003d getErasedOrNotToReadIndexes(inputs);\n+    int[] erasedOrNotToReadIndexes \u003d\n+        CoderUtil.getErasedOrNotToReadIndexes(inputs);\n \n     // Prepare for adjustedDirectBufferOutputsParameter\n \n     // First reset the positions needed this time\n     for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n       adjustedDirectBufferOutputsParameter[i] \u003d null;\n     }\n     // Use the caller passed buffers in erasedIndexes positions\n     for (int outputIdx \u003d 0, i \u003d 0; i \u003c erasedIndexes.length; i++) {\n       boolean found \u003d false;\n       for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n         // If this index is one requested by the caller via erasedIndexes, then\n         // we use the passed output buffer to avoid copying data thereafter.\n         if (erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n           found \u003d true;\n           adjustedDirectBufferOutputsParameter[j] \u003d\n               resetBuffer(outputs[outputIdx++], dataLen);\n         }\n       }\n       if (!found) {\n         throw new HadoopIllegalArgumentException(\n             \"Inputs not fully corresponding to erasedIndexes in null places\");\n       }\n     }\n     // Use shared buffers for other positions (not set yet)\n     for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n       if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n         ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n         buffer.position(0);\n         buffer.limit(dataLen);\n         adjustedDirectBufferOutputsParameter[i] \u003d resetBuffer(buffer, dataLen);\n         bufferIdx++;\n       }\n     }\n \n     doDecodeImpl(inputs, erasedOrNotToReadIndexes,\n         adjustedDirectBufferOutputsParameter);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    ByteBuffer validInput \u003d CoderUtil.findFirstValidInput(inputs);\n    int dataLen \u003d validInput.remaining();\n\n    /**\n     * As passed parameters are friendly to callers but not to the underlying\n     * implementations, so we have to adjust them before calling doDecodeImpl.\n     */\n\n    int[] erasedOrNotToReadIndexes \u003d\n        CoderUtil.getErasedOrNotToReadIndexes(inputs);\n\n    // Prepare for adjustedDirectBufferOutputsParameter\n\n    // First reset the positions needed this time\n    for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      adjustedDirectBufferOutputsParameter[i] \u003d null;\n    }\n    // Use the caller passed buffers in erasedIndexes positions\n    for (int outputIdx \u003d 0, i \u003d 0; i \u003c erasedIndexes.length; i++) {\n      boolean found \u003d false;\n      for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n        // If this index is one requested by the caller via erasedIndexes, then\n        // we use the passed output buffer to avoid copying data thereafter.\n        if (erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n          found \u003d true;\n          adjustedDirectBufferOutputsParameter[j] \u003d\n              resetBuffer(outputs[outputIdx++], dataLen);\n        }\n      }\n      if (!found) {\n        throw new HadoopIllegalArgumentException(\n            \"Inputs not fully corresponding to erasedIndexes in null places\");\n      }\n    }\n    // Use shared buffers for other positions (not set yet)\n    for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n        ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n        buffer.position(0);\n        buffer.limit(dataLen);\n        adjustedDirectBufferOutputsParameter[i] \u003d resetBuffer(buffer, dataLen);\n        bufferIdx++;\n      }\n    }\n\n    doDecodeImpl(inputs, erasedOrNotToReadIndexes,\n        adjustedDirectBufferOutputsParameter);\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java",
      "extendedDetails": {}
    },
    "5eca6dece67620f990f3306b6caaf09f317b38f6": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12327. Initialize output buffers with ZERO bytes in erasure coder. Contributed by Kai Zheng.\n",
      "commitDate": "29/10/15 12:04 AM",
      "commitName": "5eca6dece67620f990f3306b6caaf09f317b38f6",
      "commitAuthor": "Walter Su",
      "commitDateOld": "28/10/15 1:18 AM",
      "commitNameOld": "c201cf951d5adefefe7c68e882a0c07962248577",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 0.95,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n   protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                           ByteBuffer[] outputs) {\n     ByteBuffer validInput \u003d findFirstValidInput(inputs);\n     int dataLen \u003d validInput.remaining();\n \n     /**\n      * As passed parameters are friendly to callers but not to the underlying\n      * implementations, so we have to adjust them before calling doDecodeImpl.\n      */\n \n     int[] erasedOrNotToReadIndexes \u003d getErasedOrNotToReadIndexes(inputs);\n \n     // Prepare for adjustedDirectBufferOutputsParameter\n \n     // First reset the positions needed this time\n     for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n       adjustedDirectBufferOutputsParameter[i] \u003d null;\n     }\n     // Use the caller passed buffers in erasedIndexes positions\n     for (int outputIdx \u003d 0, i \u003d 0; i \u003c erasedIndexes.length; i++) {\n       boolean found \u003d false;\n       for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n         // If this index is one requested by the caller via erasedIndexes, then\n         // we use the passed output buffer to avoid copying data thereafter.\n         if (erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n           found \u003d true;\n           adjustedDirectBufferOutputsParameter[j] \u003d\n-              resetBuffer(outputs[outputIdx++]);\n+              resetBuffer(outputs[outputIdx++], dataLen);\n         }\n       }\n       if (!found) {\n         throw new HadoopIllegalArgumentException(\n             \"Inputs not fully corresponding to erasedIndexes in null places\");\n       }\n     }\n     // Use shared buffers for other positions (not set yet)\n     for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n       if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n         ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n         buffer.position(0);\n         buffer.limit(dataLen);\n-        adjustedDirectBufferOutputsParameter[i] \u003d resetBuffer(buffer);\n+        adjustedDirectBufferOutputsParameter[i] \u003d resetBuffer(buffer, dataLen);\n         bufferIdx++;\n       }\n     }\n \n     doDecodeImpl(inputs, erasedOrNotToReadIndexes,\n         adjustedDirectBufferOutputsParameter);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    ByteBuffer validInput \u003d findFirstValidInput(inputs);\n    int dataLen \u003d validInput.remaining();\n\n    /**\n     * As passed parameters are friendly to callers but not to the underlying\n     * implementations, so we have to adjust them before calling doDecodeImpl.\n     */\n\n    int[] erasedOrNotToReadIndexes \u003d getErasedOrNotToReadIndexes(inputs);\n\n    // Prepare for adjustedDirectBufferOutputsParameter\n\n    // First reset the positions needed this time\n    for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      adjustedDirectBufferOutputsParameter[i] \u003d null;\n    }\n    // Use the caller passed buffers in erasedIndexes positions\n    for (int outputIdx \u003d 0, i \u003d 0; i \u003c erasedIndexes.length; i++) {\n      boolean found \u003d false;\n      for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n        // If this index is one requested by the caller via erasedIndexes, then\n        // we use the passed output buffer to avoid copying data thereafter.\n        if (erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n          found \u003d true;\n          adjustedDirectBufferOutputsParameter[j] \u003d\n              resetBuffer(outputs[outputIdx++], dataLen);\n        }\n      }\n      if (!found) {\n        throw new HadoopIllegalArgumentException(\n            \"Inputs not fully corresponding to erasedIndexes in null places\");\n      }\n    }\n    // Use shared buffers for other positions (not set yet)\n    for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n        ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n        buffer.position(0);\n        buffer.limit(dataLen);\n        adjustedDirectBufferOutputsParameter[i] \u003d resetBuffer(buffer, dataLen);\n        bufferIdx++;\n      }\n    }\n\n    doDecodeImpl(inputs, erasedOrNotToReadIndexes,\n        adjustedDirectBufferOutputsParameter);\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java",
      "extendedDetails": {}
    },
    "4ad484883f773c702a1874fc12816ef1a4a54136": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11847 Enhance raw coder allowing to read least required inputs in decoding. Contributed by Kai Zheng\n",
      "commitDate": "26/05/15 12:07 PM",
      "commitName": "4ad484883f773c702a1874fc12816ef1a4a54136",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "26/05/15 12:07 PM",
      "commitNameOld": "b30e96bfb4b8ce5537671c97f0c9c56cd195bfdc",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,49 @@\n   protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                           ByteBuffer[] outputs) {\n-    for (int i \u003d 0; i \u003c erasedIndexes.length; i++) {\n-      errSignature[i] \u003d primitivePower[erasedIndexes[i]];\n-      RSUtil.GF.substitute(inputs, outputs[i], primitivePower[i]);\n+    ByteBuffer validInput \u003d findFirstValidInput(inputs);\n+    int dataLen \u003d validInput.remaining();\n+\n+    /**\n+     * As passed parameters are friendly to callers but not to the underlying\n+     * implementations, so we have to adjust them before calling doDecodeImpl.\n+     */\n+\n+    int[] erasedOrNotToReadIndexes \u003d getErasedOrNotToReadIndexes(inputs);\n+\n+    // Prepare for adjustedDirectBufferOutputsParameter\n+\n+    // First reset the positions needed this time\n+    for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n+      adjustedDirectBufferOutputsParameter[i] \u003d null;\n+    }\n+    // Use the caller passed buffers in erasedIndexes positions\n+    for (int outputIdx \u003d 0, i \u003d 0; i \u003c erasedIndexes.length; i++) {\n+      boolean found \u003d false;\n+      for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n+        // If this index is one requested by the caller via erasedIndexes, then\n+        // we use the passed output buffer to avoid copying data thereafter.\n+        if (erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n+          found \u003d true;\n+          adjustedDirectBufferOutputsParameter[j] \u003d\n+              resetBuffer(outputs[outputIdx++]);\n+        }\n+      }\n+      if (!found) {\n+        throw new HadoopIllegalArgumentException(\n+            \"Inputs not fully corresponding to erasedIndexes in null places\");\n+      }\n+    }\n+    // Use shared buffers for other positions (not set yet)\n+    for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n+      if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n+        ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n+        buffer.position(0);\n+        buffer.limit(dataLen);\n+        adjustedDirectBufferOutputsParameter[i] \u003d resetBuffer(buffer);\n+        bufferIdx++;\n+      }\n     }\n \n-    RSUtil.GF.solveVandermondeSystem(errSignature,\n-        outputs, erasedIndexes.length);\n+    doDecodeImpl(inputs, erasedOrNotToReadIndexes,\n+        adjustedDirectBufferOutputsParameter);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    ByteBuffer validInput \u003d findFirstValidInput(inputs);\n    int dataLen \u003d validInput.remaining();\n\n    /**\n     * As passed parameters are friendly to callers but not to the underlying\n     * implementations, so we have to adjust them before calling doDecodeImpl.\n     */\n\n    int[] erasedOrNotToReadIndexes \u003d getErasedOrNotToReadIndexes(inputs);\n\n    // Prepare for adjustedDirectBufferOutputsParameter\n\n    // First reset the positions needed this time\n    for (int i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      adjustedDirectBufferOutputsParameter[i] \u003d null;\n    }\n    // Use the caller passed buffers in erasedIndexes positions\n    for (int outputIdx \u003d 0, i \u003d 0; i \u003c erasedIndexes.length; i++) {\n      boolean found \u003d false;\n      for (int j \u003d 0; j \u003c erasedOrNotToReadIndexes.length; j++) {\n        // If this index is one requested by the caller via erasedIndexes, then\n        // we use the passed output buffer to avoid copying data thereafter.\n        if (erasedIndexes[i] \u003d\u003d erasedOrNotToReadIndexes[j]) {\n          found \u003d true;\n          adjustedDirectBufferOutputsParameter[j] \u003d\n              resetBuffer(outputs[outputIdx++]);\n        }\n      }\n      if (!found) {\n        throw new HadoopIllegalArgumentException(\n            \"Inputs not fully corresponding to erasedIndexes in null places\");\n      }\n    }\n    // Use shared buffers for other positions (not set yet)\n    for (int bufferIdx \u003d 0, i \u003d 0; i \u003c erasedOrNotToReadIndexes.length; i++) {\n      if (adjustedDirectBufferOutputsParameter[i] \u003d\u003d null) {\n        ByteBuffer buffer \u003d checkGetDirectBuffer(bufferIdx, dataLen);\n        buffer.position(0);\n        buffer.limit(dataLen);\n        adjustedDirectBufferOutputsParameter[i] \u003d resetBuffer(buffer);\n        bufferIdx++;\n      }\n    }\n\n    doDecodeImpl(inputs, erasedOrNotToReadIndexes,\n        adjustedDirectBufferOutputsParameter);\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java",
      "extendedDetails": {}
    },
    "343c0e76fcd95ac739ca7cd6742c9d617e19fc37": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11938. Enhance ByteBuffer version encode/decode API of raw erasure coder. Contributed by Kai Zheng.\n",
      "commitDate": "26/05/15 12:02 PM",
      "commitName": "343c0e76fcd95ac739ca7cd6742c9d617e19fc37",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 11:55 AM",
      "commitNameOld": "17f7cdc04764524c091bb0e9eb43399f88ac0e6b",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 76,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,10 @@\n   protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                           ByteBuffer[] outputs) {\n     for (int i \u003d 0; i \u003c erasedIndexes.length; i++) {\n       errSignature[i] \u003d primitivePower[erasedIndexes[i]];\n       RSUtil.GF.substitute(inputs, outputs[i], primitivePower[i]);\n     }\n \n-    int dataLen \u003d inputs[0].remaining();\n-    RSUtil.GF.solveVandermondeSystem(errSignature, outputs,\n-        erasedIndexes.length, dataLen);\n+    RSUtil.GF.solveVandermondeSystem(errSignature,\n+        outputs, erasedIndexes.length);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    for (int i \u003d 0; i \u003c erasedIndexes.length; i++) {\n      errSignature[i] \u003d primitivePower[erasedIndexes[i]];\n      RSUtil.GF.substitute(inputs, outputs[i], primitivePower[i]);\n    }\n\n    RSUtil.GF.solveVandermondeSystem(errSignature,\n        outputs, erasedIndexes.length);\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java",
      "extendedDetails": {}
    },
    "17f7cdc04764524c091bb0e9eb43399f88ac0e6b": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-11805 Better to rename some raw erasure coders. Contributed by Kai Zheng\n",
      "commitDate": "26/05/15 11:55 AM",
      "commitName": "17f7cdc04764524c091bb0e9eb43399f88ac0e6b",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "26/05/15 11:55 AM",
      "commitNameOld": "146ce7a9784e52432b76164009336a4b2cf2860e",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    for (int i \u003d 0; i \u003c erasedIndexes.length; i++) {\n      errSignature[i] \u003d primitivePower[erasedIndexes[i]];\n      RSUtil.GF.substitute(inputs, outputs[i], primitivePower[i]);\n    }\n\n    int dataLen \u003d inputs[0].remaining();\n    RSUtil.GF.solveVandermondeSystem(errSignature, outputs,\n        erasedIndexes.length, dataLen);\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java",
      "extendedDetails": {
        "oldPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/JRSRawDecoder.java",
        "newPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java"
      }
    },
    "dae27f6dd14ac3ed0b9821a3c5239569b13f6adf": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-11542. Raw Reed-Solomon coder in pure Java. Contributed by Kai Zheng\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "dae27f6dd14ac3ed0b9821a3c5239569b13f6adf",
      "commitAuthor": "drankye",
      "diff": "@@ -0,0 +1,11 @@\n+  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n+                          ByteBuffer[] outputs) {\n+    for (int i \u003d 0; i \u003c erasedIndexes.length; i++) {\n+      errSignature[i] \u003d primitivePower[erasedIndexes[i]];\n+      RSUtil.GF.substitute(inputs, outputs[i], primitivePower[i]);\n+    }\n+\n+    int dataLen \u003d inputs[0].remaining();\n+    RSUtil.GF.solveVandermondeSystem(errSignature, outputs,\n+        erasedIndexes.length, dataLen);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    for (int i \u003d 0; i \u003c erasedIndexes.length; i++) {\n      errSignature[i] \u003d primitivePower[erasedIndexes[i]];\n      RSUtil.GF.substitute(inputs, outputs[i], primitivePower[i]);\n    }\n\n    int dataLen \u003d inputs[0].remaining();\n    RSUtil.GF.solveVandermondeSystem(errSignature, outputs,\n        erasedIndexes.length, dataLen);\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/JRSRawDecoder.java"
    }
  }
}