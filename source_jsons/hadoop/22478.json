{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FixedLengthRecordReader.java",
  "functionName": "initialize",
  "functionId": "initialize___job-Configuration__splitStart-long__splitLength-long__file-Path",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthRecordReader.java",
  "functionStartLine": 84,
  "functionEndLine": 131,
  "numCommitsSeen": 3,
  "timeTaken": 891,
  "changeHistory": [
    "f365957c6326f88734bc0a5d01cfb7eac713db20",
    "eeda370249d3f65a36718942a15867f79654ff66"
  ],
  "changeHistoryShort": {
    "f365957c6326f88734bc0a5d01cfb7eac713db20": "Ybodychange",
    "eeda370249d3f65a36718942a15867f79654ff66": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f365957c6326f88734bc0a5d01cfb7eac713db20": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15229. Add FileSystem builder-based openFile() API to match createFile();\nS3A to implement S3 Select through this API.\n\nThe new openFile() API is asynchronous, and implemented across FileSystem and FileContext.\n\nThe MapReduce V2 inputs are moved to this API, and you can actually set must/may\noptions to pass in.\n\nThis is more useful for setting things like s3a seek policy than for S3 select,\nas the existing input format/record readers can\u0027t handle S3 select output where\nthe stream is shorter than the file length, and splitting plain text is suboptimal.\nFuture work is needed there.\n\nIn the meantime, any/all filesystem connectors are now free to add their own filesystem-specific\nconfiguration parameters which can be set in jobs and used to set filesystem input stream\noptions (seek policy, retry, encryption secrets, etc).\n\nContributed by Steve Loughran\n",
      "commitDate": "05/02/19 3:51 AM",
      "commitName": "f365957c6326f88734bc0a5d01cfb7eac713db20",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "02/11/17 1:43 AM",
      "commitNameOld": "178751ed8c9d47038acf8616c226f1f52e884feb",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 460.13,
      "commitsBetweenForRepo": 3984,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,48 @@\n   public void initialize(Configuration job, long splitStart, long splitLength,\n                          Path file) throws IOException {\n     start \u003d splitStart;\n     end \u003d start + splitLength;\n     long partialRecordLength \u003d start % recordLength;\n     long numBytesToSkip \u003d 0;\n     if (partialRecordLength !\u003d 0) {\n       numBytesToSkip \u003d recordLength - partialRecordLength;\n     }\n \n-    // open the file and seek to the start of the split\n-    final FileSystem fs \u003d file.getFileSystem(job);\n-    fileIn \u003d fs.open(file);\n+    // open the file\n+    final FutureDataInputStreamBuilder builder \u003d\n+        file.getFileSystem(job).openFile(file);\n+    FutureIOSupport.propagateOptions(builder, job,\n+        MRJobConfig.INPUT_FILE_OPTION_PREFIX,\n+        MRJobConfig.INPUT_FILE_MANDATORY_PREFIX);\n+    fileIn \u003d FutureIOSupport.awaitFuture(builder.build());\n \n     CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n     if (null !\u003d codec) {\n       isCompressedInput \u003d true;\t\n       decompressor \u003d CodecPool.getDecompressor(codec);\n       CompressionInputStream cIn\n           \u003d codec.createInputStream(fileIn, decompressor);\n       filePosition \u003d cIn;\n       inputStream \u003d cIn;\n       numRecordsRemainingInSplit \u003d Long.MAX_VALUE;\n       LOG.info(\n           \"Compressed input; cannot compute number of records in the split\");\n     } else {\n       fileIn.seek(start);\n       filePosition \u003d fileIn;\n       inputStream \u003d fileIn;\n       long splitSize \u003d end - start - numBytesToSkip;\n       numRecordsRemainingInSplit \u003d (splitSize + recordLength - 1)/recordLength;\n       if (numRecordsRemainingInSplit \u003c 0) {\n         numRecordsRemainingInSplit \u003d 0;\n       }\n       LOG.info(\"Expecting \" + numRecordsRemainingInSplit\n           + \" records each with a length of \" + recordLength\n           + \" bytes in the split with an effective size of \"\n           + splitSize + \" bytes\");\n     }\n     if (numBytesToSkip !\u003d 0) {\n       start +\u003d inputStream.skip(numBytesToSkip);\n     }\n     this.pos \u003d start;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(Configuration job, long splitStart, long splitLength,\n                         Path file) throws IOException {\n    start \u003d splitStart;\n    end \u003d start + splitLength;\n    long partialRecordLength \u003d start % recordLength;\n    long numBytesToSkip \u003d 0;\n    if (partialRecordLength !\u003d 0) {\n      numBytesToSkip \u003d recordLength - partialRecordLength;\n    }\n\n    // open the file\n    final FutureDataInputStreamBuilder builder \u003d\n        file.getFileSystem(job).openFile(file);\n    FutureIOSupport.propagateOptions(builder, job,\n        MRJobConfig.INPUT_FILE_OPTION_PREFIX,\n        MRJobConfig.INPUT_FILE_MANDATORY_PREFIX);\n    fileIn \u003d FutureIOSupport.awaitFuture(builder.build());\n\n    CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n    if (null !\u003d codec) {\n      isCompressedInput \u003d true;\t\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      CompressionInputStream cIn\n          \u003d codec.createInputStream(fileIn, decompressor);\n      filePosition \u003d cIn;\n      inputStream \u003d cIn;\n      numRecordsRemainingInSplit \u003d Long.MAX_VALUE;\n      LOG.info(\n          \"Compressed input; cannot compute number of records in the split\");\n    } else {\n      fileIn.seek(start);\n      filePosition \u003d fileIn;\n      inputStream \u003d fileIn;\n      long splitSize \u003d end - start - numBytesToSkip;\n      numRecordsRemainingInSplit \u003d (splitSize + recordLength - 1)/recordLength;\n      if (numRecordsRemainingInSplit \u003c 0) {\n        numRecordsRemainingInSplit \u003d 0;\n      }\n      LOG.info(\"Expecting \" + numRecordsRemainingInSplit\n          + \" records each with a length of \" + recordLength\n          + \" bytes in the split with an effective size of \"\n          + splitSize + \" bytes\");\n    }\n    if (numBytesToSkip !\u003d 0) {\n      start +\u003d inputStream.skip(numBytesToSkip);\n    }\n    this.pos \u003d start;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthRecordReader.java",
      "extendedDetails": {}
    },
    "eeda370249d3f65a36718942a15867f79654ff66": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-1176. FixedLengthInputFormat and FixedLengthRecordReader (Mariappan Asokan and BitsOfInfo via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1540931 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/11/13 7:06 PM",
      "commitName": "eeda370249d3f65a36718942a15867f79654ff66",
      "commitAuthor": "Sanford Ryza",
      "diff": "@@ -0,0 +1,44 @@\n+  public void initialize(Configuration job, long splitStart, long splitLength,\n+                         Path file) throws IOException {\n+    start \u003d splitStart;\n+    end \u003d start + splitLength;\n+    long partialRecordLength \u003d start % recordLength;\n+    long numBytesToSkip \u003d 0;\n+    if (partialRecordLength !\u003d 0) {\n+      numBytesToSkip \u003d recordLength - partialRecordLength;\n+    }\n+\n+    // open the file and seek to the start of the split\n+    final FileSystem fs \u003d file.getFileSystem(job);\n+    fileIn \u003d fs.open(file);\n+\n+    CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n+    if (null !\u003d codec) {\n+      isCompressedInput \u003d true;\t\n+      decompressor \u003d CodecPool.getDecompressor(codec);\n+      CompressionInputStream cIn\n+          \u003d codec.createInputStream(fileIn, decompressor);\n+      filePosition \u003d cIn;\n+      inputStream \u003d cIn;\n+      numRecordsRemainingInSplit \u003d Long.MAX_VALUE;\n+      LOG.info(\n+          \"Compressed input; cannot compute number of records in the split\");\n+    } else {\n+      fileIn.seek(start);\n+      filePosition \u003d fileIn;\n+      inputStream \u003d fileIn;\n+      long splitSize \u003d end - start - numBytesToSkip;\n+      numRecordsRemainingInSplit \u003d (splitSize + recordLength - 1)/recordLength;\n+      if (numRecordsRemainingInSplit \u003c 0) {\n+        numRecordsRemainingInSplit \u003d 0;\n+      }\n+      LOG.info(\"Expecting \" + numRecordsRemainingInSplit\n+          + \" records each with a length of \" + recordLength\n+          + \" bytes in the split with an effective size of \"\n+          + splitSize + \" bytes\");\n+    }\n+    if (numBytesToSkip !\u003d 0) {\n+      start +\u003d inputStream.skip(numBytesToSkip);\n+    }\n+    this.pos \u003d start;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(Configuration job, long splitStart, long splitLength,\n                         Path file) throws IOException {\n    start \u003d splitStart;\n    end \u003d start + splitLength;\n    long partialRecordLength \u003d start % recordLength;\n    long numBytesToSkip \u003d 0;\n    if (partialRecordLength !\u003d 0) {\n      numBytesToSkip \u003d recordLength - partialRecordLength;\n    }\n\n    // open the file and seek to the start of the split\n    final FileSystem fs \u003d file.getFileSystem(job);\n    fileIn \u003d fs.open(file);\n\n    CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n    if (null !\u003d codec) {\n      isCompressedInput \u003d true;\t\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      CompressionInputStream cIn\n          \u003d codec.createInputStream(fileIn, decompressor);\n      filePosition \u003d cIn;\n      inputStream \u003d cIn;\n      numRecordsRemainingInSplit \u003d Long.MAX_VALUE;\n      LOG.info(\n          \"Compressed input; cannot compute number of records in the split\");\n    } else {\n      fileIn.seek(start);\n      filePosition \u003d fileIn;\n      inputStream \u003d fileIn;\n      long splitSize \u003d end - start - numBytesToSkip;\n      numRecordsRemainingInSplit \u003d (splitSize + recordLength - 1)/recordLength;\n      if (numRecordsRemainingInSplit \u003c 0) {\n        numRecordsRemainingInSplit \u003d 0;\n      }\n      LOG.info(\"Expecting \" + numRecordsRemainingInSplit\n          + \" records each with a length of \" + recordLength\n          + \" bytes in the split with an effective size of \"\n          + splitSize + \" bytes\");\n    }\n    if (numBytesToSkip !\u003d 0) {\n      start +\u003d inputStream.skip(numBytesToSkip);\n    }\n    this.pos \u003d start;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthRecordReader.java"
    }
  }
}