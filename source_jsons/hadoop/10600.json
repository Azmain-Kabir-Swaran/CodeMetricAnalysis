{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockRecoveryWorker.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java",
  "functionStartLine": 602,
  "functionEndLine": 620,
  "numCommitsSeen": 15,
  "timeTaken": 2061,
  "changeHistory": [
    "6e04b00df1bf4f0a45571c9fc4361e4e8a05f7ee",
    "61ab0440f7eaff0f631cbae0378403912f88d7ad",
    "e287e7d14b838a866ba03d895fa35819999d7c09"
  ],
  "changeHistoryShort": {
    "6e04b00df1bf4f0a45571c9fc4361e4e8a05f7ee": "Ybodychange",
    "61ab0440f7eaff0f631cbae0378403912f88d7ad": "Ybodychange",
    "e287e7d14b838a866ba03d895fa35819999d7c09": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6e04b00df1bf4f0a45571c9fc4361e4e8a05f7ee": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12288. Fix DataNode\u0027s xceiver count calculation. Contributed by Lisheng Sun.\n",
      "commitDate": "23/05/20 9:58 AM",
      "commitName": "6e04b00df1bf4f0a45571c9fc4361e4e8a05f7ee",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "15/02/20 10:45 PM",
      "commitNameOld": "810783d443cce4dd560acfc3e652a912d57d6a77",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 97.43,
      "commitsBetweenForRepo": 340,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,19 @@\n       public void run() {\n-        for(RecoveringBlock b : blocks) {\n-          try {\n-            logRecoverBlock(who, b);\n-            if (b.isStriped()) {\n-              new RecoveryTaskStriped((RecoveringStripedBlock) b).recover();\n-            } else {\n-              new RecoveryTaskContiguous(b).recover();\n+        datanode.metrics.incrDataNodeBlockRecoveryWorkerCount();\n+        try {\n+          for (RecoveringBlock b : blocks) {\n+            try {\n+              logRecoverBlock(who, b);\n+              if (b.isStriped()) {\n+                new RecoveryTaskStriped((RecoveringStripedBlock) b).recover();\n+              } else {\n+                new RecoveryTaskContiguous(b).recover();\n+              }\n+            } catch (IOException e) {\n+              LOG.warn(\"recover Block: {} FAILED: {}\", b, e);\n             }\n-          } catch (IOException e) {\n-            LOG.warn(\"recoverBlocks FAILED: \" + b, e);\n           }\n+        } finally {\n+          datanode.metrics.decrDataNodeBlockRecoveryWorkerCount();\n         }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public void run() {\n        datanode.metrics.incrDataNodeBlockRecoveryWorkerCount();\n        try {\n          for (RecoveringBlock b : blocks) {\n            try {\n              logRecoverBlock(who, b);\n              if (b.isStriped()) {\n                new RecoveryTaskStriped((RecoveringStripedBlock) b).recover();\n              } else {\n                new RecoveryTaskContiguous(b).recover();\n              }\n            } catch (IOException e) {\n              LOG.warn(\"recover Block: {} FAILED: {}\", b, e);\n            }\n          }\n        } finally {\n          datanode.metrics.decrDataNodeBlockRecoveryWorkerCount();\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java",
      "extendedDetails": {}
    },
    "61ab0440f7eaff0f631cbae0378403912f88d7ad": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9173. Erasure Coding: Lease recovery for striped file. Contributed by Walter Su and Jing Zhao.\n\nChange-Id: I51703a61c9d8454f883028f3f6acb5729fde1b15\n",
      "commitDate": "18/12/15 3:57 PM",
      "commitName": "61ab0440f7eaff0f631cbae0378403912f88d7ad",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "22/11/15 3:54 PM",
      "commitNameOld": "176ff5ce90f2cbcd8342016d0f5570337d2ff79f",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 26.0,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,14 @@\n       public void run() {\n         for(RecoveringBlock b : blocks) {\n           try {\n             logRecoverBlock(who, b);\n-            RecoveryTaskContiguous task \u003d new RecoveryTaskContiguous(b);\n-            task.recover();\n+            if (b.isStriped()) {\n+              new RecoveryTaskStriped((RecoveringStripedBlock) b).recover();\n+            } else {\n+              new RecoveryTaskContiguous(b).recover();\n+            }\n           } catch (IOException e) {\n             LOG.warn(\"recoverBlocks FAILED: \" + b, e);\n           }\n         }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public void run() {\n        for(RecoveringBlock b : blocks) {\n          try {\n            logRecoverBlock(who, b);\n            if (b.isStriped()) {\n              new RecoveryTaskStriped((RecoveringStripedBlock) b).recover();\n            } else {\n              new RecoveryTaskContiguous(b).recover();\n            }\n          } catch (IOException e) {\n            LOG.warn(\"recoverBlocks FAILED: \" + b, e);\n          }\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java",
      "extendedDetails": {}
    },
    "e287e7d14b838a866ba03d895fa35819999d7c09": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9255. Consolidate block recovery related implementation into a single class. Contributed by Walter Su.\n\nChange-Id: I7a1c03f50123d79ac0a78c981d9721617e3229d1\n",
      "commitDate": "28/10/15 7:34 AM",
      "commitName": "e287e7d14b838a866ba03d895fa35819999d7c09",
      "commitAuthor": "Zhe Zhang",
      "diff": "@@ -0,0 +1,11 @@\n+      public void run() {\n+        for(RecoveringBlock b : blocks) {\n+          try {\n+            logRecoverBlock(who, b);\n+            RecoveryTaskContiguous task \u003d new RecoveryTaskContiguous(b);\n+            task.recover();\n+          } catch (IOException e) {\n+            LOG.warn(\"recoverBlocks FAILED: \" + b, e);\n+          }\n+        }\n+      }\n\\ No newline at end of file\n",
      "actualSource": "      public void run() {\n        for(RecoveringBlock b : blocks) {\n          try {\n            logRecoverBlock(who, b);\n            RecoveryTaskContiguous task \u003d new RecoveryTaskContiguous(b);\n            task.recover();\n          } catch (IOException e) {\n            LOG.warn(\"recoverBlocks FAILED: \" + b, e);\n          }\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java"
    }
  }
}