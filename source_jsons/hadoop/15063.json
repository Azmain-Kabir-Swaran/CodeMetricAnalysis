{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ImageLoaderCurrent.java",
  "functionName": "processDirectoryWithSnapshot",
  "functionId": "processDirectoryWithSnapshot___in-DataInputStream__v-ImageVisitor__skipBlocks-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageLoaderCurrent.java",
  "functionStartLine": 502,
  "functionEndLine": 528,
  "numCommitsSeen": 35,
  "timeTaken": 2039,
  "changeHistory": [
    "97f58955a6045b373ab73653bf26ab5922b00cf3"
  ],
  "changeHistoryShort": {
    "97f58955a6045b373ab73653bf26ab5922b00cf3": "Yintroduced"
  },
  "changeHistoryDetails": {
    "97f58955a6045b373ab73653bf26ab5922b00cf3": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-6293. Issues with OIV processing PB-based fsimages. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594439 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/05/14 6:15 PM",
      "commitName": "97f58955a6045b373ab73653bf26ab5922b00cf3",
      "commitAuthor": "Kihwal Lee",
      "diff": "@@ -0,0 +1,27 @@\n+  private void processDirectoryWithSnapshot(DataInputStream in, ImageVisitor v,\n+      boolean skipBlocks) throws IOException {\n+    // 1. load dir node id\n+    long inodeId \u003d in.readLong();\n+    \n+    String dirName \u003d dirNodeMap.remove(inodeId);\n+    Boolean visitedRef \u003d subtreeMap.get(inodeId);\n+    if (visitedRef !\u003d null) {\n+      if (visitedRef.booleanValue()) { // the subtree has been visited\n+        return;\n+      } else { // first time to visit\n+        subtreeMap.put(inodeId, true);\n+      }\n+    } // else the dir is not linked by a RefNode, thus cannot be revisited\n+    \n+    // 2. load possible snapshots\n+    processSnapshots(in, v, dirName);\n+    // 3. load children nodes\n+    processChildren(in, v, skipBlocks, dirName);\n+    // 4. load possible directory diff list\n+    processDirectoryDiffList(in, v, dirName);\n+    // recursively process sub-directories\n+    final int numSubTree \u003d in.readInt();\n+    for (int i \u003d 0; i \u003c numSubTree; i++) {\n+      processDirectoryWithSnapshot(in, v, skipBlocks);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void processDirectoryWithSnapshot(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks) throws IOException {\n    // 1. load dir node id\n    long inodeId \u003d in.readLong();\n    \n    String dirName \u003d dirNodeMap.remove(inodeId);\n    Boolean visitedRef \u003d subtreeMap.get(inodeId);\n    if (visitedRef !\u003d null) {\n      if (visitedRef.booleanValue()) { // the subtree has been visited\n        return;\n      } else { // first time to visit\n        subtreeMap.put(inodeId, true);\n      }\n    } // else the dir is not linked by a RefNode, thus cannot be revisited\n    \n    // 2. load possible snapshots\n    processSnapshots(in, v, dirName);\n    // 3. load children nodes\n    processChildren(in, v, skipBlocks, dirName);\n    // 4. load possible directory diff list\n    processDirectoryDiffList(in, v, dirName);\n    // recursively process sub-directories\n    final int numSubTree \u003d in.readInt();\n    for (int i \u003d 0; i \u003c numSubTree; i++) {\n      processDirectoryWithSnapshot(in, v, skipBlocks);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageLoaderCurrent.java"
    }
  }
}