{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeStats.java",
  "functionName": "subtract",
  "functionId": "subtract___node-DatanodeDescriptor(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java",
  "functionStartLine": 78,
  "functionEndLine": 105,
  "numCommitsSeen": 31,
  "timeTaken": 4579,
  "changeHistory": [
    "5f23abfa30ea29a5474513c463b4d462c0e824ee",
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978",
    "df83230948204ee2d2b06ecc66ce0163e2df27ef",
    "19a77f546657b086af8f41fa631099bdde7e010c",
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
    "d3fed8e653ed9e18d3a29a11c4b24a628ac770bb",
    "551024915d487957d9e829493ab319c8e31dfa81",
    "13edb391d06c479720202eb5ac81f1c71fe64748",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "7fac946ac983e31613fd62836c8ac9c4a579210a"
  ],
  "changeHistoryShort": {
    "5f23abfa30ea29a5474513c463b4d462c0e824ee": "Ybodychange",
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978": "Ybodychange",
    "df83230948204ee2d2b06ecc66ce0163e2df27ef": "Ybodychange",
    "19a77f546657b086af8f41fa631099bdde7e010c": "Ybodychange",
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4": "Ymultichange(Ymovefromfile,Ymodifierchange)",
    "d3fed8e653ed9e18d3a29a11c4b24a628ac770bb": "Ybodychange",
    "551024915d487957d9e829493ab319c8e31dfa81": "Ybodychange",
    "13edb391d06c479720202eb5ac81f1c71fe64748": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "7fac946ac983e31613fd62836c8ac9c4a579210a": "Yintroduced"
  },
  "changeHistoryDetails": {
    "5f23abfa30ea29a5474513c463b4d462c0e824ee": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9038. DFS reserved space is erroneously counted towards non-DFS used. (Brahma Reddy Battula)\n",
      "commitDate": "06/09/16 1:37 PM",
      "commitName": "5f23abfa30ea29a5474513c463b4d462c0e824ee",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "30/08/16 2:00 PM",
      "commitNameOld": "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978",
      "commitAuthorOld": "Ming Ma",
      "daysBetweenCommits": 6.98,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,28 @@\n   synchronized void subtract(final DatanodeDescriptor node) {\n     xceiverCount -\u003d node.getXceiverCount();\n     if (node.isInService()) {\n       capacityUsed -\u003d node.getDfsUsed();\n+      capacityUsedNonDfs -\u003d node.getNonDfsUsed();\n       blockPoolUsed -\u003d node.getBlockPoolUsed();\n       nodesInService--;\n       nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n       capacityTotal -\u003d node.getCapacity();\n       capacityRemaining -\u003d node.getRemaining();\n       cacheCapacity -\u003d node.getCacheCapacity();\n       cacheUsed -\u003d node.getCacheUsed();\n     } else if (node.isDecommissionInProgress() ||\n         node.isEnteringMaintenance()) {\n       cacheCapacity -\u003d node.getCacheCapacity();\n       cacheUsed -\u003d node.getCacheUsed();\n     }\n     Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n     for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n       if (storageInfo.getState() !\u003d DatanodeStorage.State.FAILED) {\n         statsMap.subtractStorage(storageInfo, node);\n         storageTypes.add(storageInfo.getStorageType());\n       }\n     }\n     for (StorageType storageType : storageTypes) {\n       statsMap.subtractNode(storageType, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void subtract(final DatanodeDescriptor node) {\n    xceiverCount -\u003d node.getXceiverCount();\n    if (node.isInService()) {\n      capacityUsed -\u003d node.getDfsUsed();\n      capacityUsedNonDfs -\u003d node.getNonDfsUsed();\n      blockPoolUsed -\u003d node.getBlockPoolUsed();\n      nodesInService--;\n      nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n      capacityTotal -\u003d node.getCapacity();\n      capacityRemaining -\u003d node.getRemaining();\n      cacheCapacity -\u003d node.getCacheCapacity();\n      cacheUsed -\u003d node.getCacheUsed();\n    } else if (node.isDecommissionInProgress() ||\n        node.isEnteringMaintenance()) {\n      cacheCapacity -\u003d node.getCacheCapacity();\n      cacheUsed -\u003d node.getCacheUsed();\n    }\n    Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n    for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n      if (storageInfo.getState() !\u003d DatanodeStorage.State.FAILED) {\n        statsMap.subtractStorage(storageInfo, node);\n        storageTypes.add(storageInfo.getStorageType());\n      }\n    }\n    for (StorageType storageType : storageTypes) {\n      statsMap.subtractNode(storageType, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java",
      "extendedDetails": {}
    },
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9392. Admins support for maintenance state. Contributed by Ming Ma.\n",
      "commitDate": "30/08/16 2:00 PM",
      "commitName": "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "22/12/15 3:28 PM",
      "commitNameOld": "df83230948204ee2d2b06ecc66ce0163e2df27ef",
      "commitAuthorOld": "Benoy Antony",
      "daysBetweenCommits": 251.9,
      "commitsBetweenForRepo": 1751,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,27 @@\n   synchronized void subtract(final DatanodeDescriptor node) {\n     xceiverCount -\u003d node.getXceiverCount();\n-    if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n+    if (node.isInService()) {\n       capacityUsed -\u003d node.getDfsUsed();\n       blockPoolUsed -\u003d node.getBlockPoolUsed();\n       nodesInService--;\n       nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n       capacityTotal -\u003d node.getCapacity();\n       capacityRemaining -\u003d node.getRemaining();\n       cacheCapacity -\u003d node.getCacheCapacity();\n       cacheUsed -\u003d node.getCacheUsed();\n-    } else if (!node.isDecommissioned()) {\n+    } else if (node.isDecommissionInProgress() ||\n+        node.isEnteringMaintenance()) {\n       cacheCapacity -\u003d node.getCacheCapacity();\n       cacheUsed -\u003d node.getCacheUsed();\n     }\n     Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n     for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n       if (storageInfo.getState() !\u003d DatanodeStorage.State.FAILED) {\n         statsMap.subtractStorage(storageInfo, node);\n         storageTypes.add(storageInfo.getStorageType());\n       }\n     }\n     for (StorageType storageType : storageTypes) {\n       statsMap.subtractNode(storageType, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void subtract(final DatanodeDescriptor node) {\n    xceiverCount -\u003d node.getXceiverCount();\n    if (node.isInService()) {\n      capacityUsed -\u003d node.getDfsUsed();\n      blockPoolUsed -\u003d node.getBlockPoolUsed();\n      nodesInService--;\n      nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n      capacityTotal -\u003d node.getCapacity();\n      capacityRemaining -\u003d node.getRemaining();\n      cacheCapacity -\u003d node.getCacheCapacity();\n      cacheUsed -\u003d node.getCacheUsed();\n    } else if (node.isDecommissionInProgress() ||\n        node.isEnteringMaintenance()) {\n      cacheCapacity -\u003d node.getCacheCapacity();\n      cacheUsed -\u003d node.getCacheUsed();\n    }\n    Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n    for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n      if (storageInfo.getState() !\u003d DatanodeStorage.State.FAILED) {\n        statsMap.subtractStorage(storageInfo, node);\n        storageTypes.add(storageInfo.getStorageType());\n      }\n    }\n    for (StorageType storageType : storageTypes) {\n      statsMap.subtractNode(storageType, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java",
      "extendedDetails": {}
    },
    "df83230948204ee2d2b06ecc66ce0163e2df27ef": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9034. StorageTypeStats Metric should not count failed storage. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "22/12/15 3:28 PM",
      "commitName": "df83230948204ee2d2b06ecc66ce0163e2df27ef",
      "commitAuthor": "Benoy Antony",
      "commitDateOld": "28/10/15 9:58 AM",
      "commitNameOld": "19a77f546657b086af8f41fa631099bdde7e010c",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 55.27,
      "commitsBetweenForRepo": 372,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,26 @@\n   synchronized void subtract(final DatanodeDescriptor node) {\n     xceiverCount -\u003d node.getXceiverCount();\n     if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n       capacityUsed -\u003d node.getDfsUsed();\n       blockPoolUsed -\u003d node.getBlockPoolUsed();\n       nodesInService--;\n       nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n       capacityTotal -\u003d node.getCapacity();\n       capacityRemaining -\u003d node.getRemaining();\n       cacheCapacity -\u003d node.getCacheCapacity();\n       cacheUsed -\u003d node.getCacheUsed();\n     } else if (!node.isDecommissioned()) {\n       cacheCapacity -\u003d node.getCacheCapacity();\n       cacheUsed -\u003d node.getCacheUsed();\n     }\n     Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n     for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n-      statsMap.subtractStorage(storageInfo, node);\n-      storageTypes.add(storageInfo.getStorageType());\n+      if (storageInfo.getState() !\u003d DatanodeStorage.State.FAILED) {\n+        statsMap.subtractStorage(storageInfo, node);\n+        storageTypes.add(storageInfo.getStorageType());\n+      }\n     }\n     for (StorageType storageType : storageTypes) {\n       statsMap.subtractNode(storageType, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void subtract(final DatanodeDescriptor node) {\n    xceiverCount -\u003d node.getXceiverCount();\n    if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n      capacityUsed -\u003d node.getDfsUsed();\n      blockPoolUsed -\u003d node.getBlockPoolUsed();\n      nodesInService--;\n      nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n      capacityTotal -\u003d node.getCapacity();\n      capacityRemaining -\u003d node.getRemaining();\n      cacheCapacity -\u003d node.getCacheCapacity();\n      cacheUsed -\u003d node.getCacheUsed();\n    } else if (!node.isDecommissioned()) {\n      cacheCapacity -\u003d node.getCacheCapacity();\n      cacheUsed -\u003d node.getCacheUsed();\n    }\n    Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n    for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n      if (storageInfo.getState() !\u003d DatanodeStorage.State.FAILED) {\n        statsMap.subtractStorage(storageInfo, node);\n        storageTypes.add(storageInfo.getStorageType());\n      }\n    }\n    for (StorageType storageType : storageTypes) {\n      statsMap.subtractNode(storageType, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java",
      "extendedDetails": {}
    },
    "19a77f546657b086af8f41fa631099bdde7e010c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9279. Decomissioned capacity should not be considered for configured/used capacity. Contributed by Kihu Shukla .\n",
      "commitDate": "28/10/15 9:58 AM",
      "commitName": "19a77f546657b086af8f41fa631099bdde7e010c",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "15/10/15 10:32 AM",
      "commitNameOld": "7a98d94b7b6b730b8ca3ca606fe397adc0d0fc0b",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 12.98,
      "commitsBetweenForRepo": 122,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   synchronized void subtract(final DatanodeDescriptor node) {\n-    capacityUsed -\u003d node.getDfsUsed();\n-    blockPoolUsed -\u003d node.getBlockPoolUsed();\n     xceiverCount -\u003d node.getXceiverCount();\n     if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n+      capacityUsed -\u003d node.getDfsUsed();\n+      blockPoolUsed -\u003d node.getBlockPoolUsed();\n       nodesInService--;\n       nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n       capacityTotal -\u003d node.getCapacity();\n       capacityRemaining -\u003d node.getRemaining();\n-    } else {\n-      capacityTotal -\u003d node.getDfsUsed();\n+      cacheCapacity -\u003d node.getCacheCapacity();\n+      cacheUsed -\u003d node.getCacheUsed();\n+    } else if (!node.isDecommissioned()) {\n+      cacheCapacity -\u003d node.getCacheCapacity();\n+      cacheUsed -\u003d node.getCacheUsed();\n     }\n-    cacheCapacity -\u003d node.getCacheCapacity();\n-    cacheUsed -\u003d node.getCacheUsed();\n     Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n     for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n       statsMap.subtractStorage(storageInfo, node);\n       storageTypes.add(storageInfo.getStorageType());\n     }\n     for (StorageType storageType : storageTypes) {\n       statsMap.subtractNode(storageType, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void subtract(final DatanodeDescriptor node) {\n    xceiverCount -\u003d node.getXceiverCount();\n    if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n      capacityUsed -\u003d node.getDfsUsed();\n      blockPoolUsed -\u003d node.getBlockPoolUsed();\n      nodesInService--;\n      nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n      capacityTotal -\u003d node.getCapacity();\n      capacityRemaining -\u003d node.getRemaining();\n      cacheCapacity -\u003d node.getCacheCapacity();\n      cacheUsed -\u003d node.getCacheUsed();\n    } else if (!node.isDecommissioned()) {\n      cacheCapacity -\u003d node.getCacheCapacity();\n      cacheUsed -\u003d node.getCacheUsed();\n    }\n    Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n    for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n      statsMap.subtractStorage(storageInfo, node);\n      storageTypes.add(storageInfo.getStorageType());\n    }\n    for (StorageType storageType : storageTypes) {\n      statsMap.subtractNode(storageType, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java",
      "extendedDetails": {}
    },
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange)",
      "commitMessage": "HDFS-9223. Code cleanup for DatanodeDescriptor and HeartbeatManager. Contributed by Jing Zhao.\n",
      "commitDate": "14/10/15 4:17 PM",
      "commitName": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-9223. Code cleanup for DatanodeDescriptor and HeartbeatManager. Contributed by Jing Zhao.\n",
          "commitDate": "14/10/15 4:17 PM",
          "commitName": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "14/10/15 2:41 PM",
          "commitNameOld": "a8070259f8384021bd6196e7343f1cc23de89b1c",
          "commitAuthorOld": "Xiaoyu Yao",
          "daysBetweenCommits": 0.07,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-    private void subtract(final DatanodeDescriptor node) {\n-      capacityUsed -\u003d node.getDfsUsed();\n-      blockPoolUsed -\u003d node.getBlockPoolUsed();\n-      xceiverCount -\u003d node.getXceiverCount();\n-      if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n-        nodesInService--;\n-        nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n-        capacityTotal -\u003d node.getCapacity();\n-        capacityRemaining -\u003d node.getRemaining();\n-      } else {\n-        capacityTotal -\u003d node.getDfsUsed();\n-      }\n-      cacheCapacity -\u003d node.getCacheCapacity();\n-      cacheUsed -\u003d node.getCacheUsed();\n-      Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n-      for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n-        statsMap.subtractStorage(storageInfo, node);\n-        storageTypes.add(storageInfo.getStorageType());\n-      }\n-      for (StorageType storageType : storageTypes) {\n-        statsMap.subtractNode(storageType, node);\n-      }\n-    }\n\\ No newline at end of file\n+  synchronized void subtract(final DatanodeDescriptor node) {\n+    capacityUsed -\u003d node.getDfsUsed();\n+    blockPoolUsed -\u003d node.getBlockPoolUsed();\n+    xceiverCount -\u003d node.getXceiverCount();\n+    if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n+      nodesInService--;\n+      nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n+      capacityTotal -\u003d node.getCapacity();\n+      capacityRemaining -\u003d node.getRemaining();\n+    } else {\n+      capacityTotal -\u003d node.getDfsUsed();\n+    }\n+    cacheCapacity -\u003d node.getCacheCapacity();\n+    cacheUsed -\u003d node.getCacheUsed();\n+    Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n+    for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n+      statsMap.subtractStorage(storageInfo, node);\n+      storageTypes.add(storageInfo.getStorageType());\n+    }\n+    for (StorageType storageType : storageTypes) {\n+      statsMap.subtractNode(storageType, node);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized void subtract(final DatanodeDescriptor node) {\n    capacityUsed -\u003d node.getDfsUsed();\n    blockPoolUsed -\u003d node.getBlockPoolUsed();\n    xceiverCount -\u003d node.getXceiverCount();\n    if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n      nodesInService--;\n      nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n      capacityTotal -\u003d node.getCapacity();\n      capacityRemaining -\u003d node.getRemaining();\n    } else {\n      capacityTotal -\u003d node.getDfsUsed();\n    }\n    cacheCapacity -\u003d node.getCacheCapacity();\n    cacheUsed -\u003d node.getCacheUsed();\n    Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n    for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n      statsMap.subtractStorage(storageInfo, node);\n      storageTypes.add(storageInfo.getStorageType());\n    }\n    for (StorageType storageType : storageTypes) {\n      statsMap.subtractNode(storageType, node);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java",
            "oldMethodName": "subtract",
            "newMethodName": "subtract"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-9223. Code cleanup for DatanodeDescriptor and HeartbeatManager. Contributed by Jing Zhao.\n",
          "commitDate": "14/10/15 4:17 PM",
          "commitName": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "14/10/15 2:41 PM",
          "commitNameOld": "a8070259f8384021bd6196e7343f1cc23de89b1c",
          "commitAuthorOld": "Xiaoyu Yao",
          "daysBetweenCommits": 0.07,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-    private void subtract(final DatanodeDescriptor node) {\n-      capacityUsed -\u003d node.getDfsUsed();\n-      blockPoolUsed -\u003d node.getBlockPoolUsed();\n-      xceiverCount -\u003d node.getXceiverCount();\n-      if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n-        nodesInService--;\n-        nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n-        capacityTotal -\u003d node.getCapacity();\n-        capacityRemaining -\u003d node.getRemaining();\n-      } else {\n-        capacityTotal -\u003d node.getDfsUsed();\n-      }\n-      cacheCapacity -\u003d node.getCacheCapacity();\n-      cacheUsed -\u003d node.getCacheUsed();\n-      Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n-      for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n-        statsMap.subtractStorage(storageInfo, node);\n-        storageTypes.add(storageInfo.getStorageType());\n-      }\n-      for (StorageType storageType : storageTypes) {\n-        statsMap.subtractNode(storageType, node);\n-      }\n-    }\n\\ No newline at end of file\n+  synchronized void subtract(final DatanodeDescriptor node) {\n+    capacityUsed -\u003d node.getDfsUsed();\n+    blockPoolUsed -\u003d node.getBlockPoolUsed();\n+    xceiverCount -\u003d node.getXceiverCount();\n+    if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n+      nodesInService--;\n+      nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n+      capacityTotal -\u003d node.getCapacity();\n+      capacityRemaining -\u003d node.getRemaining();\n+    } else {\n+      capacityTotal -\u003d node.getDfsUsed();\n+    }\n+    cacheCapacity -\u003d node.getCacheCapacity();\n+    cacheUsed -\u003d node.getCacheUsed();\n+    Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n+    for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n+      statsMap.subtractStorage(storageInfo, node);\n+      storageTypes.add(storageInfo.getStorageType());\n+    }\n+    for (StorageType storageType : storageTypes) {\n+      statsMap.subtractNode(storageType, node);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized void subtract(final DatanodeDescriptor node) {\n    capacityUsed -\u003d node.getDfsUsed();\n    blockPoolUsed -\u003d node.getBlockPoolUsed();\n    xceiverCount -\u003d node.getXceiverCount();\n    if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n      nodesInService--;\n      nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n      capacityTotal -\u003d node.getCapacity();\n      capacityRemaining -\u003d node.getRemaining();\n    } else {\n      capacityTotal -\u003d node.getDfsUsed();\n    }\n    cacheCapacity -\u003d node.getCacheCapacity();\n    cacheUsed -\u003d node.getCacheUsed();\n    Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n    for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n      statsMap.subtractStorage(storageInfo, node);\n      storageTypes.add(storageInfo.getStorageType());\n    }\n    for (StorageType storageType : storageTypes) {\n      statsMap.subtractNode(storageType, node);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[synchronized]"
          }
        }
      ]
    },
    "d3fed8e653ed9e18d3a29a11c4b24a628ac770bb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7390. Provide JMX metrics per storage type. (Benoy Antony)\n",
      "commitDate": "29/06/15 11:00 AM",
      "commitName": "d3fed8e653ed9e18d3a29a11c4b24a628ac770bb",
      "commitAuthor": "Benoy Antony",
      "commitDateOld": "20/04/15 12:36 AM",
      "commitNameOld": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 70.43,
      "commitsBetweenForRepo": 626,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,23 @@\n     private void subtract(final DatanodeDescriptor node) {\n       capacityUsed -\u003d node.getDfsUsed();\n       blockPoolUsed -\u003d node.getBlockPoolUsed();\n       xceiverCount -\u003d node.getXceiverCount();\n       if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n         nodesInService--;\n         nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n         capacityTotal -\u003d node.getCapacity();\n         capacityRemaining -\u003d node.getRemaining();\n       } else {\n         capacityTotal -\u003d node.getDfsUsed();\n       }\n       cacheCapacity -\u003d node.getCacheCapacity();\n       cacheUsed -\u003d node.getCacheUsed();\n+      Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n+      for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n+        statsMap.subtractStorage(storageInfo, node);\n+        storageTypes.add(storageInfo.getStorageType());\n+      }\n+      for (StorageType storageType : storageTypes) {\n+        statsMap.subtractNode(storageType, node);\n+      }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void subtract(final DatanodeDescriptor node) {\n      capacityUsed -\u003d node.getDfsUsed();\n      blockPoolUsed -\u003d node.getBlockPoolUsed();\n      xceiverCount -\u003d node.getXceiverCount();\n      if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n        nodesInService--;\n        nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n        capacityTotal -\u003d node.getCapacity();\n        capacityRemaining -\u003d node.getRemaining();\n      } else {\n        capacityTotal -\u003d node.getDfsUsed();\n      }\n      cacheCapacity -\u003d node.getCacheCapacity();\n      cacheUsed -\u003d node.getCacheUsed();\n      Set\u003cStorageType\u003e storageTypes \u003d new HashSet\u003c\u003e();\n      for (DatanodeStorageInfo storageInfo : node.getStorageInfos()) {\n        statsMap.subtractStorage(storageInfo, node);\n        storageTypes.add(storageInfo.getStorageType());\n      }\n      for (StorageType storageType : storageTypes) {\n        statsMap.subtractNode(storageType, node);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
      "extendedDetails": {}
    },
    "551024915d487957d9e829493ab319c8e31dfa81": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6599. 2.4 addBlock is 10 to 20 times slower compared to 0.23 (daryn)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1611737 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/07/14 10:58 AM",
      "commitName": "551024915d487957d9e829493ab319c8e31dfa81",
      "commitAuthor": "Daryn Sharp",
      "commitDateOld": "02/12/13 9:41 AM",
      "commitNameOld": "18159be495f96bde4bd4fa2cacb14aafb87e87bc",
      "commitAuthorOld": "",
      "daysBetweenCommits": 228.01,
      "commitsBetweenForRepo": 1548,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,15 @@\n     private void subtract(final DatanodeDescriptor node) {\n       capacityUsed -\u003d node.getDfsUsed();\n       blockPoolUsed -\u003d node.getBlockPoolUsed();\n       xceiverCount -\u003d node.getXceiverCount();\n       if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n+        nodesInService--;\n+        nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n         capacityTotal -\u003d node.getCapacity();\n         capacityRemaining -\u003d node.getRemaining();\n       } else {\n         capacityTotal -\u003d node.getDfsUsed();\n       }\n       cacheCapacity -\u003d node.getCacheCapacity();\n       cacheUsed -\u003d node.getCacheUsed();\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void subtract(final DatanodeDescriptor node) {\n      capacityUsed -\u003d node.getDfsUsed();\n      blockPoolUsed -\u003d node.getBlockPoolUsed();\n      xceiverCount -\u003d node.getXceiverCount();\n      if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n        nodesInService--;\n        nodesInServiceXceiverCount -\u003d node.getXceiverCount();\n        capacityTotal -\u003d node.getCapacity();\n        capacityRemaining -\u003d node.getRemaining();\n      } else {\n        capacityTotal -\u003d node.getDfsUsed();\n      }\n      cacheCapacity -\u003d node.getCacheCapacity();\n      cacheUsed -\u003d node.getCacheUsed();\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
      "extendedDetails": {}
    },
    "13edb391d06c479720202eb5ac81f1c71fe64748": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 9:55 AM",
      "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "30/08/13 3:15 PM",
      "commitNameOld": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 88.82,
      "commitsBetweenForRepo": 547,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,13 @@\n     private void subtract(final DatanodeDescriptor node) {\n       capacityUsed -\u003d node.getDfsUsed();\n       blockPoolUsed -\u003d node.getBlockPoolUsed();\n       xceiverCount -\u003d node.getXceiverCount();\n       if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n         capacityTotal -\u003d node.getCapacity();\n         capacityRemaining -\u003d node.getRemaining();\n       } else {\n         capacityTotal -\u003d node.getDfsUsed();\n       }\n+      cacheCapacity -\u003d node.getCacheCapacity();\n+      cacheUsed -\u003d node.getCacheUsed();\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void subtract(final DatanodeDescriptor node) {\n      capacityUsed -\u003d node.getDfsUsed();\n      blockPoolUsed -\u003d node.getBlockPoolUsed();\n      xceiverCount -\u003d node.getXceiverCount();\n      if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n        capacityTotal -\u003d node.getCapacity();\n        capacityRemaining -\u003d node.getRemaining();\n      } else {\n        capacityTotal -\u003d node.getDfsUsed();\n      }\n      cacheCapacity -\u003d node.getCacheCapacity();\n      cacheUsed -\u003d node.getCacheUsed();\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void subtract(final DatanodeDescriptor node) {\n      capacityUsed -\u003d node.getDfsUsed();\n      blockPoolUsed -\u003d node.getBlockPoolUsed();\n      xceiverCount -\u003d node.getXceiverCount();\n      if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n        capacityTotal -\u003d node.getCapacity();\n        capacityRemaining -\u003d node.getRemaining();\n      } else {\n        capacityTotal -\u003d node.getDfsUsed();\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void subtract(final DatanodeDescriptor node) {\n      capacityUsed -\u003d node.getDfsUsed();\n      blockPoolUsed -\u003d node.getBlockPoolUsed();\n      xceiverCount -\u003d node.getXceiverCount();\n      if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n        capacityTotal -\u003d node.getCapacity();\n        capacityRemaining -\u003d node.getRemaining();\n      } else {\n        capacityTotal -\u003d node.getDfsUsed();\n      }\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java"
      }
    },
    "7fac946ac983e31613fd62836c8ac9c4a579210a": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2108. Move datanode heartbeat handling from namenode package to blockmanagement package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154042 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/08/11 3:55 PM",
      "commitName": "7fac946ac983e31613fd62836c8ac9c4a579210a",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,11 @@\n+    private void subtract(final DatanodeDescriptor node) {\n+      capacityUsed -\u003d node.getDfsUsed();\n+      blockPoolUsed -\u003d node.getBlockPoolUsed();\n+      xceiverCount -\u003d node.getXceiverCount();\n+      if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n+        capacityTotal -\u003d node.getCapacity();\n+        capacityRemaining -\u003d node.getRemaining();\n+      } else {\n+        capacityTotal -\u003d node.getDfsUsed();\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void subtract(final DatanodeDescriptor node) {\n      capacityUsed -\u003d node.getDfsUsed();\n      blockPoolUsed -\u003d node.getBlockPoolUsed();\n      xceiverCount -\u003d node.getXceiverCount();\n      if (!(node.isDecommissionInProgress() || node.isDecommissioned())) {\n        capacityTotal -\u003d node.getCapacity();\n        capacityRemaining -\u003d node.getRemaining();\n      } else {\n        capacityTotal -\u003d node.getDfsUsed();\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java"
    }
  }
}