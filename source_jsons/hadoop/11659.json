{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetCache.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
  "functionStartLine": 409,
  "functionEndLine": 492,
  "numCommitsSeen": 31,
  "timeTaken": 4192,
  "changeHistory": [
    "9b0aace1e6c54f201784912c0b623707aa82b761",
    "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a",
    "f3f51284d57ef2e0c7e968b6eea56eab578f7e93",
    "e453989a5722e653bd97e3e54f9bbdffc9454fba",
    "cad14aa9168112ef1ceae80b94d9aae3ba293578",
    "93e23a99157c30b51752fc49748c3c210745a187",
    "d265dd9eb01bb4ed5335872f5976740258d6bfc0",
    "f0d64a078da7e932b9509734f75170e3e525e68c",
    "13edb391d06c479720202eb5ac81f1c71fe64748",
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce",
    "40eb94ade3161d93e7a762a839004748f6d0ae89",
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11"
  ],
  "changeHistoryShort": {
    "9b0aace1e6c54f201784912c0b623707aa82b761": "Ybodychange",
    "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a": "Ybodychange",
    "f3f51284d57ef2e0c7e968b6eea56eab578f7e93": "Ybodychange",
    "e453989a5722e653bd97e3e54f9bbdffc9454fba": "Ybodychange",
    "cad14aa9168112ef1ceae80b94d9aae3ba293578": "Ybodychange",
    "93e23a99157c30b51752fc49748c3c210745a187": "Ybodychange",
    "d265dd9eb01bb4ed5335872f5976740258d6bfc0": "Ybodychange",
    "f0d64a078da7e932b9509734f75170e3e525e68c": "Ybodychange",
    "13edb391d06c479720202eb5ac81f1c71fe64748": "Ybodychange",
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc": "Ybodychange",
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce": "Ybodychange",
    "40eb94ade3161d93e7a762a839004748f6d0ae89": "Ybodychange",
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11": "Yintroduced"
  },
  "changeHistoryDetails": {
    "9b0aace1e6c54f201784912c0b623707aa82b761": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14401. Refine the implementation for HDFS cache on SCM. Contributed by Feilong He.\n",
      "commitDate": "08/05/19 4:50 AM",
      "commitName": "9b0aace1e6c54f201784912c0b623707aa82b761",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "30/03/19 11:33 PM",
      "commitNameOld": "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 38.22,
      "commitsBetweenForRepo": 236,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,31 @@\n     public void run() {\n       Value value;\n \n       if (shouldDefer()) {\n         deferredUncachingExecutor.schedule(\n             this, revocationPollingMs, TimeUnit.MILLISECONDS);\n         return;\n       }\n \n       synchronized (FsDatasetCache.this) {\n         value \u003d mappableBlockMap.get(key);\n       }\n       Preconditions.checkNotNull(value);\n       Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n \n       IOUtils.closeQuietly(value.mappableBlock);\n       synchronized (FsDatasetCache.this) {\n         mappableBlockMap.remove(key);\n       }\n-      long newUsedBytes \u003d cacheLoader.release(value.mappableBlock.getLength());\n+      long newUsedBytes \u003d cacheLoader.\n+          release(key, value.mappableBlock.getLength());\n       numBlocksCached.addAndGet(-1);\n       dataset.datanode.getMetrics().incrBlocksUncached(1);\n       if (revocationTimeMs !\u003d 0) {\n         LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\",\n             key, newUsedBytes);\n       } else {\n         LOG.debug(\"Deferred uncaching of {} completed. usedBytes \u003d {}\",\n             key, newUsedBytes);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      Value value;\n\n      if (shouldDefer()) {\n        deferredUncachingExecutor.schedule(\n            this, revocationPollingMs, TimeUnit.MILLISECONDS);\n        return;\n      }\n\n      synchronized (FsDatasetCache.this) {\n        value \u003d mappableBlockMap.get(key);\n      }\n      Preconditions.checkNotNull(value);\n      Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n\n      IOUtils.closeQuietly(value.mappableBlock);\n      synchronized (FsDatasetCache.this) {\n        mappableBlockMap.remove(key);\n      }\n      long newUsedBytes \u003d cacheLoader.\n          release(key, value.mappableBlock.getLength());\n      numBlocksCached.addAndGet(-1);\n      dataset.datanode.getMetrics().incrBlocksUncached(1);\n      if (revocationTimeMs !\u003d 0) {\n        LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\",\n            key, newUsedBytes);\n      } else {\n        LOG.debug(\"Deferred uncaching of {} completed. usedBytes \u003d {}\",\n            key, newUsedBytes);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14355 : Implement HDFS cache on SCM by using pure java mapped byte buffer. Contributed by Feilong He.\n",
      "commitDate": "30/03/19 11:33 PM",
      "commitName": "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "28/03/19 11:48 AM",
      "commitNameOld": "f3f51284d57ef2e0c7e968b6eea56eab578f7e93",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 2.49,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,30 @@\n     public void run() {\n       Value value;\n \n       if (shouldDefer()) {\n         deferredUncachingExecutor.schedule(\n             this, revocationPollingMs, TimeUnit.MILLISECONDS);\n         return;\n       }\n \n       synchronized (FsDatasetCache.this) {\n         value \u003d mappableBlockMap.get(key);\n       }\n       Preconditions.checkNotNull(value);\n       Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n \n       IOUtils.closeQuietly(value.mappableBlock);\n       synchronized (FsDatasetCache.this) {\n         mappableBlockMap.remove(key);\n       }\n-      long newUsedBytes \u003d mappableBlockLoader\n-          .release(value.mappableBlock.getLength());\n+      long newUsedBytes \u003d cacheLoader.release(value.mappableBlock.getLength());\n       numBlocksCached.addAndGet(-1);\n       dataset.datanode.getMetrics().incrBlocksUncached(1);\n       if (revocationTimeMs !\u003d 0) {\n         LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\",\n             key, newUsedBytes);\n       } else {\n         LOG.debug(\"Deferred uncaching of {} completed. usedBytes \u003d {}\",\n             key, newUsedBytes);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      Value value;\n\n      if (shouldDefer()) {\n        deferredUncachingExecutor.schedule(\n            this, revocationPollingMs, TimeUnit.MILLISECONDS);\n        return;\n      }\n\n      synchronized (FsDatasetCache.this) {\n        value \u003d mappableBlockMap.get(key);\n      }\n      Preconditions.checkNotNull(value);\n      Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n\n      IOUtils.closeQuietly(value.mappableBlock);\n      synchronized (FsDatasetCache.this) {\n        mappableBlockMap.remove(key);\n      }\n      long newUsedBytes \u003d cacheLoader.release(value.mappableBlock.getLength());\n      numBlocksCached.addAndGet(-1);\n      dataset.datanode.getMetrics().incrBlocksUncached(1);\n      if (revocationTimeMs !\u003d 0) {\n        LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\",\n            key, newUsedBytes);\n      } else {\n        LOG.debug(\"Deferred uncaching of {} completed. usedBytes \u003d {}\",\n            key, newUsedBytes);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "f3f51284d57ef2e0c7e968b6eea56eab578f7e93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14393. Refactor FsDatasetCache for SCM cache implementation. Contributed by Rakesh R\n",
      "commitDate": "28/03/19 11:48 AM",
      "commitName": "f3f51284d57ef2e0c7e968b6eea56eab578f7e93",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "14/03/19 10:21 PM",
      "commitNameOld": "ba50a36a3ead628c3d44d384f7ed4d2b3a55dd07",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 13.56,
      "commitsBetweenForRepo": 92,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,31 @@\n     public void run() {\n       Value value;\n \n       if (shouldDefer()) {\n         deferredUncachingExecutor.schedule(\n             this, revocationPollingMs, TimeUnit.MILLISECONDS);\n         return;\n       }\n \n       synchronized (FsDatasetCache.this) {\n         value \u003d mappableBlockMap.get(key);\n       }\n       Preconditions.checkNotNull(value);\n       Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n \n       IOUtils.closeQuietly(value.mappableBlock);\n       synchronized (FsDatasetCache.this) {\n         mappableBlockMap.remove(key);\n       }\n-      long newUsedBytes \u003d release(value.mappableBlock.getLength());\n+      long newUsedBytes \u003d mappableBlockLoader\n+          .release(value.mappableBlock.getLength());\n       numBlocksCached.addAndGet(-1);\n       dataset.datanode.getMetrics().incrBlocksUncached(1);\n       if (revocationTimeMs !\u003d 0) {\n         LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\",\n             key, newUsedBytes);\n       } else {\n         LOG.debug(\"Deferred uncaching of {} completed. usedBytes \u003d {}\",\n             key, newUsedBytes);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      Value value;\n\n      if (shouldDefer()) {\n        deferredUncachingExecutor.schedule(\n            this, revocationPollingMs, TimeUnit.MILLISECONDS);\n        return;\n      }\n\n      synchronized (FsDatasetCache.this) {\n        value \u003d mappableBlockMap.get(key);\n      }\n      Preconditions.checkNotNull(value);\n      Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n\n      IOUtils.closeQuietly(value.mappableBlock);\n      synchronized (FsDatasetCache.this) {\n        mappableBlockMap.remove(key);\n      }\n      long newUsedBytes \u003d mappableBlockLoader\n          .release(value.mappableBlock.getLength());\n      numBlocksCached.addAndGet(-1);\n      dataset.datanode.getMetrics().incrBlocksUncached(1);\n      if (revocationTimeMs !\u003d 0) {\n        LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\",\n            key, newUsedBytes);\n      } else {\n        LOG.debug(\"Deferred uncaching of {} completed. usedBytes \u003d {}\",\n            key, newUsedBytes);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "e453989a5722e653bd97e3e54f9bbdffc9454fba": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8157. Writes to RAM DISK reserve locked memory for block files. (Arpit Agarwal)\n",
      "commitDate": "16/05/15 9:05 AM",
      "commitName": "e453989a5722e653bd97e3e54f9bbdffc9454fba",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "01/04/15 12:54 PM",
      "commitNameOld": "ed72daa5df97669906234e8ac9a406d78136b206",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 44.84,
      "commitsBetweenForRepo": 466,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,30 @@\n     public void run() {\n       Value value;\n \n       if (shouldDefer()) {\n         deferredUncachingExecutor.schedule(\n             this, revocationPollingMs, TimeUnit.MILLISECONDS);\n         return;\n       }\n \n       synchronized (FsDatasetCache.this) {\n         value \u003d mappableBlockMap.get(key);\n       }\n       Preconditions.checkNotNull(value);\n       Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n \n       IOUtils.closeQuietly(value.mappableBlock);\n       synchronized (FsDatasetCache.this) {\n         mappableBlockMap.remove(key);\n       }\n-      long newUsedBytes \u003d\n-          usedBytesCount.release(value.mappableBlock.getLength());\n+      long newUsedBytes \u003d release(value.mappableBlock.getLength());\n       numBlocksCached.addAndGet(-1);\n       dataset.datanode.getMetrics().incrBlocksUncached(1);\n       if (revocationTimeMs !\u003d 0) {\n         LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\",\n             key, newUsedBytes);\n       } else {\n         LOG.debug(\"Deferred uncaching of {} completed. usedBytes \u003d {}\",\n             key, newUsedBytes);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      Value value;\n\n      if (shouldDefer()) {\n        deferredUncachingExecutor.schedule(\n            this, revocationPollingMs, TimeUnit.MILLISECONDS);\n        return;\n      }\n\n      synchronized (FsDatasetCache.this) {\n        value \u003d mappableBlockMap.get(key);\n      }\n      Preconditions.checkNotNull(value);\n      Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n\n      IOUtils.closeQuietly(value.mappableBlock);\n      synchronized (FsDatasetCache.this) {\n        mappableBlockMap.remove(key);\n      }\n      long newUsedBytes \u003d release(value.mappableBlock.getLength());\n      numBlocksCached.addAndGet(-1);\n      dataset.datanode.getMetrics().incrBlocksUncached(1);\n      if (revocationTimeMs !\u003d 0) {\n        LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\",\n            key, newUsedBytes);\n      } else {\n        LOG.debug(\"Deferred uncaching of {} completed. usedBytes \u003d {}\",\n            key, newUsedBytes);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "cad14aa9168112ef1ceae80b94d9aae3ba293578": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6036. Forcibly timeout misbehaving DFSClients that try to do no-checksum reads that extend too long.  (cmccabe)\n",
      "commitDate": "08/09/14 12:51 PM",
      "commitName": "cad14aa9168112ef1ceae80b94d9aae3ba293578",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "03/07/14 10:13 AM",
      "commitNameOld": "93e23a99157c30b51752fc49748c3c210745a187",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 67.11,
      "commitsBetweenForRepo": 528,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,31 @@\n     public void run() {\n       Value value;\n-      \n+\n+      if (shouldDefer()) {\n+        deferredUncachingExecutor.schedule(\n+            this, revocationPollingMs, TimeUnit.MILLISECONDS);\n+        return;\n+      }\n+\n       synchronized (FsDatasetCache.this) {\n         value \u003d mappableBlockMap.get(key);\n       }\n       Preconditions.checkNotNull(value);\n       Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n-      // TODO: we will eventually need to do revocation here if any clients\n-      // are reading via mmap with checksums enabled.  See HDFS-5182.\n+\n       IOUtils.closeQuietly(value.mappableBlock);\n       synchronized (FsDatasetCache.this) {\n         mappableBlockMap.remove(key);\n       }\n       long newUsedBytes \u003d\n           usedBytesCount.release(value.mappableBlock.getLength());\n       numBlocksCached.addAndGet(-1);\n       dataset.datanode.getMetrics().incrBlocksUncached(1);\n-      LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\", key, newUsedBytes);\n+      if (revocationTimeMs !\u003d 0) {\n+        LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\",\n+            key, newUsedBytes);\n+      } else {\n+        LOG.debug(\"Deferred uncaching of {} completed. usedBytes \u003d {}\",\n+            key, newUsedBytes);\n+      }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      Value value;\n\n      if (shouldDefer()) {\n        deferredUncachingExecutor.schedule(\n            this, revocationPollingMs, TimeUnit.MILLISECONDS);\n        return;\n      }\n\n      synchronized (FsDatasetCache.this) {\n        value \u003d mappableBlockMap.get(key);\n      }\n      Preconditions.checkNotNull(value);\n      Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n\n      IOUtils.closeQuietly(value.mappableBlock);\n      synchronized (FsDatasetCache.this) {\n        mappableBlockMap.remove(key);\n      }\n      long newUsedBytes \u003d\n          usedBytesCount.release(value.mappableBlock.getLength());\n      numBlocksCached.addAndGet(-1);\n      dataset.datanode.getMetrics().incrBlocksUncached(1);\n      if (revocationTimeMs !\u003d 0) {\n        LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\",\n            key, newUsedBytes);\n      } else {\n        LOG.debug(\"Deferred uncaching of {} completed. usedBytes \u003d {}\",\n            key, newUsedBytes);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "93e23a99157c30b51752fc49748c3c210745a187": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6613. Improve logging in caching classes. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1607697 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/07/14 10:13 AM",
      "commitName": "93e23a99157c30b51752fc49748c3c210745a187",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "09/04/14 2:45 PM",
      "commitNameOld": "5c48f379ab359ea7a7c2421df998080f3792a1d9",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 84.81,
      "commitsBetweenForRepo": 513,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,20 @@\n     public void run() {\n       Value value;\n       \n       synchronized (FsDatasetCache.this) {\n         value \u003d mappableBlockMap.get(key);\n       }\n       Preconditions.checkNotNull(value);\n       Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n       // TODO: we will eventually need to do revocation here if any clients\n       // are reading via mmap with checksums enabled.  See HDFS-5182.\n       IOUtils.closeQuietly(value.mappableBlock);\n       synchronized (FsDatasetCache.this) {\n         mappableBlockMap.remove(key);\n       }\n       long newUsedBytes \u003d\n           usedBytesCount.release(value.mappableBlock.getLength());\n       numBlocksCached.addAndGet(-1);\n       dataset.datanode.getMetrics().incrBlocksUncached(1);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Uncaching of \" + key + \" completed.  \" +\n-            \"usedBytes \u003d \" + newUsedBytes);\n-      }\n+      LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\", key, newUsedBytes);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      Value value;\n      \n      synchronized (FsDatasetCache.this) {\n        value \u003d mappableBlockMap.get(key);\n      }\n      Preconditions.checkNotNull(value);\n      Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n      // TODO: we will eventually need to do revocation here if any clients\n      // are reading via mmap with checksums enabled.  See HDFS-5182.\n      IOUtils.closeQuietly(value.mappableBlock);\n      synchronized (FsDatasetCache.this) {\n        mappableBlockMap.remove(key);\n      }\n      long newUsedBytes \u003d\n          usedBytesCount.release(value.mappableBlock.getLength());\n      numBlocksCached.addAndGet(-1);\n      dataset.datanode.getMetrics().incrBlocksUncached(1);\n      LOG.debug(\"Uncaching of {} completed. usedBytes \u003d {}\", key, newUsedBytes);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "d265dd9eb01bb4ed5335872f5976740258d6bfc0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6107. When a block cannot be cached due to limited space on the DataNode, it becomes uncacheable (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1578508 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/03/14 11:46 AM",
      "commitName": "d265dd9eb01bb4ed5335872f5976740258d6bfc0",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "11/03/14 3:41 PM",
      "commitNameOld": "a3616c58dd2ddb16172ca3ab5d66fad52ec0e6d7",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 5.84,
      "commitsBetweenForRepo": 40,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,23 @@\n     public void run() {\n       Value value;\n       \n       synchronized (FsDatasetCache.this) {\n         value \u003d mappableBlockMap.get(key);\n       }\n       Preconditions.checkNotNull(value);\n       Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n       // TODO: we will eventually need to do revocation here if any clients\n       // are reading via mmap with checksums enabled.  See HDFS-5182.\n       IOUtils.closeQuietly(value.mappableBlock);\n       synchronized (FsDatasetCache.this) {\n         mappableBlockMap.remove(key);\n       }\n       long newUsedBytes \u003d\n           usedBytesCount.release(value.mappableBlock.getLength());\n       numBlocksCached.addAndGet(-1);\n+      dataset.datanode.getMetrics().incrBlocksUncached(1);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Uncaching of \" + key + \" completed.  \" +\n             \"usedBytes \u003d \" + newUsedBytes);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      Value value;\n      \n      synchronized (FsDatasetCache.this) {\n        value \u003d mappableBlockMap.get(key);\n      }\n      Preconditions.checkNotNull(value);\n      Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n      // TODO: we will eventually need to do revocation here if any clients\n      // are reading via mmap with checksums enabled.  See HDFS-5182.\n      IOUtils.closeQuietly(value.mappableBlock);\n      synchronized (FsDatasetCache.this) {\n        mappableBlockMap.remove(key);\n      }\n      long newUsedBytes \u003d\n          usedBytesCount.release(value.mappableBlock.getLength());\n      numBlocksCached.addAndGet(-1);\n      dataset.datanode.getMetrics().incrBlocksUncached(1);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Uncaching of \" + key + \" completed.  \" +\n            \"usedBytes \u003d \" + newUsedBytes);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "f0d64a078da7e932b9509734f75170e3e525e68c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5940.  Minor cleanups to ShortCircuitReplica, FsDatasetCache, and DomainSocketWatcher (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567835 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 7:10 PM",
      "commitName": "f0d64a078da7e932b9509734f75170e3e525e68c",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "27/11/13 9:55 AM",
      "commitNameOld": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 77.39,
      "commitsBetweenForRepo": 431,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n     public void run() {\n       Value value;\n       \n       synchronized (FsDatasetCache.this) {\n         value \u003d mappableBlockMap.get(key);\n       }\n       Preconditions.checkNotNull(value);\n       Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n       // TODO: we will eventually need to do revocation here if any clients\n       // are reading via mmap with checksums enabled.  See HDFS-5182.\n       IOUtils.closeQuietly(value.mappableBlock);\n       synchronized (FsDatasetCache.this) {\n         mappableBlockMap.remove(key);\n       }\n       long newUsedBytes \u003d\n           usedBytesCount.release(value.mappableBlock.getLength());\n       numBlocksCached.addAndGet(-1);\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Uncaching of block \" + key.id + \" in \" + key.bpid +\n-            \" completed.  usedBytes \u003d \" + newUsedBytes);\n+        LOG.debug(\"Uncaching of \" + key + \" completed.  \" +\n+            \"usedBytes \u003d \" + newUsedBytes);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      Value value;\n      \n      synchronized (FsDatasetCache.this) {\n        value \u003d mappableBlockMap.get(key);\n      }\n      Preconditions.checkNotNull(value);\n      Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n      // TODO: we will eventually need to do revocation here if any clients\n      // are reading via mmap with checksums enabled.  See HDFS-5182.\n      IOUtils.closeQuietly(value.mappableBlock);\n      synchronized (FsDatasetCache.this) {\n        mappableBlockMap.remove(key);\n      }\n      long newUsedBytes \u003d\n          usedBytesCount.release(value.mappableBlock.getLength());\n      numBlocksCached.addAndGet(-1);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Uncaching of \" + key + \" completed.  \" +\n            \"usedBytes \u003d \" + newUsedBytes);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "13edb391d06c479720202eb5ac81f1c71fe64748": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 9:55 AM",
      "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "19/11/13 4:48 PM",
      "commitNameOld": "efea68dc3538de9aafae206d64903506e41fc9e1",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 7.71,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,22 @@\n     public void run() {\n       Value value;\n       \n       synchronized (FsDatasetCache.this) {\n         value \u003d mappableBlockMap.get(key);\n       }\n       Preconditions.checkNotNull(value);\n       Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n       // TODO: we will eventually need to do revocation here if any clients\n       // are reading via mmap with checksums enabled.  See HDFS-5182.\n       IOUtils.closeQuietly(value.mappableBlock);\n       synchronized (FsDatasetCache.this) {\n         mappableBlockMap.remove(key);\n       }\n       long newUsedBytes \u003d\n           usedBytesCount.release(value.mappableBlock.getLength());\n+      numBlocksCached.addAndGet(-1);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Uncaching of block \" + key.id + \" in \" + key.bpid +\n             \" completed.  usedBytes \u003d \" + newUsedBytes);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      Value value;\n      \n      synchronized (FsDatasetCache.this) {\n        value \u003d mappableBlockMap.get(key);\n      }\n      Preconditions.checkNotNull(value);\n      Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n      // TODO: we will eventually need to do revocation here if any clients\n      // are reading via mmap with checksums enabled.  See HDFS-5182.\n      IOUtils.closeQuietly(value.mappableBlock);\n      synchronized (FsDatasetCache.this) {\n        mappableBlockMap.remove(key);\n      }\n      long newUsedBytes \u003d\n          usedBytesCount.release(value.mappableBlock.getLength());\n      numBlocksCached.addAndGet(-1);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Uncaching of block \" + key.id + \" in \" + key.bpid +\n            \" completed.  usedBytes \u003d \" + newUsedBytes);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5394: Fix race conditions in DN caching and uncaching (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/11/13 7:00 PM",
      "commitName": "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "21/10/13 12:29 PM",
      "commitNameOld": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 17.31,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,21 @@\n     public void run() {\n-      boolean success \u003d false;\n-      try {\n-        block.map();\n-        block.lock();\n-        block.verifyChecksum();\n-        success \u003d true;\n-      } catch (ChecksumException e) {\n-        // Exception message is bogus since this wasn\u0027t caused by a file read\n-        LOG.warn(\"Failed to cache block \" + block.getBlock() + \": Checksum \"\n-            + \"verification failed.\");\n-      } catch (IOException e) {\n-        LOG.warn(\"Failed to cache block \" + block.getBlock() + \": IOException\",\n-            e);\n+      Value value;\n+      \n+      synchronized (FsDatasetCache.this) {\n+        value \u003d mappableBlockMap.get(key);\n       }\n-      // If we failed or the block became uncacheable in the meantime,\n-      // clean up and return the reserved cache allocation \n-      if (!success || \n-          !dataset.validToCache(block.getBlockPoolId(),\n-              block.getBlock().getBlockId())) {\n-        block.close();\n-        long used \u003d usedBytes.get();\n-        while (!usedBytes.compareAndSet(used, used-block.getNumBytes())) {\n-          used \u003d usedBytes.get();\n-        }\n-      } else {\n-        LOG.info(\"Successfully cached block \" + block.getBlock());\n-        cachedBlocks.put(block.getBlock().getBlockId(), block);\n+      Preconditions.checkNotNull(value);\n+      Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n+      // TODO: we will eventually need to do revocation here if any clients\n+      // are reading via mmap with checksums enabled.  See HDFS-5182.\n+      IOUtils.closeQuietly(value.mappableBlock);\n+      synchronized (FsDatasetCache.this) {\n+        mappableBlockMap.remove(key);\n+      }\n+      long newUsedBytes \u003d\n+          usedBytesCount.release(value.mappableBlock.getLength());\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Uncaching of block \" + key.id + \" in \" + key.bpid +\n+            \" completed.  usedBytes \u003d \" + newUsedBytes);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      Value value;\n      \n      synchronized (FsDatasetCache.this) {\n        value \u003d mappableBlockMap.get(key);\n      }\n      Preconditions.checkNotNull(value);\n      Preconditions.checkArgument(value.state \u003d\u003d State.UNCACHING);\n      // TODO: we will eventually need to do revocation here if any clients\n      // are reading via mmap with checksums enabled.  See HDFS-5182.\n      IOUtils.closeQuietly(value.mappableBlock);\n      synchronized (FsDatasetCache.this) {\n        mappableBlockMap.remove(key);\n      }\n      long newUsedBytes \u003d\n          usedBytesCount.release(value.mappableBlock.getLength());\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Uncaching of block \" + key.id + \" in \" + key.bpid +\n            \" completed.  usedBytes \u003d \" + newUsedBytes);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5349. DNA_CACHE and DNA_UNCACHE should be by blockId only (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532116 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/10/13 3:19 PM",
      "commitName": "15d08c4778350a86d7bae0174aeb48f8d8f61cce",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "13/09/13 4:27 PM",
      "commitNameOld": "40eb94ade3161d93e7a762a839004748f6d0ae89",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 30.95,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,30 @@\n     public void run() {\n       boolean success \u003d false;\n       try {\n         block.map();\n         block.lock();\n         block.verifyChecksum();\n         success \u003d true;\n       } catch (ChecksumException e) {\n         // Exception message is bogus since this wasn\u0027t caused by a file read\n         LOG.warn(\"Failed to cache block \" + block.getBlock() + \": Checksum \"\n             + \"verification failed.\");\n       } catch (IOException e) {\n         LOG.warn(\"Failed to cache block \" + block.getBlock() + \": IOException\",\n             e);\n       }\n       // If we failed or the block became uncacheable in the meantime,\n       // clean up and return the reserved cache allocation \n       if (!success || \n-          !dataset.validToCache(block.getBlockPoolId(), block.getBlock())) {\n+          !dataset.validToCache(block.getBlockPoolId(),\n+              block.getBlock().getBlockId())) {\n         block.close();\n         long used \u003d usedBytes.get();\n         while (!usedBytes.compareAndSet(used, used-block.getNumBytes())) {\n           used \u003d usedBytes.get();\n         }\n       } else {\n         LOG.info(\"Successfully cached block \" + block.getBlock());\n         cachedBlocks.put(block.getBlock().getBlockId(), block);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      boolean success \u003d false;\n      try {\n        block.map();\n        block.lock();\n        block.verifyChecksum();\n        success \u003d true;\n      } catch (ChecksumException e) {\n        // Exception message is bogus since this wasn\u0027t caused by a file read\n        LOG.warn(\"Failed to cache block \" + block.getBlock() + \": Checksum \"\n            + \"verification failed.\");\n      } catch (IOException e) {\n        LOG.warn(\"Failed to cache block \" + block.getBlock() + \": IOException\",\n            e);\n      }\n      // If we failed or the block became uncacheable in the meantime,\n      // clean up and return the reserved cache allocation \n      if (!success || \n          !dataset.validToCache(block.getBlockPoolId(),\n              block.getBlock().getBlockId())) {\n        block.close();\n        long used \u003d usedBytes.get();\n        while (!usedBytes.compareAndSet(used, used-block.getNumBytes())) {\n          used \u003d usedBytes.get();\n        }\n      } else {\n        LOG.info(\"Successfully cached block \" + block.getBlock());\n        cachedBlocks.put(block.getBlock().getBlockId(), block);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "40eb94ade3161d93e7a762a839004748f6d0ae89": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5053. NameNode should invoke DataNode APIs to coordinate caching. (Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1523145 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/09/13 4:27 PM",
      "commitName": "40eb94ade3161d93e7a762a839004748f6d0ae89",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "23/08/13 8:41 PM",
      "commitNameOld": "b992219fa13ccee2b417d91222fd0c3e8c3ffe11",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 20.82,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,29 @@\n     public void run() {\n       boolean success \u003d false;\n       try {\n         block.map();\n         block.lock();\n         block.verifyChecksum();\n         success \u003d true;\n       } catch (ChecksumException e) {\n         // Exception message is bogus since this wasn\u0027t caused by a file read\n         LOG.warn(\"Failed to cache block \" + block.getBlock() + \": Checksum \"\n             + \"verification failed.\");\n       } catch (IOException e) {\n         LOG.warn(\"Failed to cache block \" + block.getBlock() + \": IOException\",\n             e);\n       }\n       // If we failed or the block became uncacheable in the meantime,\n       // clean up and return the reserved cache allocation \n       if (!success || \n           !dataset.validToCache(block.getBlockPoolId(), block.getBlock())) {\n         block.close();\n         long used \u003d usedBytes.get();\n         while (!usedBytes.compareAndSet(used, used-block.getNumBytes())) {\n           used \u003d usedBytes.get();\n         }\n       } else {\n+        LOG.info(\"Successfully cached block \" + block.getBlock());\n         cachedBlocks.put(block.getBlock().getBlockId(), block);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      boolean success \u003d false;\n      try {\n        block.map();\n        block.lock();\n        block.verifyChecksum();\n        success \u003d true;\n      } catch (ChecksumException e) {\n        // Exception message is bogus since this wasn\u0027t caused by a file read\n        LOG.warn(\"Failed to cache block \" + block.getBlock() + \": Checksum \"\n            + \"verification failed.\");\n      } catch (IOException e) {\n        LOG.warn(\"Failed to cache block \" + block.getBlock() + \": IOException\",\n            e);\n      }\n      // If we failed or the block became uncacheable in the meantime,\n      // clean up and return the reserved cache allocation \n      if (!success || \n          !dataset.validToCache(block.getBlockPoolId(), block.getBlock())) {\n        block.close();\n        long used \u003d usedBytes.get();\n        while (!usedBytes.compareAndSet(used, used-block.getNumBytes())) {\n          used \u003d usedBytes.get();\n        }\n      } else {\n        LOG.info(\"Successfully cached block \" + block.getBlock());\n        cachedBlocks.put(block.getBlock().getBlockId(), block);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5050.  Add DataNode support for mlock and munlock  (contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1517106 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/08/13 8:41 PM",
      "commitName": "b992219fa13ccee2b417d91222fd0c3e8c3ffe11",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,28 @@\n+    public void run() {\n+      boolean success \u003d false;\n+      try {\n+        block.map();\n+        block.lock();\n+        block.verifyChecksum();\n+        success \u003d true;\n+      } catch (ChecksumException e) {\n+        // Exception message is bogus since this wasn\u0027t caused by a file read\n+        LOG.warn(\"Failed to cache block \" + block.getBlock() + \": Checksum \"\n+            + \"verification failed.\");\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed to cache block \" + block.getBlock() + \": IOException\",\n+            e);\n+      }\n+      // If we failed or the block became uncacheable in the meantime,\n+      // clean up and return the reserved cache allocation \n+      if (!success || \n+          !dataset.validToCache(block.getBlockPoolId(), block.getBlock())) {\n+        block.close();\n+        long used \u003d usedBytes.get();\n+        while (!usedBytes.compareAndSet(used, used-block.getNumBytes())) {\n+          used \u003d usedBytes.get();\n+        }\n+      } else {\n+        cachedBlocks.put(block.getBlock().getBlockId(), block);\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      boolean success \u003d false;\n      try {\n        block.map();\n        block.lock();\n        block.verifyChecksum();\n        success \u003d true;\n      } catch (ChecksumException e) {\n        // Exception message is bogus since this wasn\u0027t caused by a file read\n        LOG.warn(\"Failed to cache block \" + block.getBlock() + \": Checksum \"\n            + \"verification failed.\");\n      } catch (IOException e) {\n        LOG.warn(\"Failed to cache block \" + block.getBlock() + \": IOException\",\n            e);\n      }\n      // If we failed or the block became uncacheable in the meantime,\n      // clean up and return the reserved cache allocation \n      if (!success || \n          !dataset.validToCache(block.getBlockPoolId(), block.getBlock())) {\n        block.close();\n        long used \u003d usedBytes.get();\n        while (!usedBytes.compareAndSet(used, used-block.getNumBytes())) {\n          used \u003d usedBytes.get();\n        }\n      } else {\n        cachedBlocks.put(block.getBlock().getBlockId(), block);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java"
    }
  }
}