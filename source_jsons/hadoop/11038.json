{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReceiver.java",
  "functionName": "translateChunks",
  "functionId": "translateChunks___dataBuf-ByteBuffer__checksumBuf-ByteBuffer",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
  "functionStartLine": 512,
  "functionEndLine": 514,
  "numCommitsSeen": 122,
  "timeTaken": 3507,
  "changeHistory": [
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
    "99a68a14237b4cd1936ba5e9468d25d35dad594c",
    "f84552ac35bb5221290be68fece9c779ebeaf4bc"
  ],
  "changeHistoryShort": {
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": "Ymultichange(Yparameterchange,Ybodychange)",
    "99a68a14237b4cd1936ba5e9468d25d35dad594c": "Yexceptionschange",
    "f84552ac35bb5221290be68fece9c779ebeaf4bc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-3721. hsync support broke wire compatibility. Contributed by Todd Lipcon and Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1371495 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/12 2:31 PM",
      "commitName": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
      "commitAuthor": "Aaron Myers",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-3721. hsync support broke wire compatibility. Contributed by Todd Lipcon and Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1371495 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/08/12 2:31 PM",
          "commitName": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "15/07/12 7:58 PM",
          "commitNameOld": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
          "commitAuthorOld": "Harsh J",
          "daysBetweenCommits": 24.77,
          "commitsBetweenForRepo": 121,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,10 +1,3 @@\n-  private void translateChunks( byte[] dataBuf, int dataOff, int len,\n-      byte[] checksumBuf, int checksumOff ) {\n-    if (len \u003d\u003d 0) return;\n-    \n-    int numChunks \u003d (len - 1)/bytesPerChecksum + 1;\n-    \n-    diskChecksum.calculateChunkedSums(\n-        ByteBuffer.wrap(dataBuf, dataOff, len),\n-        ByteBuffer.wrap(checksumBuf, checksumOff, numChunks * checksumSize));\n+  private void translateChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf) {\n+    diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void translateChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf) {\n    diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
          "extendedDetails": {
            "oldValue": "[dataBuf-byte[], dataOff-int, len-int, checksumBuf-byte[], checksumOff-int]",
            "newValue": "[dataBuf-ByteBuffer, checksumBuf-ByteBuffer]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3721. hsync support broke wire compatibility. Contributed by Todd Lipcon and Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1371495 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/08/12 2:31 PM",
          "commitName": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "15/07/12 7:58 PM",
          "commitNameOld": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
          "commitAuthorOld": "Harsh J",
          "daysBetweenCommits": 24.77,
          "commitsBetweenForRepo": 121,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,10 +1,3 @@\n-  private void translateChunks( byte[] dataBuf, int dataOff, int len,\n-      byte[] checksumBuf, int checksumOff ) {\n-    if (len \u003d\u003d 0) return;\n-    \n-    int numChunks \u003d (len - 1)/bytesPerChecksum + 1;\n-    \n-    diskChecksum.calculateChunkedSums(\n-        ByteBuffer.wrap(dataBuf, dataOff, len),\n-        ByteBuffer.wrap(checksumBuf, checksumOff, numChunks * checksumSize));\n+  private void translateChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf) {\n+    diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void translateChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf) {\n    diskChecksum.calculateChunkedSums(dataBuf, checksumBuf);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
          "extendedDetails": {}
        }
      ]
    },
    "99a68a14237b4cd1936ba5e9468d25d35dad594c": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-3155. Clean up FSDataset implemenation related code.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1306582 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/03/12 1:37 PM",
      "commitName": "99a68a14237b4cd1936ba5e9468d25d35dad594c",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "16/03/12 10:32 AM",
      "commitNameOld": "662b1887af4e39f3eadd7dda4953c7f2529b43bc",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 12.13,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,10 @@\n-  private void translateChunks( byte[] dataBuf, int dataOff, int len, \n-                             byte[] checksumBuf, int checksumOff ) \n-                             throws IOException {\n+  private void translateChunks( byte[] dataBuf, int dataOff, int len,\n+      byte[] checksumBuf, int checksumOff ) {\n     if (len \u003d\u003d 0) return;\n     \n     int numChunks \u003d (len - 1)/bytesPerChecksum + 1;\n     \n     diskChecksum.calculateChunkedSums(\n         ByteBuffer.wrap(dataBuf, dataOff, len),\n         ByteBuffer.wrap(checksumBuf, checksumOff, numChunks * checksumSize));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void translateChunks( byte[] dataBuf, int dataOff, int len,\n      byte[] checksumBuf, int checksumOff ) {\n    if (len \u003d\u003d 0) return;\n    \n    int numChunks \u003d (len - 1)/bytesPerChecksum + 1;\n    \n    diskChecksum.calculateChunkedSums(\n        ByteBuffer.wrap(dataBuf, dataOff, len),\n        ByteBuffer.wrap(checksumBuf, checksumOff, numChunks * checksumSize));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
      "extendedDetails": {
        "oldValue": "[IOException]",
        "newValue": "[]"
      }
    },
    "f84552ac35bb5221290be68fece9c779ebeaf4bc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2130. Switch default checksum to CRC32C. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196889 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/11/11 5:35 PM",
      "commitName": "f84552ac35bb5221290be68fece9c779ebeaf4bc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,11 @@\n+  private void translateChunks( byte[] dataBuf, int dataOff, int len, \n+                             byte[] checksumBuf, int checksumOff ) \n+                             throws IOException {\n+    if (len \u003d\u003d 0) return;\n+    \n+    int numChunks \u003d (len - 1)/bytesPerChecksum + 1;\n+    \n+    diskChecksum.calculateChunkedSums(\n+        ByteBuffer.wrap(dataBuf, dataOff, len),\n+        ByteBuffer.wrap(checksumBuf, checksumOff, numChunks * checksumSize));\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void translateChunks( byte[] dataBuf, int dataOff, int len, \n                             byte[] checksumBuf, int checksumOff ) \n                             throws IOException {\n    if (len \u003d\u003d 0) return;\n    \n    int numChunks \u003d (len - 1)/bytesPerChecksum + 1;\n    \n    diskChecksum.calculateChunkedSums(\n        ByteBuffer.wrap(dataBuf, dataOff, len),\n        ByteBuffer.wrap(checksumBuf, checksumOff, numChunks * checksumSize));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java"
    }
  }
}