{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DiskBalancerVolumeSet.java",
  "functionName": "computeVolumeDataDensity",
  "functionId": "computeVolumeDataDensity",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolumeSet.java",
  "functionStartLine": 124,
  "functionEndLine": 159,
  "numCommitsSeen": 6,
  "timeTaken": 797,
  "changeHistory": [
    "1594b472bb9df7537dbc001411c99058cc11ba41",
    "91a5c4814381a4d4c3ce9b29a1f85299e03be835"
  ],
  "changeHistoryShort": {
    "1594b472bb9df7537dbc001411c99058cc11ba41": "Ybodychange",
    "91a5c4814381a4d4c3ce9b29a1f85299e03be835": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1594b472bb9df7537dbc001411c99058cc11ba41": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9543. DiskBalancer: Add Data mover. Contributed by Anu Engineer.\n",
      "commitDate": "23/06/16 6:20 PM",
      "commitName": "1594b472bb9df7537dbc001411c99058cc11ba41",
      "commitAuthor": "Anu Engineer",
      "commitDateOld": "23/06/16 6:18 PM",
      "commitNameOld": "747227e9dea10ac6b5f601b7cf4dcc418b10d9c8",
      "commitAuthorOld": "Anu Engineer",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,36 @@\n   public void computeVolumeDataDensity() {\n     long totalCapacity \u003d 0;\n     long totalUsed \u003d 0;\n     sortedQueue.clear();\n \n     // when we plan to re-distribute data we need to make\n     // sure that we skip failed volumes.\n     for (DiskBalancerVolume volume : volumes) {\n       if (!volume.isFailed() \u0026\u0026 !volume.isSkip()) {\n \n         if (volume.computeEffectiveCapacity() \u003c 0) {\n           skipMisConfiguredVolume(volume);\n           continue;\n         }\n \n         totalCapacity +\u003d volume.computeEffectiveCapacity();\n         totalUsed +\u003d volume.getUsed();\n       }\n     }\n \n     if (totalCapacity !\u003d 0) {\n-      this.idealUsed \u003d totalUsed / (float) totalCapacity;\n+      this.idealUsed \u003d truncateDecimals(totalUsed /\n+          (double) totalCapacity);\n     }\n \n     for (DiskBalancerVolume volume : volumes) {\n       if (!volume.isFailed() \u0026\u0026 !volume.isSkip()) {\n-        float dfsUsedRatio \u003d\n-            volume.getUsed() / (float) volume.computeEffectiveCapacity();\n+        double dfsUsedRatio \u003d\n+            truncateDecimals(volume.getUsed() /\n+                (double) volume.computeEffectiveCapacity());\n+\n         volume.setVolumeDataDensity(this.idealUsed - dfsUsedRatio);\n         sortedQueue.add(volume);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void computeVolumeDataDensity() {\n    long totalCapacity \u003d 0;\n    long totalUsed \u003d 0;\n    sortedQueue.clear();\n\n    // when we plan to re-distribute data we need to make\n    // sure that we skip failed volumes.\n    for (DiskBalancerVolume volume : volumes) {\n      if (!volume.isFailed() \u0026\u0026 !volume.isSkip()) {\n\n        if (volume.computeEffectiveCapacity() \u003c 0) {\n          skipMisConfiguredVolume(volume);\n          continue;\n        }\n\n        totalCapacity +\u003d volume.computeEffectiveCapacity();\n        totalUsed +\u003d volume.getUsed();\n      }\n    }\n\n    if (totalCapacity !\u003d 0) {\n      this.idealUsed \u003d truncateDecimals(totalUsed /\n          (double) totalCapacity);\n    }\n\n    for (DiskBalancerVolume volume : volumes) {\n      if (!volume.isFailed() \u0026\u0026 !volume.isSkip()) {\n        double dfsUsedRatio \u003d\n            truncateDecimals(volume.getUsed() /\n                (double) volume.computeEffectiveCapacity());\n\n        volume.setVolumeDataDensity(this.idealUsed - dfsUsedRatio);\n        sortedQueue.add(volume);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolumeSet.java",
      "extendedDetails": {}
    },
    "91a5c4814381a4d4c3ce9b29a1f85299e03be835": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9420. Add DataModels for DiskBalancer. Contributed by Anu Engineer\n",
      "commitDate": "23/06/16 6:18 PM",
      "commitName": "91a5c4814381a4d4c3ce9b29a1f85299e03be835",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "diff": "@@ -0,0 +1,33 @@\n+  public void computeVolumeDataDensity() {\n+    long totalCapacity \u003d 0;\n+    long totalUsed \u003d 0;\n+    sortedQueue.clear();\n+\n+    // when we plan to re-distribute data we need to make\n+    // sure that we skip failed volumes.\n+    for (DiskBalancerVolume volume : volumes) {\n+      if (!volume.isFailed() \u0026\u0026 !volume.isSkip()) {\n+\n+        if (volume.computeEffectiveCapacity() \u003c 0) {\n+          skipMisConfiguredVolume(volume);\n+          continue;\n+        }\n+\n+        totalCapacity +\u003d volume.computeEffectiveCapacity();\n+        totalUsed +\u003d volume.getUsed();\n+      }\n+    }\n+\n+    if (totalCapacity !\u003d 0) {\n+      this.idealUsed \u003d totalUsed / (float) totalCapacity;\n+    }\n+\n+    for (DiskBalancerVolume volume : volumes) {\n+      if (!volume.isFailed() \u0026\u0026 !volume.isSkip()) {\n+        float dfsUsedRatio \u003d\n+            volume.getUsed() / (float) volume.computeEffectiveCapacity();\n+        volume.setVolumeDataDensity(this.idealUsed - dfsUsedRatio);\n+        sortedQueue.add(volume);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void computeVolumeDataDensity() {\n    long totalCapacity \u003d 0;\n    long totalUsed \u003d 0;\n    sortedQueue.clear();\n\n    // when we plan to re-distribute data we need to make\n    // sure that we skip failed volumes.\n    for (DiskBalancerVolume volume : volumes) {\n      if (!volume.isFailed() \u0026\u0026 !volume.isSkip()) {\n\n        if (volume.computeEffectiveCapacity() \u003c 0) {\n          skipMisConfiguredVolume(volume);\n          continue;\n        }\n\n        totalCapacity +\u003d volume.computeEffectiveCapacity();\n        totalUsed +\u003d volume.getUsed();\n      }\n    }\n\n    if (totalCapacity !\u003d 0) {\n      this.idealUsed \u003d totalUsed / (float) totalCapacity;\n    }\n\n    for (DiskBalancerVolume volume : volumes) {\n      if (!volume.isFailed() \u0026\u0026 !volume.isSkip()) {\n        float dfsUsedRatio \u003d\n            volume.getUsed() / (float) volume.computeEffectiveCapacity();\n        volume.setVolumeDataDensity(this.idealUsed - dfsUsedRatio);\n        sortedQueue.add(volume);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolumeSet.java"
    }
  }
}