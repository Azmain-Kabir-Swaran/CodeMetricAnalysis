{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "processDatanodeOrExternalError",
  "functionId": "processDatanodeOrExternalError",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 1225,
  "functionEndLine": 1291,
  "numCommitsSeen": 35,
  "timeTaken": 2066,
  "changeHistory": [
    "4a5819dae2b0ca8f8b6d94ef464882d079d86593",
    "193d27de0a5d23a61cabd41162ebc3292d8526d1",
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd"
  ],
  "changeHistoryShort": {
    "4a5819dae2b0ca8f8b6d94ef464882d079d86593": "Ybodychange",
    "193d27de0a5d23a61cabd41162ebc3292d8526d1": "Ybodychange",
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd": "Ybodychange"
  },
  "changeHistoryDetails": {
    "4a5819dae2b0ca8f8b6d94ef464882d079d86593": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10303. DataStreamer#ResponseProcessor calculates packet ack latency incorrectly. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "17/05/16 6:53 AM",
      "commitName": "4a5819dae2b0ca8f8b6d94ef464882d079d86593",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "27/04/16 2:22 PM",
      "commitNameOld": "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 19.69,
      "commitsBetweenForRepo": 119,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,67 @@\n   private boolean processDatanodeOrExternalError() throws IOException {\n     if (!errorState.hasDatanodeError() \u0026\u0026 !shouldHandleExternalError()) {\n       return false;\n     }\n     LOG.debug(\"start process datanode/external error, {}\", this);\n     if (response !\u003d null) {\n       LOG.info(\"Error Recovery for \" + block +\n           \" waiting for responder to exit. \");\n       return true;\n     }\n     closeStream();\n \n     // move packets from ack queue to front of the data queue\n     synchronized (dataQueue) {\n       dataQueue.addAll(0, ackQueue);\n       ackQueue.clear();\n+      packetSendTime.clear();\n     }\n \n     // If we had to recover the pipeline five times in a row for the\n     // same packet, this client likely has corrupt data or corrupting\n     // during transmission.\n     if (!errorState.isRestartingNode() \u0026\u0026 ++pipelineRecoveryCount \u003e 5) {\n       LOG.warn(\"Error recovering pipeline for writing \" +\n           block + \". Already retried 5 times for the same packet.\");\n       lastException.set(new IOException(\"Failing write. Tried pipeline \" +\n           \"recovery 5 times without success.\"));\n       streamerClosed \u003d true;\n       return false;\n     }\n \n     setupPipelineForAppendOrRecovery();\n \n     if (!streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n       if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE) {\n \n         // If we had an error while closing the pipeline, we go through a fast-path\n         // where the BlockReceiver does not run. Instead, the DataNode just finalizes\n         // the block immediately during the \u0027connect ack\u0027 process. So, we want to pull\n         // the end-of-block packet from the dataQueue, since we don\u0027t actually have\n         // a true pipeline to send it over.\n         //\n         // We also need to set lastAckedSeqno to the end-of-block Packet\u0027s seqno, so that\n         // a client waiting on close() will be aware that the flush finished.\n         synchronized (dataQueue) {\n           DFSPacket endOfBlockPacket \u003d dataQueue.remove();  // remove the end of block packet\n           // Close any trace span associated with this Packet\n           TraceScope scope \u003d endOfBlockPacket.getTraceScope();\n           if (scope !\u003d null) {\n             scope.reattach();\n             scope.close();\n             endOfBlockPacket.setTraceScope(null);\n           }\n           assert endOfBlockPacket.isLastPacketInBlock();\n           assert lastAckedSeqno \u003d\u003d endOfBlockPacket.getSeqno() - 1;\n           lastAckedSeqno \u003d endOfBlockPacket.getSeqno();\n           pipelineRecoveryCount \u003d 0;\n           dataQueue.notifyAll();\n         }\n         endBlock();\n       } else {\n         initDataStreaming();\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean processDatanodeOrExternalError() throws IOException {\n    if (!errorState.hasDatanodeError() \u0026\u0026 !shouldHandleExternalError()) {\n      return false;\n    }\n    LOG.debug(\"start process datanode/external error, {}\", this);\n    if (response !\u003d null) {\n      LOG.info(\"Error Recovery for \" + block +\n          \" waiting for responder to exit. \");\n      return true;\n    }\n    closeStream();\n\n    // move packets from ack queue to front of the data queue\n    synchronized (dataQueue) {\n      dataQueue.addAll(0, ackQueue);\n      ackQueue.clear();\n      packetSendTime.clear();\n    }\n\n    // If we had to recover the pipeline five times in a row for the\n    // same packet, this client likely has corrupt data or corrupting\n    // during transmission.\n    if (!errorState.isRestartingNode() \u0026\u0026 ++pipelineRecoveryCount \u003e 5) {\n      LOG.warn(\"Error recovering pipeline for writing \" +\n          block + \". Already retried 5 times for the same packet.\");\n      lastException.set(new IOException(\"Failing write. Tried pipeline \" +\n          \"recovery 5 times without success.\"));\n      streamerClosed \u003d true;\n      return false;\n    }\n\n    setupPipelineForAppendOrRecovery();\n\n    if (!streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE) {\n\n        // If we had an error while closing the pipeline, we go through a fast-path\n        // where the BlockReceiver does not run. Instead, the DataNode just finalizes\n        // the block immediately during the \u0027connect ack\u0027 process. So, we want to pull\n        // the end-of-block packet from the dataQueue, since we don\u0027t actually have\n        // a true pipeline to send it over.\n        //\n        // We also need to set lastAckedSeqno to the end-of-block Packet\u0027s seqno, so that\n        // a client waiting on close() will be aware that the flush finished.\n        synchronized (dataQueue) {\n          DFSPacket endOfBlockPacket \u003d dataQueue.remove();  // remove the end of block packet\n          // Close any trace span associated with this Packet\n          TraceScope scope \u003d endOfBlockPacket.getTraceScope();\n          if (scope !\u003d null) {\n            scope.reattach();\n            scope.close();\n            endOfBlockPacket.setTraceScope(null);\n          }\n          assert endOfBlockPacket.isLastPacketInBlock();\n          assert lastAckedSeqno \u003d\u003d endOfBlockPacket.getSeqno() - 1;\n          lastAckedSeqno \u003d endOfBlockPacket.getSeqno();\n          pipelineRecoveryCount \u003d 0;\n          dataQueue.notifyAll();\n        }\n        endBlock();\n      } else {\n        initDataStreaming();\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "193d27de0a5d23a61cabd41162ebc3292d8526d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9752. Permanent write failures may happen to slow writers during datanode rolling upgrades. Contributed by Walter Su.\n",
      "commitDate": "08/02/16 10:16 AM",
      "commitName": "193d27de0a5d23a61cabd41162ebc3292d8526d1",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "25/01/16 6:32 PM",
      "commitNameOld": "bd909ed9f2d853f614f04a50e2230a7932732776",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 13.66,
      "commitsBetweenForRepo": 92,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,66 @@\n   private boolean processDatanodeOrExternalError() throws IOException {\n     if (!errorState.hasDatanodeError() \u0026\u0026 !shouldHandleExternalError()) {\n       return false;\n     }\n     LOG.debug(\"start process datanode/external error, {}\", this);\n     if (response !\u003d null) {\n       LOG.info(\"Error Recovery for \" + block +\n           \" waiting for responder to exit. \");\n       return true;\n     }\n     closeStream();\n \n     // move packets from ack queue to front of the data queue\n     synchronized (dataQueue) {\n       dataQueue.addAll(0, ackQueue);\n       ackQueue.clear();\n     }\n \n-    // Record the new pipeline failure recovery.\n-    if (lastAckedSeqnoBeforeFailure !\u003d lastAckedSeqno) {\n-      lastAckedSeqnoBeforeFailure \u003d lastAckedSeqno;\n-      pipelineRecoveryCount \u003d 1;\n-    } else {\n-      // If we had to recover the pipeline five times in a row for the\n-      // same packet, this client likely has corrupt data or corrupting\n-      // during transmission.\n-      if (++pipelineRecoveryCount \u003e 5) {\n-        LOG.warn(\"Error recovering pipeline for writing \" +\n-            block + \". Already retried 5 times for the same packet.\");\n-        lastException.set(new IOException(\"Failing write. Tried pipeline \" +\n-            \"recovery 5 times without success.\"));\n-        streamerClosed \u003d true;\n-        return false;\n-      }\n+    // If we had to recover the pipeline five times in a row for the\n+    // same packet, this client likely has corrupt data or corrupting\n+    // during transmission.\n+    if (!errorState.isRestartingNode() \u0026\u0026 ++pipelineRecoveryCount \u003e 5) {\n+      LOG.warn(\"Error recovering pipeline for writing \" +\n+          block + \". Already retried 5 times for the same packet.\");\n+      lastException.set(new IOException(\"Failing write. Tried pipeline \" +\n+          \"recovery 5 times without success.\"));\n+      streamerClosed \u003d true;\n+      return false;\n     }\n \n     setupPipelineForAppendOrRecovery();\n \n     if (!streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n       if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE) {\n \n         // If we had an error while closing the pipeline, we go through a fast-path\n         // where the BlockReceiver does not run. Instead, the DataNode just finalizes\n         // the block immediately during the \u0027connect ack\u0027 process. So, we want to pull\n         // the end-of-block packet from the dataQueue, since we don\u0027t actually have\n         // a true pipeline to send it over.\n         //\n         // We also need to set lastAckedSeqno to the end-of-block Packet\u0027s seqno, so that\n         // a client waiting on close() will be aware that the flush finished.\n         synchronized (dataQueue) {\n           DFSPacket endOfBlockPacket \u003d dataQueue.remove();  // remove the end of block packet\n           // Close any trace span associated with this Packet\n           TraceScope scope \u003d endOfBlockPacket.getTraceScope();\n           if (scope !\u003d null) {\n             scope.reattach();\n             scope.close();\n             endOfBlockPacket.setTraceScope(null);\n           }\n           assert endOfBlockPacket.isLastPacketInBlock();\n           assert lastAckedSeqno \u003d\u003d endOfBlockPacket.getSeqno() - 1;\n           lastAckedSeqno \u003d endOfBlockPacket.getSeqno();\n+          pipelineRecoveryCount \u003d 0;\n           dataQueue.notifyAll();\n         }\n         endBlock();\n       } else {\n         initDataStreaming();\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean processDatanodeOrExternalError() throws IOException {\n    if (!errorState.hasDatanodeError() \u0026\u0026 !shouldHandleExternalError()) {\n      return false;\n    }\n    LOG.debug(\"start process datanode/external error, {}\", this);\n    if (response !\u003d null) {\n      LOG.info(\"Error Recovery for \" + block +\n          \" waiting for responder to exit. \");\n      return true;\n    }\n    closeStream();\n\n    // move packets from ack queue to front of the data queue\n    synchronized (dataQueue) {\n      dataQueue.addAll(0, ackQueue);\n      ackQueue.clear();\n    }\n\n    // If we had to recover the pipeline five times in a row for the\n    // same packet, this client likely has corrupt data or corrupting\n    // during transmission.\n    if (!errorState.isRestartingNode() \u0026\u0026 ++pipelineRecoveryCount \u003e 5) {\n      LOG.warn(\"Error recovering pipeline for writing \" +\n          block + \". Already retried 5 times for the same packet.\");\n      lastException.set(new IOException(\"Failing write. Tried pipeline \" +\n          \"recovery 5 times without success.\"));\n      streamerClosed \u003d true;\n      return false;\n    }\n\n    setupPipelineForAppendOrRecovery();\n\n    if (!streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE) {\n\n        // If we had an error while closing the pipeline, we go through a fast-path\n        // where the BlockReceiver does not run. Instead, the DataNode just finalizes\n        // the block immediately during the \u0027connect ack\u0027 process. So, we want to pull\n        // the end-of-block packet from the dataQueue, since we don\u0027t actually have\n        // a true pipeline to send it over.\n        //\n        // We also need to set lastAckedSeqno to the end-of-block Packet\u0027s seqno, so that\n        // a client waiting on close() will be aware that the flush finished.\n        synchronized (dataQueue) {\n          DFSPacket endOfBlockPacket \u003d dataQueue.remove();  // remove the end of block packet\n          // Close any trace span associated with this Packet\n          TraceScope scope \u003d endOfBlockPacket.getTraceScope();\n          if (scope !\u003d null) {\n            scope.reattach();\n            scope.close();\n            endOfBlockPacket.setTraceScope(null);\n          }\n          assert endOfBlockPacket.isLastPacketInBlock();\n          assert lastAckedSeqno \u003d\u003d endOfBlockPacket.getSeqno() - 1;\n          lastAckedSeqno \u003d endOfBlockPacket.getSeqno();\n          pipelineRecoveryCount \u003d 0;\n          dataQueue.notifyAll();\n        }\n        endBlock();\n      } else {\n        initDataStreaming();\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9180. Update excluded DataNodes in DFSStripedOutputStream based on failures in data streamers. Contributed by Jing Zhao.\n",
      "commitDate": "06/10/15 10:56 AM",
      "commitName": "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 2.97,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,71 @@\n   private boolean processDatanodeOrExternalError() throws IOException {\n     if (!errorState.hasDatanodeError() \u0026\u0026 !shouldHandleExternalError()) {\n       return false;\n     }\n+    LOG.debug(\"start process datanode/external error, {}\", this);\n     if (response !\u003d null) {\n       LOG.info(\"Error Recovery for \" + block +\n           \" waiting for responder to exit. \");\n       return true;\n     }\n     closeStream();\n \n     // move packets from ack queue to front of the data queue\n     synchronized (dataQueue) {\n       dataQueue.addAll(0, ackQueue);\n       ackQueue.clear();\n     }\n \n     // Record the new pipeline failure recovery.\n     if (lastAckedSeqnoBeforeFailure !\u003d lastAckedSeqno) {\n       lastAckedSeqnoBeforeFailure \u003d lastAckedSeqno;\n       pipelineRecoveryCount \u003d 1;\n     } else {\n       // If we had to recover the pipeline five times in a row for the\n       // same packet, this client likely has corrupt data or corrupting\n       // during transmission.\n       if (++pipelineRecoveryCount \u003e 5) {\n         LOG.warn(\"Error recovering pipeline for writing \" +\n             block + \". Already retried 5 times for the same packet.\");\n         lastException.set(new IOException(\"Failing write. Tried pipeline \" +\n             \"recovery 5 times without success.\"));\n         streamerClosed \u003d true;\n         return false;\n       }\n     }\n \n     setupPipelineForAppendOrRecovery();\n \n     if (!streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n       if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE) {\n \n         // If we had an error while closing the pipeline, we go through a fast-path\n         // where the BlockReceiver does not run. Instead, the DataNode just finalizes\n         // the block immediately during the \u0027connect ack\u0027 process. So, we want to pull\n         // the end-of-block packet from the dataQueue, since we don\u0027t actually have\n         // a true pipeline to send it over.\n         //\n         // We also need to set lastAckedSeqno to the end-of-block Packet\u0027s seqno, so that\n         // a client waiting on close() will be aware that the flush finished.\n         synchronized (dataQueue) {\n           DFSPacket endOfBlockPacket \u003d dataQueue.remove();  // remove the end of block packet\n           // Close any trace span associated with this Packet\n           TraceScope scope \u003d endOfBlockPacket.getTraceScope();\n           if (scope !\u003d null) {\n             scope.reattach();\n             scope.close();\n             endOfBlockPacket.setTraceScope(null);\n           }\n           assert endOfBlockPacket.isLastPacketInBlock();\n           assert lastAckedSeqno \u003d\u003d endOfBlockPacket.getSeqno() - 1;\n           lastAckedSeqno \u003d endOfBlockPacket.getSeqno();\n           dataQueue.notifyAll();\n         }\n         endBlock();\n       } else {\n         initDataStreaming();\n       }\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean processDatanodeOrExternalError() throws IOException {\n    if (!errorState.hasDatanodeError() \u0026\u0026 !shouldHandleExternalError()) {\n      return false;\n    }\n    LOG.debug(\"start process datanode/external error, {}\", this);\n    if (response !\u003d null) {\n      LOG.info(\"Error Recovery for \" + block +\n          \" waiting for responder to exit. \");\n      return true;\n    }\n    closeStream();\n\n    // move packets from ack queue to front of the data queue\n    synchronized (dataQueue) {\n      dataQueue.addAll(0, ackQueue);\n      ackQueue.clear();\n    }\n\n    // Record the new pipeline failure recovery.\n    if (lastAckedSeqnoBeforeFailure !\u003d lastAckedSeqno) {\n      lastAckedSeqnoBeforeFailure \u003d lastAckedSeqno;\n      pipelineRecoveryCount \u003d 1;\n    } else {\n      // If we had to recover the pipeline five times in a row for the\n      // same packet, this client likely has corrupt data or corrupting\n      // during transmission.\n      if (++pipelineRecoveryCount \u003e 5) {\n        LOG.warn(\"Error recovering pipeline for writing \" +\n            block + \". Already retried 5 times for the same packet.\");\n        lastException.set(new IOException(\"Failing write. Tried pipeline \" +\n            \"recovery 5 times without success.\"));\n        streamerClosed \u003d true;\n        return false;\n      }\n    }\n\n    setupPipelineForAppendOrRecovery();\n\n    if (!streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE) {\n\n        // If we had an error while closing the pipeline, we go through a fast-path\n        // where the BlockReceiver does not run. Instead, the DataNode just finalizes\n        // the block immediately during the \u0027connect ack\u0027 process. So, we want to pull\n        // the end-of-block packet from the dataQueue, since we don\u0027t actually have\n        // a true pipeline to send it over.\n        //\n        // We also need to set lastAckedSeqno to the end-of-block Packet\u0027s seqno, so that\n        // a client waiting on close() will be aware that the flush finished.\n        synchronized (dataQueue) {\n          DFSPacket endOfBlockPacket \u003d dataQueue.remove();  // remove the end of block packet\n          // Close any trace span associated with this Packet\n          TraceScope scope \u003d endOfBlockPacket.getTraceScope();\n          if (scope !\u003d null) {\n            scope.reattach();\n            scope.close();\n            endOfBlockPacket.setTraceScope(null);\n          }\n          assert endOfBlockPacket.isLastPacketInBlock();\n          assert lastAckedSeqno \u003d\u003d endOfBlockPacket.getSeqno() - 1;\n          lastAckedSeqno \u003d endOfBlockPacket.getSeqno();\n          dataQueue.notifyAll();\n        }\n        endBlock();\n      } else {\n        initDataStreaming();\n      }\n    }\n\n    return false;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    }
  }
}