{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStorage.java",
  "functionName": "linkBlocks",
  "functionId": "linkBlocks___from-File__to-File__oldLV-int__hl-HardLink__conf-Configuration",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
  "functionStartLine": 1082,
  "functionEndLine": 1142,
  "numCommitsSeen": 157,
  "timeTaken": 9570,
  "changeHistory": [
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a",
    "543aac9f281871a40473e83061f6deadc0bbdab7",
    "84ddedc0b2d58257d45c16ee5e83b15f94a7ba3a",
    "2c8496ebf3b7b31c2e18fdf8d4cb2a0115f43112",
    "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
    "e46cb800028c95f9bce575d05268cd10d0913222",
    "8fa265a290792ff42635ff9b42416c634f88bdf3",
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16",
    "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
    "64641c28b5ea8538033060452b0c45b7f2eeb60c",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a": "Ybodychange",
    "543aac9f281871a40473e83061f6deadc0bbdab7": "Ybodychange",
    "84ddedc0b2d58257d45c16ee5e83b15f94a7ba3a": "Ybodychange",
    "2c8496ebf3b7b31c2e18fdf8d4cb2a0115f43112": "Ybodychange",
    "662e17b46a0f41ade6a304e12925b70b5d09fc2f": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "e46cb800028c95f9bce575d05268cd10d0913222": "Ybodychange",
    "8fa265a290792ff42635ff9b42416c634f88bdf3": "Ybodychange",
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16": "Ymultichange(Yparameterchange,Ybodychange)",
    "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9": "Ybodychange",
    "64641c28b5ea8538033060452b0c45b7f2eeb60c": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12997. Move logging to slf4j in BlockPoolSliceStorage and Storage. Contributed by Ajay Kumar.\n",
      "commitDate": "01/02/18 10:45 AM",
      "commitName": "b3ae11d59790bb08b81848e9f944db7d3afbbd8a",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "8239e3afb31d3c4485817d4b8b8b195b554acbe7",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 47.7,
      "commitsBetweenForRepo": 240,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   private static void linkBlocks(File from, File to, int oldLV,\n       HardLink hl, Configuration conf) throws IOException {\n-    LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n+    LOG.info(\"Start linking block files from {} to {}\", from, to);\n     boolean upgradeToIdBasedLayout \u003d false;\n     // If we are upgrading from a version older than the one where we introduced\n     // block ID-based layout (32x32) AND we\u0027re working with the finalized\n     // directory, we\u0027ll need to upgrade from the old layout to the new one. The\n     // upgrade path from pre-blockid based layouts (\u003e-56) and blockid based\n     // 256x256 layouts (-56) is fortunately the same.\n     if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT_32_by_32\n         .getInfo().getLayoutVersion()\n         \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n       upgradeToIdBasedLayout \u003d true;\n     }\n \n     final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n     linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n         idBasedLayoutSingleLinks);\n \n     // Detect and remove duplicate entries.\n     final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n         findDuplicateEntries(idBasedLayoutSingleLinks);\n     if (!duplicates.isEmpty()) {\n-      LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n-          \"entries within the same volume.\");\n+      LOG.error(\"There are {} duplicate block \" +\n+          \"entries within the same volume.\", duplicates.size());\n       removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n     }\n \n     final int numLinkWorkers \u003d conf.getInt(\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n     ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n     final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n     List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n     for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n       final int iCopy \u003d i;\n       futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n         @Override\n         public Void call() throws IOException {\n           int upperBound \u003d Math.min(iCopy + step,\n               idBasedLayoutSingleLinks.size());\n           for (int j \u003d iCopy; j \u003c upperBound; j++) {\n             LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n             HardLink.createHardLink(cur.src, cur.dst);\n           }\n           return null;\n         }\n       }));\n     }\n     linkWorkers.shutdown();\n     for (Future\u003cVoid\u003e f : futures) {\n       try {\n         f.get();\n       } catch (InterruptedException e) {\n         Thread.currentThread().interrupt();\n         throw new IOException(e);\n       } catch (ExecutionException e) {\n         throw new IOException(e);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void linkBlocks(File from, File to, int oldLV,\n      HardLink hl, Configuration conf) throws IOException {\n    LOG.info(\"Start linking block files from {} to {}\", from, to);\n    boolean upgradeToIdBasedLayout \u003d false;\n    // If we are upgrading from a version older than the one where we introduced\n    // block ID-based layout (32x32) AND we\u0027re working with the finalized\n    // directory, we\u0027ll need to upgrade from the old layout to the new one. The\n    // upgrade path from pre-blockid based layouts (\u003e-56) and blockid based\n    // 256x256 layouts (-56) is fortunately the same.\n    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT_32_by_32\n        .getInfo().getLayoutVersion()\n        \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n      upgradeToIdBasedLayout \u003d true;\n    }\n\n    final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n        idBasedLayoutSingleLinks);\n\n    // Detect and remove duplicate entries.\n    final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n        findDuplicateEntries(idBasedLayoutSingleLinks);\n    if (!duplicates.isEmpty()) {\n      LOG.error(\"There are {} duplicate block \" +\n          \"entries within the same volume.\", duplicates.size());\n      removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n    }\n\n    final int numLinkWorkers \u003d conf.getInt(\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n      final int iCopy \u003d i;\n      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n        @Override\n        public Void call() throws IOException {\n          int upperBound \u003d Math.min(iCopy + step,\n              idBasedLayoutSingleLinks.size());\n          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n            HardLink.createHardLink(cur.src, cur.dst);\n          }\n          return null;\n        }\n      }));\n    }\n    linkWorkers.shutdown();\n    for (Future\u003cVoid\u003e f : futures) {\n      try {\n        f.get();\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        throw new IOException(e);\n      } catch (ExecutionException e) {\n        throw new IOException(e);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {}
    },
    "543aac9f281871a40473e83061f6deadc0bbdab7": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14386. Rewind trunk from Guava 21.0 back to Guava 11.0.2.\n",
      "commitDate": "09/05/17 9:22 AM",
      "commitName": "543aac9f281871a40473e83061f6deadc0bbdab7",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "29/03/17 10:56 AM",
      "commitNameOld": "15e3873dc3d46016344887570e5d4aa8d20ca264",
      "commitAuthorOld": "Daniel Templeton",
      "daysBetweenCommits": 40.93,
      "commitsBetweenForRepo": 228,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,61 @@\n   private static void linkBlocks(File from, File to, int oldLV,\n       HardLink hl, Configuration conf) throws IOException {\n     LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n     boolean upgradeToIdBasedLayout \u003d false;\n     // If we are upgrading from a version older than the one where we introduced\n     // block ID-based layout (32x32) AND we\u0027re working with the finalized\n     // directory, we\u0027ll need to upgrade from the old layout to the new one. The\n     // upgrade path from pre-blockid based layouts (\u003e-56) and blockid based\n     // 256x256 layouts (-56) is fortunately the same.\n     if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT_32_by_32\n         .getInfo().getLayoutVersion()\n         \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n       upgradeToIdBasedLayout \u003d true;\n     }\n \n     final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n     linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n         idBasedLayoutSingleLinks);\n \n     // Detect and remove duplicate entries.\n     final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n         findDuplicateEntries(idBasedLayoutSingleLinks);\n     if (!duplicates.isEmpty()) {\n       LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n           \"entries within the same volume.\");\n       removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n     }\n \n     final int numLinkWorkers \u003d conf.getInt(\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n     ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n     final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n     List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n     for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n       final int iCopy \u003d i;\n       futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n         @Override\n         public Void call() throws IOException {\n           int upperBound \u003d Math.min(iCopy + step,\n               idBasedLayoutSingleLinks.size());\n           for (int j \u003d iCopy; j \u003c upperBound; j++) {\n             LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n             HardLink.createHardLink(cur.src, cur.dst);\n           }\n           return null;\n         }\n       }));\n     }\n     linkWorkers.shutdown();\n     for (Future\u003cVoid\u003e f : futures) {\n-      Futures.getChecked(f, IOException.class);\n+      try {\n+        f.get();\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new IOException(e);\n+      } catch (ExecutionException e) {\n+        throw new IOException(e);\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void linkBlocks(File from, File to, int oldLV,\n      HardLink hl, Configuration conf) throws IOException {\n    LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n    boolean upgradeToIdBasedLayout \u003d false;\n    // If we are upgrading from a version older than the one where we introduced\n    // block ID-based layout (32x32) AND we\u0027re working with the finalized\n    // directory, we\u0027ll need to upgrade from the old layout to the new one. The\n    // upgrade path from pre-blockid based layouts (\u003e-56) and blockid based\n    // 256x256 layouts (-56) is fortunately the same.\n    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT_32_by_32\n        .getInfo().getLayoutVersion()\n        \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n      upgradeToIdBasedLayout \u003d true;\n    }\n\n    final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n        idBasedLayoutSingleLinks);\n\n    // Detect and remove duplicate entries.\n    final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n        findDuplicateEntries(idBasedLayoutSingleLinks);\n    if (!duplicates.isEmpty()) {\n      LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n          \"entries within the same volume.\");\n      removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n    }\n\n    final int numLinkWorkers \u003d conf.getInt(\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n      final int iCopy \u003d i;\n      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n        @Override\n        public Void call() throws IOException {\n          int upperBound \u003d Math.min(iCopy + step,\n              idBasedLayoutSingleLinks.size());\n          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n            HardLink.createHardLink(cur.src, cur.dst);\n          }\n          return null;\n        }\n      }));\n    }\n    linkWorkers.shutdown();\n    for (Future\u003cVoid\u003e f : futures) {\n      try {\n        f.get();\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        throw new IOException(e);\n      } catch (ExecutionException e) {\n        throw new IOException(e);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {}
    },
    "84ddedc0b2d58257d45c16ee5e83b15f94a7ba3a": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10101. Update guava dependency to the latest version. (ozawa)\n",
      "commitDate": "24/03/17 6:06 PM",
      "commitName": "84ddedc0b2d58257d45c16ee5e83b15f94a7ba3a",
      "commitAuthor": "Tsuyoshi Ozawa",
      "commitDateOld": "18/01/17 11:38 PM",
      "commitNameOld": "63320d1daab7ce846bb180623378efe431ad8c52",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 64.73,
      "commitsBetweenForRepo": 352,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   private static void linkBlocks(File from, File to, int oldLV,\n       HardLink hl, Configuration conf) throws IOException {\n     LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n     boolean upgradeToIdBasedLayout \u003d false;\n     // If we are upgrading from a version older than the one where we introduced\n     // block ID-based layout (32x32) AND we\u0027re working with the finalized\n     // directory, we\u0027ll need to upgrade from the old layout to the new one. The\n     // upgrade path from pre-blockid based layouts (\u003e-56) and blockid based\n     // 256x256 layouts (-56) is fortunately the same.\n     if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT_32_by_32\n         .getInfo().getLayoutVersion()\n         \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n       upgradeToIdBasedLayout \u003d true;\n     }\n \n     final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n     linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n         idBasedLayoutSingleLinks);\n \n     // Detect and remove duplicate entries.\n     final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n         findDuplicateEntries(idBasedLayoutSingleLinks);\n     if (!duplicates.isEmpty()) {\n       LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n           \"entries within the same volume.\");\n       removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n     }\n \n     final int numLinkWorkers \u003d conf.getInt(\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n     ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n     final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n     List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n     for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n       final int iCopy \u003d i;\n       futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n         @Override\n         public Void call() throws IOException {\n           int upperBound \u003d Math.min(iCopy + step,\n               idBasedLayoutSingleLinks.size());\n           for (int j \u003d iCopy; j \u003c upperBound; j++) {\n             LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n             HardLink.createHardLink(cur.src, cur.dst);\n           }\n           return null;\n         }\n       }));\n     }\n     linkWorkers.shutdown();\n     for (Future\u003cVoid\u003e f : futures) {\n-      Futures.get(f, IOException.class);\n+      Futures.getChecked(f, IOException.class);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void linkBlocks(File from, File to, int oldLV,\n      HardLink hl, Configuration conf) throws IOException {\n    LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n    boolean upgradeToIdBasedLayout \u003d false;\n    // If we are upgrading from a version older than the one where we introduced\n    // block ID-based layout (32x32) AND we\u0027re working with the finalized\n    // directory, we\u0027ll need to upgrade from the old layout to the new one. The\n    // upgrade path from pre-blockid based layouts (\u003e-56) and blockid based\n    // 256x256 layouts (-56) is fortunately the same.\n    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT_32_by_32\n        .getInfo().getLayoutVersion()\n        \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n      upgradeToIdBasedLayout \u003d true;\n    }\n\n    final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n        idBasedLayoutSingleLinks);\n\n    // Detect and remove duplicate entries.\n    final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n        findDuplicateEntries(idBasedLayoutSingleLinks);\n    if (!duplicates.isEmpty()) {\n      LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n          \"entries within the same volume.\");\n      removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n    }\n\n    final int numLinkWorkers \u003d conf.getInt(\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n      final int iCopy \u003d i;\n      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n        @Override\n        public Void call() throws IOException {\n          int upperBound \u003d Math.min(iCopy + step,\n              idBasedLayoutSingleLinks.size());\n          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n            HardLink.createHardLink(cur.src, cur.dst);\n          }\n          return null;\n        }\n      }));\n    }\n    linkWorkers.shutdown();\n    for (Future\u003cVoid\u003e f : futures) {\n      Futures.getChecked(f, IOException.class);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {}
    },
    "2c8496ebf3b7b31c2e18fdf8d4cb2a0115f43112": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8791. block ID-based DN storage layout can be very slow for datanode on ext4. Contributed by Chris Trezzo.\n",
      "commitDate": "01/03/16 1:04 PM",
      "commitName": "2c8496ebf3b7b31c2e18fdf8d4cb2a0115f43112",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "22/02/16 3:01 PM",
      "commitNameOld": "66289a3bf403f307844ea0b6ceed35b603d12c0b",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 7.92,
      "commitsBetweenForRepo": 63,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,54 @@\n   private static void linkBlocks(File from, File to, int oldLV,\n       HardLink hl, Configuration conf) throws IOException {\n     LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n     boolean upgradeToIdBasedLayout \u003d false;\n     // If we are upgrading from a version older than the one where we introduced\n-    // block ID-based layout AND we\u0027re working with the finalized directory,\n-    // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n-    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n-        getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n+    // block ID-based layout (32x32) AND we\u0027re working with the finalized\n+    // directory, we\u0027ll need to upgrade from the old layout to the new one. The\n+    // upgrade path from pre-blockid based layouts (\u003e-56) and blockid based\n+    // 256x256 layouts (-56) is fortunately the same.\n+    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT_32_by_32\n+        .getInfo().getLayoutVersion()\n+        \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n       upgradeToIdBasedLayout \u003d true;\n     }\n \n     final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n     linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n         idBasedLayoutSingleLinks);\n \n     // Detect and remove duplicate entries.\n     final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n         findDuplicateEntries(idBasedLayoutSingleLinks);\n     if (!duplicates.isEmpty()) {\n       LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n           \"entries within the same volume.\");\n       removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n     }\n \n     final int numLinkWorkers \u003d conf.getInt(\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n     ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n     final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n     List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n     for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n       final int iCopy \u003d i;\n       futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n         @Override\n         public Void call() throws IOException {\n           int upperBound \u003d Math.min(iCopy + step,\n               idBasedLayoutSingleLinks.size());\n           for (int j \u003d iCopy; j \u003c upperBound; j++) {\n             LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n             HardLink.createHardLink(cur.src, cur.dst);\n           }\n           return null;\n         }\n       }));\n     }\n     linkWorkers.shutdown();\n     for (Future\u003cVoid\u003e f : futures) {\n       Futures.get(f, IOException.class);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void linkBlocks(File from, File to, int oldLV,\n      HardLink hl, Configuration conf) throws IOException {\n    LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n    boolean upgradeToIdBasedLayout \u003d false;\n    // If we are upgrading from a version older than the one where we introduced\n    // block ID-based layout (32x32) AND we\u0027re working with the finalized\n    // directory, we\u0027ll need to upgrade from the old layout to the new one. The\n    // upgrade path from pre-blockid based layouts (\u003e-56) and blockid based\n    // 256x256 layouts (-56) is fortunately the same.\n    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT_32_by_32\n        .getInfo().getLayoutVersion()\n        \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n      upgradeToIdBasedLayout \u003d true;\n    }\n\n    final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n        idBasedLayoutSingleLinks);\n\n    // Detect and remove duplicate entries.\n    final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n        findDuplicateEntries(idBasedLayoutSingleLinks);\n    if (!duplicates.isEmpty()) {\n      LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n          \"entries within the same volume.\");\n      removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n    }\n\n    final int numLinkWorkers \u003d conf.getInt(\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n      final int iCopy \u003d i;\n      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n        @Override\n        public Void call() throws IOException {\n          int upperBound \u003d Math.min(iCopy + step,\n              idBasedLayoutSingleLinks.size());\n          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n            HardLink.createHardLink(cur.src, cur.dst);\n          }\n          return null;\n        }\n      }));\n    }\n    linkWorkers.shutdown();\n    for (Future\u003cVoid\u003e f : futures) {\n      Futures.get(f, IOException.class);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {}
    },
    "662e17b46a0f41ade6a304e12925b70b5d09fc2f": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-9654. Code refactoring for HDFS-8578.\n",
      "commitDate": "27/01/16 6:58 PM",
      "commitName": "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9654. Code refactoring for HDFS-8578.\n",
          "commitDate": "27/01/16 6:58 PM",
          "commitName": "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "05/11/15 10:00 AM",
          "commitNameOld": "efc73d7896e65a8a03d226ac21309257a7dc126c",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 83.37,
          "commitsBetweenForRepo": 510,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,51 @@\n-  static void linkBlocks(DataNode datanode, File from, File to, int oldLV,\n-      HardLink hl) throws IOException {\n+  private static void linkBlocks(File from, File to, int oldLV,\n+      HardLink hl, Configuration conf) throws IOException {\n+    LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n     boolean upgradeToIdBasedLayout \u003d false;\n     // If we are upgrading from a version older than the one where we introduced\n     // block ID-based layout AND we\u0027re working with the finalized directory,\n     // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n     if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n         getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n       upgradeToIdBasedLayout \u003d true;\n     }\n \n     final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n     linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n         idBasedLayoutSingleLinks);\n \n     // Detect and remove duplicate entries.\n     final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n         findDuplicateEntries(idBasedLayoutSingleLinks);\n     if (!duplicates.isEmpty()) {\n       LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n           \"entries within the same volume.\");\n       removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n     }\n \n-    int numLinkWorkers \u003d datanode.getConf().getInt(\n+    final int numLinkWorkers \u003d conf.getInt(\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n     ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n     final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n     List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n     for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n       final int iCopy \u003d i;\n       futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n         @Override\n         public Void call() throws IOException {\n           int upperBound \u003d Math.min(iCopy + step,\n               idBasedLayoutSingleLinks.size());\n           for (int j \u003d iCopy; j \u003c upperBound; j++) {\n             LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n             HardLink.createHardLink(cur.src, cur.dst);\n           }\n           return null;\n         }\n       }));\n     }\n     linkWorkers.shutdown();\n     for (Future\u003cVoid\u003e f : futures) {\n       Futures.get(f, IOException.class);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void linkBlocks(File from, File to, int oldLV,\n      HardLink hl, Configuration conf) throws IOException {\n    LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n    boolean upgradeToIdBasedLayout \u003d false;\n    // If we are upgrading from a version older than the one where we introduced\n    // block ID-based layout AND we\u0027re working with the finalized directory,\n    // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n        getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n      upgradeToIdBasedLayout \u003d true;\n    }\n\n    final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n        idBasedLayoutSingleLinks);\n\n    // Detect and remove duplicate entries.\n    final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n        findDuplicateEntries(idBasedLayoutSingleLinks);\n    if (!duplicates.isEmpty()) {\n      LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n          \"entries within the same volume.\");\n      removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n    }\n\n    final int numLinkWorkers \u003d conf.getInt(\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n      final int iCopy \u003d i;\n      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n        @Override\n        public Void call() throws IOException {\n          int upperBound \u003d Math.min(iCopy + step,\n              idBasedLayoutSingleLinks.size());\n          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n            HardLink.createHardLink(cur.src, cur.dst);\n          }\n          return null;\n        }\n      }));\n    }\n    linkWorkers.shutdown();\n    for (Future\u003cVoid\u003e f : futures) {\n      Futures.get(f, IOException.class);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
          "extendedDetails": {
            "oldValue": "[datanode-DataNode, from-File, to-File, oldLV-int, hl-HardLink]",
            "newValue": "[from-File, to-File, oldLV-int, hl-HardLink, conf-Configuration]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-9654. Code refactoring for HDFS-8578.\n",
          "commitDate": "27/01/16 6:58 PM",
          "commitName": "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "05/11/15 10:00 AM",
          "commitNameOld": "efc73d7896e65a8a03d226ac21309257a7dc126c",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 83.37,
          "commitsBetweenForRepo": 510,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,51 @@\n-  static void linkBlocks(DataNode datanode, File from, File to, int oldLV,\n-      HardLink hl) throws IOException {\n+  private static void linkBlocks(File from, File to, int oldLV,\n+      HardLink hl, Configuration conf) throws IOException {\n+    LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n     boolean upgradeToIdBasedLayout \u003d false;\n     // If we are upgrading from a version older than the one where we introduced\n     // block ID-based layout AND we\u0027re working with the finalized directory,\n     // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n     if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n         getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n       upgradeToIdBasedLayout \u003d true;\n     }\n \n     final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n     linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n         idBasedLayoutSingleLinks);\n \n     // Detect and remove duplicate entries.\n     final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n         findDuplicateEntries(idBasedLayoutSingleLinks);\n     if (!duplicates.isEmpty()) {\n       LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n           \"entries within the same volume.\");\n       removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n     }\n \n-    int numLinkWorkers \u003d datanode.getConf().getInt(\n+    final int numLinkWorkers \u003d conf.getInt(\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n     ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n     final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n     List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n     for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n       final int iCopy \u003d i;\n       futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n         @Override\n         public Void call() throws IOException {\n           int upperBound \u003d Math.min(iCopy + step,\n               idBasedLayoutSingleLinks.size());\n           for (int j \u003d iCopy; j \u003c upperBound; j++) {\n             LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n             HardLink.createHardLink(cur.src, cur.dst);\n           }\n           return null;\n         }\n       }));\n     }\n     linkWorkers.shutdown();\n     for (Future\u003cVoid\u003e f : futures) {\n       Futures.get(f, IOException.class);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void linkBlocks(File from, File to, int oldLV,\n      HardLink hl, Configuration conf) throws IOException {\n    LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n    boolean upgradeToIdBasedLayout \u003d false;\n    // If we are upgrading from a version older than the one where we introduced\n    // block ID-based layout AND we\u0027re working with the finalized directory,\n    // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n        getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n      upgradeToIdBasedLayout \u003d true;\n    }\n\n    final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n        idBasedLayoutSingleLinks);\n\n    // Detect and remove duplicate entries.\n    final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n        findDuplicateEntries(idBasedLayoutSingleLinks);\n    if (!duplicates.isEmpty()) {\n      LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n          \"entries within the same volume.\");\n      removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n    }\n\n    final int numLinkWorkers \u003d conf.getInt(\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n      final int iCopy \u003d i;\n      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n        @Override\n        public Void call() throws IOException {\n          int upperBound \u003d Math.min(iCopy + step,\n              idBasedLayoutSingleLinks.size());\n          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n            HardLink.createHardLink(cur.src, cur.dst);\n          }\n          return null;\n        }\n      }));\n    }\n    linkWorkers.shutdown();\n    for (Future\u003cVoid\u003e f : futures) {\n      Futures.get(f, IOException.class);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
          "extendedDetails": {
            "oldValue": "[static]",
            "newValue": "[private, static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9654. Code refactoring for HDFS-8578.\n",
          "commitDate": "27/01/16 6:58 PM",
          "commitName": "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "05/11/15 10:00 AM",
          "commitNameOld": "efc73d7896e65a8a03d226ac21309257a7dc126c",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 83.37,
          "commitsBetweenForRepo": 510,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,51 @@\n-  static void linkBlocks(DataNode datanode, File from, File to, int oldLV,\n-      HardLink hl) throws IOException {\n+  private static void linkBlocks(File from, File to, int oldLV,\n+      HardLink hl, Configuration conf) throws IOException {\n+    LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n     boolean upgradeToIdBasedLayout \u003d false;\n     // If we are upgrading from a version older than the one where we introduced\n     // block ID-based layout AND we\u0027re working with the finalized directory,\n     // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n     if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n         getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n       upgradeToIdBasedLayout \u003d true;\n     }\n \n     final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n     linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n         idBasedLayoutSingleLinks);\n \n     // Detect and remove duplicate entries.\n     final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n         findDuplicateEntries(idBasedLayoutSingleLinks);\n     if (!duplicates.isEmpty()) {\n       LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n           \"entries within the same volume.\");\n       removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n     }\n \n-    int numLinkWorkers \u003d datanode.getConf().getInt(\n+    final int numLinkWorkers \u003d conf.getInt(\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n     ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n     final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n     List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n     for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n       final int iCopy \u003d i;\n       futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n         @Override\n         public Void call() throws IOException {\n           int upperBound \u003d Math.min(iCopy + step,\n               idBasedLayoutSingleLinks.size());\n           for (int j \u003d iCopy; j \u003c upperBound; j++) {\n             LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n             HardLink.createHardLink(cur.src, cur.dst);\n           }\n           return null;\n         }\n       }));\n     }\n     linkWorkers.shutdown();\n     for (Future\u003cVoid\u003e f : futures) {\n       Futures.get(f, IOException.class);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void linkBlocks(File from, File to, int oldLV,\n      HardLink hl, Configuration conf) throws IOException {\n    LOG.info(\"Start linking block files from \" + from + \" to \" + to);\n    boolean upgradeToIdBasedLayout \u003d false;\n    // If we are upgrading from a version older than the one where we introduced\n    // block ID-based layout AND we\u0027re working with the finalized directory,\n    // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n        getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n      upgradeToIdBasedLayout \u003d true;\n    }\n\n    final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n        idBasedLayoutSingleLinks);\n\n    // Detect and remove duplicate entries.\n    final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n        findDuplicateEntries(idBasedLayoutSingleLinks);\n    if (!duplicates.isEmpty()) {\n      LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n          \"entries within the same volume.\");\n      removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n    }\n\n    final int numLinkWorkers \u003d conf.getInt(\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n      final int iCopy \u003d i;\n      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n        @Override\n        public Void call() throws IOException {\n          int upperBound \u003d Math.min(iCopy + step,\n              idBasedLayoutSingleLinks.size());\n          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n            HardLink.createHardLink(cur.src, cur.dst);\n          }\n          return null;\n        }\n      }));\n    }\n    linkWorkers.shutdown();\n    for (Future\u003cVoid\u003e f : futures) {\n      Futures.get(f, IOException.class);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
          "extendedDetails": {}
        }
      ]
    },
    "e46cb800028c95f9bce575d05268cd10d0913222": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12055. Deprecate usage of NativeIO#link. Contributed by Andrew Wang.\n",
      "commitDate": "06/06/15 9:17 AM",
      "commitName": "e46cb800028c95f9bce575d05268cd10d0913222",
      "commitAuthor": "cnauroth",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 34.97,
      "commitsBetweenForRepo": 362,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,50 @@\n   static void linkBlocks(DataNode datanode, File from, File to, int oldLV,\n       HardLink hl) throws IOException {\n     boolean upgradeToIdBasedLayout \u003d false;\n     // If we are upgrading from a version older than the one where we introduced\n     // block ID-based layout AND we\u0027re working with the finalized directory,\n     // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n     if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n         getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n       upgradeToIdBasedLayout \u003d true;\n     }\n \n     final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n     linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n         idBasedLayoutSingleLinks);\n \n     // Detect and remove duplicate entries.\n     final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n         findDuplicateEntries(idBasedLayoutSingleLinks);\n     if (!duplicates.isEmpty()) {\n       LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n           \"entries within the same volume.\");\n       removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n     }\n \n     int numLinkWorkers \u003d datanode.getConf().getInt(\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n     ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n     final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n     List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n     for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n       final int iCopy \u003d i;\n       futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n         @Override\n         public Void call() throws IOException {\n           int upperBound \u003d Math.min(iCopy + step,\n               idBasedLayoutSingleLinks.size());\n           for (int j \u003d iCopy; j \u003c upperBound; j++) {\n             LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n-            NativeIO.link(cur.src, cur.dst);\n+            HardLink.createHardLink(cur.src, cur.dst);\n           }\n           return null;\n         }\n       }));\n     }\n     linkWorkers.shutdown();\n     for (Future\u003cVoid\u003e f : futures) {\n       Futures.get(f, IOException.class);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void linkBlocks(DataNode datanode, File from, File to, int oldLV,\n      HardLink hl) throws IOException {\n    boolean upgradeToIdBasedLayout \u003d false;\n    // If we are upgrading from a version older than the one where we introduced\n    // block ID-based layout AND we\u0027re working with the finalized directory,\n    // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n        getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n      upgradeToIdBasedLayout \u003d true;\n    }\n\n    final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n        idBasedLayoutSingleLinks);\n\n    // Detect and remove duplicate entries.\n    final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n        findDuplicateEntries(idBasedLayoutSingleLinks);\n    if (!duplicates.isEmpty()) {\n      LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n          \"entries within the same volume.\");\n      removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n    }\n\n    int numLinkWorkers \u003d datanode.getConf().getInt(\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n      final int iCopy \u003d i;\n      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n        @Override\n        public Void call() throws IOException {\n          int upperBound \u003d Math.min(iCopy + step,\n              idBasedLayoutSingleLinks.size());\n          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n            HardLink.createHardLink(cur.src, cur.dst);\n          }\n          return null;\n        }\n      }));\n    }\n    linkWorkers.shutdown();\n    for (Future\u003cVoid\u003e f : futures) {\n      Futures.get(f, IOException.class);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {}
    },
    "8fa265a290792ff42635ff9b42416c634f88bdf3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7443. Datanode upgrade to BLOCKID_BASED_LAYOUT fails if duplicate block files are present in the same volume (cmccabe)\n",
      "commitDate": "19/12/14 1:18 PM",
      "commitName": "8fa265a290792ff42635ff9b42416c634f88bdf3",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "11/12/14 12:36 PM",
      "commitNameOld": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 8.03,
      "commitsBetweenForRepo": 64,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,50 @@\n   static void linkBlocks(DataNode datanode, File from, File to, int oldLV,\n       HardLink hl) throws IOException {\n     boolean upgradeToIdBasedLayout \u003d false;\n     // If we are upgrading from a version older than the one where we introduced\n     // block ID-based layout AND we\u0027re working with the finalized directory,\n     // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n     if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n         getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n       upgradeToIdBasedLayout \u003d true;\n     }\n \n-    final List\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n+    final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n     linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n         idBasedLayoutSingleLinks);\n+\n+    // Detect and remove duplicate entries.\n+    final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n+        findDuplicateEntries(idBasedLayoutSingleLinks);\n+    if (!duplicates.isEmpty()) {\n+      LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n+          \"entries within the same volume.\");\n+      removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n+    }\n+\n     int numLinkWorkers \u003d datanode.getConf().getInt(\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n     ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n     final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n     List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n     for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n       final int iCopy \u003d i;\n       futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n         @Override\n         public Void call() throws IOException {\n           int upperBound \u003d Math.min(iCopy + step,\n               idBasedLayoutSingleLinks.size());\n           for (int j \u003d iCopy; j \u003c upperBound; j++) {\n             LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n             NativeIO.link(cur.src, cur.dst);\n           }\n           return null;\n         }\n       }));\n     }\n     linkWorkers.shutdown();\n     for (Future\u003cVoid\u003e f : futures) {\n       Futures.get(f, IOException.class);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void linkBlocks(DataNode datanode, File from, File to, int oldLV,\n      HardLink hl) throws IOException {\n    boolean upgradeToIdBasedLayout \u003d false;\n    // If we are upgrading from a version older than the one where we introduced\n    // block ID-based layout AND we\u0027re working with the finalized directory,\n    // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n        getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n      upgradeToIdBasedLayout \u003d true;\n    }\n\n    final ArrayList\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n        idBasedLayoutSingleLinks);\n\n    // Detect and remove duplicate entries.\n    final ArrayList\u003cLinkArgs\u003e duplicates \u003d\n        findDuplicateEntries(idBasedLayoutSingleLinks);\n    if (!duplicates.isEmpty()) {\n      LOG.error(\"There are \" + duplicates.size() + \" duplicate block \" +\n          \"entries within the same volume.\");\n      removeDuplicateEntries(idBasedLayoutSingleLinks, duplicates);\n    }\n\n    int numLinkWorkers \u003d datanode.getConf().getInt(\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n      final int iCopy \u003d i;\n      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n        @Override\n        public Void call() throws IOException {\n          int upperBound \u003d Math.min(iCopy + step,\n              idBasedLayoutSingleLinks.size());\n          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n            NativeIO.link(cur.src, cur.dst);\n          }\n          return null;\n        }\n      }));\n    }\n    linkWorkers.shutdown();\n    for (Future\u003cVoid\u003e f : futures) {\n      Futures.get(f, IOException.class);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {}
    },
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6482. Use block ID-based block layout on datanodes (James Thomas via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615223 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/14 1:41 PM",
      "commitName": "1ba3f8971433cdbc3e43fd3605065d811dab5b16",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6482. Use block ID-based block layout on datanodes (James Thomas via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615223 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "01/08/14 1:41 PM",
          "commitName": "1ba3f8971433cdbc3e43fd3605065d811dab5b16",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "29/04/14 3:27 AM",
          "commitNameOld": "9d21180c1a625295bb9da0d9d5d8c55740944008",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 94.43,
          "commitsBetweenForRepo": 594,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,40 @@\n-  static void linkBlocks(File from, File to, int oldLV, HardLink hl) \n-  throws IOException {\n-    if (!from.exists()) {\n-      return;\n+  static void linkBlocks(DataNode datanode, File from, File to, int oldLV,\n+      HardLink hl) throws IOException {\n+    boolean upgradeToIdBasedLayout \u003d false;\n+    // If we are upgrading from a version older than the one where we introduced\n+    // block ID-based layout AND we\u0027re working with the finalized directory,\n+    // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n+    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n+        getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n+      upgradeToIdBasedLayout \u003d true;\n     }\n-    if (!from.isDirectory()) {\n-      if (from.getName().startsWith(COPY_FILE_PREFIX)) {\n-        FileInputStream in \u003d new FileInputStream(from);\n-        try {\n-          FileOutputStream out \u003d new FileOutputStream(to);\n-          try {\n-            IOUtils.copyBytes(in, out, 16*1024);\n-            hl.linkStats.countPhysicalFileCopies++;\n-          } finally {\n-            out.close();\n-          }\n-        } finally {\n-          in.close();\n-        }\n-      } else {\n-        HardLink.createHardLink(from, to);\n-        hl.linkStats.countSingleLinks++;\n-      }\n-      return;\n-    }\n-    // from is a directory\n-    hl.linkStats.countDirs++;\n-    \n-    if (!to.mkdirs())\n-      throw new IOException(\"Cannot create directory \" + to);\n-    \n-    String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n-      @Override\n-      public boolean accept(File dir, String name) {\n-        return name.startsWith(BLOCK_FILE_PREFIX);\n-      }\n-    });\n \n-    // Block files just need hard links with the same file names\n-    // but a different directory\n-    if (blockNames.length \u003e 0) {\n-      HardLink.createHardLinkMult(from, blockNames, to);\n-      hl.linkStats.countMultLinks++;\n-      hl.linkStats.countFilesMultLinks +\u003d blockNames.length;\n-    } else {\n-      hl.linkStats.countEmptyDirs++;\n-    }\n-    \n-    // Now take care of the rest of the files and subdirectories\n-    String[] otherNames \u003d from.list(new java.io.FilenameFilter() {\n+    final List\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n+    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n+        idBasedLayoutSingleLinks);\n+    int numLinkWorkers \u003d datanode.getConf().getInt(\n+        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n+        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n+    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n+    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n+    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n+    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n+      final int iCopy \u003d i;\n+      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n         @Override\n-        public boolean accept(File dir, String name) {\n-          return name.startsWith(BLOCK_SUBDIR_PREFIX) \n-            || name.startsWith(COPY_FILE_PREFIX);\n+        public Void call() throws IOException {\n+          int upperBound \u003d Math.min(iCopy + step,\n+              idBasedLayoutSingleLinks.size());\n+          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n+            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n+            NativeIO.link(cur.src, cur.dst);\n+          }\n+          return null;\n         }\n-      });\n-    for(int i \u003d 0; i \u003c otherNames.length; i++)\n-      linkBlocks(new File(from, otherNames[i]), \n-          new File(to, otherNames[i]), oldLV, hl);\n+      }));\n+    }\n+    linkWorkers.shutdown();\n+    for (Future\u003cVoid\u003e f : futures) {\n+      Futures.get(f, IOException.class);\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static void linkBlocks(DataNode datanode, File from, File to, int oldLV,\n      HardLink hl) throws IOException {\n    boolean upgradeToIdBasedLayout \u003d false;\n    // If we are upgrading from a version older than the one where we introduced\n    // block ID-based layout AND we\u0027re working with the finalized directory,\n    // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n        getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n      upgradeToIdBasedLayout \u003d true;\n    }\n\n    final List\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n        idBasedLayoutSingleLinks);\n    int numLinkWorkers \u003d datanode.getConf().getInt(\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n      final int iCopy \u003d i;\n      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n        @Override\n        public Void call() throws IOException {\n          int upperBound \u003d Math.min(iCopy + step,\n              idBasedLayoutSingleLinks.size());\n          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n            NativeIO.link(cur.src, cur.dst);\n          }\n          return null;\n        }\n      }));\n    }\n    linkWorkers.shutdown();\n    for (Future\u003cVoid\u003e f : futures) {\n      Futures.get(f, IOException.class);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
          "extendedDetails": {
            "oldValue": "[from-File, to-File, oldLV-int, hl-HardLink]",
            "newValue": "[datanode-DataNode, from-File, to-File, oldLV-int, hl-HardLink]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6482. Use block ID-based block layout on datanodes (James Thomas via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615223 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "01/08/14 1:41 PM",
          "commitName": "1ba3f8971433cdbc3e43fd3605065d811dab5b16",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "29/04/14 3:27 AM",
          "commitNameOld": "9d21180c1a625295bb9da0d9d5d8c55740944008",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 94.43,
          "commitsBetweenForRepo": 594,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,40 @@\n-  static void linkBlocks(File from, File to, int oldLV, HardLink hl) \n-  throws IOException {\n-    if (!from.exists()) {\n-      return;\n+  static void linkBlocks(DataNode datanode, File from, File to, int oldLV,\n+      HardLink hl) throws IOException {\n+    boolean upgradeToIdBasedLayout \u003d false;\n+    // If we are upgrading from a version older than the one where we introduced\n+    // block ID-based layout AND we\u0027re working with the finalized directory,\n+    // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n+    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n+        getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n+      upgradeToIdBasedLayout \u003d true;\n     }\n-    if (!from.isDirectory()) {\n-      if (from.getName().startsWith(COPY_FILE_PREFIX)) {\n-        FileInputStream in \u003d new FileInputStream(from);\n-        try {\n-          FileOutputStream out \u003d new FileOutputStream(to);\n-          try {\n-            IOUtils.copyBytes(in, out, 16*1024);\n-            hl.linkStats.countPhysicalFileCopies++;\n-          } finally {\n-            out.close();\n-          }\n-        } finally {\n-          in.close();\n-        }\n-      } else {\n-        HardLink.createHardLink(from, to);\n-        hl.linkStats.countSingleLinks++;\n-      }\n-      return;\n-    }\n-    // from is a directory\n-    hl.linkStats.countDirs++;\n-    \n-    if (!to.mkdirs())\n-      throw new IOException(\"Cannot create directory \" + to);\n-    \n-    String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n-      @Override\n-      public boolean accept(File dir, String name) {\n-        return name.startsWith(BLOCK_FILE_PREFIX);\n-      }\n-    });\n \n-    // Block files just need hard links with the same file names\n-    // but a different directory\n-    if (blockNames.length \u003e 0) {\n-      HardLink.createHardLinkMult(from, blockNames, to);\n-      hl.linkStats.countMultLinks++;\n-      hl.linkStats.countFilesMultLinks +\u003d blockNames.length;\n-    } else {\n-      hl.linkStats.countEmptyDirs++;\n-    }\n-    \n-    // Now take care of the rest of the files and subdirectories\n-    String[] otherNames \u003d from.list(new java.io.FilenameFilter() {\n+    final List\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n+    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n+        idBasedLayoutSingleLinks);\n+    int numLinkWorkers \u003d datanode.getConf().getInt(\n+        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n+        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n+    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n+    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n+    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n+    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n+      final int iCopy \u003d i;\n+      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n         @Override\n-        public boolean accept(File dir, String name) {\n-          return name.startsWith(BLOCK_SUBDIR_PREFIX) \n-            || name.startsWith(COPY_FILE_PREFIX);\n+        public Void call() throws IOException {\n+          int upperBound \u003d Math.min(iCopy + step,\n+              idBasedLayoutSingleLinks.size());\n+          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n+            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n+            NativeIO.link(cur.src, cur.dst);\n+          }\n+          return null;\n         }\n-      });\n-    for(int i \u003d 0; i \u003c otherNames.length; i++)\n-      linkBlocks(new File(from, otherNames[i]), \n-          new File(to, otherNames[i]), oldLV, hl);\n+      }));\n+    }\n+    linkWorkers.shutdown();\n+    for (Future\u003cVoid\u003e f : futures) {\n+      Futures.get(f, IOException.class);\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static void linkBlocks(DataNode datanode, File from, File to, int oldLV,\n      HardLink hl) throws IOException {\n    boolean upgradeToIdBasedLayout \u003d false;\n    // If we are upgrading from a version older than the one where we introduced\n    // block ID-based layout AND we\u0027re working with the finalized directory,\n    // we\u0027ll need to upgrade from the old flat layout to the block ID-based one\n    if (oldLV \u003e DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT.getInfo().\n        getLayoutVersion() \u0026\u0026 to.getName().equals(STORAGE_DIR_FINALIZED)) {\n      upgradeToIdBasedLayout \u003d true;\n    }\n\n    final List\u003cLinkArgs\u003e idBasedLayoutSingleLinks \u003d Lists.newArrayList();\n    linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to,\n        idBasedLayoutSingleLinks);\n    int numLinkWorkers \u003d datanode.getConf().getInt(\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,\n        DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);\n    ExecutorService linkWorkers \u003d Executors.newFixedThreadPool(numLinkWorkers);\n    final int step \u003d idBasedLayoutSingleLinks.size() / numLinkWorkers + 1;\n    List\u003cFuture\u003cVoid\u003e\u003e futures \u003d Lists.newArrayList();\n    for (int i \u003d 0; i \u003c idBasedLayoutSingleLinks.size(); i +\u003d step) {\n      final int iCopy \u003d i;\n      futures.add(linkWorkers.submit(new Callable\u003cVoid\u003e() {\n        @Override\n        public Void call() throws IOException {\n          int upperBound \u003d Math.min(iCopy + step,\n              idBasedLayoutSingleLinks.size());\n          for (int j \u003d iCopy; j \u003c upperBound; j++) {\n            LinkArgs cur \u003d idBasedLayoutSingleLinks.get(j);\n            NativeIO.link(cur.src, cur.dst);\n          }\n          return null;\n        }\n      }));\n    }\n    linkWorkers.shutdown();\n    for (Future\u003cVoid\u003e f : futures) {\n      Futures.get(f, IOException.class);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
          "extendedDetails": {}
        }
      ]
    },
    "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3659. Add missing @Override to methods across the hadoop-hdfs project. Contributed by Brandon Li. (harsh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1361894 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/07/12 7:58 PM",
      "commitName": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
      "commitAuthor": "Harsh J",
      "commitDateOld": "02/04/12 10:38 AM",
      "commitNameOld": "bc13dfb1426944ce45293cb8f444239a7406762c",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 104.39,
      "commitsBetweenForRepo": 651,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,60 @@\n   static void linkBlocks(File from, File to, int oldLV, HardLink hl) \n   throws IOException {\n     if (!from.exists()) {\n       return;\n     }\n     if (!from.isDirectory()) {\n       if (from.getName().startsWith(COPY_FILE_PREFIX)) {\n         FileInputStream in \u003d new FileInputStream(from);\n         try {\n           FileOutputStream out \u003d new FileOutputStream(to);\n           try {\n             IOUtils.copyBytes(in, out, 16*1024);\n             hl.linkStats.countPhysicalFileCopies++;\n           } finally {\n             out.close();\n           }\n         } finally {\n           in.close();\n         }\n       } else {\n         HardLink.createHardLink(from, to);\n         hl.linkStats.countSingleLinks++;\n       }\n       return;\n     }\n     // from is a directory\n     hl.linkStats.countDirs++;\n     \n     if (!to.mkdirs())\n       throw new IOException(\"Cannot create directory \" + to);\n     \n     String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n+      @Override\n       public boolean accept(File dir, String name) {\n         return name.startsWith(BLOCK_FILE_PREFIX);\n       }\n     });\n \n     // Block files just need hard links with the same file names\n     // but a different directory\n     if (blockNames.length \u003e 0) {\n       HardLink.createHardLinkMult(from, blockNames, to);\n       hl.linkStats.countMultLinks++;\n       hl.linkStats.countFilesMultLinks +\u003d blockNames.length;\n     } else {\n       hl.linkStats.countEmptyDirs++;\n     }\n     \n     // Now take care of the rest of the files and subdirectories\n     String[] otherNames \u003d from.list(new java.io.FilenameFilter() {\n+        @Override\n         public boolean accept(File dir, String name) {\n           return name.startsWith(BLOCK_SUBDIR_PREFIX) \n             || name.startsWith(COPY_FILE_PREFIX);\n         }\n       });\n     for(int i \u003d 0; i \u003c otherNames.length; i++)\n       linkBlocks(new File(from, otherNames[i]), \n           new File(to, otherNames[i]), oldLV, hl);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void linkBlocks(File from, File to, int oldLV, HardLink hl) \n  throws IOException {\n    if (!from.exists()) {\n      return;\n    }\n    if (!from.isDirectory()) {\n      if (from.getName().startsWith(COPY_FILE_PREFIX)) {\n        FileInputStream in \u003d new FileInputStream(from);\n        try {\n          FileOutputStream out \u003d new FileOutputStream(to);\n          try {\n            IOUtils.copyBytes(in, out, 16*1024);\n            hl.linkStats.countPhysicalFileCopies++;\n          } finally {\n            out.close();\n          }\n        } finally {\n          in.close();\n        }\n      } else {\n        HardLink.createHardLink(from, to);\n        hl.linkStats.countSingleLinks++;\n      }\n      return;\n    }\n    // from is a directory\n    hl.linkStats.countDirs++;\n    \n    if (!to.mkdirs())\n      throw new IOException(\"Cannot create directory \" + to);\n    \n    String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n      @Override\n      public boolean accept(File dir, String name) {\n        return name.startsWith(BLOCK_FILE_PREFIX);\n      }\n    });\n\n    // Block files just need hard links with the same file names\n    // but a different directory\n    if (blockNames.length \u003e 0) {\n      HardLink.createHardLinkMult(from, blockNames, to);\n      hl.linkStats.countMultLinks++;\n      hl.linkStats.countFilesMultLinks +\u003d blockNames.length;\n    } else {\n      hl.linkStats.countEmptyDirs++;\n    }\n    \n    // Now take care of the rest of the files and subdirectories\n    String[] otherNames \u003d from.list(new java.io.FilenameFilter() {\n        @Override\n        public boolean accept(File dir, String name) {\n          return name.startsWith(BLOCK_SUBDIR_PREFIX) \n            || name.startsWith(COPY_FILE_PREFIX);\n        }\n      });\n    for(int i \u003d 0; i \u003c otherNames.length; i++)\n      linkBlocks(new File(from, otherNames[i]), \n          new File(to, otherNames[i]), oldLV, hl);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {}
    },
    "64641c28b5ea8538033060452b0c45b7f2eeb60c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3137. Bump LAST_UPGRADABLE_LAYOUT_VERSION to -16. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1307173 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/03/12 5:11 PM",
      "commitName": "64641c28b5ea8538033060452b0c45b7f2eeb60c",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "28/03/12 1:37 PM",
      "commitNameOld": "99a68a14237b4cd1936ba5e9468d25d35dad594c",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.15,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,58 @@\n   static void linkBlocks(File from, File to, int oldLV, HardLink hl) \n   throws IOException {\n     if (!from.exists()) {\n       return;\n     }\n     if (!from.isDirectory()) {\n       if (from.getName().startsWith(COPY_FILE_PREFIX)) {\n         FileInputStream in \u003d new FileInputStream(from);\n         try {\n           FileOutputStream out \u003d new FileOutputStream(to);\n           try {\n             IOUtils.copyBytes(in, out, 16*1024);\n             hl.linkStats.countPhysicalFileCopies++;\n           } finally {\n             out.close();\n           }\n         } finally {\n           in.close();\n         }\n       } else {\n-        \n-        //check if we are upgrading from pre-generation stamp version.\n-        if (oldLV \u003e\u003d PRE_GENERATIONSTAMP_LAYOUT_VERSION) {\n-          // Link to the new file name.\n-          to \u003d new File(convertMetatadataFileName(to.getAbsolutePath()));\n-        }\n-        \n         HardLink.createHardLink(from, to);\n         hl.linkStats.countSingleLinks++;\n       }\n       return;\n     }\n     // from is a directory\n     hl.linkStats.countDirs++;\n     \n     if (!to.mkdirs())\n       throw new IOException(\"Cannot create directory \" + to);\n     \n-    //If upgrading from old stuff, need to munge the filenames.  That has to\n-    //be done one file at a time, so hardlink them one at a time (slow).\n-    if (oldLV \u003e\u003d PRE_GENERATIONSTAMP_LAYOUT_VERSION) {\n-      String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n-          public boolean accept(File dir, String name) {\n-            return name.startsWith(BLOCK_SUBDIR_PREFIX) \n-              || name.startsWith(BLOCK_FILE_PREFIX)\n-              || name.startsWith(COPY_FILE_PREFIX);\n-          }\n-        });\n-      if (blockNames.length \u003d\u003d 0) {\n-        hl.linkStats.countEmptyDirs++;\n+    String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n+      public boolean accept(File dir, String name) {\n+        return name.startsWith(BLOCK_FILE_PREFIX);\n       }\n-      else for(int i \u003d 0; i \u003c blockNames.length; i++)\n-        linkBlocks(new File(from, blockNames[i]), \n-            new File(to, blockNames[i]), oldLV, hl);\n-    } \n-    else {\n-      //If upgrading from a relatively new version, we only need to create\n-      //links with the same filename.  This can be done in bulk (much faster).\n-      String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n+    });\n+\n+    // Block files just need hard links with the same file names\n+    // but a different directory\n+    if (blockNames.length \u003e 0) {\n+      HardLink.createHardLinkMult(from, blockNames, to);\n+      hl.linkStats.countMultLinks++;\n+      hl.linkStats.countFilesMultLinks +\u003d blockNames.length;\n+    } else {\n+      hl.linkStats.countEmptyDirs++;\n+    }\n+    \n+    // Now take care of the rest of the files and subdirectories\n+    String[] otherNames \u003d from.list(new java.io.FilenameFilter() {\n         public boolean accept(File dir, String name) {\n-          return name.startsWith(BLOCK_FILE_PREFIX);\n+          return name.startsWith(BLOCK_SUBDIR_PREFIX) \n+            || name.startsWith(COPY_FILE_PREFIX);\n         }\n       });\n-      if (blockNames.length \u003e 0) {\n-        HardLink.createHardLinkMult(from, blockNames, to);\n-        hl.linkStats.countMultLinks++;\n-        hl.linkStats.countFilesMultLinks +\u003d blockNames.length;\n-      } else {\n-        hl.linkStats.countEmptyDirs++;\n-      }\n-      \n-      //now take care of the rest of the files and subdirectories\n-      String[] otherNames \u003d from.list(new java.io.FilenameFilter() {\n-          public boolean accept(File dir, String name) {\n-            return name.startsWith(BLOCK_SUBDIR_PREFIX) \n-              || name.startsWith(COPY_FILE_PREFIX);\n-          }\n-        });\n-      for(int i \u003d 0; i \u003c otherNames.length; i++)\n-        linkBlocks(new File(from, otherNames[i]), \n-            new File(to, otherNames[i]), oldLV, hl);\n-    }\n+    for(int i \u003d 0; i \u003c otherNames.length; i++)\n+      linkBlocks(new File(from, otherNames[i]), \n+          new File(to, otherNames[i]), oldLV, hl);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void linkBlocks(File from, File to, int oldLV, HardLink hl) \n  throws IOException {\n    if (!from.exists()) {\n      return;\n    }\n    if (!from.isDirectory()) {\n      if (from.getName().startsWith(COPY_FILE_PREFIX)) {\n        FileInputStream in \u003d new FileInputStream(from);\n        try {\n          FileOutputStream out \u003d new FileOutputStream(to);\n          try {\n            IOUtils.copyBytes(in, out, 16*1024);\n            hl.linkStats.countPhysicalFileCopies++;\n          } finally {\n            out.close();\n          }\n        } finally {\n          in.close();\n        }\n      } else {\n        HardLink.createHardLink(from, to);\n        hl.linkStats.countSingleLinks++;\n      }\n      return;\n    }\n    // from is a directory\n    hl.linkStats.countDirs++;\n    \n    if (!to.mkdirs())\n      throw new IOException(\"Cannot create directory \" + to);\n    \n    String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n      public boolean accept(File dir, String name) {\n        return name.startsWith(BLOCK_FILE_PREFIX);\n      }\n    });\n\n    // Block files just need hard links with the same file names\n    // but a different directory\n    if (blockNames.length \u003e 0) {\n      HardLink.createHardLinkMult(from, blockNames, to);\n      hl.linkStats.countMultLinks++;\n      hl.linkStats.countFilesMultLinks +\u003d blockNames.length;\n    } else {\n      hl.linkStats.countEmptyDirs++;\n    }\n    \n    // Now take care of the rest of the files and subdirectories\n    String[] otherNames \u003d from.list(new java.io.FilenameFilter() {\n        public boolean accept(File dir, String name) {\n          return name.startsWith(BLOCK_SUBDIR_PREFIX) \n            || name.startsWith(COPY_FILE_PREFIX);\n        }\n      });\n    for(int i \u003d 0; i \u003c otherNames.length; i++)\n      linkBlocks(new File(from, otherNames[i]), \n          new File(to, otherNames[i]), oldLV, hl);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  static void linkBlocks(File from, File to, int oldLV, HardLink hl) \n  throws IOException {\n    if (!from.exists()) {\n      return;\n    }\n    if (!from.isDirectory()) {\n      if (from.getName().startsWith(COPY_FILE_PREFIX)) {\n        FileInputStream in \u003d new FileInputStream(from);\n        try {\n          FileOutputStream out \u003d new FileOutputStream(to);\n          try {\n            IOUtils.copyBytes(in, out, 16*1024);\n            hl.linkStats.countPhysicalFileCopies++;\n          } finally {\n            out.close();\n          }\n        } finally {\n          in.close();\n        }\n      } else {\n        \n        //check if we are upgrading from pre-generation stamp version.\n        if (oldLV \u003e\u003d PRE_GENERATIONSTAMP_LAYOUT_VERSION) {\n          // Link to the new file name.\n          to \u003d new File(convertMetatadataFileName(to.getAbsolutePath()));\n        }\n        \n        HardLink.createHardLink(from, to);\n        hl.linkStats.countSingleLinks++;\n      }\n      return;\n    }\n    // from is a directory\n    hl.linkStats.countDirs++;\n    \n    if (!to.mkdirs())\n      throw new IOException(\"Cannot create directory \" + to);\n    \n    //If upgrading from old stuff, need to munge the filenames.  That has to\n    //be done one file at a time, so hardlink them one at a time (slow).\n    if (oldLV \u003e\u003d PRE_GENERATIONSTAMP_LAYOUT_VERSION) {\n      String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n          public boolean accept(File dir, String name) {\n            return name.startsWith(BLOCK_SUBDIR_PREFIX) \n              || name.startsWith(BLOCK_FILE_PREFIX)\n              || name.startsWith(COPY_FILE_PREFIX);\n          }\n        });\n      if (blockNames.length \u003d\u003d 0) {\n        hl.linkStats.countEmptyDirs++;\n      }\n      else for(int i \u003d 0; i \u003c blockNames.length; i++)\n        linkBlocks(new File(from, blockNames[i]), \n            new File(to, blockNames[i]), oldLV, hl);\n    } \n    else {\n      //If upgrading from a relatively new version, we only need to create\n      //links with the same filename.  This can be done in bulk (much faster).\n      String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n        public boolean accept(File dir, String name) {\n          return name.startsWith(BLOCK_FILE_PREFIX);\n        }\n      });\n      if (blockNames.length \u003e 0) {\n        HardLink.createHardLinkMult(from, blockNames, to);\n        hl.linkStats.countMultLinks++;\n        hl.linkStats.countFilesMultLinks +\u003d blockNames.length;\n      } else {\n        hl.linkStats.countEmptyDirs++;\n      }\n      \n      //now take care of the rest of the files and subdirectories\n      String[] otherNames \u003d from.list(new java.io.FilenameFilter() {\n          public boolean accept(File dir, String name) {\n            return name.startsWith(BLOCK_SUBDIR_PREFIX) \n              || name.startsWith(COPY_FILE_PREFIX);\n          }\n        });\n      for(int i \u003d 0; i \u003c otherNames.length; i++)\n        linkBlocks(new File(from, otherNames[i]), \n            new File(to, otherNames[i]), oldLV, hl);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  static void linkBlocks(File from, File to, int oldLV, HardLink hl) \n  throws IOException {\n    if (!from.exists()) {\n      return;\n    }\n    if (!from.isDirectory()) {\n      if (from.getName().startsWith(COPY_FILE_PREFIX)) {\n        FileInputStream in \u003d new FileInputStream(from);\n        try {\n          FileOutputStream out \u003d new FileOutputStream(to);\n          try {\n            IOUtils.copyBytes(in, out, 16*1024);\n            hl.linkStats.countPhysicalFileCopies++;\n          } finally {\n            out.close();\n          }\n        } finally {\n          in.close();\n        }\n      } else {\n        \n        //check if we are upgrading from pre-generation stamp version.\n        if (oldLV \u003e\u003d PRE_GENERATIONSTAMP_LAYOUT_VERSION) {\n          // Link to the new file name.\n          to \u003d new File(convertMetatadataFileName(to.getAbsolutePath()));\n        }\n        \n        HardLink.createHardLink(from, to);\n        hl.linkStats.countSingleLinks++;\n      }\n      return;\n    }\n    // from is a directory\n    hl.linkStats.countDirs++;\n    \n    if (!to.mkdirs())\n      throw new IOException(\"Cannot create directory \" + to);\n    \n    //If upgrading from old stuff, need to munge the filenames.  That has to\n    //be done one file at a time, so hardlink them one at a time (slow).\n    if (oldLV \u003e\u003d PRE_GENERATIONSTAMP_LAYOUT_VERSION) {\n      String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n          public boolean accept(File dir, String name) {\n            return name.startsWith(BLOCK_SUBDIR_PREFIX) \n              || name.startsWith(BLOCK_FILE_PREFIX)\n              || name.startsWith(COPY_FILE_PREFIX);\n          }\n        });\n      if (blockNames.length \u003d\u003d 0) {\n        hl.linkStats.countEmptyDirs++;\n      }\n      else for(int i \u003d 0; i \u003c blockNames.length; i++)\n        linkBlocks(new File(from, blockNames[i]), \n            new File(to, blockNames[i]), oldLV, hl);\n    } \n    else {\n      //If upgrading from a relatively new version, we only need to create\n      //links with the same filename.  This can be done in bulk (much faster).\n      String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n        public boolean accept(File dir, String name) {\n          return name.startsWith(BLOCK_FILE_PREFIX);\n        }\n      });\n      if (blockNames.length \u003e 0) {\n        HardLink.createHardLinkMult(from, blockNames, to);\n        hl.linkStats.countMultLinks++;\n        hl.linkStats.countFilesMultLinks +\u003d blockNames.length;\n      } else {\n        hl.linkStats.countEmptyDirs++;\n      }\n      \n      //now take care of the rest of the files and subdirectories\n      String[] otherNames \u003d from.list(new java.io.FilenameFilter() {\n          public boolean accept(File dir, String name) {\n            return name.startsWith(BLOCK_SUBDIR_PREFIX) \n              || name.startsWith(COPY_FILE_PREFIX);\n          }\n        });\n      for(int i \u003d 0; i \u003c otherNames.length; i++)\n        linkBlocks(new File(from, otherNames[i]), \n            new File(to, otherNames[i]), oldLV, hl);\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,83 @@\n+  static void linkBlocks(File from, File to, int oldLV, HardLink hl) \n+  throws IOException {\n+    if (!from.exists()) {\n+      return;\n+    }\n+    if (!from.isDirectory()) {\n+      if (from.getName().startsWith(COPY_FILE_PREFIX)) {\n+        FileInputStream in \u003d new FileInputStream(from);\n+        try {\n+          FileOutputStream out \u003d new FileOutputStream(to);\n+          try {\n+            IOUtils.copyBytes(in, out, 16*1024);\n+            hl.linkStats.countPhysicalFileCopies++;\n+          } finally {\n+            out.close();\n+          }\n+        } finally {\n+          in.close();\n+        }\n+      } else {\n+        \n+        //check if we are upgrading from pre-generation stamp version.\n+        if (oldLV \u003e\u003d PRE_GENERATIONSTAMP_LAYOUT_VERSION) {\n+          // Link to the new file name.\n+          to \u003d new File(convertMetatadataFileName(to.getAbsolutePath()));\n+        }\n+        \n+        HardLink.createHardLink(from, to);\n+        hl.linkStats.countSingleLinks++;\n+      }\n+      return;\n+    }\n+    // from is a directory\n+    hl.linkStats.countDirs++;\n+    \n+    if (!to.mkdirs())\n+      throw new IOException(\"Cannot create directory \" + to);\n+    \n+    //If upgrading from old stuff, need to munge the filenames.  That has to\n+    //be done one file at a time, so hardlink them one at a time (slow).\n+    if (oldLV \u003e\u003d PRE_GENERATIONSTAMP_LAYOUT_VERSION) {\n+      String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n+          public boolean accept(File dir, String name) {\n+            return name.startsWith(BLOCK_SUBDIR_PREFIX) \n+              || name.startsWith(BLOCK_FILE_PREFIX)\n+              || name.startsWith(COPY_FILE_PREFIX);\n+          }\n+        });\n+      if (blockNames.length \u003d\u003d 0) {\n+        hl.linkStats.countEmptyDirs++;\n+      }\n+      else for(int i \u003d 0; i \u003c blockNames.length; i++)\n+        linkBlocks(new File(from, blockNames[i]), \n+            new File(to, blockNames[i]), oldLV, hl);\n+    } \n+    else {\n+      //If upgrading from a relatively new version, we only need to create\n+      //links with the same filename.  This can be done in bulk (much faster).\n+      String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n+        public boolean accept(File dir, String name) {\n+          return name.startsWith(BLOCK_FILE_PREFIX);\n+        }\n+      });\n+      if (blockNames.length \u003e 0) {\n+        HardLink.createHardLinkMult(from, blockNames, to);\n+        hl.linkStats.countMultLinks++;\n+        hl.linkStats.countFilesMultLinks +\u003d blockNames.length;\n+      } else {\n+        hl.linkStats.countEmptyDirs++;\n+      }\n+      \n+      //now take care of the rest of the files and subdirectories\n+      String[] otherNames \u003d from.list(new java.io.FilenameFilter() {\n+          public boolean accept(File dir, String name) {\n+            return name.startsWith(BLOCK_SUBDIR_PREFIX) \n+              || name.startsWith(COPY_FILE_PREFIX);\n+          }\n+        });\n+      for(int i \u003d 0; i \u003c otherNames.length; i++)\n+        linkBlocks(new File(from, otherNames[i]), \n+            new File(to, otherNames[i]), oldLV, hl);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  static void linkBlocks(File from, File to, int oldLV, HardLink hl) \n  throws IOException {\n    if (!from.exists()) {\n      return;\n    }\n    if (!from.isDirectory()) {\n      if (from.getName().startsWith(COPY_FILE_PREFIX)) {\n        FileInputStream in \u003d new FileInputStream(from);\n        try {\n          FileOutputStream out \u003d new FileOutputStream(to);\n          try {\n            IOUtils.copyBytes(in, out, 16*1024);\n            hl.linkStats.countPhysicalFileCopies++;\n          } finally {\n            out.close();\n          }\n        } finally {\n          in.close();\n        }\n      } else {\n        \n        //check if we are upgrading from pre-generation stamp version.\n        if (oldLV \u003e\u003d PRE_GENERATIONSTAMP_LAYOUT_VERSION) {\n          // Link to the new file name.\n          to \u003d new File(convertMetatadataFileName(to.getAbsolutePath()));\n        }\n        \n        HardLink.createHardLink(from, to);\n        hl.linkStats.countSingleLinks++;\n      }\n      return;\n    }\n    // from is a directory\n    hl.linkStats.countDirs++;\n    \n    if (!to.mkdirs())\n      throw new IOException(\"Cannot create directory \" + to);\n    \n    //If upgrading from old stuff, need to munge the filenames.  That has to\n    //be done one file at a time, so hardlink them one at a time (slow).\n    if (oldLV \u003e\u003d PRE_GENERATIONSTAMP_LAYOUT_VERSION) {\n      String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n          public boolean accept(File dir, String name) {\n            return name.startsWith(BLOCK_SUBDIR_PREFIX) \n              || name.startsWith(BLOCK_FILE_PREFIX)\n              || name.startsWith(COPY_FILE_PREFIX);\n          }\n        });\n      if (blockNames.length \u003d\u003d 0) {\n        hl.linkStats.countEmptyDirs++;\n      }\n      else for(int i \u003d 0; i \u003c blockNames.length; i++)\n        linkBlocks(new File(from, blockNames[i]), \n            new File(to, blockNames[i]), oldLV, hl);\n    } \n    else {\n      //If upgrading from a relatively new version, we only need to create\n      //links with the same filename.  This can be done in bulk (much faster).\n      String[] blockNames \u003d from.list(new java.io.FilenameFilter() {\n        public boolean accept(File dir, String name) {\n          return name.startsWith(BLOCK_FILE_PREFIX);\n        }\n      });\n      if (blockNames.length \u003e 0) {\n        HardLink.createHardLinkMult(from, blockNames, to);\n        hl.linkStats.countMultLinks++;\n        hl.linkStats.countFilesMultLinks +\u003d blockNames.length;\n      } else {\n        hl.linkStats.countEmptyDirs++;\n      }\n      \n      //now take care of the rest of the files and subdirectories\n      String[] otherNames \u003d from.list(new java.io.FilenameFilter() {\n          public boolean accept(File dir, String name) {\n            return name.startsWith(BLOCK_SUBDIR_PREFIX) \n              || name.startsWith(COPY_FILE_PREFIX);\n          }\n        });\n      for(int i \u003d 0; i \u003c otherNames.length; i++)\n        linkBlocks(new File(from, otherNames[i]), \n            new File(to, otherNames[i]), oldLV, hl);\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java"
    }
  }
}