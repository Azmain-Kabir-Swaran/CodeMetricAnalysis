{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSNetworkTopology.java",
  "functionName": "chooseRandomWithStorageTypeAndExcludeRoot",
  "functionId": "chooseRandomWithStorageTypeAndExcludeRoot___root-DFSTopologyNodeImpl__excludeRoot-Node__type-StorageType__excludedNodes-Collection__Node__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java",
  "functionStartLine": 271,
  "functionEndLine": 329,
  "numCommitsSeen": 21,
  "timeTaken": 2631,
  "changeHistory": [
    "c84e6beada4e604175f7f138c9878a29665a8c47",
    "9832ae0ed8853d29072c9ea7031cd2373e6b16f9"
  ],
  "changeHistoryShort": {
    "c84e6beada4e604175f7f138c9878a29665a8c47": "Ymultichange(Yparameterchange,Ybodychange)",
    "9832ae0ed8853d29072c9ea7031cd2373e6b16f9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c84e6beada4e604175f7f138c9878a29665a8c47": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-14999. Avoid Potential Infinite Loop in DFSNetworkTopology. Contributed by Ayush Saxena.\n",
      "commitDate": "18/05/20 9:54 AM",
      "commitName": "c84e6beada4e604175f7f138c9878a29665a8c47",
      "commitAuthor": "Ayush Saxena",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-14999. Avoid Potential Infinite Loop in DFSNetworkTopology. Contributed by Ayush Saxena.\n",
          "commitDate": "18/05/20 9:54 AM",
          "commitName": "c84e6beada4e604175f7f138c9878a29665a8c47",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "08/04/20 3:25 AM",
          "commitNameOld": "1189af4746919774035f5d64ccb4d2ce21905aaa",
          "commitAuthorOld": "Ayush Saxena",
          "daysBetweenCommits": 40.27,
          "commitsBetweenForRepo": 144,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,59 @@\n   private Node chooseRandomWithStorageTypeAndExcludeRoot(\n-      DFSTopologyNodeImpl root, Node excludeRoot, StorageType type) {\n+      DFSTopologyNodeImpl root, Node excludeRoot, StorageType type,\n+      Collection\u003cNode\u003e excludedNodes) {\n     Node chosenNode;\n     if (root.isRack()) {\n       // children are datanode descriptor\n       ArrayList\u003cNode\u003e candidates \u003d new ArrayList\u003c\u003e();\n       for (Node node : root.getChildren()) {\n-        if (node.equals(excludeRoot)) {\n+        if (node.equals(excludeRoot) || (excludedNodes !\u003d null \u0026\u0026 excludedNodes\n+            .contains(node))) {\n           continue;\n         }\n         DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor)node;\n         if (dnDescriptor.hasStorageType(type)) {\n           candidates.add(node);\n         }\n       }\n       if (candidates.size() \u003d\u003d 0) {\n         return null;\n       }\n       // to this point, all nodes in candidates are valid choices, and they are\n       // all datanodes, pick a random one.\n       chosenNode \u003d candidates.get(RANDOM.nextInt(candidates.size()));\n     } else {\n       // the children are inner nodes\n       ArrayList\u003cDFSTopologyNodeImpl\u003e candidates \u003d\n-          getEligibleChildren(root, excludeRoot, type);\n+          getEligibleChildren(root, excludeRoot, type, excludedNodes);\n       if (candidates.size() \u003d\u003d 0) {\n         return null;\n       }\n       // again, all children are also inner nodes, we can do this cast.\n       // to maintain uniformality, the search needs to be based on the counts\n       // of valid datanodes. Below is a random weighted choose.\n       int totalCounts \u003d 0;\n       int[] countArray \u003d new int[candidates.size()];\n       for (int i \u003d 0; i \u003c candidates.size(); i++) {\n         DFSTopologyNodeImpl innerNode \u003d candidates.get(i);\n         int subTreeCount \u003d innerNode.getSubtreeStorageCount(type);\n         totalCounts +\u003d subTreeCount;\n         countArray[i] \u003d subTreeCount;\n       }\n       // generate a random val between [1, totalCounts]\n       int randomCounts \u003d RANDOM.nextInt(totalCounts) + 1;\n       int idxChosen \u003d 0;\n       // searching for the idxChosen can potentially be done with binary\n       // search, but does not seem to worth it here.\n       for (int i \u003d 0; i \u003c countArray.length; i++) {\n         if (randomCounts \u003c\u003d countArray[i]) {\n           idxChosen \u003d i;\n           break;\n         }\n         randomCounts -\u003d countArray[i];\n       }\n       DFSTopologyNodeImpl nextRoot \u003d candidates.get(idxChosen);\n       chosenNode \u003d chooseRandomWithStorageTypeAndExcludeRoot(\n-          nextRoot, excludeRoot, type);\n+          nextRoot, excludeRoot, type, excludedNodes);\n     }\n     return chosenNode;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Node chooseRandomWithStorageTypeAndExcludeRoot(\n      DFSTopologyNodeImpl root, Node excludeRoot, StorageType type,\n      Collection\u003cNode\u003e excludedNodes) {\n    Node chosenNode;\n    if (root.isRack()) {\n      // children are datanode descriptor\n      ArrayList\u003cNode\u003e candidates \u003d new ArrayList\u003c\u003e();\n      for (Node node : root.getChildren()) {\n        if (node.equals(excludeRoot) || (excludedNodes !\u003d null \u0026\u0026 excludedNodes\n            .contains(node))) {\n          continue;\n        }\n        DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor)node;\n        if (dnDescriptor.hasStorageType(type)) {\n          candidates.add(node);\n        }\n      }\n      if (candidates.size() \u003d\u003d 0) {\n        return null;\n      }\n      // to this point, all nodes in candidates are valid choices, and they are\n      // all datanodes, pick a random one.\n      chosenNode \u003d candidates.get(RANDOM.nextInt(candidates.size()));\n    } else {\n      // the children are inner nodes\n      ArrayList\u003cDFSTopologyNodeImpl\u003e candidates \u003d\n          getEligibleChildren(root, excludeRoot, type, excludedNodes);\n      if (candidates.size() \u003d\u003d 0) {\n        return null;\n      }\n      // again, all children are also inner nodes, we can do this cast.\n      // to maintain uniformality, the search needs to be based on the counts\n      // of valid datanodes. Below is a random weighted choose.\n      int totalCounts \u003d 0;\n      int[] countArray \u003d new int[candidates.size()];\n      for (int i \u003d 0; i \u003c candidates.size(); i++) {\n        DFSTopologyNodeImpl innerNode \u003d candidates.get(i);\n        int subTreeCount \u003d innerNode.getSubtreeStorageCount(type);\n        totalCounts +\u003d subTreeCount;\n        countArray[i] \u003d subTreeCount;\n      }\n      // generate a random val between [1, totalCounts]\n      int randomCounts \u003d RANDOM.nextInt(totalCounts) + 1;\n      int idxChosen \u003d 0;\n      // searching for the idxChosen can potentially be done with binary\n      // search, but does not seem to worth it here.\n      for (int i \u003d 0; i \u003c countArray.length; i++) {\n        if (randomCounts \u003c\u003d countArray[i]) {\n          idxChosen \u003d i;\n          break;\n        }\n        randomCounts -\u003d countArray[i];\n      }\n      DFSTopologyNodeImpl nextRoot \u003d candidates.get(idxChosen);\n      chosenNode \u003d chooseRandomWithStorageTypeAndExcludeRoot(\n          nextRoot, excludeRoot, type, excludedNodes);\n    }\n    return chosenNode;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java",
          "extendedDetails": {
            "oldValue": "[root-DFSTopologyNodeImpl, excludeRoot-Node, type-StorageType]",
            "newValue": "[root-DFSTopologyNodeImpl, excludeRoot-Node, type-StorageType, excludedNodes-Collection\u003cNode\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-14999. Avoid Potential Infinite Loop in DFSNetworkTopology. Contributed by Ayush Saxena.\n",
          "commitDate": "18/05/20 9:54 AM",
          "commitName": "c84e6beada4e604175f7f138c9878a29665a8c47",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "08/04/20 3:25 AM",
          "commitNameOld": "1189af4746919774035f5d64ccb4d2ce21905aaa",
          "commitAuthorOld": "Ayush Saxena",
          "daysBetweenCommits": 40.27,
          "commitsBetweenForRepo": 144,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,59 @@\n   private Node chooseRandomWithStorageTypeAndExcludeRoot(\n-      DFSTopologyNodeImpl root, Node excludeRoot, StorageType type) {\n+      DFSTopologyNodeImpl root, Node excludeRoot, StorageType type,\n+      Collection\u003cNode\u003e excludedNodes) {\n     Node chosenNode;\n     if (root.isRack()) {\n       // children are datanode descriptor\n       ArrayList\u003cNode\u003e candidates \u003d new ArrayList\u003c\u003e();\n       for (Node node : root.getChildren()) {\n-        if (node.equals(excludeRoot)) {\n+        if (node.equals(excludeRoot) || (excludedNodes !\u003d null \u0026\u0026 excludedNodes\n+            .contains(node))) {\n           continue;\n         }\n         DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor)node;\n         if (dnDescriptor.hasStorageType(type)) {\n           candidates.add(node);\n         }\n       }\n       if (candidates.size() \u003d\u003d 0) {\n         return null;\n       }\n       // to this point, all nodes in candidates are valid choices, and they are\n       // all datanodes, pick a random one.\n       chosenNode \u003d candidates.get(RANDOM.nextInt(candidates.size()));\n     } else {\n       // the children are inner nodes\n       ArrayList\u003cDFSTopologyNodeImpl\u003e candidates \u003d\n-          getEligibleChildren(root, excludeRoot, type);\n+          getEligibleChildren(root, excludeRoot, type, excludedNodes);\n       if (candidates.size() \u003d\u003d 0) {\n         return null;\n       }\n       // again, all children are also inner nodes, we can do this cast.\n       // to maintain uniformality, the search needs to be based on the counts\n       // of valid datanodes. Below is a random weighted choose.\n       int totalCounts \u003d 0;\n       int[] countArray \u003d new int[candidates.size()];\n       for (int i \u003d 0; i \u003c candidates.size(); i++) {\n         DFSTopologyNodeImpl innerNode \u003d candidates.get(i);\n         int subTreeCount \u003d innerNode.getSubtreeStorageCount(type);\n         totalCounts +\u003d subTreeCount;\n         countArray[i] \u003d subTreeCount;\n       }\n       // generate a random val between [1, totalCounts]\n       int randomCounts \u003d RANDOM.nextInt(totalCounts) + 1;\n       int idxChosen \u003d 0;\n       // searching for the idxChosen can potentially be done with binary\n       // search, but does not seem to worth it here.\n       for (int i \u003d 0; i \u003c countArray.length; i++) {\n         if (randomCounts \u003c\u003d countArray[i]) {\n           idxChosen \u003d i;\n           break;\n         }\n         randomCounts -\u003d countArray[i];\n       }\n       DFSTopologyNodeImpl nextRoot \u003d candidates.get(idxChosen);\n       chosenNode \u003d chooseRandomWithStorageTypeAndExcludeRoot(\n-          nextRoot, excludeRoot, type);\n+          nextRoot, excludeRoot, type, excludedNodes);\n     }\n     return chosenNode;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Node chooseRandomWithStorageTypeAndExcludeRoot(\n      DFSTopologyNodeImpl root, Node excludeRoot, StorageType type,\n      Collection\u003cNode\u003e excludedNodes) {\n    Node chosenNode;\n    if (root.isRack()) {\n      // children are datanode descriptor\n      ArrayList\u003cNode\u003e candidates \u003d new ArrayList\u003c\u003e();\n      for (Node node : root.getChildren()) {\n        if (node.equals(excludeRoot) || (excludedNodes !\u003d null \u0026\u0026 excludedNodes\n            .contains(node))) {\n          continue;\n        }\n        DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor)node;\n        if (dnDescriptor.hasStorageType(type)) {\n          candidates.add(node);\n        }\n      }\n      if (candidates.size() \u003d\u003d 0) {\n        return null;\n      }\n      // to this point, all nodes in candidates are valid choices, and they are\n      // all datanodes, pick a random one.\n      chosenNode \u003d candidates.get(RANDOM.nextInt(candidates.size()));\n    } else {\n      // the children are inner nodes\n      ArrayList\u003cDFSTopologyNodeImpl\u003e candidates \u003d\n          getEligibleChildren(root, excludeRoot, type, excludedNodes);\n      if (candidates.size() \u003d\u003d 0) {\n        return null;\n      }\n      // again, all children are also inner nodes, we can do this cast.\n      // to maintain uniformality, the search needs to be based on the counts\n      // of valid datanodes. Below is a random weighted choose.\n      int totalCounts \u003d 0;\n      int[] countArray \u003d new int[candidates.size()];\n      for (int i \u003d 0; i \u003c candidates.size(); i++) {\n        DFSTopologyNodeImpl innerNode \u003d candidates.get(i);\n        int subTreeCount \u003d innerNode.getSubtreeStorageCount(type);\n        totalCounts +\u003d subTreeCount;\n        countArray[i] \u003d subTreeCount;\n      }\n      // generate a random val between [1, totalCounts]\n      int randomCounts \u003d RANDOM.nextInt(totalCounts) + 1;\n      int idxChosen \u003d 0;\n      // searching for the idxChosen can potentially be done with binary\n      // search, but does not seem to worth it here.\n      for (int i \u003d 0; i \u003c countArray.length; i++) {\n        if (randomCounts \u003c\u003d countArray[i]) {\n          idxChosen \u003d i;\n          break;\n        }\n        randomCounts -\u003d countArray[i];\n      }\n      DFSTopologyNodeImpl nextRoot \u003d candidates.get(idxChosen);\n      chosenNode \u003d chooseRandomWithStorageTypeAndExcludeRoot(\n          nextRoot, excludeRoot, type, excludedNodes);\n    }\n    return chosenNode;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java",
          "extendedDetails": {}
        }
      ]
    },
    "9832ae0ed8853d29072c9ea7031cd2373e6b16f9": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11482. Add storage type demand to into DFSNetworkTopology#chooseRandom. Contributed by Chen Liang.\n",
      "commitDate": "13/03/17 5:30 PM",
      "commitName": "9832ae0ed8853d29072c9ea7031cd2373e6b16f9",
      "commitAuthor": "Chen Liang",
      "diff": "@@ -0,0 +1,57 @@\n+  private Node chooseRandomWithStorageTypeAndExcludeRoot(\n+      DFSTopologyNodeImpl root, Node excludeRoot, StorageType type) {\n+    Node chosenNode;\n+    if (root.isRack()) {\n+      // children are datanode descriptor\n+      ArrayList\u003cNode\u003e candidates \u003d new ArrayList\u003c\u003e();\n+      for (Node node : root.getChildren()) {\n+        if (node.equals(excludeRoot)) {\n+          continue;\n+        }\n+        DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor)node;\n+        if (dnDescriptor.hasStorageType(type)) {\n+          candidates.add(node);\n+        }\n+      }\n+      if (candidates.size() \u003d\u003d 0) {\n+        return null;\n+      }\n+      // to this point, all nodes in candidates are valid choices, and they are\n+      // all datanodes, pick a random one.\n+      chosenNode \u003d candidates.get(RANDOM.nextInt(candidates.size()));\n+    } else {\n+      // the children are inner nodes\n+      ArrayList\u003cDFSTopologyNodeImpl\u003e candidates \u003d\n+          getEligibleChildren(root, excludeRoot, type);\n+      if (candidates.size() \u003d\u003d 0) {\n+        return null;\n+      }\n+      // again, all children are also inner nodes, we can do this cast.\n+      // to maintain uniformality, the search needs to be based on the counts\n+      // of valid datanodes. Below is a random weighted choose.\n+      int totalCounts \u003d 0;\n+      int[] countArray \u003d new int[candidates.size()];\n+      for (int i \u003d 0; i \u003c candidates.size(); i++) {\n+        DFSTopologyNodeImpl innerNode \u003d candidates.get(i);\n+        int subTreeCount \u003d innerNode.getSubtreeStorageCount(type);\n+        totalCounts +\u003d subTreeCount;\n+        countArray[i] \u003d subTreeCount;\n+      }\n+      // generate a random val between [1, totalCounts]\n+      int randomCounts \u003d RANDOM.nextInt(totalCounts) + 1;\n+      int idxChosen \u003d 0;\n+      // searching for the idxChosen can potentially be done with binary\n+      // search, but does not seem to worth it here.\n+      for (int i \u003d 0; i \u003c countArray.length; i++) {\n+        if (randomCounts \u003c\u003d countArray[i]) {\n+          idxChosen \u003d i;\n+          break;\n+        }\n+        randomCounts -\u003d countArray[i];\n+      }\n+      DFSTopologyNodeImpl nextRoot \u003d candidates.get(idxChosen);\n+      chosenNode \u003d chooseRandomWithStorageTypeAndExcludeRoot(\n+          nextRoot, excludeRoot, type);\n+    }\n+    return chosenNode;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private Node chooseRandomWithStorageTypeAndExcludeRoot(\n      DFSTopologyNodeImpl root, Node excludeRoot, StorageType type) {\n    Node chosenNode;\n    if (root.isRack()) {\n      // children are datanode descriptor\n      ArrayList\u003cNode\u003e candidates \u003d new ArrayList\u003c\u003e();\n      for (Node node : root.getChildren()) {\n        if (node.equals(excludeRoot)) {\n          continue;\n        }\n        DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor)node;\n        if (dnDescriptor.hasStorageType(type)) {\n          candidates.add(node);\n        }\n      }\n      if (candidates.size() \u003d\u003d 0) {\n        return null;\n      }\n      // to this point, all nodes in candidates are valid choices, and they are\n      // all datanodes, pick a random one.\n      chosenNode \u003d candidates.get(RANDOM.nextInt(candidates.size()));\n    } else {\n      // the children are inner nodes\n      ArrayList\u003cDFSTopologyNodeImpl\u003e candidates \u003d\n          getEligibleChildren(root, excludeRoot, type);\n      if (candidates.size() \u003d\u003d 0) {\n        return null;\n      }\n      // again, all children are also inner nodes, we can do this cast.\n      // to maintain uniformality, the search needs to be based on the counts\n      // of valid datanodes. Below is a random weighted choose.\n      int totalCounts \u003d 0;\n      int[] countArray \u003d new int[candidates.size()];\n      for (int i \u003d 0; i \u003c candidates.size(); i++) {\n        DFSTopologyNodeImpl innerNode \u003d candidates.get(i);\n        int subTreeCount \u003d innerNode.getSubtreeStorageCount(type);\n        totalCounts +\u003d subTreeCount;\n        countArray[i] \u003d subTreeCount;\n      }\n      // generate a random val between [1, totalCounts]\n      int randomCounts \u003d RANDOM.nextInt(totalCounts) + 1;\n      int idxChosen \u003d 0;\n      // searching for the idxChosen can potentially be done with binary\n      // search, but does not seem to worth it here.\n      for (int i \u003d 0; i \u003c countArray.length; i++) {\n        if (randomCounts \u003c\u003d countArray[i]) {\n          idxChosen \u003d i;\n          break;\n        }\n        randomCounts -\u003d countArray[i];\n      }\n      DFSTopologyNodeImpl nextRoot \u003d candidates.get(idxChosen);\n      chosenNode \u003d chooseRandomWithStorageTypeAndExcludeRoot(\n          nextRoot, excludeRoot, type);\n    }\n    return chosenNode;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java"
    }
  }
}