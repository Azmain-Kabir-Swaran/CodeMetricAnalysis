{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "OpenFileCtx.java",
  "functionName": "addWritesToCache",
  "functionId": "addWritesToCache___request-WRITE3Request__channel-Channel__xid-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
  "functionStartLine": 515,
  "functionEndLine": 587,
  "numCommitsSeen": 36,
  "timeTaken": 2175,
  "changeHistory": [
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
    "151fca5032719e561226ef278e002739073c23ec",
    "f37849188b05a6251584de1aed5e66d5dfa7da4f",
    "d71d40a63d198991077d5babd70be5e9787a53f1",
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
    "28e3d09230971b32f74284311931525cb7ad1b7c"
  ],
  "changeHistoryShort": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": "Ybodychange",
    "151fca5032719e561226ef278e002739073c23ec": "Ybodychange",
    "f37849188b05a6251584de1aed5e66d5dfa7da4f": "Ybodychange",
    "d71d40a63d198991077d5babd70be5e9787a53f1": "Ybodychange",
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53": "Ybodychange",
    "28e3d09230971b32f74284311931525cb7ad1b7c": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10571. Use Log.*(Object, Throwable) overload to log exceptions.\nContributed by Andras Bokor.\n",
      "commitDate": "14/02/18 8:20 AM",
      "commitName": "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "10/10/17 10:38 AM",
      "commitNameOld": "d6602b5f39833611b4afa4581552f6c4c37e23a8",
      "commitAuthorOld": "Jitendra Pandey",
      "daysBetweenCommits": 126.95,
      "commitsBetweenForRepo": 833,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,79 +1,73 @@\n   private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n       Channel channel, int xid) {\n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n     long cachedOffset \u003d nextOffset.get();\n     int originalCount \u003d WriteCtx.INVALID_ORIGINAL_COUNT;\n     \n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"requested offset\u003d\" + offset + \" and current offset\u003d\"\n-          + cachedOffset);\n-    }\n+    LOG.debug(\"requested offset\u003d{} and current offset\u003d{}\",\n+        offset, cachedOffset);\n \n     // Ignore write request with range below the current offset\n     if (offset + count \u003c\u003d cachedOffset) {\n       LOG.warn(String.format(\"Got overwrite [%d-%d) smaller than\"\n           + \" current offset %d,\" + \" drop the request.\",\n           offset, (offset + count), cachedOffset));\n       return null;\n     }\n \n     // Handle a special case: trim request whose offset is smaller than\n     // the current offset\n     if ((offset \u003c cachedOffset) \u0026\u0026 (offset + count \u003e cachedOffset)) {\n       // One Linux client behavior: after a file is closed and reopened to\n       // write, the client sometimes combines previous written data(could still\n       // be in kernel buffer) with newly appended data in one write. This is\n       // usually the first write after file reopened. In this\n       // case, we log the event and drop the overlapped section.\n       LOG.warn(String.format(\"Got overwrite with appended data [%d-%d),\"\n           + \" current offset %d,\" + \" drop the overlapped section [%d-%d)\"\n           + \" and append new data [%d-%d).\", offset, (offset + count),\n           cachedOffset, offset, cachedOffset, cachedOffset, (offset\n               + count)));\n       \n       LOG.warn(\"Modify this write to write only the appended data\");\n       alterWriteRequest(request, cachedOffset);\n \n       // Update local variable\n       originalCount \u003d count;\n       offset \u003d request.getOffset();\n       count \u003d request.getCount();\n     }\n     \n     // Fail non-append call\n     if (offset \u003c cachedOffset) {\n-      LOG.warn(\"(offset,count,nextOffset): \" + \"(\" + offset + \",\" + count + \",\"\n-          + nextOffset + \")\");\n+      LOG.warn(\"(offset,count,nextOffset): ({},{},{})\",\n+          offset, count, nextOffset);\n       return null;\n     } else {\n       DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n           : WriteCtx.DataState.ALLOW_DUMP;\n       WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n           request.getOffset(), request.getCount(), originalCount,\n           request.getStableHow(), request.getData(), channel, xid, false,\n           dataState);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n-            + \" and requested offset\u003d\" + offset);\n-      }\n+      LOG.debug(\"Add new write to the list with nextOffset {}\" +\n+          \" and requested offset\u003d{}\", cachedOffset, offset);\n       if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n         // update the memory size\n         updateNonSequentialWriteInMemory(count);\n       }\n       // check if there is a WriteCtx with the same range in pendingWrites\n       WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n       if (oldWriteCtx \u003d\u003d null) {\n         pendingWrites.put(new OffsetRange(offset, offset + count), writeCtx);\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"New write buffered with xid \" + xid + \" nextOffset \"\n-              + cachedOffset + \" req offset\u003d\" + offset + \" mapsize\u003d\"\n-              + pendingWrites.size());\n-        }\n+        LOG.debug(\"New write buffered with xid {} nextOffset {}\" +\n+            \"req offset\u003d{} mapsize\u003d{}\",\n+            xid, cachedOffset, offset, pendingWrites.size());\n       } else {\n-        LOG.warn(\"Got a repeated request, same range, with xid: \" + xid\n-            + \" nextOffset \" + +cachedOffset + \" req offset\u003d\" + offset);\n+        LOG.warn(\"Got a repeated request, same range, with xid: \" +\n+            \"{} nextOffset {} req offset\u003d{}\", xid, cachedOffset, offset);\n       }\n       return writeCtx;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n      Channel channel, int xid) {\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    long cachedOffset \u003d nextOffset.get();\n    int originalCount \u003d WriteCtx.INVALID_ORIGINAL_COUNT;\n    \n    LOG.debug(\"requested offset\u003d{} and current offset\u003d{}\",\n        offset, cachedOffset);\n\n    // Ignore write request with range below the current offset\n    if (offset + count \u003c\u003d cachedOffset) {\n      LOG.warn(String.format(\"Got overwrite [%d-%d) smaller than\"\n          + \" current offset %d,\" + \" drop the request.\",\n          offset, (offset + count), cachedOffset));\n      return null;\n    }\n\n    // Handle a special case: trim request whose offset is smaller than\n    // the current offset\n    if ((offset \u003c cachedOffset) \u0026\u0026 (offset + count \u003e cachedOffset)) {\n      // One Linux client behavior: after a file is closed and reopened to\n      // write, the client sometimes combines previous written data(could still\n      // be in kernel buffer) with newly appended data in one write. This is\n      // usually the first write after file reopened. In this\n      // case, we log the event and drop the overlapped section.\n      LOG.warn(String.format(\"Got overwrite with appended data [%d-%d),\"\n          + \" current offset %d,\" + \" drop the overlapped section [%d-%d)\"\n          + \" and append new data [%d-%d).\", offset, (offset + count),\n          cachedOffset, offset, cachedOffset, cachedOffset, (offset\n              + count)));\n      \n      LOG.warn(\"Modify this write to write only the appended data\");\n      alterWriteRequest(request, cachedOffset);\n\n      // Update local variable\n      originalCount \u003d count;\n      offset \u003d request.getOffset();\n      count \u003d request.getCount();\n    }\n    \n    // Fail non-append call\n    if (offset \u003c cachedOffset) {\n      LOG.warn(\"(offset,count,nextOffset): ({},{},{})\",\n          offset, count, nextOffset);\n      return null;\n    } else {\n      DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n          : WriteCtx.DataState.ALLOW_DUMP;\n      WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n          request.getOffset(), request.getCount(), originalCount,\n          request.getStableHow(), request.getData(), channel, xid, false,\n          dataState);\n      LOG.debug(\"Add new write to the list with nextOffset {}\" +\n          \" and requested offset\u003d{}\", cachedOffset, offset);\n      if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n        // update the memory size\n        updateNonSequentialWriteInMemory(count);\n      }\n      // check if there is a WriteCtx with the same range in pendingWrites\n      WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n      if (oldWriteCtx \u003d\u003d null) {\n        pendingWrites.put(new OffsetRange(offset, offset + count), writeCtx);\n        LOG.debug(\"New write buffered with xid {} nextOffset {}\" +\n            \"req offset\u003d{} mapsize\u003d{}\",\n            xid, cachedOffset, offset, pendingWrites.size());\n      } else {\n        LOG.warn(\"Got a repeated request, same range, with xid: \" +\n            \"{} nextOffset {} req offset\u003d{}\", xid, cachedOffset, offset);\n      }\n      return writeCtx;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "151fca5032719e561226ef278e002739073c23ec": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9092. Nfs silently drops overlapping write requests and causes data copying to fail. Contributed by Yongjun Zhang.\n",
      "commitDate": "28/09/15 6:45 PM",
      "commitName": "151fca5032719e561226ef278e002739073c23ec",
      "commitAuthor": "Yongjun Zhang",
      "commitDateOld": "28/01/15 12:56 PM",
      "commitNameOld": "f37849188b05a6251584de1aed5e66d5dfa7da4f",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 243.2,
      "commitsBetweenForRepo": 1923,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,79 @@\n   private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n       Channel channel, int xid) {\n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n     long cachedOffset \u003d nextOffset.get();\n     int originalCount \u003d WriteCtx.INVALID_ORIGINAL_COUNT;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"requested offset\u003d\" + offset + \" and current offset\u003d\"\n           + cachedOffset);\n     }\n \n-    // Handle a special case first\n+    // Ignore write request with range below the current offset\n+    if (offset + count \u003c\u003d cachedOffset) {\n+      LOG.warn(String.format(\"Got overwrite [%d-%d) smaller than\"\n+          + \" current offset %d,\" + \" drop the request.\",\n+          offset, (offset + count), cachedOffset));\n+      return null;\n+    }\n+\n+    // Handle a special case: trim request whose offset is smaller than\n+    // the current offset\n     if ((offset \u003c cachedOffset) \u0026\u0026 (offset + count \u003e cachedOffset)) {\n       // One Linux client behavior: after a file is closed and reopened to\n       // write, the client sometimes combines previous written data(could still\n       // be in kernel buffer) with newly appended data in one write. This is\n       // usually the first write after file reopened. In this\n       // case, we log the event and drop the overlapped section.\n-      LOG.warn(String.format(\"Got overwrite with appended data (%d-%d),\"\n-          + \" current offset %d,\" + \" drop the overlapped section (%d-%d)\"\n-          + \" and append new data (%d-%d).\", offset, (offset + count - 1),\n-          cachedOffset, offset, (cachedOffset - 1), cachedOffset, (offset\n-              + count - 1)));\n-\n-      if (!pendingWrites.isEmpty()) {\n-        LOG.warn(\"There are other pending writes, fail this jumbo write\");\n-        return null;\n-      }\n+      LOG.warn(String.format(\"Got overwrite with appended data [%d-%d),\"\n+          + \" current offset %d,\" + \" drop the overlapped section [%d-%d)\"\n+          + \" and append new data [%d-%d).\", offset, (offset + count),\n+          cachedOffset, offset, cachedOffset, cachedOffset, (offset\n+              + count)));\n       \n       LOG.warn(\"Modify this write to write only the appended data\");\n       alterWriteRequest(request, cachedOffset);\n \n       // Update local variable\n       originalCount \u003d count;\n       offset \u003d request.getOffset();\n       count \u003d request.getCount();\n     }\n     \n     // Fail non-append call\n     if (offset \u003c cachedOffset) {\n       LOG.warn(\"(offset,count,nextOffset): \" + \"(\" + offset + \",\" + count + \",\"\n           + nextOffset + \")\");\n       return null;\n     } else {\n       DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n           : WriteCtx.DataState.ALLOW_DUMP;\n       WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n           request.getOffset(), request.getCount(), originalCount,\n           request.getStableHow(), request.getData(), channel, xid, false,\n           dataState);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n             + \" and requested offset\u003d\" + offset);\n       }\n       if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n         // update the memory size\n         updateNonSequentialWriteInMemory(count);\n       }\n       // check if there is a WriteCtx with the same range in pendingWrites\n       WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n       if (oldWriteCtx \u003d\u003d null) {\n         pendingWrites.put(new OffsetRange(offset, offset + count), writeCtx);\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"New write buffered with xid \" + xid + \" nextOffset \"\n               + cachedOffset + \" req offset\u003d\" + offset + \" mapsize\u003d\"\n               + pendingWrites.size());\n         }\n       } else {\n         LOG.warn(\"Got a repeated request, same range, with xid: \" + xid\n             + \" nextOffset \" + +cachedOffset + \" req offset\u003d\" + offset);\n       }\n       return writeCtx;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n      Channel channel, int xid) {\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    long cachedOffset \u003d nextOffset.get();\n    int originalCount \u003d WriteCtx.INVALID_ORIGINAL_COUNT;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"requested offset\u003d\" + offset + \" and current offset\u003d\"\n          + cachedOffset);\n    }\n\n    // Ignore write request with range below the current offset\n    if (offset + count \u003c\u003d cachedOffset) {\n      LOG.warn(String.format(\"Got overwrite [%d-%d) smaller than\"\n          + \" current offset %d,\" + \" drop the request.\",\n          offset, (offset + count), cachedOffset));\n      return null;\n    }\n\n    // Handle a special case: trim request whose offset is smaller than\n    // the current offset\n    if ((offset \u003c cachedOffset) \u0026\u0026 (offset + count \u003e cachedOffset)) {\n      // One Linux client behavior: after a file is closed and reopened to\n      // write, the client sometimes combines previous written data(could still\n      // be in kernel buffer) with newly appended data in one write. This is\n      // usually the first write after file reopened. In this\n      // case, we log the event and drop the overlapped section.\n      LOG.warn(String.format(\"Got overwrite with appended data [%d-%d),\"\n          + \" current offset %d,\" + \" drop the overlapped section [%d-%d)\"\n          + \" and append new data [%d-%d).\", offset, (offset + count),\n          cachedOffset, offset, cachedOffset, cachedOffset, (offset\n              + count)));\n      \n      LOG.warn(\"Modify this write to write only the appended data\");\n      alterWriteRequest(request, cachedOffset);\n\n      // Update local variable\n      originalCount \u003d count;\n      offset \u003d request.getOffset();\n      count \u003d request.getCount();\n    }\n    \n    // Fail non-append call\n    if (offset \u003c cachedOffset) {\n      LOG.warn(\"(offset,count,nextOffset): \" + \"(\" + offset + \",\" + count + \",\"\n          + nextOffset + \")\");\n      return null;\n    } else {\n      DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n          : WriteCtx.DataState.ALLOW_DUMP;\n      WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n          request.getOffset(), request.getCount(), originalCount,\n          request.getStableHow(), request.getData(), channel, xid, false,\n          dataState);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n            + \" and requested offset\u003d\" + offset);\n      }\n      if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n        // update the memory size\n        updateNonSequentialWriteInMemory(count);\n      }\n      // check if there is a WriteCtx with the same range in pendingWrites\n      WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n      if (oldWriteCtx \u003d\u003d null) {\n        pendingWrites.put(new OffsetRange(offset, offset + count), writeCtx);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"New write buffered with xid \" + xid + \" nextOffset \"\n              + cachedOffset + \" req offset\u003d\" + offset + \" mapsize\u003d\"\n              + pendingWrites.size());\n        }\n      } else {\n        LOG.warn(\"Got a repeated request, same range, with xid: \" + xid\n            + \" nextOffset \" + +cachedOffset + \" req offset\u003d\" + offset);\n      }\n      return writeCtx;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "f37849188b05a6251584de1aed5e66d5dfa7da4f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7423. various typos and message formatting fixes in nfs daemon and doc. (Charles Lamb via yliu)\n",
      "commitDate": "28/01/15 12:56 PM",
      "commitName": "f37849188b05a6251584de1aed5e66d5dfa7da4f",
      "commitAuthor": "yliu",
      "commitDateOld": "11/12/14 3:40 PM",
      "commitNameOld": "f6f2a3f1c73266bfedd802eacde60d8b19b81015",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 47.89,
      "commitsBetweenForRepo": 279,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,75 @@\n   private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n       Channel channel, int xid) {\n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n     long cachedOffset \u003d nextOffset.get();\n     int originalCount \u003d WriteCtx.INVALID_ORIGINAL_COUNT;\n     \n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"requesed offset\u003d\" + offset + \" and current offset\u003d\"\n+      LOG.debug(\"requested offset\u003d\" + offset + \" and current offset\u003d\"\n           + cachedOffset);\n     }\n \n     // Handle a special case first\n     if ((offset \u003c cachedOffset) \u0026\u0026 (offset + count \u003e cachedOffset)) {\n       // One Linux client behavior: after a file is closed and reopened to\n       // write, the client sometimes combines previous written data(could still\n       // be in kernel buffer) with newly appended data in one write. This is\n       // usually the first write after file reopened. In this\n       // case, we log the event and drop the overlapped section.\n       LOG.warn(String.format(\"Got overwrite with appended data (%d-%d),\"\n           + \" current offset %d,\" + \" drop the overlapped section (%d-%d)\"\n           + \" and append new data (%d-%d).\", offset, (offset + count - 1),\n           cachedOffset, offset, (cachedOffset - 1), cachedOffset, (offset\n               + count - 1)));\n \n       if (!pendingWrites.isEmpty()) {\n         LOG.warn(\"There are other pending writes, fail this jumbo write\");\n         return null;\n       }\n       \n       LOG.warn(\"Modify this write to write only the appended data\");\n       alterWriteRequest(request, cachedOffset);\n \n       // Update local variable\n       originalCount \u003d count;\n       offset \u003d request.getOffset();\n       count \u003d request.getCount();\n     }\n     \n     // Fail non-append call\n     if (offset \u003c cachedOffset) {\n-      LOG.warn(\"(offset,count,nextOffset):\" + \"(\" + offset + \",\" + count + \",\"\n+      LOG.warn(\"(offset,count,nextOffset): \" + \"(\" + offset + \",\" + count + \",\"\n           + nextOffset + \")\");\n       return null;\n     } else {\n       DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n           : WriteCtx.DataState.ALLOW_DUMP;\n       WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n           request.getOffset(), request.getCount(), originalCount,\n           request.getStableHow(), request.getData(), channel, xid, false,\n           dataState);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n-            + \" and requesed offset\u003d\" + offset);\n+            + \" and requested offset\u003d\" + offset);\n       }\n       if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n         // update the memory size\n         updateNonSequentialWriteInMemory(count);\n       }\n       // check if there is a WriteCtx with the same range in pendingWrites\n       WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n       if (oldWriteCtx \u003d\u003d null) {\n         pendingWrites.put(new OffsetRange(offset, offset + count), writeCtx);\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"New write buffered with xid \" + xid + \" nextOffset \"\n               + cachedOffset + \" req offset\u003d\" + offset + \" mapsize\u003d\"\n               + pendingWrites.size());\n         }\n       } else {\n-        LOG.warn(\"Got a repeated request, same range, with xid:\" + xid\n+        LOG.warn(\"Got a repeated request, same range, with xid: \" + xid\n             + \" nextOffset \" + +cachedOffset + \" req offset\u003d\" + offset);\n       }\n       return writeCtx;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n      Channel channel, int xid) {\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    long cachedOffset \u003d nextOffset.get();\n    int originalCount \u003d WriteCtx.INVALID_ORIGINAL_COUNT;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"requested offset\u003d\" + offset + \" and current offset\u003d\"\n          + cachedOffset);\n    }\n\n    // Handle a special case first\n    if ((offset \u003c cachedOffset) \u0026\u0026 (offset + count \u003e cachedOffset)) {\n      // One Linux client behavior: after a file is closed and reopened to\n      // write, the client sometimes combines previous written data(could still\n      // be in kernel buffer) with newly appended data in one write. This is\n      // usually the first write after file reopened. In this\n      // case, we log the event and drop the overlapped section.\n      LOG.warn(String.format(\"Got overwrite with appended data (%d-%d),\"\n          + \" current offset %d,\" + \" drop the overlapped section (%d-%d)\"\n          + \" and append new data (%d-%d).\", offset, (offset + count - 1),\n          cachedOffset, offset, (cachedOffset - 1), cachedOffset, (offset\n              + count - 1)));\n\n      if (!pendingWrites.isEmpty()) {\n        LOG.warn(\"There are other pending writes, fail this jumbo write\");\n        return null;\n      }\n      \n      LOG.warn(\"Modify this write to write only the appended data\");\n      alterWriteRequest(request, cachedOffset);\n\n      // Update local variable\n      originalCount \u003d count;\n      offset \u003d request.getOffset();\n      count \u003d request.getCount();\n    }\n    \n    // Fail non-append call\n    if (offset \u003c cachedOffset) {\n      LOG.warn(\"(offset,count,nextOffset): \" + \"(\" + offset + \",\" + count + \",\"\n          + nextOffset + \")\");\n      return null;\n    } else {\n      DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n          : WriteCtx.DataState.ALLOW_DUMP;\n      WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n          request.getOffset(), request.getCount(), originalCount,\n          request.getStableHow(), request.getData(), channel, xid, false,\n          dataState);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n            + \" and requested offset\u003d\" + offset);\n      }\n      if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n        // update the memory size\n        updateNonSequentialWriteInMemory(count);\n      }\n      // check if there is a WriteCtx with the same range in pendingWrites\n      WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n      if (oldWriteCtx \u003d\u003d null) {\n        pendingWrites.put(new OffsetRange(offset, offset + count), writeCtx);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"New write buffered with xid \" + xid + \" nextOffset \"\n              + cachedOffset + \" req offset\u003d\" + offset + \" mapsize\u003d\"\n              + pendingWrites.size());\n        }\n      } else {\n        LOG.warn(\"Got a repeated request, same range, with xid: \" + xid\n            + \" nextOffset \" + +cachedOffset + \" req offset\u003d\" + offset);\n      }\n      return writeCtx;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "d71d40a63d198991077d5babd70be5e9787a53f1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7180. NFSv3 gateway frequently gets stuck due to GC. Contributed by Brandon Li\n",
      "commitDate": "22/10/14 9:27 PM",
      "commitName": "d71d40a63d198991077d5babd70be5e9787a53f1",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "21/10/14 10:20 AM",
      "commitNameOld": "b6f9d5538cf2b425652687e99503f3d566b2056a",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 1.46,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,75 @@\n   private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n       Channel channel, int xid) {\n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n     long cachedOffset \u003d nextOffset.get();\n     int originalCount \u003d WriteCtx.INVALID_ORIGINAL_COUNT;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"requesed offset\u003d\" + offset + \" and current offset\u003d\"\n           + cachedOffset);\n     }\n \n     // Handle a special case first\n     if ((offset \u003c cachedOffset) \u0026\u0026 (offset + count \u003e cachedOffset)) {\n       // One Linux client behavior: after a file is closed and reopened to\n       // write, the client sometimes combines previous written data(could still\n       // be in kernel buffer) with newly appended data in one write. This is\n       // usually the first write after file reopened. In this\n       // case, we log the event and drop the overlapped section.\n       LOG.warn(String.format(\"Got overwrite with appended data (%d-%d),\"\n           + \" current offset %d,\" + \" drop the overlapped section (%d-%d)\"\n           + \" and append new data (%d-%d).\", offset, (offset + count - 1),\n           cachedOffset, offset, (cachedOffset - 1), cachedOffset, (offset\n               + count - 1)));\n \n       if (!pendingWrites.isEmpty()) {\n         LOG.warn(\"There are other pending writes, fail this jumbo write\");\n         return null;\n       }\n       \n       LOG.warn(\"Modify this write to write only the appended data\");\n       alterWriteRequest(request, cachedOffset);\n \n       // Update local variable\n       originalCount \u003d count;\n       offset \u003d request.getOffset();\n       count \u003d request.getCount();\n     }\n     \n     // Fail non-append call\n     if (offset \u003c cachedOffset) {\n       LOG.warn(\"(offset,count,nextOffset):\" + \"(\" + offset + \",\" + count + \",\"\n           + nextOffset + \")\");\n       return null;\n     } else {\n       DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n           : WriteCtx.DataState.ALLOW_DUMP;\n       WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n           request.getOffset(), request.getCount(), originalCount,\n           request.getStableHow(), request.getData(), channel, xid, false,\n           dataState);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n             + \" and requesed offset\u003d\" + offset);\n       }\n       if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n         // update the memory size\n         updateNonSequentialWriteInMemory(count);\n       }\n       // check if there is a WriteCtx with the same range in pendingWrites\n       WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n       if (oldWriteCtx \u003d\u003d null) {\n-        addWrite(writeCtx);\n+        pendingWrites.put(new OffsetRange(offset, offset + count), writeCtx);\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"New write buffered with xid \" + xid + \" nextOffset \"\n+              + cachedOffset + \" req offset\u003d\" + offset + \" mapsize\u003d\"\n+              + pendingWrites.size());\n+        }\n       } else {\n-        LOG.warn(\"Got a repeated request, same range, with xid:\"\n-            + writeCtx.getXid());\n+        LOG.warn(\"Got a repeated request, same range, with xid:\" + xid\n+            + \" nextOffset \" + +cachedOffset + \" req offset\u003d\" + offset);\n       }\n       return writeCtx;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n      Channel channel, int xid) {\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    long cachedOffset \u003d nextOffset.get();\n    int originalCount \u003d WriteCtx.INVALID_ORIGINAL_COUNT;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"requesed offset\u003d\" + offset + \" and current offset\u003d\"\n          + cachedOffset);\n    }\n\n    // Handle a special case first\n    if ((offset \u003c cachedOffset) \u0026\u0026 (offset + count \u003e cachedOffset)) {\n      // One Linux client behavior: after a file is closed and reopened to\n      // write, the client sometimes combines previous written data(could still\n      // be in kernel buffer) with newly appended data in one write. This is\n      // usually the first write after file reopened. In this\n      // case, we log the event and drop the overlapped section.\n      LOG.warn(String.format(\"Got overwrite with appended data (%d-%d),\"\n          + \" current offset %d,\" + \" drop the overlapped section (%d-%d)\"\n          + \" and append new data (%d-%d).\", offset, (offset + count - 1),\n          cachedOffset, offset, (cachedOffset - 1), cachedOffset, (offset\n              + count - 1)));\n\n      if (!pendingWrites.isEmpty()) {\n        LOG.warn(\"There are other pending writes, fail this jumbo write\");\n        return null;\n      }\n      \n      LOG.warn(\"Modify this write to write only the appended data\");\n      alterWriteRequest(request, cachedOffset);\n\n      // Update local variable\n      originalCount \u003d count;\n      offset \u003d request.getOffset();\n      count \u003d request.getCount();\n    }\n    \n    // Fail non-append call\n    if (offset \u003c cachedOffset) {\n      LOG.warn(\"(offset,count,nextOffset):\" + \"(\" + offset + \",\" + count + \",\"\n          + nextOffset + \")\");\n      return null;\n    } else {\n      DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n          : WriteCtx.DataState.ALLOW_DUMP;\n      WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n          request.getOffset(), request.getCount(), originalCount,\n          request.getStableHow(), request.getData(), channel, xid, false,\n          dataState);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n            + \" and requesed offset\u003d\" + offset);\n      }\n      if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n        // update the memory size\n        updateNonSequentialWriteInMemory(count);\n      }\n      // check if there is a WriteCtx with the same range in pendingWrites\n      WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n      if (oldWriteCtx \u003d\u003d null) {\n        pendingWrites.put(new OffsetRange(offset, offset + count), writeCtx);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"New write buffered with xid \" + xid + \" nextOffset \"\n              + cachedOffset + \" req offset\u003d\" + offset + \" mapsize\u003d\"\n              + pendingWrites.size());\n        }\n      } else {\n        LOG.warn(\"Got a repeated request, same range, with xid:\" + xid\n            + \" nextOffset \" + +cachedOffset + \" req offset\u003d\" + offset);\n      }\n      return writeCtx;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5259. Support client which combines appended data with old data before sends it to NFS server. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529730 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/10/13 7:57 PM",
      "commitName": "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "27/09/13 2:28 PM",
      "commitNameOld": "027419832c1125d707b45ce852032d704ab79d88",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 9.23,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,70 @@\n   private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n       Channel channel, int xid) {\n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n     long cachedOffset \u003d nextOffset.get();\n-\n+    int originalCount \u003d WriteCtx.INVALID_ORIGINAL_COUNT;\n+    \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"requesed offset\u003d\" + offset + \" and current offset\u003d\"\n           + cachedOffset);\n     }\n \n+    // Handle a special case first\n+    if ((offset \u003c cachedOffset) \u0026\u0026 (offset + count \u003e cachedOffset)) {\n+      // One Linux client behavior: after a file is closed and reopened to\n+      // write, the client sometimes combines previous written data(could still\n+      // be in kernel buffer) with newly appended data in one write. This is\n+      // usually the first write after file reopened. In this\n+      // case, we log the event and drop the overlapped section.\n+      LOG.warn(String.format(\"Got overwrite with appended data (%d-%d),\"\n+          + \" current offset %d,\" + \" drop the overlapped section (%d-%d)\"\n+          + \" and append new data (%d-%d).\", offset, (offset + count - 1),\n+          cachedOffset, offset, (cachedOffset - 1), cachedOffset, (offset\n+              + count - 1)));\n+\n+      if (!pendingWrites.isEmpty()) {\n+        LOG.warn(\"There are other pending writes, fail this jumbo write\");\n+        return null;\n+      }\n+      \n+      LOG.warn(\"Modify this write to write only the appended data\");\n+      alterWriteRequest(request, cachedOffset);\n+\n+      // Update local variable\n+      originalCount \u003d count;\n+      offset \u003d request.getOffset();\n+      count \u003d request.getCount();\n+    }\n+    \n     // Fail non-append call\n     if (offset \u003c cachedOffset) {\n       LOG.warn(\"(offset,count,nextOffset):\" + \"(\" + offset + \",\" + count + \",\"\n           + nextOffset + \")\");\n       return null;\n     } else {\n       DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n           : WriteCtx.DataState.ALLOW_DUMP;\n       WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n-          request.getOffset(), request.getCount(), request.getStableHow(),\n-          request.getData().array(), channel, xid, false, dataState);\n+          request.getOffset(), request.getCount(), originalCount,\n+          request.getStableHow(), request.getData(), channel, xid, false,\n+          dataState);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n             + \" and requesed offset\u003d\" + offset);\n       }\n       if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n         // update the memory size\n         updateNonSequentialWriteInMemory(count);\n       }\n       // check if there is a WriteCtx with the same range in pendingWrites\n       WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n       if (oldWriteCtx \u003d\u003d null) {\n         addWrite(writeCtx);\n       } else {\n         LOG.warn(\"Got a repeated request, same range, with xid:\"\n             + writeCtx.getXid());\n       }\n       return writeCtx;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n      Channel channel, int xid) {\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    long cachedOffset \u003d nextOffset.get();\n    int originalCount \u003d WriteCtx.INVALID_ORIGINAL_COUNT;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"requesed offset\u003d\" + offset + \" and current offset\u003d\"\n          + cachedOffset);\n    }\n\n    // Handle a special case first\n    if ((offset \u003c cachedOffset) \u0026\u0026 (offset + count \u003e cachedOffset)) {\n      // One Linux client behavior: after a file is closed and reopened to\n      // write, the client sometimes combines previous written data(could still\n      // be in kernel buffer) with newly appended data in one write. This is\n      // usually the first write after file reopened. In this\n      // case, we log the event and drop the overlapped section.\n      LOG.warn(String.format(\"Got overwrite with appended data (%d-%d),\"\n          + \" current offset %d,\" + \" drop the overlapped section (%d-%d)\"\n          + \" and append new data (%d-%d).\", offset, (offset + count - 1),\n          cachedOffset, offset, (cachedOffset - 1), cachedOffset, (offset\n              + count - 1)));\n\n      if (!pendingWrites.isEmpty()) {\n        LOG.warn(\"There are other pending writes, fail this jumbo write\");\n        return null;\n      }\n      \n      LOG.warn(\"Modify this write to write only the appended data\");\n      alterWriteRequest(request, cachedOffset);\n\n      // Update local variable\n      originalCount \u003d count;\n      offset \u003d request.getOffset();\n      count \u003d request.getCount();\n    }\n    \n    // Fail non-append call\n    if (offset \u003c cachedOffset) {\n      LOG.warn(\"(offset,count,nextOffset):\" + \"(\" + offset + \",\" + count + \",\"\n          + nextOffset + \")\");\n      return null;\n    } else {\n      DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n          : WriteCtx.DataState.ALLOW_DUMP;\n      WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n          request.getOffset(), request.getCount(), originalCount,\n          request.getStableHow(), request.getData(), channel, xid, false,\n          dataState);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n            + \" and requesed offset\u003d\" + offset);\n      }\n      if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n        // update the memory size\n        updateNonSequentialWriteInMemory(count);\n      }\n      // check if there is a WriteCtx with the same range in pendingWrites\n      WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n      if (oldWriteCtx \u003d\u003d null) {\n        addWrite(writeCtx);\n      } else {\n        LOG.warn(\"Got a repeated request, same range, with xid:\"\n            + writeCtx.getXid());\n      }\n      return writeCtx;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "28e3d09230971b32f74284311931525cb7ad1b7c": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4971. Move IO operations out of locking in OpenFileCtx. Contributed by Jing Zhao and Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525681 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/09/13 1:02 PM",
      "commitName": "28e3d09230971b32f74284311931525cb7ad1b7c",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,41 @@\n+  private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n+      Channel channel, int xid) {\n+    long offset \u003d request.getOffset();\n+    int count \u003d request.getCount();\n+    long cachedOffset \u003d nextOffset.get();\n+\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"requesed offset\u003d\" + offset + \" and current offset\u003d\"\n+          + cachedOffset);\n+    }\n+\n+    // Fail non-append call\n+    if (offset \u003c cachedOffset) {\n+      LOG.warn(\"(offset,count,nextOffset):\" + \"(\" + offset + \",\" + count + \",\"\n+          + nextOffset + \")\");\n+      return null;\n+    } else {\n+      DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n+          : WriteCtx.DataState.ALLOW_DUMP;\n+      WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n+          request.getOffset(), request.getCount(), request.getStableHow(),\n+          request.getData().array(), channel, xid, false, dataState);\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n+            + \" and requesed offset\u003d\" + offset);\n+      }\n+      if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n+        // update the memory size\n+        updateNonSequentialWriteInMemory(count);\n+      }\n+      // check if there is a WriteCtx with the same range in pendingWrites\n+      WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n+      if (oldWriteCtx \u003d\u003d null) {\n+        addWrite(writeCtx);\n+      } else {\n+        LOG.warn(\"Got a repeated request, same range, with xid:\"\n+            + writeCtx.getXid());\n+      }\n+      return writeCtx;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n      Channel channel, int xid) {\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    long cachedOffset \u003d nextOffset.get();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"requesed offset\u003d\" + offset + \" and current offset\u003d\"\n          + cachedOffset);\n    }\n\n    // Fail non-append call\n    if (offset \u003c cachedOffset) {\n      LOG.warn(\"(offset,count,nextOffset):\" + \"(\" + offset + \",\" + count + \",\"\n          + nextOffset + \")\");\n      return null;\n    } else {\n      DataState dataState \u003d offset \u003d\u003d cachedOffset ? WriteCtx.DataState.NO_DUMP\n          : WriteCtx.DataState.ALLOW_DUMP;\n      WriteCtx writeCtx \u003d new WriteCtx(request.getHandle(),\n          request.getOffset(), request.getCount(), request.getStableHow(),\n          request.getData().array(), channel, xid, false, dataState);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n            + \" and requesed offset\u003d\" + offset);\n      }\n      if (writeCtx.getDataState() \u003d\u003d WriteCtx.DataState.ALLOW_DUMP) {\n        // update the memory size\n        updateNonSequentialWriteInMemory(count);\n      }\n      // check if there is a WriteCtx with the same range in pendingWrites\n      WriteCtx oldWriteCtx \u003d checkRepeatedWriteRequest(request, channel, xid);\n      if (oldWriteCtx \u003d\u003d null) {\n        addWrite(writeCtx);\n      } else {\n        LOG.warn(\"Got a repeated request, same range, with xid:\"\n            + writeCtx.getXid());\n      }\n      return writeCtx;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java"
    }
  }
}