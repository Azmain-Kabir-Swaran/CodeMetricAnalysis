{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LocalReplicaInPipeline.java",
  "functionName": "createRestartMetaStream",
  "functionId": "createRestartMetaStream",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java",
  "functionStartLine": 367,
  "functionEndLine": 376,
  "numCommitsSeen": 24,
  "timeTaken": 3087,
  "changeHistory": [
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389",
    "86c9862bec0248d671e657aa56094a2919b8ac14",
    "b258b344bb76af6492828201959e36b45f0f75b8"
  ],
  "changeHistoryShort": {
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389": "Ybodychange",
    "86c9862bec0248d671e657aa56094a2919b8ac14": "Ymovefromfile",
    "b258b344bb76af6492828201959e36b45f0f75b8": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10958. Add instrumentation hooks around Datanode disk IO.\n",
      "commitDate": "14/12/16 11:18 AM",
      "commitName": "6ba9587d370fbf39c129c08c00ebbb894ccc1389",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "06/12/16 11:05 AM",
      "commitNameOld": "df983b524ab68ea0c70cee9033bfff2d28052cbf",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 8.01,
      "commitsBetweenForRepo": 51,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,10 @@\n   public OutputStream createRestartMetaStream() throws IOException {\n     File blockFile \u003d getBlockFile();\n     File restartMeta \u003d new File(blockFile.getParent()  +\n         File.pathSeparator + \".\" + blockFile.getName() + \".restart\");\n-    if (restartMeta.exists() \u0026\u0026 !restartMeta.delete()) {\n+    if (!getFileIoProvider().deleteWithExistsCheck(getVolume(), restartMeta)) {\n       DataNode.LOG.warn(\"Failed to delete restart meta file: \" +\n           restartMeta.getPath());\n     }\n-    return new FileOutputStream(restartMeta);\n+    return getFileIoProvider().getFileOutputStream(getVolume(), restartMeta);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public OutputStream createRestartMetaStream() throws IOException {\n    File blockFile \u003d getBlockFile();\n    File restartMeta \u003d new File(blockFile.getParent()  +\n        File.pathSeparator + \".\" + blockFile.getName() + \".restart\");\n    if (!getFileIoProvider().deleteWithExistsCheck(getVolume(), restartMeta)) {\n      DataNode.LOG.warn(\"Failed to delete restart meta file: \" +\n          restartMeta.getPath());\n    }\n    return getFileIoProvider().getFileOutputStream(getVolume(), restartMeta);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java",
      "extendedDetails": {}
    },
    "86c9862bec0248d671e657aa56094a2919b8ac14": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "13/09/16 12:54 PM",
      "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "13/09/16 12:42 PM",
      "commitNameOld": "1c0d18f32289837b8981aed80e7bdcd360adfb85",
      "commitAuthorOld": "Anu Engineer",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public OutputStream createRestartMetaStream() throws IOException {\n    File blockFile \u003d getBlockFile();\n    File restartMeta \u003d new File(blockFile.getParent()  +\n        File.pathSeparator + \".\" + blockFile.getName() + \".restart\");\n    if (restartMeta.exists() \u0026\u0026 !restartMeta.delete()) {\n      DataNode.LOG.warn(\"Failed to delete restart meta file: \" +\n          restartMeta.getPath());\n    }\n    return new FileOutputStream(restartMeta);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java",
        "oldMethodName": "createRestartMetaStream",
        "newMethodName": "createRestartMetaStream"
      }
    },
    "b258b344bb76af6492828201959e36b45f0f75b8": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8573. Move creation of restartMeta file logic from BlockReceiver to ReplicaInPipeline. Contributed by Eddy Xu.\n",
      "commitDate": "11/06/15 10:12 AM",
      "commitName": "b258b344bb76af6492828201959e36b45f0f75b8",
      "commitAuthor": "Andrew Wang",
      "diff": "@@ -0,0 +1,10 @@\n+  public OutputStream createRestartMetaStream() throws IOException {\n+    File blockFile \u003d getBlockFile();\n+    File restartMeta \u003d new File(blockFile.getParent()  +\n+        File.pathSeparator + \".\" + blockFile.getName() + \".restart\");\n+    if (restartMeta.exists() \u0026\u0026 !restartMeta.delete()) {\n+      DataNode.LOG.warn(\"Failed to delete restart meta file: \" +\n+          restartMeta.getPath());\n+    }\n+    return new FileOutputStream(restartMeta);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public OutputStream createRestartMetaStream() throws IOException {\n    File blockFile \u003d getBlockFile();\n    File restartMeta \u003d new File(blockFile.getParent()  +\n        File.pathSeparator + \".\" + blockFile.getName() + \".restart\");\n    if (restartMeta.exists() \u0026\u0026 !restartMeta.delete()) {\n      DataNode.LOG.warn(\"Failed to delete restart meta file: \" +\n          restartMeta.getPath());\n    }\n    return new FileOutputStream(restartMeta);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline.java"
    }
  }
}