{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataNode.java",
  "functionName": "handleDiskError",
  "functionId": "handleDiskError___failedVolumes-String__failedNumber-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
  "functionStartLine": 2220,
  "functionEndLine": 2244,
  "numCommitsSeen": 712,
  "timeTaken": 3839,
  "changeHistory": [
    "6191d4b4a0919863fda78e549ab6c60022e3ebc2",
    "dfcb331ba3516264398121c9f23af3a79c0509cc"
  ],
  "changeHistoryShort": {
    "6191d4b4a0919863fda78e549ab6c60022e3ebc2": "Ymultichange(Yparameterchange,Ybodychange)",
    "dfcb331ba3516264398121c9f23af3a79c0509cc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6191d4b4a0919863fda78e549ab6c60022e3ebc2": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-15158. The number of failed volumes mismatch with volumeFailures of Datanode metrics. Contributed by Yang Yun.\n",
      "commitDate": "09/02/20 10:02 AM",
      "commitName": "6191d4b4a0919863fda78e549ab6c60022e3ebc2",
      "commitAuthor": "Ayush Saxena",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-15158. The number of failed volumes mismatch with volumeFailures of Datanode metrics. Contributed by Yang Yun.\n",
          "commitDate": "09/02/20 10:02 AM",
          "commitName": "6191d4b4a0919863fda78e549ab6c60022e3ebc2",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "07/02/20 1:21 AM",
          "commitNameOld": "7dac7e1d13eaf0eac04fe805c7502dcecd597979",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 2.36,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n-  private void handleDiskError(String failedVolumes) {\n+  private void handleDiskError(String failedVolumes, int failedNumber) {\n     final boolean hasEnoughResources \u003d data.hasEnoughResource();\n     LOG.warn(\"DataNode.handleDiskError on: \" +\n         \"[{}] Keep Running: {}\", failedVolumes, hasEnoughResources);\n     \n     // If we have enough active valid volumes then we do not want to \n     // shutdown the DN completely.\n     int dpError \u003d hasEnoughResources ? DatanodeProtocol.DISK_ERROR  \n                                      : DatanodeProtocol.FATAL_DISK_ERROR;  \n-    metrics.incrVolumeFailures();\n+    metrics.incrVolumeFailures(failedNumber);\n \n     //inform NameNodes\n     for(BPOfferService bpos: blockPoolManager.getAllNamenodeThreads()) {\n       bpos.trySendErrorReport(dpError, failedVolumes);\n     }\n     \n     if(hasEnoughResources) {\n       scheduleAllBlockReport(0);\n       return; // do not shutdown\n     }\n     \n     LOG.warn(\"DataNode is shutting down due to failed volumes: [\"\n         + failedVolumes + \"]\");\n     shouldRun \u003d false;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void handleDiskError(String failedVolumes, int failedNumber) {\n    final boolean hasEnoughResources \u003d data.hasEnoughResource();\n    LOG.warn(\"DataNode.handleDiskError on: \" +\n        \"[{}] Keep Running: {}\", failedVolumes, hasEnoughResources);\n    \n    // If we have enough active valid volumes then we do not want to \n    // shutdown the DN completely.\n    int dpError \u003d hasEnoughResources ? DatanodeProtocol.DISK_ERROR  \n                                     : DatanodeProtocol.FATAL_DISK_ERROR;  \n    metrics.incrVolumeFailures(failedNumber);\n\n    //inform NameNodes\n    for(BPOfferService bpos: blockPoolManager.getAllNamenodeThreads()) {\n      bpos.trySendErrorReport(dpError, failedVolumes);\n    }\n    \n    if(hasEnoughResources) {\n      scheduleAllBlockReport(0);\n      return; // do not shutdown\n    }\n    \n    LOG.warn(\"DataNode is shutting down due to failed volumes: [\"\n        + failedVolumes + \"]\");\n    shouldRun \u003d false;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
          "extendedDetails": {
            "oldValue": "[failedVolumes-String]",
            "newValue": "[failedVolumes-String, failedNumber-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-15158. The number of failed volumes mismatch with volumeFailures of Datanode metrics. Contributed by Yang Yun.\n",
          "commitDate": "09/02/20 10:02 AM",
          "commitName": "6191d4b4a0919863fda78e549ab6c60022e3ebc2",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "07/02/20 1:21 AM",
          "commitNameOld": "7dac7e1d13eaf0eac04fe805c7502dcecd597979",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 2.36,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n-  private void handleDiskError(String failedVolumes) {\n+  private void handleDiskError(String failedVolumes, int failedNumber) {\n     final boolean hasEnoughResources \u003d data.hasEnoughResource();\n     LOG.warn(\"DataNode.handleDiskError on: \" +\n         \"[{}] Keep Running: {}\", failedVolumes, hasEnoughResources);\n     \n     // If we have enough active valid volumes then we do not want to \n     // shutdown the DN completely.\n     int dpError \u003d hasEnoughResources ? DatanodeProtocol.DISK_ERROR  \n                                      : DatanodeProtocol.FATAL_DISK_ERROR;  \n-    metrics.incrVolumeFailures();\n+    metrics.incrVolumeFailures(failedNumber);\n \n     //inform NameNodes\n     for(BPOfferService bpos: blockPoolManager.getAllNamenodeThreads()) {\n       bpos.trySendErrorReport(dpError, failedVolumes);\n     }\n     \n     if(hasEnoughResources) {\n       scheduleAllBlockReport(0);\n       return; // do not shutdown\n     }\n     \n     LOG.warn(\"DataNode is shutting down due to failed volumes: [\"\n         + failedVolumes + \"]\");\n     shouldRun \u003d false;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void handleDiskError(String failedVolumes, int failedNumber) {\n    final boolean hasEnoughResources \u003d data.hasEnoughResource();\n    LOG.warn(\"DataNode.handleDiskError on: \" +\n        \"[{}] Keep Running: {}\", failedVolumes, hasEnoughResources);\n    \n    // If we have enough active valid volumes then we do not want to \n    // shutdown the DN completely.\n    int dpError \u003d hasEnoughResources ? DatanodeProtocol.DISK_ERROR  \n                                     : DatanodeProtocol.FATAL_DISK_ERROR;  \n    metrics.incrVolumeFailures(failedNumber);\n\n    //inform NameNodes\n    for(BPOfferService bpos: blockPoolManager.getAllNamenodeThreads()) {\n      bpos.trySendErrorReport(dpError, failedVolumes);\n    }\n    \n    if(hasEnoughResources) {\n      scheduleAllBlockReport(0);\n      return; // do not shutdown\n    }\n    \n    LOG.warn(\"DataNode is shutting down due to failed volumes: [\"\n        + failedVolumes + \"]\");\n    shouldRun \u003d false;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
          "extendedDetails": {}
        }
      ]
    },
    "dfcb331ba3516264398121c9f23af3a79c0509cc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13076: [SPS]: Addendum. Resolve conflicts after rebasing branch to trunk. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "dfcb331ba3516264398121c9f23af3a79c0509cc",
      "commitAuthor": "Rakesh Radhakrishnan",
      "diff": "@@ -0,0 +1,25 @@\n+  private void handleDiskError(String failedVolumes) {\n+    final boolean hasEnoughResources \u003d data.hasEnoughResource();\n+    LOG.warn(\"DataNode.handleDiskError on: \" +\n+        \"[{}] Keep Running: {}\", failedVolumes, hasEnoughResources);\n+    \n+    // If we have enough active valid volumes then we do not want to \n+    // shutdown the DN completely.\n+    int dpError \u003d hasEnoughResources ? DatanodeProtocol.DISK_ERROR  \n+                                     : DatanodeProtocol.FATAL_DISK_ERROR;  \n+    metrics.incrVolumeFailures();\n+\n+    //inform NameNodes\n+    for(BPOfferService bpos: blockPoolManager.getAllNamenodeThreads()) {\n+      bpos.trySendErrorReport(dpError, failedVolumes);\n+    }\n+    \n+    if(hasEnoughResources) {\n+      scheduleAllBlockReport(0);\n+      return; // do not shutdown\n+    }\n+    \n+    LOG.warn(\"DataNode is shutting down due to failed volumes: [\"\n+        + failedVolumes + \"]\");\n+    shouldRun \u003d false;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void handleDiskError(String failedVolumes) {\n    final boolean hasEnoughResources \u003d data.hasEnoughResource();\n    LOG.warn(\"DataNode.handleDiskError on: \" +\n        \"[{}] Keep Running: {}\", failedVolumes, hasEnoughResources);\n    \n    // If we have enough active valid volumes then we do not want to \n    // shutdown the DN completely.\n    int dpError \u003d hasEnoughResources ? DatanodeProtocol.DISK_ERROR  \n                                     : DatanodeProtocol.FATAL_DISK_ERROR;  \n    metrics.incrVolumeFailures();\n\n    //inform NameNodes\n    for(BPOfferService bpos: blockPoolManager.getAllNamenodeThreads()) {\n      bpos.trySendErrorReport(dpError, failedVolumes);\n    }\n    \n    if(hasEnoughResources) {\n      scheduleAllBlockReport(0);\n      return; // do not shutdown\n    }\n    \n    LOG.warn(\"DataNode is shutting down due to failed volumes: [\"\n        + failedVolumes + \"]\");\n    shouldRun \u003d false;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
    }
  }
}