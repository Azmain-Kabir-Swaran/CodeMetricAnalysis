{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "JsonUtilClient.java",
  "functionName": "toDatanodeInfo",
  "functionId": "toDatanodeInfo___m-Map__?,?__(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java",
  "functionStartLine": 291,
  "functionEndLine": 351,
  "numCommitsSeen": 92,
  "timeTaken": 5377,
  "changeHistory": [
    "b5adc5c3011f111f86d232cb33ec522547f68a95",
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
    "ab04ff9efe632b4eca6faca7407ac35e00e6a379",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "e2262d3d18c6d5c2aa20f96920104dc07271b869",
    "b4d6c5823b04b2a8834e06e78cd109a359496eed",
    "dc2ee20aec7b3fe1d13c846926ba1b0f02c5adef",
    "82ed72d1d4f7263449ee75ebbd7d668be9f3bdc1",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
    "8e0804666189ce9a66b7b41b744776bad29770dd",
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8",
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
    "112c32415175f637a2791f2207c20393fc9ba740",
    "be7dd8333a7e56e732171db0781786987de03195",
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
    "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de"
  ],
  "changeHistoryShort": {
    "b5adc5c3011f111f86d232cb33ec522547f68a95": "Ybodychange",
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d": "Ybodychange",
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501": "Ybodychange",
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35": "Yfilerename",
    "ab04ff9efe632b4eca6faca7407ac35e00e6a379": "Ymultichange(Ymovefromfile,Ybodychange)",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "e2262d3d18c6d5c2aa20f96920104dc07271b869": "Ybodychange",
    "b4d6c5823b04b2a8834e06e78cd109a359496eed": "Ymultichange(Yexceptionschange,Ybodychange)",
    "dc2ee20aec7b3fe1d13c846926ba1b0f02c5adef": "Ybodychange",
    "82ed72d1d4f7263449ee75ebbd7d668be9f3bdc1": "Ymultichange(Ymodifierchange,Ybodychange)",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": "Ybodychange",
    "8e0804666189ce9a66b7b41b744776bad29770dd": "Ybodychange",
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8": "Ybodychange",
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0": "Ybodychange",
    "112c32415175f637a2791f2207c20393fc9ba740": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201": "Ybodychange",
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57": "Ymodifierchange",
    "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b5adc5c3011f111f86d232cb33ec522547f68a95": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10838. Last full block report received time for each DN should be easily discoverable. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "06/03/17 4:39 PM",
      "commitName": "b5adc5c3011f111f86d232cb33ec522547f68a95",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "03/01/17 9:58 AM",
      "commitNameOld": "7fcc73fc0d248aae1edbd4e1514c5818f6198928",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 62.28,
      "commitsBetweenForRepo": 330,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,61 @@\n   static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m)\n       throws IOException {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     // ipAddr and xferPort are the critical fields for accessing data.\n     // If any one of the two is missing, an exception needs to be thrown.\n \n     // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n     //  of ipAddr and xferPort.\n     String ipAddr \u003d getString(m, \"ipAddr\", null);\n     int xferPort \u003d getInt(m, \"xferPort\", -1);\n     if (ipAddr \u003d\u003d null) {\n       String name \u003d getString(m, \"name\", null);\n       if (name !\u003d null) {\n         int colonIdx \u003d name.indexOf(\u0027:\u0027);\n         if (colonIdx \u003e 0) {\n           ipAddr \u003d name.substring(0, colonIdx);\n           xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n         } else {\n           throw new IOException(\n               \"Invalid value in server response: name\u003d[\" + name + \"]\");\n         }\n       } else {\n         throw new IOException(\n             \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n       }\n       // ipAddr is non-null \u0026 non-empty string at this point.\n     }\n \n     // Check the validity of xferPort.\n     if (xferPort \u003d\u003d -1) {\n       throw new IOException(\n           \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n     }\n \n     // TODO: Fix storageID\n     return new DatanodeInfoBuilder().setIpAddr(ipAddr)\n         .setHostName((String) m.get(\"hostName\"))\n         .setDatanodeUuid((String) m.get(\"storageID\")).setXferPort(xferPort)\n         .setInfoPort(((Number) m.get(\"infoPort\")).intValue())\n         .setInfoSecurePort(getInt(m, \"infoSecurePort\", 0))\n         .setIpcPort(((Number) m.get(\"ipcPort\")).intValue())\n         .setCapacity(getLong(m, \"capacity\", 0L))\n         .setDfsUsed(getLong(m, \"dfsUsed\", 0L))\n         .setRemaining(getLong(m, \"remaining\", 0L))\n         .setBlockPoolUsed(getLong(m, \"blockPoolUsed\", 0L))\n         .setCacheCapacity(getLong(m, \"cacheCapacity\", 0L))\n         .setCacheUsed(getLong(m, \"cacheUsed\", 0L))\n         .setLastUpdate(getLong(m, \"lastUpdate\", 0L))\n         .setLastUpdateMonotonic(getLong(m, \"lastUpdateMonotonic\", 0L))\n         .setXceiverCount(getInt(m, \"xceiverCount\", 0))\n         .setNetworkLocation(getString(m, \"networkLocation\", \"\")).setAdminState(\n             DatanodeInfo.AdminStates\n                 .valueOf(getString(m, \"adminState\", \"NORMAL\")))\n         .setUpgradeDomain(getString(m, \"upgradeDomain\", \"\"))\n+        .setLastBlockReportTime(getLong(m, \"lastBlockReportTime\", 0L))\n+        .setLastBlockReportMonotonic(getLong(m, \"lastBlockReportMonotonic\", 0L))\n         .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m)\n      throws IOException {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    // ipAddr and xferPort are the critical fields for accessing data.\n    // If any one of the two is missing, an exception needs to be thrown.\n\n    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n    //  of ipAddr and xferPort.\n    String ipAddr \u003d getString(m, \"ipAddr\", null);\n    int xferPort \u003d getInt(m, \"xferPort\", -1);\n    if (ipAddr \u003d\u003d null) {\n      String name \u003d getString(m, \"name\", null);\n      if (name !\u003d null) {\n        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n        if (colonIdx \u003e 0) {\n          ipAddr \u003d name.substring(0, colonIdx);\n          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n        } else {\n          throw new IOException(\n              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n        }\n      } else {\n        throw new IOException(\n            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n      }\n      // ipAddr is non-null \u0026 non-empty string at this point.\n    }\n\n    // Check the validity of xferPort.\n    if (xferPort \u003d\u003d -1) {\n      throw new IOException(\n          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n    }\n\n    // TODO: Fix storageID\n    return new DatanodeInfoBuilder().setIpAddr(ipAddr)\n        .setHostName((String) m.get(\"hostName\"))\n        .setDatanodeUuid((String) m.get(\"storageID\")).setXferPort(xferPort)\n        .setInfoPort(((Number) m.get(\"infoPort\")).intValue())\n        .setInfoSecurePort(getInt(m, \"infoSecurePort\", 0))\n        .setIpcPort(((Number) m.get(\"ipcPort\")).intValue())\n        .setCapacity(getLong(m, \"capacity\", 0L))\n        .setDfsUsed(getLong(m, \"dfsUsed\", 0L))\n        .setRemaining(getLong(m, \"remaining\", 0L))\n        .setBlockPoolUsed(getLong(m, \"blockPoolUsed\", 0L))\n        .setCacheCapacity(getLong(m, \"cacheCapacity\", 0L))\n        .setCacheUsed(getLong(m, \"cacheUsed\", 0L))\n        .setLastUpdate(getLong(m, \"lastUpdate\", 0L))\n        .setLastUpdateMonotonic(getLong(m, \"lastUpdateMonotonic\", 0L))\n        .setXceiverCount(getInt(m, \"xceiverCount\", 0))\n        .setNetworkLocation(getString(m, \"networkLocation\", \"\")).setAdminState(\n            DatanodeInfo.AdminStates\n                .valueOf(getString(m, \"adminState\", \"NORMAL\")))\n        .setUpgradeDomain(getString(m, \"upgradeDomain\", \"\"))\n        .setLastBlockReportTime(getLong(m, \"lastBlockReportTime\", 0L))\n        .setLastBlockReportMonotonic(getLong(m, \"lastBlockReportMonotonic\", 0L))\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java",
      "extendedDetails": {}
    },
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9482. Replace DatanodeInfo constructors with a builder pattern. Contributed by Brahma Reddy Battula.\n",
      "commitDate": "08/11/16 6:17 PM",
      "commitName": "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
      "commitAuthor": "Brahma Reddy Battula",
      "commitDateOld": "06/11/16 6:16 PM",
      "commitNameOld": "049e7d27bea13d4254baccf49401daae820b71df",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 2.0,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,59 @@\n   static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m)\n       throws IOException {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     // ipAddr and xferPort are the critical fields for accessing data.\n     // If any one of the two is missing, an exception needs to be thrown.\n \n     // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n     //  of ipAddr and xferPort.\n     String ipAddr \u003d getString(m, \"ipAddr\", null);\n     int xferPort \u003d getInt(m, \"xferPort\", -1);\n     if (ipAddr \u003d\u003d null) {\n       String name \u003d getString(m, \"name\", null);\n       if (name !\u003d null) {\n         int colonIdx \u003d name.indexOf(\u0027:\u0027);\n         if (colonIdx \u003e 0) {\n           ipAddr \u003d name.substring(0, colonIdx);\n           xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n         } else {\n           throw new IOException(\n               \"Invalid value in server response: name\u003d[\" + name + \"]\");\n         }\n       } else {\n         throw new IOException(\n             \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n       }\n       // ipAddr is non-null \u0026 non-empty string at this point.\n     }\n \n     // Check the validity of xferPort.\n     if (xferPort \u003d\u003d -1) {\n       throw new IOException(\n           \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n     }\n \n     // TODO: Fix storageID\n-    return new DatanodeInfo(\n-        ipAddr,\n-        (String)m.get(\"hostName\"),\n-        (String)m.get(\"storageID\"),\n-        xferPort,\n-        ((Number) m.get(\"infoPort\")).intValue(),\n-        getInt(m, \"infoSecurePort\", 0),\n-        ((Number) m.get(\"ipcPort\")).intValue(),\n-\n-        getLong(m, \"capacity\", 0l),\n-        getLong(m, \"dfsUsed\", 0l),\n-        getLong(m, \"remaining\", 0l),\n-        getLong(m, \"blockPoolUsed\", 0l),\n-        getLong(m, \"cacheCapacity\", 0l),\n-        getLong(m, \"cacheUsed\", 0l),\n-        getLong(m, \"lastUpdate\", 0l),\n-        getLong(m, \"lastUpdateMonotonic\", 0l),\n-        getInt(m, \"xceiverCount\", 0),\n-        getString(m, \"networkLocation\", \"\"),\n-        DatanodeInfo.AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")),\n-        getString(m, \"upgradeDomain\", \"\"));\n+    return new DatanodeInfoBuilder().setIpAddr(ipAddr)\n+        .setHostName((String) m.get(\"hostName\"))\n+        .setDatanodeUuid((String) m.get(\"storageID\")).setXferPort(xferPort)\n+        .setInfoPort(((Number) m.get(\"infoPort\")).intValue())\n+        .setInfoSecurePort(getInt(m, \"infoSecurePort\", 0))\n+        .setIpcPort(((Number) m.get(\"ipcPort\")).intValue())\n+        .setCapacity(getLong(m, \"capacity\", 0L))\n+        .setDfsUsed(getLong(m, \"dfsUsed\", 0L))\n+        .setRemaining(getLong(m, \"remaining\", 0L))\n+        .setBlockPoolUsed(getLong(m, \"blockPoolUsed\", 0L))\n+        .setCacheCapacity(getLong(m, \"cacheCapacity\", 0L))\n+        .setCacheUsed(getLong(m, \"cacheUsed\", 0L))\n+        .setLastUpdate(getLong(m, \"lastUpdate\", 0L))\n+        .setLastUpdateMonotonic(getLong(m, \"lastUpdateMonotonic\", 0L))\n+        .setXceiverCount(getInt(m, \"xceiverCount\", 0))\n+        .setNetworkLocation(getString(m, \"networkLocation\", \"\")).setAdminState(\n+            DatanodeInfo.AdminStates\n+                .valueOf(getString(m, \"adminState\", \"NORMAL\")))\n+        .setUpgradeDomain(getString(m, \"upgradeDomain\", \"\"))\n+        .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m)\n      throws IOException {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    // ipAddr and xferPort are the critical fields for accessing data.\n    // If any one of the two is missing, an exception needs to be thrown.\n\n    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n    //  of ipAddr and xferPort.\n    String ipAddr \u003d getString(m, \"ipAddr\", null);\n    int xferPort \u003d getInt(m, \"xferPort\", -1);\n    if (ipAddr \u003d\u003d null) {\n      String name \u003d getString(m, \"name\", null);\n      if (name !\u003d null) {\n        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n        if (colonIdx \u003e 0) {\n          ipAddr \u003d name.substring(0, colonIdx);\n          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n        } else {\n          throw new IOException(\n              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n        }\n      } else {\n        throw new IOException(\n            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n      }\n      // ipAddr is non-null \u0026 non-empty string at this point.\n    }\n\n    // Check the validity of xferPort.\n    if (xferPort \u003d\u003d -1) {\n      throw new IOException(\n          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n    }\n\n    // TODO: Fix storageID\n    return new DatanodeInfoBuilder().setIpAddr(ipAddr)\n        .setHostName((String) m.get(\"hostName\"))\n        .setDatanodeUuid((String) m.get(\"storageID\")).setXferPort(xferPort)\n        .setInfoPort(((Number) m.get(\"infoPort\")).intValue())\n        .setInfoSecurePort(getInt(m, \"infoSecurePort\", 0))\n        .setIpcPort(((Number) m.get(\"ipcPort\")).intValue())\n        .setCapacity(getLong(m, \"capacity\", 0L))\n        .setDfsUsed(getLong(m, \"dfsUsed\", 0L))\n        .setRemaining(getLong(m, \"remaining\", 0L))\n        .setBlockPoolUsed(getLong(m, \"blockPoolUsed\", 0L))\n        .setCacheCapacity(getLong(m, \"cacheCapacity\", 0L))\n        .setCacheUsed(getLong(m, \"cacheUsed\", 0L))\n        .setLastUpdate(getLong(m, \"lastUpdate\", 0L))\n        .setLastUpdateMonotonic(getLong(m, \"lastUpdateMonotonic\", 0L))\n        .setXceiverCount(getInt(m, \"xceiverCount\", 0))\n        .setNetworkLocation(getString(m, \"networkLocation\", \"\")).setAdminState(\n            DatanodeInfo.AdminStates\n                .valueOf(getString(m, \"adminState\", \"NORMAL\")))\n        .setUpgradeDomain(getString(m, \"upgradeDomain\", \"\"))\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java",
      "extendedDetails": {}
    },
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9004. Add upgrade domain to DatanodeInfo. Contributed by Ming Ma (via Lei (Eddy) Xu).\n\nChange-Id: I887c66578eebd61acc34b94f18da6e6851c609f4\n",
      "commitDate": "19/09/15 6:08 PM",
      "commitName": "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "06/07/15 3:39 AM",
      "commitNameOld": "fc92d3e6515a391847cb6170244b3d911712d96a",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 75.6,
      "commitsBetweenForRepo": 445,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,60 @@\n   static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m)\n     throws IOException {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     // ipAddr and xferPort are the critical fields for accessing data.\n     // If any one of the two is missing, an exception needs to be thrown.\n \n     // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n     //  of ipAddr and xferPort.\n     String ipAddr \u003d getString(m, \"ipAddr\", null);\n     int xferPort \u003d getInt(m, \"xferPort\", -1);\n     if (ipAddr \u003d\u003d null) {\n       String name \u003d getString(m, \"name\", null);\n       if (name !\u003d null) {\n         int colonIdx \u003d name.indexOf(\u0027:\u0027);\n         if (colonIdx \u003e 0) {\n           ipAddr \u003d name.substring(0, colonIdx);\n           xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n         } else {\n           throw new IOException(\n               \"Invalid value in server response: name\u003d[\" + name + \"]\");\n         }\n       } else {\n         throw new IOException(\n             \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n       }\n       // ipAddr is non-null \u0026 non-empty string at this point.\n     }\n \n     // Check the validity of xferPort.\n     if (xferPort \u003d\u003d -1) {\n       throw new IOException(\n           \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n     }\n \n     // TODO: Fix storageID\n     return new DatanodeInfo(\n         ipAddr,\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         xferPort,\n         ((Number) m.get(\"infoPort\")).intValue(),\n         getInt(m, \"infoSecurePort\", 0),\n         ((Number) m.get(\"ipcPort\")).intValue(),\n \n         getLong(m, \"capacity\", 0l),\n         getLong(m, \"dfsUsed\", 0l),\n         getLong(m, \"remaining\", 0l),\n         getLong(m, \"blockPoolUsed\", 0l),\n         getLong(m, \"cacheCapacity\", 0l),\n         getLong(m, \"cacheUsed\", 0l),\n         getLong(m, \"lastUpdate\", 0l),\n         getLong(m, \"lastUpdateMonotonic\", 0l),\n         getInt(m, \"xceiverCount\", 0),\n         getString(m, \"networkLocation\", \"\"),\n-        DatanodeInfo.AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n+        DatanodeInfo.AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")),\n+        getString(m, \"upgradeDomain\", \"\"));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m)\n    throws IOException {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    // ipAddr and xferPort are the critical fields for accessing data.\n    // If any one of the two is missing, an exception needs to be thrown.\n\n    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n    //  of ipAddr and xferPort.\n    String ipAddr \u003d getString(m, \"ipAddr\", null);\n    int xferPort \u003d getInt(m, \"xferPort\", -1);\n    if (ipAddr \u003d\u003d null) {\n      String name \u003d getString(m, \"name\", null);\n      if (name !\u003d null) {\n        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n        if (colonIdx \u003e 0) {\n          ipAddr \u003d name.substring(0, colonIdx);\n          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n        } else {\n          throw new IOException(\n              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n        }\n      } else {\n        throw new IOException(\n            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n      }\n      // ipAddr is non-null \u0026 non-empty string at this point.\n    }\n\n    // Check the validity of xferPort.\n    if (xferPort \u003d\u003d -1) {\n      throw new IOException(\n          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n    }\n\n    // TODO: Fix storageID\n    return new DatanodeInfo(\n        ipAddr,\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        xferPort,\n        ((Number) m.get(\"infoPort\")).intValue(),\n        getInt(m, \"infoSecurePort\", 0),\n        ((Number) m.get(\"ipcPort\")).intValue(),\n\n        getLong(m, \"capacity\", 0l),\n        getLong(m, \"dfsUsed\", 0l),\n        getLong(m, \"remaining\", 0l),\n        getLong(m, \"blockPoolUsed\", 0l),\n        getLong(m, \"cacheCapacity\", 0l),\n        getLong(m, \"cacheUsed\", 0l),\n        getLong(m, \"lastUpdate\", 0l),\n        getLong(m, \"lastUpdateMonotonic\", 0l),\n        getInt(m, \"xceiverCount\", 0),\n        getString(m, \"networkLocation\", \"\"),\n        DatanodeInfo.AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")),\n        getString(m, \"upgradeDomain\", \"\"));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java",
      "extendedDetails": {}
    },
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8052. Move WebHdfsFileSystem into hadoop-hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "23/04/15 5:33 PM",
      "commitName": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "23/04/15 4:40 PM",
      "commitNameOld": "0b3f8957a87ada1a275c9904b211fdbdcefafb02",
      "commitAuthorOld": "Xuan",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m)\n    throws IOException {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    // ipAddr and xferPort are the critical fields for accessing data.\n    // If any one of the two is missing, an exception needs to be thrown.\n\n    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n    //  of ipAddr and xferPort.\n    String ipAddr \u003d getString(m, \"ipAddr\", null);\n    int xferPort \u003d getInt(m, \"xferPort\", -1);\n    if (ipAddr \u003d\u003d null) {\n      String name \u003d getString(m, \"name\", null);\n      if (name !\u003d null) {\n        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n        if (colonIdx \u003e 0) {\n          ipAddr \u003d name.substring(0, colonIdx);\n          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n        } else {\n          throw new IOException(\n              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n        }\n      } else {\n        throw new IOException(\n            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n      }\n      // ipAddr is non-null \u0026 non-empty string at this point.\n    }\n\n    // Check the validity of xferPort.\n    if (xferPort \u003d\u003d -1) {\n      throw new IOException(\n          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n    }\n\n    // TODO: Fix storageID\n    return new DatanodeInfo(\n        ipAddr,\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        xferPort,\n        ((Number) m.get(\"infoPort\")).intValue(),\n        getInt(m, \"infoSecurePort\", 0),\n        ((Number) m.get(\"ipcPort\")).intValue(),\n\n        getLong(m, \"capacity\", 0l),\n        getLong(m, \"dfsUsed\", 0l),\n        getLong(m, \"remaining\", 0l),\n        getLong(m, \"blockPoolUsed\", 0l),\n        getLong(m, \"cacheCapacity\", 0l),\n        getLong(m, \"cacheUsed\", 0l),\n        getLong(m, \"lastUpdate\", 0l),\n        getLong(m, \"lastUpdateMonotonic\", 0l),\n        getInt(m, \"xceiverCount\", 0),\n        getString(m, \"networkLocation\", \"\"),\n        DatanodeInfo.AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java"
      }
    },
    "ab04ff9efe632b4eca6faca7407ac35e00e6a379": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-8080. Separate JSON related routines used by WebHdfsFileSystem to a package local class. Contributed by Haohui Mai.\n",
      "commitDate": "07/04/15 9:30 PM",
      "commitName": "ab04ff9efe632b4eca6faca7407ac35e00e6a379",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-8080. Separate JSON related routines used by WebHdfsFileSystem to a package local class. Contributed by Haohui Mai.\n",
          "commitDate": "07/04/15 9:30 PM",
          "commitName": "ab04ff9efe632b4eca6faca7407ac35e00e6a379",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "07/04/15 9:26 PM",
          "commitNameOld": "d505c8acd30d6f40d0632fe9c93c886a4499a9fc",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,59 +1,59 @@\n-  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) \n+  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m)\n     throws IOException {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     // ipAddr and xferPort are the critical fields for accessing data.\n     // If any one of the two is missing, an exception needs to be thrown.\n \n     // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n     //  of ipAddr and xferPort.\n     String ipAddr \u003d getString(m, \"ipAddr\", null);\n     int xferPort \u003d getInt(m, \"xferPort\", -1);\n     if (ipAddr \u003d\u003d null) {\n       String name \u003d getString(m, \"name\", null);\n       if (name !\u003d null) {\n         int colonIdx \u003d name.indexOf(\u0027:\u0027);\n         if (colonIdx \u003e 0) {\n           ipAddr \u003d name.substring(0, colonIdx);\n           xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n         } else {\n           throw new IOException(\n               \"Invalid value in server response: name\u003d[\" + name + \"]\");\n         }\n       } else {\n         throw new IOException(\n             \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n       }\n       // ipAddr is non-null \u0026 non-empty string at this point.\n     }\n \n     // Check the validity of xferPort.\n     if (xferPort \u003d\u003d -1) {\n       throw new IOException(\n           \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n     }\n \n     // TODO: Fix storageID\n     return new DatanodeInfo(\n         ipAddr,\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         xferPort,\n         ((Number) m.get(\"infoPort\")).intValue(),\n         getInt(m, \"infoSecurePort\", 0),\n         ((Number) m.get(\"ipcPort\")).intValue(),\n \n         getLong(m, \"capacity\", 0l),\n         getLong(m, \"dfsUsed\", 0l),\n         getLong(m, \"remaining\", 0l),\n         getLong(m, \"blockPoolUsed\", 0l),\n         getLong(m, \"cacheCapacity\", 0l),\n         getLong(m, \"cacheUsed\", 0l),\n         getLong(m, \"lastUpdate\", 0l),\n         getLong(m, \"lastUpdateMonotonic\", 0l),\n         getInt(m, \"xceiverCount\", 0),\n         getString(m, \"networkLocation\", \"\"),\n-        AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n+        DatanodeInfo.AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m)\n    throws IOException {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    // ipAddr and xferPort are the critical fields for accessing data.\n    // If any one of the two is missing, an exception needs to be thrown.\n\n    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n    //  of ipAddr and xferPort.\n    String ipAddr \u003d getString(m, \"ipAddr\", null);\n    int xferPort \u003d getInt(m, \"xferPort\", -1);\n    if (ipAddr \u003d\u003d null) {\n      String name \u003d getString(m, \"name\", null);\n      if (name !\u003d null) {\n        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n        if (colonIdx \u003e 0) {\n          ipAddr \u003d name.substring(0, colonIdx);\n          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n        } else {\n          throw new IOException(\n              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n        }\n      } else {\n        throw new IOException(\n            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n      }\n      // ipAddr is non-null \u0026 non-empty string at this point.\n    }\n\n    // Check the validity of xferPort.\n    if (xferPort \u003d\u003d -1) {\n      throw new IOException(\n          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n    }\n\n    // TODO: Fix storageID\n    return new DatanodeInfo(\n        ipAddr,\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        xferPort,\n        ((Number) m.get(\"infoPort\")).intValue(),\n        getInt(m, \"infoSecurePort\", 0),\n        ((Number) m.get(\"ipcPort\")).intValue(),\n\n        getLong(m, \"capacity\", 0l),\n        getLong(m, \"dfsUsed\", 0l),\n        getLong(m, \"remaining\", 0l),\n        getLong(m, \"blockPoolUsed\", 0l),\n        getLong(m, \"cacheCapacity\", 0l),\n        getLong(m, \"cacheUsed\", 0l),\n        getLong(m, \"lastUpdate\", 0l),\n        getLong(m, \"lastUpdateMonotonic\", 0l),\n        getInt(m, \"xceiverCount\", 0),\n        getString(m, \"networkLocation\", \"\"),\n        DatanodeInfo.AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java",
            "oldMethodName": "toDatanodeInfo",
            "newMethodName": "toDatanodeInfo"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8080. Separate JSON related routines used by WebHdfsFileSystem to a package local class. Contributed by Haohui Mai.\n",
          "commitDate": "07/04/15 9:30 PM",
          "commitName": "ab04ff9efe632b4eca6faca7407ac35e00e6a379",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "07/04/15 9:26 PM",
          "commitNameOld": "d505c8acd30d6f40d0632fe9c93c886a4499a9fc",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,59 +1,59 @@\n-  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) \n+  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m)\n     throws IOException {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     // ipAddr and xferPort are the critical fields for accessing data.\n     // If any one of the two is missing, an exception needs to be thrown.\n \n     // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n     //  of ipAddr and xferPort.\n     String ipAddr \u003d getString(m, \"ipAddr\", null);\n     int xferPort \u003d getInt(m, \"xferPort\", -1);\n     if (ipAddr \u003d\u003d null) {\n       String name \u003d getString(m, \"name\", null);\n       if (name !\u003d null) {\n         int colonIdx \u003d name.indexOf(\u0027:\u0027);\n         if (colonIdx \u003e 0) {\n           ipAddr \u003d name.substring(0, colonIdx);\n           xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n         } else {\n           throw new IOException(\n               \"Invalid value in server response: name\u003d[\" + name + \"]\");\n         }\n       } else {\n         throw new IOException(\n             \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n       }\n       // ipAddr is non-null \u0026 non-empty string at this point.\n     }\n \n     // Check the validity of xferPort.\n     if (xferPort \u003d\u003d -1) {\n       throw new IOException(\n           \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n     }\n \n     // TODO: Fix storageID\n     return new DatanodeInfo(\n         ipAddr,\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         xferPort,\n         ((Number) m.get(\"infoPort\")).intValue(),\n         getInt(m, \"infoSecurePort\", 0),\n         ((Number) m.get(\"ipcPort\")).intValue(),\n \n         getLong(m, \"capacity\", 0l),\n         getLong(m, \"dfsUsed\", 0l),\n         getLong(m, \"remaining\", 0l),\n         getLong(m, \"blockPoolUsed\", 0l),\n         getLong(m, \"cacheCapacity\", 0l),\n         getLong(m, \"cacheUsed\", 0l),\n         getLong(m, \"lastUpdate\", 0l),\n         getLong(m, \"lastUpdateMonotonic\", 0l),\n         getInt(m, \"xceiverCount\", 0),\n         getString(m, \"networkLocation\", \"\"),\n-        AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n+        DatanodeInfo.AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m)\n    throws IOException {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    // ipAddr and xferPort are the critical fields for accessing data.\n    // If any one of the two is missing, an exception needs to be thrown.\n\n    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n    //  of ipAddr and xferPort.\n    String ipAddr \u003d getString(m, \"ipAddr\", null);\n    int xferPort \u003d getInt(m, \"xferPort\", -1);\n    if (ipAddr \u003d\u003d null) {\n      String name \u003d getString(m, \"name\", null);\n      if (name !\u003d null) {\n        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n        if (colonIdx \u003e 0) {\n          ipAddr \u003d name.substring(0, colonIdx);\n          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n        } else {\n          throw new IOException(\n              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n        }\n      } else {\n        throw new IOException(\n            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n      }\n      // ipAddr is non-null \u0026 non-empty string at this point.\n    }\n\n    // Check the validity of xferPort.\n    if (xferPort \u003d\u003d -1) {\n      throw new IOException(\n          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n    }\n\n    // TODO: Fix storageID\n    return new DatanodeInfo(\n        ipAddr,\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        xferPort,\n        ((Number) m.get(\"infoPort\")).intValue(),\n        getInt(m, \"infoSecurePort\", 0),\n        ((Number) m.get(\"ipcPort\")).intValue(),\n\n        getLong(m, \"capacity\", 0l),\n        getLong(m, \"dfsUsed\", 0l),\n        getLong(m, \"remaining\", 0l),\n        getLong(m, \"blockPoolUsed\", 0l),\n        getLong(m, \"cacheCapacity\", 0l),\n        getLong(m, \"cacheUsed\", 0l),\n        getLong(m, \"lastUpdate\", 0l),\n        getLong(m, \"lastUpdateMonotonic\", 0l),\n        getInt(m, \"xceiverCount\", 0),\n        getString(m, \"networkLocation\", \"\"),\n        DatanodeInfo.AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "03/03/15 5:54 PM",
      "commitNameOld": "e2262d3d18c6d5c2aa20f96920104dc07271b869",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 16.71,
      "commitsBetweenForRepo": 150,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,59 @@\n   static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) \n     throws IOException {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     // ipAddr and xferPort are the critical fields for accessing data.\n     // If any one of the two is missing, an exception needs to be thrown.\n \n     // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n     //  of ipAddr and xferPort.\n     String ipAddr \u003d getString(m, \"ipAddr\", null);\n     int xferPort \u003d getInt(m, \"xferPort\", -1);\n     if (ipAddr \u003d\u003d null) {\n       String name \u003d getString(m, \"name\", null);\n       if (name !\u003d null) {\n         int colonIdx \u003d name.indexOf(\u0027:\u0027);\n         if (colonIdx \u003e 0) {\n           ipAddr \u003d name.substring(0, colonIdx);\n           xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n         } else {\n           throw new IOException(\n               \"Invalid value in server response: name\u003d[\" + name + \"]\");\n         }\n       } else {\n         throw new IOException(\n             \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n       }\n       // ipAddr is non-null \u0026 non-empty string at this point.\n     }\n \n     // Check the validity of xferPort.\n     if (xferPort \u003d\u003d -1) {\n       throw new IOException(\n           \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n     }\n \n     // TODO: Fix storageID\n     return new DatanodeInfo(\n         ipAddr,\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         xferPort,\n         ((Number) m.get(\"infoPort\")).intValue(),\n         getInt(m, \"infoSecurePort\", 0),\n         ((Number) m.get(\"ipcPort\")).intValue(),\n \n         getLong(m, \"capacity\", 0l),\n         getLong(m, \"dfsUsed\", 0l),\n         getLong(m, \"remaining\", 0l),\n         getLong(m, \"blockPoolUsed\", 0l),\n         getLong(m, \"cacheCapacity\", 0l),\n         getLong(m, \"cacheUsed\", 0l),\n         getLong(m, \"lastUpdate\", 0l),\n+        getLong(m, \"lastUpdateMonotonic\", 0l),\n         getInt(m, \"xceiverCount\", 0),\n         getString(m, \"networkLocation\", \"\"),\n         AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) \n    throws IOException {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    // ipAddr and xferPort are the critical fields for accessing data.\n    // If any one of the two is missing, an exception needs to be thrown.\n\n    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n    //  of ipAddr and xferPort.\n    String ipAddr \u003d getString(m, \"ipAddr\", null);\n    int xferPort \u003d getInt(m, \"xferPort\", -1);\n    if (ipAddr \u003d\u003d null) {\n      String name \u003d getString(m, \"name\", null);\n      if (name !\u003d null) {\n        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n        if (colonIdx \u003e 0) {\n          ipAddr \u003d name.substring(0, colonIdx);\n          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n        } else {\n          throw new IOException(\n              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n        }\n      } else {\n        throw new IOException(\n            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n      }\n      // ipAddr is non-null \u0026 non-empty string at this point.\n    }\n\n    // Check the validity of xferPort.\n    if (xferPort \u003d\u003d -1) {\n      throw new IOException(\n          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n    }\n\n    // TODO: Fix storageID\n    return new DatanodeInfo(\n        ipAddr,\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        xferPort,\n        ((Number) m.get(\"infoPort\")).intValue(),\n        getInt(m, \"infoSecurePort\", 0),\n        ((Number) m.get(\"ipcPort\")).intValue(),\n\n        getLong(m, \"capacity\", 0l),\n        getLong(m, \"dfsUsed\", 0l),\n        getLong(m, \"remaining\", 0l),\n        getLong(m, \"blockPoolUsed\", 0l),\n        getLong(m, \"cacheCapacity\", 0l),\n        getLong(m, \"cacheUsed\", 0l),\n        getLong(m, \"lastUpdate\", 0l),\n        getLong(m, \"lastUpdateMonotonic\", 0l),\n        getInt(m, \"xceiverCount\", 0),\n        getString(m, \"networkLocation\", \"\"),\n        AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "e2262d3d18c6d5c2aa20f96920104dc07271b869": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6565. Use jackson instead jetty json in hdfs-client. Contributed by Akira AJISAKA.\n",
      "commitDate": "03/03/15 5:54 PM",
      "commitName": "e2262d3d18c6d5c2aa20f96920104dc07271b869",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "08/12/14 10:23 AM",
      "commitNameOld": "ffe942b82c1208bc7b22899da3a233944cb5ab52",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 85.31,
      "commitsBetweenForRepo": 663,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,58 @@\n   static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) \n     throws IOException {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     // ipAddr and xferPort are the critical fields for accessing data.\n     // If any one of the two is missing, an exception needs to be thrown.\n \n     // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n     //  of ipAddr and xferPort.\n     String ipAddr \u003d getString(m, \"ipAddr\", null);\n     int xferPort \u003d getInt(m, \"xferPort\", -1);\n     if (ipAddr \u003d\u003d null) {\n       String name \u003d getString(m, \"name\", null);\n       if (name !\u003d null) {\n         int colonIdx \u003d name.indexOf(\u0027:\u0027);\n         if (colonIdx \u003e 0) {\n           ipAddr \u003d name.substring(0, colonIdx);\n           xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n         } else {\n           throw new IOException(\n               \"Invalid value in server response: name\u003d[\" + name + \"]\");\n         }\n       } else {\n         throw new IOException(\n             \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n       }\n       // ipAddr is non-null \u0026 non-empty string at this point.\n     }\n \n     // Check the validity of xferPort.\n     if (xferPort \u003d\u003d -1) {\n       throw new IOException(\n           \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n     }\n \n     // TODO: Fix storageID\n     return new DatanodeInfo(\n         ipAddr,\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         xferPort,\n-        (int)(long)(Long)m.get(\"infoPort\"),\n+        ((Number) m.get(\"infoPort\")).intValue(),\n         getInt(m, \"infoSecurePort\", 0),\n-        (int)(long)(Long)m.get(\"ipcPort\"),\n+        ((Number) m.get(\"ipcPort\")).intValue(),\n \n         getLong(m, \"capacity\", 0l),\n         getLong(m, \"dfsUsed\", 0l),\n         getLong(m, \"remaining\", 0l),\n         getLong(m, \"blockPoolUsed\", 0l),\n         getLong(m, \"cacheCapacity\", 0l),\n         getLong(m, \"cacheUsed\", 0l),\n         getLong(m, \"lastUpdate\", 0l),\n         getInt(m, \"xceiverCount\", 0),\n         getString(m, \"networkLocation\", \"\"),\n         AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) \n    throws IOException {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    // ipAddr and xferPort are the critical fields for accessing data.\n    // If any one of the two is missing, an exception needs to be thrown.\n\n    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n    //  of ipAddr and xferPort.\n    String ipAddr \u003d getString(m, \"ipAddr\", null);\n    int xferPort \u003d getInt(m, \"xferPort\", -1);\n    if (ipAddr \u003d\u003d null) {\n      String name \u003d getString(m, \"name\", null);\n      if (name !\u003d null) {\n        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n        if (colonIdx \u003e 0) {\n          ipAddr \u003d name.substring(0, colonIdx);\n          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n        } else {\n          throw new IOException(\n              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n        }\n      } else {\n        throw new IOException(\n            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n      }\n      // ipAddr is non-null \u0026 non-empty string at this point.\n    }\n\n    // Check the validity of xferPort.\n    if (xferPort \u003d\u003d -1) {\n      throw new IOException(\n          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n    }\n\n    // TODO: Fix storageID\n    return new DatanodeInfo(\n        ipAddr,\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        xferPort,\n        ((Number) m.get(\"infoPort\")).intValue(),\n        getInt(m, \"infoSecurePort\", 0),\n        ((Number) m.get(\"ipcPort\")).intValue(),\n\n        getLong(m, \"capacity\", 0l),\n        getLong(m, \"dfsUsed\", 0l),\n        getLong(m, \"remaining\", 0l),\n        getLong(m, \"blockPoolUsed\", 0l),\n        getLong(m, \"cacheCapacity\", 0l),\n        getLong(m, \"cacheUsed\", 0l),\n        getLong(m, \"lastUpdate\", 0l),\n        getInt(m, \"xceiverCount\", 0),\n        getString(m, \"networkLocation\", \"\"),\n        AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "b4d6c5823b04b2a8834e06e78cd109a359496eed": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-5449. WebHdfs compatibility broken between 2.2 and 1.x / 23.x. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556927 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/14 11:24 AM",
      "commitName": "b4d6c5823b04b2a8834e06e78cd109a359496eed",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-5449. WebHdfs compatibility broken between 2.2 and 1.x / 23.x. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556927 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/01/14 11:24 AM",
          "commitName": "b4d6c5823b04b2a8834e06e78cd109a359496eed",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "12/11/13 6:00 PM",
          "commitNameOld": "46cbce9af1272ce0eb6e300f96a1a8d4b08e23e3",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 57.72,
          "commitsBetweenForRepo": 308,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,58 @@\n-  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n+  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) \n+    throws IOException {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n+    // ipAddr and xferPort are the critical fields for accessing data.\n+    // If any one of the two is missing, an exception needs to be thrown.\n+\n+    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n+    //  of ipAddr and xferPort.\n+    String ipAddr \u003d getString(m, \"ipAddr\", null);\n+    int xferPort \u003d getInt(m, \"xferPort\", -1);\n+    if (ipAddr \u003d\u003d null) {\n+      String name \u003d getString(m, \"name\", null);\n+      if (name !\u003d null) {\n+        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n+        if (colonIdx \u003e 0) {\n+          ipAddr \u003d name.substring(0, colonIdx);\n+          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n+        } else {\n+          throw new IOException(\n+              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n+        }\n+      } else {\n+        throw new IOException(\n+            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n+      }\n+      // ipAddr is non-null \u0026 non-empty string at this point.\n+    }\n+\n+    // Check the validity of xferPort.\n+    if (xferPort \u003d\u003d -1) {\n+      throw new IOException(\n+          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n+    }\n+\n     // TODO: Fix storageID\n     return new DatanodeInfo(\n-        (String)m.get(\"ipAddr\"),\n+        ipAddr,\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n-        (int)(long)(Long)m.get(\"xferPort\"),\n+        xferPort,\n         (int)(long)(Long)m.get(\"infoPort\"),\n         getInt(m, \"infoSecurePort\", 0),\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         getLong(m, \"capacity\", 0l),\n         getLong(m, \"dfsUsed\", 0l),\n         getLong(m, \"remaining\", 0l),\n         getLong(m, \"blockPoolUsed\", 0l),\n         getLong(m, \"cacheCapacity\", 0l),\n         getLong(m, \"cacheUsed\", 0l),\n         getLong(m, \"lastUpdate\", 0l),\n         getInt(m, \"xceiverCount\", 0),\n         getString(m, \"networkLocation\", \"\"),\n         AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) \n    throws IOException {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    // ipAddr and xferPort are the critical fields for accessing data.\n    // If any one of the two is missing, an exception needs to be thrown.\n\n    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n    //  of ipAddr and xferPort.\n    String ipAddr \u003d getString(m, \"ipAddr\", null);\n    int xferPort \u003d getInt(m, \"xferPort\", -1);\n    if (ipAddr \u003d\u003d null) {\n      String name \u003d getString(m, \"name\", null);\n      if (name !\u003d null) {\n        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n        if (colonIdx \u003e 0) {\n          ipAddr \u003d name.substring(0, colonIdx);\n          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n        } else {\n          throw new IOException(\n              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n        }\n      } else {\n        throw new IOException(\n            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n      }\n      // ipAddr is non-null \u0026 non-empty string at this point.\n    }\n\n    // Check the validity of xferPort.\n    if (xferPort \u003d\u003d -1) {\n      throw new IOException(\n          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n    }\n\n    // TODO: Fix storageID\n    return new DatanodeInfo(\n        ipAddr,\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        xferPort,\n        (int)(long)(Long)m.get(\"infoPort\"),\n        getInt(m, \"infoSecurePort\", 0),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        getLong(m, \"capacity\", 0l),\n        getLong(m, \"dfsUsed\", 0l),\n        getLong(m, \"remaining\", 0l),\n        getLong(m, \"blockPoolUsed\", 0l),\n        getLong(m, \"cacheCapacity\", 0l),\n        getLong(m, \"cacheUsed\", 0l),\n        getLong(m, \"lastUpdate\", 0l),\n        getInt(m, \"xceiverCount\", 0),\n        getString(m, \"networkLocation\", \"\"),\n        AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5449. WebHdfs compatibility broken between 2.2 and 1.x / 23.x. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556927 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/01/14 11:24 AM",
          "commitName": "b4d6c5823b04b2a8834e06e78cd109a359496eed",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "12/11/13 6:00 PM",
          "commitNameOld": "46cbce9af1272ce0eb6e300f96a1a8d4b08e23e3",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 57.72,
          "commitsBetweenForRepo": 308,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,58 @@\n-  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n+  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) \n+    throws IOException {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n+    // ipAddr and xferPort are the critical fields for accessing data.\n+    // If any one of the two is missing, an exception needs to be thrown.\n+\n+    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n+    //  of ipAddr and xferPort.\n+    String ipAddr \u003d getString(m, \"ipAddr\", null);\n+    int xferPort \u003d getInt(m, \"xferPort\", -1);\n+    if (ipAddr \u003d\u003d null) {\n+      String name \u003d getString(m, \"name\", null);\n+      if (name !\u003d null) {\n+        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n+        if (colonIdx \u003e 0) {\n+          ipAddr \u003d name.substring(0, colonIdx);\n+          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n+        } else {\n+          throw new IOException(\n+              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n+        }\n+      } else {\n+        throw new IOException(\n+            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n+      }\n+      // ipAddr is non-null \u0026 non-empty string at this point.\n+    }\n+\n+    // Check the validity of xferPort.\n+    if (xferPort \u003d\u003d -1) {\n+      throw new IOException(\n+          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n+    }\n+\n     // TODO: Fix storageID\n     return new DatanodeInfo(\n-        (String)m.get(\"ipAddr\"),\n+        ipAddr,\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n-        (int)(long)(Long)m.get(\"xferPort\"),\n+        xferPort,\n         (int)(long)(Long)m.get(\"infoPort\"),\n         getInt(m, \"infoSecurePort\", 0),\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         getLong(m, \"capacity\", 0l),\n         getLong(m, \"dfsUsed\", 0l),\n         getLong(m, \"remaining\", 0l),\n         getLong(m, \"blockPoolUsed\", 0l),\n         getLong(m, \"cacheCapacity\", 0l),\n         getLong(m, \"cacheUsed\", 0l),\n         getLong(m, \"lastUpdate\", 0l),\n         getInt(m, \"xceiverCount\", 0),\n         getString(m, \"networkLocation\", \"\"),\n         AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) \n    throws IOException {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    // ipAddr and xferPort are the critical fields for accessing data.\n    // If any one of the two is missing, an exception needs to be thrown.\n\n    // Handle the case of old servers (1.x, 0.23.x) sending \u0027name\u0027 instead\n    //  of ipAddr and xferPort.\n    String ipAddr \u003d getString(m, \"ipAddr\", null);\n    int xferPort \u003d getInt(m, \"xferPort\", -1);\n    if (ipAddr \u003d\u003d null) {\n      String name \u003d getString(m, \"name\", null);\n      if (name !\u003d null) {\n        int colonIdx \u003d name.indexOf(\u0027:\u0027);\n        if (colonIdx \u003e 0) {\n          ipAddr \u003d name.substring(0, colonIdx);\n          xferPort \u003d Integer.parseInt(name.substring(colonIdx +1));\n        } else {\n          throw new IOException(\n              \"Invalid value in server response: name\u003d[\" + name + \"]\");\n        }\n      } else {\n        throw new IOException(\n            \"Missing both \u0027ipAddr\u0027 and \u0027name\u0027 in server response.\");\n      }\n      // ipAddr is non-null \u0026 non-empty string at this point.\n    }\n\n    // Check the validity of xferPort.\n    if (xferPort \u003d\u003d -1) {\n      throw new IOException(\n          \"Invalid or missing \u0027xferPort\u0027 in server response.\");\n    }\n\n    // TODO: Fix storageID\n    return new DatanodeInfo(\n        ipAddr,\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        xferPort,\n        (int)(long)(Long)m.get(\"infoPort\"),\n        getInt(m, \"infoSecurePort\", 0),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        getLong(m, \"capacity\", 0l),\n        getLong(m, \"dfsUsed\", 0l),\n        getLong(m, \"remaining\", 0l),\n        getLong(m, \"blockPoolUsed\", 0l),\n        getLong(m, \"cacheCapacity\", 0l),\n        getLong(m, \"cacheUsed\", 0l),\n        getLong(m, \"lastUpdate\", 0l),\n        getInt(m, \"xceiverCount\", 0),\n        getString(m, \"networkLocation\", \"\"),\n        AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {}
        }
      ]
    },
    "dc2ee20aec7b3fe1d13c846926ba1b0f02c5adef": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5419. Fixup test-patch.sh warnings on HDFS-4949 branch. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1535607 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/10/13 6:56 PM",
      "commitName": "dc2ee20aec7b3fe1d13c846926ba1b0f02c5adef",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "24/10/13 2:38 PM",
      "commitNameOld": "e87b2a3684c58aa6bc90d51a839a5cc4bc03c25b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.18,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,25 @@\n   static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n-    \n-    Object infoSecurePort \u003d m.get(\"infoSecurePort\");\n-    if (infoSecurePort \u003d\u003d null) {\n-      infoSecurePort \u003d 0l; // same as the default value in hdfs.proto\n-    }\n \n     return new DatanodeInfo(\n         (String)m.get(\"ipAddr\"),\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         (int)(long)(Long)m.get(\"xferPort\"),\n         (int)(long)(Long)m.get(\"infoPort\"),\n-        (int)(long)(Long)infoSecurePort,\n+        getInt(m, \"infoSecurePort\", 0),\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n-        (Long)m.get(\"capacity\"),\n-        (Long)m.get(\"dfsUsed\"),\n-        (Long)m.get(\"remaining\"),\n-        (Long)m.get(\"blockPoolUsed\"),\n-        (Long)m.get(\"cacheCapacity\"),\n-        (Long)m.get(\"cacheUsed\"),\n-        (Long)m.get(\"lastUpdate\"),\n-        (int)(long)(Long)m.get(\"xceiverCount\"),\n-        (String)m.get(\"networkLocation\"),\n-        AdminStates.valueOf((String)m.get(\"adminState\")));\n+        getLong(m, \"capacity\", 0l),\n+        getLong(m, \"dfsUsed\", 0l),\n+        getLong(m, \"remaining\", 0l),\n+        getLong(m, \"blockPoolUsed\", 0l),\n+        getLong(m, \"cacheCapacity\", 0l),\n+        getLong(m, \"cacheUsed\", 0l),\n+        getLong(m, \"lastUpdate\", 0l),\n+        getInt(m, \"xceiverCount\", 0),\n+        getString(m, \"networkLocation\", \"\"),\n+        AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    return new DatanodeInfo(\n        (String)m.get(\"ipAddr\"),\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"xferPort\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        getInt(m, \"infoSecurePort\", 0),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        getLong(m, \"capacity\", 0l),\n        getLong(m, \"dfsUsed\", 0l),\n        getLong(m, \"remaining\", 0l),\n        getLong(m, \"blockPoolUsed\", 0l),\n        getLong(m, \"cacheCapacity\", 0l),\n        getLong(m, \"cacheUsed\", 0l),\n        getLong(m, \"lastUpdate\", 0l),\n        getInt(m, \"xceiverCount\", 0),\n        getString(m, \"networkLocation\", \"\"),\n        AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "82ed72d1d4f7263449ee75ebbd7d668be9f3bdc1": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-5403. WebHdfs client cannot communicate with older WebHdfs servers post HDFS-5306. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1535056 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/10/13 8:15 AM",
      "commitName": "82ed72d1d4f7263449ee75ebbd7d668be9f3bdc1",
      "commitAuthor": "Aaron Myers",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5403. WebHdfs client cannot communicate with older WebHdfs servers post HDFS-5306. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1535056 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/10/13 8:15 AM",
          "commitName": "82ed72d1d4f7263449ee75ebbd7d668be9f3bdc1",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "05/10/13 8:22 PM",
          "commitNameOld": "8e0804666189ce9a66b7b41b744776bad29770dd",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 17.49,
          "commitsBetweenForRepo": 111,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,28 @@\n-  private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n+  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n+    \n+    Object infoSecurePort \u003d m.get(\"infoSecurePort\");\n+    if (infoSecurePort \u003d\u003d null) {\n+      infoSecurePort \u003d 0l; // same as the default value in hdfs.proto\n+    }\n \n     return new DatanodeInfo(\n         (String)m.get(\"ipAddr\"),\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         (int)(long)(Long)m.get(\"xferPort\"),\n         (int)(long)(Long)m.get(\"infoPort\"),\n-        (int)(long)(Long)m.get(\"infoSecurePort\"),\n+        (int)(long)(Long)infoSecurePort,\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         (Long)m.get(\"capacity\"),\n         (Long)m.get(\"dfsUsed\"),\n         (Long)m.get(\"remaining\"),\n         (Long)m.get(\"blockPoolUsed\"),\n         (Long)m.get(\"lastUpdate\"),\n         (int)(long)(Long)m.get(\"xceiverCount\"),\n         (String)m.get(\"networkLocation\"),\n         AdminStates.valueOf((String)m.get(\"adminState\")));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n    \n    Object infoSecurePort \u003d m.get(\"infoSecurePort\");\n    if (infoSecurePort \u003d\u003d null) {\n      infoSecurePort \u003d 0l; // same as the default value in hdfs.proto\n    }\n\n    return new DatanodeInfo(\n        (String)m.get(\"ipAddr\"),\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"xferPort\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        (int)(long)(Long)infoSecurePort,\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        (Long)m.get(\"capacity\"),\n        (Long)m.get(\"dfsUsed\"),\n        (Long)m.get(\"remaining\"),\n        (Long)m.get(\"blockPoolUsed\"),\n        (Long)m.get(\"lastUpdate\"),\n        (int)(long)(Long)m.get(\"xceiverCount\"),\n        (String)m.get(\"networkLocation\"),\n        AdminStates.valueOf((String)m.get(\"adminState\")));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {
            "oldValue": "[private, static]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5403. WebHdfs client cannot communicate with older WebHdfs servers post HDFS-5306. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1535056 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/10/13 8:15 AM",
          "commitName": "82ed72d1d4f7263449ee75ebbd7d668be9f3bdc1",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "05/10/13 8:22 PM",
          "commitNameOld": "8e0804666189ce9a66b7b41b744776bad29770dd",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 17.49,
          "commitsBetweenForRepo": 111,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,28 @@\n-  private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n+  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n+    \n+    Object infoSecurePort \u003d m.get(\"infoSecurePort\");\n+    if (infoSecurePort \u003d\u003d null) {\n+      infoSecurePort \u003d 0l; // same as the default value in hdfs.proto\n+    }\n \n     return new DatanodeInfo(\n         (String)m.get(\"ipAddr\"),\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         (int)(long)(Long)m.get(\"xferPort\"),\n         (int)(long)(Long)m.get(\"infoPort\"),\n-        (int)(long)(Long)m.get(\"infoSecurePort\"),\n+        (int)(long)(Long)infoSecurePort,\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         (Long)m.get(\"capacity\"),\n         (Long)m.get(\"dfsUsed\"),\n         (Long)m.get(\"remaining\"),\n         (Long)m.get(\"blockPoolUsed\"),\n         (Long)m.get(\"lastUpdate\"),\n         (int)(long)(Long)m.get(\"xceiverCount\"),\n         (String)m.get(\"networkLocation\"),\n         AdminStates.valueOf((String)m.get(\"adminState\")));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n    \n    Object infoSecurePort \u003d m.get(\"infoSecurePort\");\n    if (infoSecurePort \u003d\u003d null) {\n      infoSecurePort \u003d 0l; // same as the default value in hdfs.proto\n    }\n\n    return new DatanodeInfo(\n        (String)m.get(\"ipAddr\"),\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"xferPort\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        (int)(long)(Long)infoSecurePort,\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        (Long)m.get(\"capacity\"),\n        (Long)m.get(\"dfsUsed\"),\n        (Long)m.get(\"remaining\"),\n        (Long)m.get(\"blockPoolUsed\"),\n        (Long)m.get(\"lastUpdate\"),\n        (int)(long)(Long)m.get(\"xceiverCount\"),\n        (String)m.get(\"networkLocation\"),\n        AdminStates.valueOf((String)m.get(\"adminState\")));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {}
        }
      ]
    },
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": {
      "type": "Ybodychange",
      "commitMessage": "merge trunk to branch HDFS-4949\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532952 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 7:14 PM",
      "commitName": "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/08/13 3:15 PM",
      "commitNameOld": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 47.17,
      "commitsBetweenForRepo": 86,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,25 @@\n   private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     return new DatanodeInfo(\n         (String)m.get(\"ipAddr\"),\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         (int)(long)(Long)m.get(\"xferPort\"),\n         (int)(long)(Long)m.get(\"infoPort\"),\n+        (int)(long)(Long)m.get(\"infoSecurePort\"),\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         (Long)m.get(\"capacity\"),\n         (Long)m.get(\"dfsUsed\"),\n         (Long)m.get(\"remaining\"),\n         (Long)m.get(\"blockPoolUsed\"),\n         (Long)m.get(\"cacheCapacity\"),\n         (Long)m.get(\"cacheUsed\"),\n         (Long)m.get(\"lastUpdate\"),\n         (int)(long)(Long)m.get(\"xceiverCount\"),\n         (String)m.get(\"networkLocation\"),\n         AdminStates.valueOf((String)m.get(\"adminState\")));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    return new DatanodeInfo(\n        (String)m.get(\"ipAddr\"),\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"xferPort\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        (int)(long)(Long)m.get(\"infoSecurePort\"),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        (Long)m.get(\"capacity\"),\n        (Long)m.get(\"dfsUsed\"),\n        (Long)m.get(\"remaining\"),\n        (Long)m.get(\"blockPoolUsed\"),\n        (Long)m.get(\"cacheCapacity\"),\n        (Long)m.get(\"cacheUsed\"),\n        (Long)m.get(\"lastUpdate\"),\n        (int)(long)(Long)m.get(\"xceiverCount\"),\n        (String)m.get(\"networkLocation\"),\n        AdminStates.valueOf((String)m.get(\"adminState\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "8e0804666189ce9a66b7b41b744776bad29770dd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5306. Datanode https port is not available at the namenode. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529562 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/13 8:22 PM",
      "commitName": "8e0804666189ce9a66b7b41b744776bad29770dd",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "30/07/13 5:49 PM",
      "commitNameOld": "4f68aa060090319b8de5c30f41d193632503ed17",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 67.11,
      "commitsBetweenForRepo": 397,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,23 @@\n   private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     return new DatanodeInfo(\n         (String)m.get(\"ipAddr\"),\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         (int)(long)(Long)m.get(\"xferPort\"),\n         (int)(long)(Long)m.get(\"infoPort\"),\n+        (int)(long)(Long)m.get(\"infoSecurePort\"),\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         (Long)m.get(\"capacity\"),\n         (Long)m.get(\"dfsUsed\"),\n         (Long)m.get(\"remaining\"),\n         (Long)m.get(\"blockPoolUsed\"),\n         (Long)m.get(\"lastUpdate\"),\n         (int)(long)(Long)m.get(\"xceiverCount\"),\n         (String)m.get(\"networkLocation\"),\n         AdminStates.valueOf((String)m.get(\"adminState\")));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    return new DatanodeInfo(\n        (String)m.get(\"ipAddr\"),\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"xferPort\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        (int)(long)(Long)m.get(\"infoSecurePort\"),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        (Long)m.get(\"capacity\"),\n        (Long)m.get(\"dfsUsed\"),\n        (Long)m.get(\"remaining\"),\n        (Long)m.get(\"blockPoolUsed\"),\n        (Long)m.get(\"lastUpdate\"),\n        (int)(long)(Long)m.get(\"xceiverCount\"),\n        (String)m.get(\"networkLocation\"),\n        AdminStates.valueOf((String)m.get(\"adminState\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4988. Datanode must support all the volumes as individual storages.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1526969 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 9:05 AM",
      "commitName": "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/09/13 11:03 AM",
      "commitNameOld": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 4.92,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,23 @@\n   private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n+    // TODO: Fix storageID\n     return new DatanodeInfo(\n         (String)m.get(\"ipAddr\"),\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         (int)(long)(Long)m.get(\"xferPort\"),\n         (int)(long)(Long)m.get(\"infoPort\"),\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         (Long)m.get(\"capacity\"),\n         (Long)m.get(\"dfsUsed\"),\n         (Long)m.get(\"remaining\"),\n         (Long)m.get(\"blockPoolUsed\"),\n         (Long)m.get(\"lastUpdate\"),\n         (int)(long)(Long)m.get(\"xceiverCount\"),\n         (String)m.get(\"networkLocation\"),\n         AdminStates.valueOf((String)m.get(\"adminState\")));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    // TODO: Fix storageID\n    return new DatanodeInfo(\n        (String)m.get(\"ipAddr\"),\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"xferPort\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        (Long)m.get(\"capacity\"),\n        (Long)m.get(\"dfsUsed\"),\n        (Long)m.get(\"remaining\"),\n        (Long)m.get(\"blockPoolUsed\"),\n        (Long)m.get(\"lastUpdate\"),\n        (int)(long)(Long)m.get(\"xceiverCount\"),\n        (String)m.get(\"networkLocation\"),\n        AdminStates.valueOf((String)m.get(\"adminState\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5141. Add cache status information to datanode heartbeat. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1519101 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/08/13 3:15 PM",
      "commitName": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/07/13 5:49 PM",
      "commitNameOld": "4f68aa060090319b8de5c30f41d193632503ed17",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 30.89,
      "commitsBetweenForRepo": 90,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,24 @@\n   private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     return new DatanodeInfo(\n         (String)m.get(\"ipAddr\"),\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         (int)(long)(Long)m.get(\"xferPort\"),\n         (int)(long)(Long)m.get(\"infoPort\"),\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         (Long)m.get(\"capacity\"),\n         (Long)m.get(\"dfsUsed\"),\n         (Long)m.get(\"remaining\"),\n         (Long)m.get(\"blockPoolUsed\"),\n+        (Long)m.get(\"cacheCapacity\"),\n+        (Long)m.get(\"cacheUsed\"),\n         (Long)m.get(\"lastUpdate\"),\n         (int)(long)(Long)m.get(\"xceiverCount\"),\n         (String)m.get(\"networkLocation\"),\n         AdminStates.valueOf((String)m.get(\"adminState\")));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    return new DatanodeInfo(\n        (String)m.get(\"ipAddr\"),\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"xferPort\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        (Long)m.get(\"capacity\"),\n        (Long)m.get(\"dfsUsed\"),\n        (Long)m.get(\"remaining\"),\n        (Long)m.get(\"blockPoolUsed\"),\n        (Long)m.get(\"cacheCapacity\"),\n        (Long)m.get(\"cacheUsed\"),\n        (Long)m.get(\"lastUpdate\"),\n        (int)(long)(Long)m.get(\"xceiverCount\"),\n        (String)m.get(\"networkLocation\"),\n        AdminStates.valueOf((String)m.get(\"adminState\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "112c32415175f637a2791f2207c20393fc9ba740": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3210. JsonUtil#toJsonMap for for a DatanodeInfo should use \"ipAddr\" instead of \"name\". Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1310135 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/04/12 5:10 PM",
      "commitName": "112c32415175f637a2791f2207c20393fc9ba740",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "03/04/12 12:44 PM",
      "commitNameOld": "e561cb9e78810bb6d2a7293fc8c46d571e7ec2bc",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.18,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     return new DatanodeInfo(\n-        (String)m.get(\"name\"),\n+        (String)m.get(\"ipAddr\"),\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         (int)(long)(Long)m.get(\"xferPort\"),\n         (int)(long)(Long)m.get(\"infoPort\"),\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         (Long)m.get(\"capacity\"),\n         (Long)m.get(\"dfsUsed\"),\n         (Long)m.get(\"remaining\"),\n         (Long)m.get(\"blockPoolUsed\"),\n         (Long)m.get(\"lastUpdate\"),\n         (int)(long)(Long)m.get(\"xceiverCount\"),\n         (String)m.get(\"networkLocation\"),\n         AdminStates.valueOf((String)m.get(\"adminState\")));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    return new DatanodeInfo(\n        (String)m.get(\"ipAddr\"),\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"xferPort\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        (Long)m.get(\"capacity\"),\n        (Long)m.get(\"dfsUsed\"),\n        (Long)m.get(\"remaining\"),\n        (Long)m.get(\"blockPoolUsed\"),\n        (Long)m.get(\"lastUpdate\"),\n        (int)(long)(Long)m.get(\"xceiverCount\"),\n        (String)m.get(\"networkLocation\"),\n        AdminStates.valueOf((String)m.get(\"adminState\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "31/03/12 12:58 PM",
      "commitNameOld": "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.09,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,22 @@\n   private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     return new DatanodeInfo(\n         (String)m.get(\"name\"),\n         (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n+        (int)(long)(Long)m.get(\"xferPort\"),\n         (int)(long)(Long)m.get(\"infoPort\"),\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         (Long)m.get(\"capacity\"),\n         (Long)m.get(\"dfsUsed\"),\n         (Long)m.get(\"remaining\"),\n         (Long)m.get(\"blockPoolUsed\"),\n         (Long)m.get(\"lastUpdate\"),\n         (int)(long)(Long)m.get(\"xceiverCount\"),\n         (String)m.get(\"networkLocation\"),\n         AdminStates.valueOf((String)m.get(\"adminState\")));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    return new DatanodeInfo(\n        (String)m.get(\"name\"),\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"xferPort\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        (Long)m.get(\"capacity\"),\n        (Long)m.get(\"dfsUsed\"),\n        (Long)m.get(\"remaining\"),\n        (Long)m.get(\"blockPoolUsed\"),\n        (Long)m.get(\"lastUpdate\"),\n        (int)(long)(Long)m.get(\"xceiverCount\"),\n        (String)m.get(\"networkLocation\"),\n        AdminStates.valueOf((String)m.get(\"adminState\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3164. Move DatanodeInfo#hostName to DatanodeID. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1307890 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/03/12 12:58 PM",
      "commitName": "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "09/12/11 1:26 AM",
      "commitNameOld": "9b1f47226b076bf912a922aba293218dcadc7024",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 113.44,
      "commitsBetweenForRepo": 797,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     return new DatanodeInfo(\n         (String)m.get(\"name\"),\n+        (String)m.get(\"hostName\"),\n         (String)m.get(\"storageID\"),\n         (int)(long)(Long)m.get(\"infoPort\"),\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         (Long)m.get(\"capacity\"),\n         (Long)m.get(\"dfsUsed\"),\n         (Long)m.get(\"remaining\"),\n         (Long)m.get(\"blockPoolUsed\"),\n         (Long)m.get(\"lastUpdate\"),\n         (int)(long)(Long)m.get(\"xceiverCount\"),\n         (String)m.get(\"networkLocation\"),\n-        (String)m.get(\"hostName\"),\n         AdminStates.valueOf((String)m.get(\"adminState\")));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    return new DatanodeInfo(\n        (String)m.get(\"name\"),\n        (String)m.get(\"hostName\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        (Long)m.get(\"capacity\"),\n        (Long)m.get(\"dfsUsed\"),\n        (Long)m.get(\"remaining\"),\n        (Long)m.get(\"blockPoolUsed\"),\n        (Long)m.get(\"lastUpdate\"),\n        (int)(long)(Long)m.get(\"xceiverCount\"),\n        (String)m.get(\"networkLocation\"),\n        AdminStates.valueOf((String)m.get(\"adminState\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-2395. Add a root element in the JSON responses of webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/11 4:29 AM",
      "commitName": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/09/11 9:49 PM",
      "commitNameOld": "dc8464f943b61b795df0cc8baec171bf07355763",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.28,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n-  public static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n+  private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n     if (m \u003d\u003d null) {\n       return null;\n     }\n \n     return new DatanodeInfo(\n         (String)m.get(\"name\"),\n         (String)m.get(\"storageID\"),\n         (int)(long)(Long)m.get(\"infoPort\"),\n         (int)(long)(Long)m.get(\"ipcPort\"),\n \n         (Long)m.get(\"capacity\"),\n         (Long)m.get(\"dfsUsed\"),\n         (Long)m.get(\"remaining\"),\n         (Long)m.get(\"blockPoolUsed\"),\n         (Long)m.get(\"lastUpdate\"),\n         (int)(long)(Long)m.get(\"xceiverCount\"),\n         (String)m.get(\"networkLocation\"),\n         (String)m.get(\"hostName\"),\n         AdminStates.valueOf((String)m.get(\"adminState\")));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    return new DatanodeInfo(\n        (String)m.get(\"name\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        (Long)m.get(\"capacity\"),\n        (Long)m.get(\"dfsUsed\"),\n        (Long)m.get(\"remaining\"),\n        (Long)m.get(\"blockPoolUsed\"),\n        (Long)m.get(\"lastUpdate\"),\n        (int)(long)(Long)m.get(\"xceiverCount\"),\n        (String)m.get(\"networkLocation\"),\n        (String)m.get(\"hostName\"),\n        AdminStates.valueOf((String)m.get(\"adminState\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {
        "oldValue": "[public, static]",
        "newValue": "[private, static]"
      }
    },
    "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2340. Support getFileBlockLocations and getDelegationToken in webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1173468 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/09/11 7:56 PM",
      "commitName": "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,21 @@\n+  public static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n+    if (m \u003d\u003d null) {\n+      return null;\n+    }\n+\n+    return new DatanodeInfo(\n+        (String)m.get(\"name\"),\n+        (String)m.get(\"storageID\"),\n+        (int)(long)(Long)m.get(\"infoPort\"),\n+        (int)(long)(Long)m.get(\"ipcPort\"),\n+\n+        (Long)m.get(\"capacity\"),\n+        (Long)m.get(\"dfsUsed\"),\n+        (Long)m.get(\"remaining\"),\n+        (Long)m.get(\"blockPoolUsed\"),\n+        (Long)m.get(\"lastUpdate\"),\n+        (int)(long)(Long)m.get(\"xceiverCount\"),\n+        (String)m.get(\"networkLocation\"),\n+        (String)m.get(\"hostName\"),\n+        AdminStates.valueOf((String)m.get(\"adminState\")));\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static DatanodeInfo toDatanodeInfo(final Map\u003c?, ?\u003e m) {\n    if (m \u003d\u003d null) {\n      return null;\n    }\n\n    return new DatanodeInfo(\n        (String)m.get(\"name\"),\n        (String)m.get(\"storageID\"),\n        (int)(long)(Long)m.get(\"infoPort\"),\n        (int)(long)(Long)m.get(\"ipcPort\"),\n\n        (Long)m.get(\"capacity\"),\n        (Long)m.get(\"dfsUsed\"),\n        (Long)m.get(\"remaining\"),\n        (Long)m.get(\"blockPoolUsed\"),\n        (Long)m.get(\"lastUpdate\"),\n        (int)(long)(Long)m.get(\"xceiverCount\"),\n        (String)m.get(\"networkLocation\"),\n        (String)m.get(\"hostName\"),\n        AdminStates.valueOf((String)m.get(\"adminState\")));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java"
    }
  }
}