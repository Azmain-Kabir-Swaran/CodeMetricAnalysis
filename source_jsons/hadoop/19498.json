{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BackupStore.java",
  "functionName": "hasNext",
  "functionId": "hasNext",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/BackupStore.java",
  "functionStartLine": 223,
  "functionEndLine": 283,
  "numCommitsSeen": 8,
  "timeTaken": 9055,
  "changeHistory": [
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymovefromfile",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public boolean hasNext() throws IOException {\n    \n    if (lastSegmentEOF) {\n      return false;\n    }\n    \n    // We read the next KV from the cache to decide if there is any left.\n    // Since hasNext can be called several times before the actual call to \n    // next(), we use hasMore to avoid extra reads. hasMore is set to false\n    // when the user actually consumes this record in next()\n\n    if (hasMore) {\n      return true;\n    }\n\n    Segment\u003cK,V\u003e seg \u003d segmentList.get(readSegmentIndex);\n    // Mark the current position. This would be set to currentKVOffset\n    // when the user consumes this record in next(). \n    nextKVOffset \u003d (int) seg.getActualPosition();\n    if (seg.nextRawKey()) {\n      currentKey \u003d seg.getKey();\n      seg.getValue(currentValue);\n      hasMore \u003d true;\n      return true;\n    } else {\n      if (!seg.inMemory()) {\n        seg.closeReader();\n      }\n    }\n\n    // If this is the last segment, mark the lastSegmentEOF flag and return\n    if (readSegmentIndex \u003d\u003d segmentList.size() - 1) {\n      nextKVOffset \u003d -1;\n      lastSegmentEOF \u003d true;\n      return false;\n    }\n\n    nextKVOffset \u003d 0;\n    readSegmentIndex ++;\n\n    Segment\u003cK,V\u003e nextSegment \u003d segmentList.get(readSegmentIndex);\n    \n    // We possibly are moving from a memory segment to a disk segment.\n    // Reset so that we do not corrupt the in-memory segment buffer.\n    // See HADOOP-5494\n    \n    if (!nextSegment.inMemory()) {\n      currentValue.reset(currentDiskValue.getData(), \n          currentDiskValue.getLength());\n      nextSegment.init(null);\n    }\n \n    if (nextSegment.nextRawKey()) {\n      currentKey \u003d nextSegment.getKey();\n      nextSegment.getValue(currentValue);\n      hasMore \u003d true;\n      return true;\n    } else {\n      throw new IOException(\"New segment did not have even one K/V\");\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/BackupStore.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/BackupStore.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/BackupStore.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public boolean hasNext() throws IOException {\n    \n    if (lastSegmentEOF) {\n      return false;\n    }\n    \n    // We read the next KV from the cache to decide if there is any left.\n    // Since hasNext can be called several times before the actual call to \n    // next(), we use hasMore to avoid extra reads. hasMore is set to false\n    // when the user actually consumes this record in next()\n\n    if (hasMore) {\n      return true;\n    }\n\n    Segment\u003cK,V\u003e seg \u003d segmentList.get(readSegmentIndex);\n    // Mark the current position. This would be set to currentKVOffset\n    // when the user consumes this record in next(). \n    nextKVOffset \u003d (int) seg.getActualPosition();\n    if (seg.nextRawKey()) {\n      currentKey \u003d seg.getKey();\n      seg.getValue(currentValue);\n      hasMore \u003d true;\n      return true;\n    } else {\n      if (!seg.inMemory()) {\n        seg.closeReader();\n      }\n    }\n\n    // If this is the last segment, mark the lastSegmentEOF flag and return\n    if (readSegmentIndex \u003d\u003d segmentList.size() - 1) {\n      nextKVOffset \u003d -1;\n      lastSegmentEOF \u003d true;\n      return false;\n    }\n\n    nextKVOffset \u003d 0;\n    readSegmentIndex ++;\n\n    Segment\u003cK,V\u003e nextSegment \u003d segmentList.get(readSegmentIndex);\n    \n    // We possibly are moving from a memory segment to a disk segment.\n    // Reset so that we do not corrupt the in-memory segment buffer.\n    // See HADOOP-5494\n    \n    if (!nextSegment.inMemory()) {\n      currentValue.reset(currentDiskValue.getData(), \n          currentDiskValue.getLength());\n      nextSegment.init(null);\n    }\n \n    if (nextSegment.nextRawKey()) {\n      currentKey \u003d nextSegment.getKey();\n      nextSegment.getValue(currentValue);\n      hasMore \u003d true;\n      return true;\n    } else {\n      throw new IOException(\"New segment did not have even one K/V\");\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/BackupStore.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/BackupStore.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/BackupStore.java",
        "oldMethodName": "hasNext",
        "newMethodName": "hasNext"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,61 @@\n+  public boolean hasNext() throws IOException {\n+    \n+    if (lastSegmentEOF) {\n+      return false;\n+    }\n+    \n+    // We read the next KV from the cache to decide if there is any left.\n+    // Since hasNext can be called several times before the actual call to \n+    // next(), we use hasMore to avoid extra reads. hasMore is set to false\n+    // when the user actually consumes this record in next()\n+\n+    if (hasMore) {\n+      return true;\n+    }\n+\n+    Segment\u003cK,V\u003e seg \u003d segmentList.get(readSegmentIndex);\n+    // Mark the current position. This would be set to currentKVOffset\n+    // when the user consumes this record in next(). \n+    nextKVOffset \u003d (int) seg.getActualPosition();\n+    if (seg.nextRawKey()) {\n+      currentKey \u003d seg.getKey();\n+      seg.getValue(currentValue);\n+      hasMore \u003d true;\n+      return true;\n+    } else {\n+      if (!seg.inMemory()) {\n+        seg.closeReader();\n+      }\n+    }\n+\n+    // If this is the last segment, mark the lastSegmentEOF flag and return\n+    if (readSegmentIndex \u003d\u003d segmentList.size() - 1) {\n+      nextKVOffset \u003d -1;\n+      lastSegmentEOF \u003d true;\n+      return false;\n+    }\n+\n+    nextKVOffset \u003d 0;\n+    readSegmentIndex ++;\n+\n+    Segment\u003cK,V\u003e nextSegment \u003d segmentList.get(readSegmentIndex);\n+    \n+    // We possibly are moving from a memory segment to a disk segment.\n+    // Reset so that we do not corrupt the in-memory segment buffer.\n+    // See HADOOP-5494\n+    \n+    if (!nextSegment.inMemory()) {\n+      currentValue.reset(currentDiskValue.getData(), \n+          currentDiskValue.getLength());\n+      nextSegment.init(null);\n+    }\n+ \n+    if (nextSegment.nextRawKey()) {\n+      currentKey \u003d nextSegment.getKey();\n+      nextSegment.getValue(currentValue);\n+      hasMore \u003d true;\n+      return true;\n+    } else {\n+      throw new IOException(\"New segment did not have even one K/V\");\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean hasNext() throws IOException {\n    \n    if (lastSegmentEOF) {\n      return false;\n    }\n    \n    // We read the next KV from the cache to decide if there is any left.\n    // Since hasNext can be called several times before the actual call to \n    // next(), we use hasMore to avoid extra reads. hasMore is set to false\n    // when the user actually consumes this record in next()\n\n    if (hasMore) {\n      return true;\n    }\n\n    Segment\u003cK,V\u003e seg \u003d segmentList.get(readSegmentIndex);\n    // Mark the current position. This would be set to currentKVOffset\n    // when the user consumes this record in next(). \n    nextKVOffset \u003d (int) seg.getActualPosition();\n    if (seg.nextRawKey()) {\n      currentKey \u003d seg.getKey();\n      seg.getValue(currentValue);\n      hasMore \u003d true;\n      return true;\n    } else {\n      if (!seg.inMemory()) {\n        seg.closeReader();\n      }\n    }\n\n    // If this is the last segment, mark the lastSegmentEOF flag and return\n    if (readSegmentIndex \u003d\u003d segmentList.size() - 1) {\n      nextKVOffset \u003d -1;\n      lastSegmentEOF \u003d true;\n      return false;\n    }\n\n    nextKVOffset \u003d 0;\n    readSegmentIndex ++;\n\n    Segment\u003cK,V\u003e nextSegment \u003d segmentList.get(readSegmentIndex);\n    \n    // We possibly are moving from a memory segment to a disk segment.\n    // Reset so that we do not corrupt the in-memory segment buffer.\n    // See HADOOP-5494\n    \n    if (!nextSegment.inMemory()) {\n      currentValue.reset(currentDiskValue.getData(), \n          currentDiskValue.getLength());\n      nextSegment.init(null);\n    }\n \n    if (nextSegment.nextRawKey()) {\n      currentKey \u003d nextSegment.getKey();\n      nextSegment.getValue(currentValue);\n      hasMore \u003d true;\n      return true;\n    } else {\n      throw new IOException(\"New segment did not have even one K/V\");\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/BackupStore.java"
    }
  }
}