{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "GenerateDistCacheData.java",
  "functionName": "getSplits",
  "functionId": "getSplits___jobCtxt-JobContext",
  "sourceFilePath": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java",
  "functionStartLine": 190,
  "functionEndLine": 254,
  "numCommitsSeen": 7,
  "timeTaken": 4269,
  "changeHistory": [
    "dcf84707ab50662add112bd6b01c0bfd63374853",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "dcf84707ab50662add112bd6b01c0bfd63374853": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "dcf84707ab50662add112bd6b01c0bfd63374853": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3543. Mavenize Gridmix. (tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1339629 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/05/12 8:06 AM",
      "commitName": "dcf84707ab50662add112bd6b01c0bfd63374853",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "17/05/12 7:20 AM",
      "commitNameOld": "e1f09365ca0bee093f849fcf2e546dd6e2c0a965",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public List\u003cInputSplit\u003e getSplits(JobContext jobCtxt) throws IOException {\n      final JobConf jobConf \u003d new JobConf(jobCtxt.getConfiguration());\n      final JobClient client \u003d new JobClient(jobConf);\n      ClusterStatus stat \u003d client.getClusterStatus(true);\n      int numTrackers \u003d stat.getTaskTrackers();\n      final int fileCount \u003d jobConf.getInt(GRIDMIX_DISTCACHE_FILE_COUNT, -1);\n\n      // Total size of distributed cache files to be generated\n      final long totalSize \u003d jobConf.getLong(GRIDMIX_DISTCACHE_BYTE_COUNT, -1);\n      // Get the path of the special file\n      String distCacheFileList \u003d jobConf.get(GRIDMIX_DISTCACHE_FILE_LIST);\n      if (fileCount \u003c 0 || totalSize \u003c 0 || distCacheFileList \u003d\u003d null) {\n        throw new RuntimeException(\"Invalid metadata: #files (\" + fileCount\n            + \"), total_size (\" + totalSize + \"), filelisturi (\"\n            + distCacheFileList + \")\");\n      }\n\n      Path sequenceFile \u003d new Path(distCacheFileList);\n      FileSystem fs \u003d sequenceFile.getFileSystem(jobConf);\n      FileStatus srcst \u003d fs.getFileStatus(sequenceFile);\n      // Consider the number of TTs * mapSlotsPerTracker as number of mappers.\n      int numMapSlotsPerTracker \u003d jobConf.getInt(TTConfig.TT_MAP_SLOTS, 2);\n      int numSplits \u003d numTrackers * numMapSlotsPerTracker;\n\n      List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e(numSplits);\n      LongWritable key \u003d new LongWritable();\n      BytesWritable value \u003d new BytesWritable();\n\n      // Average size of data to be generated by each map task\n      final long targetSize \u003d Math.max(totalSize / numSplits,\n                                DistributedCacheEmulator.AVG_BYTES_PER_MAP);\n      long splitStartPosition \u003d 0L;\n      long splitEndPosition \u003d 0L;\n      long acc \u003d 0L;\n      long bytesRemaining \u003d srcst.getLen();\n      SequenceFile.Reader reader \u003d null;\n      try {\n        reader \u003d new SequenceFile.Reader(fs, sequenceFile, jobConf);\n        while (reader.next(key, value)) {\n\n          // If adding this file would put this split past the target size,\n          // cut the last split and put this file in the next split.\n          if (acc + key.get() \u003e targetSize \u0026\u0026 acc !\u003d 0) {\n            long splitSize \u003d splitEndPosition - splitStartPosition;\n            splits.add(new FileSplit(\n                sequenceFile, splitStartPosition, splitSize, (String[])null));\n            bytesRemaining -\u003d splitSize;\n            splitStartPosition \u003d splitEndPosition;\n            acc \u003d 0L;\n          }\n          acc +\u003d key.get();\n          splitEndPosition \u003d reader.getPosition();\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close();\n        }\n      }\n      if (bytesRemaining !\u003d 0) {\n        splits.add(new FileSplit(\n            sequenceFile, splitStartPosition, bytesRemaining, (String[])null));\n      }\n\n      return splits;\n    }",
      "path": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java",
        "newPath": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public List\u003cInputSplit\u003e getSplits(JobContext jobCtxt) throws IOException {\n      final JobConf jobConf \u003d new JobConf(jobCtxt.getConfiguration());\n      final JobClient client \u003d new JobClient(jobConf);\n      ClusterStatus stat \u003d client.getClusterStatus(true);\n      int numTrackers \u003d stat.getTaskTrackers();\n      final int fileCount \u003d jobConf.getInt(GRIDMIX_DISTCACHE_FILE_COUNT, -1);\n\n      // Total size of distributed cache files to be generated\n      final long totalSize \u003d jobConf.getLong(GRIDMIX_DISTCACHE_BYTE_COUNT, -1);\n      // Get the path of the special file\n      String distCacheFileList \u003d jobConf.get(GRIDMIX_DISTCACHE_FILE_LIST);\n      if (fileCount \u003c 0 || totalSize \u003c 0 || distCacheFileList \u003d\u003d null) {\n        throw new RuntimeException(\"Invalid metadata: #files (\" + fileCount\n            + \"), total_size (\" + totalSize + \"), filelisturi (\"\n            + distCacheFileList + \")\");\n      }\n\n      Path sequenceFile \u003d new Path(distCacheFileList);\n      FileSystem fs \u003d sequenceFile.getFileSystem(jobConf);\n      FileStatus srcst \u003d fs.getFileStatus(sequenceFile);\n      // Consider the number of TTs * mapSlotsPerTracker as number of mappers.\n      int numMapSlotsPerTracker \u003d jobConf.getInt(TTConfig.TT_MAP_SLOTS, 2);\n      int numSplits \u003d numTrackers * numMapSlotsPerTracker;\n\n      List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e(numSplits);\n      LongWritable key \u003d new LongWritable();\n      BytesWritable value \u003d new BytesWritable();\n\n      // Average size of data to be generated by each map task\n      final long targetSize \u003d Math.max(totalSize / numSplits,\n                                DistributedCacheEmulator.AVG_BYTES_PER_MAP);\n      long splitStartPosition \u003d 0L;\n      long splitEndPosition \u003d 0L;\n      long acc \u003d 0L;\n      long bytesRemaining \u003d srcst.getLen();\n      SequenceFile.Reader reader \u003d null;\n      try {\n        reader \u003d new SequenceFile.Reader(fs, sequenceFile, jobConf);\n        while (reader.next(key, value)) {\n\n          // If adding this file would put this split past the target size,\n          // cut the last split and put this file in the next split.\n          if (acc + key.get() \u003e targetSize \u0026\u0026 acc !\u003d 0) {\n            long splitSize \u003d splitEndPosition - splitStartPosition;\n            splits.add(new FileSplit(\n                sequenceFile, splitStartPosition, splitSize, (String[])null));\n            bytesRemaining -\u003d splitSize;\n            splitStartPosition \u003d splitEndPosition;\n            acc \u003d 0L;\n          }\n          acc +\u003d key.get();\n          splitEndPosition \u003d reader.getPosition();\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close();\n        }\n      }\n      if (bytesRemaining !\u003d 0) {\n        splits.add(new FileSplit(\n            sequenceFile, splitStartPosition, bytesRemaining, (String[])null));\n      }\n\n      return splits;\n    }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java",
        "newPath": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public List\u003cInputSplit\u003e getSplits(JobContext jobCtxt) throws IOException {\n      final JobConf jobConf \u003d new JobConf(jobCtxt.getConfiguration());\n      final JobClient client \u003d new JobClient(jobConf);\n      ClusterStatus stat \u003d client.getClusterStatus(true);\n      int numTrackers \u003d stat.getTaskTrackers();\n      final int fileCount \u003d jobConf.getInt(GRIDMIX_DISTCACHE_FILE_COUNT, -1);\n\n      // Total size of distributed cache files to be generated\n      final long totalSize \u003d jobConf.getLong(GRIDMIX_DISTCACHE_BYTE_COUNT, -1);\n      // Get the path of the special file\n      String distCacheFileList \u003d jobConf.get(GRIDMIX_DISTCACHE_FILE_LIST);\n      if (fileCount \u003c 0 || totalSize \u003c 0 || distCacheFileList \u003d\u003d null) {\n        throw new RuntimeException(\"Invalid metadata: #files (\" + fileCount\n            + \"), total_size (\" + totalSize + \"), filelisturi (\"\n            + distCacheFileList + \")\");\n      }\n\n      Path sequenceFile \u003d new Path(distCacheFileList);\n      FileSystem fs \u003d sequenceFile.getFileSystem(jobConf);\n      FileStatus srcst \u003d fs.getFileStatus(sequenceFile);\n      // Consider the number of TTs * mapSlotsPerTracker as number of mappers.\n      int numMapSlotsPerTracker \u003d jobConf.getInt(TTConfig.TT_MAP_SLOTS, 2);\n      int numSplits \u003d numTrackers * numMapSlotsPerTracker;\n\n      List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e(numSplits);\n      LongWritable key \u003d new LongWritable();\n      BytesWritable value \u003d new BytesWritable();\n\n      // Average size of data to be generated by each map task\n      final long targetSize \u003d Math.max(totalSize / numSplits,\n                                DistributedCacheEmulator.AVG_BYTES_PER_MAP);\n      long splitStartPosition \u003d 0L;\n      long splitEndPosition \u003d 0L;\n      long acc \u003d 0L;\n      long bytesRemaining \u003d srcst.getLen();\n      SequenceFile.Reader reader \u003d null;\n      try {\n        reader \u003d new SequenceFile.Reader(fs, sequenceFile, jobConf);\n        while (reader.next(key, value)) {\n\n          // If adding this file would put this split past the target size,\n          // cut the last split and put this file in the next split.\n          if (acc + key.get() \u003e targetSize \u0026\u0026 acc !\u003d 0) {\n            long splitSize \u003d splitEndPosition - splitStartPosition;\n            splits.add(new FileSplit(\n                sequenceFile, splitStartPosition, splitSize, (String[])null));\n            bytesRemaining -\u003d splitSize;\n            splitStartPosition \u003d splitEndPosition;\n            acc \u003d 0L;\n          }\n          acc +\u003d key.get();\n          splitEndPosition \u003d reader.getPosition();\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close();\n        }\n      }\n      if (bytesRemaining !\u003d 0) {\n        splits.add(new FileSplit(\n            sequenceFile, splitStartPosition, bytesRemaining, (String[])null));\n      }\n\n      return splits;\n    }",
      "path": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java",
        "newPath": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,65 @@\n+    public List\u003cInputSplit\u003e getSplits(JobContext jobCtxt) throws IOException {\n+      final JobConf jobConf \u003d new JobConf(jobCtxt.getConfiguration());\n+      final JobClient client \u003d new JobClient(jobConf);\n+      ClusterStatus stat \u003d client.getClusterStatus(true);\n+      int numTrackers \u003d stat.getTaskTrackers();\n+      final int fileCount \u003d jobConf.getInt(GRIDMIX_DISTCACHE_FILE_COUNT, -1);\n+\n+      // Total size of distributed cache files to be generated\n+      final long totalSize \u003d jobConf.getLong(GRIDMIX_DISTCACHE_BYTE_COUNT, -1);\n+      // Get the path of the special file\n+      String distCacheFileList \u003d jobConf.get(GRIDMIX_DISTCACHE_FILE_LIST);\n+      if (fileCount \u003c 0 || totalSize \u003c 0 || distCacheFileList \u003d\u003d null) {\n+        throw new RuntimeException(\"Invalid metadata: #files (\" + fileCount\n+            + \"), total_size (\" + totalSize + \"), filelisturi (\"\n+            + distCacheFileList + \")\");\n+      }\n+\n+      Path sequenceFile \u003d new Path(distCacheFileList);\n+      FileSystem fs \u003d sequenceFile.getFileSystem(jobConf);\n+      FileStatus srcst \u003d fs.getFileStatus(sequenceFile);\n+      // Consider the number of TTs * mapSlotsPerTracker as number of mappers.\n+      int numMapSlotsPerTracker \u003d jobConf.getInt(TTConfig.TT_MAP_SLOTS, 2);\n+      int numSplits \u003d numTrackers * numMapSlotsPerTracker;\n+\n+      List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e(numSplits);\n+      LongWritable key \u003d new LongWritable();\n+      BytesWritable value \u003d new BytesWritable();\n+\n+      // Average size of data to be generated by each map task\n+      final long targetSize \u003d Math.max(totalSize / numSplits,\n+                                DistributedCacheEmulator.AVG_BYTES_PER_MAP);\n+      long splitStartPosition \u003d 0L;\n+      long splitEndPosition \u003d 0L;\n+      long acc \u003d 0L;\n+      long bytesRemaining \u003d srcst.getLen();\n+      SequenceFile.Reader reader \u003d null;\n+      try {\n+        reader \u003d new SequenceFile.Reader(fs, sequenceFile, jobConf);\n+        while (reader.next(key, value)) {\n+\n+          // If adding this file would put this split past the target size,\n+          // cut the last split and put this file in the next split.\n+          if (acc + key.get() \u003e targetSize \u0026\u0026 acc !\u003d 0) {\n+            long splitSize \u003d splitEndPosition - splitStartPosition;\n+            splits.add(new FileSplit(\n+                sequenceFile, splitStartPosition, splitSize, (String[])null));\n+            bytesRemaining -\u003d splitSize;\n+            splitStartPosition \u003d splitEndPosition;\n+            acc \u003d 0L;\n+          }\n+          acc +\u003d key.get();\n+          splitEndPosition \u003d reader.getPosition();\n+        }\n+      } finally {\n+        if (reader !\u003d null) {\n+          reader.close();\n+        }\n+      }\n+      if (bytesRemaining !\u003d 0) {\n+        splits.add(new FileSplit(\n+            sequenceFile, splitStartPosition, bytesRemaining, (String[])null));\n+      }\n+\n+      return splits;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public List\u003cInputSplit\u003e getSplits(JobContext jobCtxt) throws IOException {\n      final JobConf jobConf \u003d new JobConf(jobCtxt.getConfiguration());\n      final JobClient client \u003d new JobClient(jobConf);\n      ClusterStatus stat \u003d client.getClusterStatus(true);\n      int numTrackers \u003d stat.getTaskTrackers();\n      final int fileCount \u003d jobConf.getInt(GRIDMIX_DISTCACHE_FILE_COUNT, -1);\n\n      // Total size of distributed cache files to be generated\n      final long totalSize \u003d jobConf.getLong(GRIDMIX_DISTCACHE_BYTE_COUNT, -1);\n      // Get the path of the special file\n      String distCacheFileList \u003d jobConf.get(GRIDMIX_DISTCACHE_FILE_LIST);\n      if (fileCount \u003c 0 || totalSize \u003c 0 || distCacheFileList \u003d\u003d null) {\n        throw new RuntimeException(\"Invalid metadata: #files (\" + fileCount\n            + \"), total_size (\" + totalSize + \"), filelisturi (\"\n            + distCacheFileList + \")\");\n      }\n\n      Path sequenceFile \u003d new Path(distCacheFileList);\n      FileSystem fs \u003d sequenceFile.getFileSystem(jobConf);\n      FileStatus srcst \u003d fs.getFileStatus(sequenceFile);\n      // Consider the number of TTs * mapSlotsPerTracker as number of mappers.\n      int numMapSlotsPerTracker \u003d jobConf.getInt(TTConfig.TT_MAP_SLOTS, 2);\n      int numSplits \u003d numTrackers * numMapSlotsPerTracker;\n\n      List\u003cInputSplit\u003e splits \u003d new ArrayList\u003cInputSplit\u003e(numSplits);\n      LongWritable key \u003d new LongWritable();\n      BytesWritable value \u003d new BytesWritable();\n\n      // Average size of data to be generated by each map task\n      final long targetSize \u003d Math.max(totalSize / numSplits,\n                                DistributedCacheEmulator.AVG_BYTES_PER_MAP);\n      long splitStartPosition \u003d 0L;\n      long splitEndPosition \u003d 0L;\n      long acc \u003d 0L;\n      long bytesRemaining \u003d srcst.getLen();\n      SequenceFile.Reader reader \u003d null;\n      try {\n        reader \u003d new SequenceFile.Reader(fs, sequenceFile, jobConf);\n        while (reader.next(key, value)) {\n\n          // If adding this file would put this split past the target size,\n          // cut the last split and put this file in the next split.\n          if (acc + key.get() \u003e targetSize \u0026\u0026 acc !\u003d 0) {\n            long splitSize \u003d splitEndPosition - splitStartPosition;\n            splits.add(new FileSplit(\n                sequenceFile, splitStartPosition, splitSize, (String[])null));\n            bytesRemaining -\u003d splitSize;\n            splitStartPosition \u003d splitEndPosition;\n            acc \u003d 0L;\n          }\n          acc +\u003d key.get();\n          splitEndPosition \u003d reader.getPosition();\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close();\n        }\n      }\n      if (bytesRemaining !\u003d 0) {\n        splits.add(new FileSplit(\n            sequenceFile, splitStartPosition, bytesRemaining, (String[])null));\n      }\n\n      return splits;\n    }",
      "path": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java"
    }
  }
}