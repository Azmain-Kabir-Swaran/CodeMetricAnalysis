{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LocalJobRunner.java",
  "functionName": "statusUpdate",
  "functionId": "statusUpdate___taskId-TaskAttemptID__taskStatus-TaskStatus",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
  "functionStartLine": 618,
  "functionEndLine": 665,
  "numCommitsSeen": 22,
  "timeTaken": 5169,
  "changeHistory": [
    "47cca0cb6d1f4e5979d11d9a624b005e6e666f2f",
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
    "daaa73b657fb28b2d437bfb03821faaa0d458f4e",
    "cfb6a9883d2bf02c99f258e9f19ffcd83805d077",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "47cca0cb6d1f4e5979d11d9a624b005e6e666f2f": "Ymultichange(Yreturntypechange,Ybodychange)",
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4": "Ybodychange",
    "daaa73b657fb28b2d437bfb03821faaa0d458f4e": "Ybodychange",
    "cfb6a9883d2bf02c99f258e9f19ffcd83805d077": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "47cca0cb6d1f4e5979d11d9a624b005e6e666f2f": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "MAPREDUCE-5196. Add bookkeeping for managing checkpoints of task state.\nContributed by Carlo Curino\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1553939 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/12/13 1:58 PM",
      "commitName": "47cca0cb6d1f4e5979d11d9a624b005e6e666f2f",
      "commitAuthor": "Christopher Douglas",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "MAPREDUCE-5196. Add bookkeeping for managing checkpoints of task state.\nContributed by Carlo Curino\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1553939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/12/13 1:58 PM",
          "commitName": "47cca0cb6d1f4e5979d11d9a624b005e6e666f2f",
          "commitAuthor": "Christopher Douglas",
          "commitDateOld": "22/08/13 2:39 PM",
          "commitNameOld": "200220e8f3684258c281736fd31f0ebdcbac91f0",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 128.01,
          "commitsBetweenForRepo": 797,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,48 @@\n-    public synchronized boolean statusUpdate(TaskAttemptID taskId,\n+    public synchronized AMFeedback statusUpdate(TaskAttemptID taskId,\n         TaskStatus taskStatus) throws IOException, InterruptedException {\n+      AMFeedback feedback \u003d new AMFeedback();\n+      feedback.setTaskFound(true);\n+      if (null \u003d\u003d taskStatus) {\n+        return feedback;\n+      }\n       // Serialize as we would if distributed in order to make deep copy\n       ByteArrayOutputStream baos \u003d new ByteArrayOutputStream();\n       DataOutputStream dos \u003d new DataOutputStream(baos);\n       taskStatus.write(dos);\n       dos.close();\n       taskStatus \u003d TaskStatus.createTaskStatus(taskStatus.getIsMap());\n       taskStatus.readFields(new DataInputStream(\n           new ByteArrayInputStream(baos.toByteArray())));\n       \n       LOG.info(taskStatus.getStateString());\n       int mapTaskIndex \u003d mapIds.indexOf(taskId);\n       if (mapTaskIndex \u003e\u003d 0) {\n         // mapping\n         float numTasks \u003d (float) this.numMapTasks;\n \n         partialMapProgress[mapTaskIndex] \u003d taskStatus.getProgress();\n         mapCounters[mapTaskIndex] \u003d taskStatus.getCounters();\n \n         float partialProgress \u003d 0.0f;\n         for (float f : partialMapProgress) {\n           partialProgress +\u003d f;\n         }\n         status.setMapProgress(partialProgress / numTasks);\n       } else {\n         // reducing\n         int reduceTaskIndex \u003d taskId.getTaskID().getId();\n         float numTasks \u003d (float) this.numReduceTasks;\n \n         partialReduceProgress[reduceTaskIndex] \u003d taskStatus.getProgress();\n         reduceCounters[reduceTaskIndex] \u003d taskStatus.getCounters();\n \n         float partialProgress \u003d 0.0f;\n         for (float f : partialReduceProgress) {\n           partialProgress +\u003d f;\n         }\n         status.setReduceProgress(partialProgress / numTasks);\n       }\n \n       // ignore phase\n-      return true;\n+      return feedback;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public synchronized AMFeedback statusUpdate(TaskAttemptID taskId,\n        TaskStatus taskStatus) throws IOException, InterruptedException {\n      AMFeedback feedback \u003d new AMFeedback();\n      feedback.setTaskFound(true);\n      if (null \u003d\u003d taskStatus) {\n        return feedback;\n      }\n      // Serialize as we would if distributed in order to make deep copy\n      ByteArrayOutputStream baos \u003d new ByteArrayOutputStream();\n      DataOutputStream dos \u003d new DataOutputStream(baos);\n      taskStatus.write(dos);\n      dos.close();\n      taskStatus \u003d TaskStatus.createTaskStatus(taskStatus.getIsMap());\n      taskStatus.readFields(new DataInputStream(\n          new ByteArrayInputStream(baos.toByteArray())));\n      \n      LOG.info(taskStatus.getStateString());\n      int mapTaskIndex \u003d mapIds.indexOf(taskId);\n      if (mapTaskIndex \u003e\u003d 0) {\n        // mapping\n        float numTasks \u003d (float) this.numMapTasks;\n\n        partialMapProgress[mapTaskIndex] \u003d taskStatus.getProgress();\n        mapCounters[mapTaskIndex] \u003d taskStatus.getCounters();\n\n        float partialProgress \u003d 0.0f;\n        for (float f : partialMapProgress) {\n          partialProgress +\u003d f;\n        }\n        status.setMapProgress(partialProgress / numTasks);\n      } else {\n        // reducing\n        int reduceTaskIndex \u003d taskId.getTaskID().getId();\n        float numTasks \u003d (float) this.numReduceTasks;\n\n        partialReduceProgress[reduceTaskIndex] \u003d taskStatus.getProgress();\n        reduceCounters[reduceTaskIndex] \u003d taskStatus.getCounters();\n\n        float partialProgress \u003d 0.0f;\n        for (float f : partialReduceProgress) {\n          partialProgress +\u003d f;\n        }\n        status.setReduceProgress(partialProgress / numTasks);\n      }\n\n      // ignore phase\n      return feedback;\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
          "extendedDetails": {
            "oldValue": "boolean",
            "newValue": "AMFeedback"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-5196. Add bookkeeping for managing checkpoints of task state.\nContributed by Carlo Curino\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1553939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/12/13 1:58 PM",
          "commitName": "47cca0cb6d1f4e5979d11d9a624b005e6e666f2f",
          "commitAuthor": "Christopher Douglas",
          "commitDateOld": "22/08/13 2:39 PM",
          "commitNameOld": "200220e8f3684258c281736fd31f0ebdcbac91f0",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 128.01,
          "commitsBetweenForRepo": 797,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,48 @@\n-    public synchronized boolean statusUpdate(TaskAttemptID taskId,\n+    public synchronized AMFeedback statusUpdate(TaskAttemptID taskId,\n         TaskStatus taskStatus) throws IOException, InterruptedException {\n+      AMFeedback feedback \u003d new AMFeedback();\n+      feedback.setTaskFound(true);\n+      if (null \u003d\u003d taskStatus) {\n+        return feedback;\n+      }\n       // Serialize as we would if distributed in order to make deep copy\n       ByteArrayOutputStream baos \u003d new ByteArrayOutputStream();\n       DataOutputStream dos \u003d new DataOutputStream(baos);\n       taskStatus.write(dos);\n       dos.close();\n       taskStatus \u003d TaskStatus.createTaskStatus(taskStatus.getIsMap());\n       taskStatus.readFields(new DataInputStream(\n           new ByteArrayInputStream(baos.toByteArray())));\n       \n       LOG.info(taskStatus.getStateString());\n       int mapTaskIndex \u003d mapIds.indexOf(taskId);\n       if (mapTaskIndex \u003e\u003d 0) {\n         // mapping\n         float numTasks \u003d (float) this.numMapTasks;\n \n         partialMapProgress[mapTaskIndex] \u003d taskStatus.getProgress();\n         mapCounters[mapTaskIndex] \u003d taskStatus.getCounters();\n \n         float partialProgress \u003d 0.0f;\n         for (float f : partialMapProgress) {\n           partialProgress +\u003d f;\n         }\n         status.setMapProgress(partialProgress / numTasks);\n       } else {\n         // reducing\n         int reduceTaskIndex \u003d taskId.getTaskID().getId();\n         float numTasks \u003d (float) this.numReduceTasks;\n \n         partialReduceProgress[reduceTaskIndex] \u003d taskStatus.getProgress();\n         reduceCounters[reduceTaskIndex] \u003d taskStatus.getCounters();\n \n         float partialProgress \u003d 0.0f;\n         for (float f : partialReduceProgress) {\n           partialProgress +\u003d f;\n         }\n         status.setReduceProgress(partialProgress / numTasks);\n       }\n \n       // ignore phase\n-      return true;\n+      return feedback;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public synchronized AMFeedback statusUpdate(TaskAttemptID taskId,\n        TaskStatus taskStatus) throws IOException, InterruptedException {\n      AMFeedback feedback \u003d new AMFeedback();\n      feedback.setTaskFound(true);\n      if (null \u003d\u003d taskStatus) {\n        return feedback;\n      }\n      // Serialize as we would if distributed in order to make deep copy\n      ByteArrayOutputStream baos \u003d new ByteArrayOutputStream();\n      DataOutputStream dos \u003d new DataOutputStream(baos);\n      taskStatus.write(dos);\n      dos.close();\n      taskStatus \u003d TaskStatus.createTaskStatus(taskStatus.getIsMap());\n      taskStatus.readFields(new DataInputStream(\n          new ByteArrayInputStream(baos.toByteArray())));\n      \n      LOG.info(taskStatus.getStateString());\n      int mapTaskIndex \u003d mapIds.indexOf(taskId);\n      if (mapTaskIndex \u003e\u003d 0) {\n        // mapping\n        float numTasks \u003d (float) this.numMapTasks;\n\n        partialMapProgress[mapTaskIndex] \u003d taskStatus.getProgress();\n        mapCounters[mapTaskIndex] \u003d taskStatus.getCounters();\n\n        float partialProgress \u003d 0.0f;\n        for (float f : partialMapProgress) {\n          partialProgress +\u003d f;\n        }\n        status.setMapProgress(partialProgress / numTasks);\n      } else {\n        // reducing\n        int reduceTaskIndex \u003d taskId.getTaskID().getId();\n        float numTasks \u003d (float) this.numReduceTasks;\n\n        partialReduceProgress[reduceTaskIndex] \u003d taskStatus.getProgress();\n        reduceCounters[reduceTaskIndex] \u003d taskStatus.getCounters();\n\n        float partialProgress \u003d 0.0f;\n        for (float f : partialReduceProgress) {\n          partialProgress +\u003d f;\n        }\n        status.setReduceProgress(partialProgress / numTasks);\n      }\n\n      // ignore phase\n      return feedback;\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
          "extendedDetails": {}
        }
      ]
    },
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-434. LocalJobRunner limited to single reducer (Sandy Ryza and Aaron Kimball via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1510866 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/08/13 11:36 PM",
      "commitName": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
      "commitAuthor": "Sanford Ryza",
      "commitDateOld": "05/08/13 10:07 AM",
      "commitNameOld": "6ea797d0d0a65275aa6a194e97f8d016ac7803f3",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 0.56,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,43 @@\n     public synchronized boolean statusUpdate(TaskAttemptID taskId,\n         TaskStatus taskStatus) throws IOException, InterruptedException {\n       // Serialize as we would if distributed in order to make deep copy\n       ByteArrayOutputStream baos \u003d new ByteArrayOutputStream();\n       DataOutputStream dos \u003d new DataOutputStream(baos);\n       taskStatus.write(dos);\n       dos.close();\n       taskStatus \u003d TaskStatus.createTaskStatus(taskStatus.getIsMap());\n       taskStatus.readFields(new DataInputStream(\n           new ByteArrayInputStream(baos.toByteArray())));\n       \n       LOG.info(taskStatus.getStateString());\n-      int taskIndex \u003d mapIds.indexOf(taskId);\n-      if (taskIndex \u003e\u003d 0) {                       // mapping\n+      int mapTaskIndex \u003d mapIds.indexOf(taskId);\n+      if (mapTaskIndex \u003e\u003d 0) {\n+        // mapping\n         float numTasks \u003d (float) this.numMapTasks;\n \n-        partialMapProgress[taskIndex] \u003d taskStatus.getProgress();\n-        mapCounters[taskIndex] \u003d taskStatus.getCounters();\n+        partialMapProgress[mapTaskIndex] \u003d taskStatus.getProgress();\n+        mapCounters[mapTaskIndex] \u003d taskStatus.getCounters();\n \n         float partialProgress \u003d 0.0f;\n         for (float f : partialMapProgress) {\n           partialProgress +\u003d f;\n         }\n         status.setMapProgress(partialProgress / numTasks);\n       } else {\n-        reduceCounters \u003d taskStatus.getCounters();\n-        status.setReduceProgress(taskStatus.getProgress());\n+        // reducing\n+        int reduceTaskIndex \u003d taskId.getTaskID().getId();\n+        float numTasks \u003d (float) this.numReduceTasks;\n+\n+        partialReduceProgress[reduceTaskIndex] \u003d taskStatus.getProgress();\n+        reduceCounters[reduceTaskIndex] \u003d taskStatus.getCounters();\n+\n+        float partialProgress \u003d 0.0f;\n+        for (float f : partialReduceProgress) {\n+          partialProgress +\u003d f;\n+        }\n+        status.setReduceProgress(partialProgress / numTasks);\n       }\n \n       // ignore phase\n       return true;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public synchronized boolean statusUpdate(TaskAttemptID taskId,\n        TaskStatus taskStatus) throws IOException, InterruptedException {\n      // Serialize as we would if distributed in order to make deep copy\n      ByteArrayOutputStream baos \u003d new ByteArrayOutputStream();\n      DataOutputStream dos \u003d new DataOutputStream(baos);\n      taskStatus.write(dos);\n      dos.close();\n      taskStatus \u003d TaskStatus.createTaskStatus(taskStatus.getIsMap());\n      taskStatus.readFields(new DataInputStream(\n          new ByteArrayInputStream(baos.toByteArray())));\n      \n      LOG.info(taskStatus.getStateString());\n      int mapTaskIndex \u003d mapIds.indexOf(taskId);\n      if (mapTaskIndex \u003e\u003d 0) {\n        // mapping\n        float numTasks \u003d (float) this.numMapTasks;\n\n        partialMapProgress[mapTaskIndex] \u003d taskStatus.getProgress();\n        mapCounters[mapTaskIndex] \u003d taskStatus.getCounters();\n\n        float partialProgress \u003d 0.0f;\n        for (float f : partialMapProgress) {\n          partialProgress +\u003d f;\n        }\n        status.setMapProgress(partialProgress / numTasks);\n      } else {\n        // reducing\n        int reduceTaskIndex \u003d taskId.getTaskID().getId();\n        float numTasks \u003d (float) this.numReduceTasks;\n\n        partialReduceProgress[reduceTaskIndex] \u003d taskStatus.getProgress();\n        reduceCounters[reduceTaskIndex] \u003d taskStatus.getCounters();\n\n        float partialProgress \u003d 0.0f;\n        for (float f : partialReduceProgress) {\n          partialProgress +\u003d f;\n        }\n        status.setReduceProgress(partialProgress / numTasks);\n      }\n\n      // ignore phase\n      return true;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {}
    },
    "daaa73b657fb28b2d437bfb03821faaa0d458f4e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5166. Fix ConcurrentModificationException due to insufficient synchronization on updates to task Counters. Contributed by Sandy Ryza.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1471796 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/04/13 5:54 PM",
      "commitName": "daaa73b657fb28b2d437bfb03821faaa0d458f4e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "08/01/13 8:26 AM",
      "commitNameOld": "0f1f5491bc4c6ce9e56c178430a7bb05e2b25843",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 106.35,
      "commitsBetweenForRepo": 556,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,32 @@\n     public synchronized boolean statusUpdate(TaskAttemptID taskId,\n         TaskStatus taskStatus) throws IOException, InterruptedException {\n+      // Serialize as we would if distributed in order to make deep copy\n+      ByteArrayOutputStream baos \u003d new ByteArrayOutputStream();\n+      DataOutputStream dos \u003d new DataOutputStream(baos);\n+      taskStatus.write(dos);\n+      dos.close();\n+      taskStatus \u003d TaskStatus.createTaskStatus(taskStatus.getIsMap());\n+      taskStatus.readFields(new DataInputStream(\n+          new ByteArrayInputStream(baos.toByteArray())));\n+      \n       LOG.info(taskStatus.getStateString());\n       int taskIndex \u003d mapIds.indexOf(taskId);\n       if (taskIndex \u003e\u003d 0) {                       // mapping\n         float numTasks \u003d (float) this.numMapTasks;\n \n         partialMapProgress[taskIndex] \u003d taskStatus.getProgress();\n         mapCounters[taskIndex] \u003d taskStatus.getCounters();\n \n         float partialProgress \u003d 0.0f;\n         for (float f : partialMapProgress) {\n           partialProgress +\u003d f;\n         }\n         status.setMapProgress(partialProgress / numTasks);\n       } else {\n         reduceCounters \u003d taskStatus.getCounters();\n         status.setReduceProgress(taskStatus.getProgress());\n       }\n \n       // ignore phase\n       return true;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public synchronized boolean statusUpdate(TaskAttemptID taskId,\n        TaskStatus taskStatus) throws IOException, InterruptedException {\n      // Serialize as we would if distributed in order to make deep copy\n      ByteArrayOutputStream baos \u003d new ByteArrayOutputStream();\n      DataOutputStream dos \u003d new DataOutputStream(baos);\n      taskStatus.write(dos);\n      dos.close();\n      taskStatus \u003d TaskStatus.createTaskStatus(taskStatus.getIsMap());\n      taskStatus.readFields(new DataInputStream(\n          new ByteArrayInputStream(baos.toByteArray())));\n      \n      LOG.info(taskStatus.getStateString());\n      int taskIndex \u003d mapIds.indexOf(taskId);\n      if (taskIndex \u003e\u003d 0) {                       // mapping\n        float numTasks \u003d (float) this.numMapTasks;\n\n        partialMapProgress[taskIndex] \u003d taskStatus.getProgress();\n        mapCounters[taskIndex] \u003d taskStatus.getCounters();\n\n        float partialProgress \u003d 0.0f;\n        for (float f : partialMapProgress) {\n          partialProgress +\u003d f;\n        }\n        status.setMapProgress(partialProgress / numTasks);\n      } else {\n        reduceCounters \u003d taskStatus.getCounters();\n        status.setReduceProgress(taskStatus.getProgress());\n      }\n\n      // ignore phase\n      return true;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {}
    },
    "cfb6a9883d2bf02c99f258e9f19ffcd83805d077": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3237. Move LocalJobRunner to hadoop-mapreduce-client-core. Contributed by Tom White.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195792 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/10/11 8:16 PM",
      "commitName": "cfb6a9883d2bf02c99f258e9f19ffcd83805d077",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "31/10/11 7:09 PM",
      "commitNameOld": "e5badc0c1a817ca8f7e4255ec4dcfdf858abb596",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.05,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public synchronized boolean statusUpdate(TaskAttemptID taskId,\n        TaskStatus taskStatus) throws IOException, InterruptedException {\n      LOG.info(taskStatus.getStateString());\n      int taskIndex \u003d mapIds.indexOf(taskId);\n      if (taskIndex \u003e\u003d 0) {                       // mapping\n        float numTasks \u003d (float) this.numMapTasks;\n\n        partialMapProgress[taskIndex] \u003d taskStatus.getProgress();\n        mapCounters[taskIndex] \u003d taskStatus.getCounters();\n\n        float partialProgress \u003d 0.0f;\n        for (float f : partialMapProgress) {\n          partialProgress +\u003d f;\n        }\n        status.setMapProgress(partialProgress / numTasks);\n      } else {\n        reduceCounters \u003d taskStatus.getCounters();\n        status.setReduceProgress(taskStatus.getProgress());\n      }\n\n      // ignore phase\n      return true;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public synchronized boolean statusUpdate(TaskAttemptID taskId,\n        TaskStatus taskStatus) throws IOException, InterruptedException {\n      LOG.info(taskStatus.getStateString());\n      int taskIndex \u003d mapIds.indexOf(taskId);\n      if (taskIndex \u003e\u003d 0) {                       // mapping\n        float numTasks \u003d (float) this.numMapTasks;\n\n        partialMapProgress[taskIndex] \u003d taskStatus.getProgress();\n        mapCounters[taskIndex] \u003d taskStatus.getCounters();\n\n        float partialProgress \u003d 0.0f;\n        for (float f : partialMapProgress) {\n          partialProgress +\u003d f;\n        }\n        status.setMapProgress(partialProgress / numTasks);\n      } else {\n        reduceCounters \u003d taskStatus.getCounters();\n        status.setReduceProgress(taskStatus.getProgress());\n      }\n\n      // ignore phase\n      return true;\n    }",
      "path": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
        "newPath": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalJobRunner.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public synchronized boolean statusUpdate(TaskAttemptID taskId,\n        TaskStatus taskStatus) throws IOException, InterruptedException {\n      LOG.info(taskStatus.getStateString());\n      int taskIndex \u003d mapIds.indexOf(taskId);\n      if (taskIndex \u003e\u003d 0) {                       // mapping\n        float numTasks \u003d (float) this.numMapTasks;\n\n        partialMapProgress[taskIndex] \u003d taskStatus.getProgress();\n        mapCounters[taskIndex] \u003d taskStatus.getCounters();\n\n        float partialProgress \u003d 0.0f;\n        for (float f : partialMapProgress) {\n          partialProgress +\u003d f;\n        }\n        status.setMapProgress(partialProgress / numTasks);\n      } else {\n        reduceCounters \u003d taskStatus.getCounters();\n        status.setReduceProgress(taskStatus.getProgress());\n      }\n\n      // ignore phase\n      return true;\n    }",
      "path": "hadoop-mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
        "newPath": "hadoop-mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,23 @@\n+    public synchronized boolean statusUpdate(TaskAttemptID taskId,\n+        TaskStatus taskStatus) throws IOException, InterruptedException {\n+      LOG.info(taskStatus.getStateString());\n+      int taskIndex \u003d mapIds.indexOf(taskId);\n+      if (taskIndex \u003e\u003d 0) {                       // mapping\n+        float numTasks \u003d (float) this.numMapTasks;\n+\n+        partialMapProgress[taskIndex] \u003d taskStatus.getProgress();\n+        mapCounters[taskIndex] \u003d taskStatus.getCounters();\n+\n+        float partialProgress \u003d 0.0f;\n+        for (float f : partialMapProgress) {\n+          partialProgress +\u003d f;\n+        }\n+        status.setMapProgress(partialProgress / numTasks);\n+      } else {\n+        reduceCounters \u003d taskStatus.getCounters();\n+        status.setReduceProgress(taskStatus.getProgress());\n+      }\n+\n+      // ignore phase\n+      return true;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public synchronized boolean statusUpdate(TaskAttemptID taskId,\n        TaskStatus taskStatus) throws IOException, InterruptedException {\n      LOG.info(taskStatus.getStateString());\n      int taskIndex \u003d mapIds.indexOf(taskId);\n      if (taskIndex \u003e\u003d 0) {                       // mapping\n        float numTasks \u003d (float) this.numMapTasks;\n\n        partialMapProgress[taskIndex] \u003d taskStatus.getProgress();\n        mapCounters[taskIndex] \u003d taskStatus.getCounters();\n\n        float partialProgress \u003d 0.0f;\n        for (float f : partialMapProgress) {\n          partialProgress +\u003d f;\n        }\n        status.setMapProgress(partialProgress / numTasks);\n      } else {\n        reduceCounters \u003d taskStatus.getCounters();\n        status.setReduceProgress(taskStatus.getProgress());\n      }\n\n      // ignore phase\n      return true;\n    }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java"
    }
  }
}