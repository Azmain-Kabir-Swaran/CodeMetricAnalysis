{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DumpS3GuardDynamoTable.java",
  "functionName": "dumpRawS3ObjectStore",
  "functionId": "dumpRawS3ObjectStore___csv-CsvFile(modifiers-final)",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DumpS3GuardDynamoTable.java",
  "functionStartLine": 347,
  "functionEndLine": 366,
  "numCommitsSeen": 2,
  "timeTaken": 1145,
  "changeHistory": [
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0"
  ],
  "changeHistoryShort": {
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-16384: S3A: Avoid inconsistencies between DDB and S3.\n\nContributed by Steve Loughran\n\nContains\n\n- HADOOP-16397. Hadoop S3Guard Prune command to support a -tombstone option.\n- HADOOP-16406. ITestDynamoDBMetadataStore.testProvisionTable times out intermittently\n\nThis patch doesn\u0027t fix the underlying problem but it\n\n* changes some tests to clean up better\n* does a lot more in logging operations in against DDB, if enabled\n* adds an entry point to dump the state of the metastore and s3 tables (precursor to fsck)\n* adds a purge entry point to help clean up after a test run has got a store into a mess\n* s3guard prune command adds -tombstone option to only clear tombstones\n\nThe outcome is that tests should pass consistently and if problems occur we have better diagnostics.\n\nChange-Id: I3eca3f5529d7f6fec398c0ff0472919f08f054eb\n",
      "commitDate": "12/07/19 5:02 AM",
      "commitName": "b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,20 @@\n+  protected long dumpRawS3ObjectStore(\n+      final CsvFile csv) throws IOException {\n+    S3AFileSystem fs \u003d getFilesystem();\n+    Path rootPath \u003d fs.qualify(new Path(\"/\"));\n+    Listing listing \u003d new Listing(fs);\n+    S3ListRequest request \u003d fs.createListObjectsRequest(\"\", null);\n+    long count \u003d 0;\n+    RemoteIterator\u003cS3AFileStatus\u003e st \u003d\n+        listing.createFileStatusListingIterator(rootPath, request,\n+            ACCEPT_ALL,\n+            new Listing.AcceptAllButSelfAndS3nDirs(rootPath));\n+    while (st.hasNext()) {\n+      count++;\n+      S3AFileStatus next \u003d st.next();\n+      LOG.debug(\"[{}] {}\", count, next);\n+      csv.entry(next);\n+    }\n+    LOG.info(\"entry count: {}\", count);\n+    return count;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected long dumpRawS3ObjectStore(\n      final CsvFile csv) throws IOException {\n    S3AFileSystem fs \u003d getFilesystem();\n    Path rootPath \u003d fs.qualify(new Path(\"/\"));\n    Listing listing \u003d new Listing(fs);\n    S3ListRequest request \u003d fs.createListObjectsRequest(\"\", null);\n    long count \u003d 0;\n    RemoteIterator\u003cS3AFileStatus\u003e st \u003d\n        listing.createFileStatusListingIterator(rootPath, request,\n            ACCEPT_ALL,\n            new Listing.AcceptAllButSelfAndS3nDirs(rootPath));\n    while (st.hasNext()) {\n      count++;\n      S3AFileStatus next \u003d st.next();\n      LOG.debug(\"[{}] {}\", count, next);\n      csv.entry(next);\n    }\n    LOG.info(\"entry count: {}\", count);\n    return count;\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DumpS3GuardDynamoTable.java"
    }
  }
}