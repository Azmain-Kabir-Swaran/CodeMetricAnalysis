{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "INodeFile.java",
  "functionName": "storagespaceConsumed",
  "functionId": "storagespaceConsumed___bsp-BlockStoragePolicy",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
  "functionStartLine": 1002,
  "functionEndLine": 1008,
  "numCommitsSeen": 459,
  "timeTaken": 10737,
  "changeHistory": [
    "7e091de1366f4b57b5433bc19d738199dc05313d",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "97a2396af685838c9fcb31e48573e758c124d8d7",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b",
    "6d5da9484185ca9f585195d6da069b9cd5be4044",
    "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
    "f2231cebcddc80f0b753c4a7cb45ee4040846951",
    "5dae97a584d30cef3e34141edfaca49c4ec57913",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f",
    "3b3ea5c4220e674064c7603a449f63904c10bac1",
    "c7cf85ccb4ff2f58839e113f1baf903a468b606d",
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "7e091de1366f4b57b5433bc19d738199dc05313d": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "97a2396af685838c9fcb31e48573e758c124d8d7": "Ybodychange",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": "Ybodychange",
    "6d5da9484185ca9f585195d6da069b9cd5be4044": "Ybodychange",
    "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Ybodychange)",
    "f2231cebcddc80f0b753c4a7cb45ee4040846951": "Yrename",
    "5dae97a584d30cef3e34141edfaca49c4ec57913": "Ymultichange(Yrename,Ybodychange)",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": "Ybodychange",
    "3b3ea5c4220e674064c7603a449f63904c10bac1": "Ymodifierchange",
    "c7cf85ccb4ff2f58839e113f1baf903a468b606d": "Ymodifierchange",
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7e091de1366f4b57b5433bc19d738199dc05313d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8058. Erasure coding: use BlockInfo[] for both striped and contiguous blocks in INodeFile. Contributed by Zhe Zhang and Yi Liu.\n",
      "commitDate": "15/07/15 9:49 AM",
      "commitName": "7e091de1366f4b57b5433bc19d738199dc05313d",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "06/07/15 4:14 AM",
      "commitNameOld": "ee01a09500224136464f2c3e0a5d9ba53242d93f",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 9.23,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,7 +1,7 @@\n   public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n     if (isStriped()) {\n-      return storagespaceConsumedWithStriped();\n+      return storagespaceConsumedStriped();\n     } else {\n-      return storagespaceConsumedWithReplication(bsp);\n+      return storagespaceConsumedContiguous(bsp);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n    if (isStriped()) {\n      return storagespaceConsumedStriped();\n    } else {\n      return storagespaceConsumedContiguous(bsp);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "13/05/15 9:50 PM",
      "commitNameOld": "b2c85db86c9a62b0a03ee87547265077f664970a",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 13.74,
      "commitsBetweenForRepo": 96,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,35 @@\n   public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n     QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n-    final Iterable\u003cBlockInfoContiguous\u003e blocks;\n+    final Iterable\u003cBlockInfo\u003e blocks;\n     FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n     if (sf \u003d\u003d null) {\n       blocks \u003d Arrays.asList(getBlocks());\n     } else {\n       // Collect all distinct blocks\n-      Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n+      Set\u003cBlockInfo\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n       List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n       for(FileDiff diff : diffs) {\n-        BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n+        BlockInfo[] diffBlocks \u003d diff.getBlocks();\n         if (diffBlocks !\u003d null) {\n           allBlocks.addAll(Arrays.asList(diffBlocks));\n         }\n       }\n       blocks \u003d allBlocks;\n     }\n \n     final short replication \u003d getPreferredBlockReplication();\n-    for (BlockInfoContiguous b : blocks) {\n+    for (BlockInfo b : blocks) {\n       long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n           getPreferredBlockSize();\n       counts.addStorageSpace(blockSize * replication);\n       if (bsp !\u003d null) {\n         List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n         for (StorageType t : types) {\n           if (t.supportTypeQuota()) {\n             counts.addTypeSpace(t, blockSize);\n           }\n         }\n       }\n     }\n     return counts;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n    final Iterable\u003cBlockInfo\u003e blocks;\n    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n    if (sf \u003d\u003d null) {\n      blocks \u003d Arrays.asList(getBlocks());\n    } else {\n      // Collect all distinct blocks\n      Set\u003cBlockInfo\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n      List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n      for(FileDiff diff : diffs) {\n        BlockInfo[] diffBlocks \u003d diff.getBlocks();\n        if (diffBlocks !\u003d null) {\n          allBlocks.addAll(Arrays.asList(diffBlocks));\n        }\n      }\n      blocks \u003d allBlocks;\n    }\n\n    final short replication \u003d getPreferredBlockReplication();\n    for (BlockInfo b : blocks) {\n      long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n          getPreferredBlockSize();\n      counts.addStorageSpace(blockSize * replication);\n      if (bsp !\u003d null) {\n        List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n        for (StorageType t : types) {\n          if (t.supportTypeQuota()) {\n            counts.addTypeSpace(t, blockSize);\n          }\n        }\n      }\n    }\n    return counts;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "97a2396af685838c9fcb31e48573e758c124d8d7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8372. Erasure coding: compute storage type quotas for striped files, to be consistent with HDFS-8327. Contributed by Zhe Zhang.\n",
      "commitDate": "26/05/15 12:01 PM",
      "commitName": "97a2396af685838c9fcb31e48573e758c124d8d7",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 12:01 PM",
      "commitNameOld": "6bacaa9a5233cbad7f311ccd9d8f8dc9375c732d",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,7 @@\n   public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n-    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n     if (isStriped()) {\n-      return storagespaceConsumedWithStriped(bsp);\n+      return storagespaceConsumedWithStriped();\n     } else {\n       return storagespaceConsumedWithReplication(bsp);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n    if (isStriped()) {\n      return storagespaceConsumedWithStriped();\n    } else {\n      return storagespaceConsumedWithReplication(bsp);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7749. Erasure Coding: Add striped block support in INodeFile. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:03 AM",
      "commitNameOld": "bc2833b1c91e107d090619d755c584f6eae82327",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,8 @@\n   public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n     QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n-    final Iterable\u003cBlockInfoContiguous\u003e blocks;\n-    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n-    if (sf \u003d\u003d null) {\n-      blocks \u003d Arrays.asList(getBlocks());\n+    if (isStriped()) {\n+      return storagespaceConsumedWithStriped(bsp);\n     } else {\n-      // Collect all distinct blocks\n-      Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n-      List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n-      for(FileDiff diff : diffs) {\n-        BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n-        if (diffBlocks !\u003d null) {\n-          allBlocks.addAll(Arrays.asList(diffBlocks));\n-        }\n-      }\n-      blocks \u003d allBlocks;\n+      return storagespaceConsumedWithReplication(bsp);\n     }\n-\n-    final short replication \u003d getPreferredBlockReplication();\n-    for (BlockInfoContiguous b : blocks) {\n-      long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n-          getPreferredBlockSize();\n-      counts.addStorageSpace(blockSize * replication);\n-      if (bsp !\u003d null) {\n-        List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n-        for (StorageType t : types) {\n-          if (t.supportTypeQuota()) {\n-            counts.addTypeSpace(t, blockSize);\n-          }\n-        }\n-      }\n-    }\n-    return counts;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n    if (isStriped()) {\n      return storagespaceConsumedWithStriped(bsp);\n    } else {\n      return storagespaceConsumedWithReplication(bsp);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "6d5da9484185ca9f585195d6da069b9cd5be4044": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8255. Rename getBlockReplication to getPreferredBlockReplication. (Contributed by Zhe Zhang)\n",
      "commitDate": "12/05/15 6:29 AM",
      "commitName": "6d5da9484185ca9f585195d6da069b9cd5be4044",
      "commitAuthor": "yliu",
      "commitDateOld": "09/05/15 10:51 PM",
      "commitNameOld": "4536399d47f6c061e149e2504600804a0f1e093d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 2.32,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,35 @@\n   public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n     QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n     final Iterable\u003cBlockInfoContiguous\u003e blocks;\n     FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n     if (sf \u003d\u003d null) {\n       blocks \u003d Arrays.asList(getBlocks());\n     } else {\n       // Collect all distinct blocks\n       Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n       List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n       for(FileDiff diff : diffs) {\n         BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n         if (diffBlocks !\u003d null) {\n           allBlocks.addAll(Arrays.asList(diffBlocks));\n         }\n       }\n       blocks \u003d allBlocks;\n     }\n \n-    final short replication \u003d getBlockReplication();\n+    final short replication \u003d getPreferredBlockReplication();\n     for (BlockInfoContiguous b : blocks) {\n       long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n           getPreferredBlockSize();\n       counts.addStorageSpace(blockSize * replication);\n       if (bsp !\u003d null) {\n         List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n         for (StorageType t : types) {\n           if (t.supportTypeQuota()) {\n             counts.addTypeSpace(t, blockSize);\n           }\n         }\n       }\n     }\n     return counts;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n    final Iterable\u003cBlockInfoContiguous\u003e blocks;\n    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n    if (sf \u003d\u003d null) {\n      blocks \u003d Arrays.asList(getBlocks());\n    } else {\n      // Collect all distinct blocks\n      Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n      List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n      for(FileDiff diff : diffs) {\n        BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n        if (diffBlocks !\u003d null) {\n          allBlocks.addAll(Arrays.asList(diffBlocks));\n        }\n      }\n      blocks \u003d allBlocks;\n    }\n\n    final short replication \u003d getPreferredBlockReplication();\n    for (BlockInfoContiguous b : blocks) {\n      long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n          getPreferredBlockSize();\n      counts.addStorageSpace(blockSize * replication);\n      if (bsp !\u003d null) {\n        List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n        for (StorageType t : types) {\n          if (t.supportTypeQuota()) {\n            counts.addTypeSpace(t, blockSize);\n          }\n        }\n      }\n    }\n    return counts;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f": {
      "type": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-8327. Compute storage type quotas in INodeFile.computeQuotaDeltaForTruncate(). Contributed by Haohui Mai.\n",
      "commitDate": "08/05/15 11:09 PM",
      "commitName": "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-8327. Compute storage type quotas in INodeFile.computeQuotaDeltaForTruncate(). Contributed by Haohui Mai.\n",
          "commitDate": "08/05/15 11:09 PM",
          "commitName": "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "08/05/15 11:04 PM",
          "commitNameOld": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,35 @@\n-  public final long storagespaceConsumedNoReplication() {\n+  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n+    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n+    final Iterable\u003cBlockInfoContiguous\u003e blocks;\n     FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n-    if(sf \u003d\u003d null) {\n-      return computeFileSize(true, true);\n+    if (sf \u003d\u003d null) {\n+      blocks \u003d Arrays.asList(getBlocks());\n+    } else {\n+      // Collect all distinct blocks\n+      Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n+      List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n+      for(FileDiff diff : diffs) {\n+        BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n+        if (diffBlocks !\u003d null) {\n+          allBlocks.addAll(Arrays.asList(diffBlocks));\n+        }\n+      }\n+      blocks \u003d allBlocks;\n     }\n \n-    // Collect all distinct blocks\n-    long size \u003d 0;\n-    Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n-    List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n-    for(FileDiff diff : diffs) {\n-      BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n-      if (diffBlocks !\u003d null) {\n-        allBlocks.addAll(Arrays.asList(diffBlocks));\n+    final short replication \u003d getBlockReplication();\n+    for (BlockInfoContiguous b : blocks) {\n+      long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n+          getPreferredBlockSize();\n+      counts.addStorageSpace(blockSize * replication);\n+      if (bsp !\u003d null) {\n+        List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n+        for (StorageType t : types) {\n+          if (t.supportTypeQuota()) {\n+            counts.addTypeSpace(t, blockSize);\n+          }\n+        }\n       }\n     }\n-    for(Block block : allBlocks) {\n-      size +\u003d block.getNumBytes();\n-    }\n-    // check if the last block is under construction\n-    BlockInfoContiguous lastBlock \u003d getLastBlock();\n-    if(lastBlock !\u003d null \u0026\u0026\n-        lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n-      size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n-    }\n-    return size;\n+    return counts;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n    final Iterable\u003cBlockInfoContiguous\u003e blocks;\n    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n    if (sf \u003d\u003d null) {\n      blocks \u003d Arrays.asList(getBlocks());\n    } else {\n      // Collect all distinct blocks\n      Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n      List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n      for(FileDiff diff : diffs) {\n        BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n        if (diffBlocks !\u003d null) {\n          allBlocks.addAll(Arrays.asList(diffBlocks));\n        }\n      }\n      blocks \u003d allBlocks;\n    }\n\n    final short replication \u003d getBlockReplication();\n    for (BlockInfoContiguous b : blocks) {\n      long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n          getPreferredBlockSize();\n      counts.addStorageSpace(blockSize * replication);\n      if (bsp !\u003d null) {\n        List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n        for (StorageType t : types) {\n          if (t.supportTypeQuota()) {\n            counts.addTypeSpace(t, blockSize);\n          }\n        }\n      }\n    }\n    return counts;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {
            "oldValue": "storagespaceConsumedNoReplication",
            "newValue": "storagespaceConsumed"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8327. Compute storage type quotas in INodeFile.computeQuotaDeltaForTruncate(). Contributed by Haohui Mai.\n",
          "commitDate": "08/05/15 11:09 PM",
          "commitName": "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "08/05/15 11:04 PM",
          "commitNameOld": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,35 @@\n-  public final long storagespaceConsumedNoReplication() {\n+  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n+    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n+    final Iterable\u003cBlockInfoContiguous\u003e blocks;\n     FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n-    if(sf \u003d\u003d null) {\n-      return computeFileSize(true, true);\n+    if (sf \u003d\u003d null) {\n+      blocks \u003d Arrays.asList(getBlocks());\n+    } else {\n+      // Collect all distinct blocks\n+      Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n+      List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n+      for(FileDiff diff : diffs) {\n+        BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n+        if (diffBlocks !\u003d null) {\n+          allBlocks.addAll(Arrays.asList(diffBlocks));\n+        }\n+      }\n+      blocks \u003d allBlocks;\n     }\n \n-    // Collect all distinct blocks\n-    long size \u003d 0;\n-    Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n-    List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n-    for(FileDiff diff : diffs) {\n-      BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n-      if (diffBlocks !\u003d null) {\n-        allBlocks.addAll(Arrays.asList(diffBlocks));\n+    final short replication \u003d getBlockReplication();\n+    for (BlockInfoContiguous b : blocks) {\n+      long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n+          getPreferredBlockSize();\n+      counts.addStorageSpace(blockSize * replication);\n+      if (bsp !\u003d null) {\n+        List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n+        for (StorageType t : types) {\n+          if (t.supportTypeQuota()) {\n+            counts.addTypeSpace(t, blockSize);\n+          }\n+        }\n       }\n     }\n-    for(Block block : allBlocks) {\n-      size +\u003d block.getNumBytes();\n-    }\n-    // check if the last block is under construction\n-    BlockInfoContiguous lastBlock \u003d getLastBlock();\n-    if(lastBlock !\u003d null \u0026\u0026\n-        lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n-      size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n-    }\n-    return size;\n+    return counts;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n    final Iterable\u003cBlockInfoContiguous\u003e blocks;\n    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n    if (sf \u003d\u003d null) {\n      blocks \u003d Arrays.asList(getBlocks());\n    } else {\n      // Collect all distinct blocks\n      Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n      List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n      for(FileDiff diff : diffs) {\n        BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n        if (diffBlocks !\u003d null) {\n          allBlocks.addAll(Arrays.asList(diffBlocks));\n        }\n      }\n      blocks \u003d allBlocks;\n    }\n\n    final short replication \u003d getBlockReplication();\n    for (BlockInfoContiguous b : blocks) {\n      long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n          getPreferredBlockSize();\n      counts.addStorageSpace(blockSize * replication);\n      if (bsp !\u003d null) {\n        List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n        for (StorageType t : types) {\n          if (t.supportTypeQuota()) {\n            counts.addTypeSpace(t, blockSize);\n          }\n        }\n      }\n    }\n    return counts;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[bsp-BlockStoragePolicy]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-8327. Compute storage type quotas in INodeFile.computeQuotaDeltaForTruncate(). Contributed by Haohui Mai.\n",
          "commitDate": "08/05/15 11:09 PM",
          "commitName": "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "08/05/15 11:04 PM",
          "commitNameOld": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,35 @@\n-  public final long storagespaceConsumedNoReplication() {\n+  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n+    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n+    final Iterable\u003cBlockInfoContiguous\u003e blocks;\n     FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n-    if(sf \u003d\u003d null) {\n-      return computeFileSize(true, true);\n+    if (sf \u003d\u003d null) {\n+      blocks \u003d Arrays.asList(getBlocks());\n+    } else {\n+      // Collect all distinct blocks\n+      Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n+      List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n+      for(FileDiff diff : diffs) {\n+        BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n+        if (diffBlocks !\u003d null) {\n+          allBlocks.addAll(Arrays.asList(diffBlocks));\n+        }\n+      }\n+      blocks \u003d allBlocks;\n     }\n \n-    // Collect all distinct blocks\n-    long size \u003d 0;\n-    Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n-    List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n-    for(FileDiff diff : diffs) {\n-      BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n-      if (diffBlocks !\u003d null) {\n-        allBlocks.addAll(Arrays.asList(diffBlocks));\n+    final short replication \u003d getBlockReplication();\n+    for (BlockInfoContiguous b : blocks) {\n+      long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n+          getPreferredBlockSize();\n+      counts.addStorageSpace(blockSize * replication);\n+      if (bsp !\u003d null) {\n+        List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n+        for (StorageType t : types) {\n+          if (t.supportTypeQuota()) {\n+            counts.addTypeSpace(t, blockSize);\n+          }\n+        }\n       }\n     }\n-    for(Block block : allBlocks) {\n-      size +\u003d block.getNumBytes();\n-    }\n-    // check if the last block is under construction\n-    BlockInfoContiguous lastBlock \u003d getLastBlock();\n-    if(lastBlock !\u003d null \u0026\u0026\n-        lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n-      size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n-    }\n-    return size;\n+    return counts;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n    final Iterable\u003cBlockInfoContiguous\u003e blocks;\n    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n    if (sf \u003d\u003d null) {\n      blocks \u003d Arrays.asList(getBlocks());\n    } else {\n      // Collect all distinct blocks\n      Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n      List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n      for(FileDiff diff : diffs) {\n        BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n        if (diffBlocks !\u003d null) {\n          allBlocks.addAll(Arrays.asList(diffBlocks));\n        }\n      }\n      blocks \u003d allBlocks;\n    }\n\n    final short replication \u003d getBlockReplication();\n    for (BlockInfoContiguous b : blocks) {\n      long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n          getPreferredBlockSize();\n      counts.addStorageSpace(blockSize * replication);\n      if (bsp !\u003d null) {\n        List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n        for (StorageType t : types) {\n          if (t.supportTypeQuota()) {\n            counts.addTypeSpace(t, blockSize);\n          }\n        }\n      }\n    }\n    return counts;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {
            "oldValue": "long",
            "newValue": "QuotaCounts"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8327. Compute storage type quotas in INodeFile.computeQuotaDeltaForTruncate(). Contributed by Haohui Mai.\n",
          "commitDate": "08/05/15 11:09 PM",
          "commitName": "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "08/05/15 11:04 PM",
          "commitNameOld": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,35 @@\n-  public final long storagespaceConsumedNoReplication() {\n+  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n+    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n+    final Iterable\u003cBlockInfoContiguous\u003e blocks;\n     FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n-    if(sf \u003d\u003d null) {\n-      return computeFileSize(true, true);\n+    if (sf \u003d\u003d null) {\n+      blocks \u003d Arrays.asList(getBlocks());\n+    } else {\n+      // Collect all distinct blocks\n+      Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n+      List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n+      for(FileDiff diff : diffs) {\n+        BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n+        if (diffBlocks !\u003d null) {\n+          allBlocks.addAll(Arrays.asList(diffBlocks));\n+        }\n+      }\n+      blocks \u003d allBlocks;\n     }\n \n-    // Collect all distinct blocks\n-    long size \u003d 0;\n-    Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n-    List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n-    for(FileDiff diff : diffs) {\n-      BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n-      if (diffBlocks !\u003d null) {\n-        allBlocks.addAll(Arrays.asList(diffBlocks));\n+    final short replication \u003d getBlockReplication();\n+    for (BlockInfoContiguous b : blocks) {\n+      long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n+          getPreferredBlockSize();\n+      counts.addStorageSpace(blockSize * replication);\n+      if (bsp !\u003d null) {\n+        List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n+        for (StorageType t : types) {\n+          if (t.supportTypeQuota()) {\n+            counts.addTypeSpace(t, blockSize);\n+          }\n+        }\n       }\n     }\n-    for(Block block : allBlocks) {\n-      size +\u003d block.getNumBytes();\n-    }\n-    // check if the last block is under construction\n-    BlockInfoContiguous lastBlock \u003d getLastBlock();\n-    if(lastBlock !\u003d null \u0026\u0026\n-        lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n-      size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n-    }\n-    return size;\n+    return counts;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final QuotaCounts storagespaceConsumed(BlockStoragePolicy bsp) {\n    QuotaCounts counts \u003d new QuotaCounts.Builder().build();\n    final Iterable\u003cBlockInfoContiguous\u003e blocks;\n    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n    if (sf \u003d\u003d null) {\n      blocks \u003d Arrays.asList(getBlocks());\n    } else {\n      // Collect all distinct blocks\n      Set\u003cBlockInfoContiguous\u003e allBlocks \u003d new HashSet\u003c\u003e(Arrays.asList(getBlocks()));\n      List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n      for(FileDiff diff : diffs) {\n        BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n        if (diffBlocks !\u003d null) {\n          allBlocks.addAll(Arrays.asList(diffBlocks));\n        }\n      }\n      blocks \u003d allBlocks;\n    }\n\n    final short replication \u003d getBlockReplication();\n    for (BlockInfoContiguous b : blocks) {\n      long blockSize \u003d b.isComplete() ? b.getNumBytes() :\n          getPreferredBlockSize();\n      counts.addStorageSpace(blockSize * replication);\n      if (bsp !\u003d null) {\n        List\u003cStorageType\u003e types \u003d bsp.chooseStorageTypes(replication);\n        for (StorageType t : types) {\n          if (t.supportTypeQuota()) {\n            counts.addTypeSpace(t, blockSize);\n          }\n        }\n      }\n    }\n    return counts;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {}
        }
      ]
    },
    "f2231cebcddc80f0b753c4a7cb45ee4040846951": {
      "type": "Yrename",
      "commitMessage": "HDFS-7775. Use consistent naming for NN-internal quota related types and functions. (Contributed bu Xiaoyu Yao)\n",
      "commitDate": "13/02/15 9:01 PM",
      "commitName": "f2231cebcddc80f0b753c4a7cb45ee4040846951",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "11/02/15 10:41 AM",
      "commitNameOld": "5dae97a584d30cef3e34141edfaca49c4ec57913",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.43,
      "commitsBetweenForRepo": 58,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n-  public final long diskspaceConsumedNoReplication() {\n+  public final long storagespaceConsumedNoReplication() {\n     FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n     if(sf \u003d\u003d null) {\n       return computeFileSize(true, true);\n     }\n \n     // Collect all distinct blocks\n     long size \u003d 0;\n     Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n     List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n     for(FileDiff diff : diffs) {\n       BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n       if (diffBlocks !\u003d null) {\n         allBlocks.addAll(Arrays.asList(diffBlocks));\n       }\n     }\n     for(Block block : allBlocks) {\n       size +\u003d block.getNumBytes();\n     }\n     // check if the last block is under construction\n     BlockInfoContiguous lastBlock \u003d getLastBlock();\n     if(lastBlock !\u003d null \u0026\u0026\n         lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n       size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n     }\n     return size;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final long storagespaceConsumedNoReplication() {\n    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n    if(sf \u003d\u003d null) {\n      return computeFileSize(true, true);\n    }\n\n    // Collect all distinct blocks\n    long size \u003d 0;\n    Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n    List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n    for(FileDiff diff : diffs) {\n      BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n      if (diffBlocks !\u003d null) {\n        allBlocks.addAll(Arrays.asList(diffBlocks));\n      }\n    }\n    for(Block block : allBlocks) {\n      size +\u003d block.getNumBytes();\n    }\n    // check if the last block is under construction\n    BlockInfoContiguous lastBlock \u003d getLastBlock();\n    if(lastBlock !\u003d null \u0026\u0026\n        lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n      size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n    }\n    return size;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {
        "oldValue": "diskspaceConsumedNoReplication",
        "newValue": "storagespaceConsumedNoReplication"
      }
    },
    "5dae97a584d30cef3e34141edfaca49c4ec57913": {
      "type": "Ymultichange(Yrename,Ybodychange)",
      "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
      "commitDate": "11/02/15 10:41 AM",
      "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
          "commitDate": "11/02/15 10:41 AM",
          "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "08/02/15 11:51 AM",
          "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 2.95,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,27 @@\n-  public final long diskspaceConsumed() {\n+  public final long diskspaceConsumedNoReplication() {\n     FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n     if(sf \u003d\u003d null) {\n-      return computeFileSize(true, true) * getBlockReplication();\n+      return computeFileSize(true, true);\n     }\n \n     // Collect all distinct blocks\n     long size \u003d 0;\n     Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n     List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n     for(FileDiff diff : diffs) {\n       BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n       if (diffBlocks !\u003d null) {\n         allBlocks.addAll(Arrays.asList(diffBlocks));\n       }\n     }\n     for(Block block : allBlocks) {\n       size +\u003d block.getNumBytes();\n     }\n     // check if the last block is under construction\n     BlockInfoContiguous lastBlock \u003d getLastBlock();\n     if(lastBlock !\u003d null \u0026\u0026\n         lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n       size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n     }\n-    return size * getBlockReplication();\n+    return size;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final long diskspaceConsumedNoReplication() {\n    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n    if(sf \u003d\u003d null) {\n      return computeFileSize(true, true);\n    }\n\n    // Collect all distinct blocks\n    long size \u003d 0;\n    Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n    List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n    for(FileDiff diff : diffs) {\n      BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n      if (diffBlocks !\u003d null) {\n        allBlocks.addAll(Arrays.asList(diffBlocks));\n      }\n    }\n    for(Block block : allBlocks) {\n      size +\u003d block.getNumBytes();\n    }\n    // check if the last block is under construction\n    BlockInfoContiguous lastBlock \u003d getLastBlock();\n    if(lastBlock !\u003d null \u0026\u0026\n        lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n      size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n    }\n    return size;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {
            "oldValue": "diskspaceConsumed",
            "newValue": "diskspaceConsumedNoReplication"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
          "commitDate": "11/02/15 10:41 AM",
          "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "08/02/15 11:51 AM",
          "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 2.95,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,27 @@\n-  public final long diskspaceConsumed() {\n+  public final long diskspaceConsumedNoReplication() {\n     FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n     if(sf \u003d\u003d null) {\n-      return computeFileSize(true, true) * getBlockReplication();\n+      return computeFileSize(true, true);\n     }\n \n     // Collect all distinct blocks\n     long size \u003d 0;\n     Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n     List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n     for(FileDiff diff : diffs) {\n       BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n       if (diffBlocks !\u003d null) {\n         allBlocks.addAll(Arrays.asList(diffBlocks));\n       }\n     }\n     for(Block block : allBlocks) {\n       size +\u003d block.getNumBytes();\n     }\n     // check if the last block is under construction\n     BlockInfoContiguous lastBlock \u003d getLastBlock();\n     if(lastBlock !\u003d null \u0026\u0026\n         lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n       size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n     }\n-    return size * getBlockReplication();\n+    return size;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final long diskspaceConsumedNoReplication() {\n    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n    if(sf \u003d\u003d null) {\n      return computeFileSize(true, true);\n    }\n\n    // Collect all distinct blocks\n    long size \u003d 0;\n    Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n    List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n    for(FileDiff diff : diffs) {\n      BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n      if (diffBlocks !\u003d null) {\n        allBlocks.addAll(Arrays.asList(diffBlocks));\n      }\n    }\n    for(Block block : allBlocks) {\n      size +\u003d block.getNumBytes();\n    }\n    // check if the last block is under construction\n    BlockInfoContiguous lastBlock \u003d getLastBlock();\n    if(lastBlock !\u003d null \u0026\u0026\n        lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n      size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n    }\n    return size;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
          "extendedDetails": {}
        }
      ]
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/02/15 4:32 PM",
      "commitNameOld": "8cb473124c1cf1c6f68ead7bde06558ebf7ce47e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 5.8,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,27 @@\n   public final long diskspaceConsumed() {\n     FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n     if(sf \u003d\u003d null) {\n       return computeFileSize(true, true) * getBlockReplication();\n     }\n \n     // Collect all distinct blocks\n     long size \u003d 0;\n     Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n     List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n     for(FileDiff diff : diffs) {\n-      BlockInfo[] diffBlocks \u003d diff.getBlocks();\n+      BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n       if (diffBlocks !\u003d null) {\n         allBlocks.addAll(Arrays.asList(diffBlocks));\n       }\n     }\n     for(Block block : allBlocks) {\n       size +\u003d block.getNumBytes();\n     }\n     // check if the last block is under construction\n-    BlockInfo lastBlock \u003d getLastBlock();\n-    if(lastBlock !\u003d null \u0026\u0026 lastBlock instanceof BlockInfoUnderConstruction) {\n+    BlockInfoContiguous lastBlock \u003d getLastBlock();\n+    if(lastBlock !\u003d null \u0026\u0026\n+        lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n       size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n     }\n     return size * getBlockReplication();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final long diskspaceConsumed() {\n    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n    if(sf \u003d\u003d null) {\n      return computeFileSize(true, true) * getBlockReplication();\n    }\n\n    // Collect all distinct blocks\n    long size \u003d 0;\n    Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n    List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n    for(FileDiff diff : diffs) {\n      BlockInfoContiguous[] diffBlocks \u003d diff.getBlocks();\n      if (diffBlocks !\u003d null) {\n        allBlocks.addAll(Arrays.asList(diffBlocks));\n      }\n    }\n    for(Block block : allBlocks) {\n      size +\u003d block.getNumBytes();\n    }\n    // check if the last block is under construction\n    BlockInfoContiguous lastBlock \u003d getLastBlock();\n    if(lastBlock !\u003d null \u0026\u0026\n        lastBlock instanceof BlockInfoContiguousUnderConstruction) {\n      size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n    }\n    return size * getBlockReplication();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7056. Snapshot support for truncate. Contributed by Konstantin Shvachko and Plamen Jeliazkov.",
      "commitDate": "13/01/15 12:24 AM",
      "commitName": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "12/01/15 10:50 PM",
      "commitNameOld": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
      "commitAuthorOld": "Plamen Jeliazkov",
      "daysBetweenCommits": 0.07,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,4 +1,26 @@\n   public final long diskspaceConsumed() {\n-    // use preferred block size for the last block if it is under construction\n-    return computeFileSize(true, true) * getBlockReplication();\n+    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n+    if(sf \u003d\u003d null) {\n+      return computeFileSize(true, true) * getBlockReplication();\n+    }\n+\n+    // Collect all distinct blocks\n+    long size \u003d 0;\n+    Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n+    List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n+    for(FileDiff diff : diffs) {\n+      BlockInfo[] diffBlocks \u003d diff.getBlocks();\n+      if (diffBlocks !\u003d null) {\n+        allBlocks.addAll(Arrays.asList(diffBlocks));\n+      }\n+    }\n+    for(Block block : allBlocks) {\n+      size +\u003d block.getNumBytes();\n+    }\n+    // check if the last block is under construction\n+    BlockInfo lastBlock \u003d getLastBlock();\n+    if(lastBlock !\u003d null \u0026\u0026 lastBlock instanceof BlockInfoUnderConstruction) {\n+      size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n+    }\n+    return size * getBlockReplication();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final long diskspaceConsumed() {\n    FileWithSnapshotFeature sf \u003d getFileWithSnapshotFeature();\n    if(sf \u003d\u003d null) {\n      return computeFileSize(true, true) * getBlockReplication();\n    }\n\n    // Collect all distinct blocks\n    long size \u003d 0;\n    Set\u003cBlock\u003e allBlocks \u003d new HashSet\u003cBlock\u003e(Arrays.asList(getBlocks()));\n    List\u003cFileDiff\u003e diffs \u003d sf.getDiffs().asList();\n    for(FileDiff diff : diffs) {\n      BlockInfo[] diffBlocks \u003d diff.getBlocks();\n      if (diffBlocks !\u003d null) {\n        allBlocks.addAll(Arrays.asList(diffBlocks));\n      }\n    }\n    for(Block block : allBlocks) {\n      size +\u003d block.getNumBytes();\n    }\n    // check if the last block is under construction\n    BlockInfo lastBlock \u003d getLastBlock();\n    if(lastBlock !\u003d null \u0026\u0026 lastBlock instanceof BlockInfoUnderConstruction) {\n      size +\u003d getPreferredBlockSize() - lastBlock.getNumBytes();\n    }\n    return size * getBlockReplication();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "3b3ea5c4220e674064c7603a449f63904c10bac1": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-4563. Update namespace/diskspace usage after deleting snapshots.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1455396 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/03/13 7:41 PM",
      "commitName": "3b3ea5c4220e674064c7603a449f63904c10bac1",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "10/03/13 8:45 PM",
      "commitNameOld": "43f8d0b9c9e209eb503451613c2f8d3fed07c203",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.96,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,4 +1,4 @@\n-  final long diskspaceConsumed() {\n+  public final long diskspaceConsumed() {\n     // use preferred block size for the last block if it is under construction\n     return computeFileSize(true, true) * getBlockReplication();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final long diskspaceConsumed() {\n    // use preferred block size for the last block if it is under construction\n    return computeFileSize(true, true) * getBlockReplication();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {
        "oldValue": "[final]",
        "newValue": "[public, final]"
      }
    },
    "c7cf85ccb4ff2f58839e113f1baf903a468b606d": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-4507. Update quota verification for snapshots.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1451081 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/02/13 7:08 PM",
      "commitName": "c7cf85ccb4ff2f58839e113f1baf903a468b606d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "26/02/13 2:04 PM",
      "commitNameOld": "e2a618e1cc3fb99115547af6540932860dc6766e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.21,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,4 +1,4 @@\n-  long diskspaceConsumed() {\n+  final long diskspaceConsumed() {\n     // use preferred block size for the last block if it is under construction\n     return computeFileSize(true, true) * getBlockReplication();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  final long diskspaceConsumed() {\n    // use preferred block size for the last block if it is under construction\n    return computeFileSize(true, true) * getBlockReplication();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[final]"
      }
    },
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4503. Update computeContentSummary(..), spaceConsumedInTree(..) and diskspaceConsumed(..) in INode for snapshot.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1448373 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/02/13 12:02 PM",
      "commitName": "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "14/02/13 2:00 PM",
      "commitNameOld": "d42d0860cb670c8284bb298029cd6f8f59db9510",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.92,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,3 +1,4 @@\n   long diskspaceConsumed() {\n-    return diskspaceConsumed(blocks);\n+    // use preferred block size for the last block if it is under construction\n+    return computeFileSize(true, true) * getBlockReplication();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long diskspaceConsumed() {\n    // use preferred block size for the last block if it is under construction\n    return computeFileSize(true, true) * getBlockReplication();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  long diskspaceConsumed() {\n    return diskspaceConsumed(blocks);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  long diskspaceConsumed() {\n    return diskspaceConsumed(blocks);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,3 @@\n+  long diskspaceConsumed() {\n+    return diskspaceConsumed(blocks);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  long diskspaceConsumed() {\n    return diskspaceConsumed(blocks);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java"
    }
  }
}