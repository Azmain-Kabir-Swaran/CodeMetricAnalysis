{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockPoolSliceStorage.java",
  "functionName": "recoverTransitionRead",
  "functionId": "recoverTransitionRead___nsInfo-NamespaceInfo__location-StorageLocation__startOpt-StartupOption__callables-List__Callable__StorageDirectory______conf-Configuration",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
  "functionStartLine": 252,
  "functionEndLine": 264,
  "numCommitsSeen": 151,
  "timeTaken": 10212,
  "changeHistory": [
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a",
    "f209e93566b159c22054dcb276e45f23a2b7b7d1",
    "96b12662ea76e3ded4ef13944fc8df206cfb4613",
    "66289a3bf403f307844ea0b6ceed35b603d12c0b",
    "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
    "a9331fe9b071fdcdae0c6c747d7b6b306142e671",
    "eca80dca3ee0888304519ec96e9e4113cc35b112",
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16",
    "5df82fa01d26c18685ad7617128dbc2913547cb3",
    "00067895a01c66d53715b50bbcb3605efd6425f2",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de",
    "6c0ccb5989c2053f5a1ebab0dd9fdb7b4019fda8",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "ffbe9e5972bf3eee9037e2602c1330e0dc744646",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a": "Ybodychange",
    "f209e93566b159c22054dcb276e45f23a2b7b7d1": "Ymultichange(Yparameterchange,Ybodychange)",
    "96b12662ea76e3ded4ef13944fc8df206cfb4613": "Ymultichange(Yparameterchange,Ybodychange)",
    "66289a3bf403f307844ea0b6ceed35b603d12c0b": "Ymultichange(Yparameterchange,Ybodychange)",
    "662e17b46a0f41ade6a304e12925b70b5d09fc2f": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "a9331fe9b071fdcdae0c6c747d7b6b306142e671": "Ybodychange",
    "eca80dca3ee0888304519ec96e9e4113cc35b112": "Ybodychange",
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16": "Ybodychange",
    "5df82fa01d26c18685ad7617128dbc2913547cb3": "Ybodychange",
    "00067895a01c66d53715b50bbcb3605efd6425f2": "Ybodychange",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": "Ybodychange",
    "6c0ccb5989c2053f5a1ebab0dd9fdb7b4019fda8": "Ybodychange",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "ffbe9e5972bf3eee9037e2602c1330e0dc744646": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b3ae11d59790bb08b81848e9f944db7d3afbbd8a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12997. Move logging to slf4j in BlockPoolSliceStorage and Storage. Contributed by Ajay Kumar.\n",
      "commitDate": "01/02/18 10:45 AM",
      "commitName": "b3ae11d59790bb08b81848e9f944db7d3afbbd8a",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "8239e3afb31d3c4485817d4b8b8b195b554acbe7",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 47.7,
      "commitsBetweenForRepo": 240,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,13 @@\n   List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n       StorageLocation location, StartupOption startOpt,\n       List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n           throws IOException {\n-    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n+    LOG.info(\"Analyzing storage directories for bpid {}\", nsInfo\n+        .getBlockPoolID());\n     final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n         nsInfo, location, startOpt, callables, conf);\n     for (StorageDirectory sd : loaded) {\n       addStorageDir(sd);\n     }\n     return loaded;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n      StorageLocation location, StartupOption startOpt,\n      List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n          throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid {}\", nsInfo\n        .getBlockPoolID());\n    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n        nsInfo, location, startOpt, callables, conf);\n    for (StorageDirectory sd : loaded) {\n      addStorageDir(sd);\n    }\n    return loaded;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {}
    },
    "f209e93566b159c22054dcb276e45f23a2b7b7d1": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10638. Modifications to remove the assumption that StorageLocation is associated with java.io.File in Datanode. (Virajith Jalaparti via lei)\n",
      "commitDate": "26/10/16 10:32 AM",
      "commitName": "f209e93566b159c22054dcb276e45f23a2b7b7d1",
      "commitAuthor": "Lei Xu",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10638. Modifications to remove the assumption that StorageLocation is associated with java.io.File in Datanode. (Virajith Jalaparti via lei)\n",
          "commitDate": "26/10/16 10:32 AM",
          "commitName": "f209e93566b159c22054dcb276e45f23a2b7b7d1",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "10/10/16 3:30 PM",
          "commitNameOld": "96b12662ea76e3ded4ef13944fc8df206cfb4613",
          "commitAuthorOld": "Lei Xu",
          "daysBetweenCommits": 15.79,
          "commitsBetweenForRepo": 122,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,12 @@\n   List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n-      Collection\u003cFile\u003e dataDirs, StorageLocation location,\n-      StartupOption startOpt, List\u003cCallable\u003cStorageDirectory\u003e\u003e callables,\n-      Configuration conf) throws IOException {\n+      StorageLocation location, StartupOption startOpt,\n+      List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n+          throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n     final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n-        nsInfo, dataDirs, location, startOpt, callables, conf);\n+        nsInfo, location, startOpt, callables, conf);\n     for (StorageDirectory sd : loaded) {\n       addStorageDir(sd);\n     }\n     return loaded;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n      StorageLocation location, StartupOption startOpt,\n      List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n          throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n        nsInfo, location, startOpt, callables, conf);\n    for (StorageDirectory sd : loaded) {\n      addStorageDir(sd);\n    }\n    return loaded;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
          "extendedDetails": {
            "oldValue": "[nsInfo-NamespaceInfo, dataDirs-Collection\u003cFile\u003e, location-StorageLocation, startOpt-StartupOption, callables-List\u003cCallable\u003cStorageDirectory\u003e\u003e, conf-Configuration]",
            "newValue": "[nsInfo-NamespaceInfo, location-StorageLocation, startOpt-StartupOption, callables-List\u003cCallable\u003cStorageDirectory\u003e\u003e, conf-Configuration]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10638. Modifications to remove the assumption that StorageLocation is associated with java.io.File in Datanode. (Virajith Jalaparti via lei)\n",
          "commitDate": "26/10/16 10:32 AM",
          "commitName": "f209e93566b159c22054dcb276e45f23a2b7b7d1",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "10/10/16 3:30 PM",
          "commitNameOld": "96b12662ea76e3ded4ef13944fc8df206cfb4613",
          "commitAuthorOld": "Lei Xu",
          "daysBetweenCommits": 15.79,
          "commitsBetweenForRepo": 122,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,12 @@\n   List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n-      Collection\u003cFile\u003e dataDirs, StorageLocation location,\n-      StartupOption startOpt, List\u003cCallable\u003cStorageDirectory\u003e\u003e callables,\n-      Configuration conf) throws IOException {\n+      StorageLocation location, StartupOption startOpt,\n+      List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n+          throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n     final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n-        nsInfo, dataDirs, location, startOpt, callables, conf);\n+        nsInfo, location, startOpt, callables, conf);\n     for (StorageDirectory sd : loaded) {\n       addStorageDir(sd);\n     }\n     return loaded;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n      StorageLocation location, StartupOption startOpt,\n      List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n          throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n        nsInfo, location, startOpt, callables, conf);\n    for (StorageDirectory sd : loaded) {\n      addStorageDir(sd);\n    }\n    return loaded;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
          "extendedDetails": {}
        }
      ]
    },
    "96b12662ea76e3ded4ef13944fc8df206cfb4613": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10637. Modifications to remove the assumption that FsVolumes are backed by java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "10/10/16 3:30 PM",
      "commitName": "96b12662ea76e3ded4ef13944fc8df206cfb4613",
      "commitAuthor": "Lei Xu",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10637. Modifications to remove the assumption that FsVolumes are backed by java.io.File. (Virajith Jalaparti via lei)\n",
          "commitDate": "10/10/16 3:30 PM",
          "commitName": "96b12662ea76e3ded4ef13944fc8df206cfb4613",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "13/09/16 12:54 PM",
          "commitNameOld": "86c9862bec0248d671e657aa56094a2919b8ac14",
          "commitAuthorOld": "Lei Xu",
          "daysBetweenCommits": 27.11,
          "commitsBetweenForRepo": 180,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,12 @@\n   List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n-      Collection\u003cFile\u003e dataDirs, StartupOption startOpt,\n-      List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n-          throws IOException {\n+      Collection\u003cFile\u003e dataDirs, StorageLocation location,\n+      StartupOption startOpt, List\u003cCallable\u003cStorageDirectory\u003e\u003e callables,\n+      Configuration conf) throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n     final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n-        nsInfo, dataDirs, startOpt, callables, conf);\n+        nsInfo, dataDirs, location, startOpt, callables, conf);\n     for (StorageDirectory sd : loaded) {\n       addStorageDir(sd);\n     }\n     return loaded;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StorageLocation location,\n      StartupOption startOpt, List\u003cCallable\u003cStorageDirectory\u003e\u003e callables,\n      Configuration conf) throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n        nsInfo, dataDirs, location, startOpt, callables, conf);\n    for (StorageDirectory sd : loaded) {\n      addStorageDir(sd);\n    }\n    return loaded;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
          "extendedDetails": {
            "oldValue": "[nsInfo-NamespaceInfo, dataDirs-Collection\u003cFile\u003e, startOpt-StartupOption, callables-List\u003cCallable\u003cStorageDirectory\u003e\u003e, conf-Configuration]",
            "newValue": "[nsInfo-NamespaceInfo, dataDirs-Collection\u003cFile\u003e, location-StorageLocation, startOpt-StartupOption, callables-List\u003cCallable\u003cStorageDirectory\u003e\u003e, conf-Configuration]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10637. Modifications to remove the assumption that FsVolumes are backed by java.io.File. (Virajith Jalaparti via lei)\n",
          "commitDate": "10/10/16 3:30 PM",
          "commitName": "96b12662ea76e3ded4ef13944fc8df206cfb4613",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "13/09/16 12:54 PM",
          "commitNameOld": "86c9862bec0248d671e657aa56094a2919b8ac14",
          "commitAuthorOld": "Lei Xu",
          "daysBetweenCommits": 27.11,
          "commitsBetweenForRepo": 180,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,12 @@\n   List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n-      Collection\u003cFile\u003e dataDirs, StartupOption startOpt,\n-      List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n-          throws IOException {\n+      Collection\u003cFile\u003e dataDirs, StorageLocation location,\n+      StartupOption startOpt, List\u003cCallable\u003cStorageDirectory\u003e\u003e callables,\n+      Configuration conf) throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n     final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n-        nsInfo, dataDirs, startOpt, callables, conf);\n+        nsInfo, dataDirs, location, startOpt, callables, conf);\n     for (StorageDirectory sd : loaded) {\n       addStorageDir(sd);\n     }\n     return loaded;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StorageLocation location,\n      StartupOption startOpt, List\u003cCallable\u003cStorageDirectory\u003e\u003e callables,\n      Configuration conf) throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n        nsInfo, dataDirs, location, startOpt, callables, conf);\n    for (StorageDirectory sd : loaded) {\n      addStorageDir(sd);\n    }\n    return loaded;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
          "extendedDetails": {}
        }
      ]
    },
    "66289a3bf403f307844ea0b6ceed35b603d12c0b": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8578. On upgrade, Datanode should process all storage/data dirs in parallel.  Contributed by vinayakumarb and szetszwo\n",
      "commitDate": "22/02/16 3:01 PM",
      "commitName": "66289a3bf403f307844ea0b6ceed35b603d12c0b",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8578. On upgrade, Datanode should process all storage/data dirs in parallel.  Contributed by vinayakumarb and szetszwo\n",
          "commitDate": "22/02/16 3:01 PM",
          "commitName": "66289a3bf403f307844ea0b6ceed35b603d12c0b",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "27/01/16 6:58 PM",
          "commitNameOld": "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 25.84,
          "commitsBetweenForRepo": 173,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,11 +1,12 @@\n   List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n-      Collection\u003cFile\u003e dataDirs, StartupOption startOpt, Configuration conf)\n+      Collection\u003cFile\u003e dataDirs, StartupOption startOpt,\n+      List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n           throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n     final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n-        nsInfo, dataDirs, startOpt, conf);\n+        nsInfo, dataDirs, startOpt, callables, conf);\n     for (StorageDirectory sd : loaded) {\n       addStorageDir(sd);\n     }\n     return loaded;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt,\n      List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n          throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n        nsInfo, dataDirs, startOpt, callables, conf);\n    for (StorageDirectory sd : loaded) {\n      addStorageDir(sd);\n    }\n    return loaded;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
          "extendedDetails": {
            "oldValue": "[nsInfo-NamespaceInfo, dataDirs-Collection\u003cFile\u003e, startOpt-StartupOption, conf-Configuration]",
            "newValue": "[nsInfo-NamespaceInfo, dataDirs-Collection\u003cFile\u003e, startOpt-StartupOption, callables-List\u003cCallable\u003cStorageDirectory\u003e\u003e, conf-Configuration]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8578. On upgrade, Datanode should process all storage/data dirs in parallel.  Contributed by vinayakumarb and szetszwo\n",
          "commitDate": "22/02/16 3:01 PM",
          "commitName": "66289a3bf403f307844ea0b6ceed35b603d12c0b",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "27/01/16 6:58 PM",
          "commitNameOld": "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 25.84,
          "commitsBetweenForRepo": 173,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,11 +1,12 @@\n   List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n-      Collection\u003cFile\u003e dataDirs, StartupOption startOpt, Configuration conf)\n+      Collection\u003cFile\u003e dataDirs, StartupOption startOpt,\n+      List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n           throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n     final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n-        nsInfo, dataDirs, startOpt, conf);\n+        nsInfo, dataDirs, startOpt, callables, conf);\n     for (StorageDirectory sd : loaded) {\n       addStorageDir(sd);\n     }\n     return loaded;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt,\n      List\u003cCallable\u003cStorageDirectory\u003e\u003e callables, Configuration conf)\n          throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n        nsInfo, dataDirs, startOpt, callables, conf);\n    for (StorageDirectory sd : loaded) {\n      addStorageDir(sd);\n    }\n    return loaded;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
          "extendedDetails": {}
        }
      ]
    },
    "662e17b46a0f41ade6a304e12925b70b5d09fc2f": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-9654. Code refactoring for HDFS-8578.\n",
      "commitDate": "27/01/16 6:58 PM",
      "commitName": "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9654. Code refactoring for HDFS-8578.\n",
          "commitDate": "27/01/16 6:58 PM",
          "commitName": "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "13/10/15 8:04 AM",
          "commitNameOld": "5b43db47a313decccdcca8f45c5708aab46396df",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 106.5,
          "commitsBetweenForRepo": 736,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,11 @@\n-  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n-      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n+  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n+      Collection\u003cFile\u003e dataDirs, StartupOption startOpt, Configuration conf)\n+          throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n-    for (StorageDirectory sd : loadBpStorageDirectories(\n-        datanode, nsInfo, dataDirs, startOpt)) {\n+    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n+        nsInfo, dataDirs, startOpt, conf);\n+    for (StorageDirectory sd : loaded) {\n       addStorageDir(sd);\n     }\n+    return loaded;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt, Configuration conf)\n          throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n        nsInfo, dataDirs, startOpt, conf);\n    for (StorageDirectory sd : loaded) {\n      addStorageDir(sd);\n    }\n    return loaded;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
          "extendedDetails": {
            "oldValue": "[datanode-DataNode, nsInfo-NamespaceInfo, dataDirs-Collection\u003cFile\u003e, startOpt-StartupOption]",
            "newValue": "[nsInfo-NamespaceInfo, dataDirs-Collection\u003cFile\u003e, startOpt-StartupOption, conf-Configuration]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-9654. Code refactoring for HDFS-8578.\n",
          "commitDate": "27/01/16 6:58 PM",
          "commitName": "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "13/10/15 8:04 AM",
          "commitNameOld": "5b43db47a313decccdcca8f45c5708aab46396df",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 106.5,
          "commitsBetweenForRepo": 736,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,11 @@\n-  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n-      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n+  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n+      Collection\u003cFile\u003e dataDirs, StartupOption startOpt, Configuration conf)\n+          throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n-    for (StorageDirectory sd : loadBpStorageDirectories(\n-        datanode, nsInfo, dataDirs, startOpt)) {\n+    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n+        nsInfo, dataDirs, startOpt, conf);\n+    for (StorageDirectory sd : loaded) {\n       addStorageDir(sd);\n     }\n+    return loaded;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt, Configuration conf)\n          throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n        nsInfo, dataDirs, startOpt, conf);\n    for (StorageDirectory sd : loaded) {\n      addStorageDir(sd);\n    }\n    return loaded;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "List\u003cStorageDirectory\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9654. Code refactoring for HDFS-8578.\n",
          "commitDate": "27/01/16 6:58 PM",
          "commitName": "662e17b46a0f41ade6a304e12925b70b5d09fc2f",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "13/10/15 8:04 AM",
          "commitNameOld": "5b43db47a313decccdcca8f45c5708aab46396df",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 106.5,
          "commitsBetweenForRepo": 736,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,11 @@\n-  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n-      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n+  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n+      Collection\u003cFile\u003e dataDirs, StartupOption startOpt, Configuration conf)\n+          throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n-    for (StorageDirectory sd : loadBpStorageDirectories(\n-        datanode, nsInfo, dataDirs, startOpt)) {\n+    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n+        nsInfo, dataDirs, startOpt, conf);\n+    for (StorageDirectory sd : loaded) {\n       addStorageDir(sd);\n     }\n+    return loaded;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  List\u003cStorageDirectory\u003e recoverTransitionRead(NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt, Configuration conf)\n          throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    final List\u003cStorageDirectory\u003e loaded \u003d loadBpStorageDirectories(\n        nsInfo, dataDirs, startOpt, conf);\n    for (StorageDirectory sd : loaded) {\n      addStorageDir(sd);\n    }\n    return loaded;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
          "extendedDetails": {}
        }
      ]
    },
    "a9331fe9b071fdcdae0c6c747d7b6b306142e671": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7035. Make adding a new data directory to the DataNode an atomic operation and improve error handling (Lei Xu via Colin P. McCabe)\n",
      "commitDate": "30/10/14 5:31 PM",
      "commitName": "a9331fe9b071fdcdae0c6c747d7b6b306142e671",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "08/09/14 9:20 PM",
      "commitNameOld": "f949f6b54825dac61511a5761837e2fd14437239",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 51.84,
      "commitsBetweenForRepo": 533,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,8 @@\n   void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n       Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n-    Set\u003cString\u003e existingStorageDirs \u003d new HashSet\u003cString\u003e();\n-    for (int i \u003d 0; i \u003c getNumStorageDirs(); i++) {\n-      existingStorageDirs.add(getStorageDir(i).getRoot().getAbsolutePath());\n-    }\n-\n-    // 1. For each BP data directory analyze the state and\n-    // check whether all is consistent before transitioning.\n-    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n-        dataDirs.size());\n-    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n-      File dataDir \u003d it.next();\n-      if (existingStorageDirs.contains(dataDir.getAbsolutePath())) {\n-        LOG.info(\"Storage directory \" + dataDir + \" has already been used.\");\n-        it.remove();\n-        continue;\n-      }\n-      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, true);\n-      StorageState curState;\n-      try {\n-        curState \u003d sd.analyzeStorage(startOpt, this);\n-        // sd is locked but not opened\n-        switch (curState) {\n-        case NORMAL:\n-          break;\n-        case NON_EXISTENT:\n-          // ignore this storage\n-          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n-          it.remove();\n-          continue;\n-        case NOT_FORMATTED: // format\n-          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n-          LOG.info(\"Formatting ...\");\n-          format(sd, nsInfo);\n-          break;\n-        default: // recovery part is common\n-          sd.doRecover(curState);\n-        }\n-      } catch (IOException ioe) {\n-        sd.unlock();\n-        throw ioe;\n-      }\n-      // add to the storage list. This is inherited from parent class, Storage.\n+    for (StorageDirectory sd : loadBpStorageDirectories(\n+        datanode, nsInfo, dataDirs, startOpt)) {\n       addStorageDir(sd);\n-      dataDirStates.add(curState);\n     }\n-\n-    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n-      throw new IOException(\n-          \"All specified directories are not accessible or do not exist.\");\n-\n-    // 2. Do transitions\n-    // Each storage directory is treated individually.\n-    // During startup some of them can upgrade or roll back\n-    // while others could be up-to-date for the regular startup.\n-    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n-      doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n-      assert getCTime() \u003d\u003d nsInfo.getCTime() \n-          : \"Data-node and name-node CTimes must be the same.\";\n-    }\n-\n-    // 3. Update all storages. Some of them might have just been formatted.\n-    this.writeAll();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    for (StorageDirectory sd : loadBpStorageDirectories(\n        datanode, nsInfo, dataDirs, startOpt)) {\n      addStorageDir(sd);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {}
    },
    "eca80dca3ee0888304519ec96e9e4113cc35b112": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6728. Dynamically add new volumes to DataStorage, formatted if necessary. Contributed by Lei Xu.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616615 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/14 3:46 PM",
      "commitName": "eca80dca3ee0888304519ec96e9e4113cc35b112",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "01/08/14 1:41 PM",
      "commitNameOld": "1ba3f8971433cdbc3e43fd3605065d811dab5b16",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 6.09,
      "commitsBetweenForRepo": 40,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,66 @@\n   void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n       Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n+    Set\u003cString\u003e existingStorageDirs \u003d new HashSet\u003cString\u003e();\n+    for (int i \u003d 0; i \u003c getNumStorageDirs(); i++) {\n+      existingStorageDirs.add(getStorageDir(i).getRoot().getAbsolutePath());\n+    }\n+\n     // 1. For each BP data directory analyze the state and\n     // check whether all is consistent before transitioning.\n-    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n     ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n         dataDirs.size());\n     for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n       File dataDir \u003d it.next();\n+      if (existingStorageDirs.contains(dataDir.getAbsolutePath())) {\n+        LOG.info(\"Storage directory \" + dataDir + \" has already been used.\");\n+        it.remove();\n+        continue;\n+      }\n       StorageDirectory sd \u003d new StorageDirectory(dataDir, null, true);\n       StorageState curState;\n       try {\n         curState \u003d sd.analyzeStorage(startOpt, this);\n         // sd is locked but not opened\n         switch (curState) {\n         case NORMAL:\n           break;\n         case NON_EXISTENT:\n           // ignore this storage\n           LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n           it.remove();\n           continue;\n         case NOT_FORMATTED: // format\n           LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n           LOG.info(\"Formatting ...\");\n           format(sd, nsInfo);\n           break;\n         default: // recovery part is common\n           sd.doRecover(curState);\n         }\n       } catch (IOException ioe) {\n         sd.unlock();\n         throw ioe;\n       }\n       // add to the storage list. This is inherited from parent class, Storage.\n       addStorageDir(sd);\n       dataDirStates.add(curState);\n     }\n \n     if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n       throw new IOException(\n           \"All specified directories are not accessible or do not exist.\");\n \n     // 2. Do transitions\n     // Each storage directory is treated individually.\n     // During startup some of them can upgrade or roll back\n     // while others could be up-to-date for the regular startup.\n     for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n       doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n       assert getCTime() \u003d\u003d nsInfo.getCTime() \n           : \"Data-node and name-node CTimes must be the same.\";\n     }\n \n     // 3. Update all storages. Some of them might have just been formatted.\n     this.writeAll();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    Set\u003cString\u003e existingStorageDirs \u003d new HashSet\u003cString\u003e();\n    for (int i \u003d 0; i \u003c getNumStorageDirs(); i++) {\n      existingStorageDirs.add(getStorageDir(i).getRoot().getAbsolutePath());\n    }\n\n    // 1. For each BP data directory analyze the state and\n    // check whether all is consistent before transitioning.\n    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n        dataDirs.size());\n    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n      File dataDir \u003d it.next();\n      if (existingStorageDirs.contains(dataDir.getAbsolutePath())) {\n        LOG.info(\"Storage directory \" + dataDir + \" has already been used.\");\n        it.remove();\n        continue;\n      }\n      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, true);\n      StorageState curState;\n      try {\n        curState \u003d sd.analyzeStorage(startOpt, this);\n        // sd is locked but not opened\n        switch (curState) {\n        case NORMAL:\n          break;\n        case NON_EXISTENT:\n          // ignore this storage\n          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n          it.remove();\n          continue;\n        case NOT_FORMATTED: // format\n          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n          LOG.info(\"Formatting ...\");\n          format(sd, nsInfo);\n          break;\n        default: // recovery part is common\n          sd.doRecover(curState);\n        }\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      // add to the storage list. This is inherited from parent class, Storage.\n      addStorageDir(sd);\n      dataDirStates.add(curState);\n    }\n\n    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n\n    // 2. Do transitions\n    // Each storage directory is treated individually.\n    // During startup some of them can upgrade or roll back\n    // while others could be up-to-date for the regular startup.\n    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n      doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n      assert getCTime() \u003d\u003d nsInfo.getCTime() \n          : \"Data-node and name-node CTimes must be the same.\";\n    }\n\n    // 3. Update all storages. Some of them might have just been formatted.\n    this.writeAll();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {}
    },
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6482. Use block ID-based block layout on datanodes (James Thomas via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615223 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/14 1:41 PM",
      "commitName": "1ba3f8971433cdbc3e43fd3605065d811dab5b16",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "31/07/14 6:13 PM",
      "commitNameOld": "b5b862e3afd0797dc8f940204622e174c1382f5e",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.81,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n       Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n     LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n     // 1. For each BP data directory analyze the state and\n     // check whether all is consistent before transitioning.\n     this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n     ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n         dataDirs.size());\n     for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n       File dataDir \u003d it.next();\n       StorageDirectory sd \u003d new StorageDirectory(dataDir, null, true);\n       StorageState curState;\n       try {\n         curState \u003d sd.analyzeStorage(startOpt, this);\n         // sd is locked but not opened\n         switch (curState) {\n         case NORMAL:\n           break;\n         case NON_EXISTENT:\n           // ignore this storage\n           LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n           it.remove();\n           continue;\n         case NOT_FORMATTED: // format\n           LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n           LOG.info(\"Formatting ...\");\n           format(sd, nsInfo);\n           break;\n         default: // recovery part is common\n           sd.doRecover(curState);\n         }\n       } catch (IOException ioe) {\n         sd.unlock();\n         throw ioe;\n       }\n       // add to the storage list. This is inherited from parent class, Storage.\n       addStorageDir(sd);\n       dataDirStates.add(curState);\n     }\n \n     if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n       throw new IOException(\n           \"All specified directories are not accessible or do not exist.\");\n \n     // 2. Do transitions\n     // Each storage directory is treated individually.\n     // During startup some of them can upgrade or roll back\n     // while others could be up-to-date for the regular startup.\n     for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n-      doTransition(getStorageDir(idx), nsInfo, startOpt);\n+      doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n       assert getCTime() \u003d\u003d nsInfo.getCTime() \n           : \"Data-node and name-node CTimes must be the same.\";\n     }\n \n     // 3. Update all storages. Some of them might have just been formatted.\n     this.writeAll();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    // 1. For each BP data directory analyze the state and\n    // check whether all is consistent before transitioning.\n    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n        dataDirs.size());\n    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n      File dataDir \u003d it.next();\n      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, true);\n      StorageState curState;\n      try {\n        curState \u003d sd.analyzeStorage(startOpt, this);\n        // sd is locked but not opened\n        switch (curState) {\n        case NORMAL:\n          break;\n        case NON_EXISTENT:\n          // ignore this storage\n          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n          it.remove();\n          continue;\n        case NOT_FORMATTED: // format\n          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n          LOG.info(\"Formatting ...\");\n          format(sd, nsInfo);\n          break;\n        default: // recovery part is common\n          sd.doRecover(curState);\n        }\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      // add to the storage list. This is inherited from parent class, Storage.\n      addStorageDir(sd);\n      dataDirStates.add(curState);\n    }\n\n    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n\n    // 2. Do transitions\n    // Each storage directory is treated individually.\n    // During startup some of them can upgrade or roll back\n    // while others could be up-to-date for the regular startup.\n    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n      doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n      assert getCTime() \u003d\u003d nsInfo.getCTime() \n          : \"Data-node and name-node CTimes must be the same.\";\n    }\n\n    // 3. Update all storages. Some of them might have just been formatted.\n    this.writeAll();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {}
    },
    "5df82fa01d26c18685ad7617128dbc2913547cb3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5907. BlockPoolSliceStorage trash to handle block deletions during rolling upgrade. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1568346 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/02/14 8:37 AM",
      "commitName": "5df82fa01d26c18685ad7617128dbc2913547cb3",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "30/01/14 7:21 PM",
      "commitNameOld": "00067895a01c66d53715b50bbcb3605efd6425f2",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 14.55,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,57 @@\n   void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n       Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n+    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n     // 1. For each BP data directory analyze the state and\n     // check whether all is consistent before transitioning.\n     this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n     ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n         dataDirs.size());\n     for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n       File dataDir \u003d it.next();\n       StorageDirectory sd \u003d new StorageDirectory(dataDir, null, true);\n       StorageState curState;\n       try {\n         curState \u003d sd.analyzeStorage(startOpt, this);\n         // sd is locked but not opened\n         switch (curState) {\n         case NORMAL:\n           break;\n         case NON_EXISTENT:\n           // ignore this storage\n           LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n           it.remove();\n           continue;\n         case NOT_FORMATTED: // format\n           LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n           LOG.info(\"Formatting ...\");\n           format(sd, nsInfo);\n           break;\n         default: // recovery part is common\n           sd.doRecover(curState);\n         }\n       } catch (IOException ioe) {\n         sd.unlock();\n         throw ioe;\n       }\n       // add to the storage list. This is inherited from parent class, Storage.\n       addStorageDir(sd);\n       dataDirStates.add(curState);\n     }\n \n     if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n       throw new IOException(\n           \"All specified directories are not accessible or do not exist.\");\n \n     // 2. Do transitions\n     // Each storage directory is treated individually.\n     // During startup some of them can upgrade or roll back\n     // while others could be up-to-date for the regular startup.\n     for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n       doTransition(getStorageDir(idx), nsInfo, startOpt);\n       assert getCTime() \u003d\u003d nsInfo.getCTime() \n           : \"Data-node and name-node CTimes must be the same.\";\n     }\n \n     // 3. Update all storages. Some of them might have just been formatted.\n     this.writeAll();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    LOG.info(\"Analyzing storage directories for bpid \" + nsInfo.getBlockPoolID());\n    // 1. For each BP data directory analyze the state and\n    // check whether all is consistent before transitioning.\n    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n        dataDirs.size());\n    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n      File dataDir \u003d it.next();\n      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, true);\n      StorageState curState;\n      try {\n        curState \u003d sd.analyzeStorage(startOpt, this);\n        // sd is locked but not opened\n        switch (curState) {\n        case NORMAL:\n          break;\n        case NON_EXISTENT:\n          // ignore this storage\n          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n          it.remove();\n          continue;\n        case NOT_FORMATTED: // format\n          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n          LOG.info(\"Formatting ...\");\n          format(sd, nsInfo);\n          break;\n        default: // recovery part is common\n          sd.doRecover(curState);\n        }\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      // add to the storage list. This is inherited from parent class, Storage.\n      addStorageDir(sd);\n      dataDirStates.add(curState);\n    }\n\n    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n\n    // 2. Do transitions\n    // Each storage directory is treated individually.\n    // During startup some of them can upgrade or roll back\n    // while others could be up-to-date for the regular startup.\n    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n      doTransition(getStorageDir(idx), nsInfo, startOpt);\n      assert getCTime() \u003d\u003d nsInfo.getCTime() \n          : \"Data-node and name-node CTimes must be the same.\";\n    }\n\n    // 3. Update all storages. Some of them might have just been formatted.\n    this.writeAll();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {}
    },
    "00067895a01c66d53715b50bbcb3605efd6425f2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5754. Split LayoutVerion into NameNodeLayoutVersion and DataNodeLayoutVersion. Contributed by Brandon Li\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1563041 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/01/14 7:21 PM",
      "commitName": "00067895a01c66d53715b50bbcb3605efd6425f2",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "25/01/14 12:01 PM",
      "commitNameOld": "edb6dc5f303093c2604cd07b0c0dacf12dbce5de",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 5.31,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,56 @@\n   void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n       Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n-    assert HdfsConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n-        : \"Block-pool and name-node layout versions must be the same.\";\n-\n     // 1. For each BP data directory analyze the state and\n     // check whether all is consistent before transitioning.\n     this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n     ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n         dataDirs.size());\n     for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n       File dataDir \u003d it.next();\n       StorageDirectory sd \u003d new StorageDirectory(dataDir, null, true);\n       StorageState curState;\n       try {\n         curState \u003d sd.analyzeStorage(startOpt, this);\n         // sd is locked but not opened\n         switch (curState) {\n         case NORMAL:\n           break;\n         case NON_EXISTENT:\n           // ignore this storage\n           LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n           it.remove();\n           continue;\n         case NOT_FORMATTED: // format\n           LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n           LOG.info(\"Formatting ...\");\n           format(sd, nsInfo);\n           break;\n         default: // recovery part is common\n           sd.doRecover(curState);\n         }\n       } catch (IOException ioe) {\n         sd.unlock();\n         throw ioe;\n       }\n       // add to the storage list. This is inherited from parent class, Storage.\n       addStorageDir(sd);\n       dataDirStates.add(curState);\n     }\n \n     if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n       throw new IOException(\n           \"All specified directories are not accessible or do not exist.\");\n \n     // 2. Do transitions\n     // Each storage directory is treated individually.\n     // During startup some of them can upgrade or roll back\n     // while others could be up-to-date for the regular startup.\n     for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n       doTransition(getStorageDir(idx), nsInfo, startOpt);\n-      assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n-          : \"Data-node and name-node layout versions must be the same.\";\n       assert getCTime() \u003d\u003d nsInfo.getCTime() \n           : \"Data-node and name-node CTimes must be the same.\";\n     }\n \n     // 3. Update all storages. Some of them might have just been formatted.\n     this.writeAll();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    // 1. For each BP data directory analyze the state and\n    // check whether all is consistent before transitioning.\n    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n        dataDirs.size());\n    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n      File dataDir \u003d it.next();\n      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, true);\n      StorageState curState;\n      try {\n        curState \u003d sd.analyzeStorage(startOpt, this);\n        // sd is locked but not opened\n        switch (curState) {\n        case NORMAL:\n          break;\n        case NON_EXISTENT:\n          // ignore this storage\n          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n          it.remove();\n          continue;\n        case NOT_FORMATTED: // format\n          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n          LOG.info(\"Formatting ...\");\n          format(sd, nsInfo);\n          break;\n        default: // recovery part is common\n          sd.doRecover(curState);\n        }\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      // add to the storage list. This is inherited from parent class, Storage.\n      addStorageDir(sd);\n      dataDirStates.add(curState);\n    }\n\n    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n\n    // 2. Do transitions\n    // Each storage directory is treated individually.\n    // During startup some of them can upgrade or roll back\n    // while others could be up-to-date for the regular startup.\n    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n      doTransition(getStorageDir(idx), nsInfo, startOpt);\n      assert getCTime() \u003d\u003d nsInfo.getCTime() \n          : \"Data-node and name-node CTimes must be the same.\";\n    }\n\n    // 3. Update all storages. Some of them might have just been formatted.\n    this.writeAll();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {}
    },
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5138. Support HDFS upgrade in HA. Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1561381 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/14 12:01 PM",
      "commitName": "edb6dc5f303093c2604cd07b0c0dacf12dbce5de",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "07/01/13 1:47 PM",
      "commitNameOld": "d3949058b84c393413ffea11de5c81ab8ad2ae3c",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 382.93,
      "commitsBetweenForRepo": 2314,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n       Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n     assert HdfsConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n         : \"Block-pool and name-node layout versions must be the same.\";\n \n     // 1. For each BP data directory analyze the state and\n     // check whether all is consistent before transitioning.\n     this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n     ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n         dataDirs.size());\n     for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n       File dataDir \u003d it.next();\n-      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, false);\n+      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, true);\n       StorageState curState;\n       try {\n         curState \u003d sd.analyzeStorage(startOpt, this);\n         // sd is locked but not opened\n         switch (curState) {\n         case NORMAL:\n           break;\n         case NON_EXISTENT:\n           // ignore this storage\n           LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n           it.remove();\n           continue;\n         case NOT_FORMATTED: // format\n           LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n           LOG.info(\"Formatting ...\");\n           format(sd, nsInfo);\n           break;\n         default: // recovery part is common\n           sd.doRecover(curState);\n         }\n       } catch (IOException ioe) {\n         sd.unlock();\n         throw ioe;\n       }\n       // add to the storage list. This is inherited from parent class, Storage.\n       addStorageDir(sd);\n       dataDirStates.add(curState);\n     }\n \n     if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n       throw new IOException(\n           \"All specified directories are not accessible or do not exist.\");\n \n     // 2. Do transitions\n     // Each storage directory is treated individually.\n     // During startup some of them can upgrade or roll back\n     // while others could be up-to-date for the regular startup.\n     for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n       doTransition(getStorageDir(idx), nsInfo, startOpt);\n       assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n           : \"Data-node and name-node layout versions must be the same.\";\n       assert getCTime() \u003d\u003d nsInfo.getCTime() \n           : \"Data-node and name-node CTimes must be the same.\";\n     }\n \n     // 3. Update all storages. Some of them might have just been formatted.\n     this.writeAll();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    assert HdfsConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n        : \"Block-pool and name-node layout versions must be the same.\";\n\n    // 1. For each BP data directory analyze the state and\n    // check whether all is consistent before transitioning.\n    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n        dataDirs.size());\n    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n      File dataDir \u003d it.next();\n      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, true);\n      StorageState curState;\n      try {\n        curState \u003d sd.analyzeStorage(startOpt, this);\n        // sd is locked but not opened\n        switch (curState) {\n        case NORMAL:\n          break;\n        case NON_EXISTENT:\n          // ignore this storage\n          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n          it.remove();\n          continue;\n        case NOT_FORMATTED: // format\n          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n          LOG.info(\"Formatting ...\");\n          format(sd, nsInfo);\n          break;\n        default: // recovery part is common\n          sd.doRecover(curState);\n        }\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      // add to the storage list. This is inherited from parent class, Storage.\n      addStorageDir(sd);\n      dataDirStates.add(curState);\n    }\n\n    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n\n    // 2. Do transitions\n    // Each storage directory is treated individually.\n    // During startup some of them can upgrade or roll back\n    // while others could be up-to-date for the regular startup.\n    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n      doTransition(getStorageDir(idx), nsInfo, startOpt);\n      assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n          : \"Data-node and name-node layout versions must be the same.\";\n      assert getCTime() \u003d\u003d nsInfo.getCTime() \n          : \"Data-node and name-node CTimes must be the same.\";\n    }\n\n    // 3. Update all storages. Some of them might have just been formatted.\n    this.writeAll();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {}
    },
    "6c0ccb5989c2053f5a1ebab0dd9fdb7b4019fda8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2686. Remove DistributedUpgrade related code. Contributed by Suresh Srinivas\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1375800 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/08/12 2:18 PM",
      "commitName": "6c0ccb5989c2053f5a1ebab0dd9fdb7b4019fda8",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "15/07/12 7:58 PM",
      "commitNameOld": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 36.76,
      "commitsBetweenForRepo": 189,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n       Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n     assert HdfsConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n         : \"Block-pool and name-node layout versions must be the same.\";\n \n     // 1. For each BP data directory analyze the state and\n     // check whether all is consistent before transitioning.\n     this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n     ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n         dataDirs.size());\n     for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n       File dataDir \u003d it.next();\n       StorageDirectory sd \u003d new StorageDirectory(dataDir, null, false);\n       StorageState curState;\n       try {\n         curState \u003d sd.analyzeStorage(startOpt, this);\n         // sd is locked but not opened\n         switch (curState) {\n         case NORMAL:\n           break;\n         case NON_EXISTENT:\n           // ignore this storage\n           LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n           it.remove();\n           continue;\n         case NOT_FORMATTED: // format\n           LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n           LOG.info(\"Formatting ...\");\n           format(sd, nsInfo);\n           break;\n         default: // recovery part is common\n           sd.doRecover(curState);\n         }\n       } catch (IOException ioe) {\n         sd.unlock();\n         throw ioe;\n       }\n       // add to the storage list. This is inherited from parent class, Storage.\n       addStorageDir(sd);\n       dataDirStates.add(curState);\n     }\n \n     if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n       throw new IOException(\n           \"All specified directories are not accessible or do not exist.\");\n \n     // 2. Do transitions\n     // Each storage directory is treated individually.\n     // During startup some of them can upgrade or roll back\n     // while others could be up-to-date for the regular startup.\n     for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n-      doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n+      doTransition(getStorageDir(idx), nsInfo, startOpt);\n       assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n           : \"Data-node and name-node layout versions must be the same.\";\n       assert getCTime() \u003d\u003d nsInfo.getCTime() \n           : \"Data-node and name-node CTimes must be the same.\";\n     }\n \n     // 3. Update all storages. Some of them might have just been formatted.\n     this.writeAll();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    assert HdfsConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n        : \"Block-pool and name-node layout versions must be the same.\";\n\n    // 1. For each BP data directory analyze the state and\n    // check whether all is consistent before transitioning.\n    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n        dataDirs.size());\n    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n      File dataDir \u003d it.next();\n      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, false);\n      StorageState curState;\n      try {\n        curState \u003d sd.analyzeStorage(startOpt, this);\n        // sd is locked but not opened\n        switch (curState) {\n        case NORMAL:\n          break;\n        case NON_EXISTENT:\n          // ignore this storage\n          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n          it.remove();\n          continue;\n        case NOT_FORMATTED: // format\n          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n          LOG.info(\"Formatting ...\");\n          format(sd, nsInfo);\n          break;\n        default: // recovery part is common\n          sd.doRecover(curState);\n        }\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      // add to the storage list. This is inherited from parent class, Storage.\n      addStorageDir(sd);\n      dataDirStates.add(curState);\n    }\n\n    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n\n    // 2. Do transitions\n    // Each storage directory is treated individually.\n    // During startup some of them can upgrade or roll back\n    // while others could be up-to-date for the regular startup.\n    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n      doTransition(getStorageDir(idx), nsInfo, startOpt);\n      assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n          : \"Data-node and name-node layout versions must be the same.\";\n      assert getCTime() \u003d\u003d nsInfo.getCTime() \n          : \"Data-node and name-node CTimes must be the same.\";\n    }\n\n    // 3. Update all storages. Some of them might have just been formatted.\n    this.writeAll();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {}
    },
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1620. Rename HdfsConstants -\u003e HdfsServerConstants, FSConstants -\u003e HdfsConstants. (Harsh J Chouraria via atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165096 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/11 12:30 PM",
      "commitName": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 10.8,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n       Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n-    assert FSConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n+    assert HdfsConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n         : \"Block-pool and name-node layout versions must be the same.\";\n \n     // 1. For each BP data directory analyze the state and\n     // check whether all is consistent before transitioning.\n     this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n     ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n         dataDirs.size());\n     for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n       File dataDir \u003d it.next();\n       StorageDirectory sd \u003d new StorageDirectory(dataDir, null, false);\n       StorageState curState;\n       try {\n         curState \u003d sd.analyzeStorage(startOpt, this);\n         // sd is locked but not opened\n         switch (curState) {\n         case NORMAL:\n           break;\n         case NON_EXISTENT:\n           // ignore this storage\n           LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n           it.remove();\n           continue;\n         case NOT_FORMATTED: // format\n           LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n           LOG.info(\"Formatting ...\");\n           format(sd, nsInfo);\n           break;\n         default: // recovery part is common\n           sd.doRecover(curState);\n         }\n       } catch (IOException ioe) {\n         sd.unlock();\n         throw ioe;\n       }\n       // add to the storage list. This is inherited from parent class, Storage.\n       addStorageDir(sd);\n       dataDirStates.add(curState);\n     }\n \n     if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n       throw new IOException(\n           \"All specified directories are not accessible or do not exist.\");\n \n     // 2. Do transitions\n     // Each storage directory is treated individually.\n     // During startup some of them can upgrade or roll back\n     // while others could be up-to-date for the regular startup.\n     for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n       doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n       assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n           : \"Data-node and name-node layout versions must be the same.\";\n       assert getCTime() \u003d\u003d nsInfo.getCTime() \n           : \"Data-node and name-node CTimes must be the same.\";\n     }\n \n     // 3. Update all storages. Some of them might have just been formatted.\n     this.writeAll();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    assert HdfsConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n        : \"Block-pool and name-node layout versions must be the same.\";\n\n    // 1. For each BP data directory analyze the state and\n    // check whether all is consistent before transitioning.\n    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n        dataDirs.size());\n    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n      File dataDir \u003d it.next();\n      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, false);\n      StorageState curState;\n      try {\n        curState \u003d sd.analyzeStorage(startOpt, this);\n        // sd is locked but not opened\n        switch (curState) {\n        case NORMAL:\n          break;\n        case NON_EXISTENT:\n          // ignore this storage\n          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n          it.remove();\n          continue;\n        case NOT_FORMATTED: // format\n          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n          LOG.info(\"Formatting ...\");\n          format(sd, nsInfo);\n          break;\n        default: // recovery part is common\n          sd.doRecover(curState);\n        }\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      // add to the storage list. This is inherited from parent class, Storage.\n      addStorageDir(sd);\n      dataDirStates.add(curState);\n    }\n\n    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n\n    // 2. Do transitions\n    // Each storage directory is treated individually.\n    // During startup some of them can upgrade or roll back\n    // while others could be up-to-date for the regular startup.\n    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n      doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n      assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n          : \"Data-node and name-node layout versions must be the same.\";\n      assert getCTime() \u003d\u003d nsInfo.getCTime() \n          : \"Data-node and name-node CTimes must be the same.\";\n    }\n\n    // 3. Update all storages. Some of them might have just been formatted.\n    this.writeAll();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    assert FSConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n        : \"Block-pool and name-node layout versions must be the same.\";\n\n    // 1. For each BP data directory analyze the state and\n    // check whether all is consistent before transitioning.\n    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n        dataDirs.size());\n    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n      File dataDir \u003d it.next();\n      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, false);\n      StorageState curState;\n      try {\n        curState \u003d sd.analyzeStorage(startOpt, this);\n        // sd is locked but not opened\n        switch (curState) {\n        case NORMAL:\n          break;\n        case NON_EXISTENT:\n          // ignore this storage\n          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n          it.remove();\n          continue;\n        case NOT_FORMATTED: // format\n          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n          LOG.info(\"Formatting ...\");\n          format(sd, nsInfo);\n          break;\n        default: // recovery part is common\n          sd.doRecover(curState);\n        }\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      // add to the storage list. This is inherited from parent class, Storage.\n      addStorageDir(sd);\n      dataDirStates.add(curState);\n    }\n\n    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n\n    // 2. Do transitions\n    // Each storage directory is treated individually.\n    // During startup some of them can upgrade or roll back\n    // while others could be up-to-date for the regular startup.\n    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n      doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n      assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n          : \"Data-node and name-node layout versions must be the same.\";\n      assert getCTime() \u003d\u003d nsInfo.getCTime() \n          : \"Data-node and name-node CTimes must be the same.\";\n    }\n\n    // 3. Update all storages. Some of them might have just been formatted.\n    this.writeAll();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    assert FSConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n        : \"Block-pool and name-node layout versions must be the same.\";\n\n    // 1. For each BP data directory analyze the state and\n    // check whether all is consistent before transitioning.\n    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n        dataDirs.size());\n    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n      File dataDir \u003d it.next();\n      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, false);\n      StorageState curState;\n      try {\n        curState \u003d sd.analyzeStorage(startOpt, this);\n        // sd is locked but not opened\n        switch (curState) {\n        case NORMAL:\n          break;\n        case NON_EXISTENT:\n          // ignore this storage\n          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n          it.remove();\n          continue;\n        case NOT_FORMATTED: // format\n          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n          LOG.info(\"Formatting ...\");\n          format(sd, nsInfo);\n          break;\n        default: // recovery part is common\n          sd.doRecover(curState);\n        }\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      // add to the storage list. This is inherited from parent class, Storage.\n      addStorageDir(sd);\n      dataDirStates.add(curState);\n    }\n\n    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n\n    // 2. Do transitions\n    // Each storage directory is treated individually.\n    // During startup some of them can upgrade or roll back\n    // while others could be up-to-date for the regular startup.\n    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n      doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n      assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n          : \"Data-node and name-node layout versions must be the same.\";\n      assert getCTime() \u003d\u003d nsInfo.getCTime() \n          : \"Data-node and name-node CTimes must be the same.\";\n    }\n\n    // 3. Update all storages. Some of them might have just been formatted.\n    this.writeAll();\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java"
      }
    },
    "ffbe9e5972bf3eee9037e2602c1330e0dc744646": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2195. Refactor StorageDirectory to not be an non-static inner class. Contributed by Todd Lipcon\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151707 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/07/11 8:19 PM",
      "commitName": "ffbe9e5972bf3eee9037e2602c1330e0dc744646",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 45.22,
      "commitsBetweenForRepo": 158,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n       Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n     assert FSConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n         : \"Block-pool and name-node layout versions must be the same.\";\n \n     // 1. For each BP data directory analyze the state and\n     // check whether all is consistent before transitioning.\n     this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n     ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n         dataDirs.size());\n     for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n       File dataDir \u003d it.next();\n       StorageDirectory sd \u003d new StorageDirectory(dataDir, null, false);\n       StorageState curState;\n       try {\n-        curState \u003d sd.analyzeStorage(startOpt);\n+        curState \u003d sd.analyzeStorage(startOpt, this);\n         // sd is locked but not opened\n         switch (curState) {\n         case NORMAL:\n           break;\n         case NON_EXISTENT:\n           // ignore this storage\n           LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n           it.remove();\n           continue;\n         case NOT_FORMATTED: // format\n           LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n           LOG.info(\"Formatting ...\");\n           format(sd, nsInfo);\n           break;\n         default: // recovery part is common\n           sd.doRecover(curState);\n         }\n       } catch (IOException ioe) {\n         sd.unlock();\n         throw ioe;\n       }\n       // add to the storage list. This is inherited from parent class, Storage.\n       addStorageDir(sd);\n       dataDirStates.add(curState);\n     }\n \n     if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n       throw new IOException(\n           \"All specified directories are not accessible or do not exist.\");\n \n     // 2. Do transitions\n     // Each storage directory is treated individually.\n     // During startup some of them can upgrade or roll back\n     // while others could be up-to-date for the regular startup.\n     for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n       doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n       assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n           : \"Data-node and name-node layout versions must be the same.\";\n       assert getCTime() \u003d\u003d nsInfo.getCTime() \n           : \"Data-node and name-node CTimes must be the same.\";\n     }\n \n     // 3. Update all storages. Some of them might have just been formatted.\n     this.writeAll();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    assert FSConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n        : \"Block-pool and name-node layout versions must be the same.\";\n\n    // 1. For each BP data directory analyze the state and\n    // check whether all is consistent before transitioning.\n    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n        dataDirs.size());\n    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n      File dataDir \u003d it.next();\n      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, false);\n      StorageState curState;\n      try {\n        curState \u003d sd.analyzeStorage(startOpt, this);\n        // sd is locked but not opened\n        switch (curState) {\n        case NORMAL:\n          break;\n        case NON_EXISTENT:\n          // ignore this storage\n          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n          it.remove();\n          continue;\n        case NOT_FORMATTED: // format\n          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n          LOG.info(\"Formatting ...\");\n          format(sd, nsInfo);\n          break;\n        default: // recovery part is common\n          sd.doRecover(curState);\n        }\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      // add to the storage list. This is inherited from parent class, Storage.\n      addStorageDir(sd);\n      dataDirStates.add(curState);\n    }\n\n    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n\n    // 2. Do transitions\n    // Each storage directory is treated individually.\n    // During startup some of them can upgrade or roll back\n    // while others could be up-to-date for the regular startup.\n    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n      doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n      assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n          : \"Data-node and name-node layout versions must be the same.\";\n      assert getCTime() \u003d\u003d nsInfo.getCTime() \n          : \"Data-node and name-node CTimes must be the same.\";\n    }\n\n    // 3. Update all storages. Some of them might have just been formatted.\n    this.writeAll();\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,61 @@\n+  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n+      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n+    assert FSConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n+        : \"Block-pool and name-node layout versions must be the same.\";\n+\n+    // 1. For each BP data directory analyze the state and\n+    // check whether all is consistent before transitioning.\n+    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n+    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n+        dataDirs.size());\n+    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n+      File dataDir \u003d it.next();\n+      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, false);\n+      StorageState curState;\n+      try {\n+        curState \u003d sd.analyzeStorage(startOpt);\n+        // sd is locked but not opened\n+        switch (curState) {\n+        case NORMAL:\n+          break;\n+        case NON_EXISTENT:\n+          // ignore this storage\n+          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n+          it.remove();\n+          continue;\n+        case NOT_FORMATTED: // format\n+          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n+          LOG.info(\"Formatting ...\");\n+          format(sd, nsInfo);\n+          break;\n+        default: // recovery part is common\n+          sd.doRecover(curState);\n+        }\n+      } catch (IOException ioe) {\n+        sd.unlock();\n+        throw ioe;\n+      }\n+      // add to the storage list. This is inherited from parent class, Storage.\n+      addStorageDir(sd);\n+      dataDirStates.add(curState);\n+    }\n+\n+    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n+      throw new IOException(\n+          \"All specified directories are not accessible or do not exist.\");\n+\n+    // 2. Do transitions\n+    // Each storage directory is treated individually.\n+    // During startup some of them can upgrade or roll back\n+    // while others could be up-to-date for the regular startup.\n+    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n+      doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n+      assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n+          : \"Data-node and name-node layout versions must be the same.\";\n+      assert getCTime() \u003d\u003d nsInfo.getCTime() \n+          : \"Data-node and name-node CTimes must be the same.\";\n+    }\n+\n+    // 3. Update all storages. Some of them might have just been formatted.\n+    this.writeAll();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo,\n      Collection\u003cFile\u003e dataDirs, StartupOption startOpt) throws IOException {\n    assert FSConstants.LAYOUT_VERSION \u003d\u003d nsInfo.getLayoutVersion() \n        : \"Block-pool and name-node layout versions must be the same.\";\n\n    // 1. For each BP data directory analyze the state and\n    // check whether all is consistent before transitioning.\n    this.storageDirs \u003d new ArrayList\u003cStorageDirectory\u003e(dataDirs.size());\n    ArrayList\u003cStorageState\u003e dataDirStates \u003d new ArrayList\u003cStorageState\u003e(\n        dataDirs.size());\n    for (Iterator\u003cFile\u003e it \u003d dataDirs.iterator(); it.hasNext();) {\n      File dataDir \u003d it.next();\n      StorageDirectory sd \u003d new StorageDirectory(dataDir, null, false);\n      StorageState curState;\n      try {\n        curState \u003d sd.analyzeStorage(startOpt);\n        // sd is locked but not opened\n        switch (curState) {\n        case NORMAL:\n          break;\n        case NON_EXISTENT:\n          // ignore this storage\n          LOG.info(\"Storage directory \" + dataDir + \" does not exist.\");\n          it.remove();\n          continue;\n        case NOT_FORMATTED: // format\n          LOG.info(\"Storage directory \" + dataDir + \" is not formatted.\");\n          LOG.info(\"Formatting ...\");\n          format(sd, nsInfo);\n          break;\n        default: // recovery part is common\n          sd.doRecover(curState);\n        }\n      } catch (IOException ioe) {\n        sd.unlock();\n        throw ioe;\n      }\n      // add to the storage list. This is inherited from parent class, Storage.\n      addStorageDir(sd);\n      dataDirStates.add(curState);\n    }\n\n    if (dataDirs.size() \u003d\u003d 0) // none of the data dirs exist\n      throw new IOException(\n          \"All specified directories are not accessible or do not exist.\");\n\n    // 2. Do transitions\n    // Each storage directory is treated individually.\n    // During startup some of them can upgrade or roll back\n    // while others could be up-to-date for the regular startup.\n    for (int idx \u003d 0; idx \u003c getNumStorageDirs(); idx++) {\n      doTransition(datanode, getStorageDir(idx), nsInfo, startOpt);\n      assert getLayoutVersion() \u003d\u003d nsInfo.getLayoutVersion() \n          : \"Data-node and name-node layout versions must be the same.\";\n      assert getCTime() \u003d\u003d nsInfo.getCTime() \n          : \"Data-node and name-node CTimes must be the same.\";\n    }\n\n    // 3. Update all storages. Some of them might have just been formatted.\n    this.writeAll();\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java"
    }
  }
}