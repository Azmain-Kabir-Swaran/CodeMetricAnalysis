{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "JsonUtil.java",
  "functionName": "toJsonMap",
  "functionId": "toJsonMap___datanodeinfo-DatanodeInfo(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
  "functionStartLine": 190,
  "functionEndLine": 226,
  "numCommitsSeen": 82,
  "timeTaken": 4041,
  "changeHistory": [
    "b5adc5c3011f111f86d232cb33ec522547f68a95",
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "b4d6c5823b04b2a8834e06e78cd109a359496eed",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
    "8e0804666189ce9a66b7b41b744776bad29770dd",
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8",
    "4551da302d94cffea0313eac79479ab6f9b7cb34",
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
    "be7dd8333a7e56e732171db0781786987de03195",
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
    "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de"
  ],
  "changeHistoryShort": {
    "b5adc5c3011f111f86d232cb33ec522547f68a95": "Ybodychange",
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501": "Ybodychange",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "b4d6c5823b04b2a8834e06e78cd109a359496eed": "Ymultichange(Ymodifierchange,Ybodychange)",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": "Ybodychange",
    "8e0804666189ce9a66b7b41b744776bad29770dd": "Ybodychange",
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8": "Ybodychange",
    "4551da302d94cffea0313eac79479ab6f9b7cb34": "Ybodychange",
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57": "Ymultichange(Yrename,Yreturntypechange,Ymodifierchange,Ybodychange)",
    "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b5adc5c3011f111f86d232cb33ec522547f68a95": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10838. Last full block report received time for each DN should be easily discoverable. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "06/03/17 4:39 PM",
      "commitName": "b5adc5c3011f111f86d232cb33ec522547f68a95",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "03/01/17 9:58 AM",
      "commitNameOld": "7fcc73fc0d248aae1edbd4e1514c5818f6198928",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 62.28,
      "commitsBetweenForRepo": 330,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,37 @@\n   static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n     // TODO: Fix storageID\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n     // \u0027name\u0027 is equivalent to ipAddr:xferPort. Older clients (1.x, 0.23.x) \n     // expects this instead of the two fields.\n     m.put(\"name\", datanodeinfo.getXferAddr());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n     m.put(\"xferPort\", datanodeinfo.getXferPort());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n     m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n     m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"lastUpdateMonotonic\", datanodeinfo.getLastUpdateMonotonic());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n     if (datanodeinfo.getUpgradeDomain() !\u003d null) {\n       m.put(\"upgradeDomain\", datanodeinfo.getUpgradeDomain());\n     }\n+    m.put(\"lastBlockReportTime\", datanodeinfo.getLastBlockReportTime());\n+    m.put(\"lastBlockReportMonotonic\",\n+        datanodeinfo.getLastBlockReportMonotonic());\n     return m;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    // TODO: Fix storageID\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n    // \u0027name\u0027 is equivalent to ipAddr:xferPort. Older clients (1.x, 0.23.x) \n    // expects this instead of the two fields.\n    m.put(\"name\", datanodeinfo.getXferAddr());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n    m.put(\"xferPort\", datanodeinfo.getXferPort());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n    m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n    m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"lastUpdateMonotonic\", datanodeinfo.getLastUpdateMonotonic());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    if (datanodeinfo.getUpgradeDomain() !\u003d null) {\n      m.put(\"upgradeDomain\", datanodeinfo.getUpgradeDomain());\n    }\n    m.put(\"lastBlockReportTime\", datanodeinfo.getLastBlockReportTime());\n    m.put(\"lastBlockReportMonotonic\",\n        datanodeinfo.getLastBlockReportMonotonic());\n    return m;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9004. Add upgrade domain to DatanodeInfo. Contributed by Ming Ma (via Lei (Eddy) Xu).\n\nChange-Id: I887c66578eebd61acc34b94f18da6e6851c609f4\n",
      "commitDate": "19/09/15 6:08 PM",
      "commitName": "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "25/08/15 1:16 AM",
      "commitNameOld": "eee0d4563c62647cfaaed6605ee713aaf69add78",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 25.7,
      "commitsBetweenForRepo": 160,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,34 @@\n   static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n     // TODO: Fix storageID\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n     // \u0027name\u0027 is equivalent to ipAddr:xferPort. Older clients (1.x, 0.23.x) \n     // expects this instead of the two fields.\n     m.put(\"name\", datanodeinfo.getXferAddr());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n     m.put(\"xferPort\", datanodeinfo.getXferPort());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n     m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n     m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"lastUpdateMonotonic\", datanodeinfo.getLastUpdateMonotonic());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n+    if (datanodeinfo.getUpgradeDomain() !\u003d null) {\n+      m.put(\"upgradeDomain\", datanodeinfo.getUpgradeDomain());\n+    }\n     return m;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    // TODO: Fix storageID\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n    // \u0027name\u0027 is equivalent to ipAddr:xferPort. Older clients (1.x, 0.23.x) \n    // expects this instead of the two fields.\n    m.put(\"name\", datanodeinfo.getXferAddr());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n    m.put(\"xferPort\", datanodeinfo.getXferPort());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n    m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n    m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"lastUpdateMonotonic\", datanodeinfo.getLastUpdateMonotonic());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    if (datanodeinfo.getUpgradeDomain() !\u003d null) {\n      m.put(\"upgradeDomain\", datanodeinfo.getUpgradeDomain());\n    }\n    return m;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "03/03/15 5:54 PM",
      "commitNameOld": "e2262d3d18c6d5c2aa20f96920104dc07271b869",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 16.71,
      "commitsBetweenForRepo": 150,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,31 @@\n   static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n     // TODO: Fix storageID\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n     // \u0027name\u0027 is equivalent to ipAddr:xferPort. Older clients (1.x, 0.23.x) \n     // expects this instead of the two fields.\n     m.put(\"name\", datanodeinfo.getXferAddr());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n     m.put(\"xferPort\", datanodeinfo.getXferPort());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n     m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n     m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n+    m.put(\"lastUpdateMonotonic\", datanodeinfo.getLastUpdateMonotonic());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n     return m;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    // TODO: Fix storageID\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n    // \u0027name\u0027 is equivalent to ipAddr:xferPort. Older clients (1.x, 0.23.x) \n    // expects this instead of the two fields.\n    m.put(\"name\", datanodeinfo.getXferAddr());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n    m.put(\"xferPort\", datanodeinfo.getXferPort());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n    m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n    m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"lastUpdateMonotonic\", datanodeinfo.getLastUpdateMonotonic());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "b4d6c5823b04b2a8834e06e78cd109a359496eed": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-5449. WebHdfs compatibility broken between 2.2 and 1.x / 23.x. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556927 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/14 11:24 AM",
      "commitName": "b4d6c5823b04b2a8834e06e78cd109a359496eed",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5449. WebHdfs compatibility broken between 2.2 and 1.x / 23.x. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556927 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/01/14 11:24 AM",
          "commitName": "b4d6c5823b04b2a8834e06e78cd109a359496eed",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "12/11/13 6:00 PM",
          "commitNameOld": "46cbce9af1272ce0eb6e300f96a1a8d4b08e23e3",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 57.72,
          "commitsBetweenForRepo": 308,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,30 @@\n-  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n+  static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n     // TODO: Fix storageID\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n+    // \u0027name\u0027 is equivalent to ipAddr:xferPort. Older clients (1.x, 0.23.x) \n+    // expects this instead of the two fields.\n+    m.put(\"name\", datanodeinfo.getXferAddr());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n     m.put(\"xferPort\", datanodeinfo.getXferPort());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n     m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n     m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n     return m;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    // TODO: Fix storageID\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n    // \u0027name\u0027 is equivalent to ipAddr:xferPort. Older clients (1.x, 0.23.x) \n    // expects this instead of the two fields.\n    m.put(\"name\", datanodeinfo.getXferAddr());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n    m.put(\"xferPort\", datanodeinfo.getXferPort());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n    m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n    m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {
            "oldValue": "[private, static]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5449. WebHdfs compatibility broken between 2.2 and 1.x / 23.x. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556927 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/01/14 11:24 AM",
          "commitName": "b4d6c5823b04b2a8834e06e78cd109a359496eed",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "12/11/13 6:00 PM",
          "commitNameOld": "46cbce9af1272ce0eb6e300f96a1a8d4b08e23e3",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 57.72,
          "commitsBetweenForRepo": 308,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,30 @@\n-  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n+  static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n     // TODO: Fix storageID\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n+    // \u0027name\u0027 is equivalent to ipAddr:xferPort. Older clients (1.x, 0.23.x) \n+    // expects this instead of the two fields.\n+    m.put(\"name\", datanodeinfo.getXferAddr());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n     m.put(\"xferPort\", datanodeinfo.getXferPort());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n     m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n     m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n     return m;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    // TODO: Fix storageID\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n    // \u0027name\u0027 is equivalent to ipAddr:xferPort. Older clients (1.x, 0.23.x) \n    // expects this instead of the two fields.\n    m.put(\"name\", datanodeinfo.getXferAddr());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n    m.put(\"xferPort\", datanodeinfo.getXferPort());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n    m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n    m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {}
        }
      ]
    },
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": {
      "type": "Ybodychange",
      "commitMessage": "merge trunk to branch HDFS-4949\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532952 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 7:14 PM",
      "commitName": "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/08/13 3:15 PM",
      "commitNameOld": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 47.17,
      "commitsBetweenForRepo": 86,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,26 @@\n   private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"storageID\", datanodeinfo.getStorageID());\n     m.put(\"xferPort\", datanodeinfo.getXferPort());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n+    m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n     m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n     return m;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"storageID\", datanodeinfo.getStorageID());\n    m.put(\"xferPort\", datanodeinfo.getXferPort());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n    m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n    m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "8e0804666189ce9a66b7b41b744776bad29770dd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5306. Datanode https port is not available at the namenode. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529562 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/13 8:22 PM",
      "commitName": "8e0804666189ce9a66b7b41b744776bad29770dd",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "30/07/13 5:49 PM",
      "commitNameOld": "4f68aa060090319b8de5c30f41d193632503ed17",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 67.11,
      "commitsBetweenForRepo": 397,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"storageID\", datanodeinfo.getStorageID());\n     m.put(\"xferPort\", datanodeinfo.getXferPort());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n+    m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n     return m;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"storageID\", datanodeinfo.getStorageID());\n    m.put(\"xferPort\", datanodeinfo.getXferPort());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n    m.put(\"infoSecurePort\", datanodeinfo.getInfoSecurePort());\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4988. Datanode must support all the volumes as individual storages.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1526969 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 9:05 AM",
      "commitName": "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/09/13 11:03 AM",
      "commitNameOld": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 4.92,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n+    // TODO: Fix storageID\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n     m.put(\"xferPort\", datanodeinfo.getXferPort());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n     return m;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    // TODO: Fix storageID\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n    m.put(\"xferPort\", datanodeinfo.getXferPort());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "4551da302d94cffea0313eac79479ab6f9b7cb34": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5233. Use Datanode UUID to identify Datanodes.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1525407 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/09/13 11:03 AM",
      "commitName": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "30/07/13 5:49 PM",
      "commitNameOld": "4f68aa060090319b8de5c30f41d193632503ed17",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 53.72,
      "commitsBetweenForRepo": 287,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n-    m.put(\"storageID\", datanodeinfo.getStorageID());\n+    m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n     m.put(\"xferPort\", datanodeinfo.getXferPort());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n     return m;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"storageID\", datanodeinfo.getDatanodeUuid());\n    m.put(\"xferPort\", datanodeinfo.getXferPort());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5141. Add cache status information to datanode heartbeat. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1519101 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/08/13 3:15 PM",
      "commitName": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/07/13 5:49 PM",
      "commitNameOld": "4f68aa060090319b8de5c30f41d193632503ed17",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 30.89,
      "commitsBetweenForRepo": 90,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,25 @@\n   private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"storageID\", datanodeinfo.getStorageID());\n     m.put(\"xferPort\", datanodeinfo.getXferPort());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n+    m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n+    m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n     return m;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"storageID\", datanodeinfo.getStorageID());\n    m.put(\"xferPort\", datanodeinfo.getXferPort());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"cacheCapacity\", datanodeinfo.getCacheCapacity());\n    m.put(\"cacheUsed\", datanodeinfo.getCacheUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "31/03/12 12:58 PM",
      "commitNameOld": "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.09,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n-    m.put(\"name\", datanodeinfo.getName());\n+    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n+    m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"storageID\", datanodeinfo.getStorageID());\n+    m.put(\"xferPort\", datanodeinfo.getXferPort());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n-\n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n-    m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n     return m;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"ipAddr\", datanodeinfo.getIpAddr());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"storageID\", datanodeinfo.getStorageID());\n    m.put(\"xferPort\", datanodeinfo.getXferPort());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57": {
      "type": "Ymultichange(Yrename,Yreturntypechange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-2395. Add a root element in the JSON responses of webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/11 4:29 AM",
      "commitName": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-2395. Add a root element in the JSON responses of webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/10/11 4:29 AM",
          "commitName": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "30/09/11 9:49 PM",
          "commitNameOld": "dc8464f943b61b795df0cc8baec171bf07355763",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 4.28,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  public static String toJsonString(final DatanodeInfo datanodeinfo) {\n+  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n-    final Map\u003cString, Object\u003e m \u003d datanodeInfoMap.get();\n+    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"name\", datanodeinfo.getName());\n     m.put(\"storageID\", datanodeinfo.getStorageID());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n \n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n-    return JSON.toString(m);\n+    return m;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"name\", datanodeinfo.getName());\n    m.put(\"storageID\", datanodeinfo.getStorageID());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {
            "oldValue": "toJsonString",
            "newValue": "toJsonMap"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-2395. Add a root element in the JSON responses of webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/10/11 4:29 AM",
          "commitName": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "30/09/11 9:49 PM",
          "commitNameOld": "dc8464f943b61b795df0cc8baec171bf07355763",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 4.28,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  public static String toJsonString(final DatanodeInfo datanodeinfo) {\n+  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n-    final Map\u003cString, Object\u003e m \u003d datanodeInfoMap.get();\n+    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"name\", datanodeinfo.getName());\n     m.put(\"storageID\", datanodeinfo.getStorageID());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n \n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n-    return JSON.toString(m);\n+    return m;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"name\", datanodeinfo.getName());\n    m.put(\"storageID\", datanodeinfo.getStorageID());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {
            "oldValue": "String",
            "newValue": "Map\u003cString,Object\u003e"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2395. Add a root element in the JSON responses of webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/10/11 4:29 AM",
          "commitName": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "30/09/11 9:49 PM",
          "commitNameOld": "dc8464f943b61b795df0cc8baec171bf07355763",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 4.28,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  public static String toJsonString(final DatanodeInfo datanodeinfo) {\n+  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n-    final Map\u003cString, Object\u003e m \u003d datanodeInfoMap.get();\n+    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"name\", datanodeinfo.getName());\n     m.put(\"storageID\", datanodeinfo.getStorageID());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n \n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n-    return JSON.toString(m);\n+    return m;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"name\", datanodeinfo.getName());\n    m.put(\"storageID\", datanodeinfo.getStorageID());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {
            "oldValue": "[public, static]",
            "newValue": "[private, static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2395. Add a root element in the JSON responses of webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/10/11 4:29 AM",
          "commitName": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "30/09/11 9:49 PM",
          "commitNameOld": "dc8464f943b61b795df0cc8baec171bf07355763",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 4.28,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  public static String toJsonString(final DatanodeInfo datanodeinfo) {\n+  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n     if (datanodeinfo \u003d\u003d null) {\n       return null;\n     }\n \n-    final Map\u003cString, Object\u003e m \u003d datanodeInfoMap.get();\n+    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"name\", datanodeinfo.getName());\n     m.put(\"storageID\", datanodeinfo.getStorageID());\n     m.put(\"infoPort\", datanodeinfo.getInfoPort());\n \n     m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n \n     m.put(\"capacity\", datanodeinfo.getCapacity());\n     m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n     m.put(\"remaining\", datanodeinfo.getRemaining());\n     m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n     m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n     m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n     m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n     m.put(\"hostName\", datanodeinfo.getHostName());\n     m.put(\"adminState\", datanodeinfo.getAdminState().name());\n-    return JSON.toString(m);\n+    return m;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static Map\u003cString, Object\u003e toJsonMap(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"name\", datanodeinfo.getName());\n    m.put(\"storageID\", datanodeinfo.getStorageID());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return m;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {}
        }
      ]
    },
    "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2340. Support getFileBlockLocations and getDelegationToken in webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1173468 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/09/11 7:56 PM",
      "commitName": "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,23 @@\n+  public static String toJsonString(final DatanodeInfo datanodeinfo) {\n+    if (datanodeinfo \u003d\u003d null) {\n+      return null;\n+    }\n+\n+    final Map\u003cString, Object\u003e m \u003d datanodeInfoMap.get();\n+    m.put(\"name\", datanodeinfo.getName());\n+    m.put(\"storageID\", datanodeinfo.getStorageID());\n+    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n+\n+    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n+\n+    m.put(\"capacity\", datanodeinfo.getCapacity());\n+    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n+    m.put(\"remaining\", datanodeinfo.getRemaining());\n+    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n+    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n+    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n+    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n+    m.put(\"hostName\", datanodeinfo.getHostName());\n+    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n+    return JSON.toString(m);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final DatanodeInfo datanodeinfo) {\n    if (datanodeinfo \u003d\u003d null) {\n      return null;\n    }\n\n    final Map\u003cString, Object\u003e m \u003d datanodeInfoMap.get();\n    m.put(\"name\", datanodeinfo.getName());\n    m.put(\"storageID\", datanodeinfo.getStorageID());\n    m.put(\"infoPort\", datanodeinfo.getInfoPort());\n\n    m.put(\"ipcPort\", datanodeinfo.getIpcPort());\n\n    m.put(\"capacity\", datanodeinfo.getCapacity());\n    m.put(\"dfsUsed\", datanodeinfo.getDfsUsed());\n    m.put(\"remaining\", datanodeinfo.getRemaining());\n    m.put(\"blockPoolUsed\", datanodeinfo.getBlockPoolUsed());\n    m.put(\"lastUpdate\", datanodeinfo.getLastUpdate());\n    m.put(\"xceiverCount\", datanodeinfo.getXceiverCount());\n    m.put(\"networkLocation\", datanodeinfo.getNetworkLocation());\n    m.put(\"hostName\", datanodeinfo.getHostName());\n    m.put(\"adminState\", datanodeinfo.getAdminState().name());\n    return JSON.toString(m);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java"
    }
  }
}