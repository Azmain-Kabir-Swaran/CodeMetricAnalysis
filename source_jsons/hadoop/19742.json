{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "MapTask.java",
  "functionName": "runNewMapper",
  "functionId": "runNewMapper___job-JobConf(modifiers-final)__splitIndex-TaskSplitIndex(modifiers-final)__umbilical-TaskUmbilicalProtocol(modifiers-final)__reporter-TaskReporter",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
  "functionStartLine": 745,
  "functionEndLine": 811,
  "numCommitsSeen": 36,
  "timeTaken": 9434,
  "changeHistory": [
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e",
    "0aa8188d188e73e2126086e7a67d15cfc3a3c432",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "4796e1adcb912005198c9003305c97cf3a8b523e",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e": "Ybodychange",
    "0aa8188d188e73e2126086e7a67d15cfc3a3c432": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymovefromfile",
    "4796e1adcb912005198c9003305c97cf3a8b523e": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4737. Ensure that mapreduce APIs are semantically consistent with mapred API w.r.t Mapper.cleanup and Reducer.cleanup; in the sense that cleanup is now called even if there is an error. The old mapred API already ensures that Mapper.close and Reducer.close are invoked during error handling. Note that it is an incompatible change, however end-users can override Mapper.run and Reducer.run to get the old (inconsistent) behaviour. Contributed by Arun C. Murthy.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1471556 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/04/13 10:38 AM",
      "commitName": "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "19/03/13 10:56 AM",
      "commitNameOld": "c19633da5b0cc190cc64e812ee89c38f28d5a670",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 35.99,
      "commitsBetweenForRepo": 195,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,67 @@\n   void runNewMapper(final JobConf job,\n                     final TaskSplitIndex splitIndex,\n                     final TaskUmbilicalProtocol umbilical,\n                     TaskReporter reporter\n                     ) throws IOException, ClassNotFoundException,\n                              InterruptedException {\n     // make a task context so we can get the classes\n     org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n       new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                   getTaskID(),\n                                                                   reporter);\n     // make a mapper\n     org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e mapper \u003d\n       (org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n         ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n     // make the input format\n     org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e inputFormat \u003d\n       (org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e)\n         ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n     // rebuild the input split\n     org.apache.hadoop.mapreduce.InputSplit split \u003d null;\n     split \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n         splitIndex.getStartOffset());\n     LOG.info(\"Processing split: \" + split);\n \n     org.apache.hadoop.mapreduce.RecordReader\u003cINKEY,INVALUE\u003e input \u003d\n       new NewTrackingRecordReader\u003cINKEY,INVALUE\u003e\n         (split, inputFormat, reporter, taskContext);\n     \n     job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n     org.apache.hadoop.mapreduce.RecordWriter output \u003d null;\n     \n     // get an output object\n     if (job.getNumReduceTasks() \u003d\u003d 0) {\n       output \u003d \n         new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n     } else {\n       output \u003d new NewOutputCollector(taskContext, job, umbilical, reporter);\n     }\n \n     org.apache.hadoop.mapreduce.MapContext\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e \n     mapContext \u003d \n       new MapContextImpl\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e(job, getTaskID(), \n           input, output, \n           committer, \n           reporter, split);\n \n     org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e.Context \n         mapperContext \u003d \n           new WrappedMapper\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e().getMapContext(\n               mapContext);\n \n-    input.initialize(split, mapperContext);\n-    mapper.run(mapperContext);\n-    mapPhase.complete();\n-    setPhase(TaskStatus.Phase.SORT);\n-    statusUpdate(umbilical);\n-    input.close();\n-    output.close(mapperContext);\n+    try {\n+      input.initialize(split, mapperContext);\n+      mapper.run(mapperContext);\n+      mapPhase.complete();\n+      setPhase(TaskStatus.Phase.SORT);\n+      statusUpdate(umbilical);\n+      input.close();\n+      input \u003d null;\n+      output.close(mapperContext);\n+      output \u003d null;\n+    } finally {\n+      closeQuietly(input);\n+      closeQuietly(output, mapperContext);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runNewMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, ClassNotFoundException,\n                             InterruptedException {\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                  getTaskID(),\n                                                                  reporter);\n    // make a mapper\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e mapper \u003d\n      (org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n    // make the input format\n    org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e inputFormat \u003d\n      (org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n    // rebuild the input split\n    org.apache.hadoop.mapreduce.InputSplit split \u003d null;\n    split \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n        splitIndex.getStartOffset());\n    LOG.info(\"Processing split: \" + split);\n\n    org.apache.hadoop.mapreduce.RecordReader\u003cINKEY,INVALUE\u003e input \u003d\n      new NewTrackingRecordReader\u003cINKEY,INVALUE\u003e\n        (split, inputFormat, reporter, taskContext);\n    \n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.RecordWriter output \u003d null;\n    \n    // get an output object\n    if (job.getNumReduceTasks() \u003d\u003d 0) {\n      output \u003d \n        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n    } else {\n      output \u003d new NewOutputCollector(taskContext, job, umbilical, reporter);\n    }\n\n    org.apache.hadoop.mapreduce.MapContext\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e \n    mapContext \u003d \n      new MapContextImpl\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e(job, getTaskID(), \n          input, output, \n          committer, \n          reporter, split);\n\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e.Context \n        mapperContext \u003d \n          new WrappedMapper\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e().getMapContext(\n              mapContext);\n\n    try {\n      input.initialize(split, mapperContext);\n      mapper.run(mapperContext);\n      mapPhase.complete();\n      setPhase(TaskStatus.Phase.SORT);\n      statusUpdate(umbilical);\n      input.close();\n      input \u003d null;\n      output.close(mapperContext);\n      output \u003d null;\n    } finally {\n      closeQuietly(input);\n      closeQuietly(output, mapperContext);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "0aa8188d188e73e2126086e7a67d15cfc3a3c432": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3678. The Map tasks logs should have the value of input split it processed. Contributed by Harsh J. (harsh)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1396032 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/10/12 6:39 AM",
      "commitName": "0aa8188d188e73e2126086e7a67d15cfc3a3c432",
      "commitAuthor": "Harsh J",
      "commitDateOld": "21/05/12 12:05 PM",
      "commitNameOld": "2eebc21a55ebc9230f4fb41149eb7c2e26a53b52",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 140.77,
      "commitsBetweenForRepo": 736,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,60 @@\n   void runNewMapper(final JobConf job,\n                     final TaskSplitIndex splitIndex,\n                     final TaskUmbilicalProtocol umbilical,\n                     TaskReporter reporter\n                     ) throws IOException, ClassNotFoundException,\n                              InterruptedException {\n     // make a task context so we can get the classes\n     org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n       new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                   getTaskID(),\n                                                                   reporter);\n     // make a mapper\n     org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e mapper \u003d\n       (org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n         ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n     // make the input format\n     org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e inputFormat \u003d\n       (org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e)\n         ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n     // rebuild the input split\n     org.apache.hadoop.mapreduce.InputSplit split \u003d null;\n     split \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n         splitIndex.getStartOffset());\n+    LOG.info(\"Processing split: \" + split);\n \n     org.apache.hadoop.mapreduce.RecordReader\u003cINKEY,INVALUE\u003e input \u003d\n       new NewTrackingRecordReader\u003cINKEY,INVALUE\u003e\n         (split, inputFormat, reporter, taskContext);\n     \n     job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n     org.apache.hadoop.mapreduce.RecordWriter output \u003d null;\n     \n     // get an output object\n     if (job.getNumReduceTasks() \u003d\u003d 0) {\n       output \u003d \n         new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n     } else {\n       output \u003d new NewOutputCollector(taskContext, job, umbilical, reporter);\n     }\n \n     org.apache.hadoop.mapreduce.MapContext\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e \n     mapContext \u003d \n       new MapContextImpl\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e(job, getTaskID(), \n           input, output, \n           committer, \n           reporter, split);\n \n     org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e.Context \n         mapperContext \u003d \n           new WrappedMapper\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e().getMapContext(\n               mapContext);\n \n     input.initialize(split, mapperContext);\n     mapper.run(mapperContext);\n     mapPhase.complete();\n     setPhase(TaskStatus.Phase.SORT);\n     statusUpdate(umbilical);\n     input.close();\n     output.close(mapperContext);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runNewMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, ClassNotFoundException,\n                             InterruptedException {\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                  getTaskID(),\n                                                                  reporter);\n    // make a mapper\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e mapper \u003d\n      (org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n    // make the input format\n    org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e inputFormat \u003d\n      (org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n    // rebuild the input split\n    org.apache.hadoop.mapreduce.InputSplit split \u003d null;\n    split \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n        splitIndex.getStartOffset());\n    LOG.info(\"Processing split: \" + split);\n\n    org.apache.hadoop.mapreduce.RecordReader\u003cINKEY,INVALUE\u003e input \u003d\n      new NewTrackingRecordReader\u003cINKEY,INVALUE\u003e\n        (split, inputFormat, reporter, taskContext);\n    \n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.RecordWriter output \u003d null;\n    \n    // get an output object\n    if (job.getNumReduceTasks() \u003d\u003d 0) {\n      output \u003d \n        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n    } else {\n      output \u003d new NewOutputCollector(taskContext, job, umbilical, reporter);\n    }\n\n    org.apache.hadoop.mapreduce.MapContext\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e \n    mapContext \u003d \n      new MapContextImpl\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e(job, getTaskID(), \n          input, output, \n          committer, \n          reporter, split);\n\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e.Context \n        mapperContext \u003d \n          new WrappedMapper\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e().getMapContext(\n              mapContext);\n\n    input.initialize(split, mapperContext);\n    mapper.run(mapperContext);\n    mapPhase.complete();\n    setPhase(TaskStatus.Phase.SORT);\n    statusUpdate(umbilical);\n    input.close();\n    output.close(mapperContext);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void runNewMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, ClassNotFoundException,\n                             InterruptedException {\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                  getTaskID(),\n                                                                  reporter);\n    // make a mapper\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e mapper \u003d\n      (org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n    // make the input format\n    org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e inputFormat \u003d\n      (org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n    // rebuild the input split\n    org.apache.hadoop.mapreduce.InputSplit split \u003d null;\n    split \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n        splitIndex.getStartOffset());\n\n    org.apache.hadoop.mapreduce.RecordReader\u003cINKEY,INVALUE\u003e input \u003d\n      new NewTrackingRecordReader\u003cINKEY,INVALUE\u003e\n        (split, inputFormat, reporter, taskContext);\n    \n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.RecordWriter output \u003d null;\n    \n    // get an output object\n    if (job.getNumReduceTasks() \u003d\u003d 0) {\n      output \u003d \n        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n    } else {\n      output \u003d new NewOutputCollector(taskContext, job, umbilical, reporter);\n    }\n\n    org.apache.hadoop.mapreduce.MapContext\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e \n    mapContext \u003d \n      new MapContextImpl\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e(job, getTaskID(), \n          input, output, \n          committer, \n          reporter, split);\n\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e.Context \n        mapperContext \u003d \n          new WrappedMapper\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e().getMapContext(\n              mapContext);\n\n    input.initialize(split, mapperContext);\n    mapper.run(mapperContext);\n    mapPhase.complete();\n    setPhase(TaskStatus.Phase.SORT);\n    statusUpdate(umbilical);\n    input.close();\n    output.close(mapperContext);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void runNewMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, ClassNotFoundException,\n                             InterruptedException {\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                  getTaskID(),\n                                                                  reporter);\n    // make a mapper\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e mapper \u003d\n      (org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n    // make the input format\n    org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e inputFormat \u003d\n      (org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n    // rebuild the input split\n    org.apache.hadoop.mapreduce.InputSplit split \u003d null;\n    split \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n        splitIndex.getStartOffset());\n\n    org.apache.hadoop.mapreduce.RecordReader\u003cINKEY,INVALUE\u003e input \u003d\n      new NewTrackingRecordReader\u003cINKEY,INVALUE\u003e\n        (split, inputFormat, reporter, taskContext);\n    \n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.RecordWriter output \u003d null;\n    \n    // get an output object\n    if (job.getNumReduceTasks() \u003d\u003d 0) {\n      output \u003d \n        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n    } else {\n      output \u003d new NewOutputCollector(taskContext, job, umbilical, reporter);\n    }\n\n    org.apache.hadoop.mapreduce.MapContext\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e \n    mapContext \u003d \n      new MapContextImpl\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e(job, getTaskID(), \n          input, output, \n          committer, \n          reporter, split);\n\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e.Context \n        mapperContext \u003d \n          new WrappedMapper\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e().getMapContext(\n              mapContext);\n\n    input.initialize(split, mapperContext);\n    mapper.run(mapperContext);\n    mapPhase.complete();\n    setPhase(TaskStatus.Phase.SORT);\n    statusUpdate(umbilical);\n    input.close();\n    output.close(mapperContext);\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
        "oldMethodName": "runNewMapper",
        "newMethodName": "runNewMapper"
      }
    },
    "4796e1adcb912005198c9003305c97cf3a8b523e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2365. Add counters to track bytes (read,written) via File(Input,Output)Format. Contributed by Siddharth Seth. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1146515 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/07/11 4:36 PM",
      "commitName": "4796e1adcb912005198c9003305c97cf3a8b523e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "11/07/11 5:54 PM",
      "commitNameOld": "ad7cf36d5fd99ecaf29e33d8de437e21f81a32d3",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.95,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,59 @@\n   void runNewMapper(final JobConf job,\n                     final TaskSplitIndex splitIndex,\n                     final TaskUmbilicalProtocol umbilical,\n                     TaskReporter reporter\n                     ) throws IOException, ClassNotFoundException,\n                              InterruptedException {\n     // make a task context so we can get the classes\n     org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n       new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                   getTaskID(),\n                                                                   reporter);\n     // make a mapper\n     org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e mapper \u003d\n       (org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n         ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n     // make the input format\n     org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e inputFormat \u003d\n       (org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e)\n         ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n     // rebuild the input split\n     org.apache.hadoop.mapreduce.InputSplit split \u003d null;\n     split \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n         splitIndex.getStartOffset());\n \n     org.apache.hadoop.mapreduce.RecordReader\u003cINKEY,INVALUE\u003e input \u003d\n       new NewTrackingRecordReader\u003cINKEY,INVALUE\u003e\n-          (inputFormat.createRecordReader(split, taskContext), reporter);\n+        (split, inputFormat, reporter, taskContext);\n     \n     job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n     org.apache.hadoop.mapreduce.RecordWriter output \u003d null;\n     \n     // get an output object\n     if (job.getNumReduceTasks() \u003d\u003d 0) {\n       output \u003d \n         new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n     } else {\n       output \u003d new NewOutputCollector(taskContext, job, umbilical, reporter);\n     }\n \n     org.apache.hadoop.mapreduce.MapContext\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e \n     mapContext \u003d \n       new MapContextImpl\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e(job, getTaskID(), \n           input, output, \n           committer, \n           reporter, split);\n \n     org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e.Context \n         mapperContext \u003d \n           new WrappedMapper\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e().getMapContext(\n               mapContext);\n \n     input.initialize(split, mapperContext);\n     mapper.run(mapperContext);\n     mapPhase.complete();\n     setPhase(TaskStatus.Phase.SORT);\n     statusUpdate(umbilical);\n     input.close();\n     output.close(mapperContext);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runNewMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, ClassNotFoundException,\n                             InterruptedException {\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                  getTaskID(),\n                                                                  reporter);\n    // make a mapper\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e mapper \u003d\n      (org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n    // make the input format\n    org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e inputFormat \u003d\n      (org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n    // rebuild the input split\n    org.apache.hadoop.mapreduce.InputSplit split \u003d null;\n    split \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n        splitIndex.getStartOffset());\n\n    org.apache.hadoop.mapreduce.RecordReader\u003cINKEY,INVALUE\u003e input \u003d\n      new NewTrackingRecordReader\u003cINKEY,INVALUE\u003e\n        (split, inputFormat, reporter, taskContext);\n    \n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.RecordWriter output \u003d null;\n    \n    // get an output object\n    if (job.getNumReduceTasks() \u003d\u003d 0) {\n      output \u003d \n        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n    } else {\n      output \u003d new NewOutputCollector(taskContext, job, umbilical, reporter);\n    }\n\n    org.apache.hadoop.mapreduce.MapContext\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e \n    mapContext \u003d \n      new MapContextImpl\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e(job, getTaskID(), \n          input, output, \n          committer, \n          reporter, split);\n\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e.Context \n        mapperContext \u003d \n          new WrappedMapper\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e().getMapContext(\n              mapContext);\n\n    input.initialize(split, mapperContext);\n    mapper.run(mapperContext);\n    mapPhase.complete();\n    setPhase(TaskStatus.Phase.SORT);\n    statusUpdate(umbilical);\n    input.close();\n    output.close(mapperContext);\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,59 @@\n+  void runNewMapper(final JobConf job,\n+                    final TaskSplitIndex splitIndex,\n+                    final TaskUmbilicalProtocol umbilical,\n+                    TaskReporter reporter\n+                    ) throws IOException, ClassNotFoundException,\n+                             InterruptedException {\n+    // make a task context so we can get the classes\n+    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n+      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n+                                                                  getTaskID(),\n+                                                                  reporter);\n+    // make a mapper\n+    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e mapper \u003d\n+      (org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n+        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n+    // make the input format\n+    org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e inputFormat \u003d\n+      (org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e)\n+        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n+    // rebuild the input split\n+    org.apache.hadoop.mapreduce.InputSplit split \u003d null;\n+    split \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n+        splitIndex.getStartOffset());\n+\n+    org.apache.hadoop.mapreduce.RecordReader\u003cINKEY,INVALUE\u003e input \u003d\n+      new NewTrackingRecordReader\u003cINKEY,INVALUE\u003e\n+          (inputFormat.createRecordReader(split, taskContext), reporter);\n+    \n+    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n+    org.apache.hadoop.mapreduce.RecordWriter output \u003d null;\n+    \n+    // get an output object\n+    if (job.getNumReduceTasks() \u003d\u003d 0) {\n+      output \u003d \n+        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n+    } else {\n+      output \u003d new NewOutputCollector(taskContext, job, umbilical, reporter);\n+    }\n+\n+    org.apache.hadoop.mapreduce.MapContext\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e \n+    mapContext \u003d \n+      new MapContextImpl\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e(job, getTaskID(), \n+          input, output, \n+          committer, \n+          reporter, split);\n+\n+    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e.Context \n+        mapperContext \u003d \n+          new WrappedMapper\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e().getMapContext(\n+              mapContext);\n+\n+    input.initialize(split, mapperContext);\n+    mapper.run(mapperContext);\n+    mapPhase.complete();\n+    setPhase(TaskStatus.Phase.SORT);\n+    statusUpdate(umbilical);\n+    input.close();\n+    output.close(mapperContext);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void runNewMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, ClassNotFoundException,\n                             InterruptedException {\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, \n                                                                  getTaskID(),\n                                                                  reporter);\n    // make a mapper\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e mapper \u003d\n      (org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\n    // make the input format\n    org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e inputFormat \u003d\n      (org.apache.hadoop.mapreduce.InputFormat\u003cINKEY,INVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\n    // rebuild the input split\n    org.apache.hadoop.mapreduce.InputSplit split \u003d null;\n    split \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n        splitIndex.getStartOffset());\n\n    org.apache.hadoop.mapreduce.RecordReader\u003cINKEY,INVALUE\u003e input \u003d\n      new NewTrackingRecordReader\u003cINKEY,INVALUE\u003e\n          (inputFormat.createRecordReader(split, taskContext), reporter);\n    \n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.RecordWriter output \u003d null;\n    \n    // get an output object\n    if (job.getNumReduceTasks() \u003d\u003d 0) {\n      output \u003d \n        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\n    } else {\n      output \u003d new NewOutputCollector(taskContext, job, umbilical, reporter);\n    }\n\n    org.apache.hadoop.mapreduce.MapContext\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e \n    mapContext \u003d \n      new MapContextImpl\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e(job, getTaskID(), \n          input, output, \n          committer, \n          reporter, split);\n\n    org.apache.hadoop.mapreduce.Mapper\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e.Context \n        mapperContext \u003d \n          new WrappedMapper\u003cINKEY, INVALUE, OUTKEY, OUTVALUE\u003e().getMapContext(\n              mapContext);\n\n    input.initialize(split, mapperContext);\n    mapper.run(mapperContext);\n    mapPhase.complete();\n    setPhase(TaskStatus.Phase.SORT);\n    statusUpdate(umbilical);\n    input.close();\n    output.close(mapperContext);\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java"
    }
  }
}