{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockStorageMovementAttemptedItems.java",
  "functionName": "blocksStorageMovementUnReportedItemsCheck",
  "functionId": "blocksStorageMovementUnReportedItemsCheck",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java",
  "functionStartLine": 246,
  "functionEndLine": 265,
  "numCommitsSeen": 22,
  "timeTaken": 7550,
  "changeHistory": [
    "66e8f9b31529226309c924226a53dead3e6fcf11",
    "2acc50b826fa8b00f2b09d9546c4b3215b89d46d",
    "8467ec24fb74f30371d5a13e893fc56309ee9372",
    "8d4f74e7339abc77dc0daa162d7bd2814bd79b3d",
    "78420719eb1f138c6f10558befb7bc8ebcc28a54",
    "c561cb316e365ef674784cd6cf0b12c0fbc271a3",
    "00eceed233d6e80d5c7137bf5b5286746ec4d5fb",
    "bfd3f8bd8a9ae2186ec3e4addc71f912ec7b8923",
    "7ea24fc06c081e2ba6f5f66d212abb14b80c9064",
    "695a402fcad20c711c5d845e0664c43fd6b06286",
    "b7bed9f00a25bcad6f9c3543f5a1fb0a1f23b0e9",
    "047526b4c27909f78313e1ed6216de85c6137f14"
  ],
  "changeHistoryShort": {
    "66e8f9b31529226309c924226a53dead3e6fcf11": "Ybodychange",
    "2acc50b826fa8b00f2b09d9546c4b3215b89d46d": "Ybodychange",
    "8467ec24fb74f30371d5a13e893fc56309ee9372": "Ybodychange",
    "8d4f74e7339abc77dc0daa162d7bd2814bd79b3d": "Ybodychange",
    "78420719eb1f138c6f10558befb7bc8ebcc28a54": "Yfilerename",
    "c561cb316e365ef674784cd6cf0b12c0fbc271a3": "Ybodychange",
    "00eceed233d6e80d5c7137bf5b5286746ec4d5fb": "Ybodychange",
    "bfd3f8bd8a9ae2186ec3e4addc71f912ec7b8923": "Ybodychange",
    "7ea24fc06c081e2ba6f5f66d212abb14b80c9064": "Ybodychange",
    "695a402fcad20c711c5d845e0664c43fd6b06286": "Ybodychange",
    "b7bed9f00a25bcad6f9c3543f5a1fb0a1f23b0e9": "Ymultichange(Ymodifierchange,Ybodychange)",
    "047526b4c27909f78313e1ed6216de85c6137f14": "Yintroduced"
  },
  "changeHistoryDetails": {
    "66e8f9b31529226309c924226a53dead3e6fcf11": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13381 : [SPS]: Use DFSUtilClient#makePathFromFileId() to prepare satisfier file path. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "66e8f9b31529226309c924226a53dead3e6fcf11",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "2acc50b826fa8b00f2b09d9546c4b3215b89d46d",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   void blocksStorageMovementUnReportedItemsCheck() {\n     synchronized (storageMovementAttemptedItems) {\n-      Iterator\u003cAttemptedItemInfo\u003cT\u003e\u003e iter \u003d storageMovementAttemptedItems\n+      Iterator\u003cAttemptedItemInfo\u003e iter \u003d storageMovementAttemptedItems\n           .iterator();\n       long now \u003d monotonicNow();\n       while (iter.hasNext()) {\n-        AttemptedItemInfo\u003cT\u003e itemInfo \u003d iter.next();\n+        AttemptedItemInfo itemInfo \u003d iter.next();\n         if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n             + selfRetryTimeout) {\n-          T file \u003d itemInfo.getFile();\n-          ItemInfo\u003cT\u003e candidate \u003d new ItemInfo\u003cT\u003e(itemInfo.getStartPath(), file,\n+          long file \u003d itemInfo.getFile();\n+          ItemInfo candidate \u003d new ItemInfo(itemInfo.getStartPath(), file,\n               itemInfo.getRetryCount() + 1);\n           blockStorageMovementNeeded.add(candidate);\n           iter.remove();\n           LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n               + \"retries queue for next iteration.\", file);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cAttemptedItemInfo\u003e iter \u003d storageMovementAttemptedItems\n          .iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        AttemptedItemInfo itemInfo \u003d iter.next();\n        if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n            + selfRetryTimeout) {\n          long file \u003d itemInfo.getFile();\n          ItemInfo candidate \u003d new ItemInfo(itemInfo.getStartPath(), file,\n              itemInfo.getRetryCount() + 1);\n          blockStorageMovementNeeded.add(candidate);\n          iter.remove();\n          LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n              + \"retries queue for next iteration.\", file);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java",
      "extendedDetails": {}
    },
    "2acc50b826fa8b00f2b09d9546c4b3215b89d46d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13165: [SPS]: Collects successfully moved block details via IBR. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "2acc50b826fa8b00f2b09d9546c4b3215b89d46d",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "8467ec24fb74f30371d5a13e893fc56309ee9372",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,20 @@\n   void blocksStorageMovementUnReportedItemsCheck() {\n     synchronized (storageMovementAttemptedItems) {\n       Iterator\u003cAttemptedItemInfo\u003cT\u003e\u003e iter \u003d storageMovementAttemptedItems\n           .iterator();\n       long now \u003d monotonicNow();\n       while (iter.hasNext()) {\n         AttemptedItemInfo\u003cT\u003e itemInfo \u003d iter.next();\n         if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n             + selfRetryTimeout) {\n           T file \u003d itemInfo.getFile();\n-          synchronized (movementFinishedBlocks) {\n-            ItemInfo\u003cT\u003e candidate \u003d new ItemInfo\u003cT\u003e(itemInfo.getStartPath(),\n-                file, itemInfo.getRetryCount() + 1);\n-            blockStorageMovementNeeded.add(candidate);\n-            iter.remove();\n-            LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n-                + \"retries queue for next iteration.\", file);\n-          }\n+          ItemInfo\u003cT\u003e candidate \u003d new ItemInfo\u003cT\u003e(itemInfo.getStartPath(), file,\n+              itemInfo.getRetryCount() + 1);\n+          blockStorageMovementNeeded.add(candidate);\n+          iter.remove();\n+          LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n+              + \"retries queue for next iteration.\", file);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cAttemptedItemInfo\u003cT\u003e\u003e iter \u003d storageMovementAttemptedItems\n          .iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        AttemptedItemInfo\u003cT\u003e itemInfo \u003d iter.next();\n        if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n            + selfRetryTimeout) {\n          T file \u003d itemInfo.getFile();\n          ItemInfo\u003cT\u003e candidate \u003d new ItemInfo\u003cT\u003e(itemInfo.getStartPath(), file,\n              itemInfo.getRetryCount() + 1);\n          blockStorageMovementNeeded.add(candidate);\n          iter.remove();\n          LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n              + \"retries queue for next iteration.\", file);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java",
      "extendedDetails": {}
    },
    "8467ec24fb74f30371d5a13e893fc56309ee9372": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13110: [SPS]: Reduce the number of APIs in NamenodeProtocol used by external satisfier. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "8467ec24fb74f30371d5a13e893fc56309ee9372",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "b0cb8d9bb44c963ae686d2b5c1b70bc76b955e10",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,22 @@\n   void blocksStorageMovementUnReportedItemsCheck() {\n     synchronized (storageMovementAttemptedItems) {\n-      Iterator\u003cAttemptedItemInfo\u003e iter \u003d storageMovementAttemptedItems\n+      Iterator\u003cAttemptedItemInfo\u003cT\u003e\u003e iter \u003d storageMovementAttemptedItems\n           .iterator();\n       long now \u003d monotonicNow();\n       while (iter.hasNext()) {\n-        AttemptedItemInfo itemInfo \u003d iter.next();\n+        AttemptedItemInfo\u003cT\u003e itemInfo \u003d iter.next();\n         if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n             + selfRetryTimeout) {\n-          Long blockCollectionID \u003d itemInfo.getFileId();\n+          T file \u003d itemInfo.getFile();\n           synchronized (movementFinishedBlocks) {\n-            ItemInfo candidate \u003d new ItemInfo(itemInfo.getStartId(),\n-                blockCollectionID, itemInfo.getRetryCount() + 1);\n+            ItemInfo\u003cT\u003e candidate \u003d new ItemInfo\u003cT\u003e(itemInfo.getStartPath(),\n+                file, itemInfo.getRetryCount() + 1);\n             blockStorageMovementNeeded.add(candidate);\n             iter.remove();\n             LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n-                + \"retries queue for next iteration.\", blockCollectionID);\n+                + \"retries queue for next iteration.\", file);\n           }\n         }\n       }\n-\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cAttemptedItemInfo\u003cT\u003e\u003e iter \u003d storageMovementAttemptedItems\n          .iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        AttemptedItemInfo\u003cT\u003e itemInfo \u003d iter.next();\n        if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n            + selfRetryTimeout) {\n          T file \u003d itemInfo.getFile();\n          synchronized (movementFinishedBlocks) {\n            ItemInfo\u003cT\u003e candidate \u003d new ItemInfo\u003cT\u003e(itemInfo.getStartPath(),\n                file, itemInfo.getRetryCount() + 1);\n            blockStorageMovementNeeded.add(candidate);\n            iter.remove();\n            LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                + \"retries queue for next iteration.\", file);\n          }\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java",
      "extendedDetails": {}
    },
    "8d4f74e7339abc77dc0daa162d7bd2814bd79b3d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12911. [SPS]: Modularize the SPS code and expose necessary interfaces for external/internal implementations. Contributed by Uma Maheswara Rao G\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "8d4f74e7339abc77dc0daa162d7bd2814bd79b3d",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "05d4daf6ba3e5bd40f46e8003ee12fc7c613453d",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   void blocksStorageMovementUnReportedItemsCheck() {\n     synchronized (storageMovementAttemptedItems) {\n       Iterator\u003cAttemptedItemInfo\u003e iter \u003d storageMovementAttemptedItems\n           .iterator();\n       long now \u003d monotonicNow();\n       while (iter.hasNext()) {\n         AttemptedItemInfo itemInfo \u003d iter.next();\n         if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n             + selfRetryTimeout) {\n-          Long blockCollectionID \u003d itemInfo.getTrackId();\n+          Long blockCollectionID \u003d itemInfo.getFileId();\n           synchronized (movementFinishedBlocks) {\n             ItemInfo candidate \u003d new ItemInfo(itemInfo.getStartId(),\n                 blockCollectionID, itemInfo.getRetryCount() + 1);\n             blockStorageMovementNeeded.add(candidate);\n             iter.remove();\n             LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                 + \"retries queue for next iteration.\", blockCollectionID);\n           }\n         }\n       }\n \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cAttemptedItemInfo\u003e iter \u003d storageMovementAttemptedItems\n          .iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        AttemptedItemInfo itemInfo \u003d iter.next();\n        if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n            + selfRetryTimeout) {\n          Long blockCollectionID \u003d itemInfo.getFileId();\n          synchronized (movementFinishedBlocks) {\n            ItemInfo candidate \u003d new ItemInfo(itemInfo.getStartId(),\n                blockCollectionID, itemInfo.getRetryCount() + 1);\n            blockStorageMovementNeeded.add(candidate);\n            iter.remove();\n            LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                + \"retries queue for next iteration.\", blockCollectionID);\n          }\n        }\n      }\n\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java",
      "extendedDetails": {}
    },
    "78420719eb1f138c6f10558befb7bc8ebcc28a54": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-12955: [SPS]: Move SPS classes to a separate package. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "78420719eb1f138c6f10558befb7bc8ebcc28a54",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "c561cb316e365ef674784cd6cf0b12c0fbc271a3",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cAttemptedItemInfo\u003e iter \u003d storageMovementAttemptedItems\n          .iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        AttemptedItemInfo itemInfo \u003d iter.next();\n        if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n            + selfRetryTimeout) {\n          Long blockCollectionID \u003d itemInfo.getTrackId();\n          synchronized (movementFinishedBlocks) {\n            ItemInfo candidate \u003d new ItemInfo(itemInfo.getStartId(),\n                blockCollectionID, itemInfo.getRetryCount() + 1);\n            blockStorageMovementNeeded.add(candidate);\n            iter.remove();\n            LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                + \"retries queue for next iteration.\", blockCollectionID);\n          }\n        }\n      }\n\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementAttemptedItems.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java"
      }
    },
    "c561cb316e365ef674784cd6cf0b12c0fbc271a3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12106: [SPS]: Improve storage policy satisfier configurations. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "c561cb316e365ef674784cd6cf0b12c0fbc271a3",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "00eceed233d6e80d5c7137bf5b5286746ec4d5fb",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   void blocksStorageMovementUnReportedItemsCheck() {\n     synchronized (storageMovementAttemptedItems) {\n       Iterator\u003cAttemptedItemInfo\u003e iter \u003d storageMovementAttemptedItems\n           .iterator();\n       long now \u003d monotonicNow();\n       while (iter.hasNext()) {\n         AttemptedItemInfo itemInfo \u003d iter.next();\n         if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n             + selfRetryTimeout) {\n           Long blockCollectionID \u003d itemInfo.getTrackId();\n           synchronized (movementFinishedBlocks) {\n             ItemInfo candidate \u003d new ItemInfo(itemInfo.getStartId(),\n-                blockCollectionID);\n+                blockCollectionID, itemInfo.getRetryCount() + 1);\n             blockStorageMovementNeeded.add(candidate);\n             iter.remove();\n             LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                 + \"retries queue for next iteration.\", blockCollectionID);\n           }\n         }\n       }\n \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cAttemptedItemInfo\u003e iter \u003d storageMovementAttemptedItems\n          .iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        AttemptedItemInfo itemInfo \u003d iter.next();\n        if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n            + selfRetryTimeout) {\n          Long blockCollectionID \u003d itemInfo.getTrackId();\n          synchronized (movementFinishedBlocks) {\n            ItemInfo candidate \u003d new ItemInfo(itemInfo.getStartId(),\n                blockCollectionID, itemInfo.getRetryCount() + 1);\n            blockStorageMovementNeeded.add(candidate);\n            iter.remove();\n            LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                + \"retries queue for next iteration.\", blockCollectionID);\n          }\n        }\n      }\n\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementAttemptedItems.java",
      "extendedDetails": {}
    },
    "00eceed233d6e80d5c7137bf5b5286746ec4d5fb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12570: [SPS]: Refactor Co-ordinator datanode logic to track the block storage movements. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "00eceed233d6e80d5c7137bf5b5286746ec4d5fb",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "bfd3f8bd8a9ae2186ec3e4addc71f912ec7b8923",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,23 @@\n   void blocksStorageMovementUnReportedItemsCheck() {\n     synchronized (storageMovementAttemptedItems) {\n-      Iterator\u003cEntry\u003cLong, AttemptedItemInfo\u003e\u003e iter \u003d\n-          storageMovementAttemptedItems.entrySet().iterator();\n+      Iterator\u003cAttemptedItemInfo\u003e iter \u003d storageMovementAttemptedItems\n+          .iterator();\n       long now \u003d monotonicNow();\n       while (iter.hasNext()) {\n-        Entry\u003cLong, AttemptedItemInfo\u003e entry \u003d iter.next();\n-        AttemptedItemInfo itemInfo \u003d entry.getValue();\n+        AttemptedItemInfo itemInfo \u003d iter.next();\n         if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n             + selfRetryTimeout) {\n-          Long blockCollectionID \u003d entry.getKey();\n-          synchronized (storageMovementAttemptedResults) {\n-            if (!isExistInResult(blockCollectionID)) {\n-              ItemInfo candidate \u003d new ItemInfo(\n-                  itemInfo.getStartId(), blockCollectionID);\n-              blockStorageMovementNeeded.add(candidate);\n-              iter.remove();\n-              LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n-                  + \"retries queue for next iteration.\", blockCollectionID);\n-            } else {\n-              LOG.info(\"Blocks storage movement results for the\"\n-                  + \" tracking id : \" + blockCollectionID\n-                  + \" is reported from one of the co-ordinating datanode.\"\n-                  + \" So, the result will be processed soon.\");\n-            }\n+          Long blockCollectionID \u003d itemInfo.getTrackId();\n+          synchronized (movementFinishedBlocks) {\n+            ItemInfo candidate \u003d new ItemInfo(itemInfo.getStartId(),\n+                blockCollectionID);\n+            blockStorageMovementNeeded.add(candidate);\n+            iter.remove();\n+            LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n+                + \"retries queue for next iteration.\", blockCollectionID);\n           }\n         }\n       }\n \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cAttemptedItemInfo\u003e iter \u003d storageMovementAttemptedItems\n          .iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        AttemptedItemInfo itemInfo \u003d iter.next();\n        if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n            + selfRetryTimeout) {\n          Long blockCollectionID \u003d itemInfo.getTrackId();\n          synchronized (movementFinishedBlocks) {\n            ItemInfo candidate \u003d new ItemInfo(itemInfo.getStartId(),\n                blockCollectionID);\n            blockStorageMovementNeeded.add(candidate);\n            iter.remove();\n            LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                + \"retries queue for next iteration.\", blockCollectionID);\n          }\n        }\n      }\n\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementAttemptedItems.java",
      "extendedDetails": {}
    },
    "bfd3f8bd8a9ae2186ec3e4addc71f912ec7b8923": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12291: [SPS]: Provide a mechanism to recursively iterate and satisfy storage policy of all the files under the given dir. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "bfd3f8bd8a9ae2186ec3e4addc71f912ec7b8923",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "7ea24fc06c081e2ba6f5f66d212abb14b80c9064",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   void blocksStorageMovementUnReportedItemsCheck() {\n     synchronized (storageMovementAttemptedItems) {\n       Iterator\u003cEntry\u003cLong, AttemptedItemInfo\u003e\u003e iter \u003d\n           storageMovementAttemptedItems.entrySet().iterator();\n       long now \u003d monotonicNow();\n       while (iter.hasNext()) {\n         Entry\u003cLong, AttemptedItemInfo\u003e entry \u003d iter.next();\n         AttemptedItemInfo itemInfo \u003d entry.getValue();\n         if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n             + selfRetryTimeout) {\n           Long blockCollectionID \u003d entry.getKey();\n           synchronized (storageMovementAttemptedResults) {\n             if (!isExistInResult(blockCollectionID)) {\n               ItemInfo candidate \u003d new ItemInfo(\n-                  itemInfo.getRootId(), blockCollectionID);\n+                  itemInfo.getStartId(), blockCollectionID);\n               blockStorageMovementNeeded.add(candidate);\n               iter.remove();\n               LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                   + \"retries queue for next iteration.\", blockCollectionID);\n             } else {\n               LOG.info(\"Blocks storage movement results for the\"\n                   + \" tracking id : \" + blockCollectionID\n                   + \" is reported from one of the co-ordinating datanode.\"\n                   + \" So, the result will be processed soon.\");\n             }\n           }\n         }\n       }\n \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cEntry\u003cLong, AttemptedItemInfo\u003e\u003e iter \u003d\n          storageMovementAttemptedItems.entrySet().iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        Entry\u003cLong, AttemptedItemInfo\u003e entry \u003d iter.next();\n        AttemptedItemInfo itemInfo \u003d entry.getValue();\n        if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n            + selfRetryTimeout) {\n          Long blockCollectionID \u003d entry.getKey();\n          synchronized (storageMovementAttemptedResults) {\n            if (!isExistInResult(blockCollectionID)) {\n              ItemInfo candidate \u003d new ItemInfo(\n                  itemInfo.getStartId(), blockCollectionID);\n              blockStorageMovementNeeded.add(candidate);\n              iter.remove();\n              LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                  + \"retries queue for next iteration.\", blockCollectionID);\n            } else {\n              LOG.info(\"Blocks storage movement results for the\"\n                  + \" tracking id : \" + blockCollectionID\n                  + \" is reported from one of the co-ordinating datanode.\"\n                  + \" So, the result will be processed soon.\");\n            }\n          }\n        }\n      }\n\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementAttemptedItems.java",
      "extendedDetails": {}
    },
    "7ea24fc06c081e2ba6f5f66d212abb14b80c9064": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12225: [SPS]: Optimize extended attributes for tracking SPS movements. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "7ea24fc06c081e2ba6f5f66d212abb14b80c9064",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "0e820f16af309cc8476edba448dd548686431133",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,31 @@\n   void blocksStorageMovementUnReportedItemsCheck() {\n     synchronized (storageMovementAttemptedItems) {\n-      Iterator\u003cEntry\u003cLong, ItemInfo\u003e\u003e iter \u003d storageMovementAttemptedItems\n-          .entrySet().iterator();\n+      Iterator\u003cEntry\u003cLong, AttemptedItemInfo\u003e\u003e iter \u003d\n+          storageMovementAttemptedItems.entrySet().iterator();\n       long now \u003d monotonicNow();\n       while (iter.hasNext()) {\n-        Entry\u003cLong, ItemInfo\u003e entry \u003d iter.next();\n-        ItemInfo itemInfo \u003d entry.getValue();\n+        Entry\u003cLong, AttemptedItemInfo\u003e entry \u003d iter.next();\n+        AttemptedItemInfo itemInfo \u003d entry.getValue();\n         if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n             + selfRetryTimeout) {\n           Long blockCollectionID \u003d entry.getKey();\n           synchronized (storageMovementAttemptedResults) {\n             if (!isExistInResult(blockCollectionID)) {\n-              blockStorageMovementNeeded.add(blockCollectionID);\n+              ItemInfo candidate \u003d new ItemInfo(\n+                  itemInfo.getRootId(), blockCollectionID);\n+              blockStorageMovementNeeded.add(candidate);\n               iter.remove();\n               LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                   + \"retries queue for next iteration.\", blockCollectionID);\n             } else {\n               LOG.info(\"Blocks storage movement results for the\"\n                   + \" tracking id : \" + blockCollectionID\n                   + \" is reported from one of the co-ordinating datanode.\"\n                   + \" So, the result will be processed soon.\");\n             }\n           }\n         }\n       }\n \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cEntry\u003cLong, AttemptedItemInfo\u003e\u003e iter \u003d\n          storageMovementAttemptedItems.entrySet().iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        Entry\u003cLong, AttemptedItemInfo\u003e entry \u003d iter.next();\n        AttemptedItemInfo itemInfo \u003d entry.getValue();\n        if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n            + selfRetryTimeout) {\n          Long blockCollectionID \u003d entry.getKey();\n          synchronized (storageMovementAttemptedResults) {\n            if (!isExistInResult(blockCollectionID)) {\n              ItemInfo candidate \u003d new ItemInfo(\n                  itemInfo.getRootId(), blockCollectionID);\n              blockStorageMovementNeeded.add(candidate);\n              iter.remove();\n              LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                  + \"retries queue for next iteration.\", blockCollectionID);\n            } else {\n              LOG.info(\"Blocks storage movement results for the\"\n                  + \" tracking id : \" + blockCollectionID\n                  + \" is reported from one of the co-ordinating datanode.\"\n                  + \" So, the result will be processed soon.\");\n            }\n          }\n        }\n      }\n\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementAttemptedItems.java",
      "extendedDetails": {}
    },
    "695a402fcad20c711c5d845e0664c43fd6b06286": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11334: [SPS]: NN switch and rescheduling movements can lead to have more than one coordinator for same file blocks. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:05 AM",
      "commitName": "695a402fcad20c711c5d845e0664c43fd6b06286",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:05 AM",
      "commitNameOld": "11a08a7e8f727449f17d1e31855996353b2975fe",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,29 @@\n   void blocksStorageMovementUnReportedItemsCheck() {\n     synchronized (storageMovementAttemptedItems) {\n       Iterator\u003cEntry\u003cLong, ItemInfo\u003e\u003e iter \u003d storageMovementAttemptedItems\n           .entrySet().iterator();\n       long now \u003d monotonicNow();\n       while (iter.hasNext()) {\n         Entry\u003cLong, ItemInfo\u003e entry \u003d iter.next();\n         ItemInfo itemInfo \u003d entry.getValue();\n-        if (now \u003e itemInfo.getLastAttemptedTimeStamp() + selfRetryTimeout) {\n+        if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n+            + selfRetryTimeout) {\n           Long blockCollectionID \u003d entry.getKey();\n           synchronized (storageMovementAttemptedResults) {\n             if (!isExistInResult(blockCollectionID)) {\n               blockStorageMovementNeeded.add(blockCollectionID);\n               iter.remove();\n               LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                   + \"retries queue for next iteration.\", blockCollectionID);\n             } else {\n               LOG.info(\"Blocks storage movement results for the\"\n                   + \" tracking id : \" + blockCollectionID\n                   + \" is reported from one of the co-ordinating datanode.\"\n                   + \" So, the result will be processed soon.\");\n             }\n           }\n         }\n       }\n \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cEntry\u003cLong, ItemInfo\u003e\u003e iter \u003d storageMovementAttemptedItems\n          .entrySet().iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        Entry\u003cLong, ItemInfo\u003e entry \u003d iter.next();\n        ItemInfo itemInfo \u003d entry.getValue();\n        if (now \u003e itemInfo.getLastAttemptedOrReportedTime()\n            + selfRetryTimeout) {\n          Long blockCollectionID \u003d entry.getKey();\n          synchronized (storageMovementAttemptedResults) {\n            if (!isExistInResult(blockCollectionID)) {\n              blockStorageMovementNeeded.add(blockCollectionID);\n              iter.remove();\n              LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                  + \"retries queue for next iteration.\", blockCollectionID);\n            } else {\n              LOG.info(\"Blocks storage movement results for the\"\n                  + \" tracking id : \" + blockCollectionID\n                  + \" is reported from one of the co-ordinating datanode.\"\n                  + \" So, the result will be processed soon.\");\n            }\n          }\n        }\n      }\n\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementAttemptedItems.java",
      "extendedDetails": {}
    },
    "b7bed9f00a25bcad6f9c3543f5a1fb0a1f23b0e9": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-11248: [SPS]: Handle partial block location movements. Contributed by Rakesh R\n",
      "commitDate": "12/08/18 3:05 AM",
      "commitName": "b7bed9f00a25bcad6f9c3543f5a1fb0a1f23b0e9",
      "commitAuthor": "Uma Maheswara Rao G",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-11248: [SPS]: Handle partial block location movements. Contributed by Rakesh R\n",
          "commitDate": "12/08/18 3:05 AM",
          "commitName": "b7bed9f00a25bcad6f9c3543f5a1fb0a1f23b0e9",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "12/08/18 3:05 AM",
          "commitNameOld": "5179d99b7e1faeac1ce041967480115913d9f795",
          "commitAuthorOld": "Rakesh Radhakrishnan",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,28 @@\n-    private void blocksStorageMovementUnReportedItemsCheck() {\n-      synchronized (storageMovementAttemptedItems) {\n-        Iterator\u003cEntry\u003cLong, Long\u003e\u003e iter \u003d\n-            storageMovementAttemptedItems.entrySet().iterator();\n-        long now \u003d monotonicNow();\n-        while (iter.hasNext()) {\n-          Entry\u003cLong, Long\u003e entry \u003d iter.next();\n-          if (now \u003e entry.getValue() + selfRetryTimeout) {\n-            Long blockCollectionID \u003d entry.getKey();\n-            synchronized (storageMovementAttemptedResults) {\n-              boolean exist \u003d isExistInResult(blockCollectionID);\n-              if (!exist) {\n-                blockStorageMovementNeeded.add(blockCollectionID);\n-              } else {\n-                LOG.info(\"Blocks storage movement results for the\"\n-                    + \" tracking id : \" + blockCollectionID\n-                    + \" is reported from one of the co-ordinating datanode.\"\n-                    + \" So, the result will be processed soon.\");\n-              }\n+  void blocksStorageMovementUnReportedItemsCheck() {\n+    synchronized (storageMovementAttemptedItems) {\n+      Iterator\u003cEntry\u003cLong, ItemInfo\u003e\u003e iter \u003d storageMovementAttemptedItems\n+          .entrySet().iterator();\n+      long now \u003d monotonicNow();\n+      while (iter.hasNext()) {\n+        Entry\u003cLong, ItemInfo\u003e entry \u003d iter.next();\n+        ItemInfo itemInfo \u003d entry.getValue();\n+        if (now \u003e itemInfo.getLastAttemptedTimeStamp() + selfRetryTimeout) {\n+          Long blockCollectionID \u003d entry.getKey();\n+          synchronized (storageMovementAttemptedResults) {\n+            if (!isExistInResult(blockCollectionID)) {\n+              blockStorageMovementNeeded.add(blockCollectionID);\n               iter.remove();\n+              LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n+                  + \"retries queue for next iteration.\", blockCollectionID);\n+            } else {\n+              LOG.info(\"Blocks storage movement results for the\"\n+                  + \" tracking id : \" + blockCollectionID\n+                  + \" is reported from one of the co-ordinating datanode.\"\n+                  + \" So, the result will be processed soon.\");\n             }\n           }\n         }\n-\n       }\n-    }\n\\ No newline at end of file\n+\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cEntry\u003cLong, ItemInfo\u003e\u003e iter \u003d storageMovementAttemptedItems\n          .entrySet().iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        Entry\u003cLong, ItemInfo\u003e entry \u003d iter.next();\n        ItemInfo itemInfo \u003d entry.getValue();\n        if (now \u003e itemInfo.getLastAttemptedTimeStamp() + selfRetryTimeout) {\n          Long blockCollectionID \u003d entry.getKey();\n          synchronized (storageMovementAttemptedResults) {\n            if (!isExistInResult(blockCollectionID)) {\n              blockStorageMovementNeeded.add(blockCollectionID);\n              iter.remove();\n              LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                  + \"retries queue for next iteration.\", blockCollectionID);\n            } else {\n              LOG.info(\"Blocks storage movement results for the\"\n                  + \" tracking id : \" + blockCollectionID\n                  + \" is reported from one of the co-ordinating datanode.\"\n                  + \" So, the result will be processed soon.\");\n            }\n          }\n        }\n      }\n\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementAttemptedItems.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-11248: [SPS]: Handle partial block location movements. Contributed by Rakesh R\n",
          "commitDate": "12/08/18 3:05 AM",
          "commitName": "b7bed9f00a25bcad6f9c3543f5a1fb0a1f23b0e9",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "12/08/18 3:05 AM",
          "commitNameOld": "5179d99b7e1faeac1ce041967480115913d9f795",
          "commitAuthorOld": "Rakesh Radhakrishnan",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,28 @@\n-    private void blocksStorageMovementUnReportedItemsCheck() {\n-      synchronized (storageMovementAttemptedItems) {\n-        Iterator\u003cEntry\u003cLong, Long\u003e\u003e iter \u003d\n-            storageMovementAttemptedItems.entrySet().iterator();\n-        long now \u003d monotonicNow();\n-        while (iter.hasNext()) {\n-          Entry\u003cLong, Long\u003e entry \u003d iter.next();\n-          if (now \u003e entry.getValue() + selfRetryTimeout) {\n-            Long blockCollectionID \u003d entry.getKey();\n-            synchronized (storageMovementAttemptedResults) {\n-              boolean exist \u003d isExistInResult(blockCollectionID);\n-              if (!exist) {\n-                blockStorageMovementNeeded.add(blockCollectionID);\n-              } else {\n-                LOG.info(\"Blocks storage movement results for the\"\n-                    + \" tracking id : \" + blockCollectionID\n-                    + \" is reported from one of the co-ordinating datanode.\"\n-                    + \" So, the result will be processed soon.\");\n-              }\n+  void blocksStorageMovementUnReportedItemsCheck() {\n+    synchronized (storageMovementAttemptedItems) {\n+      Iterator\u003cEntry\u003cLong, ItemInfo\u003e\u003e iter \u003d storageMovementAttemptedItems\n+          .entrySet().iterator();\n+      long now \u003d monotonicNow();\n+      while (iter.hasNext()) {\n+        Entry\u003cLong, ItemInfo\u003e entry \u003d iter.next();\n+        ItemInfo itemInfo \u003d entry.getValue();\n+        if (now \u003e itemInfo.getLastAttemptedTimeStamp() + selfRetryTimeout) {\n+          Long blockCollectionID \u003d entry.getKey();\n+          synchronized (storageMovementAttemptedResults) {\n+            if (!isExistInResult(blockCollectionID)) {\n+              blockStorageMovementNeeded.add(blockCollectionID);\n               iter.remove();\n+              LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n+                  + \"retries queue for next iteration.\", blockCollectionID);\n+            } else {\n+              LOG.info(\"Blocks storage movement results for the\"\n+                  + \" tracking id : \" + blockCollectionID\n+                  + \" is reported from one of the co-ordinating datanode.\"\n+                  + \" So, the result will be processed soon.\");\n             }\n           }\n         }\n-\n       }\n-    }\n\\ No newline at end of file\n+\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  void blocksStorageMovementUnReportedItemsCheck() {\n    synchronized (storageMovementAttemptedItems) {\n      Iterator\u003cEntry\u003cLong, ItemInfo\u003e\u003e iter \u003d storageMovementAttemptedItems\n          .entrySet().iterator();\n      long now \u003d monotonicNow();\n      while (iter.hasNext()) {\n        Entry\u003cLong, ItemInfo\u003e entry \u003d iter.next();\n        ItemInfo itemInfo \u003d entry.getValue();\n        if (now \u003e itemInfo.getLastAttemptedTimeStamp() + selfRetryTimeout) {\n          Long blockCollectionID \u003d entry.getKey();\n          synchronized (storageMovementAttemptedResults) {\n            if (!isExistInResult(blockCollectionID)) {\n              blockStorageMovementNeeded.add(blockCollectionID);\n              iter.remove();\n              LOG.info(\"TrackID: {} becomes timed out and moved to needed \"\n                  + \"retries queue for next iteration.\", blockCollectionID);\n            } else {\n              LOG.info(\"Blocks storage movement results for the\"\n                  + \" tracking id : \" + blockCollectionID\n                  + \" is reported from one of the co-ordinating datanode.\"\n                  + \" So, the result will be processed soon.\");\n            }\n          }\n        }\n      }\n\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementAttemptedItems.java",
          "extendedDetails": {}
        }
      ]
    },
    "047526b4c27909f78313e1ed6216de85c6137f14": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11029. [SPS]:Provide retry mechanism for the blocks which were failed while moving its storage at DNs. Contributed by Uma Maheswara Rao G\n",
      "commitDate": "12/08/18 3:05 AM",
      "commitName": "047526b4c27909f78313e1ed6216de85c6137f14",
      "commitAuthor": "Rakesh Radhakrishnan",
      "diff": "@@ -0,0 +1,26 @@\n+    private void blocksStorageMovementUnReportedItemsCheck() {\n+      synchronized (storageMovementAttemptedItems) {\n+        Iterator\u003cEntry\u003cLong, Long\u003e\u003e iter \u003d\n+            storageMovementAttemptedItems.entrySet().iterator();\n+        long now \u003d monotonicNow();\n+        while (iter.hasNext()) {\n+          Entry\u003cLong, Long\u003e entry \u003d iter.next();\n+          if (now \u003e entry.getValue() + selfRetryTimeout) {\n+            Long blockCollectionID \u003d entry.getKey();\n+            synchronized (storageMovementAttemptedResults) {\n+              boolean exist \u003d isExistInResult(blockCollectionID);\n+              if (!exist) {\n+                blockStorageMovementNeeded.add(blockCollectionID);\n+              } else {\n+                LOG.info(\"Blocks storage movement results for the\"\n+                    + \" tracking id : \" + blockCollectionID\n+                    + \" is reported from one of the co-ordinating datanode.\"\n+                    + \" So, the result will be processed soon.\");\n+              }\n+              iter.remove();\n+            }\n+          }\n+        }\n+\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void blocksStorageMovementUnReportedItemsCheck() {\n      synchronized (storageMovementAttemptedItems) {\n        Iterator\u003cEntry\u003cLong, Long\u003e\u003e iter \u003d\n            storageMovementAttemptedItems.entrySet().iterator();\n        long now \u003d monotonicNow();\n        while (iter.hasNext()) {\n          Entry\u003cLong, Long\u003e entry \u003d iter.next();\n          if (now \u003e entry.getValue() + selfRetryTimeout) {\n            Long blockCollectionID \u003d entry.getKey();\n            synchronized (storageMovementAttemptedResults) {\n              boolean exist \u003d isExistInResult(blockCollectionID);\n              if (!exist) {\n                blockStorageMovementNeeded.add(blockCollectionID);\n              } else {\n                LOG.info(\"Blocks storage movement results for the\"\n                    + \" tracking id : \" + blockCollectionID\n                    + \" is reported from one of the co-ordinating datanode.\"\n                    + \" So, the result will be processed soon.\");\n              }\n              iter.remove();\n            }\n          }\n        }\n\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementAttemptedItems.java"
    }
  }
}