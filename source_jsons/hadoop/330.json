{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSStripedInputStream.java",
  "functionName": "createBlockReader",
  "functionId": "createBlockReader___block-LocatedBlock__offsetInBlock-long__targetBlocks-LocatedBlock[]__readerInfos-BlockReaderInfo[]__chunkIndex-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
  "functionStartLine": 241,
  "functionEndLine": 292,
  "numCommitsSeen": 39,
  "timeTaken": 1870,
  "changeHistory": [
    "b3119b9ab60a19d624db476c4e1c53410870c7a6",
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11",
    "734d54c1a8950446e68098f62d8964e02ecc2890"
  ],
  "changeHistoryShort": {
    "b3119b9ab60a19d624db476c4e1c53410870c7a6": "Ybodychange",
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11": "Ybodychange",
    "734d54c1a8950446e68098f62d8964e02ecc2890": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "b3119b9ab60a19d624db476c4e1c53410870c7a6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14648. Implement DeadNodeDetector basic model. Contributed by Lisheng Sun.\n",
      "commitDate": "15/11/19 7:32 PM",
      "commitName": "b3119b9ab60a19d624db476c4e1c53410870c7a6",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "25/10/19 1:09 PM",
      "commitNameOld": "30db895b59d250788d029cb2013bb4712ef9b546",
      "commitAuthorOld": "zhaoyim",
      "daysBetweenCommits": 21.31,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,52 @@\n   boolean createBlockReader(LocatedBlock block, long offsetInBlock,\n       LocatedBlock[] targetBlocks, BlockReaderInfo[] readerInfos,\n       int chunkIndex) throws IOException {\n     BlockReader reader \u003d null;\n     final ReaderRetryPolicy retry \u003d new ReaderRetryPolicy();\n     DFSInputStream.DNAddrPair dnInfo \u003d\n         new DFSInputStream.DNAddrPair(null, null, null, null);\n \n     while (true) {\n       try {\n         // the cached block location might have been re-fetched, so always\n         // get it from cache.\n         block \u003d refreshLocatedBlock(block);\n         targetBlocks[chunkIndex] \u003d block;\n \n         // internal block has one location, just rule out the deadNodes\n         dnInfo \u003d getBestNodeDNAddrPair(block, null);\n         if (dnInfo \u003d\u003d null) {\n           break;\n         }\n         reader \u003d getBlockReader(block, offsetInBlock,\n             block.getBlockSize() - offsetInBlock,\n             dnInfo.addr, dnInfo.storageType, dnInfo.info);\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026\n             retry.shouldRefetchEncryptionKey()) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \" + dnInfo.addr\n               + \" : \" + e);\n           dfsClient.clearDataEncryptionKey();\n           retry.refetchEncryptionKey();\n         } else if (retry.shouldRefetchToken() \u0026\u0026\n             tokenRefetchNeeded(e, dnInfo.addr)) {\n           fetchBlockAt(block.getStartOffset());\n           retry.refetchToken();\n         } else {\n           //TODO: handles connection issues\n           DFSClient.LOG.warn(\"Failed to connect to \" + dnInfo.addr + \" for \" +\n               \"block\" + block.getBlock(), e);\n           // re-fetch the block in case the block has been moved\n           fetchBlockAt(block.getStartOffset());\n-          addToDeadNodes(dnInfo.info);\n+          addToLocalDeadNodes(dnInfo.info);\n         }\n       }\n       if (reader !\u003d null) {\n         readerInfos[chunkIndex] \u003d\n             new BlockReaderInfo(reader, dnInfo.info, offsetInBlock);\n         return true;\n       }\n     }\n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean createBlockReader(LocatedBlock block, long offsetInBlock,\n      LocatedBlock[] targetBlocks, BlockReaderInfo[] readerInfos,\n      int chunkIndex) throws IOException {\n    BlockReader reader \u003d null;\n    final ReaderRetryPolicy retry \u003d new ReaderRetryPolicy();\n    DFSInputStream.DNAddrPair dnInfo \u003d\n        new DFSInputStream.DNAddrPair(null, null, null, null);\n\n    while (true) {\n      try {\n        // the cached block location might have been re-fetched, so always\n        // get it from cache.\n        block \u003d refreshLocatedBlock(block);\n        targetBlocks[chunkIndex] \u003d block;\n\n        // internal block has one location, just rule out the deadNodes\n        dnInfo \u003d getBestNodeDNAddrPair(block, null);\n        if (dnInfo \u003d\u003d null) {\n          break;\n        }\n        reader \u003d getBlockReader(block, offsetInBlock,\n            block.getBlockSize() - offsetInBlock,\n            dnInfo.addr, dnInfo.storageType, dnInfo.info);\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026\n            retry.shouldRefetchEncryptionKey()) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \" + dnInfo.addr\n              + \" : \" + e);\n          dfsClient.clearDataEncryptionKey();\n          retry.refetchEncryptionKey();\n        } else if (retry.shouldRefetchToken() \u0026\u0026\n            tokenRefetchNeeded(e, dnInfo.addr)) {\n          fetchBlockAt(block.getStartOffset());\n          retry.refetchToken();\n        } else {\n          //TODO: handles connection issues\n          DFSClient.LOG.warn(\"Failed to connect to \" + dnInfo.addr + \" for \" +\n              \"block\" + block.getBlock(), e);\n          // re-fetch the block in case the block has been moved\n          fetchBlockAt(block.getStartOffset());\n          addToLocalDeadNodes(dnInfo.info);\n        }\n      }\n      if (reader !\u003d null) {\n        readerInfos[chunkIndex] \u003d\n            new BlockReaderInfo(reader, dnInfo.info, offsetInBlock);\n        return true;\n      }\n    }\n    return false;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
      "extendedDetails": {}
    },
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11708. Positional read will fail if replicas moved to different DNs after stream is opened. Contributed by Vinayakumar B.\n",
      "commitDate": "06/06/17 10:25 PM",
      "commitName": "70fc6746b326b9a913e8bebca5f5afaf01ab9e11",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "03/04/17 8:13 PM",
      "commitNameOld": "6eba79232f36b36e0196163adc8fe4219a6b6bf9",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 64.09,
      "commitsBetweenForRepo": 349,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,52 @@\n   boolean createBlockReader(LocatedBlock block, long offsetInBlock,\n       LocatedBlock[] targetBlocks, BlockReaderInfo[] readerInfos,\n       int chunkIndex) throws IOException {\n     BlockReader reader \u003d null;\n     final ReaderRetryPolicy retry \u003d new ReaderRetryPolicy();\n     DFSInputStream.DNAddrPair dnInfo \u003d\n-        new DFSInputStream.DNAddrPair(null, null, null);\n+        new DFSInputStream.DNAddrPair(null, null, null, null);\n \n     while (true) {\n       try {\n         // the cached block location might have been re-fetched, so always\n         // get it from cache.\n         block \u003d refreshLocatedBlock(block);\n         targetBlocks[chunkIndex] \u003d block;\n \n         // internal block has one location, just rule out the deadNodes\n         dnInfo \u003d getBestNodeDNAddrPair(block, null);\n         if (dnInfo \u003d\u003d null) {\n           break;\n         }\n         reader \u003d getBlockReader(block, offsetInBlock,\n             block.getBlockSize() - offsetInBlock,\n             dnInfo.addr, dnInfo.storageType, dnInfo.info);\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026\n             retry.shouldRefetchEncryptionKey()) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \" + dnInfo.addr\n               + \" : \" + e);\n           dfsClient.clearDataEncryptionKey();\n           retry.refetchEncryptionKey();\n         } else if (retry.shouldRefetchToken() \u0026\u0026\n             tokenRefetchNeeded(e, dnInfo.addr)) {\n           fetchBlockAt(block.getStartOffset());\n           retry.refetchToken();\n         } else {\n           //TODO: handles connection issues\n           DFSClient.LOG.warn(\"Failed to connect to \" + dnInfo.addr + \" for \" +\n               \"block\" + block.getBlock(), e);\n           // re-fetch the block in case the block has been moved\n           fetchBlockAt(block.getStartOffset());\n           addToDeadNodes(dnInfo.info);\n         }\n       }\n       if (reader !\u003d null) {\n         readerInfos[chunkIndex] \u003d\n             new BlockReaderInfo(reader, dnInfo.info, offsetInBlock);\n         return true;\n       }\n     }\n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean createBlockReader(LocatedBlock block, long offsetInBlock,\n      LocatedBlock[] targetBlocks, BlockReaderInfo[] readerInfos,\n      int chunkIndex) throws IOException {\n    BlockReader reader \u003d null;\n    final ReaderRetryPolicy retry \u003d new ReaderRetryPolicy();\n    DFSInputStream.DNAddrPair dnInfo \u003d\n        new DFSInputStream.DNAddrPair(null, null, null, null);\n\n    while (true) {\n      try {\n        // the cached block location might have been re-fetched, so always\n        // get it from cache.\n        block \u003d refreshLocatedBlock(block);\n        targetBlocks[chunkIndex] \u003d block;\n\n        // internal block has one location, just rule out the deadNodes\n        dnInfo \u003d getBestNodeDNAddrPair(block, null);\n        if (dnInfo \u003d\u003d null) {\n          break;\n        }\n        reader \u003d getBlockReader(block, offsetInBlock,\n            block.getBlockSize() - offsetInBlock,\n            dnInfo.addr, dnInfo.storageType, dnInfo.info);\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026\n            retry.shouldRefetchEncryptionKey()) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \" + dnInfo.addr\n              + \" : \" + e);\n          dfsClient.clearDataEncryptionKey();\n          retry.refetchEncryptionKey();\n        } else if (retry.shouldRefetchToken() \u0026\u0026\n            tokenRefetchNeeded(e, dnInfo.addr)) {\n          fetchBlockAt(block.getStartOffset());\n          retry.refetchToken();\n        } else {\n          //TODO: handles connection issues\n          DFSClient.LOG.warn(\"Failed to connect to \" + dnInfo.addr + \" for \" +\n              \"block\" + block.getBlock(), e);\n          // re-fetch the block in case the block has been moved\n          fetchBlockAt(block.getStartOffset());\n          addToDeadNodes(dnInfo.info);\n        }\n      }\n      if (reader !\u003d null) {\n        readerInfos[chunkIndex] \u003d\n            new BlockReaderInfo(reader, dnInfo.info, offsetInBlock);\n        return true;\n      }\n    }\n    return false;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
      "extendedDetails": {}
    },
    "734d54c1a8950446e68098f62d8964e02ecc2890": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
      "commitDate": "21/09/16 6:34 AM",
      "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "08/09/16 11:54 AM",
          "commitNameOld": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 12.78,
          "commitsBetweenForRepo": 59,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,52 @@\n-    boolean createBlockReader(LocatedBlock block, int chunkIndex)\n-        throws IOException {\n-      BlockReader reader \u003d null;\n-      final ReaderRetryPolicy retry \u003d new ReaderRetryPolicy();\n-      DNAddrPair dnInfo \u003d new DNAddrPair(null, null, null);\n+  boolean createBlockReader(LocatedBlock block, long offsetInBlock,\n+      LocatedBlock[] targetBlocks, BlockReaderInfo[] readerInfos,\n+      int chunkIndex) throws IOException {\n+    BlockReader reader \u003d null;\n+    final ReaderRetryPolicy retry \u003d new ReaderRetryPolicy();\n+    DFSInputStream.DNAddrPair dnInfo \u003d\n+        new DFSInputStream.DNAddrPair(null, null, null);\n \n-      while(true) {\n-        try {\n-          // the cached block location might have been re-fetched, so always\n-          // get it from cache.\n-          block \u003d refreshLocatedBlock(block);\n-          targetBlocks[chunkIndex] \u003d block;\n+    while (true) {\n+      try {\n+        // the cached block location might have been re-fetched, so always\n+        // get it from cache.\n+        block \u003d refreshLocatedBlock(block);\n+        targetBlocks[chunkIndex] \u003d block;\n \n-          // internal block has one location, just rule out the deadNodes\n-          dnInfo \u003d getBestNodeDNAddrPair(block, null);\n-          if (dnInfo \u003d\u003d null) {\n-            break;\n-          }\n-          reader \u003d getBlockReader(block, alignedStripe.getOffsetInBlock(),\n-              block.getBlockSize() - alignedStripe.getOffsetInBlock(),\n-              dnInfo.addr, dnInfo.storageType, dnInfo.info);\n-        } catch (IOException e) {\n-          if (e instanceof InvalidEncryptionKeyException \u0026\u0026\n-              retry.shouldRefetchEncryptionKey()) {\n-            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n-                + \"encryption key was invalid when connecting to \" + dnInfo.addr\n-                + \" : \" + e);\n-            dfsClient.clearDataEncryptionKey();\n-            retry.refetchEncryptionKey();\n-          } else if (retry.shouldRefetchToken() \u0026\u0026\n-              tokenRefetchNeeded(e, dnInfo.addr)) {\n-            fetchBlockAt(block.getStartOffset());\n-            retry.refetchToken();\n-          } else {\n-            //TODO: handles connection issues\n-            DFSClient.LOG.warn(\"Failed to connect to \" + dnInfo.addr + \" for \" +\n-                \"block\" + block.getBlock(), e);\n-            // re-fetch the block in case the block has been moved\n-            fetchBlockAt(block.getStartOffset());\n-            addToDeadNodes(dnInfo.info);\n-          }\n+        // internal block has one location, just rule out the deadNodes\n+        dnInfo \u003d getBestNodeDNAddrPair(block, null);\n+        if (dnInfo \u003d\u003d null) {\n+          break;\n         }\n-        if (reader !\u003d null) {\n-          readerInfos[chunkIndex] \u003d new BlockReaderInfo(reader, dnInfo.info,\n-              alignedStripe.getOffsetInBlock());\n-          return true;\n+        reader \u003d getBlockReader(block, offsetInBlock,\n+            block.getBlockSize() - offsetInBlock,\n+            dnInfo.addr, dnInfo.storageType, dnInfo.info);\n+      } catch (IOException e) {\n+        if (e instanceof InvalidEncryptionKeyException \u0026\u0026\n+            retry.shouldRefetchEncryptionKey()) {\n+          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n+              + \"encryption key was invalid when connecting to \" + dnInfo.addr\n+              + \" : \" + e);\n+          dfsClient.clearDataEncryptionKey();\n+          retry.refetchEncryptionKey();\n+        } else if (retry.shouldRefetchToken() \u0026\u0026\n+            tokenRefetchNeeded(e, dnInfo.addr)) {\n+          fetchBlockAt(block.getStartOffset());\n+          retry.refetchToken();\n+        } else {\n+          //TODO: handles connection issues\n+          DFSClient.LOG.warn(\"Failed to connect to \" + dnInfo.addr + \" for \" +\n+              \"block\" + block.getBlock(), e);\n+          // re-fetch the block in case the block has been moved\n+          fetchBlockAt(block.getStartOffset());\n+          addToDeadNodes(dnInfo.info);\n         }\n       }\n-      return false;\n-    }\n\\ No newline at end of file\n+      if (reader !\u003d null) {\n+        readerInfos[chunkIndex] \u003d\n+            new BlockReaderInfo(reader, dnInfo.info, offsetInBlock);\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  boolean createBlockReader(LocatedBlock block, long offsetInBlock,\n      LocatedBlock[] targetBlocks, BlockReaderInfo[] readerInfos,\n      int chunkIndex) throws IOException {\n    BlockReader reader \u003d null;\n    final ReaderRetryPolicy retry \u003d new ReaderRetryPolicy();\n    DFSInputStream.DNAddrPair dnInfo \u003d\n        new DFSInputStream.DNAddrPair(null, null, null);\n\n    while (true) {\n      try {\n        // the cached block location might have been re-fetched, so always\n        // get it from cache.\n        block \u003d refreshLocatedBlock(block);\n        targetBlocks[chunkIndex] \u003d block;\n\n        // internal block has one location, just rule out the deadNodes\n        dnInfo \u003d getBestNodeDNAddrPair(block, null);\n        if (dnInfo \u003d\u003d null) {\n          break;\n        }\n        reader \u003d getBlockReader(block, offsetInBlock,\n            block.getBlockSize() - offsetInBlock,\n            dnInfo.addr, dnInfo.storageType, dnInfo.info);\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026\n            retry.shouldRefetchEncryptionKey()) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \" + dnInfo.addr\n              + \" : \" + e);\n          dfsClient.clearDataEncryptionKey();\n          retry.refetchEncryptionKey();\n        } else if (retry.shouldRefetchToken() \u0026\u0026\n            tokenRefetchNeeded(e, dnInfo.addr)) {\n          fetchBlockAt(block.getStartOffset());\n          retry.refetchToken();\n        } else {\n          //TODO: handles connection issues\n          DFSClient.LOG.warn(\"Failed to connect to \" + dnInfo.addr + \" for \" +\n              \"block\" + block.getBlock(), e);\n          // re-fetch the block in case the block has been moved\n          fetchBlockAt(block.getStartOffset());\n          addToDeadNodes(dnInfo.info);\n        }\n      }\n      if (reader !\u003d null) {\n        readerInfos[chunkIndex] \u003d\n            new BlockReaderInfo(reader, dnInfo.info, offsetInBlock);\n        return true;\n      }\n    }\n    return false;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
          "extendedDetails": {
            "oldValue": "[block-LocatedBlock, chunkIndex-int]",
            "newValue": "[block-LocatedBlock, offsetInBlock-long, targetBlocks-LocatedBlock[], readerInfos-BlockReaderInfo[], chunkIndex-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "08/09/16 11:54 AM",
          "commitNameOld": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 12.78,
          "commitsBetweenForRepo": 59,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,52 @@\n-    boolean createBlockReader(LocatedBlock block, int chunkIndex)\n-        throws IOException {\n-      BlockReader reader \u003d null;\n-      final ReaderRetryPolicy retry \u003d new ReaderRetryPolicy();\n-      DNAddrPair dnInfo \u003d new DNAddrPair(null, null, null);\n+  boolean createBlockReader(LocatedBlock block, long offsetInBlock,\n+      LocatedBlock[] targetBlocks, BlockReaderInfo[] readerInfos,\n+      int chunkIndex) throws IOException {\n+    BlockReader reader \u003d null;\n+    final ReaderRetryPolicy retry \u003d new ReaderRetryPolicy();\n+    DFSInputStream.DNAddrPair dnInfo \u003d\n+        new DFSInputStream.DNAddrPair(null, null, null);\n \n-      while(true) {\n-        try {\n-          // the cached block location might have been re-fetched, so always\n-          // get it from cache.\n-          block \u003d refreshLocatedBlock(block);\n-          targetBlocks[chunkIndex] \u003d block;\n+    while (true) {\n+      try {\n+        // the cached block location might have been re-fetched, so always\n+        // get it from cache.\n+        block \u003d refreshLocatedBlock(block);\n+        targetBlocks[chunkIndex] \u003d block;\n \n-          // internal block has one location, just rule out the deadNodes\n-          dnInfo \u003d getBestNodeDNAddrPair(block, null);\n-          if (dnInfo \u003d\u003d null) {\n-            break;\n-          }\n-          reader \u003d getBlockReader(block, alignedStripe.getOffsetInBlock(),\n-              block.getBlockSize() - alignedStripe.getOffsetInBlock(),\n-              dnInfo.addr, dnInfo.storageType, dnInfo.info);\n-        } catch (IOException e) {\n-          if (e instanceof InvalidEncryptionKeyException \u0026\u0026\n-              retry.shouldRefetchEncryptionKey()) {\n-            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n-                + \"encryption key was invalid when connecting to \" + dnInfo.addr\n-                + \" : \" + e);\n-            dfsClient.clearDataEncryptionKey();\n-            retry.refetchEncryptionKey();\n-          } else if (retry.shouldRefetchToken() \u0026\u0026\n-              tokenRefetchNeeded(e, dnInfo.addr)) {\n-            fetchBlockAt(block.getStartOffset());\n-            retry.refetchToken();\n-          } else {\n-            //TODO: handles connection issues\n-            DFSClient.LOG.warn(\"Failed to connect to \" + dnInfo.addr + \" for \" +\n-                \"block\" + block.getBlock(), e);\n-            // re-fetch the block in case the block has been moved\n-            fetchBlockAt(block.getStartOffset());\n-            addToDeadNodes(dnInfo.info);\n-          }\n+        // internal block has one location, just rule out the deadNodes\n+        dnInfo \u003d getBestNodeDNAddrPair(block, null);\n+        if (dnInfo \u003d\u003d null) {\n+          break;\n         }\n-        if (reader !\u003d null) {\n-          readerInfos[chunkIndex] \u003d new BlockReaderInfo(reader, dnInfo.info,\n-              alignedStripe.getOffsetInBlock());\n-          return true;\n+        reader \u003d getBlockReader(block, offsetInBlock,\n+            block.getBlockSize() - offsetInBlock,\n+            dnInfo.addr, dnInfo.storageType, dnInfo.info);\n+      } catch (IOException e) {\n+        if (e instanceof InvalidEncryptionKeyException \u0026\u0026\n+            retry.shouldRefetchEncryptionKey()) {\n+          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n+              + \"encryption key was invalid when connecting to \" + dnInfo.addr\n+              + \" : \" + e);\n+          dfsClient.clearDataEncryptionKey();\n+          retry.refetchEncryptionKey();\n+        } else if (retry.shouldRefetchToken() \u0026\u0026\n+            tokenRefetchNeeded(e, dnInfo.addr)) {\n+          fetchBlockAt(block.getStartOffset());\n+          retry.refetchToken();\n+        } else {\n+          //TODO: handles connection issues\n+          DFSClient.LOG.warn(\"Failed to connect to \" + dnInfo.addr + \" for \" +\n+              \"block\" + block.getBlock(), e);\n+          // re-fetch the block in case the block has been moved\n+          fetchBlockAt(block.getStartOffset());\n+          addToDeadNodes(dnInfo.info);\n         }\n       }\n-      return false;\n-    }\n\\ No newline at end of file\n+      if (reader !\u003d null) {\n+        readerInfos[chunkIndex] \u003d\n+            new BlockReaderInfo(reader, dnInfo.info, offsetInBlock);\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  boolean createBlockReader(LocatedBlock block, long offsetInBlock,\n      LocatedBlock[] targetBlocks, BlockReaderInfo[] readerInfos,\n      int chunkIndex) throws IOException {\n    BlockReader reader \u003d null;\n    final ReaderRetryPolicy retry \u003d new ReaderRetryPolicy();\n    DFSInputStream.DNAddrPair dnInfo \u003d\n        new DFSInputStream.DNAddrPair(null, null, null);\n\n    while (true) {\n      try {\n        // the cached block location might have been re-fetched, so always\n        // get it from cache.\n        block \u003d refreshLocatedBlock(block);\n        targetBlocks[chunkIndex] \u003d block;\n\n        // internal block has one location, just rule out the deadNodes\n        dnInfo \u003d getBestNodeDNAddrPair(block, null);\n        if (dnInfo \u003d\u003d null) {\n          break;\n        }\n        reader \u003d getBlockReader(block, offsetInBlock,\n            block.getBlockSize() - offsetInBlock,\n            dnInfo.addr, dnInfo.storageType, dnInfo.info);\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026\n            retry.shouldRefetchEncryptionKey()) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \" + dnInfo.addr\n              + \" : \" + e);\n          dfsClient.clearDataEncryptionKey();\n          retry.refetchEncryptionKey();\n        } else if (retry.shouldRefetchToken() \u0026\u0026\n            tokenRefetchNeeded(e, dnInfo.addr)) {\n          fetchBlockAt(block.getStartOffset());\n          retry.refetchToken();\n        } else {\n          //TODO: handles connection issues\n          DFSClient.LOG.warn(\"Failed to connect to \" + dnInfo.addr + \" for \" +\n              \"block\" + block.getBlock(), e);\n          // re-fetch the block in case the block has been moved\n          fetchBlockAt(block.getStartOffset());\n          addToDeadNodes(dnInfo.info);\n        }\n      }\n      if (reader !\u003d null) {\n        readerInfos[chunkIndex] \u003d\n            new BlockReaderInfo(reader, dnInfo.info, offsetInBlock);\n        return true;\n      }\n    }\n    return false;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}