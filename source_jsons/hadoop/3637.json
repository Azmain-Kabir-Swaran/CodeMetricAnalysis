{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "tryReadZeroCopy",
  "functionId": "tryReadZeroCopy___maxLength-int__opts-EnumSet__ReadOption__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 1848,
  "functionEndLine": 1921,
  "numCommitsSeen": 174,
  "timeTaken": 5566,
  "changeHistory": [
    "793447f79924c97c2b562d5e41fa85adf19673fe",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "871cb56152e6039ff56c6fabfcd45451029471c3",
    "45ea53f9388e6bff1ac0aa3989a1dad56a611fd3",
    "f730fa919e8a2a820f4fefcd882628bbd4f2d9ab",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
    "124e507674c0d396f8494585e64226957199097b",
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036"
  ],
  "changeHistoryShort": {
    "793447f79924c97c2b562d5e41fa85adf19673fe": "Ybodychange",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "871cb56152e6039ff56c6fabfcd45451029471c3": "Ybodychange",
    "45ea53f9388e6bff1ac0aa3989a1dad56a611fd3": "Ybodychange",
    "f730fa919e8a2a820f4fefcd882628bbd4f2d9ab": "Ybodychange",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": "Ybodychange",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": "Ybodychange",
    "124e507674c0d396f8494585e64226957199097b": "Ymultichange(Yparameterchange,Ybodychange)",
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": "Yintroduced"
  },
  "changeHistoryDetails": {
    "793447f79924c97c2b562d5e41fa85adf19673fe": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8905. Refactor DFSInputStream#ReaderStrategy. Contributed by Kai Zheng and Sammi Chen\n",
      "commitDate": "24/08/16 6:57 AM",
      "commitName": "793447f79924c97c2b562d5e41fa85adf19673fe",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "08/06/16 10:52 PM",
      "commitNameOld": "8ea9bbce2614e8eb499af73589f021ed1789e78f",
      "commitAuthorOld": "Masatake Iwasaki",
      "daysBetweenCommits": 76.34,
      "commitsBetweenForRepo": 637,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,74 @@\n   private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n       EnumSet\u003cReadOption\u003e opts) throws IOException {\n     // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n     // JVM to optimize this function.\n     final long curPos \u003d pos;\n     final long curEnd \u003d blockEnd;\n     final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n     final long blockPos \u003d curPos - blockStartInFile;\n \n     // Shorten this read if the end of the block is nearby.\n     long length63;\n     if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n       length63 \u003d maxLength;\n     } else {\n       length63 \u003d 1 + curEnd - curPos;\n       if (length63 \u003c\u003d 0) {\n         DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {}\"\n                 + \" of {}; {} bytes left in block. blockPos\u003d{}; curPos\u003d{};\"\n                 + \"curEnd\u003d{}\",\n             curPos, src, length63, blockPos, curPos, curEnd);\n         return null;\n       }\n       DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid going \"\n               + \"more than one byte past the end of the block.  blockPos\u003d{}; \"\n               +\" curPos\u003d{}; curEnd\u003d{}\",\n           maxLength, length63, blockPos, curPos, curEnd);\n     }\n     // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n     int length;\n     if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n       length \u003d (int)length63;\n     } else {\n       long length31 \u003d Integer.MAX_VALUE - blockPos;\n       if (length31 \u003c\u003d 0) {\n         // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n         // 4-byte signed integers to represent capacity, etc.\n         // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n         // FIXME: we could work around this with multiple memory maps.\n         // See HDFS-5101.\n         DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {} \"\n             + \" of {}; 31-bit MappedByteBuffer limit exceeded.  blockPos\u003d{}, \"\n             + \"curEnd\u003d{}\", curPos, src, blockPos, curEnd);\n         return null;\n       }\n       length \u003d (int)length31;\n       DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid 31-bit \"\n           + \"limit.  blockPos\u003d{}; curPos\u003d{}; curEnd\u003d{}\",\n           maxLength, length, blockPos, curPos, curEnd);\n     }\n     final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n     if (clientMmap \u003d\u003d null) {\n       DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset {} of\"\n           + \" {}; BlockReader#getClientMmap returned null.\", curPos, src);\n       return null;\n     }\n     boolean success \u003d false;\n     ByteBuffer buffer;\n     try {\n       seek(curPos + length);\n       buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n       buffer.position((int)blockPos);\n       buffer.limit((int)(blockPos + length));\n       getExtendedReadBuffers().put(buffer, clientMmap);\n-      synchronized (infoLock) {\n-        readStatistics.addZeroCopyBytes(length);\n-      }\n+      readStatistics.addZeroCopyBytes(length);\n       DFSClient.LOG.debug(\"readZeroCopy read {} bytes from offset {} via the \"\n           + \"zero-copy read path.  blockEnd \u003d {}\", length, curPos, blockEnd);\n       success \u003d true;\n     } finally {\n       if (!success) {\n         IOUtils.closeQuietly(clientMmap);\n       }\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n    // JVM to optimize this function.\n    final long curPos \u003d pos;\n    final long curEnd \u003d blockEnd;\n    final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    final long blockPos \u003d curPos - blockStartInFile;\n\n    // Shorten this read if the end of the block is nearby.\n    long length63;\n    if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n      length63 \u003d maxLength;\n    } else {\n      length63 \u003d 1 + curEnd - curPos;\n      if (length63 \u003c\u003d 0) {\n        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {}\"\n                + \" of {}; {} bytes left in block. blockPos\u003d{}; curPos\u003d{};\"\n                + \"curEnd\u003d{}\",\n            curPos, src, length63, blockPos, curPos, curEnd);\n        return null;\n      }\n      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid going \"\n              + \"more than one byte past the end of the block.  blockPos\u003d{}; \"\n              +\" curPos\u003d{}; curEnd\u003d{}\",\n          maxLength, length63, blockPos, curPos, curEnd);\n    }\n    // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n    int length;\n    if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n      length \u003d (int)length63;\n    } else {\n      long length31 \u003d Integer.MAX_VALUE - blockPos;\n      if (length31 \u003c\u003d 0) {\n        // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n        // 4-byte signed integers to represent capacity, etc.\n        // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n        // FIXME: we could work around this with multiple memory maps.\n        // See HDFS-5101.\n        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {} \"\n            + \" of {}; 31-bit MappedByteBuffer limit exceeded.  blockPos\u003d{}, \"\n            + \"curEnd\u003d{}\", curPos, src, blockPos, curEnd);\n        return null;\n      }\n      length \u003d (int)length31;\n      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid 31-bit \"\n          + \"limit.  blockPos\u003d{}; curPos\u003d{}; curEnd\u003d{}\",\n          maxLength, length, blockPos, curPos, curEnd);\n    }\n    final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n    if (clientMmap \u003d\u003d null) {\n      DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset {} of\"\n          + \" {}; BlockReader#getClientMmap returned null.\", curPos, src);\n      return null;\n    }\n    boolean success \u003d false;\n    ByteBuffer buffer;\n    try {\n      seek(curPos + length);\n      buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n      buffer.position((int)blockPos);\n      buffer.limit((int)(blockPos + length));\n      getExtendedReadBuffers().put(buffer, clientMmap);\n      readStatistics.addZeroCopyBytes(length);\n      DFSClient.LOG.debug(\"readZeroCopy read {} bytes from offset {} via the \"\n          + \"zero-copy read path.  blockEnd \u003d {}\", length, curPos, blockEnd);\n      success \u003d true;\n    } finally {\n      if (!success) {\n        IOUtils.closeQuietly(clientMmap);\n      }\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,76 @@\n   private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n       EnumSet\u003cReadOption\u003e opts) throws IOException {\n     // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n     // JVM to optimize this function.\n     final long curPos \u003d pos;\n     final long curEnd \u003d blockEnd;\n     final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n     final long blockPos \u003d curPos - blockStartInFile;\n \n     // Shorten this read if the end of the block is nearby.\n     long length63;\n     if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n       length63 \u003d maxLength;\n     } else {\n       length63 \u003d 1 + curEnd - curPos;\n       if (length63 \u003c\u003d 0) {\n-        if (DFSClient.LOG.isDebugEnabled()) {\n-          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n-            curPos + \" of \" + src + \"; \" + length63 + \" bytes left in block.  \" +\n-            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n-            \"; curEnd\u003d\" + curEnd);\n-        }\n+        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {}\"\n+                + \" of {}; {} bytes left in block. blockPos\u003d{}; curPos\u003d{};\"\n+                + \"curEnd\u003d{}\",\n+            curPos, src, length63, blockPos, curPos, curEnd);\n         return null;\n       }\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n-            \" to \" + length63 + \" to avoid going more than one byte \" +\n-            \"past the end of the block.  blockPos\u003d\" + blockPos +\n-            \"; curPos\u003d\" + curPos + \"; curEnd\u003d\" + curEnd);\n-      }\n+      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid going \"\n+              + \"more than one byte past the end of the block.  blockPos\u003d{}; \"\n+              +\" curPos\u003d{}; curEnd\u003d{}\",\n+          maxLength, length63, blockPos, curPos, curEnd);\n     }\n     // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n     int length;\n     if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n       length \u003d (int)length63;\n     } else {\n       long length31 \u003d Integer.MAX_VALUE - blockPos;\n       if (length31 \u003c\u003d 0) {\n         // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n         // 4-byte signed integers to represent capacity, etc.\n         // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n         // FIXME: we could work around this with multiple memory maps.\n         // See HDFS-5101.\n-        if (DFSClient.LOG.isDebugEnabled()) {\n-          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n-            curPos + \" of \" + src + \"; 31-bit MappedByteBuffer limit \" +\n-            \"exceeded.  blockPos\u003d\" + blockPos + \", curEnd\u003d\" + curEnd);\n-        }\n+        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {} \"\n+            + \" of {}; 31-bit MappedByteBuffer limit exceeded.  blockPos\u003d{}, \"\n+            + \"curEnd\u003d{}\", curPos, src, blockPos, curEnd);\n         return null;\n       }\n       length \u003d (int)length31;\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n-            \" to \" + length + \" to avoid 31-bit limit.  \" +\n-            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n-            \"; curEnd\u003d\" + curEnd);\n-      }\n+      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid 31-bit \"\n+          + \"limit.  blockPos\u003d{}; curPos\u003d{}; curEnd\u003d{}\",\n+          maxLength, length, blockPos, curPos, curEnd);\n     }\n     final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n     if (clientMmap \u003d\u003d null) {\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n-          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n-          \"null.\");\n-      }\n+      DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset {} of\"\n+          + \" {}; BlockReader#getClientMmap returned null.\", curPos, src);\n       return null;\n     }\n     boolean success \u003d false;\n     ByteBuffer buffer;\n     try {\n       seek(curPos + length);\n       buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n       buffer.position((int)blockPos);\n       buffer.limit((int)(blockPos + length));\n       getExtendedReadBuffers().put(buffer, clientMmap);\n       synchronized (infoLock) {\n         readStatistics.addZeroCopyBytes(length);\n       }\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"readZeroCopy read \" + length + \n-            \" bytes from offset \" + curPos + \" via the zero-copy read \" +\n-            \"path.  blockEnd \u003d \" + blockEnd);\n-      }\n+      DFSClient.LOG.debug(\"readZeroCopy read {} bytes from offset {} via the \"\n+          + \"zero-copy read path.  blockEnd \u003d {}\", length, curPos, blockEnd);\n       success \u003d true;\n     } finally {\n       if (!success) {\n         IOUtils.closeQuietly(clientMmap);\n       }\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n    // JVM to optimize this function.\n    final long curPos \u003d pos;\n    final long curEnd \u003d blockEnd;\n    final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    final long blockPos \u003d curPos - blockStartInFile;\n\n    // Shorten this read if the end of the block is nearby.\n    long length63;\n    if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n      length63 \u003d maxLength;\n    } else {\n      length63 \u003d 1 + curEnd - curPos;\n      if (length63 \u003c\u003d 0) {\n        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {}\"\n                + \" of {}; {} bytes left in block. blockPos\u003d{}; curPos\u003d{};\"\n                + \"curEnd\u003d{}\",\n            curPos, src, length63, blockPos, curPos, curEnd);\n        return null;\n      }\n      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid going \"\n              + \"more than one byte past the end of the block.  blockPos\u003d{}; \"\n              +\" curPos\u003d{}; curEnd\u003d{}\",\n          maxLength, length63, blockPos, curPos, curEnd);\n    }\n    // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n    int length;\n    if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n      length \u003d (int)length63;\n    } else {\n      long length31 \u003d Integer.MAX_VALUE - blockPos;\n      if (length31 \u003c\u003d 0) {\n        // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n        // 4-byte signed integers to represent capacity, etc.\n        // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n        // FIXME: we could work around this with multiple memory maps.\n        // See HDFS-5101.\n        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {} \"\n            + \" of {}; 31-bit MappedByteBuffer limit exceeded.  blockPos\u003d{}, \"\n            + \"curEnd\u003d{}\", curPos, src, blockPos, curEnd);\n        return null;\n      }\n      length \u003d (int)length31;\n      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid 31-bit \"\n          + \"limit.  blockPos\u003d{}; curPos\u003d{}; curEnd\u003d{}\",\n          maxLength, length, blockPos, curPos, curEnd);\n    }\n    final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n    if (clientMmap \u003d\u003d null) {\n      DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset {} of\"\n          + \" {}; BlockReader#getClientMmap returned null.\", curPos, src);\n      return null;\n    }\n    boolean success \u003d false;\n    ByteBuffer buffer;\n    try {\n      seek(curPos + length);\n      buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n      buffer.position((int)blockPos);\n      buffer.limit((int)(blockPos + length));\n      getExtendedReadBuffers().put(buffer, clientMmap);\n      synchronized (infoLock) {\n        readStatistics.addZeroCopyBytes(length);\n      }\n      DFSClient.LOG.debug(\"readZeroCopy read {} bytes from offset {} via the \"\n          + \"zero-copy read path.  blockEnd \u003d {}\", length, curPos, blockEnd);\n      success \u003d true;\n    } finally {\n      if (!success) {\n        IOUtils.closeQuietly(clientMmap);\n      }\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,91 @@\n   private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n       EnumSet\u003cReadOption\u003e opts) throws IOException {\n     // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n     // JVM to optimize this function.\n     final long curPos \u003d pos;\n     final long curEnd \u003d blockEnd;\n     final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n     final long blockPos \u003d curPos - blockStartInFile;\n \n     // Shorten this read if the end of the block is nearby.\n     long length63;\n     if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n       length63 \u003d maxLength;\n     } else {\n       length63 \u003d 1 + curEnd - curPos;\n       if (length63 \u003c\u003d 0) {\n-        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {}\"\n-                + \" of {}; {} bytes left in block. blockPos\u003d{}; curPos\u003d{};\"\n-                + \"curEnd\u003d{}\",\n-            curPos, src, length63, blockPos, curPos, curEnd);\n+        if (DFSClient.LOG.isDebugEnabled()) {\n+          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n+            curPos + \" of \" + src + \"; \" + length63 + \" bytes left in block.  \" +\n+            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n+            \"; curEnd\u003d\" + curEnd);\n+        }\n         return null;\n       }\n-      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid going \"\n-              + \"more than one byte past the end of the block.  blockPos\u003d{}; \"\n-              +\" curPos\u003d{}; curEnd\u003d{}\",\n-          maxLength, length63, blockPos, curPos, curEnd);\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n+            \" to \" + length63 + \" to avoid going more than one byte \" +\n+            \"past the end of the block.  blockPos\u003d\" + blockPos +\n+            \"; curPos\u003d\" + curPos + \"; curEnd\u003d\" + curEnd);\n+      }\n     }\n     // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n     int length;\n     if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n       length \u003d (int)length63;\n     } else {\n       long length31 \u003d Integer.MAX_VALUE - blockPos;\n       if (length31 \u003c\u003d 0) {\n         // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n         // 4-byte signed integers to represent capacity, etc.\n         // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n         // FIXME: we could work around this with multiple memory maps.\n         // See HDFS-5101.\n-        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {} \"\n-            + \" of {}; 31-bit MappedByteBuffer limit exceeded.  blockPos\u003d{}, \"\n-            + \"curEnd\u003d{}\", curPos, src, blockPos, curEnd);\n+        if (DFSClient.LOG.isDebugEnabled()) {\n+          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n+            curPos + \" of \" + src + \"; 31-bit MappedByteBuffer limit \" +\n+            \"exceeded.  blockPos\u003d\" + blockPos + \", curEnd\u003d\" + curEnd);\n+        }\n         return null;\n       }\n       length \u003d (int)length31;\n-      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid 31-bit \"\n-          + \"limit.  blockPos\u003d{}; curPos\u003d{}; curEnd\u003d{}\",\n-          maxLength, length, blockPos, curPos, curEnd);\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n+            \" to \" + length + \" to avoid 31-bit limit.  \" +\n+            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n+            \"; curEnd\u003d\" + curEnd);\n+      }\n     }\n     final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n     if (clientMmap \u003d\u003d null) {\n-      DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset {} of\"\n-          + \" {}; BlockReader#getClientMmap returned null.\", curPos, src);\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n+          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n+          \"null.\");\n+      }\n       return null;\n     }\n     boolean success \u003d false;\n     ByteBuffer buffer;\n     try {\n       seek(curPos + length);\n       buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n       buffer.position((int)blockPos);\n       buffer.limit((int)(blockPos + length));\n       getExtendedReadBuffers().put(buffer, clientMmap);\n       synchronized (infoLock) {\n         readStatistics.addZeroCopyBytes(length);\n       }\n-      DFSClient.LOG.debug(\"readZeroCopy read {} bytes from offset {} via the \"\n-          + \"zero-copy read path.  blockEnd \u003d {}\", length, curPos, blockEnd);\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"readZeroCopy read \" + length + \n+            \" bytes from offset \" + curPos + \" via the zero-copy read \" +\n+            \"path.  blockEnd \u003d \" + blockEnd);\n+      }\n       success \u003d true;\n     } finally {\n       if (!success) {\n         IOUtils.closeQuietly(clientMmap);\n       }\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n    // JVM to optimize this function.\n    final long curPos \u003d pos;\n    final long curEnd \u003d blockEnd;\n    final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    final long blockPos \u003d curPos - blockStartInFile;\n\n    // Shorten this read if the end of the block is nearby.\n    long length63;\n    if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n      length63 \u003d maxLength;\n    } else {\n      length63 \u003d 1 + curEnd - curPos;\n      if (length63 \u003c\u003d 0) {\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n            curPos + \" of \" + src + \"; \" + length63 + \" bytes left in block.  \" +\n            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n            \"; curEnd\u003d\" + curEnd);\n        }\n        return null;\n      }\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n            \" to \" + length63 + \" to avoid going more than one byte \" +\n            \"past the end of the block.  blockPos\u003d\" + blockPos +\n            \"; curPos\u003d\" + curPos + \"; curEnd\u003d\" + curEnd);\n      }\n    }\n    // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n    int length;\n    if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n      length \u003d (int)length63;\n    } else {\n      long length31 \u003d Integer.MAX_VALUE - blockPos;\n      if (length31 \u003c\u003d 0) {\n        // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n        // 4-byte signed integers to represent capacity, etc.\n        // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n        // FIXME: we could work around this with multiple memory maps.\n        // See HDFS-5101.\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n            curPos + \" of \" + src + \"; 31-bit MappedByteBuffer limit \" +\n            \"exceeded.  blockPos\u003d\" + blockPos + \", curEnd\u003d\" + curEnd);\n        }\n        return null;\n      }\n      length \u003d (int)length31;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n            \" to \" + length + \" to avoid 31-bit limit.  \" +\n            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n            \"; curEnd\u003d\" + curEnd);\n      }\n    }\n    final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n    if (clientMmap \u003d\u003d null) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n          \"null.\");\n      }\n      return null;\n    }\n    boolean success \u003d false;\n    ByteBuffer buffer;\n    try {\n      seek(curPos + length);\n      buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n      buffer.position((int)blockPos);\n      buffer.limit((int)(blockPos + length));\n      getExtendedReadBuffers().put(buffer, clientMmap);\n      synchronized (infoLock) {\n        readStatistics.addZeroCopyBytes(length);\n      }\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"readZeroCopy read \" + length + \n            \" bytes from offset \" + curPos + \" via the zero-copy read \" +\n            \"path.  blockEnd \u003d \" + blockEnd);\n      }\n      success \u003d true;\n    } finally {\n      if (!success) {\n        IOUtils.closeQuietly(clientMmap);\n      }\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,76 @@\n   private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n       EnumSet\u003cReadOption\u003e opts) throws IOException {\n     // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n     // JVM to optimize this function.\n     final long curPos \u003d pos;\n     final long curEnd \u003d blockEnd;\n     final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n     final long blockPos \u003d curPos - blockStartInFile;\n \n     // Shorten this read if the end of the block is nearby.\n     long length63;\n     if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n       length63 \u003d maxLength;\n     } else {\n       length63 \u003d 1 + curEnd - curPos;\n       if (length63 \u003c\u003d 0) {\n-        if (DFSClient.LOG.isDebugEnabled()) {\n-          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n-            curPos + \" of \" + src + \"; \" + length63 + \" bytes left in block.  \" +\n-            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n-            \"; curEnd\u003d\" + curEnd);\n-        }\n+        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {}\"\n+                + \" of {}; {} bytes left in block. blockPos\u003d{}; curPos\u003d{};\"\n+                + \"curEnd\u003d{}\",\n+            curPos, src, length63, blockPos, curPos, curEnd);\n         return null;\n       }\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n-            \" to \" + length63 + \" to avoid going more than one byte \" +\n-            \"past the end of the block.  blockPos\u003d\" + blockPos +\n-            \"; curPos\u003d\" + curPos + \"; curEnd\u003d\" + curEnd);\n-      }\n+      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid going \"\n+              + \"more than one byte past the end of the block.  blockPos\u003d{}; \"\n+              +\" curPos\u003d{}; curEnd\u003d{}\",\n+          maxLength, length63, blockPos, curPos, curEnd);\n     }\n     // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n     int length;\n     if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n       length \u003d (int)length63;\n     } else {\n       long length31 \u003d Integer.MAX_VALUE - blockPos;\n       if (length31 \u003c\u003d 0) {\n         // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n         // 4-byte signed integers to represent capacity, etc.\n         // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n         // FIXME: we could work around this with multiple memory maps.\n         // See HDFS-5101.\n-        if (DFSClient.LOG.isDebugEnabled()) {\n-          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n-            curPos + \" of \" + src + \"; 31-bit MappedByteBuffer limit \" +\n-            \"exceeded.  blockPos\u003d\" + blockPos + \", curEnd\u003d\" + curEnd);\n-        }\n+        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {} \"\n+            + \" of {}; 31-bit MappedByteBuffer limit exceeded.  blockPos\u003d{}, \"\n+            + \"curEnd\u003d{}\", curPos, src, blockPos, curEnd);\n         return null;\n       }\n       length \u003d (int)length31;\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n-            \" to \" + length + \" to avoid 31-bit limit.  \" +\n-            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n-            \"; curEnd\u003d\" + curEnd);\n-      }\n+      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid 31-bit \"\n+          + \"limit.  blockPos\u003d{}; curPos\u003d{}; curEnd\u003d{}\",\n+          maxLength, length, blockPos, curPos, curEnd);\n     }\n     final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n     if (clientMmap \u003d\u003d null) {\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n-          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n-          \"null.\");\n-      }\n+      DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset {} of\"\n+          + \" {}; BlockReader#getClientMmap returned null.\", curPos, src);\n       return null;\n     }\n     boolean success \u003d false;\n     ByteBuffer buffer;\n     try {\n       seek(curPos + length);\n       buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n       buffer.position((int)blockPos);\n       buffer.limit((int)(blockPos + length));\n       getExtendedReadBuffers().put(buffer, clientMmap);\n       synchronized (infoLock) {\n         readStatistics.addZeroCopyBytes(length);\n       }\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"readZeroCopy read \" + length + \n-            \" bytes from offset \" + curPos + \" via the zero-copy read \" +\n-            \"path.  blockEnd \u003d \" + blockEnd);\n-      }\n+      DFSClient.LOG.debug(\"readZeroCopy read {} bytes from offset {} via the \"\n+          + \"zero-copy read path.  blockEnd \u003d {}\", length, curPos, blockEnd);\n       success \u003d true;\n     } finally {\n       if (!success) {\n         IOUtils.closeQuietly(clientMmap);\n       }\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n    // JVM to optimize this function.\n    final long curPos \u003d pos;\n    final long curEnd \u003d blockEnd;\n    final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    final long blockPos \u003d curPos - blockStartInFile;\n\n    // Shorten this read if the end of the block is nearby.\n    long length63;\n    if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n      length63 \u003d maxLength;\n    } else {\n      length63 \u003d 1 + curEnd - curPos;\n      if (length63 \u003c\u003d 0) {\n        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {}\"\n                + \" of {}; {} bytes left in block. blockPos\u003d{}; curPos\u003d{};\"\n                + \"curEnd\u003d{}\",\n            curPos, src, length63, blockPos, curPos, curEnd);\n        return null;\n      }\n      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid going \"\n              + \"more than one byte past the end of the block.  blockPos\u003d{}; \"\n              +\" curPos\u003d{}; curEnd\u003d{}\",\n          maxLength, length63, blockPos, curPos, curEnd);\n    }\n    // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n    int length;\n    if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n      length \u003d (int)length63;\n    } else {\n      long length31 \u003d Integer.MAX_VALUE - blockPos;\n      if (length31 \u003c\u003d 0) {\n        // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n        // 4-byte signed integers to represent capacity, etc.\n        // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n        // FIXME: we could work around this with multiple memory maps.\n        // See HDFS-5101.\n        DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {} \"\n            + \" of {}; 31-bit MappedByteBuffer limit exceeded.  blockPos\u003d{}, \"\n            + \"curEnd\u003d{}\", curPos, src, blockPos, curEnd);\n        return null;\n      }\n      length \u003d (int)length31;\n      DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid 31-bit \"\n          + \"limit.  blockPos\u003d{}; curPos\u003d{}; curEnd\u003d{}\",\n          maxLength, length, blockPos, curPos, curEnd);\n    }\n    final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n    if (clientMmap \u003d\u003d null) {\n      DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset {} of\"\n          + \" {}; BlockReader#getClientMmap returned null.\", curPos, src);\n      return null;\n    }\n    boolean success \u003d false;\n    ByteBuffer buffer;\n    try {\n      seek(curPos + length);\n      buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n      buffer.position((int)blockPos);\n      buffer.limit((int)(blockPos + length));\n      getExtendedReadBuffers().put(buffer, clientMmap);\n      synchronized (infoLock) {\n        readStatistics.addZeroCopyBytes(length);\n      }\n      DFSClient.LOG.debug(\"readZeroCopy read {} bytes from offset {} via the \"\n          + \"zero-copy read path.  blockEnd \u003d {}\", length, curPos, blockEnd);\n      success \u003d true;\n    } finally {\n      if (!success) {\n        IOUtils.closeQuietly(clientMmap);\n      }\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n    // JVM to optimize this function.\n    final long curPos \u003d pos;\n    final long curEnd \u003d blockEnd;\n    final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    final long blockPos \u003d curPos - blockStartInFile;\n\n    // Shorten this read if the end of the block is nearby.\n    long length63;\n    if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n      length63 \u003d maxLength;\n    } else {\n      length63 \u003d 1 + curEnd - curPos;\n      if (length63 \u003c\u003d 0) {\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n            curPos + \" of \" + src + \"; \" + length63 + \" bytes left in block.  \" +\n            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n            \"; curEnd\u003d\" + curEnd);\n        }\n        return null;\n      }\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n            \" to \" + length63 + \" to avoid going more than one byte \" +\n            \"past the end of the block.  blockPos\u003d\" + blockPos +\n            \"; curPos\u003d\" + curPos + \"; curEnd\u003d\" + curEnd);\n      }\n    }\n    // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n    int length;\n    if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n      length \u003d (int)length63;\n    } else {\n      long length31 \u003d Integer.MAX_VALUE - blockPos;\n      if (length31 \u003c\u003d 0) {\n        // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n        // 4-byte signed integers to represent capacity, etc.\n        // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n        // FIXME: we could work around this with multiple memory maps.\n        // See HDFS-5101.\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n            curPos + \" of \" + src + \"; 31-bit MappedByteBuffer limit \" +\n            \"exceeded.  blockPos\u003d\" + blockPos + \", curEnd\u003d\" + curEnd);\n        }\n        return null;\n      }\n      length \u003d (int)length31;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n            \" to \" + length + \" to avoid 31-bit limit.  \" +\n            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n            \"; curEnd\u003d\" + curEnd);\n      }\n    }\n    final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n    if (clientMmap \u003d\u003d null) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n          \"null.\");\n      }\n      return null;\n    }\n    boolean success \u003d false;\n    ByteBuffer buffer;\n    try {\n      seek(curPos + length);\n      buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n      buffer.position((int)blockPos);\n      buffer.limit((int)(blockPos + length));\n      getExtendedReadBuffers().put(buffer, clientMmap);\n      synchronized (infoLock) {\n        readStatistics.addZeroCopyBytes(length);\n      }\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"readZeroCopy read \" + length + \n            \" bytes from offset \" + curPos + \" via the zero-copy read \" +\n            \"path.  blockEnd \u003d \" + blockEnd);\n      }\n      success \u003d true;\n    } finally {\n      if (!success) {\n        IOUtils.closeQuietly(clientMmap);\n      }\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "871cb56152e6039ff56c6fabfcd45451029471c3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7790. Do not create optional fields in DFSInputStream unless they are needed (cmccabe)\n",
      "commitDate": "12/02/15 5:48 PM",
      "commitName": "871cb56152e6039ff56c6fabfcd45451029471c3",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "12/02/15 10:40 AM",
      "commitNameOld": "6b39ad0865cb2a7960dd59d68178f0bf28865ce2",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.3,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,91 @@\n   private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n       EnumSet\u003cReadOption\u003e opts) throws IOException {\n     // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n     // JVM to optimize this function.\n     final long curPos \u003d pos;\n     final long curEnd \u003d blockEnd;\n     final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n     final long blockPos \u003d curPos - blockStartInFile;\n \n     // Shorten this read if the end of the block is nearby.\n     long length63;\n     if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n       length63 \u003d maxLength;\n     } else {\n       length63 \u003d 1 + curEnd - curPos;\n       if (length63 \u003c\u003d 0) {\n         if (DFSClient.LOG.isDebugEnabled()) {\n           DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n             curPos + \" of \" + src + \"; \" + length63 + \" bytes left in block.  \" +\n             \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n             \"; curEnd\u003d\" + curEnd);\n         }\n         return null;\n       }\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n             \" to \" + length63 + \" to avoid going more than one byte \" +\n             \"past the end of the block.  blockPos\u003d\" + blockPos +\n             \"; curPos\u003d\" + curPos + \"; curEnd\u003d\" + curEnd);\n       }\n     }\n     // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n     int length;\n     if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n       length \u003d (int)length63;\n     } else {\n       long length31 \u003d Integer.MAX_VALUE - blockPos;\n       if (length31 \u003c\u003d 0) {\n         // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n         // 4-byte signed integers to represent capacity, etc.\n         // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n         // FIXME: we could work around this with multiple memory maps.\n         // See HDFS-5101.\n         if (DFSClient.LOG.isDebugEnabled()) {\n           DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n             curPos + \" of \" + src + \"; 31-bit MappedByteBuffer limit \" +\n             \"exceeded.  blockPos\u003d\" + blockPos + \", curEnd\u003d\" + curEnd);\n         }\n         return null;\n       }\n       length \u003d (int)length31;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n             \" to \" + length + \" to avoid 31-bit limit.  \" +\n             \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n             \"; curEnd\u003d\" + curEnd);\n       }\n     }\n     final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n     if (clientMmap \u003d\u003d null) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n           curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n           \"null.\");\n       }\n       return null;\n     }\n     boolean success \u003d false;\n     ByteBuffer buffer;\n     try {\n       seek(curPos + length);\n       buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n       buffer.position((int)blockPos);\n       buffer.limit((int)(blockPos + length));\n-      extendedReadBuffers.put(buffer, clientMmap);\n+      getExtendedReadBuffers().put(buffer, clientMmap);\n       synchronized (infoLock) {\n         readStatistics.addZeroCopyBytes(length);\n       }\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"readZeroCopy read \" + length + \n             \" bytes from offset \" + curPos + \" via the zero-copy read \" +\n             \"path.  blockEnd \u003d \" + blockEnd);\n       }\n       success \u003d true;\n     } finally {\n       if (!success) {\n         IOUtils.closeQuietly(clientMmap);\n       }\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n    // JVM to optimize this function.\n    final long curPos \u003d pos;\n    final long curEnd \u003d blockEnd;\n    final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    final long blockPos \u003d curPos - blockStartInFile;\n\n    // Shorten this read if the end of the block is nearby.\n    long length63;\n    if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n      length63 \u003d maxLength;\n    } else {\n      length63 \u003d 1 + curEnd - curPos;\n      if (length63 \u003c\u003d 0) {\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n            curPos + \" of \" + src + \"; \" + length63 + \" bytes left in block.  \" +\n            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n            \"; curEnd\u003d\" + curEnd);\n        }\n        return null;\n      }\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n            \" to \" + length63 + \" to avoid going more than one byte \" +\n            \"past the end of the block.  blockPos\u003d\" + blockPos +\n            \"; curPos\u003d\" + curPos + \"; curEnd\u003d\" + curEnd);\n      }\n    }\n    // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n    int length;\n    if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n      length \u003d (int)length63;\n    } else {\n      long length31 \u003d Integer.MAX_VALUE - blockPos;\n      if (length31 \u003c\u003d 0) {\n        // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n        // 4-byte signed integers to represent capacity, etc.\n        // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n        // FIXME: we could work around this with multiple memory maps.\n        // See HDFS-5101.\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n            curPos + \" of \" + src + \"; 31-bit MappedByteBuffer limit \" +\n            \"exceeded.  blockPos\u003d\" + blockPos + \", curEnd\u003d\" + curEnd);\n        }\n        return null;\n      }\n      length \u003d (int)length31;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n            \" to \" + length + \" to avoid 31-bit limit.  \" +\n            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n            \"; curEnd\u003d\" + curEnd);\n      }\n    }\n    final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n    if (clientMmap \u003d\u003d null) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n          \"null.\");\n      }\n      return null;\n    }\n    boolean success \u003d false;\n    ByteBuffer buffer;\n    try {\n      seek(curPos + length);\n      buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n      buffer.position((int)blockPos);\n      buffer.limit((int)(blockPos + length));\n      getExtendedReadBuffers().put(buffer, clientMmap);\n      synchronized (infoLock) {\n        readStatistics.addZeroCopyBytes(length);\n      }\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"readZeroCopy read \" + length + \n            \" bytes from offset \" + curPos + \" via the zero-copy read \" +\n            \"path.  blockEnd \u003d \" + blockEnd);\n      }\n      success \u003d true;\n    } finally {\n      if (!success) {\n        IOUtils.closeQuietly(clientMmap);\n      }\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "45ea53f9388e6bff1ac0aa3989a1dad56a611fd3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7698. Fix locking on HDFS read statistics and add a method for clearing them. (Colin P. McCabe via yliu)\n",
      "commitDate": "05/02/15 7:56 AM",
      "commitName": "45ea53f9388e6bff1ac0aa3989a1dad56a611fd3",
      "commitAuthor": "yliu",
      "commitDateOld": "30/01/15 4:01 PM",
      "commitNameOld": "09ad9a868a89922e9b55b3e7c5b9f41fa54d3770",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 5.66,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,89 +1,91 @@\n   private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n       EnumSet\u003cReadOption\u003e opts) throws IOException {\n     // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n     // JVM to optimize this function.\n     final long curPos \u003d pos;\n     final long curEnd \u003d blockEnd;\n     final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n     final long blockPos \u003d curPos - blockStartInFile;\n \n     // Shorten this read if the end of the block is nearby.\n     long length63;\n     if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n       length63 \u003d maxLength;\n     } else {\n       length63 \u003d 1 + curEnd - curPos;\n       if (length63 \u003c\u003d 0) {\n         if (DFSClient.LOG.isDebugEnabled()) {\n           DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n             curPos + \" of \" + src + \"; \" + length63 + \" bytes left in block.  \" +\n             \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n             \"; curEnd\u003d\" + curEnd);\n         }\n         return null;\n       }\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n             \" to \" + length63 + \" to avoid going more than one byte \" +\n             \"past the end of the block.  blockPos\u003d\" + blockPos +\n             \"; curPos\u003d\" + curPos + \"; curEnd\u003d\" + curEnd);\n       }\n     }\n     // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n     int length;\n     if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n       length \u003d (int)length63;\n     } else {\n       long length31 \u003d Integer.MAX_VALUE - blockPos;\n       if (length31 \u003c\u003d 0) {\n         // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n         // 4-byte signed integers to represent capacity, etc.\n         // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n         // FIXME: we could work around this with multiple memory maps.\n         // See HDFS-5101.\n         if (DFSClient.LOG.isDebugEnabled()) {\n           DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n             curPos + \" of \" + src + \"; 31-bit MappedByteBuffer limit \" +\n             \"exceeded.  blockPos\u003d\" + blockPos + \", curEnd\u003d\" + curEnd);\n         }\n         return null;\n       }\n       length \u003d (int)length31;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n             \" to \" + length + \" to avoid 31-bit limit.  \" +\n             \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n             \"; curEnd\u003d\" + curEnd);\n       }\n     }\n     final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n     if (clientMmap \u003d\u003d null) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n           curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n           \"null.\");\n       }\n       return null;\n     }\n     boolean success \u003d false;\n     ByteBuffer buffer;\n     try {\n       seek(curPos + length);\n       buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n       buffer.position((int)blockPos);\n       buffer.limit((int)(blockPos + length));\n       extendedReadBuffers.put(buffer, clientMmap);\n-      readStatistics.addZeroCopyBytes(length);\n+      synchronized (infoLock) {\n+        readStatistics.addZeroCopyBytes(length);\n+      }\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"readZeroCopy read \" + length + \n             \" bytes from offset \" + curPos + \" via the zero-copy read \" +\n             \"path.  blockEnd \u003d \" + blockEnd);\n       }\n       success \u003d true;\n     } finally {\n       if (!success) {\n         IOUtils.closeQuietly(clientMmap);\n       }\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n    // JVM to optimize this function.\n    final long curPos \u003d pos;\n    final long curEnd \u003d blockEnd;\n    final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    final long blockPos \u003d curPos - blockStartInFile;\n\n    // Shorten this read if the end of the block is nearby.\n    long length63;\n    if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n      length63 \u003d maxLength;\n    } else {\n      length63 \u003d 1 + curEnd - curPos;\n      if (length63 \u003c\u003d 0) {\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n            curPos + \" of \" + src + \"; \" + length63 + \" bytes left in block.  \" +\n            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n            \"; curEnd\u003d\" + curEnd);\n        }\n        return null;\n      }\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n            \" to \" + length63 + \" to avoid going more than one byte \" +\n            \"past the end of the block.  blockPos\u003d\" + blockPos +\n            \"; curPos\u003d\" + curPos + \"; curEnd\u003d\" + curEnd);\n      }\n    }\n    // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n    int length;\n    if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n      length \u003d (int)length63;\n    } else {\n      long length31 \u003d Integer.MAX_VALUE - blockPos;\n      if (length31 \u003c\u003d 0) {\n        // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n        // 4-byte signed integers to represent capacity, etc.\n        // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n        // FIXME: we could work around this with multiple memory maps.\n        // See HDFS-5101.\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n            curPos + \" of \" + src + \"; 31-bit MappedByteBuffer limit \" +\n            \"exceeded.  blockPos\u003d\" + blockPos + \", curEnd\u003d\" + curEnd);\n        }\n        return null;\n      }\n      length \u003d (int)length31;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n            \" to \" + length + \" to avoid 31-bit limit.  \" +\n            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n            \"; curEnd\u003d\" + curEnd);\n      }\n    }\n    final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n    if (clientMmap \u003d\u003d null) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n          \"null.\");\n      }\n      return null;\n    }\n    boolean success \u003d false;\n    ByteBuffer buffer;\n    try {\n      seek(curPos + length);\n      buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n      buffer.position((int)blockPos);\n      buffer.limit((int)(blockPos + length));\n      extendedReadBuffers.put(buffer, clientMmap);\n      synchronized (infoLock) {\n        readStatistics.addZeroCopyBytes(length);\n      }\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"readZeroCopy read \" + length + \n            \" bytes from offset \" + curPos + \" via the zero-copy read \" +\n            \"path.  blockEnd \u003d \" + blockEnd);\n      }\n      success \u003d true;\n    } finally {\n      if (!success) {\n        IOUtils.closeQuietly(clientMmap);\n      }\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "f730fa919e8a2a820f4fefcd882628bbd4f2d9ab": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6097. Zero-copy reads are incorrectly disabled on file offsets above 2GB (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1577350 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/03/14 3:30 PM",
      "commitName": "f730fa919e8a2a820f4fefcd882628bbd4f2d9ab",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "10/03/14 12:04 PM",
      "commitNameOld": "daaa8f03f411652da7b7919e16e0fbe24367f106",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 3.14,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,89 @@\n   private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n       EnumSet\u003cReadOption\u003e opts) throws IOException {\n-    // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n-    // 4-byte signed integers to represent capacity, etc.\n-    // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n-    // FIXME: we could work around this with multiple memory maps.\n-    // See HDFS-5101.\n-    long blockEnd32 \u003d Math.min(Integer.MAX_VALUE, blockEnd);\n-    long curPos \u003d pos;\n-    long blockLeft \u003d blockEnd32 - curPos + 1;\n-    if (blockLeft \u003c\u003d 0) {\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n-          curPos + \" of \" + src + \"; blockLeft \u003d \" + blockLeft +\n-          \"; blockEnd32 \u003d \" + blockEnd32 + \", blockEnd \u003d \" + blockEnd +\n-          \"; maxLength \u003d \" + maxLength);\n+    // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n+    // JVM to optimize this function.\n+    final long curPos \u003d pos;\n+    final long curEnd \u003d blockEnd;\n+    final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n+    final long blockPos \u003d curPos - blockStartInFile;\n+\n+    // Shorten this read if the end of the block is nearby.\n+    long length63;\n+    if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n+      length63 \u003d maxLength;\n+    } else {\n+      length63 \u003d 1 + curEnd - curPos;\n+      if (length63 \u003c\u003d 0) {\n+        if (DFSClient.LOG.isDebugEnabled()) {\n+          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n+            curPos + \" of \" + src + \"; \" + length63 + \" bytes left in block.  \" +\n+            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n+            \"; curEnd\u003d\" + curEnd);\n+        }\n+        return null;\n       }\n-      return null;\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n+            \" to \" + length63 + \" to avoid going more than one byte \" +\n+            \"past the end of the block.  blockPos\u003d\" + blockPos +\n+            \"; curPos\u003d\" + curPos + \"; curEnd\u003d\" + curEnd);\n+      }\n     }\n-    int length \u003d Math.min((int)blockLeft, maxLength);\n-    long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n-    long blockPos \u003d curPos - blockStartInFile;\n-    long limit \u003d blockPos + length;\n-    ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n+    // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n+    int length;\n+    if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n+      length \u003d (int)length63;\n+    } else {\n+      long length31 \u003d Integer.MAX_VALUE - blockPos;\n+      if (length31 \u003c\u003d 0) {\n+        // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n+        // 4-byte signed integers to represent capacity, etc.\n+        // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n+        // FIXME: we could work around this with multiple memory maps.\n+        // See HDFS-5101.\n+        if (DFSClient.LOG.isDebugEnabled()) {\n+          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n+            curPos + \" of \" + src + \"; 31-bit MappedByteBuffer limit \" +\n+            \"exceeded.  blockPos\u003d\" + blockPos + \", curEnd\u003d\" + curEnd);\n+        }\n+        return null;\n+      }\n+      length \u003d (int)length31;\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n+            \" to \" + length + \" to avoid 31-bit limit.  \" +\n+            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n+            \"; curEnd\u003d\" + curEnd);\n+      }\n+    }\n+    final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n     if (clientMmap \u003d\u003d null) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n           curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n           \"null.\");\n       }\n       return null;\n     }\n     boolean success \u003d false;\n     ByteBuffer buffer;\n     try {\n-      seek(pos + length);\n+      seek(curPos + length);\n       buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n       buffer.position((int)blockPos);\n-      buffer.limit((int)limit);\n+      buffer.limit((int)(blockPos + length));\n       extendedReadBuffers.put(buffer, clientMmap);\n       readStatistics.addZeroCopyBytes(length);\n       if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n-            \"offset \" + curPos + \" via the zero-copy read path.  \" +\n-            \"blockEnd \u003d \" + blockEnd);\n+        DFSClient.LOG.debug(\"readZeroCopy read \" + length + \n+            \" bytes from offset \" + curPos + \" via the zero-copy read \" +\n+            \"path.  blockEnd \u003d \" + blockEnd);\n       }\n       success \u003d true;\n     } finally {\n       if (!success) {\n         IOUtils.closeQuietly(clientMmap);\n       }\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Copy \u0027pos\u0027 and \u0027blockEnd\u0027 to local variables to make it easier for the\n    // JVM to optimize this function.\n    final long curPos \u003d pos;\n    final long curEnd \u003d blockEnd;\n    final long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    final long blockPos \u003d curPos - blockStartInFile;\n\n    // Shorten this read if the end of the block is nearby.\n    long length63;\n    if ((curPos + maxLength) \u003c\u003d (curEnd + 1)) {\n      length63 \u003d maxLength;\n    } else {\n      length63 \u003d 1 + curEnd - curPos;\n      if (length63 \u003c\u003d 0) {\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n            curPos + \" of \" + src + \"; \" + length63 + \" bytes left in block.  \" +\n            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n            \"; curEnd\u003d\" + curEnd);\n        }\n        return null;\n      }\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n            \" to \" + length63 + \" to avoid going more than one byte \" +\n            \"past the end of the block.  blockPos\u003d\" + blockPos +\n            \"; curPos\u003d\" + curPos + \"; curEnd\u003d\" + curEnd);\n      }\n    }\n    // Make sure that don\u0027t go beyond 31-bit offsets in the MappedByteBuffer.\n    int length;\n    if (blockPos + length63 \u003c\u003d Integer.MAX_VALUE) {\n      length \u003d (int)length63;\n    } else {\n      long length31 \u003d Integer.MAX_VALUE - blockPos;\n      if (length31 \u003c\u003d 0) {\n        // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n        // 4-byte signed integers to represent capacity, etc.\n        // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n        // FIXME: we could work around this with multiple memory maps.\n        // See HDFS-5101.\n        if (DFSClient.LOG.isDebugEnabled()) {\n          DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset \" +\n            curPos + \" of \" + src + \"; 31-bit MappedByteBuffer limit \" +\n            \"exceeded.  blockPos\u003d\" + blockPos + \", curEnd\u003d\" + curEnd);\n        }\n        return null;\n      }\n      length \u003d (int)length31;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"Reducing read length from \" + maxLength +\n            \" to \" + length + \" to avoid 31-bit limit.  \" +\n            \"blockPos\u003d\" + blockPos + \"; curPos\u003d\" + curPos +\n            \"; curEnd\u003d\" + curEnd);\n      }\n    }\n    final ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n    if (clientMmap \u003d\u003d null) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n          \"null.\");\n      }\n      return null;\n    }\n    boolean success \u003d false;\n    ByteBuffer buffer;\n    try {\n      seek(curPos + length);\n      buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n      buffer.position((int)blockPos);\n      buffer.limit((int)(blockPos + length));\n      extendedReadBuffers.put(buffer, clientMmap);\n      readStatistics.addZeroCopyBytes(length);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"readZeroCopy read \" + length + \n            \" bytes from offset \" + curPos + \" via the zero-copy read \" +\n            \"path.  blockEnd \u003d \" + blockEnd);\n      }\n      success \u003d true;\n    } finally {\n      if (!success) {\n        IOUtils.closeQuietly(clientMmap);\n      }\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5950. The DFSClient and DataNode should use shared memory segments to communicate short-circuit information (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1573433 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/03/14 7:58 PM",
      "commitName": "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "24/02/14 2:34 PM",
      "commitNameOld": "17db74a1c1972392a5aba48a3e0334dcd6c76487",
      "commitAuthorOld": "Michael Stack",
      "daysBetweenCommits": 6.22,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n       EnumSet\u003cReadOption\u003e opts) throws IOException {\n     // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n     // 4-byte signed integers to represent capacity, etc.\n     // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n     // FIXME: we could work around this with multiple memory maps.\n     // See HDFS-5101.\n     long blockEnd32 \u003d Math.min(Integer.MAX_VALUE, blockEnd);\n     long curPos \u003d pos;\n     long blockLeft \u003d blockEnd32 - curPos + 1;\n     if (blockLeft \u003c\u003d 0) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n           curPos + \" of \" + src + \"; blockLeft \u003d \" + blockLeft +\n           \"; blockEnd32 \u003d \" + blockEnd32 + \", blockEnd \u003d \" + blockEnd +\n           \"; maxLength \u003d \" + maxLength);\n       }\n       return null;\n     }\n     int length \u003d Math.min((int)blockLeft, maxLength);\n     long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n     long blockPos \u003d curPos - blockStartInFile;\n     long limit \u003d blockPos + length;\n     ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n     if (clientMmap \u003d\u003d null) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n           curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n           \"null.\");\n       }\n       return null;\n     }\n     boolean success \u003d false;\n     ByteBuffer buffer;\n     try {\n       seek(pos + length);\n       buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n       buffer.position((int)blockPos);\n       buffer.limit((int)limit);\n       extendedReadBuffers.put(buffer, clientMmap);\n       readStatistics.addZeroCopyBytes(length);\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n             \"offset \" + curPos + \" via the zero-copy read path.  \" +\n             \"blockEnd \u003d \" + blockEnd);\n       }\n       success \u003d true;\n     } finally {\n       if (!success) {\n-        clientMmap.unref();\n+        IOUtils.closeQuietly(clientMmap);\n       }\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n    // 4-byte signed integers to represent capacity, etc.\n    // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n    // FIXME: we could work around this with multiple memory maps.\n    // See HDFS-5101.\n    long blockEnd32 \u003d Math.min(Integer.MAX_VALUE, blockEnd);\n    long curPos \u003d pos;\n    long blockLeft \u003d blockEnd32 - curPos + 1;\n    if (blockLeft \u003c\u003d 0) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; blockLeft \u003d \" + blockLeft +\n          \"; blockEnd32 \u003d \" + blockEnd32 + \", blockEnd \u003d \" + blockEnd +\n          \"; maxLength \u003d \" + maxLength);\n      }\n      return null;\n    }\n    int length \u003d Math.min((int)blockLeft, maxLength);\n    long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    long blockPos \u003d curPos - blockStartInFile;\n    long limit \u003d blockPos + length;\n    ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n    if (clientMmap \u003d\u003d null) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n          \"null.\");\n      }\n      return null;\n    }\n    boolean success \u003d false;\n    ByteBuffer buffer;\n    try {\n      seek(pos + length);\n      buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n      buffer.position((int)blockPos);\n      buffer.limit((int)limit);\n      extendedReadBuffers.put(buffer, clientMmap);\n      readStatistics.addZeroCopyBytes(length);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n            \"offset \" + curPos + \" via the zero-copy read path.  \" +\n            \"blockEnd \u003d \" + blockEnd);\n      }\n      success \u003d true;\n    } finally {\n      if (!success) {\n        IOUtils.closeQuietly(clientMmap);\n      }\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 11:08 AM",
      "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "06/02/14 7:45 AM",
      "commitNameOld": "ab96a0838dafbfea77382135914feadbfd03cf53",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 6.14,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,54 @@\n   private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n       EnumSet\u003cReadOption\u003e opts) throws IOException {\n     // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n     // 4-byte signed integers to represent capacity, etc.\n     // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n     // FIXME: we could work around this with multiple memory maps.\n     // See HDFS-5101.\n     long blockEnd32 \u003d Math.min(Integer.MAX_VALUE, blockEnd);\n     long curPos \u003d pos;\n     long blockLeft \u003d blockEnd32 - curPos + 1;\n     if (blockLeft \u003c\u003d 0) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n           curPos + \" of \" + src + \"; blockLeft \u003d \" + blockLeft +\n           \"; blockEnd32 \u003d \" + blockEnd32 + \", blockEnd \u003d \" + blockEnd +\n           \"; maxLength \u003d \" + maxLength);\n       }\n       return null;\n     }\n     int length \u003d Math.min((int)blockLeft, maxLength);\n     long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n     long blockPos \u003d curPos - blockStartInFile;\n     long limit \u003d blockPos + length;\n-    ClientMmap clientMmap \u003d\n-        blockReader.getClientMmap(opts, dfsClient.getMmapManager());\n+    ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n     if (clientMmap \u003d\u003d null) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n           curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n           \"null.\");\n       }\n       return null;\n     }\n-    seek(pos + length);\n-    ByteBuffer buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n-    buffer.position((int)blockPos);\n-    buffer.limit((int)limit);\n-    clientMmap.ref();\n-    extendedReadBuffers.put(buffer, clientMmap);\n-    readStatistics.addZeroCopyBytes(length);\n-    if (DFSClient.LOG.isDebugEnabled()) {\n-      DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n-          \"offset \" + curPos + \" via the zero-copy read path.  \" +\n-          \"blockEnd \u003d \" + blockEnd);\n+    boolean success \u003d false;\n+    ByteBuffer buffer;\n+    try {\n+      seek(pos + length);\n+      buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n+      buffer.position((int)blockPos);\n+      buffer.limit((int)limit);\n+      extendedReadBuffers.put(buffer, clientMmap);\n+      readStatistics.addZeroCopyBytes(length);\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n+            \"offset \" + curPos + \" via the zero-copy read path.  \" +\n+            \"blockEnd \u003d \" + blockEnd);\n+      }\n+      success \u003d true;\n+    } finally {\n+      if (!success) {\n+        clientMmap.unref();\n+      }\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n    // 4-byte signed integers to represent capacity, etc.\n    // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n    // FIXME: we could work around this with multiple memory maps.\n    // See HDFS-5101.\n    long blockEnd32 \u003d Math.min(Integer.MAX_VALUE, blockEnd);\n    long curPos \u003d pos;\n    long blockLeft \u003d blockEnd32 - curPos + 1;\n    if (blockLeft \u003c\u003d 0) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; blockLeft \u003d \" + blockLeft +\n          \"; blockEnd32 \u003d \" + blockEnd32 + \", blockEnd \u003d \" + blockEnd +\n          \"; maxLength \u003d \" + maxLength);\n      }\n      return null;\n    }\n    int length \u003d Math.min((int)blockLeft, maxLength);\n    long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    long blockPos \u003d curPos - blockStartInFile;\n    long limit \u003d blockPos + length;\n    ClientMmap clientMmap \u003d blockReader.getClientMmap(opts);\n    if (clientMmap \u003d\u003d null) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n          \"null.\");\n      }\n      return null;\n    }\n    boolean success \u003d false;\n    ByteBuffer buffer;\n    try {\n      seek(pos + length);\n      buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n      buffer.position((int)blockPos);\n      buffer.limit((int)limit);\n      extendedReadBuffers.put(buffer, clientMmap);\n      readStatistics.addZeroCopyBytes(length);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n            \"offset \" + curPos + \" via the zero-copy read path.  \" +\n            \"blockEnd \u003d \" + blockEnd);\n      }\n      success \u003d true;\n    } finally {\n      if (!success) {\n        clientMmap.unref();\n      }\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "124e507674c0d396f8494585e64226957199097b": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5634. Allow BlockReaderLocal to switch between checksumming and not (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551701 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 12:57 PM",
      "commitName": "124e507674c0d396f8494585e64226957199097b",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5634. Allow BlockReaderLocal to switch between checksumming and not (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551701 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/12/13 12:57 PM",
          "commitName": "124e507674c0d396f8494585e64226957199097b",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "12/12/13 6:47 PM",
          "commitNameOld": "a31a3a68ead2773f8daa5b3ac1041e271b442789",
          "commitAuthorOld": "Junping Du",
          "daysBetweenCommits": 4.76,
          "commitsBetweenForRepo": 23,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,47 @@\n-  private synchronized ByteBuffer tryReadZeroCopy(int maxLength)\n-      throws IOException {\n+  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n+      EnumSet\u003cReadOption\u003e opts) throws IOException {\n     // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n     // 4-byte signed integers to represent capacity, etc.\n     // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n     // FIXME: we could work around this with multiple memory maps.\n     // See HDFS-5101.\n     long blockEnd32 \u003d Math.min(Integer.MAX_VALUE, blockEnd);\n     long curPos \u003d pos;\n     long blockLeft \u003d blockEnd32 - curPos + 1;\n     if (blockLeft \u003c\u003d 0) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n           curPos + \" of \" + src + \"; blockLeft \u003d \" + blockLeft +\n           \"; blockEnd32 \u003d \" + blockEnd32 + \", blockEnd \u003d \" + blockEnd +\n           \"; maxLength \u003d \" + maxLength);\n       }\n       return null;\n     }\n     int length \u003d Math.min((int)blockLeft, maxLength);\n     long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n     long blockPos \u003d curPos - blockStartInFile;\n     long limit \u003d blockPos + length;\n     ClientMmap clientMmap \u003d\n-        blockReader.getClientMmap(currentLocatedBlock,\n-            dfsClient.getMmapManager());\n+        blockReader.getClientMmap(opts, dfsClient.getMmapManager());\n     if (clientMmap \u003d\u003d null) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n           curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n           \"null.\");\n       }\n       return null;\n     }\n     seek(pos + length);\n     ByteBuffer buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n     buffer.position((int)blockPos);\n     buffer.limit((int)limit);\n     clientMmap.ref();\n     extendedReadBuffers.put(buffer, clientMmap);\n     readStatistics.addZeroCopyBytes(length);\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n           \"offset \" + curPos + \" via the zero-copy read path.  \" +\n           \"blockEnd \u003d \" + blockEnd);\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n    // 4-byte signed integers to represent capacity, etc.\n    // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n    // FIXME: we could work around this with multiple memory maps.\n    // See HDFS-5101.\n    long blockEnd32 \u003d Math.min(Integer.MAX_VALUE, blockEnd);\n    long curPos \u003d pos;\n    long blockLeft \u003d blockEnd32 - curPos + 1;\n    if (blockLeft \u003c\u003d 0) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; blockLeft \u003d \" + blockLeft +\n          \"; blockEnd32 \u003d \" + blockEnd32 + \", blockEnd \u003d \" + blockEnd +\n          \"; maxLength \u003d \" + maxLength);\n      }\n      return null;\n    }\n    int length \u003d Math.min((int)blockLeft, maxLength);\n    long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    long blockPos \u003d curPos - blockStartInFile;\n    long limit \u003d blockPos + length;\n    ClientMmap clientMmap \u003d\n        blockReader.getClientMmap(opts, dfsClient.getMmapManager());\n    if (clientMmap \u003d\u003d null) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n          \"null.\");\n      }\n      return null;\n    }\n    seek(pos + length);\n    ByteBuffer buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n    buffer.position((int)blockPos);\n    buffer.limit((int)limit);\n    clientMmap.ref();\n    extendedReadBuffers.put(buffer, clientMmap);\n    readStatistics.addZeroCopyBytes(length);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n          \"offset \" + curPos + \" via the zero-copy read path.  \" +\n          \"blockEnd \u003d \" + blockEnd);\n    }\n    return buffer;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[maxLength-int]",
            "newValue": "[maxLength-int, opts-EnumSet\u003cReadOption\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5634. Allow BlockReaderLocal to switch between checksumming and not (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551701 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/12/13 12:57 PM",
          "commitName": "124e507674c0d396f8494585e64226957199097b",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "12/12/13 6:47 PM",
          "commitNameOld": "a31a3a68ead2773f8daa5b3ac1041e271b442789",
          "commitAuthorOld": "Junping Du",
          "daysBetweenCommits": 4.76,
          "commitsBetweenForRepo": 23,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,47 @@\n-  private synchronized ByteBuffer tryReadZeroCopy(int maxLength)\n-      throws IOException {\n+  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n+      EnumSet\u003cReadOption\u003e opts) throws IOException {\n     // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n     // 4-byte signed integers to represent capacity, etc.\n     // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n     // FIXME: we could work around this with multiple memory maps.\n     // See HDFS-5101.\n     long blockEnd32 \u003d Math.min(Integer.MAX_VALUE, blockEnd);\n     long curPos \u003d pos;\n     long blockLeft \u003d blockEnd32 - curPos + 1;\n     if (blockLeft \u003c\u003d 0) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n           curPos + \" of \" + src + \"; blockLeft \u003d \" + blockLeft +\n           \"; blockEnd32 \u003d \" + blockEnd32 + \", blockEnd \u003d \" + blockEnd +\n           \"; maxLength \u003d \" + maxLength);\n       }\n       return null;\n     }\n     int length \u003d Math.min((int)blockLeft, maxLength);\n     long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n     long blockPos \u003d curPos - blockStartInFile;\n     long limit \u003d blockPos + length;\n     ClientMmap clientMmap \u003d\n-        blockReader.getClientMmap(currentLocatedBlock,\n-            dfsClient.getMmapManager());\n+        blockReader.getClientMmap(opts, dfsClient.getMmapManager());\n     if (clientMmap \u003d\u003d null) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n           curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n           \"null.\");\n       }\n       return null;\n     }\n     seek(pos + length);\n     ByteBuffer buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n     buffer.position((int)blockPos);\n     buffer.limit((int)limit);\n     clientMmap.ref();\n     extendedReadBuffers.put(buffer, clientMmap);\n     readStatistics.addZeroCopyBytes(length);\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n           \"offset \" + curPos + \" via the zero-copy read path.  \" +\n           \"blockEnd \u003d \" + blockEnd);\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength,\n      EnumSet\u003cReadOption\u003e opts) throws IOException {\n    // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n    // 4-byte signed integers to represent capacity, etc.\n    // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n    // FIXME: we could work around this with multiple memory maps.\n    // See HDFS-5101.\n    long blockEnd32 \u003d Math.min(Integer.MAX_VALUE, blockEnd);\n    long curPos \u003d pos;\n    long blockLeft \u003d blockEnd32 - curPos + 1;\n    if (blockLeft \u003c\u003d 0) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; blockLeft \u003d \" + blockLeft +\n          \"; blockEnd32 \u003d \" + blockEnd32 + \", blockEnd \u003d \" + blockEnd +\n          \"; maxLength \u003d \" + maxLength);\n      }\n      return null;\n    }\n    int length \u003d Math.min((int)blockLeft, maxLength);\n    long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    long blockPos \u003d curPos - blockStartInFile;\n    long limit \u003d blockPos + length;\n    ClientMmap clientMmap \u003d\n        blockReader.getClientMmap(opts, dfsClient.getMmapManager());\n    if (clientMmap \u003d\u003d null) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n          \"null.\");\n      }\n      return null;\n    }\n    seek(pos + length);\n    ByteBuffer buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n    buffer.position((int)blockPos);\n    buffer.limit((int)limit);\n    clientMmap.ref();\n    extendedReadBuffers.put(buffer, clientMmap);\n    readStatistics.addZeroCopyBytes(length);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n          \"offset \" + curPos + \" via the zero-copy read path.  \" +\n          \"blockEnd \u003d \" + blockEnd);\n    }\n    return buffer;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5260. Merge zero-copy memory-mapped HDFS client reads to trunk and branch-2. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1527113 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 3:51 PM",
      "commitName": "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
      "commitAuthor": "Chris Nauroth",
      "diff": "@@ -0,0 +1,48 @@\n+  private synchronized ByteBuffer tryReadZeroCopy(int maxLength)\n+      throws IOException {\n+    // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n+    // 4-byte signed integers to represent capacity, etc.\n+    // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n+    // FIXME: we could work around this with multiple memory maps.\n+    // See HDFS-5101.\n+    long blockEnd32 \u003d Math.min(Integer.MAX_VALUE, blockEnd);\n+    long curPos \u003d pos;\n+    long blockLeft \u003d blockEnd32 - curPos + 1;\n+    if (blockLeft \u003c\u003d 0) {\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n+          curPos + \" of \" + src + \"; blockLeft \u003d \" + blockLeft +\n+          \"; blockEnd32 \u003d \" + blockEnd32 + \", blockEnd \u003d \" + blockEnd +\n+          \"; maxLength \u003d \" + maxLength);\n+      }\n+      return null;\n+    }\n+    int length \u003d Math.min((int)blockLeft, maxLength);\n+    long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n+    long blockPos \u003d curPos - blockStartInFile;\n+    long limit \u003d blockPos + length;\n+    ClientMmap clientMmap \u003d\n+        blockReader.getClientMmap(currentLocatedBlock,\n+            dfsClient.getMmapManager());\n+    if (clientMmap \u003d\u003d null) {\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n+          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n+          \"null.\");\n+      }\n+      return null;\n+    }\n+    seek(pos + length);\n+    ByteBuffer buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n+    buffer.position((int)blockPos);\n+    buffer.limit((int)limit);\n+    clientMmap.ref();\n+    extendedReadBuffers.put(buffer, clientMmap);\n+    readStatistics.addZeroCopyBytes(length);\n+    if (DFSClient.LOG.isDebugEnabled()) {\n+      DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n+          \"offset \" + curPos + \" via the zero-copy read path.  \" +\n+          \"blockEnd \u003d \" + blockEnd);\n+    }\n+    return buffer;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized ByteBuffer tryReadZeroCopy(int maxLength)\n      throws IOException {\n    // Java ByteBuffers can\u0027t be longer than 2 GB, because they use\n    // 4-byte signed integers to represent capacity, etc.\n    // So we can\u0027t mmap the parts of the block higher than the 2 GB offset.\n    // FIXME: we could work around this with multiple memory maps.\n    // See HDFS-5101.\n    long blockEnd32 \u003d Math.min(Integer.MAX_VALUE, blockEnd);\n    long curPos \u003d pos;\n    long blockLeft \u003d blockEnd32 - curPos + 1;\n    if (blockLeft \u003c\u003d 0) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; blockLeft \u003d \" + blockLeft +\n          \"; blockEnd32 \u003d \" + blockEnd32 + \", blockEnd \u003d \" + blockEnd +\n          \"; maxLength \u003d \" + maxLength);\n      }\n      return null;\n    }\n    int length \u003d Math.min((int)blockLeft, maxLength);\n    long blockStartInFile \u003d currentLocatedBlock.getStartOffset();\n    long blockPos \u003d curPos - blockStartInFile;\n    long limit \u003d blockPos + length;\n    ClientMmap clientMmap \u003d\n        blockReader.getClientMmap(currentLocatedBlock,\n            dfsClient.getMmapManager());\n    if (clientMmap \u003d\u003d null) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset \" +\n          curPos + \" of \" + src + \"; BlockReader#getClientMmap returned \" +\n          \"null.\");\n      }\n      return null;\n    }\n    seek(pos + length);\n    ByteBuffer buffer \u003d clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\n    buffer.position((int)blockPos);\n    buffer.limit((int)limit);\n    clientMmap.ref();\n    extendedReadBuffers.put(buffer, clientMmap);\n    readStatistics.addZeroCopyBytes(length);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"readZeroCopy read \" + maxLength + \" bytes from \" +\n          \"offset \" + curPos + \" via the zero-copy read path.  \" +\n          \"blockEnd \u003d \" + blockEnd);\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}