{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StripeReader.java",
  "functionName": "finalizeDecodeInputs",
  "functionId": "finalizeDecodeInputs",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
  "functionStartLine": 406,
  "functionEndLine": 419,
  "numCommitsSeen": 26,
  "timeTaken": 3084,
  "changeHistory": [
    "734d54c1a8950446e68098f62d8964e02ecc2890",
    "401db4fc65140979fe7665983e36905e886df971",
    "c201cf951d5adefefe7c68e882a0c07962248577"
  ],
  "changeHistoryShort": {
    "734d54c1a8950446e68098f62d8964e02ecc2890": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "401db4fc65140979fe7665983e36905e886df971": "Ymultichange(Yparameterchange,Ybodychange)",
    "c201cf951d5adefefe7c68e882a0c07962248577": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "734d54c1a8950446e68098f62d8964e02ecc2890": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
      "commitDate": "21/09/16 6:34 AM",
      "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "20/09/16 12:03 AM",
          "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,14 @@\n-  public static void finalizeDecodeInputs(final ByteBuffer[] decodeInputs,\n-                                          AlignedStripe alignedStripe) {\n+  void finalizeDecodeInputs() {\n     for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n       final StripingChunk chunk \u003d alignedStripe.chunks[i];\n       if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n         if (chunk.useChunkBuffer()) {\n-          chunk.getChunkBuffer().copyTo(decodeInputs[i]);\n+          chunk.getChunkBuffer().copyTo(decodeInputs[i].getBuffer());\n         } else {\n           chunk.getByteBuffer().flip();\n         }\n       } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n-        //ZERO it. Will be better handled in other following issue.\n-        byte[] emptyBytes \u003d new byte[decodeInputs[i].limit()];\n-        decodeInputs[i].put(emptyBytes);\n-        decodeInputs[i].flip();\n-      } else {\n-        decodeInputs[i] \u003d null;\n+        decodeInputs[i].setAllZero(true);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void finalizeDecodeInputs() {\n    for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n      final StripingChunk chunk \u003d alignedStripe.chunks[i];\n      if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n        if (chunk.useChunkBuffer()) {\n          chunk.getChunkBuffer().copyTo(decodeInputs[i].getBuffer());\n        } else {\n          chunk.getByteBuffer().flip();\n        }\n      } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n        decodeInputs[i].setAllZero(true);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
            "oldMethodName": "finalizeDecodeInputs",
            "newMethodName": "finalizeDecodeInputs"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "20/09/16 12:03 AM",
          "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,14 @@\n-  public static void finalizeDecodeInputs(final ByteBuffer[] decodeInputs,\n-                                          AlignedStripe alignedStripe) {\n+  void finalizeDecodeInputs() {\n     for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n       final StripingChunk chunk \u003d alignedStripe.chunks[i];\n       if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n         if (chunk.useChunkBuffer()) {\n-          chunk.getChunkBuffer().copyTo(decodeInputs[i]);\n+          chunk.getChunkBuffer().copyTo(decodeInputs[i].getBuffer());\n         } else {\n           chunk.getByteBuffer().flip();\n         }\n       } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n-        //ZERO it. Will be better handled in other following issue.\n-        byte[] emptyBytes \u003d new byte[decodeInputs[i].limit()];\n-        decodeInputs[i].put(emptyBytes);\n-        decodeInputs[i].flip();\n-      } else {\n-        decodeInputs[i] \u003d null;\n+        decodeInputs[i].setAllZero(true);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void finalizeDecodeInputs() {\n    for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n      final StripingChunk chunk \u003d alignedStripe.chunks[i];\n      if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n        if (chunk.useChunkBuffer()) {\n          chunk.getChunkBuffer().copyTo(decodeInputs[i].getBuffer());\n        } else {\n          chunk.getByteBuffer().flip();\n        }\n      } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n        decodeInputs[i].setAllZero(true);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
          "extendedDetails": {
            "oldValue": "[public, static]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "20/09/16 12:03 AM",
          "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,14 @@\n-  public static void finalizeDecodeInputs(final ByteBuffer[] decodeInputs,\n-                                          AlignedStripe alignedStripe) {\n+  void finalizeDecodeInputs() {\n     for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n       final StripingChunk chunk \u003d alignedStripe.chunks[i];\n       if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n         if (chunk.useChunkBuffer()) {\n-          chunk.getChunkBuffer().copyTo(decodeInputs[i]);\n+          chunk.getChunkBuffer().copyTo(decodeInputs[i].getBuffer());\n         } else {\n           chunk.getByteBuffer().flip();\n         }\n       } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n-        //ZERO it. Will be better handled in other following issue.\n-        byte[] emptyBytes \u003d new byte[decodeInputs[i].limit()];\n-        decodeInputs[i].put(emptyBytes);\n-        decodeInputs[i].flip();\n-      } else {\n-        decodeInputs[i] \u003d null;\n+        decodeInputs[i].setAllZero(true);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void finalizeDecodeInputs() {\n    for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n      final StripingChunk chunk \u003d alignedStripe.chunks[i];\n      if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n        if (chunk.useChunkBuffer()) {\n          chunk.getChunkBuffer().copyTo(decodeInputs[i].getBuffer());\n        } else {\n          chunk.getByteBuffer().flip();\n        }\n      } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n        decodeInputs[i].setAllZero(true);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "20/09/16 12:03 AM",
          "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,14 @@\n-  public static void finalizeDecodeInputs(final ByteBuffer[] decodeInputs,\n-                                          AlignedStripe alignedStripe) {\n+  void finalizeDecodeInputs() {\n     for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n       final StripingChunk chunk \u003d alignedStripe.chunks[i];\n       if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n         if (chunk.useChunkBuffer()) {\n-          chunk.getChunkBuffer().copyTo(decodeInputs[i]);\n+          chunk.getChunkBuffer().copyTo(decodeInputs[i].getBuffer());\n         } else {\n           chunk.getByteBuffer().flip();\n         }\n       } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n-        //ZERO it. Will be better handled in other following issue.\n-        byte[] emptyBytes \u003d new byte[decodeInputs[i].limit()];\n-        decodeInputs[i].put(emptyBytes);\n-        decodeInputs[i].flip();\n-      } else {\n-        decodeInputs[i] \u003d null;\n+        decodeInputs[i].setAllZero(true);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void finalizeDecodeInputs() {\n    for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n      final StripingChunk chunk \u003d alignedStripe.chunks[i];\n      if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n        if (chunk.useChunkBuffer()) {\n          chunk.getChunkBuffer().copyTo(decodeInputs[i].getBuffer());\n        } else {\n          chunk.getByteBuffer().flip();\n        }\n      } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n        decodeInputs[i].setAllZero(true);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
          "extendedDetails": {
            "oldValue": "[decodeInputs-ByteBuffer[](modifiers-final), alignedStripe-AlignedStripe]",
            "newValue": "[]"
          }
        }
      ]
    },
    "401db4fc65140979fe7665983e36905e886df971": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
      "commitDate": "08/09/16 11:54 AM",
      "commitName": "401db4fc65140979fe7665983e36905e886df971",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
          "commitDate": "08/09/16 11:54 AM",
          "commitName": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "06/04/16 10:50 PM",
          "commitNameOld": "3c18a53cbd2efabb2ad108d63a0b0b558424115f",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 154.54,
          "commitsBetweenForRepo": 1132,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,13 +1,20 @@\n-  public static void finalizeDecodeInputs(final byte[][] decodeInputs,\n+  public static void finalizeDecodeInputs(final ByteBuffer[] decodeInputs,\n                                           AlignedStripe alignedStripe) {\n     for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n       final StripingChunk chunk \u003d alignedStripe.chunks[i];\n       if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n-        chunk.copyTo(decodeInputs[i]);\n+        if (chunk.useChunkBuffer()) {\n+          chunk.getChunkBuffer().copyTo(decodeInputs[i]);\n+        } else {\n+          chunk.getByteBuffer().flip();\n+        }\n       } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n-        Arrays.fill(decodeInputs[i], (byte) 0);\n+        //ZERO it. Will be better handled in other following issue.\n+        byte[] emptyBytes \u003d new byte[decodeInputs[i].limit()];\n+        decodeInputs[i].put(emptyBytes);\n+        decodeInputs[i].flip();\n       } else {\n         decodeInputs[i] \u003d null;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void finalizeDecodeInputs(final ByteBuffer[] decodeInputs,\n                                          AlignedStripe alignedStripe) {\n    for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n      final StripingChunk chunk \u003d alignedStripe.chunks[i];\n      if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n        if (chunk.useChunkBuffer()) {\n          chunk.getChunkBuffer().copyTo(decodeInputs[i]);\n        } else {\n          chunk.getByteBuffer().flip();\n        }\n      } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n        //ZERO it. Will be better handled in other following issue.\n        byte[] emptyBytes \u003d new byte[decodeInputs[i].limit()];\n        decodeInputs[i].put(emptyBytes);\n        decodeInputs[i].flip();\n      } else {\n        decodeInputs[i] \u003d null;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
          "extendedDetails": {
            "oldValue": "[decodeInputs-byte[][](modifiers-final), alignedStripe-AlignedStripe]",
            "newValue": "[decodeInputs-ByteBuffer[](modifiers-final), alignedStripe-AlignedStripe]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
          "commitDate": "08/09/16 11:54 AM",
          "commitName": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "06/04/16 10:50 PM",
          "commitNameOld": "3c18a53cbd2efabb2ad108d63a0b0b558424115f",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 154.54,
          "commitsBetweenForRepo": 1132,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,13 +1,20 @@\n-  public static void finalizeDecodeInputs(final byte[][] decodeInputs,\n+  public static void finalizeDecodeInputs(final ByteBuffer[] decodeInputs,\n                                           AlignedStripe alignedStripe) {\n     for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n       final StripingChunk chunk \u003d alignedStripe.chunks[i];\n       if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n-        chunk.copyTo(decodeInputs[i]);\n+        if (chunk.useChunkBuffer()) {\n+          chunk.getChunkBuffer().copyTo(decodeInputs[i]);\n+        } else {\n+          chunk.getByteBuffer().flip();\n+        }\n       } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n-        Arrays.fill(decodeInputs[i], (byte) 0);\n+        //ZERO it. Will be better handled in other following issue.\n+        byte[] emptyBytes \u003d new byte[decodeInputs[i].limit()];\n+        decodeInputs[i].put(emptyBytes);\n+        decodeInputs[i].flip();\n       } else {\n         decodeInputs[i] \u003d null;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void finalizeDecodeInputs(final ByteBuffer[] decodeInputs,\n                                          AlignedStripe alignedStripe) {\n    for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n      final StripingChunk chunk \u003d alignedStripe.chunks[i];\n      if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n        if (chunk.useChunkBuffer()) {\n          chunk.getChunkBuffer().copyTo(decodeInputs[i]);\n        } else {\n          chunk.getByteBuffer().flip();\n        }\n      } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n        //ZERO it. Will be better handled in other following issue.\n        byte[] emptyBytes \u003d new byte[decodeInputs[i].limit()];\n        decodeInputs[i].put(emptyBytes);\n        decodeInputs[i].flip();\n      } else {\n        decodeInputs[i] \u003d null;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
          "extendedDetails": {}
        }
      ]
    },
    "c201cf951d5adefefe7c68e882a0c07962248577": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-12040. Adjust inputs order for the decode API in raw erasure coder. (Kai Zheng via yliu)\n",
      "commitDate": "28/10/15 1:18 AM",
      "commitName": "c201cf951d5adefefe7c68e882a0c07962248577",
      "commitAuthor": "yliu",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-12040. Adjust inputs order for the decode API in raw erasure coder. (Kai Zheng via yliu)\n",
          "commitDate": "28/10/15 1:18 AM",
          "commitName": "c201cf951d5adefefe7c68e882a0c07962248577",
          "commitAuthor": "yliu",
          "commitDateOld": "03/10/15 11:38 AM",
          "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 24.57,
          "commitsBetweenForRepo": 211,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,13 @@\n   public static void finalizeDecodeInputs(final byte[][] decodeInputs,\n-      int dataBlkNum, int parityBlkNum, AlignedStripe alignedStripe) {\n+                                          AlignedStripe alignedStripe) {\n     for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n       final StripingChunk chunk \u003d alignedStripe.chunks[i];\n-      final int decodeIndex \u003d convertIndex4Decode(i, dataBlkNum, parityBlkNum);\n       if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n-        chunk.copyTo(decodeInputs[decodeIndex]);\n+        chunk.copyTo(decodeInputs[i]);\n       } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n-        Arrays.fill(decodeInputs[decodeIndex], (byte) 0);\n+        Arrays.fill(decodeInputs[i], (byte) 0);\n       } else {\n-        decodeInputs[decodeIndex] \u003d null;\n+        decodeInputs[i] \u003d null;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void finalizeDecodeInputs(final byte[][] decodeInputs,\n                                          AlignedStripe alignedStripe) {\n    for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n      final StripingChunk chunk \u003d alignedStripe.chunks[i];\n      if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n        chunk.copyTo(decodeInputs[i]);\n      } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n        Arrays.fill(decodeInputs[i], (byte) 0);\n      } else {\n        decodeInputs[i] \u003d null;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
          "extendedDetails": {
            "oldValue": "[decodeInputs-byte[][](modifiers-final), dataBlkNum-int, parityBlkNum-int, alignedStripe-AlignedStripe]",
            "newValue": "[decodeInputs-byte[][](modifiers-final), alignedStripe-AlignedStripe]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-12040. Adjust inputs order for the decode API in raw erasure coder. (Kai Zheng via yliu)\n",
          "commitDate": "28/10/15 1:18 AM",
          "commitName": "c201cf951d5adefefe7c68e882a0c07962248577",
          "commitAuthor": "yliu",
          "commitDateOld": "03/10/15 11:38 AM",
          "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 24.57,
          "commitsBetweenForRepo": 211,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,13 @@\n   public static void finalizeDecodeInputs(final byte[][] decodeInputs,\n-      int dataBlkNum, int parityBlkNum, AlignedStripe alignedStripe) {\n+                                          AlignedStripe alignedStripe) {\n     for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n       final StripingChunk chunk \u003d alignedStripe.chunks[i];\n-      final int decodeIndex \u003d convertIndex4Decode(i, dataBlkNum, parityBlkNum);\n       if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n-        chunk.copyTo(decodeInputs[decodeIndex]);\n+        chunk.copyTo(decodeInputs[i]);\n       } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n-        Arrays.fill(decodeInputs[decodeIndex], (byte) 0);\n+        Arrays.fill(decodeInputs[i], (byte) 0);\n       } else {\n-        decodeInputs[decodeIndex] \u003d null;\n+        decodeInputs[i] \u003d null;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void finalizeDecodeInputs(final byte[][] decodeInputs,\n                                          AlignedStripe alignedStripe) {\n    for (int i \u003d 0; i \u003c alignedStripe.chunks.length; i++) {\n      final StripingChunk chunk \u003d alignedStripe.chunks[i];\n      if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.FETCHED) {\n        chunk.copyTo(decodeInputs[i]);\n      } else if (chunk !\u003d null \u0026\u0026 chunk.state \u003d\u003d StripingChunk.ALLZERO) {\n        Arrays.fill(decodeInputs[i], (byte) 0);\n      } else {\n        decodeInputs[i] \u003d null;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}