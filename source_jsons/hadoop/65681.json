{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DumpS3GuardDynamoTable.java",
  "functionName": "execute",
  "functionId": "execute",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DumpS3GuardDynamoTable.java",
  "functionStartLine": 185,
  "functionEndLine": 257,
  "numCommitsSeen": 2,
  "timeTaken": 1098,
  "changeHistory": [
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0"
  ],
  "changeHistoryShort": {
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-16384: S3A: Avoid inconsistencies between DDB and S3.\n\nContributed by Steve Loughran\n\nContains\n\n- HADOOP-16397. Hadoop S3Guard Prune command to support a -tombstone option.\n- HADOOP-16406. ITestDynamoDBMetadataStore.testProvisionTable times out intermittently\n\nThis patch doesn\u0027t fix the underlying problem but it\n\n* changes some tests to clean up better\n* does a lot more in logging operations in against DDB, if enabled\n* adds an entry point to dump the state of the metastore and s3 tables (precursor to fsck)\n* adds a purge entry point to help clean up after a test run has got a store into a mess\n* s3guard prune command adds -tombstone option to only clear tombstones\n\nThe outcome is that tests should pass consistently and if problems occur we have better diagnostics.\n\nChange-Id: I3eca3f5529d7f6fec398c0ff0472919f08f054eb\n",
      "commitDate": "12/07/19 5:02 AM",
      "commitName": "b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,73 @@\n+  public int execute() throws ServiceLaunchException, IOException {\n+\n+    try {\n+      final File scanFile \u003d new File(\n+          destPath + SCAN_CSV).getCanonicalFile();\n+      File parentDir \u003d scanFile.getParentFile();\n+      if (!parentDir.mkdirs() \u0026\u0026 !parentDir.isDirectory()) {\n+        throw new PathIOException(parentDir.toString(),\n+            \"Could not create destination directory\");\n+      }\n+\n+      try (CsvFile csv \u003d new CsvFile(scanFile);\n+           DurationInfo ignored \u003d new DurationInfo(LOG,\n+               \"scanFile dump to %s\", scanFile)) {\n+        scanEntryResult \u003d scanMetastore(csv);\n+      }\n+\n+      if (getFilesystem() !\u003d null) {\n+\n+        Path basePath \u003d getFilesystem().qualify(new Path(getUri()));\n+\n+        final File destFile \u003d new File(destPath + STORE_CSV)\n+            .getCanonicalFile();\n+        LOG.info(\"Writing Store details to {}\", destFile);\n+        try (CsvFile csv \u003d new CsvFile(destFile);\n+             DurationInfo ignored \u003d new DurationInfo(LOG, \"List metastore\")) {\n+\n+          LOG.info(\"Base path: {}\", basePath);\n+          dumpMetastore(csv, basePath);\n+        }\n+\n+        // these operations all update the metastore as they list,\n+        // that is: they are side-effecting.\n+        final File treewalkFile \u003d new File(destPath + TREE_CSV)\n+            .getCanonicalFile();\n+\n+        try (CsvFile csv \u003d new CsvFile(treewalkFile);\n+             DurationInfo ignored \u003d new DurationInfo(LOG,\n+                 \"Treewalk to %s\", treewalkFile)) {\n+          treewalkCount \u003d treewalkFilesystem(csv, basePath);\n+        }\n+        final File flatlistFile \u003d new File(\n+            destPath + FLAT_CSV).getCanonicalFile();\n+\n+        try (CsvFile csv \u003d new CsvFile(flatlistFile);\n+             DurationInfo ignored \u003d new DurationInfo(LOG,\n+                 \"Flat list to %s\", flatlistFile)) {\n+          listStatusCount \u003d listStatusFilesystem(csv, basePath);\n+        }\n+        final File rawFile \u003d new File(\n+            destPath + RAW_CSV).getCanonicalFile();\n+\n+        try (CsvFile csv \u003d new CsvFile(rawFile);\n+             DurationInfo ignored \u003d new DurationInfo(LOG,\n+                 \"Raw dump to %s\", rawFile)) {\n+          rawObjectStoreCount \u003d dumpRawS3ObjectStore(csv);\n+        }\n+        final File scanFile2 \u003d new File(\n+            destPath + SCAN2_CSV).getCanonicalFile();\n+\n+        try (CsvFile csv \u003d new CsvFile(scanFile);\n+             DurationInfo ignored \u003d new DurationInfo(LOG,\n+                 \"scanFile dump to %s\", scanFile2)) {\n+          secondScanResult \u003d scanMetastore(csv);\n+        }\n+      }\n+\n+      return LauncherExitCodes.EXIT_SUCCESS;\n+    } catch (IOException | RuntimeException e) {\n+      LOG.error(\"failure\", e);\n+      throw e;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public int execute() throws ServiceLaunchException, IOException {\n\n    try {\n      final File scanFile \u003d new File(\n          destPath + SCAN_CSV).getCanonicalFile();\n      File parentDir \u003d scanFile.getParentFile();\n      if (!parentDir.mkdirs() \u0026\u0026 !parentDir.isDirectory()) {\n        throw new PathIOException(parentDir.toString(),\n            \"Could not create destination directory\");\n      }\n\n      try (CsvFile csv \u003d new CsvFile(scanFile);\n           DurationInfo ignored \u003d new DurationInfo(LOG,\n               \"scanFile dump to %s\", scanFile)) {\n        scanEntryResult \u003d scanMetastore(csv);\n      }\n\n      if (getFilesystem() !\u003d null) {\n\n        Path basePath \u003d getFilesystem().qualify(new Path(getUri()));\n\n        final File destFile \u003d new File(destPath + STORE_CSV)\n            .getCanonicalFile();\n        LOG.info(\"Writing Store details to {}\", destFile);\n        try (CsvFile csv \u003d new CsvFile(destFile);\n             DurationInfo ignored \u003d new DurationInfo(LOG, \"List metastore\")) {\n\n          LOG.info(\"Base path: {}\", basePath);\n          dumpMetastore(csv, basePath);\n        }\n\n        // these operations all update the metastore as they list,\n        // that is: they are side-effecting.\n        final File treewalkFile \u003d new File(destPath + TREE_CSV)\n            .getCanonicalFile();\n\n        try (CsvFile csv \u003d new CsvFile(treewalkFile);\n             DurationInfo ignored \u003d new DurationInfo(LOG,\n                 \"Treewalk to %s\", treewalkFile)) {\n          treewalkCount \u003d treewalkFilesystem(csv, basePath);\n        }\n        final File flatlistFile \u003d new File(\n            destPath + FLAT_CSV).getCanonicalFile();\n\n        try (CsvFile csv \u003d new CsvFile(flatlistFile);\n             DurationInfo ignored \u003d new DurationInfo(LOG,\n                 \"Flat list to %s\", flatlistFile)) {\n          listStatusCount \u003d listStatusFilesystem(csv, basePath);\n        }\n        final File rawFile \u003d new File(\n            destPath + RAW_CSV).getCanonicalFile();\n\n        try (CsvFile csv \u003d new CsvFile(rawFile);\n             DurationInfo ignored \u003d new DurationInfo(LOG,\n                 \"Raw dump to %s\", rawFile)) {\n          rawObjectStoreCount \u003d dumpRawS3ObjectStore(csv);\n        }\n        final File scanFile2 \u003d new File(\n            destPath + SCAN2_CSV).getCanonicalFile();\n\n        try (CsvFile csv \u003d new CsvFile(scanFile);\n             DurationInfo ignored \u003d new DurationInfo(LOG,\n                 \"scanFile dump to %s\", scanFile2)) {\n          secondScanResult \u003d scanMetastore(csv);\n        }\n      }\n\n      return LauncherExitCodes.EXIT_SUCCESS;\n    } catch (IOException | RuntimeException e) {\n      LOG.error(\"failure\", e);\n      throw e;\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DumpS3GuardDynamoTable.java"
    }
  }
}