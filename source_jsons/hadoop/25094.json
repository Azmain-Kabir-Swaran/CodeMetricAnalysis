{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ClientServiceDelegate.java",
  "functionName": "getTaskCompletionEvents",
  "functionId": "getTaskCompletionEvents___arg0-JobID__arg1-int__arg2-int",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
  "functionStartLine": 388,
  "functionEndLine": 404,
  "numCommitsSeen": 52,
  "timeTaken": 9685,
  "changeHistory": [
    "0d2bb0623696c2cc822cb44e431345b2c773dbff",
    "f2b91a8367a762091482074505618b570a520b19",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "0d2bb0623696c2cc822cb44e431345b2c773dbff": "Ymodifierchange",
    "f2b91a8367a762091482074505618b570a520b19": "Ymultichange(Ymodifierchange,Ybodychange)",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Yexceptionschange,Ybodychange,Yrename,Yparameterchange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "0d2bb0623696c2cc822cb44e431345b2c773dbff": {
      "type": "Ymodifierchange",
      "commitMessage": "MAPREDUCE-3054. Unable to kill submitted jobs. (mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1176600 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/11 1:30 PM",
      "commitName": "0d2bb0623696c2cc822cb44e431345b2c773dbff",
      "commitAuthor": "Mahadev Konar",
      "commitDateOld": "25/09/11 7:46 AM",
      "commitNameOld": "a5c9ede1433871fcf4e2e802ee2a65950ecd1e72",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 2.24,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,17 @@\n-  TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1, int arg2)\n+  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1, int arg2)\n       throws IOException, InterruptedException {\n     org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter\n         .toYarn(arg0);\n     GetTaskAttemptCompletionEventsRequest request \u003d recordFactory\n         .newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n     request.setJobId(jobID);\n     request.setFromEventId(arg1);\n     request.setMaxEvents(arg2);\n     List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d \n       ((GetTaskAttemptCompletionEventsResponse) invoke(\n         \"getTaskAttemptCompletionEvents\", GetTaskAttemptCompletionEventsRequest.class, request)).\n         getCompletionEventList();\n     return TypeConverter\n         .fromYarn(list\n             .toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1, int arg2)\n      throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter\n        .toYarn(arg0);\n    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory\n        .newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    request.setJobId(jobID);\n    request.setFromEventId(arg1);\n    request.setMaxEvents(arg2);\n    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d \n      ((GetTaskAttemptCompletionEventsResponse) invoke(\n        \"getTaskAttemptCompletionEvents\", GetTaskAttemptCompletionEventsRequest.class, request)).\n        getCompletionEventList();\n    return TypeConverter\n        .fromYarn(list\n            .toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[public]"
      }
    },
    "f2b91a8367a762091482074505618b570a520b19": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": " MAPREDUCE-2807. Fix AM restart and client redirection. Contributed by Sharad Agarwal.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161408 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 11:35 PM",
      "commitName": "f2b91a8367a762091482074505618b570a520b19",
      "commitAuthor": "Sharad Agarwal",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": " MAPREDUCE-2807. Fix AM restart and client redirection. Contributed by Sharad Agarwal.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161408 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "24/08/11 11:35 PM",
          "commitName": "f2b91a8367a762091482074505618b570a520b19",
          "commitAuthor": "Sharad Agarwal",
          "commitDateOld": "24/08/11 5:14 PM",
          "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
          "commitAuthorOld": "Arun Murthy",
          "daysBetweenCommits": 0.26,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,17 @@\n-  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n-      int arg2) throws IOException, InterruptedException {\n-    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n-    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n-    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n-    MRClientProtocol protocol;\n-    try {\n-      request.setJobId(jobID);\n-      request.setFromEventId(arg1);\n-      request.setMaxEvents(arg2);\n-      protocol \u003d getProxy(arg0);\n-      /** This is hack to get around the issue of faking jobstatus while the AM\n-       * is coming up.\n-       */\n-      if (protocol \u003d\u003d null) {\n-        return new TaskCompletionEvent[0];\n-      }\n-      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n-    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n-      LOG.warn(RPCUtil.toString(yre));\n-      throw yre;\n-    } catch(Exception e) {\n-      LOG.debug(\"Failed to contact application master \", e);\n-      try {\n-        request.setJobId(jobID);\n-        request.setFromEventId(arg1);\n-        request.setMaxEvents(arg2);\n-        protocol \u003d getRefreshedProxy(arg0);\n-        if (protocol \u003d\u003d null) {\n-          return new TaskCompletionEvent[0];\n-        }\n-        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n-      } catch(YarnRemoteException yre) {\n-        LOG.warn(RPCUtil.toString(yre));\n-        throw yre;\n-      }\n-    }\n-    return TypeConverter.fromYarn(\n-        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n+  TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1, int arg2)\n+      throws IOException, InterruptedException {\n+    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter\n+        .toYarn(arg0);\n+    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory\n+        .newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n+    request.setJobId(jobID);\n+    request.setFromEventId(arg1);\n+    request.setMaxEvents(arg2);\n+    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d \n+      ((GetTaskAttemptCompletionEventsResponse) invoke(\n+        \"getTaskAttemptCompletionEvents\", GetTaskAttemptCompletionEventsRequest.class, request)).\n+        getCompletionEventList();\n+    return TypeConverter\n+        .fromYarn(list\n+            .toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1, int arg2)\n      throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter\n        .toYarn(arg0);\n    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory\n        .newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    request.setJobId(jobID);\n    request.setFromEventId(arg1);\n    request.setMaxEvents(arg2);\n    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d \n      ((GetTaskAttemptCompletionEventsResponse) invoke(\n        \"getTaskAttemptCompletionEvents\", GetTaskAttemptCompletionEventsRequest.class, request)).\n        getCompletionEventList();\n    return TypeConverter\n        .fromYarn(list\n            .toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": " MAPREDUCE-2807. Fix AM restart and client redirection. Contributed by Sharad Agarwal.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161408 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "24/08/11 11:35 PM",
          "commitName": "f2b91a8367a762091482074505618b570a520b19",
          "commitAuthor": "Sharad Agarwal",
          "commitDateOld": "24/08/11 5:14 PM",
          "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
          "commitAuthorOld": "Arun Murthy",
          "daysBetweenCommits": 0.26,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,17 @@\n-  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n-      int arg2) throws IOException, InterruptedException {\n-    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n-    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n-    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n-    MRClientProtocol protocol;\n-    try {\n-      request.setJobId(jobID);\n-      request.setFromEventId(arg1);\n-      request.setMaxEvents(arg2);\n-      protocol \u003d getProxy(arg0);\n-      /** This is hack to get around the issue of faking jobstatus while the AM\n-       * is coming up.\n-       */\n-      if (protocol \u003d\u003d null) {\n-        return new TaskCompletionEvent[0];\n-      }\n-      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n-    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n-      LOG.warn(RPCUtil.toString(yre));\n-      throw yre;\n-    } catch(Exception e) {\n-      LOG.debug(\"Failed to contact application master \", e);\n-      try {\n-        request.setJobId(jobID);\n-        request.setFromEventId(arg1);\n-        request.setMaxEvents(arg2);\n-        protocol \u003d getRefreshedProxy(arg0);\n-        if (protocol \u003d\u003d null) {\n-          return new TaskCompletionEvent[0];\n-        }\n-        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n-      } catch(YarnRemoteException yre) {\n-        LOG.warn(RPCUtil.toString(yre));\n-        throw yre;\n-      }\n-    }\n-    return TypeConverter.fromYarn(\n-        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n+  TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1, int arg2)\n+      throws IOException, InterruptedException {\n+    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter\n+        .toYarn(arg0);\n+    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory\n+        .newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n+    request.setJobId(jobID);\n+    request.setFromEventId(arg1);\n+    request.setMaxEvents(arg2);\n+    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d \n+      ((GetTaskAttemptCompletionEventsResponse) invoke(\n+        \"getTaskAttemptCompletionEvents\", GetTaskAttemptCompletionEventsRequest.class, request)).\n+        getCompletionEventList();\n+    return TypeConverter\n+        .fromYarn(list\n+            .toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1, int arg2)\n      throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter\n        .toYarn(arg0);\n    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory\n        .newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    request.setJobId(jobID);\n    request.setFromEventId(arg1);\n    request.setMaxEvents(arg2);\n    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d \n      ((GetTaskAttemptCompletionEventsResponse) invoke(\n        \"getTaskAttemptCompletionEvents\", GetTaskAttemptCompletionEventsRequest.class, request)).\n        getCompletionEventList();\n    return TypeConverter\n        .fromYarn(list\n            .toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
          "extendedDetails": {}
        }
      ]
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n      int arg2) throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    MRClientProtocol protocol;\n    try {\n      request.setJobId(jobID);\n      request.setFromEventId(arg1);\n      request.setMaxEvents(arg2);\n      protocol \u003d getProxy(arg0);\n      /** This is hack to get around the issue of faking jobstatus while the AM\n       * is coming up.\n       */\n      if (protocol \u003d\u003d null) {\n        return new TaskCompletionEvent[0];\n      }\n      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n      LOG.warn(RPCUtil.toString(yre));\n      throw yre;\n    } catch(Exception e) {\n      LOG.debug(\"Failed to contact application master \", e);\n      try {\n        request.setJobId(jobID);\n        request.setFromEventId(arg1);\n        request.setMaxEvents(arg2);\n        protocol \u003d getRefreshedProxy(arg0);\n        if (protocol \u003d\u003d null) {\n          return new TaskCompletionEvent[0];\n        }\n        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n      } catch(YarnRemoteException yre) {\n        LOG.warn(RPCUtil.toString(yre));\n        throw yre;\n      }\n    }\n    return TypeConverter.fromYarn(\n        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Yexceptionschange,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,40 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n+      int arg2) throws IOException, InterruptedException {\n+    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n+    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n+    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n+    MRClientProtocol protocol;\n+    try {\n+      request.setJobId(jobID);\n+      request.setFromEventId(arg1);\n+      request.setMaxEvents(arg2);\n+      protocol \u003d getProxy(arg0);\n+      /** This is hack to get around the issue of faking jobstatus while the AM\n+       * is coming up.\n+       */\n+      if (protocol \u003d\u003d null) {\n+        return new TaskCompletionEvent[0];\n+      }\n+      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n+      LOG.warn(RPCUtil.toString(yre));\n+      throw yre;\n+    } catch(Exception e) {\n+      LOG.debug(\"Failed to contact application master \", e);\n+      try {\n+        request.setJobId(jobID);\n+        request.setFromEventId(arg1);\n+        request.setMaxEvents(arg2);\n+        protocol \u003d getRefreshedProxy(arg0);\n+        if (protocol \u003d\u003d null) {\n+          return new TaskCompletionEvent[0];\n+        }\n+        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+      } catch(YarnRemoteException yre) {\n+        LOG.warn(RPCUtil.toString(yre));\n+        throw yre;\n       }\n     }\n-    return numNewMaps;\n+    return TypeConverter.fromYarn(\n+        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n      int arg2) throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    MRClientProtocol protocol;\n    try {\n      request.setJobId(jobID);\n      request.setFromEventId(arg1);\n      request.setMaxEvents(arg2);\n      protocol \u003d getProxy(arg0);\n      /** This is hack to get around the issue of faking jobstatus while the AM\n       * is coming up.\n       */\n      if (protocol \u003d\u003d null) {\n        return new TaskCompletionEvent[0];\n      }\n      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n      LOG.warn(RPCUtil.toString(yre));\n      throw yre;\n    } catch(Exception e) {\n      LOG.debug(\"Failed to contact application master \", e);\n      try {\n        request.setJobId(jobID);\n        request.setFromEventId(arg1);\n        request.setMaxEvents(arg2);\n        protocol \u003d getRefreshedProxy(arg0);\n        if (protocol \u003d\u003d null) {\n          return new TaskCompletionEvent[0];\n        }\n        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n      } catch(YarnRemoteException yre) {\n        LOG.warn(RPCUtil.toString(yre));\n        throw yre;\n      }\n    }\n    return TypeConverter.fromYarn(\n        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
          "extendedDetails": {
            "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
            "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
            "oldMethodName": "getMapCompletionEvents",
            "newMethodName": "getTaskCompletionEvents"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,40 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n+      int arg2) throws IOException, InterruptedException {\n+    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n+    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n+    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n+    MRClientProtocol protocol;\n+    try {\n+      request.setJobId(jobID);\n+      request.setFromEventId(arg1);\n+      request.setMaxEvents(arg2);\n+      protocol \u003d getProxy(arg0);\n+      /** This is hack to get around the issue of faking jobstatus while the AM\n+       * is coming up.\n+       */\n+      if (protocol \u003d\u003d null) {\n+        return new TaskCompletionEvent[0];\n+      }\n+      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n+      LOG.warn(RPCUtil.toString(yre));\n+      throw yre;\n+    } catch(Exception e) {\n+      LOG.debug(\"Failed to contact application master \", e);\n+      try {\n+        request.setJobId(jobID);\n+        request.setFromEventId(arg1);\n+        request.setMaxEvents(arg2);\n+        protocol \u003d getRefreshedProxy(arg0);\n+        if (protocol \u003d\u003d null) {\n+          return new TaskCompletionEvent[0];\n+        }\n+        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+      } catch(YarnRemoteException yre) {\n+        LOG.warn(RPCUtil.toString(yre));\n+        throw yre;\n       }\n     }\n-    return numNewMaps;\n+    return TypeConverter.fromYarn(\n+        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n      int arg2) throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    MRClientProtocol protocol;\n    try {\n      request.setJobId(jobID);\n      request.setFromEventId(arg1);\n      request.setMaxEvents(arg2);\n      protocol \u003d getProxy(arg0);\n      /** This is hack to get around the issue of faking jobstatus while the AM\n       * is coming up.\n       */\n      if (protocol \u003d\u003d null) {\n        return new TaskCompletionEvent[0];\n      }\n      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n      LOG.warn(RPCUtil.toString(yre));\n      throw yre;\n    } catch(Exception e) {\n      LOG.debug(\"Failed to contact application master \", e);\n      try {\n        request.setJobId(jobID);\n        request.setFromEventId(arg1);\n        request.setMaxEvents(arg2);\n        protocol \u003d getRefreshedProxy(arg0);\n        if (protocol \u003d\u003d null) {\n          return new TaskCompletionEvent[0];\n        }\n        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n      } catch(YarnRemoteException yre) {\n        LOG.warn(RPCUtil.toString(yre));\n        throw yre;\n      }\n    }\n    return TypeConverter.fromYarn(\n        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
          "extendedDetails": {
            "oldValue": "int",
            "newValue": "TaskCompletionEvent[]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,40 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n+      int arg2) throws IOException, InterruptedException {\n+    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n+    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n+    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n+    MRClientProtocol protocol;\n+    try {\n+      request.setJobId(jobID);\n+      request.setFromEventId(arg1);\n+      request.setMaxEvents(arg2);\n+      protocol \u003d getProxy(arg0);\n+      /** This is hack to get around the issue of faking jobstatus while the AM\n+       * is coming up.\n+       */\n+      if (protocol \u003d\u003d null) {\n+        return new TaskCompletionEvent[0];\n+      }\n+      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n+      LOG.warn(RPCUtil.toString(yre));\n+      throw yre;\n+    } catch(Exception e) {\n+      LOG.debug(\"Failed to contact application master \", e);\n+      try {\n+        request.setJobId(jobID);\n+        request.setFromEventId(arg1);\n+        request.setMaxEvents(arg2);\n+        protocol \u003d getRefreshedProxy(arg0);\n+        if (protocol \u003d\u003d null) {\n+          return new TaskCompletionEvent[0];\n+        }\n+        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+      } catch(YarnRemoteException yre) {\n+        LOG.warn(RPCUtil.toString(yre));\n+        throw yre;\n       }\n     }\n-    return numNewMaps;\n+    return TypeConverter.fromYarn(\n+        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n      int arg2) throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    MRClientProtocol protocol;\n    try {\n      request.setJobId(jobID);\n      request.setFromEventId(arg1);\n      request.setMaxEvents(arg2);\n      protocol \u003d getProxy(arg0);\n      /** This is hack to get around the issue of faking jobstatus while the AM\n       * is coming up.\n       */\n      if (protocol \u003d\u003d null) {\n        return new TaskCompletionEvent[0];\n      }\n      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n      LOG.warn(RPCUtil.toString(yre));\n      throw yre;\n    } catch(Exception e) {\n      LOG.debug(\"Failed to contact application master \", e);\n      try {\n        request.setJobId(jobID);\n        request.setFromEventId(arg1);\n        request.setMaxEvents(arg2);\n        protocol \u003d getRefreshedProxy(arg0);\n        if (protocol \u003d\u003d null) {\n          return new TaskCompletionEvent[0];\n        }\n        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n      } catch(YarnRemoteException yre) {\n        LOG.warn(RPCUtil.toString(yre));\n        throw yre;\n      }\n    }\n    return TypeConverter.fromYarn(\n        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,40 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n+      int arg2) throws IOException, InterruptedException {\n+    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n+    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n+    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n+    MRClientProtocol protocol;\n+    try {\n+      request.setJobId(jobID);\n+      request.setFromEventId(arg1);\n+      request.setMaxEvents(arg2);\n+      protocol \u003d getProxy(arg0);\n+      /** This is hack to get around the issue of faking jobstatus while the AM\n+       * is coming up.\n+       */\n+      if (protocol \u003d\u003d null) {\n+        return new TaskCompletionEvent[0];\n+      }\n+      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n+      LOG.warn(RPCUtil.toString(yre));\n+      throw yre;\n+    } catch(Exception e) {\n+      LOG.debug(\"Failed to contact application master \", e);\n+      try {\n+        request.setJobId(jobID);\n+        request.setFromEventId(arg1);\n+        request.setMaxEvents(arg2);\n+        protocol \u003d getRefreshedProxy(arg0);\n+        if (protocol \u003d\u003d null) {\n+          return new TaskCompletionEvent[0];\n+        }\n+        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+      } catch(YarnRemoteException yre) {\n+        LOG.warn(RPCUtil.toString(yre));\n+        throw yre;\n       }\n     }\n-    return numNewMaps;\n+    return TypeConverter.fromYarn(\n+        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n      int arg2) throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    MRClientProtocol protocol;\n    try {\n      request.setJobId(jobID);\n      request.setFromEventId(arg1);\n      request.setMaxEvents(arg2);\n      protocol \u003d getProxy(arg0);\n      /** This is hack to get around the issue of faking jobstatus while the AM\n       * is coming up.\n       */\n      if (protocol \u003d\u003d null) {\n        return new TaskCompletionEvent[0];\n      }\n      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n      LOG.warn(RPCUtil.toString(yre));\n      throw yre;\n    } catch(Exception e) {\n      LOG.debug(\"Failed to contact application master \", e);\n      try {\n        request.setJobId(jobID);\n        request.setFromEventId(arg1);\n        request.setMaxEvents(arg2);\n        protocol \u003d getRefreshedProxy(arg0);\n        if (protocol \u003d\u003d null) {\n          return new TaskCompletionEvent[0];\n        }\n        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n      } catch(YarnRemoteException yre) {\n        LOG.warn(RPCUtil.toString(yre));\n        throw yre;\n      }\n    }\n    return TypeConverter.fromYarn(\n        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
          "extendedDetails": {
            "oldValue": "[IOException]",
            "newValue": "[IOException, InterruptedException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,40 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n+      int arg2) throws IOException, InterruptedException {\n+    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n+    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n+    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n+    MRClientProtocol protocol;\n+    try {\n+      request.setJobId(jobID);\n+      request.setFromEventId(arg1);\n+      request.setMaxEvents(arg2);\n+      protocol \u003d getProxy(arg0);\n+      /** This is hack to get around the issue of faking jobstatus while the AM\n+       * is coming up.\n+       */\n+      if (protocol \u003d\u003d null) {\n+        return new TaskCompletionEvent[0];\n+      }\n+      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n+      LOG.warn(RPCUtil.toString(yre));\n+      throw yre;\n+    } catch(Exception e) {\n+      LOG.debug(\"Failed to contact application master \", e);\n+      try {\n+        request.setJobId(jobID);\n+        request.setFromEventId(arg1);\n+        request.setMaxEvents(arg2);\n+        protocol \u003d getRefreshedProxy(arg0);\n+        if (protocol \u003d\u003d null) {\n+          return new TaskCompletionEvent[0];\n+        }\n+        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+      } catch(YarnRemoteException yre) {\n+        LOG.warn(RPCUtil.toString(yre));\n+        throw yre;\n       }\n     }\n-    return numNewMaps;\n+    return TypeConverter.fromYarn(\n+        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n      int arg2) throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    MRClientProtocol protocol;\n    try {\n      request.setJobId(jobID);\n      request.setFromEventId(arg1);\n      request.setMaxEvents(arg2);\n      protocol \u003d getProxy(arg0);\n      /** This is hack to get around the issue of faking jobstatus while the AM\n       * is coming up.\n       */\n      if (protocol \u003d\u003d null) {\n        return new TaskCompletionEvent[0];\n      }\n      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n      LOG.warn(RPCUtil.toString(yre));\n      throw yre;\n    } catch(Exception e) {\n      LOG.debug(\"Failed to contact application master \", e);\n      try {\n        request.setJobId(jobID);\n        request.setFromEventId(arg1);\n        request.setMaxEvents(arg2);\n        protocol \u003d getRefreshedProxy(arg0);\n        if (protocol \u003d\u003d null) {\n          return new TaskCompletionEvent[0];\n        }\n        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n      } catch(YarnRemoteException yre) {\n        LOG.warn(RPCUtil.toString(yre));\n        throw yre;\n      }\n    }\n    return TypeConverter.fromYarn(\n        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,40 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n+      int arg2) throws IOException, InterruptedException {\n+    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n+    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n+    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n+    MRClientProtocol protocol;\n+    try {\n+      request.setJobId(jobID);\n+      request.setFromEventId(arg1);\n+      request.setMaxEvents(arg2);\n+      protocol \u003d getProxy(arg0);\n+      /** This is hack to get around the issue of faking jobstatus while the AM\n+       * is coming up.\n+       */\n+      if (protocol \u003d\u003d null) {\n+        return new TaskCompletionEvent[0];\n+      }\n+      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n+      LOG.warn(RPCUtil.toString(yre));\n+      throw yre;\n+    } catch(Exception e) {\n+      LOG.debug(\"Failed to contact application master \", e);\n+      try {\n+        request.setJobId(jobID);\n+        request.setFromEventId(arg1);\n+        request.setMaxEvents(arg2);\n+        protocol \u003d getRefreshedProxy(arg0);\n+        if (protocol \u003d\u003d null) {\n+          return new TaskCompletionEvent[0];\n+        }\n+        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+      } catch(YarnRemoteException yre) {\n+        LOG.warn(RPCUtil.toString(yre));\n+        throw yre;\n       }\n     }\n-    return numNewMaps;\n+    return TypeConverter.fromYarn(\n+        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n      int arg2) throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    MRClientProtocol protocol;\n    try {\n      request.setJobId(jobID);\n      request.setFromEventId(arg1);\n      request.setMaxEvents(arg2);\n      protocol \u003d getProxy(arg0);\n      /** This is hack to get around the issue of faking jobstatus while the AM\n       * is coming up.\n       */\n      if (protocol \u003d\u003d null) {\n        return new TaskCompletionEvent[0];\n      }\n      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n      LOG.warn(RPCUtil.toString(yre));\n      throw yre;\n    } catch(Exception e) {\n      LOG.debug(\"Failed to contact application master \", e);\n      try {\n        request.setJobId(jobID);\n        request.setFromEventId(arg1);\n        request.setMaxEvents(arg2);\n        protocol \u003d getRefreshedProxy(arg0);\n        if (protocol \u003d\u003d null) {\n          return new TaskCompletionEvent[0];\n        }\n        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n      } catch(YarnRemoteException yre) {\n        LOG.warn(RPCUtil.toString(yre));\n        throw yre;\n      }\n    }\n    return TypeConverter.fromYarn(\n        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
          "extendedDetails": {
            "oldValue": "getMapCompletionEvents",
            "newValue": "getTaskCompletionEvents"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,40 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n+      int arg2) throws IOException, InterruptedException {\n+    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n+    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n+    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n+    MRClientProtocol protocol;\n+    try {\n+      request.setJobId(jobID);\n+      request.setFromEventId(arg1);\n+      request.setMaxEvents(arg2);\n+      protocol \u003d getProxy(arg0);\n+      /** This is hack to get around the issue of faking jobstatus while the AM\n+       * is coming up.\n+       */\n+      if (protocol \u003d\u003d null) {\n+        return new TaskCompletionEvent[0];\n+      }\n+      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n+      LOG.warn(RPCUtil.toString(yre));\n+      throw yre;\n+    } catch(Exception e) {\n+      LOG.debug(\"Failed to contact application master \", e);\n+      try {\n+        request.setJobId(jobID);\n+        request.setFromEventId(arg1);\n+        request.setMaxEvents(arg2);\n+        protocol \u003d getRefreshedProxy(arg0);\n+        if (protocol \u003d\u003d null) {\n+          return new TaskCompletionEvent[0];\n+        }\n+        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n+      } catch(YarnRemoteException yre) {\n+        LOG.warn(RPCUtil.toString(yre));\n+        throw yre;\n       }\n     }\n-    return numNewMaps;\n+    return TypeConverter.fromYarn(\n+        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public TaskCompletionEvent[] getTaskCompletionEvents(JobID arg0, int arg1,\n      int arg2) throws IOException, InterruptedException {\n    org.apache.hadoop.mapreduce.v2.api.records.JobId jobID \u003d TypeConverter.toYarn(arg0);\n    List\u003corg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent\u003e list \u003d null;\n    GetTaskAttemptCompletionEventsRequest request \u003d recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\n    MRClientProtocol protocol;\n    try {\n      request.setJobId(jobID);\n      request.setFromEventId(arg1);\n      request.setMaxEvents(arg2);\n      protocol \u003d getProxy(arg0);\n      /** This is hack to get around the issue of faking jobstatus while the AM\n       * is coming up.\n       */\n      if (protocol \u003d\u003d null) {\n        return new TaskCompletionEvent[0];\n      }\n      list \u003d getProxy(arg0).getTaskAttemptCompletionEvents(request).getCompletionEventList();\n    } catch(YarnRemoteException yre) {//thrown by remote server, no need to redirect\n      LOG.warn(RPCUtil.toString(yre));\n      throw yre;\n    } catch(Exception e) {\n      LOG.debug(\"Failed to contact application master \", e);\n      try {\n        request.setJobId(jobID);\n        request.setFromEventId(arg1);\n        request.setMaxEvents(arg2);\n        protocol \u003d getRefreshedProxy(arg0);\n        if (protocol \u003d\u003d null) {\n          return new TaskCompletionEvent[0];\n        }\n        list \u003d protocol.getTaskAttemptCompletionEvents(request).getCompletionEventList();\n      } catch(YarnRemoteException yre) {\n        LOG.warn(RPCUtil.toString(yre));\n        throw yre;\n      }\n    }\n    return TypeConverter.fromYarn(\n        list.toArray(new org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[arg0-JobID, arg1-int, arg2-int]"
          }
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,63 @@\n+  private int getMapCompletionEvents() throws IOException {\n+    \n+    int numNewMaps \u003d 0;\n+    \n+    MapTaskCompletionEventsUpdate update \u003d \n+      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n+                                       reduce.getJobID(), \n+                                       fromEventId, \n+                                       MAX_EVENTS_TO_FETCH,\n+                                       (org.apache.hadoop.mapred.TaskAttemptID)\n+                                         reduce);\n+    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n+    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n+             fromEventId);\n+      \n+    // Check if the reset is required.\n+    // Since there is no ordering of the task completion events at the \n+    // reducer, the only option to sync with the new jobtracker is to reset \n+    // the events index\n+    if (update.shouldReset()) {\n+      fromEventId \u003d 0;\n+      scheduler.resetKnownMaps();\n+    }\n+    \n+    // Update the last seen event ID\n+    fromEventId +\u003d events.length;\n+    \n+    // Process the TaskCompletionEvents:\n+    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n+    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n+    //    fetching from those maps.\n+    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n+    //    outputs at all.\n+    for (TaskCompletionEvent event : events) {\n+      switch (event.getTaskStatus()) {\n+        case SUCCEEDED:\n+          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n+          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n+                                      u.toString(),\n+                                      event.getTaskAttemptId());\n+          numNewMaps ++;\n+          int duration \u003d event.getTaskRunTime();\n+          if (duration \u003e maxMapRuntime) {\n+            maxMapRuntime \u003d duration;\n+            scheduler.informMaxMapRunTime(maxMapRuntime);\n+          }\n+          break;\n+        case FAILED:\n+        case KILLED:\n+        case OBSOLETE:\n+          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n+          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n+                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n+          break;\n+        case TIPFAILED:\n+          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n+          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n+               event.getTaskAttemptId() + \"\u0027\");\n+          break;\n+      }\n+    }\n+    return numNewMaps;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private int getMapCompletionEvents() throws IOException {\n    \n    int numNewMaps \u003d 0;\n    \n    MapTaskCompletionEventsUpdate update \u003d \n      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n                                       reduce.getJobID(), \n                                       fromEventId, \n                                       MAX_EVENTS_TO_FETCH,\n                                       (org.apache.hadoop.mapred.TaskAttemptID)\n                                         reduce);\n    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n             fromEventId);\n      \n    // Check if the reset is required.\n    // Since there is no ordering of the task completion events at the \n    // reducer, the only option to sync with the new jobtracker is to reset \n    // the events index\n    if (update.shouldReset()) {\n      fromEventId \u003d 0;\n      scheduler.resetKnownMaps();\n    }\n    \n    // Update the last seen event ID\n    fromEventId +\u003d events.length;\n    \n    // Process the TaskCompletionEvents:\n    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n    //    fetching from those maps.\n    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n    //    outputs at all.\n    for (TaskCompletionEvent event : events) {\n      switch (event.getTaskStatus()) {\n        case SUCCEEDED:\n          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n                                      u.toString(),\n                                      event.getTaskAttemptId());\n          numNewMaps ++;\n          int duration \u003d event.getTaskRunTime();\n          if (duration \u003e maxMapRuntime) {\n            maxMapRuntime \u003d duration;\n            scheduler.informMaxMapRunTime(maxMapRuntime);\n          }\n          break;\n        case FAILED:\n        case KILLED:\n        case OBSOLETE:\n          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n          break;\n        case TIPFAILED:\n          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n               event.getTaskAttemptId() + \"\u0027\");\n          break;\n      }\n    }\n    return numNewMaps;\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java"
    }
  }
}