{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "InvalidateBlocks.java",
  "functionName": "invalidateWork",
  "functionId": "invalidateWork___dn-DatanodeDescriptor(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
  "functionStartLine": 271,
  "functionEndLine": 298,
  "numCommitsSeen": 35,
  "timeTaken": 3677,
  "changeHistory": [
    "55fc2d6485702a99c6d4bb261a720d1f0498af2b",
    "4e50dc976a92a9560630c87cfc4e4513916e5735",
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
    "a7f085d6bf499edf23e650a4f7211c53a442da0e",
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e",
    "8e5b5165c14486af6d5d73e7b4e591d4787ad8f2",
    "b7887f31fbe28d35005abdc439b2771b58c91225",
    "9a3f147fdd5421460889b266ead3a2300323cda2",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "513f17d115564e49124bb744cecf36d16a144ffc"
  ],
  "changeHistoryShort": {
    "55fc2d6485702a99c6d4bb261a720d1f0498af2b": "Ybodychange",
    "4e50dc976a92a9560630c87cfc4e4513916e5735": "Ybodychange",
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e": "Ybodychange",
    "a7f085d6bf499edf23e650a4f7211c53a442da0e": "Ybodychange",
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e": "Ymultichange(Yparameterchange,Ybodychange)",
    "8e5b5165c14486af6d5d73e7b4e591d4787ad8f2": "Ybodychange",
    "b7887f31fbe28d35005abdc439b2771b58c91225": "Ymodifierchange",
    "9a3f147fdd5421460889b266ead3a2300323cda2": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "513f17d115564e49124bb744cecf36d16a144ffc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "55fc2d6485702a99c6d4bb261a720d1f0498af2b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12891. Do not invalidate blocks if toInvalidate is empty. Contributed by Zsolt Venczel.\n",
      "commitDate": "11/12/17 4:43 PM",
      "commitName": "55fc2d6485702a99c6d4bb261a720d1f0498af2b",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "07/09/17 4:57 PM",
      "commitNameOld": "4e50dc976a92a9560630c87cfc4e4513916e5735",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 95.03,
      "commitsBetweenForRepo": 773,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,28 @@\n   synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n     final long delay \u003d getInvalidationDelay();\n     if (delay \u003e 0) {\n       BlockManager.LOG\n           .debug(\"Block deletion is delayed during NameNode startup. \"\n               + \"The deletion will start after {} ms.\", delay);\n       return null;\n     }\n \n     int remainingLimit \u003d blockInvalidateLimit;\n     final List\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003c\u003e();\n \n     if (nodeToBlocks.get(dn) !\u003d null) {\n       remainingLimit \u003d getBlocksToInvalidateByLimit(nodeToBlocks.get(dn),\n           toInvalidate, numBlocks, remainingLimit);\n     }\n     if ((remainingLimit \u003e 0) \u0026\u0026 (nodeToECBlocks.get(dn) !\u003d null)) {\n       getBlocksToInvalidateByLimit(nodeToECBlocks.get(dn),\n           toInvalidate, numECBlocks, remainingLimit);\n     }\n-    if (toInvalidate.size() \u003e 0 \u0026\u0026 getBlockSetsSize(dn) \u003d\u003d 0) {\n-      remove(dn);\n+    if (toInvalidate.size() \u003e 0) {\n+      if (getBlockSetsSize(dn) \u003d\u003d 0) {\n+        remove(dn);\n+      }\n+      dn.addBlocksToBeInvalidated(toInvalidate);\n     }\n-    dn.addBlocksToBeInvalidated(toInvalidate);\n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n    final long delay \u003d getInvalidationDelay();\n    if (delay \u003e 0) {\n      BlockManager.LOG\n          .debug(\"Block deletion is delayed during NameNode startup. \"\n              + \"The deletion will start after {} ms.\", delay);\n      return null;\n    }\n\n    int remainingLimit \u003d blockInvalidateLimit;\n    final List\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003c\u003e();\n\n    if (nodeToBlocks.get(dn) !\u003d null) {\n      remainingLimit \u003d getBlocksToInvalidateByLimit(nodeToBlocks.get(dn),\n          toInvalidate, numBlocks, remainingLimit);\n    }\n    if ((remainingLimit \u003e 0) \u0026\u0026 (nodeToECBlocks.get(dn) !\u003d null)) {\n      getBlocksToInvalidateByLimit(nodeToECBlocks.get(dn),\n          toInvalidate, numECBlocks, remainingLimit);\n    }\n    if (toInvalidate.size() \u003e 0) {\n      if (getBlockSetsSize(dn) \u003d\u003d 0) {\n        remove(dn);\n      }\n      dn.addBlocksToBeInvalidated(toInvalidate);\n    }\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "4e50dc976a92a9560630c87cfc4e4513916e5735": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12218. Addendum. Rename split EC / replicated block metrics in BlockManager.\n",
      "commitDate": "07/09/17 4:57 PM",
      "commitName": "4e50dc976a92a9560630c87cfc4e4513916e5735",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "14/06/17 10:44 AM",
      "commitNameOld": "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 85.26,
      "commitsBetweenForRepo": 589,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n     final long delay \u003d getInvalidationDelay();\n     if (delay \u003e 0) {\n       BlockManager.LOG\n           .debug(\"Block deletion is delayed during NameNode startup. \"\n               + \"The deletion will start after {} ms.\", delay);\n       return null;\n     }\n \n     int remainingLimit \u003d blockInvalidateLimit;\n     final List\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003c\u003e();\n \n     if (nodeToBlocks.get(dn) !\u003d null) {\n       remainingLimit \u003d getBlocksToInvalidateByLimit(nodeToBlocks.get(dn),\n           toInvalidate, numBlocks, remainingLimit);\n     }\n-    if ((remainingLimit \u003e 0) \u0026\u0026 (nodeToECBlockGroups.get(dn) !\u003d null)) {\n-      getBlocksToInvalidateByLimit(nodeToECBlockGroups.get(dn),\n-          toInvalidate, numECBlockGroups, remainingLimit);\n+    if ((remainingLimit \u003e 0) \u0026\u0026 (nodeToECBlocks.get(dn) !\u003d null)) {\n+      getBlocksToInvalidateByLimit(nodeToECBlocks.get(dn),\n+          toInvalidate, numECBlocks, remainingLimit);\n     }\n     if (toInvalidate.size() \u003e 0 \u0026\u0026 getBlockSetsSize(dn) \u003d\u003d 0) {\n       remove(dn);\n     }\n     dn.addBlocksToBeInvalidated(toInvalidate);\n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n    final long delay \u003d getInvalidationDelay();\n    if (delay \u003e 0) {\n      BlockManager.LOG\n          .debug(\"Block deletion is delayed during NameNode startup. \"\n              + \"The deletion will start after {} ms.\", delay);\n      return null;\n    }\n\n    int remainingLimit \u003d blockInvalidateLimit;\n    final List\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003c\u003e();\n\n    if (nodeToBlocks.get(dn) !\u003d null) {\n      remainingLimit \u003d getBlocksToInvalidateByLimit(nodeToBlocks.get(dn),\n          toInvalidate, numBlocks, remainingLimit);\n    }\n    if ((remainingLimit \u003e 0) \u0026\u0026 (nodeToECBlocks.get(dn) !\u003d null)) {\n      getBlocksToInvalidateByLimit(nodeToECBlocks.get(dn),\n          toInvalidate, numECBlocks, remainingLimit);\n    }\n    if (toInvalidate.size() \u003e 0 \u0026\u0026 getBlockSetsSize(dn) \u003d\u003d 0) {\n      remove(dn);\n    }\n    dn.addBlocksToBeInvalidated(toInvalidate);\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10999. Introduce separate stats for Replicated and Erasure Coded Blocks apart from the current Aggregated stats. (Manoj Govindassamy via lei)\n",
      "commitDate": "14/06/17 10:44 AM",
      "commitName": "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "29/05/17 1:30 AM",
      "commitNameOld": "a7f085d6bf499edf23e650a4f7211c53a442da0e",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 16.38,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n     final long delay \u003d getInvalidationDelay();\n     if (delay \u003e 0) {\n       BlockManager.LOG\n           .debug(\"Block deletion is delayed during NameNode startup. \"\n               + \"The deletion will start after {} ms.\", delay);\n       return null;\n     }\n-    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(dn);\n-    if (set \u003d\u003d null) {\n-      return null;\n+\n+    int remainingLimit \u003d blockInvalidateLimit;\n+    final List\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003c\u003e();\n+\n+    if (nodeToBlocks.get(dn) !\u003d null) {\n+      remainingLimit \u003d getBlocksToInvalidateByLimit(nodeToBlocks.get(dn),\n+          toInvalidate, numBlocks, remainingLimit);\n     }\n-\n-    // # blocks that can be sent in one message is limited\n-    final int limit \u003d blockInvalidateLimit;\n-    final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n-\n-    // If we send everything in this message, remove this node entry\n-    if (set.isEmpty()) {\n+    if ((remainingLimit \u003e 0) \u0026\u0026 (nodeToECBlockGroups.get(dn) !\u003d null)) {\n+      getBlocksToInvalidateByLimit(nodeToECBlockGroups.get(dn),\n+          toInvalidate, numECBlockGroups, remainingLimit);\n+    }\n+    if (toInvalidate.size() \u003e 0 \u0026\u0026 getBlockSetsSize(dn) \u003d\u003d 0) {\n       remove(dn);\n     }\n-\n     dn.addBlocksToBeInvalidated(toInvalidate);\n-    numBlocks -\u003d toInvalidate.size();\n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n    final long delay \u003d getInvalidationDelay();\n    if (delay \u003e 0) {\n      BlockManager.LOG\n          .debug(\"Block deletion is delayed during NameNode startup. \"\n              + \"The deletion will start after {} ms.\", delay);\n      return null;\n    }\n\n    int remainingLimit \u003d blockInvalidateLimit;\n    final List\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003c\u003e();\n\n    if (nodeToBlocks.get(dn) !\u003d null) {\n      remainingLimit \u003d getBlocksToInvalidateByLimit(nodeToBlocks.get(dn),\n          toInvalidate, numBlocks, remainingLimit);\n    }\n    if ((remainingLimit \u003e 0) \u0026\u0026 (nodeToECBlockGroups.get(dn) !\u003d null)) {\n      getBlocksToInvalidateByLimit(nodeToECBlockGroups.get(dn),\n          toInvalidate, numECBlockGroups, remainingLimit);\n    }\n    if (toInvalidate.size() \u003e 0 \u0026\u0026 getBlockSetsSize(dn) \u003d\u003d 0) {\n      remove(dn);\n    }\n    dn.addBlocksToBeInvalidated(toInvalidate);\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "a7f085d6bf499edf23e650a4f7211c53a442da0e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11832. Switch leftover logs to slf4j format in BlockManager.java. Contributed by Hui Xu and Chen Liang.\n",
      "commitDate": "29/05/17 1:30 AM",
      "commitName": "a7f085d6bf499edf23e650a4f7211c53a442da0e",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "07/08/16 2:29 PM",
      "commitNameOld": "4d3af47f2765f6f57936d316ef2a4150b787cc97",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 294.46,
      "commitsBetweenForRepo": 1803,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,26 @@\n   synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n     final long delay \u003d getInvalidationDelay();\n     if (delay \u003e 0) {\n-      if (BlockManager.LOG.isDebugEnabled()) {\n-        BlockManager.LOG\n-            .debug(\"Block deletion is delayed during NameNode startup. \"\n-                + \"The deletion will start after \" + delay + \" ms.\");\n-      }\n+      BlockManager.LOG\n+          .debug(\"Block deletion is delayed during NameNode startup. \"\n+              + \"The deletion will start after {} ms.\", delay);\n       return null;\n     }\n     final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(dn);\n     if (set \u003d\u003d null) {\n       return null;\n     }\n \n     // # blocks that can be sent in one message is limited\n     final int limit \u003d blockInvalidateLimit;\n     final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n \n     // If we send everything in this message, remove this node entry\n     if (set.isEmpty()) {\n       remove(dn);\n     }\n \n     dn.addBlocksToBeInvalidated(toInvalidate);\n     numBlocks -\u003d toInvalidate.size();\n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n    final long delay \u003d getInvalidationDelay();\n    if (delay \u003e 0) {\n      BlockManager.LOG\n          .debug(\"Block deletion is delayed during NameNode startup. \"\n              + \"The deletion will start after {} ms.\", delay);\n      return null;\n    }\n    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(dn);\n    if (set \u003d\u003d null) {\n      return null;\n    }\n\n    // # blocks that can be sent in one message is limited\n    final int limit \u003d blockInvalidateLimit;\n    final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n\n    // If we send everything in this message, remove this node entry\n    if (set.isEmpty()) {\n      remove(dn);\n    }\n\n    dn.addBlocksToBeInvalidated(toInvalidate);\n    numBlocks -\u003d toInvalidate.size();\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6362. InvalidateBlocks is inconsistent in usage of DatanodeUuid and StorageID. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595056 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/05/14 2:30 PM",
      "commitName": "9a0fcae5bc9e481201e101c3c98e23b6e827774e",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6362. InvalidateBlocks is inconsistent in usage of DatanodeUuid and StorageID. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595056 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "15/05/14 2:30 PM",
          "commitName": "9a0fcae5bc9e481201e101c3c98e23b6e827774e",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "13/05/14 11:22 AM",
          "commitNameOld": "8e5b5165c14486af6d5d73e7b4e591d4787ad8f2",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 2.13,
          "commitsBetweenForRepo": 24,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,28 @@\n-  synchronized List\u003cBlock\u003e invalidateWork(\n-      final String storageId, final DatanodeDescriptor dn) {\n+  synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n     final long delay \u003d getInvalidationDelay();\n     if (delay \u003e 0) {\n       if (BlockManager.LOG.isDebugEnabled()) {\n         BlockManager.LOG\n             .debug(\"Block deletion is delayed during NameNode startup. \"\n                 + \"The deletion will start after \" + delay + \" ms.\");\n       }\n       return null;\n     }\n-    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n+    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(dn);\n     if (set \u003d\u003d null) {\n       return null;\n     }\n \n     // # blocks that can be sent in one message is limited\n-    final int limit \u003d datanodeManager.blockInvalidateLimit;\n+    final int limit \u003d blockInvalidateLimit;\n     final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n \n     // If we send everything in this message, remove this node entry\n     if (set.isEmpty()) {\n-      remove(storageId);\n+      remove(dn);\n     }\n \n     dn.addBlocksToBeInvalidated(toInvalidate);\n     numBlocks -\u003d toInvalidate.size();\n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n    final long delay \u003d getInvalidationDelay();\n    if (delay \u003e 0) {\n      if (BlockManager.LOG.isDebugEnabled()) {\n        BlockManager.LOG\n            .debug(\"Block deletion is delayed during NameNode startup. \"\n                + \"The deletion will start after \" + delay + \" ms.\");\n      }\n      return null;\n    }\n    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(dn);\n    if (set \u003d\u003d null) {\n      return null;\n    }\n\n    // # blocks that can be sent in one message is limited\n    final int limit \u003d blockInvalidateLimit;\n    final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n\n    // If we send everything in this message, remove this node entry\n    if (set.isEmpty()) {\n      remove(dn);\n    }\n\n    dn.addBlocksToBeInvalidated(toInvalidate);\n    numBlocks -\u003d toInvalidate.size();\n    return toInvalidate;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
          "extendedDetails": {
            "oldValue": "[storageId-String(modifiers-final), dn-DatanodeDescriptor(modifiers-final)]",
            "newValue": "[dn-DatanodeDescriptor(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6362. InvalidateBlocks is inconsistent in usage of DatanodeUuid and StorageID. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595056 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "15/05/14 2:30 PM",
          "commitName": "9a0fcae5bc9e481201e101c3c98e23b6e827774e",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "13/05/14 11:22 AM",
          "commitNameOld": "8e5b5165c14486af6d5d73e7b4e591d4787ad8f2",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 2.13,
          "commitsBetweenForRepo": 24,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,28 @@\n-  synchronized List\u003cBlock\u003e invalidateWork(\n-      final String storageId, final DatanodeDescriptor dn) {\n+  synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n     final long delay \u003d getInvalidationDelay();\n     if (delay \u003e 0) {\n       if (BlockManager.LOG.isDebugEnabled()) {\n         BlockManager.LOG\n             .debug(\"Block deletion is delayed during NameNode startup. \"\n                 + \"The deletion will start after \" + delay + \" ms.\");\n       }\n       return null;\n     }\n-    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n+    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(dn);\n     if (set \u003d\u003d null) {\n       return null;\n     }\n \n     // # blocks that can be sent in one message is limited\n-    final int limit \u003d datanodeManager.blockInvalidateLimit;\n+    final int limit \u003d blockInvalidateLimit;\n     final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n \n     // If we send everything in this message, remove this node entry\n     if (set.isEmpty()) {\n-      remove(storageId);\n+      remove(dn);\n     }\n \n     dn.addBlocksToBeInvalidated(toInvalidate);\n     numBlocks -\u003d toInvalidate.size();\n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized List\u003cBlock\u003e invalidateWork(final DatanodeDescriptor dn) {\n    final long delay \u003d getInvalidationDelay();\n    if (delay \u003e 0) {\n      if (BlockManager.LOG.isDebugEnabled()) {\n        BlockManager.LOG\n            .debug(\"Block deletion is delayed during NameNode startup. \"\n                + \"The deletion will start after \" + delay + \" ms.\");\n      }\n      return null;\n    }\n    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(dn);\n    if (set \u003d\u003d null) {\n      return null;\n    }\n\n    // # blocks that can be sent in one message is limited\n    final int limit \u003d blockInvalidateLimit;\n    final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n\n    // If we send everything in this message, remove this node entry\n    if (set.isEmpty()) {\n      remove(dn);\n    }\n\n    dn.addBlocksToBeInvalidated(toInvalidate);\n    numBlocks -\u003d toInvalidate.size();\n    return toInvalidate;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
          "extendedDetails": {}
        }
      ]
    },
    "8e5b5165c14486af6d5d73e7b4e591d4787ad8f2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6186. Pause deletion of blocks when the namenode starts up. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594314 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/05/14 11:22 AM",
      "commitName": "8e5b5165c14486af6d5d73e7b4e591d4787ad8f2",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "22/09/13 11:03 AM",
      "commitNameOld": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 233.01,
      "commitsBetweenForRepo": 1604,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,29 @@\n   synchronized List\u003cBlock\u003e invalidateWork(\n       final String storageId, final DatanodeDescriptor dn) {\n+    final long delay \u003d getInvalidationDelay();\n+    if (delay \u003e 0) {\n+      if (BlockManager.LOG.isDebugEnabled()) {\n+        BlockManager.LOG\n+            .debug(\"Block deletion is delayed during NameNode startup. \"\n+                + \"The deletion will start after \" + delay + \" ms.\");\n+      }\n+      return null;\n+    }\n     final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n     if (set \u003d\u003d null) {\n       return null;\n     }\n \n     // # blocks that can be sent in one message is limited\n     final int limit \u003d datanodeManager.blockInvalidateLimit;\n     final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n \n     // If we send everything in this message, remove this node entry\n     if (set.isEmpty()) {\n       remove(storageId);\n     }\n \n     dn.addBlocksToBeInvalidated(toInvalidate);\n     numBlocks -\u003d toInvalidate.size();\n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized List\u003cBlock\u003e invalidateWork(\n      final String storageId, final DatanodeDescriptor dn) {\n    final long delay \u003d getInvalidationDelay();\n    if (delay \u003e 0) {\n      if (BlockManager.LOG.isDebugEnabled()) {\n        BlockManager.LOG\n            .debug(\"Block deletion is delayed during NameNode startup. \"\n                + \"The deletion will start after \" + delay + \" ms.\");\n      }\n      return null;\n    }\n    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n    if (set \u003d\u003d null) {\n      return null;\n    }\n\n    // # blocks that can be sent in one message is limited\n    final int limit \u003d datanodeManager.blockInvalidateLimit;\n    final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n\n    // If we send everything in this message, remove this node entry\n    if (set.isEmpty()) {\n      remove(storageId);\n    }\n\n    dn.addBlocksToBeInvalidated(toInvalidate);\n    numBlocks -\u003d toInvalidate.size();\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "b7887f31fbe28d35005abdc439b2771b58c91225": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-4052. BlockManager#invalidateWork should print log outside the lock. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1398631 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/10/12 8:37 PM",
      "commitName": "b7887f31fbe28d35005abdc439b2771b58c91225",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "17/05/12 3:30 PM",
      "commitNameOld": "5258d6bf3fb8090739cf96f5089f96cee87393c4",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 151.21,
      "commitsBetweenForRepo": 830,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n-  private synchronized List\u003cBlock\u003e invalidateWork(\n+  synchronized List\u003cBlock\u003e invalidateWork(\n       final String storageId, final DatanodeDescriptor dn) {\n     final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n     if (set \u003d\u003d null) {\n       return null;\n     }\n \n     // # blocks that can be sent in one message is limited\n     final int limit \u003d datanodeManager.blockInvalidateLimit;\n     final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n \n     // If we send everything in this message, remove this node entry\n     if (set.isEmpty()) {\n       remove(storageId);\n     }\n \n     dn.addBlocksToBeInvalidated(toInvalidate);\n     numBlocks -\u003d toInvalidate.size();\n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized List\u003cBlock\u003e invalidateWork(\n      final String storageId, final DatanodeDescriptor dn) {\n    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n    if (set \u003d\u003d null) {\n      return null;\n    }\n\n    // # blocks that can be sent in one message is limited\n    final int limit \u003d datanodeManager.blockInvalidateLimit;\n    final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n\n    // If we send everything in this message, remove this node entry\n    if (set.isEmpty()) {\n      remove(storageId);\n    }\n\n    dn.addBlocksToBeInvalidated(toInvalidate);\n    numBlocks -\u003d toInvalidate.size();\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {
        "oldValue": "[private, synchronized]",
        "newValue": "[synchronized]"
      }
    },
    "9a3f147fdd5421460889b266ead3a2300323cda2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2476. More CPU efficient data structure for under-replicated, over-replicated, and invalidated blocks. Contributed by Tomasz Nykiel.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1201991 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/11/11 5:13 PM",
      "commitName": "9a3f147fdd5421460889b266ead3a2300323cda2",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 82.04,
      "commitsBetweenForRepo": 586,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,20 @@\n   private synchronized List\u003cBlock\u003e invalidateWork(\n       final String storageId, final DatanodeDescriptor dn) {\n-    final Collection\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n+    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n     if (set \u003d\u003d null) {\n       return null;\n     }\n \n     // # blocks that can be sent in one message is limited\n     final int limit \u003d datanodeManager.blockInvalidateLimit;\n-    final List\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003cBlock\u003e(limit);\n-    final Iterator\u003cBlock\u003e it \u003d set.iterator();\n-    for(int count \u003d 0; count \u003c limit \u0026\u0026 it.hasNext(); count++) {\n-      toInvalidate.add(it.next());\n-      it.remove();\n-    }\n+    final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n+\n     // If we send everything in this message, remove this node entry\n-    if (!it.hasNext()) {\n+    if (set.isEmpty()) {\n       remove(storageId);\n     }\n \n     dn.addBlocksToBeInvalidated(toInvalidate);\n     numBlocks -\u003d toInvalidate.size();\n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized List\u003cBlock\u003e invalidateWork(\n      final String storageId, final DatanodeDescriptor dn) {\n    final LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n    if (set \u003d\u003d null) {\n      return null;\n    }\n\n    // # blocks that can be sent in one message is limited\n    final int limit \u003d datanodeManager.blockInvalidateLimit;\n    final List\u003cBlock\u003e toInvalidate \u003d set.pollN(limit);\n\n    // If we send everything in this message, remove this node entry\n    if (set.isEmpty()) {\n      remove(storageId);\n    }\n\n    dn.addBlocksToBeInvalidated(toInvalidate);\n    numBlocks -\u003d toInvalidate.size();\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private synchronized List\u003cBlock\u003e invalidateWork(\n      final String storageId, final DatanodeDescriptor dn) {\n    final Collection\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n    if (set \u003d\u003d null) {\n      return null;\n    }\n\n    // # blocks that can be sent in one message is limited\n    final int limit \u003d datanodeManager.blockInvalidateLimit;\n    final List\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003cBlock\u003e(limit);\n    final Iterator\u003cBlock\u003e it \u003d set.iterator();\n    for(int count \u003d 0; count \u003c limit \u0026\u0026 it.hasNext(); count++) {\n      toInvalidate.add(it.next());\n      it.remove();\n    }\n    // If we send everything in this message, remove this node entry\n    if (!it.hasNext()) {\n      remove(storageId);\n    }\n\n    dn.addBlocksToBeInvalidated(toInvalidate);\n    numBlocks -\u003d toInvalidate.size();\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java"
      }
    },
    "513f17d115564e49124bb744cecf36d16a144ffc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2273.  Refactor BlockManager.recentInvalidateSets to a new class.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1160475 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/08/11 3:28 PM",
      "commitName": "513f17d115564e49124bb744cecf36d16a144ffc",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,24 @@\n+  private synchronized List\u003cBlock\u003e invalidateWork(\n+      final String storageId, final DatanodeDescriptor dn) {\n+    final Collection\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n+    if (set \u003d\u003d null) {\n+      return null;\n+    }\n+\n+    // # blocks that can be sent in one message is limited\n+    final int limit \u003d datanodeManager.blockInvalidateLimit;\n+    final List\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003cBlock\u003e(limit);\n+    final Iterator\u003cBlock\u003e it \u003d set.iterator();\n+    for(int count \u003d 0; count \u003c limit \u0026\u0026 it.hasNext(); count++) {\n+      toInvalidate.add(it.next());\n+      it.remove();\n+    }\n+    // If we send everything in this message, remove this node entry\n+    if (!it.hasNext()) {\n+      remove(storageId);\n+    }\n+\n+    dn.addBlocksToBeInvalidated(toInvalidate);\n+    numBlocks -\u003d toInvalidate.size();\n+    return toInvalidate;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized List\u003cBlock\u003e invalidateWork(\n      final String storageId, final DatanodeDescriptor dn) {\n    final Collection\u003cBlock\u003e set \u003d node2blocks.get(storageId);\n    if (set \u003d\u003d null) {\n      return null;\n    }\n\n    // # blocks that can be sent in one message is limited\n    final int limit \u003d datanodeManager.blockInvalidateLimit;\n    final List\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003cBlock\u003e(limit);\n    final Iterator\u003cBlock\u003e it \u003d set.iterator();\n    for(int count \u003d 0; count \u003c limit \u0026\u0026 it.hasNext(); count++) {\n      toInvalidate.add(it.next());\n      it.remove();\n    }\n    // If we send everything in this message, remove this node entry\n    if (!it.hasNext()) {\n      remove(storageId);\n    }\n\n    dn.addBlocksToBeInvalidated(toInvalidate);\n    numBlocks -\u003d toInvalidate.size();\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java"
    }
  }
}