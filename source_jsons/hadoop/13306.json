{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeManager.java",
  "functionName": "refreshDatanodes",
  "functionId": "refreshDatanodes",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
  "functionStartLine": 1245,
  "functionEndLine": 1269,
  "numCommitsSeen": 184,
  "timeTaken": 6977,
  "changeHistory": [
    "79df1e750ef558afed6d166ce225a23061b36aed",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978",
    "fde8ac5d8514f5146f438f8d0794116aaef20416",
    "8602692338d6f493647205e0241e4116211fab75",
    "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a",
    "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8",
    "88209ce181b5ecc55c0ae2bceff4893ab4817e88",
    "a7bfb25d2bbab0a329712d1efb143edc49a4076d",
    "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7",
    "be7dd8333a7e56e732171db0781786987de03195",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee",
    "7fac946ac983e31613fd62836c8ac9c4a579210a",
    "969a263188f7015261719fe45fa1505121ebb80e",
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9"
  ],
  "changeHistoryShort": {
    "79df1e750ef558afed6d166ce225a23061b36aed": "Ybodychange",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978": "Ybodychange",
    "fde8ac5d8514f5146f438f8d0794116aaef20416": "Ybodychange",
    "8602692338d6f493647205e0241e4116211fab75": "Ybodychange",
    "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a": "Ybodychange",
    "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8": "Ymodifierchange",
    "88209ce181b5ecc55c0ae2bceff4893ab4817e88": "Ymodifierchange",
    "a7bfb25d2bbab0a329712d1efb143edc49a4076d": "Ybodychange",
    "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7": "Yexceptionschange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee": "Ymodifierchange",
    "7fac946ac983e31613fd62836c8ac9c4a579210a": "Ybodychange",
    "969a263188f7015261719fe45fa1505121ebb80e": "Ybodychange",
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "79df1e750ef558afed6d166ce225a23061b36aed": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9388. Decommission related code to support Maintenance State for datanodes.\n",
      "commitDate": "02/08/17 2:22 PM",
      "commitName": "79df1e750ef558afed6d166ce225a23061b36aed",
      "commitAuthor": "Manoj Govindassamy",
      "commitDateOld": "31/07/17 11:33 AM",
      "commitNameOld": "3e23415a92d43ce8818124f0b180227a52a33eaf",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.12,
      "commitsBetweenForRepo": 79,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,25 @@\n   private void refreshDatanodes() {\n     final Map\u003cString, DatanodeDescriptor\u003e copy;\n     synchronized (this) {\n       copy \u003d new HashMap\u003c\u003e(datanodeMap);\n     }\n     for (DatanodeDescriptor node : copy.values()) {\n       // Check if not include.\n       if (!hostConfigManager.isIncluded(node)) {\n         node.setDisallowed(true);\n       } else {\n         long maintenanceExpireTimeInMS \u003d\n             hostConfigManager.getMaintenanceExpirationTimeInMS(node);\n         if (node.maintenanceNotExpired(maintenanceExpireTimeInMS)) {\n-          decomManager.startMaintenance(node, maintenanceExpireTimeInMS);\n+          datanodeAdminManager.startMaintenance(\n+              node, maintenanceExpireTimeInMS);\n         } else if (hostConfigManager.isExcluded(node)) {\n-          decomManager.startDecommission(node);\n+          datanodeAdminManager.startDecommission(node);\n         } else {\n-          decomManager.stopMaintenance(node);\n-          decomManager.stopDecommission(node);\n+          datanodeAdminManager.stopMaintenance(node);\n+          datanodeAdminManager.stopDecommission(node);\n         }\n       }\n       node.setUpgradeDomain(hostConfigManager.getUpgradeDomain(node));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void refreshDatanodes() {\n    final Map\u003cString, DatanodeDescriptor\u003e copy;\n    synchronized (this) {\n      copy \u003d new HashMap\u003c\u003e(datanodeMap);\n    }\n    for (DatanodeDescriptor node : copy.values()) {\n      // Check if not include.\n      if (!hostConfigManager.isIncluded(node)) {\n        node.setDisallowed(true);\n      } else {\n        long maintenanceExpireTimeInMS \u003d\n            hostConfigManager.getMaintenanceExpirationTimeInMS(node);\n        if (node.maintenanceNotExpired(maintenanceExpireTimeInMS)) {\n          datanodeAdminManager.startMaintenance(\n              node, maintenanceExpireTimeInMS);\n        } else if (hostConfigManager.isExcluded(node)) {\n          datanodeAdminManager.startDecommission(node);\n        } else {\n          datanodeAdminManager.stopMaintenance(node);\n          datanodeAdminManager.stopDecommission(node);\n        }\n      }\n      node.setUpgradeDomain(hostConfigManager.getUpgradeDomain(node));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "06/09/16 10:38 AM",
      "commitNameOld": "d37dc5d1b8e022a7085118a2e7066623483c293f",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 41.3,
      "commitsBetweenForRepo": 278,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   private void refreshDatanodes() {\n     final Map\u003cString, DatanodeDescriptor\u003e copy;\n     synchronized (this) {\n       copy \u003d new HashMap\u003c\u003e(datanodeMap);\n     }\n     for (DatanodeDescriptor node : copy.values()) {\n       // Check if not include.\n       if (!hostConfigManager.isIncluded(node)) {\n-        node.setDisallowed(true); // case 2.\n+        node.setDisallowed(true);\n       } else {\n         long maintenanceExpireTimeInMS \u003d\n             hostConfigManager.getMaintenanceExpirationTimeInMS(node);\n         if (node.maintenanceNotExpired(maintenanceExpireTimeInMS)) {\n           decomManager.startMaintenance(node, maintenanceExpireTimeInMS);\n         } else if (hostConfigManager.isExcluded(node)) {\n-          decomManager.startDecommission(node); // case 3.\n+          decomManager.startDecommission(node);\n         } else {\n           decomManager.stopMaintenance(node);\n-          decomManager.stopDecommission(node); // case 4.\n+          decomManager.stopDecommission(node);\n         }\n       }\n       node.setUpgradeDomain(hostConfigManager.getUpgradeDomain(node));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void refreshDatanodes() {\n    final Map\u003cString, DatanodeDescriptor\u003e copy;\n    synchronized (this) {\n      copy \u003d new HashMap\u003c\u003e(datanodeMap);\n    }\n    for (DatanodeDescriptor node : copy.values()) {\n      // Check if not include.\n      if (!hostConfigManager.isIncluded(node)) {\n        node.setDisallowed(true);\n      } else {\n        long maintenanceExpireTimeInMS \u003d\n            hostConfigManager.getMaintenanceExpirationTimeInMS(node);\n        if (node.maintenanceNotExpired(maintenanceExpireTimeInMS)) {\n          decomManager.startMaintenance(node, maintenanceExpireTimeInMS);\n        } else if (hostConfigManager.isExcluded(node)) {\n          decomManager.startDecommission(node);\n        } else {\n          decomManager.stopMaintenance(node);\n          decomManager.stopDecommission(node);\n        }\n      }\n      node.setUpgradeDomain(hostConfigManager.getUpgradeDomain(node));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9392. Admins support for maintenance state. Contributed by Ming Ma.\n",
      "commitDate": "30/08/16 2:00 PM",
      "commitName": "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "12/04/16 1:38 PM",
      "commitNameOld": "6ef42873a02bfcbff5521869f4d6f66539d1db41",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 140.01,
      "commitsBetweenForRepo": 1065,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,24 @@\n   private void refreshDatanodes() {\n     final Map\u003cString, DatanodeDescriptor\u003e copy;\n     synchronized (this) {\n       copy \u003d new HashMap\u003c\u003e(datanodeMap);\n     }\n     for (DatanodeDescriptor node : copy.values()) {\n       // Check if not include.\n       if (!hostConfigManager.isIncluded(node)) {\n         node.setDisallowed(true); // case 2.\n       } else {\n-        if (hostConfigManager.isExcluded(node)) {\n+        long maintenanceExpireTimeInMS \u003d\n+            hostConfigManager.getMaintenanceExpirationTimeInMS(node);\n+        if (node.maintenanceNotExpired(maintenanceExpireTimeInMS)) {\n+          decomManager.startMaintenance(node, maintenanceExpireTimeInMS);\n+        } else if (hostConfigManager.isExcluded(node)) {\n           decomManager.startDecommission(node); // case 3.\n         } else {\n+          decomManager.stopMaintenance(node);\n           decomManager.stopDecommission(node); // case 4.\n         }\n       }\n       node.setUpgradeDomain(hostConfigManager.getUpgradeDomain(node));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void refreshDatanodes() {\n    final Map\u003cString, DatanodeDescriptor\u003e copy;\n    synchronized (this) {\n      copy \u003d new HashMap\u003c\u003e(datanodeMap);\n    }\n    for (DatanodeDescriptor node : copy.values()) {\n      // Check if not include.\n      if (!hostConfigManager.isIncluded(node)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        long maintenanceExpireTimeInMS \u003d\n            hostConfigManager.getMaintenanceExpirationTimeInMS(node);\n        if (node.maintenanceNotExpired(maintenanceExpireTimeInMS)) {\n          decomManager.startMaintenance(node, maintenanceExpireTimeInMS);\n        } else if (hostConfigManager.isExcluded(node)) {\n          decomManager.startDecommission(node); // case 3.\n        } else {\n          decomManager.stopMaintenance(node);\n          decomManager.stopDecommission(node); // case 4.\n        }\n      }\n      node.setUpgradeDomain(hostConfigManager.getUpgradeDomain(node));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "fde8ac5d8514f5146f438f8d0794116aaef20416": {
      "type": "Ybodychange",
      "commitMessage": "Add missing files from HDFS-9005. (lei)\n",
      "commitDate": "25/03/16 5:11 PM",
      "commitName": "fde8ac5d8514f5146f438f8d0794116aaef20416",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 14.88,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,19 @@\n   private void refreshDatanodes() {\n     final Map\u003cString, DatanodeDescriptor\u003e copy;\n     synchronized (this) {\n       copy \u003d new HashMap\u003c\u003e(datanodeMap);\n     }\n     for (DatanodeDescriptor node : copy.values()) {\n       // Check if not include.\n-      if (!hostFileManager.isIncluded(node)) {\n+      if (!hostConfigManager.isIncluded(node)) {\n         node.setDisallowed(true); // case 2.\n       } else {\n-        if (hostFileManager.isExcluded(node)) {\n+        if (hostConfigManager.isExcluded(node)) {\n           decomManager.startDecommission(node); // case 3.\n         } else {\n           decomManager.stopDecommission(node); // case 4.\n         }\n       }\n+      node.setUpgradeDomain(hostConfigManager.getUpgradeDomain(node));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void refreshDatanodes() {\n    final Map\u003cString, DatanodeDescriptor\u003e copy;\n    synchronized (this) {\n      copy \u003d new HashMap\u003c\u003e(datanodeMap);\n    }\n    for (DatanodeDescriptor node : copy.values()) {\n      // Check if not include.\n      if (!hostConfigManager.isIncluded(node)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (hostConfigManager.isExcluded(node)) {\n          decomManager.startDecommission(node); // case 3.\n        } else {\n          decomManager.stopDecommission(node); // case 4.\n        }\n      }\n      node.setUpgradeDomain(hostConfigManager.getUpgradeDomain(node));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "8602692338d6f493647205e0241e4116211fab75": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9371. Code cleanup for DatanodeManager. Contributed by Jing Zhao.\n",
      "commitDate": "15/12/15 10:47 AM",
      "commitName": "8602692338d6f493647205e0241e4116211fab75",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "01/12/15 4:09 PM",
      "commitNameOld": "a49cc74b4c72195dee1dfb6f9548e5e411dff553",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 13.78,
      "commitsBetweenForRepo": 73,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,18 @@\n   private void refreshDatanodes() {\n-    for(DatanodeDescriptor node : datanodeMap.values()) {\n+    final Map\u003cString, DatanodeDescriptor\u003e copy;\n+    synchronized (this) {\n+      copy \u003d new HashMap\u003c\u003e(datanodeMap);\n+    }\n+    for (DatanodeDescriptor node : copy.values()) {\n       // Check if not include.\n       if (!hostFileManager.isIncluded(node)) {\n         node.setDisallowed(true); // case 2.\n       } else {\n         if (hostFileManager.isExcluded(node)) {\n           decomManager.startDecommission(node); // case 3.\n         } else {\n           decomManager.stopDecommission(node); // case 4.\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void refreshDatanodes() {\n    final Map\u003cString, DatanodeDescriptor\u003e copy;\n    synchronized (this) {\n      copy \u003d new HashMap\u003c\u003e(datanodeMap);\n    }\n    for (DatanodeDescriptor node : copy.values()) {\n      // Check if not include.\n      if (!hostFileManager.isIncluded(node)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (hostFileManager.isExcluded(node)) {\n          decomManager.startDecommission(node); // case 3.\n        } else {\n          decomManager.stopDecommission(node); // case 4.\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7411. Change decommission logic to throttle by blocks rather\nthan nodes in each interval. Contributed by Andrew Wang\n",
      "commitDate": "08/03/15 6:31 PM",
      "commitName": "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "16/02/15 2:43 PM",
      "commitNameOld": "9729b244de50322c2cc889c97c2ffb2b4675cf77",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 20.12,
      "commitsBetweenForRepo": 164,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   private void refreshDatanodes() {\n     for(DatanodeDescriptor node : datanodeMap.values()) {\n       // Check if not include.\n       if (!hostFileManager.isIncluded(node)) {\n         node.setDisallowed(true); // case 2.\n       } else {\n         if (hostFileManager.isExcluded(node)) {\n-          startDecommission(node); // case 3.\n+          decomManager.startDecommission(node); // case 3.\n         } else {\n-          stopDecommission(node); // case 4.\n+          decomManager.stopDecommission(node); // case 4.\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void refreshDatanodes() {\n    for(DatanodeDescriptor node : datanodeMap.values()) {\n      // Check if not include.\n      if (!hostFileManager.isIncluded(node)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (hostFileManager.isExcluded(node)) {\n          decomManager.startDecommission(node); // case 3.\n        } else {\n          decomManager.stopDecommission(node); // case 4.\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8": {
      "type": "Ymodifierchange",
      "commitMessage": "Revert HDFS-6940.",
      "commitDate": "09/09/14 5:30 PM",
      "commitName": "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "06/09/14 12:07 PM",
      "commitNameOld": "88209ce181b5ecc55c0ae2bceff4893ab4817e88",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 3.22,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n-  void refreshDatanodes() {\n+  private void refreshDatanodes() {\n     for(DatanodeDescriptor node : datanodeMap.values()) {\n       // Check if not include.\n       if (!hostFileManager.isIncluded(node)) {\n         node.setDisallowed(true); // case 2.\n       } else {\n         if (hostFileManager.isExcluded(node)) {\n           startDecommission(node); // case 3.\n         } else {\n           stopDecommission(node); // case 4.\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void refreshDatanodes() {\n    for(DatanodeDescriptor node : datanodeMap.values()) {\n      // Check if not include.\n      if (!hostFileManager.isIncluded(node)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (hostFileManager.isExcluded(node)) {\n          startDecommission(node); // case 3.\n        } else {\n          stopDecommission(node); // case 4.\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[private]"
      }
    },
    "88209ce181b5ecc55c0ae2bceff4893ab4817e88": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-6940. Refactoring to allow ConsensusNode implementation.\nContributed by Konstantin Shvachko.",
      "commitDate": "06/09/14 12:07 PM",
      "commitName": "88209ce181b5ecc55c0ae2bceff4893ab4817e88",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "07/08/14 10:41 PM",
      "commitNameOld": "d3a2fe280775e9320181b671d5951f06837bddad",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 29.56,
      "commitsBetweenForRepo": 229,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n-  private void refreshDatanodes() {\n+  void refreshDatanodes() {\n     for(DatanodeDescriptor node : datanodeMap.values()) {\n       // Check if not include.\n       if (!hostFileManager.isIncluded(node)) {\n         node.setDisallowed(true); // case 2.\n       } else {\n         if (hostFileManager.isExcluded(node)) {\n           startDecommission(node); // case 3.\n         } else {\n           stopDecommission(node); // case 4.\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void refreshDatanodes() {\n    for(DatanodeDescriptor node : datanodeMap.values()) {\n      // Check if not include.\n      if (!hostFileManager.isIncluded(node)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (hostFileManager.isExcluded(node)) {\n          startDecommission(node); // case 3.\n        } else {\n          stopDecommission(node); // case 4.\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[]"
      }
    },
    "a7bfb25d2bbab0a329712d1efb143edc49a4076d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3934. duplicative dfs_hosts entries handled wrong. (cmccabe)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489065 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/06/13 10:14 AM",
      "commitName": "a7bfb25d2bbab0a329712d1efb143edc49a4076d",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "28/05/13 1:17 PM",
      "commitNameOld": "4bb72210c266707806f3ce3e974968a9a137b25b",
      "commitAuthorOld": "Devaraj Das",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 49,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   private void refreshDatanodes() {\n     for(DatanodeDescriptor node : datanodeMap.values()) {\n       // Check if not include.\n-      if (!inHostsList(node)) {\n+      if (!hostFileManager.isIncluded(node)) {\n         node.setDisallowed(true); // case 2.\n       } else {\n-        if (inExcludedHostsList(node)) {\n+        if (hostFileManager.isExcluded(node)) {\n           startDecommission(node); // case 3.\n         } else {\n           stopDecommission(node); // case 4.\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void refreshDatanodes() {\n    for(DatanodeDescriptor node : datanodeMap.values()) {\n      // Check if not include.\n      if (!hostFileManager.isIncluded(node)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (hostFileManager.isExcluded(node)) {\n          startDecommission(node); // case 3.\n        } else {\n          stopDecommission(node); // case 4.\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-3912. Detect and avoid stale datanodes for writes. Contributed by Jing Zhao\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1397211 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/10/12 11:08 AM",
      "commitName": "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "10/10/12 2:15 PM",
      "commitNameOld": "08f35a04c69ea20913bb28b00a1827c77e0e23e3",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 0.87,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n-  private void refreshDatanodes() throws IOException {\n+  private void refreshDatanodes() {\n     for(DatanodeDescriptor node : datanodeMap.values()) {\n       // Check if not include.\n       if (!inHostsList(node)) {\n         node.setDisallowed(true); // case 2.\n       } else {\n         if (inExcludedHostsList(node)) {\n           startDecommission(node); // case 3.\n         } else {\n           stopDecommission(node); // case 4.\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void refreshDatanodes() {\n    for(DatanodeDescriptor node : datanodeMap.values()) {\n      // Check if not include.\n      if (!inHostsList(node)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (inExcludedHostsList(node)) {\n          startDecommission(node); // case 3.\n        } else {\n          stopDecommission(node); // case 4.\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {
        "oldValue": "[IOException]",
        "newValue": "[]"
      }
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "31/03/12 8:41 PM",
      "commitNameOld": "0663dbaac0a19719ddf9cd4290ba893bfca69da2",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.77,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   private void refreshDatanodes() throws IOException {\n     for(DatanodeDescriptor node : datanodeMap.values()) {\n       // Check if not include.\n-      if (!inHostsList(node, null)) {\n+      if (!inHostsList(node)) {\n         node.setDisallowed(true); // case 2.\n       } else {\n-        if (inExcludedHostsList(node, null)) {\n+        if (inExcludedHostsList(node)) {\n           startDecommission(node); // case 3.\n         } else {\n           stopDecommission(node); // case 4.\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void refreshDatanodes() throws IOException {\n    for(DatanodeDescriptor node : datanodeMap.values()) {\n      // Check if not include.\n      if (!inHostsList(node)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (inExcludedHostsList(node)) {\n          startDecommission(node); // case 3.\n        } else {\n          stopDecommission(node); // case 4.\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void refreshDatanodes() throws IOException {\n    for(DatanodeDescriptor node : datanodeMap.values()) {\n      // Check if not include.\n      if (!inHostsList(node, null)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (inExcludedHostsList(node, null)) {\n          startDecommission(node); // case 3.\n        } else {\n          stopDecommission(node); // case 4.\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void refreshDatanodes() throws IOException {\n    for(DatanodeDescriptor node : datanodeMap.values()) {\n      // Check if not include.\n      if (!inHostsList(node, null)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (inExcludedHostsList(node, null)) {\n          startDecommission(node); // case 3.\n        } else {\n          stopDecommission(node); // case 4.\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java"
      }
    },
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-2239. Reduce access levels of the fields and methods in FSNamesystem.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1155998 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/11 6:50 PM",
      "commitName": "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/08/11 3:06 AM",
      "commitNameOld": "371f4a59059322000a40eb4bdf5386b96b626ece",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.66,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n-  public void refreshDatanodes() throws IOException {\n+  private void refreshDatanodes() throws IOException {\n     for(DatanodeDescriptor node : datanodeMap.values()) {\n       // Check if not include.\n       if (!inHostsList(node, null)) {\n         node.setDisallowed(true); // case 2.\n       } else {\n         if (inExcludedHostsList(node, null)) {\n           startDecommission(node); // case 3.\n         } else {\n           stopDecommission(node); // case 4.\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void refreshDatanodes() throws IOException {\n    for(DatanodeDescriptor node : datanodeMap.values()) {\n      // Check if not include.\n      if (!inHostsList(node, null)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (inExcludedHostsList(node, null)) {\n          startDecommission(node); // case 3.\n        } else {\n          stopDecommission(node); // case 4.\n        }\n      }\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {
        "oldValue": "[public]",
        "newValue": "[private]"
      }
    },
    "7fac946ac983e31613fd62836c8ac9c4a579210a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2108. Move datanode heartbeat handling from namenode package to blockmanagement package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154042 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/08/11 3:55 PM",
      "commitName": "7fac946ac983e31613fd62836c8ac9c4a579210a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/08/11 6:57 AM",
      "commitNameOld": "d68e38b78d9687987c4de2046ce9aa0016685e98",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.37,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   public void refreshDatanodes() throws IOException {\n     for(DatanodeDescriptor node : datanodeMap.values()) {\n       // Check if not include.\n       if (!inHostsList(node, null)) {\n-        node.setDisallowed(true);  // case 2.\n+        node.setDisallowed(true); // case 2.\n       } else {\n         if (inExcludedHostsList(node, null)) {\n-          namesystem.getBlockManager().startDecommission(node);   // case 3.\n+          startDecommission(node); // case 3.\n         } else {\n-          namesystem.getBlockManager().stopDecommission(node);   // case 4.\n+          stopDecommission(node); // case 4.\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void refreshDatanodes() throws IOException {\n    for(DatanodeDescriptor node : datanodeMap.values()) {\n      // Check if not include.\n      if (!inHostsList(node, null)) {\n        node.setDisallowed(true); // case 2.\n      } else {\n        if (inExcludedHostsList(node, null)) {\n          startDecommission(node); // case 3.\n        } else {\n          stopDecommission(node); // case 4.\n        }\n      }\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "969a263188f7015261719fe45fa1505121ebb80e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2191.  Move datanodeMap from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151339 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/11 10:46 PM",
      "commitName": "969a263188f7015261719fe45fa1505121ebb80e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "21/07/11 9:20 PM",
      "commitNameOld": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.06,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   public void refreshDatanodes() throws IOException {\n-    for(DatanodeDescriptor node : namesystem.datanodeMap.values()) {\n+    for(DatanodeDescriptor node : datanodeMap.values()) {\n       // Check if not include.\n       if (!inHostsList(node, null)) {\n         node.setDisallowed(true);  // case 2.\n       } else {\n         if (inExcludedHostsList(node, null)) {\n           namesystem.getBlockManager().startDecommission(node);   // case 3.\n         } else {\n           namesystem.getBlockManager().stopDecommission(node);   // case 4.\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void refreshDatanodes() throws IOException {\n    for(DatanodeDescriptor node : datanodeMap.values()) {\n      // Check if not include.\n      if (!inHostsList(node, null)) {\n        node.setDisallowed(true);  // case 2.\n      } else {\n        if (inExcludedHostsList(node, null)) {\n          namesystem.getBlockManager().startDecommission(node);   // case 3.\n        } else {\n          namesystem.getBlockManager().stopDecommission(node);   // case 4.\n        }\n      }\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/07/11 9:20 PM",
      "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,14 @@\n+  public void refreshDatanodes() throws IOException {\n+    for(DatanodeDescriptor node : namesystem.datanodeMap.values()) {\n+      // Check if not include.\n+      if (!inHostsList(node, null)) {\n+        node.setDisallowed(true);  // case 2.\n+      } else {\n+        if (inExcludedHostsList(node, null)) {\n+          namesystem.getBlockManager().startDecommission(node);   // case 3.\n+        } else {\n+          namesystem.getBlockManager().stopDecommission(node);   // case 4.\n+        }\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void refreshDatanodes() throws IOException {\n    for(DatanodeDescriptor node : namesystem.datanodeMap.values()) {\n      // Check if not include.\n      if (!inHostsList(node, null)) {\n        node.setDisallowed(true);  // case 2.\n      } else {\n        if (inExcludedHostsList(node, null)) {\n          namesystem.getBlockManager().startDecommission(node);   // case 3.\n        } else {\n          namesystem.getBlockManager().stopDecommission(node);   // case 4.\n        }\n      }\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java"
    }
  }
}