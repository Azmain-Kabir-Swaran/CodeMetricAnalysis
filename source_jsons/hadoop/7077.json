{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeCacheManager.java",
  "functionName": "getLiveDatanodeStorageReport",
  "functionId": "getLiveDatanodeStorageReport___spsContext-Context",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/DatanodeCacheManager.java",
  "functionStartLine": 80,
  "functionEndLine": 116,
  "numCommitsSeen": 4,
  "timeTaken": 2132,
  "changeHistory": [
    "66e8f9b31529226309c924226a53dead3e6fcf11",
    "75ccc1396b677777cdc0d4992a4af3911f9f88c2"
  ],
  "changeHistoryShort": {
    "66e8f9b31529226309c924226a53dead3e6fcf11": "Yparameterchange",
    "75ccc1396b677777cdc0d4992a4af3911f9f88c2": "Yintroduced"
  },
  "changeHistoryDetails": {
    "66e8f9b31529226309c924226a53dead3e6fcf11": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-13381 : [SPS]: Use DFSUtilClient#makePathFromFileId() to prepare satisfier file path. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "66e8f9b31529226309c924226a53dead3e6fcf11",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "75ccc1396b677777cdc0d4992a4af3911f9f88c2",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n   public DatanodeMap getLiveDatanodeStorageReport(\n-      Context\u003cT\u003e spsContext) throws IOException {\n+      Context spsContext) throws IOException {\n     long now \u003d Time.monotonicNow();\n     long elapsedTimeMs \u003d now - lastAccessedTime;\n     boolean refreshNeeded \u003d elapsedTimeMs \u003e\u003d refreshIntervalMs;\n     lastAccessedTime \u003d now;\n     if (refreshNeeded) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"elapsedTimeMs \u003e refreshIntervalMs : {} \u003e {},\"\n             + \" so refreshing cache\", elapsedTimeMs, refreshIntervalMs);\n       }\n       datanodeMap.reset(); // clear all previously cached items.\n \n       // Fetch live datanodes from namenode and prepare DatanodeMap.\n       DatanodeStorageReport[] liveDns \u003d spsContext\n           .getLiveDatanodeStorageReport();\n       for (DatanodeStorageReport storage : liveDns) {\n         StorageReport[] storageReports \u003d storage.getStorageReports();\n         List\u003cStorageType\u003e storageTypes \u003d new ArrayList\u003c\u003e();\n         List\u003cLong\u003e remainingSizeList \u003d new ArrayList\u003c\u003e();\n         for (StorageReport t : storageReports) {\n           if (t.getRemaining() \u003e 0) {\n             storageTypes.add(t.getStorage().getStorageType());\n             remainingSizeList.add(t.getRemaining());\n           }\n         }\n         datanodeMap.addTarget(storage.getDatanodeInfo(), storageTypes,\n             remainingSizeList);\n       }\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"LIVE datanodes: {}\", datanodeMap);\n       }\n       // get network topology\n       cluster \u003d spsContext.getNetworkTopology(datanodeMap);\n     }\n     return datanodeMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeMap getLiveDatanodeStorageReport(\n      Context spsContext) throws IOException {\n    long now \u003d Time.monotonicNow();\n    long elapsedTimeMs \u003d now - lastAccessedTime;\n    boolean refreshNeeded \u003d elapsedTimeMs \u003e\u003d refreshIntervalMs;\n    lastAccessedTime \u003d now;\n    if (refreshNeeded) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"elapsedTimeMs \u003e refreshIntervalMs : {} \u003e {},\"\n            + \" so refreshing cache\", elapsedTimeMs, refreshIntervalMs);\n      }\n      datanodeMap.reset(); // clear all previously cached items.\n\n      // Fetch live datanodes from namenode and prepare DatanodeMap.\n      DatanodeStorageReport[] liveDns \u003d spsContext\n          .getLiveDatanodeStorageReport();\n      for (DatanodeStorageReport storage : liveDns) {\n        StorageReport[] storageReports \u003d storage.getStorageReports();\n        List\u003cStorageType\u003e storageTypes \u003d new ArrayList\u003c\u003e();\n        List\u003cLong\u003e remainingSizeList \u003d new ArrayList\u003c\u003e();\n        for (StorageReport t : storageReports) {\n          if (t.getRemaining() \u003e 0) {\n            storageTypes.add(t.getStorage().getStorageType());\n            remainingSizeList.add(t.getRemaining());\n          }\n        }\n        datanodeMap.addTarget(storage.getDatanodeInfo(), storageTypes,\n            remainingSizeList);\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"LIVE datanodes: {}\", datanodeMap);\n      }\n      // get network topology\n      cluster \u003d spsContext.getNetworkTopology(datanodeMap);\n    }\n    return datanodeMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/DatanodeCacheManager.java",
      "extendedDetails": {
        "oldValue": "[spsContext-Context\u003cT\u003e]",
        "newValue": "[spsContext-Context]"
      }
    },
    "75ccc1396b677777cdc0d4992a4af3911f9f88c2": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13166: [SPS]: Implement caching mechanism to keep LIVE datanodes to minimize costly getLiveDatanodeStorageReport() calls. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "75ccc1396b677777cdc0d4992a4af3911f9f88c2",
      "commitAuthor": "Surendra Singh Lilhore",
      "diff": "@@ -0,0 +1,37 @@\n+  public DatanodeMap getLiveDatanodeStorageReport(\n+      Context\u003cT\u003e spsContext) throws IOException {\n+    long now \u003d Time.monotonicNow();\n+    long elapsedTimeMs \u003d now - lastAccessedTime;\n+    boolean refreshNeeded \u003d elapsedTimeMs \u003e\u003d refreshIntervalMs;\n+    lastAccessedTime \u003d now;\n+    if (refreshNeeded) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"elapsedTimeMs \u003e refreshIntervalMs : {} \u003e {},\"\n+            + \" so refreshing cache\", elapsedTimeMs, refreshIntervalMs);\n+      }\n+      datanodeMap.reset(); // clear all previously cached items.\n+\n+      // Fetch live datanodes from namenode and prepare DatanodeMap.\n+      DatanodeStorageReport[] liveDns \u003d spsContext\n+          .getLiveDatanodeStorageReport();\n+      for (DatanodeStorageReport storage : liveDns) {\n+        StorageReport[] storageReports \u003d storage.getStorageReports();\n+        List\u003cStorageType\u003e storageTypes \u003d new ArrayList\u003c\u003e();\n+        List\u003cLong\u003e remainingSizeList \u003d new ArrayList\u003c\u003e();\n+        for (StorageReport t : storageReports) {\n+          if (t.getRemaining() \u003e 0) {\n+            storageTypes.add(t.getStorage().getStorageType());\n+            remainingSizeList.add(t.getRemaining());\n+          }\n+        }\n+        datanodeMap.addTarget(storage.getDatanodeInfo(), storageTypes,\n+            remainingSizeList);\n+      }\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"LIVE datanodes: {}\", datanodeMap);\n+      }\n+      // get network topology\n+      cluster \u003d spsContext.getNetworkTopology(datanodeMap);\n+    }\n+    return datanodeMap;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeMap getLiveDatanodeStorageReport(\n      Context\u003cT\u003e spsContext) throws IOException {\n    long now \u003d Time.monotonicNow();\n    long elapsedTimeMs \u003d now - lastAccessedTime;\n    boolean refreshNeeded \u003d elapsedTimeMs \u003e\u003d refreshIntervalMs;\n    lastAccessedTime \u003d now;\n    if (refreshNeeded) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"elapsedTimeMs \u003e refreshIntervalMs : {} \u003e {},\"\n            + \" so refreshing cache\", elapsedTimeMs, refreshIntervalMs);\n      }\n      datanodeMap.reset(); // clear all previously cached items.\n\n      // Fetch live datanodes from namenode and prepare DatanodeMap.\n      DatanodeStorageReport[] liveDns \u003d spsContext\n          .getLiveDatanodeStorageReport();\n      for (DatanodeStorageReport storage : liveDns) {\n        StorageReport[] storageReports \u003d storage.getStorageReports();\n        List\u003cStorageType\u003e storageTypes \u003d new ArrayList\u003c\u003e();\n        List\u003cLong\u003e remainingSizeList \u003d new ArrayList\u003c\u003e();\n        for (StorageReport t : storageReports) {\n          if (t.getRemaining() \u003e 0) {\n            storageTypes.add(t.getStorage().getStorageType());\n            remainingSizeList.add(t.getRemaining());\n          }\n        }\n        datanodeMap.addTarget(storage.getDatanodeInfo(), storageTypes,\n            remainingSizeList);\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"LIVE datanodes: {}\", datanodeMap);\n      }\n      // get network topology\n      cluster \u003d spsContext.getNetworkTopology(datanodeMap);\n    }\n    return datanodeMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/DatanodeCacheManager.java"
    }
  }
}