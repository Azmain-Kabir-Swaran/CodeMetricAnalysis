{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSOutputStream.java",
  "functionName": "newStreamForCreate",
  "functionId": "newStreamForCreate___dfsClient-DFSClient__src-String__masked-FsPermission__flag-EnumSet__CreateFlag____createParent-boolean__replication-short__blockSize-long__progress-Progressable__checksum-DataChecksum__favoredNodes-String[]__ecPolicyName-String__storagePolicy-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
  "functionStartLine": 263,
  "functionEndLine": 324,
  "numCommitsSeen": 253,
  "timeTaken": 7047,
  "changeHistory": [
    "0d7a5ac5f526801367a9ec963e6d72783b637d55",
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
    "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
    "20a076bafce548298729bab4fb81d12f829e8f7e"
  ],
  "changeHistoryShort": {
    "0d7a5ac5f526801367a9ec963e6d72783b637d55": "Ymultichange(Yparameterchange,Ybodychange)",
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1": "Ymultichange(Yparameterchange,Ybodychange)",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ymultichange(Yparameterchange,Ybodychange)",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d": "Ybodychange",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": "Ybodychange",
    "e96ce6f3e3e549202ce3c48d4733ba34098870ad": "Ymultichange(Yparameterchange,Ybodychange)",
    "20a076bafce548298729bab4fb81d12f829e8f7e": "Ybodychange"
  },
  "changeHistoryDetails": {
    "0d7a5ac5f526801367a9ec963e6d72783b637d55": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13209. DistributedFileSystem.create should allow an option to provide StoragePolicy. Contributed by Ayush Saxena.\n",
      "commitDate": "14/02/19 8:43 AM",
      "commitName": "0d7a5ac5f526801367a9ec963e6d72783b637d55",
      "commitAuthor": "Surendra Singh Lilhore",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13209. DistributedFileSystem.create should allow an option to provide StoragePolicy. Contributed by Ayush Saxena.\n",
          "commitDate": "14/02/19 8:43 AM",
          "commitName": "0d7a5ac5f526801367a9ec963e6d72783b637d55",
          "commitAuthor": "Surendra Singh Lilhore",
          "commitDateOld": "10/10/18 8:55 AM",
          "commitNameOld": "10185d9a77ce07080588f3c77399a07cd7ccf427",
          "commitAuthorOld": "Kitti Nanasi",
          "daysBetweenCommits": 127.03,
          "commitsBetweenForRepo": 929,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,62 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n       short replication, long blockSize, Progressable progress,\n-      DataChecksum checksum, String[] favoredNodes, String ecPolicyName)\n+      DataChecksum checksum, String[] favoredNodes, String ecPolicyName,\n+      String storagePolicy)\n       throws IOException {\n     try (TraceScope ignored \u003d\n              dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n       HdfsFileStatus stat \u003d null;\n \n       // Retry the create if we get a RetryStartFileException up to a maximum\n       // number of times\n       boolean shouldRetry \u003d true;\n       int retryCount \u003d CREATE_RETRY_COUNT;\n       while (shouldRetry) {\n         shouldRetry \u003d false;\n         try {\n           stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n               new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n-              blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName);\n+              blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName,\n+              storagePolicy);\n           break;\n         } catch (RemoteException re) {\n           IOException e \u003d re.unwrapRemoteException(\n               AccessControlException.class,\n               DSQuotaExceededException.class,\n               QuotaByStorageTypeExceededException.class,\n               FileAlreadyExistsException.class,\n               FileNotFoundException.class,\n               ParentNotDirectoryException.class,\n               NSQuotaExceededException.class,\n               RetryStartFileException.class,\n               SafeModeException.class,\n               UnresolvedPathException.class,\n               SnapshotAccessControlException.class,\n               UnknownCryptoProtocolVersionException.class);\n           if (e instanceof RetryStartFileException) {\n             if (retryCount \u003e 0) {\n               shouldRetry \u003d true;\n               retryCount--;\n             } else {\n               throw new IOException(\"Too many retries because of encryption\" +\n                   \" zone operations\", e);\n             }\n           } else {\n             throw e;\n           }\n         }\n       }\n       Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n       final DFSOutputStream out;\n       if(stat.getErasureCodingPolicy() !\u003d null) {\n         out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes);\n       } else {\n         out \u003d new DFSOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes, true);\n       }\n       out.start();\n       return out;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress,\n      DataChecksum checksum, String[] favoredNodes, String ecPolicyName,\n      String storagePolicy)\n      throws IOException {\n    try (TraceScope ignored \u003d\n             dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n      HdfsFileStatus stat \u003d null;\n\n      // Retry the create if we get a RetryStartFileException up to a maximum\n      // number of times\n      boolean shouldRetry \u003d true;\n      int retryCount \u003d CREATE_RETRY_COUNT;\n      while (shouldRetry) {\n        shouldRetry \u003d false;\n        try {\n          stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n              new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n              blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName,\n              storagePolicy);\n          break;\n        } catch (RemoteException re) {\n          IOException e \u003d re.unwrapRemoteException(\n              AccessControlException.class,\n              DSQuotaExceededException.class,\n              QuotaByStorageTypeExceededException.class,\n              FileAlreadyExistsException.class,\n              FileNotFoundException.class,\n              ParentNotDirectoryException.class,\n              NSQuotaExceededException.class,\n              RetryStartFileException.class,\n              SafeModeException.class,\n              UnresolvedPathException.class,\n              SnapshotAccessControlException.class,\n              UnknownCryptoProtocolVersionException.class);\n          if (e instanceof RetryStartFileException) {\n            if (retryCount \u003e 0) {\n              shouldRetry \u003d true;\n              retryCount--;\n            } else {\n              throw new IOException(\"Too many retries because of encryption\" +\n                  \" zone operations\", e);\n            }\n          } else {\n            throw e;\n          }\n        }\n      }\n      Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n      final DFSOutputStream out;\n      if(stat.getErasureCodingPolicy() !\u003d null) {\n        out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes);\n      } else {\n        out \u003d new DFSOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes, true);\n      }\n      out.start();\n      return out;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[dfsClient-DFSClient, src-String, masked-FsPermission, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, progress-Progressable, checksum-DataChecksum, favoredNodes-String[], ecPolicyName-String]",
            "newValue": "[dfsClient-DFSClient, src-String, masked-FsPermission, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, progress-Progressable, checksum-DataChecksum, favoredNodes-String[], ecPolicyName-String, storagePolicy-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13209. DistributedFileSystem.create should allow an option to provide StoragePolicy. Contributed by Ayush Saxena.\n",
          "commitDate": "14/02/19 8:43 AM",
          "commitName": "0d7a5ac5f526801367a9ec963e6d72783b637d55",
          "commitAuthor": "Surendra Singh Lilhore",
          "commitDateOld": "10/10/18 8:55 AM",
          "commitNameOld": "10185d9a77ce07080588f3c77399a07cd7ccf427",
          "commitAuthorOld": "Kitti Nanasi",
          "daysBetweenCommits": 127.03,
          "commitsBetweenForRepo": 929,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,62 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n       short replication, long blockSize, Progressable progress,\n-      DataChecksum checksum, String[] favoredNodes, String ecPolicyName)\n+      DataChecksum checksum, String[] favoredNodes, String ecPolicyName,\n+      String storagePolicy)\n       throws IOException {\n     try (TraceScope ignored \u003d\n              dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n       HdfsFileStatus stat \u003d null;\n \n       // Retry the create if we get a RetryStartFileException up to a maximum\n       // number of times\n       boolean shouldRetry \u003d true;\n       int retryCount \u003d CREATE_RETRY_COUNT;\n       while (shouldRetry) {\n         shouldRetry \u003d false;\n         try {\n           stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n               new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n-              blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName);\n+              blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName,\n+              storagePolicy);\n           break;\n         } catch (RemoteException re) {\n           IOException e \u003d re.unwrapRemoteException(\n               AccessControlException.class,\n               DSQuotaExceededException.class,\n               QuotaByStorageTypeExceededException.class,\n               FileAlreadyExistsException.class,\n               FileNotFoundException.class,\n               ParentNotDirectoryException.class,\n               NSQuotaExceededException.class,\n               RetryStartFileException.class,\n               SafeModeException.class,\n               UnresolvedPathException.class,\n               SnapshotAccessControlException.class,\n               UnknownCryptoProtocolVersionException.class);\n           if (e instanceof RetryStartFileException) {\n             if (retryCount \u003e 0) {\n               shouldRetry \u003d true;\n               retryCount--;\n             } else {\n               throw new IOException(\"Too many retries because of encryption\" +\n                   \" zone operations\", e);\n             }\n           } else {\n             throw e;\n           }\n         }\n       }\n       Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n       final DFSOutputStream out;\n       if(stat.getErasureCodingPolicy() !\u003d null) {\n         out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes);\n       } else {\n         out \u003d new DFSOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes, true);\n       }\n       out.start();\n       return out;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress,\n      DataChecksum checksum, String[] favoredNodes, String ecPolicyName,\n      String storagePolicy)\n      throws IOException {\n    try (TraceScope ignored \u003d\n             dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n      HdfsFileStatus stat \u003d null;\n\n      // Retry the create if we get a RetryStartFileException up to a maximum\n      // number of times\n      boolean shouldRetry \u003d true;\n      int retryCount \u003d CREATE_RETRY_COUNT;\n      while (shouldRetry) {\n        shouldRetry \u003d false;\n        try {\n          stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n              new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n              blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName,\n              storagePolicy);\n          break;\n        } catch (RemoteException re) {\n          IOException e \u003d re.unwrapRemoteException(\n              AccessControlException.class,\n              DSQuotaExceededException.class,\n              QuotaByStorageTypeExceededException.class,\n              FileAlreadyExistsException.class,\n              FileNotFoundException.class,\n              ParentNotDirectoryException.class,\n              NSQuotaExceededException.class,\n              RetryStartFileException.class,\n              SafeModeException.class,\n              UnresolvedPathException.class,\n              SnapshotAccessControlException.class,\n              UnknownCryptoProtocolVersionException.class);\n          if (e instanceof RetryStartFileException) {\n            if (retryCount \u003e 0) {\n              shouldRetry \u003d true;\n              retryCount--;\n            } else {\n              throw new IOException(\"Too many retries because of encryption\" +\n                  \" zone operations\", e);\n            }\n          } else {\n            throw e;\n          }\n        }\n      }\n      Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n      final DFSOutputStream out;\n      if(stat.getErasureCodingPolicy() !\u003d null) {\n        out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes);\n      } else {\n        out \u003d new DFSOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes, true);\n      }\n      out.start();\n      return out;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10996. Ability to specify per-file EC policy at create time. Contributed by SammiChen.\n",
      "commitDate": "12/04/17 12:27 PM",
      "commitName": "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10996. Ability to specify per-file EC policy at create time. Contributed by SammiChen.\n",
          "commitDate": "12/04/17 12:27 PM",
          "commitName": "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "06/04/17 4:11 PM",
          "commitNameOld": "0eacd4c13be9bad0fbed9421a6539c64bbda4df1",
          "commitAuthorOld": "Xiaoyu Yao",
          "daysBetweenCommits": 5.84,
          "commitsBetweenForRepo": 39,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,59 +1,60 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n       short replication, long blockSize, Progressable progress,\n-      DataChecksum checksum, String[] favoredNodes) throws IOException {\n+      DataChecksum checksum, String[] favoredNodes, String ecPolicyName)\n+      throws IOException {\n     try (TraceScope ignored \u003d\n              dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n       HdfsFileStatus stat \u003d null;\n \n       // Retry the create if we get a RetryStartFileException up to a maximum\n       // number of times\n       boolean shouldRetry \u003d true;\n       int retryCount \u003d CREATE_RETRY_COUNT;\n       while (shouldRetry) {\n         shouldRetry \u003d false;\n         try {\n           stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n               new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n-              blockSize, SUPPORTED_CRYPTO_VERSIONS);\n+              blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName);\n           break;\n         } catch (RemoteException re) {\n           IOException e \u003d re.unwrapRemoteException(\n               AccessControlException.class,\n               DSQuotaExceededException.class,\n               QuotaByStorageTypeExceededException.class,\n               FileAlreadyExistsException.class,\n               FileNotFoundException.class,\n               ParentNotDirectoryException.class,\n               NSQuotaExceededException.class,\n               RetryStartFileException.class,\n               SafeModeException.class,\n               UnresolvedPathException.class,\n               SnapshotAccessControlException.class,\n               UnknownCryptoProtocolVersionException.class);\n           if (e instanceof RetryStartFileException) {\n             if (retryCount \u003e 0) {\n               shouldRetry \u003d true;\n               retryCount--;\n             } else {\n               throw new IOException(\"Too many retries because of encryption\" +\n                   \" zone operations\", e);\n             }\n           } else {\n             throw e;\n           }\n         }\n       }\n       Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n       final DFSOutputStream out;\n       if(stat.getErasureCodingPolicy() !\u003d null) {\n         out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes);\n       } else {\n         out \u003d new DFSOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes, true);\n       }\n       out.start();\n       return out;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress,\n      DataChecksum checksum, String[] favoredNodes, String ecPolicyName)\n      throws IOException {\n    try (TraceScope ignored \u003d\n             dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n      HdfsFileStatus stat \u003d null;\n\n      // Retry the create if we get a RetryStartFileException up to a maximum\n      // number of times\n      boolean shouldRetry \u003d true;\n      int retryCount \u003d CREATE_RETRY_COUNT;\n      while (shouldRetry) {\n        shouldRetry \u003d false;\n        try {\n          stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n              new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n              blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName);\n          break;\n        } catch (RemoteException re) {\n          IOException e \u003d re.unwrapRemoteException(\n              AccessControlException.class,\n              DSQuotaExceededException.class,\n              QuotaByStorageTypeExceededException.class,\n              FileAlreadyExistsException.class,\n              FileNotFoundException.class,\n              ParentNotDirectoryException.class,\n              NSQuotaExceededException.class,\n              RetryStartFileException.class,\n              SafeModeException.class,\n              UnresolvedPathException.class,\n              SnapshotAccessControlException.class,\n              UnknownCryptoProtocolVersionException.class);\n          if (e instanceof RetryStartFileException) {\n            if (retryCount \u003e 0) {\n              shouldRetry \u003d true;\n              retryCount--;\n            } else {\n              throw new IOException(\"Too many retries because of encryption\" +\n                  \" zone operations\", e);\n            }\n          } else {\n            throw e;\n          }\n        }\n      }\n      Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n      final DFSOutputStream out;\n      if(stat.getErasureCodingPolicy() !\u003d null) {\n        out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes);\n      } else {\n        out \u003d new DFSOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes, true);\n      }\n      out.start();\n      return out;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[dfsClient-DFSClient, src-String, masked-FsPermission, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, progress-Progressable, checksum-DataChecksum, favoredNodes-String[]]",
            "newValue": "[dfsClient-DFSClient, src-String, masked-FsPermission, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, progress-Progressable, checksum-DataChecksum, favoredNodes-String[], ecPolicyName-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10996. Ability to specify per-file EC policy at create time. Contributed by SammiChen.\n",
          "commitDate": "12/04/17 12:27 PM",
          "commitName": "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "06/04/17 4:11 PM",
          "commitNameOld": "0eacd4c13be9bad0fbed9421a6539c64bbda4df1",
          "commitAuthorOld": "Xiaoyu Yao",
          "daysBetweenCommits": 5.84,
          "commitsBetweenForRepo": 39,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,59 +1,60 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n       short replication, long blockSize, Progressable progress,\n-      DataChecksum checksum, String[] favoredNodes) throws IOException {\n+      DataChecksum checksum, String[] favoredNodes, String ecPolicyName)\n+      throws IOException {\n     try (TraceScope ignored \u003d\n              dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n       HdfsFileStatus stat \u003d null;\n \n       // Retry the create if we get a RetryStartFileException up to a maximum\n       // number of times\n       boolean shouldRetry \u003d true;\n       int retryCount \u003d CREATE_RETRY_COUNT;\n       while (shouldRetry) {\n         shouldRetry \u003d false;\n         try {\n           stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n               new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n-              blockSize, SUPPORTED_CRYPTO_VERSIONS);\n+              blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName);\n           break;\n         } catch (RemoteException re) {\n           IOException e \u003d re.unwrapRemoteException(\n               AccessControlException.class,\n               DSQuotaExceededException.class,\n               QuotaByStorageTypeExceededException.class,\n               FileAlreadyExistsException.class,\n               FileNotFoundException.class,\n               ParentNotDirectoryException.class,\n               NSQuotaExceededException.class,\n               RetryStartFileException.class,\n               SafeModeException.class,\n               UnresolvedPathException.class,\n               SnapshotAccessControlException.class,\n               UnknownCryptoProtocolVersionException.class);\n           if (e instanceof RetryStartFileException) {\n             if (retryCount \u003e 0) {\n               shouldRetry \u003d true;\n               retryCount--;\n             } else {\n               throw new IOException(\"Too many retries because of encryption\" +\n                   \" zone operations\", e);\n             }\n           } else {\n             throw e;\n           }\n         }\n       }\n       Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n       final DFSOutputStream out;\n       if(stat.getErasureCodingPolicy() !\u003d null) {\n         out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes);\n       } else {\n         out \u003d new DFSOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes, true);\n       }\n       out.start();\n       return out;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress,\n      DataChecksum checksum, String[] favoredNodes, String ecPolicyName)\n      throws IOException {\n    try (TraceScope ignored \u003d\n             dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n      HdfsFileStatus stat \u003d null;\n\n      // Retry the create if we get a RetryStartFileException up to a maximum\n      // number of times\n      boolean shouldRetry \u003d true;\n      int retryCount \u003d CREATE_RETRY_COUNT;\n      while (shouldRetry) {\n        shouldRetry \u003d false;\n        try {\n          stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n              new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n              blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName);\n          break;\n        } catch (RemoteException re) {\n          IOException e \u003d re.unwrapRemoteException(\n              AccessControlException.class,\n              DSQuotaExceededException.class,\n              QuotaByStorageTypeExceededException.class,\n              FileAlreadyExistsException.class,\n              FileNotFoundException.class,\n              ParentNotDirectoryException.class,\n              NSQuotaExceededException.class,\n              RetryStartFileException.class,\n              SafeModeException.class,\n              UnresolvedPathException.class,\n              SnapshotAccessControlException.class,\n              UnknownCryptoProtocolVersionException.class);\n          if (e instanceof RetryStartFileException) {\n            if (retryCount \u003e 0) {\n              shouldRetry \u003d true;\n              retryCount--;\n            } else {\n              throw new IOException(\"Too many retries because of encryption\" +\n                  \" zone operations\", e);\n            }\n          } else {\n            throw e;\n          }\n        }\n      }\n      Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n      final DFSOutputStream out;\n      if(stat.getErasureCodingPolicy() !\u003d null) {\n        out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes);\n      } else {\n        out \u003d new DFSOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes, true);\n      }\n      out.start();\n      return out;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
          "commitDate": "03/10/15 11:38 AM",
          "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "30/09/15 8:39 AM",
          "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
          "commitAuthorOld": "",
          "daysBetweenCommits": 3.12,
          "commitsBetweenForRepo": 16,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,62 +1,59 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n-      short replication, long blockSize, Progressable progress, int buffersize,\n+      short replication, long blockSize, Progressable progress,\n       DataChecksum checksum, String[] favoredNodes) throws IOException {\n-    TraceScope scope \u003d\n-        dfsClient.newPathTraceScope(\"newStreamForCreate\", src);\n-    try {\n+    try (TraceScope ignored \u003d\n+             dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n       HdfsFileStatus stat \u003d null;\n \n       // Retry the create if we get a RetryStartFileException up to a maximum\n       // number of times\n       boolean shouldRetry \u003d true;\n       int retryCount \u003d CREATE_RETRY_COUNT;\n       while (shouldRetry) {\n         shouldRetry \u003d false;\n         try {\n           stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n-              new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n+              new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n               blockSize, SUPPORTED_CRYPTO_VERSIONS);\n           break;\n         } catch (RemoteException re) {\n           IOException e \u003d re.unwrapRemoteException(\n               AccessControlException.class,\n               DSQuotaExceededException.class,\n               QuotaByStorageTypeExceededException.class,\n               FileAlreadyExistsException.class,\n               FileNotFoundException.class,\n               ParentNotDirectoryException.class,\n               NSQuotaExceededException.class,\n               RetryStartFileException.class,\n               SafeModeException.class,\n               UnresolvedPathException.class,\n               SnapshotAccessControlException.class,\n               UnknownCryptoProtocolVersionException.class);\n           if (e instanceof RetryStartFileException) {\n             if (retryCount \u003e 0) {\n               shouldRetry \u003d true;\n               retryCount--;\n             } else {\n               throw new IOException(\"Too many retries because of encryption\" +\n                   \" zone operations\", e);\n             }\n           } else {\n             throw e;\n           }\n         }\n       }\n       Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n       final DFSOutputStream out;\n       if(stat.getErasureCodingPolicy() !\u003d null) {\n         out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes);\n       } else {\n         out \u003d new DFSOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes, true);\n       }\n       out.start();\n       return out;\n-    } finally {\n-      scope.close();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress,\n      DataChecksum checksum, String[] favoredNodes) throws IOException {\n    try (TraceScope ignored \u003d\n             dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n      HdfsFileStatus stat \u003d null;\n\n      // Retry the create if we get a RetryStartFileException up to a maximum\n      // number of times\n      boolean shouldRetry \u003d true;\n      int retryCount \u003d CREATE_RETRY_COUNT;\n      while (shouldRetry) {\n        shouldRetry \u003d false;\n        try {\n          stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n              new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n              blockSize, SUPPORTED_CRYPTO_VERSIONS);\n          break;\n        } catch (RemoteException re) {\n          IOException e \u003d re.unwrapRemoteException(\n              AccessControlException.class,\n              DSQuotaExceededException.class,\n              QuotaByStorageTypeExceededException.class,\n              FileAlreadyExistsException.class,\n              FileNotFoundException.class,\n              ParentNotDirectoryException.class,\n              NSQuotaExceededException.class,\n              RetryStartFileException.class,\n              SafeModeException.class,\n              UnresolvedPathException.class,\n              SnapshotAccessControlException.class,\n              UnknownCryptoProtocolVersionException.class);\n          if (e instanceof RetryStartFileException) {\n            if (retryCount \u003e 0) {\n              shouldRetry \u003d true;\n              retryCount--;\n            } else {\n              throw new IOException(\"Too many retries because of encryption\" +\n                  \" zone operations\", e);\n            }\n          } else {\n            throw e;\n          }\n        }\n      }\n      Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n      final DFSOutputStream out;\n      if(stat.getErasureCodingPolicy() !\u003d null) {\n        out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes);\n      } else {\n        out \u003d new DFSOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes, true);\n      }\n      out.start();\n      return out;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[dfsClient-DFSClient, src-String, masked-FsPermission, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, progress-Progressable, buffersize-int, checksum-DataChecksum, favoredNodes-String[]]",
            "newValue": "[dfsClient-DFSClient, src-String, masked-FsPermission, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, progress-Progressable, checksum-DataChecksum, favoredNodes-String[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
          "commitDate": "03/10/15 11:38 AM",
          "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "30/09/15 8:39 AM",
          "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
          "commitAuthorOld": "",
          "daysBetweenCommits": 3.12,
          "commitsBetweenForRepo": 16,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,62 +1,59 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n-      short replication, long blockSize, Progressable progress, int buffersize,\n+      short replication, long blockSize, Progressable progress,\n       DataChecksum checksum, String[] favoredNodes) throws IOException {\n-    TraceScope scope \u003d\n-        dfsClient.newPathTraceScope(\"newStreamForCreate\", src);\n-    try {\n+    try (TraceScope ignored \u003d\n+             dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n       HdfsFileStatus stat \u003d null;\n \n       // Retry the create if we get a RetryStartFileException up to a maximum\n       // number of times\n       boolean shouldRetry \u003d true;\n       int retryCount \u003d CREATE_RETRY_COUNT;\n       while (shouldRetry) {\n         shouldRetry \u003d false;\n         try {\n           stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n-              new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n+              new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n               blockSize, SUPPORTED_CRYPTO_VERSIONS);\n           break;\n         } catch (RemoteException re) {\n           IOException e \u003d re.unwrapRemoteException(\n               AccessControlException.class,\n               DSQuotaExceededException.class,\n               QuotaByStorageTypeExceededException.class,\n               FileAlreadyExistsException.class,\n               FileNotFoundException.class,\n               ParentNotDirectoryException.class,\n               NSQuotaExceededException.class,\n               RetryStartFileException.class,\n               SafeModeException.class,\n               UnresolvedPathException.class,\n               SnapshotAccessControlException.class,\n               UnknownCryptoProtocolVersionException.class);\n           if (e instanceof RetryStartFileException) {\n             if (retryCount \u003e 0) {\n               shouldRetry \u003d true;\n               retryCount--;\n             } else {\n               throw new IOException(\"Too many retries because of encryption\" +\n                   \" zone operations\", e);\n             }\n           } else {\n             throw e;\n           }\n         }\n       }\n       Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n       final DFSOutputStream out;\n       if(stat.getErasureCodingPolicy() !\u003d null) {\n         out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes);\n       } else {\n         out \u003d new DFSOutputStream(dfsClient, src, stat,\n             flag, progress, checksum, favoredNodes, true);\n       }\n       out.start();\n       return out;\n-    } finally {\n-      scope.close();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress,\n      DataChecksum checksum, String[] favoredNodes) throws IOException {\n    try (TraceScope ignored \u003d\n             dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\n      HdfsFileStatus stat \u003d null;\n\n      // Retry the create if we get a RetryStartFileException up to a maximum\n      // number of times\n      boolean shouldRetry \u003d true;\n      int retryCount \u003d CREATE_RETRY_COUNT;\n      while (shouldRetry) {\n        shouldRetry \u003d false;\n        try {\n          stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n              new EnumSetWritable\u003c\u003e(flag), createParent, replication,\n              blockSize, SUPPORTED_CRYPTO_VERSIONS);\n          break;\n        } catch (RemoteException re) {\n          IOException e \u003d re.unwrapRemoteException(\n              AccessControlException.class,\n              DSQuotaExceededException.class,\n              QuotaByStorageTypeExceededException.class,\n              FileAlreadyExistsException.class,\n              FileNotFoundException.class,\n              ParentNotDirectoryException.class,\n              NSQuotaExceededException.class,\n              RetryStartFileException.class,\n              SafeModeException.class,\n              UnresolvedPathException.class,\n              SnapshotAccessControlException.class,\n              UnknownCryptoProtocolVersionException.class);\n          if (e instanceof RetryStartFileException) {\n            if (retryCount \u003e 0) {\n              shouldRetry \u003d true;\n              retryCount--;\n            } else {\n              throw new IOException(\"Too many retries because of encryption\" +\n                  \" zone operations\", e);\n            }\n          } else {\n            throw e;\n          }\n        }\n      }\n      Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n      final DFSOutputStream out;\n      if(stat.getErasureCodingPolicy() !\u003d null) {\n        out \u003d new DFSStripedOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes);\n      } else {\n        out \u003d new DFSOutputStream(dfsClient, src, stat,\n            flag, progress, checksum, favoredNodes, true);\n      }\n      out.start();\n      return out;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/09/15 11:08 AM",
      "commitNameOld": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.86,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n       short replication, long blockSize, Progressable progress, int buffersize,\n       DataChecksum checksum, String[] favoredNodes) throws IOException {\n     TraceScope scope \u003d\n-        dfsClient.getPathTraceScope(\"newStreamForCreate\", src);\n+        dfsClient.newPathTraceScope(\"newStreamForCreate\", src);\n     try {\n       HdfsFileStatus stat \u003d null;\n \n       // Retry the create if we get a RetryStartFileException up to a maximum\n       // number of times\n       boolean shouldRetry \u003d true;\n       int retryCount \u003d CREATE_RETRY_COUNT;\n       while (shouldRetry) {\n         shouldRetry \u003d false;\n         try {\n           stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n               new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n               blockSize, SUPPORTED_CRYPTO_VERSIONS);\n           break;\n         } catch (RemoteException re) {\n           IOException e \u003d re.unwrapRemoteException(\n               AccessControlException.class,\n               DSQuotaExceededException.class,\n               QuotaByStorageTypeExceededException.class,\n               FileAlreadyExistsException.class,\n               FileNotFoundException.class,\n               ParentNotDirectoryException.class,\n               NSQuotaExceededException.class,\n               RetryStartFileException.class,\n               SafeModeException.class,\n               UnresolvedPathException.class,\n               SnapshotAccessControlException.class,\n               UnknownCryptoProtocolVersionException.class);\n           if (e instanceof RetryStartFileException) {\n             if (retryCount \u003e 0) {\n               shouldRetry \u003d true;\n               retryCount--;\n             } else {\n               throw new IOException(\"Too many retries because of encryption\" +\n                   \" zone operations\", e);\n             }\n           } else {\n             throw e;\n           }\n         }\n       }\n       Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n       final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n           flag, progress, checksum, favoredNodes);\n       out.start();\n       return out;\n     } finally {\n       scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress, int buffersize,\n      DataChecksum checksum, String[] favoredNodes) throws IOException {\n    TraceScope scope \u003d\n        dfsClient.newPathTraceScope(\"newStreamForCreate\", src);\n    try {\n      HdfsFileStatus stat \u003d null;\n\n      // Retry the create if we get a RetryStartFileException up to a maximum\n      // number of times\n      boolean shouldRetry \u003d true;\n      int retryCount \u003d CREATE_RETRY_COUNT;\n      while (shouldRetry) {\n        shouldRetry \u003d false;\n        try {\n          stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n              new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n              blockSize, SUPPORTED_CRYPTO_VERSIONS);\n          break;\n        } catch (RemoteException re) {\n          IOException e \u003d re.unwrapRemoteException(\n              AccessControlException.class,\n              DSQuotaExceededException.class,\n              QuotaByStorageTypeExceededException.class,\n              FileAlreadyExistsException.class,\n              FileNotFoundException.class,\n              ParentNotDirectoryException.class,\n              NSQuotaExceededException.class,\n              RetryStartFileException.class,\n              SafeModeException.class,\n              UnresolvedPathException.class,\n              SnapshotAccessControlException.class,\n              UnknownCryptoProtocolVersionException.class);\n          if (e instanceof RetryStartFileException) {\n            if (retryCount \u003e 0) {\n              shouldRetry \u003d true;\n              retryCount--;\n            } else {\n              throw new IOException(\"Too many retries because of encryption\" +\n                  \" zone operations\", e);\n            }\n          } else {\n            throw e;\n          }\n        }\n      }\n      Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n      final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n          flag, progress, checksum, favoredNodes);\n      out.start();\n      return out;\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress, int buffersize,\n      DataChecksum checksum, String[] favoredNodes) throws IOException {\n    TraceScope scope \u003d\n        dfsClient.getPathTraceScope(\"newStreamForCreate\", src);\n    try {\n      HdfsFileStatus stat \u003d null;\n\n      // Retry the create if we get a RetryStartFileException up to a maximum\n      // number of times\n      boolean shouldRetry \u003d true;\n      int retryCount \u003d CREATE_RETRY_COUNT;\n      while (shouldRetry) {\n        shouldRetry \u003d false;\n        try {\n          stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n              new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n              blockSize, SUPPORTED_CRYPTO_VERSIONS);\n          break;\n        } catch (RemoteException re) {\n          IOException e \u003d re.unwrapRemoteException(\n              AccessControlException.class,\n              DSQuotaExceededException.class,\n              QuotaByStorageTypeExceededException.class,\n              FileAlreadyExistsException.class,\n              FileNotFoundException.class,\n              ParentNotDirectoryException.class,\n              NSQuotaExceededException.class,\n              RetryStartFileException.class,\n              SafeModeException.class,\n              UnresolvedPathException.class,\n              SnapshotAccessControlException.class,\n              UnknownCryptoProtocolVersionException.class);\n          if (e instanceof RetryStartFileException) {\n            if (retryCount \u003e 0) {\n              shouldRetry \u003d true;\n              retryCount--;\n            } else {\n              throw new IOException(\"Too many retries because of encryption\" +\n                  \" zone operations\", e);\n            }\n          } else {\n            throw e;\n          }\n        }\n      }\n      Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n      final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n          flag, progress, checksum, favoredNodes);\n      out.start();\n      return out;\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8231. StackTrace displayed at client while QuotaByStorageType exceeds (Contributed by J.Andreina and Xiaoyu Yao)\n",
      "commitDate": "24/04/15 12:21 AM",
      "commitName": "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "10/04/15 2:48 PM",
      "commitNameOld": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 13.4,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,56 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n       short replication, long blockSize, Progressable progress, int buffersize,\n       DataChecksum checksum, String[] favoredNodes) throws IOException {\n     TraceScope scope \u003d\n         dfsClient.getPathTraceScope(\"newStreamForCreate\", src);\n     try {\n       HdfsFileStatus stat \u003d null;\n \n       // Retry the create if we get a RetryStartFileException up to a maximum\n       // number of times\n       boolean shouldRetry \u003d true;\n       int retryCount \u003d CREATE_RETRY_COUNT;\n       while (shouldRetry) {\n         shouldRetry \u003d false;\n         try {\n           stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n               new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n               blockSize, SUPPORTED_CRYPTO_VERSIONS);\n           break;\n         } catch (RemoteException re) {\n           IOException e \u003d re.unwrapRemoteException(\n               AccessControlException.class,\n               DSQuotaExceededException.class,\n+              QuotaByStorageTypeExceededException.class,\n               FileAlreadyExistsException.class,\n               FileNotFoundException.class,\n               ParentNotDirectoryException.class,\n               NSQuotaExceededException.class,\n               RetryStartFileException.class,\n               SafeModeException.class,\n               UnresolvedPathException.class,\n               SnapshotAccessControlException.class,\n               UnknownCryptoProtocolVersionException.class);\n           if (e instanceof RetryStartFileException) {\n             if (retryCount \u003e 0) {\n               shouldRetry \u003d true;\n               retryCount--;\n             } else {\n               throw new IOException(\"Too many retries because of encryption\" +\n                   \" zone operations\", e);\n             }\n           } else {\n             throw e;\n           }\n         }\n       }\n       Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n       final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n           flag, progress, checksum, favoredNodes);\n       out.start();\n       return out;\n     } finally {\n       scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress, int buffersize,\n      DataChecksum checksum, String[] favoredNodes) throws IOException {\n    TraceScope scope \u003d\n        dfsClient.getPathTraceScope(\"newStreamForCreate\", src);\n    try {\n      HdfsFileStatus stat \u003d null;\n\n      // Retry the create if we get a RetryStartFileException up to a maximum\n      // number of times\n      boolean shouldRetry \u003d true;\n      int retryCount \u003d CREATE_RETRY_COUNT;\n      while (shouldRetry) {\n        shouldRetry \u003d false;\n        try {\n          stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n              new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n              blockSize, SUPPORTED_CRYPTO_VERSIONS);\n          break;\n        } catch (RemoteException re) {\n          IOException e \u003d re.unwrapRemoteException(\n              AccessControlException.class,\n              DSQuotaExceededException.class,\n              QuotaByStorageTypeExceededException.class,\n              FileAlreadyExistsException.class,\n              FileNotFoundException.class,\n              ParentNotDirectoryException.class,\n              NSQuotaExceededException.class,\n              RetryStartFileException.class,\n              SafeModeException.class,\n              UnresolvedPathException.class,\n              SnapshotAccessControlException.class,\n              UnknownCryptoProtocolVersionException.class);\n          if (e instanceof RetryStartFileException) {\n            if (retryCount \u003e 0) {\n              shouldRetry \u003d true;\n              retryCount--;\n            } else {\n              throw new IOException(\"Too many retries because of encryption\" +\n                  \" zone operations\", e);\n            }\n          } else {\n            throw e;\n          }\n        }\n      }\n      Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n      final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n          flag, progress, checksum, favoredNodes);\n      out.start();\n      return out;\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7054. Make DFSOutputStream tracing more fine-grained (cmccabe)\n",
      "commitDate": "18/03/15 6:14 PM",
      "commitName": "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "16/03/15 9:58 PM",
      "commitNameOld": "046521cd6511b7fc6d9478cb2bed90d8e75fca20",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,55 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n       short replication, long blockSize, Progressable progress, int buffersize,\n       DataChecksum checksum, String[] favoredNodes) throws IOException {\n-    HdfsFileStatus stat \u003d null;\n+    TraceScope scope \u003d\n+        dfsClient.getPathTraceScope(\"newStreamForCreate\", src);\n+    try {\n+      HdfsFileStatus stat \u003d null;\n \n-    // Retry the create if we get a RetryStartFileException up to a maximum\n-    // number of times\n-    boolean shouldRetry \u003d true;\n-    int retryCount \u003d CREATE_RETRY_COUNT;\n-    while (shouldRetry) {\n-      shouldRetry \u003d false;\n-      try {\n-        stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n-            new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n-            blockSize, SUPPORTED_CRYPTO_VERSIONS);\n-        break;\n-      } catch (RemoteException re) {\n-        IOException e \u003d re.unwrapRemoteException(\n-            AccessControlException.class,\n-            DSQuotaExceededException.class,\n-            FileAlreadyExistsException.class,\n-            FileNotFoundException.class,\n-            ParentNotDirectoryException.class,\n-            NSQuotaExceededException.class,\n-            RetryStartFileException.class,\n-            SafeModeException.class,\n-            UnresolvedPathException.class,\n-            SnapshotAccessControlException.class,\n-            UnknownCryptoProtocolVersionException.class);\n-        if (e instanceof RetryStartFileException) {\n-          if (retryCount \u003e 0) {\n-            shouldRetry \u003d true;\n-            retryCount--;\n+      // Retry the create if we get a RetryStartFileException up to a maximum\n+      // number of times\n+      boolean shouldRetry \u003d true;\n+      int retryCount \u003d CREATE_RETRY_COUNT;\n+      while (shouldRetry) {\n+        shouldRetry \u003d false;\n+        try {\n+          stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n+              new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n+              blockSize, SUPPORTED_CRYPTO_VERSIONS);\n+          break;\n+        } catch (RemoteException re) {\n+          IOException e \u003d re.unwrapRemoteException(\n+              AccessControlException.class,\n+              DSQuotaExceededException.class,\n+              FileAlreadyExistsException.class,\n+              FileNotFoundException.class,\n+              ParentNotDirectoryException.class,\n+              NSQuotaExceededException.class,\n+              RetryStartFileException.class,\n+              SafeModeException.class,\n+              UnresolvedPathException.class,\n+              SnapshotAccessControlException.class,\n+              UnknownCryptoProtocolVersionException.class);\n+          if (e instanceof RetryStartFileException) {\n+            if (retryCount \u003e 0) {\n+              shouldRetry \u003d true;\n+              retryCount--;\n+            } else {\n+              throw new IOException(\"Too many retries because of encryption\" +\n+                  \" zone operations\", e);\n+            }\n           } else {\n-            throw new IOException(\"Too many retries because of encryption\" +\n-                \" zone operations\", e);\n+            throw e;\n           }\n-        } else {\n-          throw e;\n         }\n       }\n+      Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n+      final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n+          flag, progress, checksum, favoredNodes);\n+      out.start();\n+      return out;\n+    } finally {\n+      scope.close();\n     }\n-    Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n-    final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n-        flag, progress, checksum, favoredNodes);\n-    out.start();\n-    return out;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress, int buffersize,\n      DataChecksum checksum, String[] favoredNodes) throws IOException {\n    TraceScope scope \u003d\n        dfsClient.getPathTraceScope(\"newStreamForCreate\", src);\n    try {\n      HdfsFileStatus stat \u003d null;\n\n      // Retry the create if we get a RetryStartFileException up to a maximum\n      // number of times\n      boolean shouldRetry \u003d true;\n      int retryCount \u003d CREATE_RETRY_COUNT;\n      while (shouldRetry) {\n        shouldRetry \u003d false;\n        try {\n          stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n              new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n              blockSize, SUPPORTED_CRYPTO_VERSIONS);\n          break;\n        } catch (RemoteException re) {\n          IOException e \u003d re.unwrapRemoteException(\n              AccessControlException.class,\n              DSQuotaExceededException.class,\n              FileAlreadyExistsException.class,\n              FileNotFoundException.class,\n              ParentNotDirectoryException.class,\n              NSQuotaExceededException.class,\n              RetryStartFileException.class,\n              SafeModeException.class,\n              UnresolvedPathException.class,\n              SnapshotAccessControlException.class,\n              UnknownCryptoProtocolVersionException.class);\n          if (e instanceof RetryStartFileException) {\n            if (retryCount \u003e 0) {\n              shouldRetry \u003d true;\n              retryCount--;\n            } else {\n              throw new IOException(\"Too many retries because of encryption\" +\n                  \" zone operations\", e);\n            }\n          } else {\n            throw e;\n          }\n        }\n      }\n      Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n      final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n          flag, progress, checksum, favoredNodes);\n      out.start();\n      return out;\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "e96ce6f3e3e549202ce3c48d4733ba34098870ad": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7077. Separate CipherSuite from crypto protocol version. (wang)\n",
      "commitDate": "25/09/14 6:40 PM",
      "commitName": "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7077. Separate CipherSuite from crypto protocol version. (wang)\n",
          "commitDate": "25/09/14 6:40 PM",
          "commitName": "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "18/09/14 5:35 PM",
          "commitNameOld": "20a076bafce548298729bab4fb81d12f829e8f7e",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 7.05,
          "commitsBetweenForRepo": 70,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,49 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n       short replication, long blockSize, Progressable progress, int buffersize,\n-      DataChecksum checksum, String[] favoredNodes,\n-      List\u003cCipherSuite\u003e cipherSuites) throws IOException {\n+      DataChecksum checksum, String[] favoredNodes) throws IOException {\n     HdfsFileStatus stat \u003d null;\n \n     // Retry the create if we get a RetryStartFileException up to a maximum\n     // number of times\n     boolean shouldRetry \u003d true;\n     int retryCount \u003d CREATE_RETRY_COUNT;\n     while (shouldRetry) {\n       shouldRetry \u003d false;\n       try {\n         stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n             new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n-            blockSize, cipherSuites);\n+            blockSize, SUPPORTED_CRYPTO_VERSIONS);\n         break;\n       } catch (RemoteException re) {\n         IOException e \u003d re.unwrapRemoteException(\n             AccessControlException.class,\n             DSQuotaExceededException.class,\n             FileAlreadyExistsException.class,\n             FileNotFoundException.class,\n             ParentNotDirectoryException.class,\n             NSQuotaExceededException.class,\n             RetryStartFileException.class,\n             SafeModeException.class,\n             UnresolvedPathException.class,\n             SnapshotAccessControlException.class,\n-            UnknownCipherSuiteException.class);\n+            UnknownCryptoProtocolVersionException.class);\n         if (e instanceof RetryStartFileException) {\n           if (retryCount \u003e 0) {\n             shouldRetry \u003d true;\n             retryCount--;\n           } else {\n             throw new IOException(\"Too many retries because of encryption\" +\n                 \" zone operations\", e);\n           }\n         } else {\n           throw e;\n         }\n       }\n     }\n     Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n     final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n         flag, progress, checksum, favoredNodes);\n     out.start();\n     return out;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress, int buffersize,\n      DataChecksum checksum, String[] favoredNodes) throws IOException {\n    HdfsFileStatus stat \u003d null;\n\n    // Retry the create if we get a RetryStartFileException up to a maximum\n    // number of times\n    boolean shouldRetry \u003d true;\n    int retryCount \u003d CREATE_RETRY_COUNT;\n    while (shouldRetry) {\n      shouldRetry \u003d false;\n      try {\n        stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n            new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n            blockSize, SUPPORTED_CRYPTO_VERSIONS);\n        break;\n      } catch (RemoteException re) {\n        IOException e \u003d re.unwrapRemoteException(\n            AccessControlException.class,\n            DSQuotaExceededException.class,\n            FileAlreadyExistsException.class,\n            FileNotFoundException.class,\n            ParentNotDirectoryException.class,\n            NSQuotaExceededException.class,\n            RetryStartFileException.class,\n            SafeModeException.class,\n            UnresolvedPathException.class,\n            SnapshotAccessControlException.class,\n            UnknownCryptoProtocolVersionException.class);\n        if (e instanceof RetryStartFileException) {\n          if (retryCount \u003e 0) {\n            shouldRetry \u003d true;\n            retryCount--;\n          } else {\n            throw new IOException(\"Too many retries because of encryption\" +\n                \" zone operations\", e);\n          }\n        } else {\n          throw e;\n        }\n      }\n    }\n    Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n    final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n        flag, progress, checksum, favoredNodes);\n    out.start();\n    return out;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[dfsClient-DFSClient, src-String, masked-FsPermission, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, progress-Progressable, buffersize-int, checksum-DataChecksum, favoredNodes-String[], cipherSuites-List\u003cCipherSuite\u003e]",
            "newValue": "[dfsClient-DFSClient, src-String, masked-FsPermission, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, progress-Progressable, buffersize-int, checksum-DataChecksum, favoredNodes-String[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7077. Separate CipherSuite from crypto protocol version. (wang)\n",
          "commitDate": "25/09/14 6:40 PM",
          "commitName": "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "18/09/14 5:35 PM",
          "commitNameOld": "20a076bafce548298729bab4fb81d12f829e8f7e",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 7.05,
          "commitsBetweenForRepo": 70,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,49 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n       short replication, long blockSize, Progressable progress, int buffersize,\n-      DataChecksum checksum, String[] favoredNodes,\n-      List\u003cCipherSuite\u003e cipherSuites) throws IOException {\n+      DataChecksum checksum, String[] favoredNodes) throws IOException {\n     HdfsFileStatus stat \u003d null;\n \n     // Retry the create if we get a RetryStartFileException up to a maximum\n     // number of times\n     boolean shouldRetry \u003d true;\n     int retryCount \u003d CREATE_RETRY_COUNT;\n     while (shouldRetry) {\n       shouldRetry \u003d false;\n       try {\n         stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n             new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n-            blockSize, cipherSuites);\n+            blockSize, SUPPORTED_CRYPTO_VERSIONS);\n         break;\n       } catch (RemoteException re) {\n         IOException e \u003d re.unwrapRemoteException(\n             AccessControlException.class,\n             DSQuotaExceededException.class,\n             FileAlreadyExistsException.class,\n             FileNotFoundException.class,\n             ParentNotDirectoryException.class,\n             NSQuotaExceededException.class,\n             RetryStartFileException.class,\n             SafeModeException.class,\n             UnresolvedPathException.class,\n             SnapshotAccessControlException.class,\n-            UnknownCipherSuiteException.class);\n+            UnknownCryptoProtocolVersionException.class);\n         if (e instanceof RetryStartFileException) {\n           if (retryCount \u003e 0) {\n             shouldRetry \u003d true;\n             retryCount--;\n           } else {\n             throw new IOException(\"Too many retries because of encryption\" +\n                 \" zone operations\", e);\n           }\n         } else {\n           throw e;\n         }\n       }\n     }\n     Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n     final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n         flag, progress, checksum, favoredNodes);\n     out.start();\n     return out;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress, int buffersize,\n      DataChecksum checksum, String[] favoredNodes) throws IOException {\n    HdfsFileStatus stat \u003d null;\n\n    // Retry the create if we get a RetryStartFileException up to a maximum\n    // number of times\n    boolean shouldRetry \u003d true;\n    int retryCount \u003d CREATE_RETRY_COUNT;\n    while (shouldRetry) {\n      shouldRetry \u003d false;\n      try {\n        stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n            new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n            blockSize, SUPPORTED_CRYPTO_VERSIONS);\n        break;\n      } catch (RemoteException re) {\n        IOException e \u003d re.unwrapRemoteException(\n            AccessControlException.class,\n            DSQuotaExceededException.class,\n            FileAlreadyExistsException.class,\n            FileNotFoundException.class,\n            ParentNotDirectoryException.class,\n            NSQuotaExceededException.class,\n            RetryStartFileException.class,\n            SafeModeException.class,\n            UnresolvedPathException.class,\n            SnapshotAccessControlException.class,\n            UnknownCryptoProtocolVersionException.class);\n        if (e instanceof RetryStartFileException) {\n          if (retryCount \u003e 0) {\n            shouldRetry \u003d true;\n            retryCount--;\n          } else {\n            throw new IOException(\"Too many retries because of encryption\" +\n                \" zone operations\", e);\n          }\n        } else {\n          throw e;\n        }\n      }\n    }\n    Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n    final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n        flag, progress, checksum, favoredNodes);\n    out.start();\n    return out;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "20a076bafce548298729bab4fb81d12f829e8f7e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6970. Move startFile EDEK retries to the DFSClient. (wang)\n",
      "commitDate": "18/09/14 5:35 PM",
      "commitName": "20a076bafce548298729bab4fb81d12f829e8f7e",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "16/09/14 1:59 PM",
      "commitNameOld": "56119fec96abbcc44c5dd82fdb694d2c3b53feb3",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 2.15,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,50 @@\n   static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n       FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n       short replication, long blockSize, Progressable progress, int buffersize,\n       DataChecksum checksum, String[] favoredNodes,\n       List\u003cCipherSuite\u003e cipherSuites) throws IOException {\n-    final HdfsFileStatus stat;\n-    try {\n-      stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n-          new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n-          blockSize, cipherSuites);\n-    } catch(RemoteException re) {\n-      throw re.unwrapRemoteException(AccessControlException.class,\n-                                     DSQuotaExceededException.class,\n-                                     FileAlreadyExistsException.class,\n-                                     FileNotFoundException.class,\n-                                     ParentNotDirectoryException.class,\n-                                     NSQuotaExceededException.class,\n-                                     SafeModeException.class,\n-                                     UnresolvedPathException.class,\n-                                     SnapshotAccessControlException.class,\n-                                     UnknownCipherSuiteException.class);\n+    HdfsFileStatus stat \u003d null;\n+\n+    // Retry the create if we get a RetryStartFileException up to a maximum\n+    // number of times\n+    boolean shouldRetry \u003d true;\n+    int retryCount \u003d CREATE_RETRY_COUNT;\n+    while (shouldRetry) {\n+      shouldRetry \u003d false;\n+      try {\n+        stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n+            new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n+            blockSize, cipherSuites);\n+        break;\n+      } catch (RemoteException re) {\n+        IOException e \u003d re.unwrapRemoteException(\n+            AccessControlException.class,\n+            DSQuotaExceededException.class,\n+            FileAlreadyExistsException.class,\n+            FileNotFoundException.class,\n+            ParentNotDirectoryException.class,\n+            NSQuotaExceededException.class,\n+            RetryStartFileException.class,\n+            SafeModeException.class,\n+            UnresolvedPathException.class,\n+            SnapshotAccessControlException.class,\n+            UnknownCipherSuiteException.class);\n+        if (e instanceof RetryStartFileException) {\n+          if (retryCount \u003e 0) {\n+            shouldRetry \u003d true;\n+            retryCount--;\n+          } else {\n+            throw new IOException(\"Too many retries because of encryption\" +\n+                \" zone operations\", e);\n+          }\n+        } else {\n+          throw e;\n+        }\n+      }\n     }\n+    Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n     final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n         flag, progress, checksum, favoredNodes);\n     out.start();\n     return out;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,\n      FsPermission masked, EnumSet\u003cCreateFlag\u003e flag, boolean createParent,\n      short replication, long blockSize, Progressable progress, int buffersize,\n      DataChecksum checksum, String[] favoredNodes,\n      List\u003cCipherSuite\u003e cipherSuites) throws IOException {\n    HdfsFileStatus stat \u003d null;\n\n    // Retry the create if we get a RetryStartFileException up to a maximum\n    // number of times\n    boolean shouldRetry \u003d true;\n    int retryCount \u003d CREATE_RETRY_COUNT;\n    while (shouldRetry) {\n      shouldRetry \u003d false;\n      try {\n        stat \u003d dfsClient.namenode.create(src, masked, dfsClient.clientName,\n            new EnumSetWritable\u003cCreateFlag\u003e(flag), createParent, replication,\n            blockSize, cipherSuites);\n        break;\n      } catch (RemoteException re) {\n        IOException e \u003d re.unwrapRemoteException(\n            AccessControlException.class,\n            DSQuotaExceededException.class,\n            FileAlreadyExistsException.class,\n            FileNotFoundException.class,\n            ParentNotDirectoryException.class,\n            NSQuotaExceededException.class,\n            RetryStartFileException.class,\n            SafeModeException.class,\n            UnresolvedPathException.class,\n            SnapshotAccessControlException.class,\n            UnknownCipherSuiteException.class);\n        if (e instanceof RetryStartFileException) {\n          if (retryCount \u003e 0) {\n            shouldRetry \u003d true;\n            retryCount--;\n          } else {\n            throw new IOException(\"Too many retries because of encryption\" +\n                \" zone operations\", e);\n          }\n        } else {\n          throw e;\n        }\n      }\n    }\n    Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\n    final DFSOutputStream out \u003d new DFSOutputStream(dfsClient, src, stat,\n        flag, progress, checksum, favoredNodes);\n    out.start();\n    return out;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    }
  }
}