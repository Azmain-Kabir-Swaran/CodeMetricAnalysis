{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DynamoDBMetadataStore.java",
  "functionName": "innerPrune",
  "functionId": "innerPrune___pruneMode-PruneMode(modifiers-final)__cutoff-long(modifiers-final)__keyPrefix-String(modifiers-final)__items-ItemCollection__ScanOutcome__(modifiers-final)",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
  "functionStartLine": 1608,
  "functionEndLine": 1685,
  "numCommitsSeen": 107,
  "timeTaken": 5203,
  "changeHistory": [
    "49df83899543586bbcaf80f01399ade031cf68b0",
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
    "e02eb24e0a9139418120027b694492e0738df20a",
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c"
  ],
  "changeHistoryShort": {
    "49df83899543586bbcaf80f01399ade031cf68b0": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange,Yparametermetachange)",
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0": "Ybodychange",
    "e02eb24e0a9139418120027b694492e0738df20a": "Ymultichange(Yparameterchange,Ybodychange)",
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c": "Yintroduced"
  },
  "changeHistoryDetails": {
    "49df83899543586bbcaf80f01399ade031cf68b0": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange,Yparametermetachange)",
      "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
      "commitDate": "10/01/20 3:11 AM",
      "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
          "commitDate": "10/01/20 3:11 AM",
          "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "26/11/19 7:36 AM",
          "commitNameOld": "ea25f4de236611d388e14a710ebe5d6872c421b6",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 44.82,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,71 +1,78 @@\n-  private void innerPrune(String keyPrefix, ItemCollection\u003cScanOutcome\u003e items)\n+  private int innerPrune(\n+      final PruneMode pruneMode, final long cutoff, final String keyPrefix,\n+      final ItemCollection\u003cScanOutcome\u003e items)\n       throws IOException {\n     int itemCount \u003d 0;\n     try (AncestorState state \u003d initiateBulkWrite(\n         BulkOperationState.OperationType.Prune, null);\n          DurationInfo ignored \u003d\n              new DurationInfo(LOG, \"Pruning DynamoDB Store\")) {\n       ArrayList\u003cPath\u003e deletionBatch \u003d\n           new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n       long delay \u003d conf.getTimeDuration(\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n           TimeUnit.MILLISECONDS);\n       Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n       Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n+      // declare the operation to delete a batch as a function so\n+      // as to keep the code consistent across multiple uses.\n+      FunctionsRaisingIOE.CallableRaisingIOE\u003cVoid\u003e deleteBatchOperation \u003d\n+          () -\u003e {\n+            // lowest path entries get deleted first.\n+            deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n+            processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n+\n+            // set authoritative false for each pruned dir listing\n+            // if at least one entry was not a tombstone\n+            removeAuthoritativeDirFlag(parentPathSet, state);\n+            // already cleared parent paths.\n+            clearedParentPathSet.addAll(parentPathSet);\n+            parentPathSet.clear();\n+            return null;\n+          };\n       for (Item item : items) {\n         DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n             .itemToPathMetadata(item, username);\n         Path path \u003d md.getFileStatus().getPath();\n         boolean tombstone \u003d md.isDeleted();\n         LOG.debug(\"Prune entry {}\", path);\n         deletionBatch.add(path);\n \n         // add parent path of item so it can be marked as non-auth.\n         // this is only done if\n         // * it has not already been processed\n         // * the entry pruned is not a tombstone (no need to update)\n         // * the file is not in the root dir\n         Path parentPath \u003d path.getParent();\n         if (!tombstone\n             \u0026\u0026 parentPath !\u003d null\n+            \u0026\u0026 !parentPath.isRoot()\n             \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n           parentPathSet.add(parentPath);\n         }\n \n         itemCount++;\n         if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n-          // lowest path entries get deleted first.\n-          deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n-          processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n-\n-          // set authoritative false for each pruned dir listing\n-          removeAuthoritativeDirFlag(parentPathSet, state);\n-          // already cleared parent paths.\n-          clearedParentPathSet.addAll(parentPathSet);\n-          parentPathSet.clear();\n-\n+          deleteBatchOperation.apply();\n           deletionBatch.clear();\n           if (delay \u003e 0) {\n             Thread.sleep(delay);\n           }\n         }\n       }\n       // final batch of deletes\n       if (!deletionBatch.isEmpty()) {\n-        processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n-\n-        // set authoritative false for each pruned dir listing\n-        removeAuthoritativeDirFlag(parentPathSet, state);\n-        parentPathSet.clear();\n+        deleteBatchOperation.apply();\n       }\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\"Pruning was interrupted\");\n     } catch (AmazonDynamoDBException e) {\n       throw translateDynamoDBException(keyPrefix,\n           \"Prune of \" + keyPrefix + \" failed\", e);\n     }\n     LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n         S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n+    return itemCount;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int innerPrune(\n      final PruneMode pruneMode, final long cutoff, final String keyPrefix,\n      final ItemCollection\u003cScanOutcome\u003e items)\n      throws IOException {\n    int itemCount \u003d 0;\n    try (AncestorState state \u003d initiateBulkWrite(\n        BulkOperationState.OperationType.Prune, null);\n         DurationInfo ignored \u003d\n             new DurationInfo(LOG, \"Pruning DynamoDB Store\")) {\n      ArrayList\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      long delay \u003d conf.getTimeDuration(\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n          TimeUnit.MILLISECONDS);\n      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n      Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n      // declare the operation to delete a batch as a function so\n      // as to keep the code consistent across multiple uses.\n      FunctionsRaisingIOE.CallableRaisingIOE\u003cVoid\u003e deleteBatchOperation \u003d\n          () -\u003e {\n            // lowest path entries get deleted first.\n            deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n            processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n\n            // set authoritative false for each pruned dir listing\n            // if at least one entry was not a tombstone\n            removeAuthoritativeDirFlag(parentPathSet, state);\n            // already cleared parent paths.\n            clearedParentPathSet.addAll(parentPathSet);\n            parentPathSet.clear();\n            return null;\n          };\n      for (Item item : items) {\n        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        boolean tombstone \u003d md.isDeleted();\n        LOG.debug(\"Prune entry {}\", path);\n        deletionBatch.add(path);\n\n        // add parent path of item so it can be marked as non-auth.\n        // this is only done if\n        // * it has not already been processed\n        // * the entry pruned is not a tombstone (no need to update)\n        // * the file is not in the root dir\n        Path parentPath \u003d path.getParent();\n        if (!tombstone\n            \u0026\u0026 parentPath !\u003d null\n            \u0026\u0026 !parentPath.isRoot()\n            \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n          parentPathSet.add(parentPath);\n        }\n\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          deleteBatchOperation.apply();\n          deletionBatch.clear();\n          if (delay \u003e 0) {\n            Thread.sleep(delay);\n          }\n        }\n      }\n      // final batch of deletes\n      if (!deletionBatch.isEmpty()) {\n        deleteBatchOperation.apply();\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    } catch (AmazonDynamoDBException e) {\n      throw translateDynamoDBException(keyPrefix,\n          \"Prune of \" + keyPrefix + \" failed\", e);\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n    return itemCount;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {
            "oldValue": "[keyPrefix-String, items-ItemCollection\u003cScanOutcome\u003e]",
            "newValue": "[pruneMode-PruneMode(modifiers-final), cutoff-long(modifiers-final), keyPrefix-String(modifiers-final), items-ItemCollection\u003cScanOutcome\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
          "commitDate": "10/01/20 3:11 AM",
          "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "26/11/19 7:36 AM",
          "commitNameOld": "ea25f4de236611d388e14a710ebe5d6872c421b6",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 44.82,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,71 +1,78 @@\n-  private void innerPrune(String keyPrefix, ItemCollection\u003cScanOutcome\u003e items)\n+  private int innerPrune(\n+      final PruneMode pruneMode, final long cutoff, final String keyPrefix,\n+      final ItemCollection\u003cScanOutcome\u003e items)\n       throws IOException {\n     int itemCount \u003d 0;\n     try (AncestorState state \u003d initiateBulkWrite(\n         BulkOperationState.OperationType.Prune, null);\n          DurationInfo ignored \u003d\n              new DurationInfo(LOG, \"Pruning DynamoDB Store\")) {\n       ArrayList\u003cPath\u003e deletionBatch \u003d\n           new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n       long delay \u003d conf.getTimeDuration(\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n           TimeUnit.MILLISECONDS);\n       Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n       Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n+      // declare the operation to delete a batch as a function so\n+      // as to keep the code consistent across multiple uses.\n+      FunctionsRaisingIOE.CallableRaisingIOE\u003cVoid\u003e deleteBatchOperation \u003d\n+          () -\u003e {\n+            // lowest path entries get deleted first.\n+            deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n+            processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n+\n+            // set authoritative false for each pruned dir listing\n+            // if at least one entry was not a tombstone\n+            removeAuthoritativeDirFlag(parentPathSet, state);\n+            // already cleared parent paths.\n+            clearedParentPathSet.addAll(parentPathSet);\n+            parentPathSet.clear();\n+            return null;\n+          };\n       for (Item item : items) {\n         DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n             .itemToPathMetadata(item, username);\n         Path path \u003d md.getFileStatus().getPath();\n         boolean tombstone \u003d md.isDeleted();\n         LOG.debug(\"Prune entry {}\", path);\n         deletionBatch.add(path);\n \n         // add parent path of item so it can be marked as non-auth.\n         // this is only done if\n         // * it has not already been processed\n         // * the entry pruned is not a tombstone (no need to update)\n         // * the file is not in the root dir\n         Path parentPath \u003d path.getParent();\n         if (!tombstone\n             \u0026\u0026 parentPath !\u003d null\n+            \u0026\u0026 !parentPath.isRoot()\n             \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n           parentPathSet.add(parentPath);\n         }\n \n         itemCount++;\n         if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n-          // lowest path entries get deleted first.\n-          deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n-          processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n-\n-          // set authoritative false for each pruned dir listing\n-          removeAuthoritativeDirFlag(parentPathSet, state);\n-          // already cleared parent paths.\n-          clearedParentPathSet.addAll(parentPathSet);\n-          parentPathSet.clear();\n-\n+          deleteBatchOperation.apply();\n           deletionBatch.clear();\n           if (delay \u003e 0) {\n             Thread.sleep(delay);\n           }\n         }\n       }\n       // final batch of deletes\n       if (!deletionBatch.isEmpty()) {\n-        processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n-\n-        // set authoritative false for each pruned dir listing\n-        removeAuthoritativeDirFlag(parentPathSet, state);\n-        parentPathSet.clear();\n+        deleteBatchOperation.apply();\n       }\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\"Pruning was interrupted\");\n     } catch (AmazonDynamoDBException e) {\n       throw translateDynamoDBException(keyPrefix,\n           \"Prune of \" + keyPrefix + \" failed\", e);\n     }\n     LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n         S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n+    return itemCount;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int innerPrune(\n      final PruneMode pruneMode, final long cutoff, final String keyPrefix,\n      final ItemCollection\u003cScanOutcome\u003e items)\n      throws IOException {\n    int itemCount \u003d 0;\n    try (AncestorState state \u003d initiateBulkWrite(\n        BulkOperationState.OperationType.Prune, null);\n         DurationInfo ignored \u003d\n             new DurationInfo(LOG, \"Pruning DynamoDB Store\")) {\n      ArrayList\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      long delay \u003d conf.getTimeDuration(\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n          TimeUnit.MILLISECONDS);\n      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n      Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n      // declare the operation to delete a batch as a function so\n      // as to keep the code consistent across multiple uses.\n      FunctionsRaisingIOE.CallableRaisingIOE\u003cVoid\u003e deleteBatchOperation \u003d\n          () -\u003e {\n            // lowest path entries get deleted first.\n            deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n            processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n\n            // set authoritative false for each pruned dir listing\n            // if at least one entry was not a tombstone\n            removeAuthoritativeDirFlag(parentPathSet, state);\n            // already cleared parent paths.\n            clearedParentPathSet.addAll(parentPathSet);\n            parentPathSet.clear();\n            return null;\n          };\n      for (Item item : items) {\n        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        boolean tombstone \u003d md.isDeleted();\n        LOG.debug(\"Prune entry {}\", path);\n        deletionBatch.add(path);\n\n        // add parent path of item so it can be marked as non-auth.\n        // this is only done if\n        // * it has not already been processed\n        // * the entry pruned is not a tombstone (no need to update)\n        // * the file is not in the root dir\n        Path parentPath \u003d path.getParent();\n        if (!tombstone\n            \u0026\u0026 parentPath !\u003d null\n            \u0026\u0026 !parentPath.isRoot()\n            \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n          parentPathSet.add(parentPath);\n        }\n\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          deleteBatchOperation.apply();\n          deletionBatch.clear();\n          if (delay \u003e 0) {\n            Thread.sleep(delay);\n          }\n        }\n      }\n      // final batch of deletes\n      if (!deletionBatch.isEmpty()) {\n        deleteBatchOperation.apply();\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    } catch (AmazonDynamoDBException e) {\n      throw translateDynamoDBException(keyPrefix,\n          \"Prune of \" + keyPrefix + \" failed\", e);\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n    return itemCount;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "int"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
          "commitDate": "10/01/20 3:11 AM",
          "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "26/11/19 7:36 AM",
          "commitNameOld": "ea25f4de236611d388e14a710ebe5d6872c421b6",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 44.82,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,71 +1,78 @@\n-  private void innerPrune(String keyPrefix, ItemCollection\u003cScanOutcome\u003e items)\n+  private int innerPrune(\n+      final PruneMode pruneMode, final long cutoff, final String keyPrefix,\n+      final ItemCollection\u003cScanOutcome\u003e items)\n       throws IOException {\n     int itemCount \u003d 0;\n     try (AncestorState state \u003d initiateBulkWrite(\n         BulkOperationState.OperationType.Prune, null);\n          DurationInfo ignored \u003d\n              new DurationInfo(LOG, \"Pruning DynamoDB Store\")) {\n       ArrayList\u003cPath\u003e deletionBatch \u003d\n           new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n       long delay \u003d conf.getTimeDuration(\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n           TimeUnit.MILLISECONDS);\n       Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n       Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n+      // declare the operation to delete a batch as a function so\n+      // as to keep the code consistent across multiple uses.\n+      FunctionsRaisingIOE.CallableRaisingIOE\u003cVoid\u003e deleteBatchOperation \u003d\n+          () -\u003e {\n+            // lowest path entries get deleted first.\n+            deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n+            processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n+\n+            // set authoritative false for each pruned dir listing\n+            // if at least one entry was not a tombstone\n+            removeAuthoritativeDirFlag(parentPathSet, state);\n+            // already cleared parent paths.\n+            clearedParentPathSet.addAll(parentPathSet);\n+            parentPathSet.clear();\n+            return null;\n+          };\n       for (Item item : items) {\n         DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n             .itemToPathMetadata(item, username);\n         Path path \u003d md.getFileStatus().getPath();\n         boolean tombstone \u003d md.isDeleted();\n         LOG.debug(\"Prune entry {}\", path);\n         deletionBatch.add(path);\n \n         // add parent path of item so it can be marked as non-auth.\n         // this is only done if\n         // * it has not already been processed\n         // * the entry pruned is not a tombstone (no need to update)\n         // * the file is not in the root dir\n         Path parentPath \u003d path.getParent();\n         if (!tombstone\n             \u0026\u0026 parentPath !\u003d null\n+            \u0026\u0026 !parentPath.isRoot()\n             \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n           parentPathSet.add(parentPath);\n         }\n \n         itemCount++;\n         if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n-          // lowest path entries get deleted first.\n-          deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n-          processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n-\n-          // set authoritative false for each pruned dir listing\n-          removeAuthoritativeDirFlag(parentPathSet, state);\n-          // already cleared parent paths.\n-          clearedParentPathSet.addAll(parentPathSet);\n-          parentPathSet.clear();\n-\n+          deleteBatchOperation.apply();\n           deletionBatch.clear();\n           if (delay \u003e 0) {\n             Thread.sleep(delay);\n           }\n         }\n       }\n       // final batch of deletes\n       if (!deletionBatch.isEmpty()) {\n-        processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n-\n-        // set authoritative false for each pruned dir listing\n-        removeAuthoritativeDirFlag(parentPathSet, state);\n-        parentPathSet.clear();\n+        deleteBatchOperation.apply();\n       }\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\"Pruning was interrupted\");\n     } catch (AmazonDynamoDBException e) {\n       throw translateDynamoDBException(keyPrefix,\n           \"Prune of \" + keyPrefix + \" failed\", e);\n     }\n     LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n         S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n+    return itemCount;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int innerPrune(\n      final PruneMode pruneMode, final long cutoff, final String keyPrefix,\n      final ItemCollection\u003cScanOutcome\u003e items)\n      throws IOException {\n    int itemCount \u003d 0;\n    try (AncestorState state \u003d initiateBulkWrite(\n        BulkOperationState.OperationType.Prune, null);\n         DurationInfo ignored \u003d\n             new DurationInfo(LOG, \"Pruning DynamoDB Store\")) {\n      ArrayList\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      long delay \u003d conf.getTimeDuration(\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n          TimeUnit.MILLISECONDS);\n      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n      Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n      // declare the operation to delete a batch as a function so\n      // as to keep the code consistent across multiple uses.\n      FunctionsRaisingIOE.CallableRaisingIOE\u003cVoid\u003e deleteBatchOperation \u003d\n          () -\u003e {\n            // lowest path entries get deleted first.\n            deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n            processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n\n            // set authoritative false for each pruned dir listing\n            // if at least one entry was not a tombstone\n            removeAuthoritativeDirFlag(parentPathSet, state);\n            // already cleared parent paths.\n            clearedParentPathSet.addAll(parentPathSet);\n            parentPathSet.clear();\n            return null;\n          };\n      for (Item item : items) {\n        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        boolean tombstone \u003d md.isDeleted();\n        LOG.debug(\"Prune entry {}\", path);\n        deletionBatch.add(path);\n\n        // add parent path of item so it can be marked as non-auth.\n        // this is only done if\n        // * it has not already been processed\n        // * the entry pruned is not a tombstone (no need to update)\n        // * the file is not in the root dir\n        Path parentPath \u003d path.getParent();\n        if (!tombstone\n            \u0026\u0026 parentPath !\u003d null\n            \u0026\u0026 !parentPath.isRoot()\n            \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n          parentPathSet.add(parentPath);\n        }\n\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          deleteBatchOperation.apply();\n          deletionBatch.clear();\n          if (delay \u003e 0) {\n            Thread.sleep(delay);\n          }\n        }\n      }\n      // final batch of deletes\n      if (!deletionBatch.isEmpty()) {\n        deleteBatchOperation.apply();\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    } catch (AmazonDynamoDBException e) {\n      throw translateDynamoDBException(keyPrefix,\n          \"Prune of \" + keyPrefix + \" failed\", e);\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n    return itemCount;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
          "commitDate": "10/01/20 3:11 AM",
          "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "26/11/19 7:36 AM",
          "commitNameOld": "ea25f4de236611d388e14a710ebe5d6872c421b6",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 44.82,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,71 +1,78 @@\n-  private void innerPrune(String keyPrefix, ItemCollection\u003cScanOutcome\u003e items)\n+  private int innerPrune(\n+      final PruneMode pruneMode, final long cutoff, final String keyPrefix,\n+      final ItemCollection\u003cScanOutcome\u003e items)\n       throws IOException {\n     int itemCount \u003d 0;\n     try (AncestorState state \u003d initiateBulkWrite(\n         BulkOperationState.OperationType.Prune, null);\n          DurationInfo ignored \u003d\n              new DurationInfo(LOG, \"Pruning DynamoDB Store\")) {\n       ArrayList\u003cPath\u003e deletionBatch \u003d\n           new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n       long delay \u003d conf.getTimeDuration(\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n           TimeUnit.MILLISECONDS);\n       Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n       Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n+      // declare the operation to delete a batch as a function so\n+      // as to keep the code consistent across multiple uses.\n+      FunctionsRaisingIOE.CallableRaisingIOE\u003cVoid\u003e deleteBatchOperation \u003d\n+          () -\u003e {\n+            // lowest path entries get deleted first.\n+            deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n+            processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n+\n+            // set authoritative false for each pruned dir listing\n+            // if at least one entry was not a tombstone\n+            removeAuthoritativeDirFlag(parentPathSet, state);\n+            // already cleared parent paths.\n+            clearedParentPathSet.addAll(parentPathSet);\n+            parentPathSet.clear();\n+            return null;\n+          };\n       for (Item item : items) {\n         DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n             .itemToPathMetadata(item, username);\n         Path path \u003d md.getFileStatus().getPath();\n         boolean tombstone \u003d md.isDeleted();\n         LOG.debug(\"Prune entry {}\", path);\n         deletionBatch.add(path);\n \n         // add parent path of item so it can be marked as non-auth.\n         // this is only done if\n         // * it has not already been processed\n         // * the entry pruned is not a tombstone (no need to update)\n         // * the file is not in the root dir\n         Path parentPath \u003d path.getParent();\n         if (!tombstone\n             \u0026\u0026 parentPath !\u003d null\n+            \u0026\u0026 !parentPath.isRoot()\n             \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n           parentPathSet.add(parentPath);\n         }\n \n         itemCount++;\n         if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n-          // lowest path entries get deleted first.\n-          deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n-          processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n-\n-          // set authoritative false for each pruned dir listing\n-          removeAuthoritativeDirFlag(parentPathSet, state);\n-          // already cleared parent paths.\n-          clearedParentPathSet.addAll(parentPathSet);\n-          parentPathSet.clear();\n-\n+          deleteBatchOperation.apply();\n           deletionBatch.clear();\n           if (delay \u003e 0) {\n             Thread.sleep(delay);\n           }\n         }\n       }\n       // final batch of deletes\n       if (!deletionBatch.isEmpty()) {\n-        processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n-\n-        // set authoritative false for each pruned dir listing\n-        removeAuthoritativeDirFlag(parentPathSet, state);\n-        parentPathSet.clear();\n+        deleteBatchOperation.apply();\n       }\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\"Pruning was interrupted\");\n     } catch (AmazonDynamoDBException e) {\n       throw translateDynamoDBException(keyPrefix,\n           \"Prune of \" + keyPrefix + \" failed\", e);\n     }\n     LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n         S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n+    return itemCount;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int innerPrune(\n      final PruneMode pruneMode, final long cutoff, final String keyPrefix,\n      final ItemCollection\u003cScanOutcome\u003e items)\n      throws IOException {\n    int itemCount \u003d 0;\n    try (AncestorState state \u003d initiateBulkWrite(\n        BulkOperationState.OperationType.Prune, null);\n         DurationInfo ignored \u003d\n             new DurationInfo(LOG, \"Pruning DynamoDB Store\")) {\n      ArrayList\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      long delay \u003d conf.getTimeDuration(\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n          TimeUnit.MILLISECONDS);\n      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n      Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n      // declare the operation to delete a batch as a function so\n      // as to keep the code consistent across multiple uses.\n      FunctionsRaisingIOE.CallableRaisingIOE\u003cVoid\u003e deleteBatchOperation \u003d\n          () -\u003e {\n            // lowest path entries get deleted first.\n            deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n            processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n\n            // set authoritative false for each pruned dir listing\n            // if at least one entry was not a tombstone\n            removeAuthoritativeDirFlag(parentPathSet, state);\n            // already cleared parent paths.\n            clearedParentPathSet.addAll(parentPathSet);\n            parentPathSet.clear();\n            return null;\n          };\n      for (Item item : items) {\n        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        boolean tombstone \u003d md.isDeleted();\n        LOG.debug(\"Prune entry {}\", path);\n        deletionBatch.add(path);\n\n        // add parent path of item so it can be marked as non-auth.\n        // this is only done if\n        // * it has not already been processed\n        // * the entry pruned is not a tombstone (no need to update)\n        // * the file is not in the root dir\n        Path parentPath \u003d path.getParent();\n        if (!tombstone\n            \u0026\u0026 parentPath !\u003d null\n            \u0026\u0026 !parentPath.isRoot()\n            \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n          parentPathSet.add(parentPath);\n        }\n\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          deleteBatchOperation.apply();\n          deletionBatch.clear();\n          if (delay \u003e 0) {\n            Thread.sleep(delay);\n          }\n        }\n      }\n      // final batch of deletes\n      if (!deletionBatch.isEmpty()) {\n        deleteBatchOperation.apply();\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    } catch (AmazonDynamoDBException e) {\n      throw translateDynamoDBException(keyPrefix,\n          \"Prune of \" + keyPrefix + \" failed\", e);\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n    return itemCount;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {
            "oldValue": "[keyPrefix-String, items-ItemCollection\u003cScanOutcome\u003e]",
            "newValue": "[pruneMode-PruneMode(modifiers-final), cutoff-long(modifiers-final), keyPrefix-String(modifiers-final), items-ItemCollection\u003cScanOutcome\u003e(modifiers-final)]"
          }
        }
      ]
    },
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16384: S3A: Avoid inconsistencies between DDB and S3.\n\nContributed by Steve Loughran\n\nContains\n\n- HADOOP-16397. Hadoop S3Guard Prune command to support a -tombstone option.\n- HADOOP-16406. ITestDynamoDBMetadataStore.testProvisionTable times out intermittently\n\nThis patch doesn\u0027t fix the underlying problem but it\n\n* changes some tests to clean up better\n* does a lot more in logging operations in against DDB, if enabled\n* adds an entry point to dump the state of the metastore and s3 tables (precursor to fsck)\n* adds a purge entry point to help clean up after a test run has got a store into a mess\n* s3guard prune command adds -tombstone option to only clear tombstones\n\nThe outcome is that tests should pass consistently and if problems occur we have better diagnostics.\n\nChange-Id: I3eca3f5529d7f6fec398c0ff0472919f08f054eb\n",
      "commitDate": "12/07/19 5:02 AM",
      "commitName": "b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "08/07/19 10:27 AM",
      "commitNameOld": "de6b7bc67ace7744adb0320ee7de79cf28259d2d",
      "commitAuthorOld": "Sean Mackrory",
      "daysBetweenCommits": 3.77,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,71 @@\n   private void innerPrune(String keyPrefix, ItemCollection\u003cScanOutcome\u003e items)\n       throws IOException {\n     int itemCount \u003d 0;\n     try (AncestorState state \u003d initiateBulkWrite(\n-        BulkOperationState.OperationType.Prune, null)) {\n+        BulkOperationState.OperationType.Prune, null);\n+         DurationInfo ignored \u003d\n+             new DurationInfo(LOG, \"Pruning DynamoDB Store\")) {\n       ArrayList\u003cPath\u003e deletionBatch \u003d\n           new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n       long delay \u003d conf.getTimeDuration(\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n           TimeUnit.MILLISECONDS);\n       Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n       Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n       for (Item item : items) {\n         DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n             .itemToPathMetadata(item, username);\n         Path path \u003d md.getFileStatus().getPath();\n+        boolean tombstone \u003d md.isDeleted();\n+        LOG.debug(\"Prune entry {}\", path);\n         deletionBatch.add(path);\n \n-        // add parent path of what we remove if it has not\n-        // already been processed\n+        // add parent path of item so it can be marked as non-auth.\n+        // this is only done if\n+        // * it has not already been processed\n+        // * the entry pruned is not a tombstone (no need to update)\n+        // * the file is not in the root dir\n         Path parentPath \u003d path.getParent();\n-        if (parentPath !\u003d null \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n+        if (!tombstone\n+            \u0026\u0026 parentPath !\u003d null\n+            \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n           parentPathSet.add(parentPath);\n         }\n \n         itemCount++;\n         if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n           // lowest path entries get deleted first.\n           deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n-          processBatchWriteRequest(pathToKey(deletionBatch), null);\n+          processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n \n           // set authoritative false for each pruned dir listing\n           removeAuthoritativeDirFlag(parentPathSet, state);\n           // already cleared parent paths.\n           clearedParentPathSet.addAll(parentPathSet);\n           parentPathSet.clear();\n \n           deletionBatch.clear();\n           if (delay \u003e 0) {\n             Thread.sleep(delay);\n           }\n         }\n       }\n       // final batch of deletes\n       if (!deletionBatch.isEmpty()) {\n-        processBatchWriteRequest(pathToKey(deletionBatch), null);\n+        processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n \n         // set authoritative false for each pruned dir listing\n         removeAuthoritativeDirFlag(parentPathSet, state);\n         parentPathSet.clear();\n       }\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\"Pruning was interrupted\");\n     } catch (AmazonDynamoDBException e) {\n       throw translateDynamoDBException(keyPrefix,\n           \"Prune of \" + keyPrefix + \" failed\", e);\n     }\n     LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n         S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void innerPrune(String keyPrefix, ItemCollection\u003cScanOutcome\u003e items)\n      throws IOException {\n    int itemCount \u003d 0;\n    try (AncestorState state \u003d initiateBulkWrite(\n        BulkOperationState.OperationType.Prune, null);\n         DurationInfo ignored \u003d\n             new DurationInfo(LOG, \"Pruning DynamoDB Store\")) {\n      ArrayList\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      long delay \u003d conf.getTimeDuration(\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n          TimeUnit.MILLISECONDS);\n      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n      Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n      for (Item item : items) {\n        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        boolean tombstone \u003d md.isDeleted();\n        LOG.debug(\"Prune entry {}\", path);\n        deletionBatch.add(path);\n\n        // add parent path of item so it can be marked as non-auth.\n        // this is only done if\n        // * it has not already been processed\n        // * the entry pruned is not a tombstone (no need to update)\n        // * the file is not in the root dir\n        Path parentPath \u003d path.getParent();\n        if (!tombstone\n            \u0026\u0026 parentPath !\u003d null\n            \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n          parentPathSet.add(parentPath);\n        }\n\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          // lowest path entries get deleted first.\n          deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n          processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n\n          // set authoritative false for each pruned dir listing\n          removeAuthoritativeDirFlag(parentPathSet, state);\n          // already cleared parent paths.\n          clearedParentPathSet.addAll(parentPathSet);\n          parentPathSet.clear();\n\n          deletionBatch.clear();\n          if (delay \u003e 0) {\n            Thread.sleep(delay);\n          }\n        }\n      }\n      // final batch of deletes\n      if (!deletionBatch.isEmpty()) {\n        processBatchWriteRequest(state, pathToKey(deletionBatch), null);\n\n        // set authoritative false for each pruned dir listing\n        removeAuthoritativeDirFlag(parentPathSet, state);\n        parentPathSet.clear();\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    } catch (AmazonDynamoDBException e) {\n      throw translateDynamoDBException(keyPrefix,\n          \"Prune of \" + keyPrefix + \" failed\", e);\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
      "extendedDetails": {}
    },
    "e02eb24e0a9139418120027b694492e0738df20a": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
      "commitDate": "20/06/19 1:56 AM",
      "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
          "commitDate": "20/06/19 1:56 AM",
          "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "16/06/19 9:05 AM",
          "commitNameOld": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 3.7,
          "commitsBetweenForRepo": 44,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,62 @@\n-  private void innerPrune(ItemCollection\u003cScanOutcome\u003e items)\n+  private void innerPrune(String keyPrefix, ItemCollection\u003cScanOutcome\u003e items)\n       throws IOException {\n     int itemCount \u003d 0;\n-    try {\n-      Collection\u003cPath\u003e deletionBatch \u003d\n+    try (AncestorState state \u003d initiateBulkWrite(\n+        BulkOperationState.OperationType.Prune, null)) {\n+      ArrayList\u003cPath\u003e deletionBatch \u003d\n           new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n       long delay \u003d conf.getTimeDuration(\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n           TimeUnit.MILLISECONDS);\n       Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n+      Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n       for (Item item : items) {\n         DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n             .itemToPathMetadata(item, username);\n         Path path \u003d md.getFileStatus().getPath();\n         deletionBatch.add(path);\n \n-        // add parent path of what we remove\n+        // add parent path of what we remove if it has not\n+        // already been processed\n         Path parentPath \u003d path.getParent();\n-        if (parentPath !\u003d null) {\n+        if (parentPath !\u003d null \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n           parentPathSet.add(parentPath);\n         }\n \n         itemCount++;\n         if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n-          Thread.sleep(delay);\n+          // lowest path entries get deleted first.\n+          deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n           processBatchWriteRequest(pathToKey(deletionBatch), null);\n \n           // set authoritative false for each pruned dir listing\n-          removeAuthoritativeDirFlag(parentPathSet);\n+          removeAuthoritativeDirFlag(parentPathSet, state);\n+          // already cleared parent paths.\n+          clearedParentPathSet.addAll(parentPathSet);\n           parentPathSet.clear();\n \n           deletionBatch.clear();\n+          if (delay \u003e 0) {\n+            Thread.sleep(delay);\n+          }\n         }\n       }\n       // final batch of deletes\n       if (!deletionBatch.isEmpty()) {\n-        Thread.sleep(delay);\n         processBatchWriteRequest(pathToKey(deletionBatch), null);\n \n         // set authoritative false for each pruned dir listing\n-        removeAuthoritativeDirFlag(parentPathSet);\n+        removeAuthoritativeDirFlag(parentPathSet, state);\n         parentPathSet.clear();\n       }\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\"Pruning was interrupted\");\n+    } catch (AmazonDynamoDBException e) {\n+      throw translateDynamoDBException(keyPrefix,\n+          \"Prune of \" + keyPrefix + \" failed\", e);\n     }\n     LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n         S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void innerPrune(String keyPrefix, ItemCollection\u003cScanOutcome\u003e items)\n      throws IOException {\n    int itemCount \u003d 0;\n    try (AncestorState state \u003d initiateBulkWrite(\n        BulkOperationState.OperationType.Prune, null)) {\n      ArrayList\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      long delay \u003d conf.getTimeDuration(\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n          TimeUnit.MILLISECONDS);\n      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n      Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n      for (Item item : items) {\n        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        deletionBatch.add(path);\n\n        // add parent path of what we remove if it has not\n        // already been processed\n        Path parentPath \u003d path.getParent();\n        if (parentPath !\u003d null \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n          parentPathSet.add(parentPath);\n        }\n\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          // lowest path entries get deleted first.\n          deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n          processBatchWriteRequest(pathToKey(deletionBatch), null);\n\n          // set authoritative false for each pruned dir listing\n          removeAuthoritativeDirFlag(parentPathSet, state);\n          // already cleared parent paths.\n          clearedParentPathSet.addAll(parentPathSet);\n          parentPathSet.clear();\n\n          deletionBatch.clear();\n          if (delay \u003e 0) {\n            Thread.sleep(delay);\n          }\n        }\n      }\n      // final batch of deletes\n      if (!deletionBatch.isEmpty()) {\n        processBatchWriteRequest(pathToKey(deletionBatch), null);\n\n        // set authoritative false for each pruned dir listing\n        removeAuthoritativeDirFlag(parentPathSet, state);\n        parentPathSet.clear();\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    } catch (AmazonDynamoDBException e) {\n      throw translateDynamoDBException(keyPrefix,\n          \"Prune of \" + keyPrefix + \" failed\", e);\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {
            "oldValue": "[items-ItemCollection\u003cScanOutcome\u003e]",
            "newValue": "[keyPrefix-String, items-ItemCollection\u003cScanOutcome\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
          "commitDate": "20/06/19 1:56 AM",
          "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "16/06/19 9:05 AM",
          "commitNameOld": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 3.7,
          "commitsBetweenForRepo": 44,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,62 @@\n-  private void innerPrune(ItemCollection\u003cScanOutcome\u003e items)\n+  private void innerPrune(String keyPrefix, ItemCollection\u003cScanOutcome\u003e items)\n       throws IOException {\n     int itemCount \u003d 0;\n-    try {\n-      Collection\u003cPath\u003e deletionBatch \u003d\n+    try (AncestorState state \u003d initiateBulkWrite(\n+        BulkOperationState.OperationType.Prune, null)) {\n+      ArrayList\u003cPath\u003e deletionBatch \u003d\n           new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n       long delay \u003d conf.getTimeDuration(\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n           S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n           TimeUnit.MILLISECONDS);\n       Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n+      Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n       for (Item item : items) {\n         DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n             .itemToPathMetadata(item, username);\n         Path path \u003d md.getFileStatus().getPath();\n         deletionBatch.add(path);\n \n-        // add parent path of what we remove\n+        // add parent path of what we remove if it has not\n+        // already been processed\n         Path parentPath \u003d path.getParent();\n-        if (parentPath !\u003d null) {\n+        if (parentPath !\u003d null \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n           parentPathSet.add(parentPath);\n         }\n \n         itemCount++;\n         if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n-          Thread.sleep(delay);\n+          // lowest path entries get deleted first.\n+          deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n           processBatchWriteRequest(pathToKey(deletionBatch), null);\n \n           // set authoritative false for each pruned dir listing\n-          removeAuthoritativeDirFlag(parentPathSet);\n+          removeAuthoritativeDirFlag(parentPathSet, state);\n+          // already cleared parent paths.\n+          clearedParentPathSet.addAll(parentPathSet);\n           parentPathSet.clear();\n \n           deletionBatch.clear();\n+          if (delay \u003e 0) {\n+            Thread.sleep(delay);\n+          }\n         }\n       }\n       // final batch of deletes\n       if (!deletionBatch.isEmpty()) {\n-        Thread.sleep(delay);\n         processBatchWriteRequest(pathToKey(deletionBatch), null);\n \n         // set authoritative false for each pruned dir listing\n-        removeAuthoritativeDirFlag(parentPathSet);\n+        removeAuthoritativeDirFlag(parentPathSet, state);\n         parentPathSet.clear();\n       }\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\"Pruning was interrupted\");\n+    } catch (AmazonDynamoDBException e) {\n+      throw translateDynamoDBException(keyPrefix,\n+          \"Prune of \" + keyPrefix + \" failed\", e);\n     }\n     LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n         S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void innerPrune(String keyPrefix, ItemCollection\u003cScanOutcome\u003e items)\n      throws IOException {\n    int itemCount \u003d 0;\n    try (AncestorState state \u003d initiateBulkWrite(\n        BulkOperationState.OperationType.Prune, null)) {\n      ArrayList\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      long delay \u003d conf.getTimeDuration(\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n          TimeUnit.MILLISECONDS);\n      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n      Set\u003cPath\u003e clearedParentPathSet \u003d new HashSet\u003c\u003e();\n      for (Item item : items) {\n        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        deletionBatch.add(path);\n\n        // add parent path of what we remove if it has not\n        // already been processed\n        Path parentPath \u003d path.getParent();\n        if (parentPath !\u003d null \u0026\u0026 !clearedParentPathSet.contains(parentPath)) {\n          parentPathSet.add(parentPath);\n        }\n\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          // lowest path entries get deleted first.\n          deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);\n          processBatchWriteRequest(pathToKey(deletionBatch), null);\n\n          // set authoritative false for each pruned dir listing\n          removeAuthoritativeDirFlag(parentPathSet, state);\n          // already cleared parent paths.\n          clearedParentPathSet.addAll(parentPathSet);\n          parentPathSet.clear();\n\n          deletionBatch.clear();\n          if (delay \u003e 0) {\n            Thread.sleep(delay);\n          }\n        }\n      }\n      // final batch of deletes\n      if (!deletionBatch.isEmpty()) {\n        processBatchWriteRequest(pathToKey(deletionBatch), null);\n\n        // set authoritative false for each pruned dir listing\n        removeAuthoritativeDirFlag(parentPathSet, state);\n        parentPathSet.clear();\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    } catch (AmazonDynamoDBException e) {\n      throw translateDynamoDBException(keyPrefix,\n          \"Prune of \" + keyPrefix + \" failed\", e);\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-16279. S3Guard: Implement time-based (TTL) expiry for entries (and tombstones).\n\nContributed by Gabor Bota.\n\nChange-Id: I73a2d2861901dedfe7a0e783b310fbb95e7c1af9\n",
      "commitDate": "16/06/19 9:05 AM",
      "commitName": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
      "commitAuthor": "Gabor Bota",
      "diff": "@@ -0,0 +1,51 @@\n+  private void innerPrune(ItemCollection\u003cScanOutcome\u003e items)\n+      throws IOException {\n+    int itemCount \u003d 0;\n+    try {\n+      Collection\u003cPath\u003e deletionBatch \u003d\n+          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n+      long delay \u003d conf.getTimeDuration(\n+          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n+          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n+          TimeUnit.MILLISECONDS);\n+      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n+      for (Item item : items) {\n+        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n+            .itemToPathMetadata(item, username);\n+        Path path \u003d md.getFileStatus().getPath();\n+        deletionBatch.add(path);\n+\n+        // add parent path of what we remove\n+        Path parentPath \u003d path.getParent();\n+        if (parentPath !\u003d null) {\n+          parentPathSet.add(parentPath);\n+        }\n+\n+        itemCount++;\n+        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n+          Thread.sleep(delay);\n+          processBatchWriteRequest(pathToKey(deletionBatch), null);\n+\n+          // set authoritative false for each pruned dir listing\n+          removeAuthoritativeDirFlag(parentPathSet);\n+          parentPathSet.clear();\n+\n+          deletionBatch.clear();\n+        }\n+      }\n+      // final batch of deletes\n+      if (!deletionBatch.isEmpty()) {\n+        Thread.sleep(delay);\n+        processBatchWriteRequest(pathToKey(deletionBatch), null);\n+\n+        // set authoritative false for each pruned dir listing\n+        removeAuthoritativeDirFlag(parentPathSet);\n+        parentPathSet.clear();\n+      }\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      throw new InterruptedIOException(\"Pruning was interrupted\");\n+    }\n+    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n+        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void innerPrune(ItemCollection\u003cScanOutcome\u003e items)\n      throws IOException {\n    int itemCount \u003d 0;\n    try {\n      Collection\u003cPath\u003e deletionBatch \u003d\n          new ArrayList\u003c\u003e(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n      long delay \u003d conf.getTimeDuration(\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,\n          S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,\n          TimeUnit.MILLISECONDS);\n      Set\u003cPath\u003e parentPathSet \u003d new HashSet\u003c\u003e();\n      for (Item item : items) {\n        DDBPathMetadata md \u003d PathMetadataDynamoDBTranslation\n            .itemToPathMetadata(item, username);\n        Path path \u003d md.getFileStatus().getPath();\n        deletionBatch.add(path);\n\n        // add parent path of what we remove\n        Path parentPath \u003d path.getParent();\n        if (parentPath !\u003d null) {\n          parentPathSet.add(parentPath);\n        }\n\n        itemCount++;\n        if (deletionBatch.size() \u003d\u003d S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {\n          Thread.sleep(delay);\n          processBatchWriteRequest(pathToKey(deletionBatch), null);\n\n          // set authoritative false for each pruned dir listing\n          removeAuthoritativeDirFlag(parentPathSet);\n          parentPathSet.clear();\n\n          deletionBatch.clear();\n        }\n      }\n      // final batch of deletes\n      if (!deletionBatch.isEmpty()) {\n        Thread.sleep(delay);\n        processBatchWriteRequest(pathToKey(deletionBatch), null);\n\n        // set authoritative false for each pruned dir listing\n        removeAuthoritativeDirFlag(parentPathSet);\n        parentPathSet.clear();\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\"Pruning was interrupted\");\n    }\n    LOG.info(\"Finished pruning {} items in batches of {}\", itemCount,\n        S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java"
    }
  }
}