{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "TaskImpl.java",
  "functionName": "addAndScheduleAttempt",
  "functionId": "addAndScheduleAttempt___avataar-Avataar__reschedule-boolean",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
  "functionStartLine": 606,
  "functionEndLine": 617,
  "numCommitsSeen": 110,
  "timeTaken": 9365,
  "changeHistory": [
    "8b2880c0b62102fc5c8b6962752f72cb2c416a01",
    "6a1c41111edcdc58c846fc50e53554fbba230171",
    "c163dc2fce10c1f4f4def6f079069b1bae901fcd",
    "dd72ca35365b59fcfc5724d2c182ff77e013fc47",
    "0515b3322f9d94f1743504085967b29efe1dd7fe",
    "28a2eb9d722bb8cbbeee87a1c43b4dc4ef4467ce",
    "d6546fc0a444228c9d45b5bef89aeef120f98831",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517"
  ],
  "changeHistoryShort": {
    "8b2880c0b62102fc5c8b6962752f72cb2c416a01": "Ymultichange(Yparameterchange,Ybodychange)",
    "6a1c41111edcdc58c846fc50e53554fbba230171": "Ybodychange",
    "c163dc2fce10c1f4f4def6f079069b1bae901fcd": "Ymultichange(Yparameterchange,Ybodychange)",
    "dd72ca35365b59fcfc5724d2c182ff77e013fc47": "Ybodychange",
    "0515b3322f9d94f1743504085967b29efe1dd7fe": "Ybodychange",
    "28a2eb9d722bb8cbbeee87a1c43b4dc4ef4467ce": "Ybodychange",
    "d6546fc0a444228c9d45b5bef89aeef120f98831": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8b2880c0b62102fc5c8b6962752f72cb2c416a01": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-6513. MR job got hanged forever when one NM unstable for some time. (Varun Saxena via wangda)\n",
      "commitDate": "14/04/16 11:00 AM",
      "commitName": "8b2880c0b62102fc5c8b6962752f72cb2c416a01",
      "commitAuthor": "Wangda Tan",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-6513. MR job got hanged forever when one NM unstable for some time. (Varun Saxena via wangda)\n",
          "commitDate": "14/04/16 11:00 AM",
          "commitName": "8b2880c0b62102fc5c8b6962752f72cb2c416a01",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "02/10/15 8:04 AM",
          "commitNameOld": "439f43ad3defbac907eda2d139a793f153544430",
          "commitAuthorOld": "rohithsharmaks",
          "daysBetweenCommits": 195.12,
          "commitsBetweenForRepo": 1304,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,12 @@\n-  private void addAndScheduleAttempt(Avataar avataar) {\n+  private void addAndScheduleAttempt(Avataar avataar, boolean reschedule) {\n     TaskAttempt attempt \u003d addAttempt(avataar);\n     inProgressAttempts.add(attempt.getID());\n     //schedule the nextAttemptNumber\n-    if (failedAttempts.size() \u003e 0) {\n+    if (failedAttempts.size() \u003e 0 || reschedule) {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n           TaskAttemptEventType.TA_RESCHEDULE));\n     } else {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n           TaskAttemptEventType.TA_SCHEDULE));\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void addAndScheduleAttempt(Avataar avataar, boolean reschedule) {\n    TaskAttempt attempt \u003d addAttempt(avataar);\n    inProgressAttempts.add(attempt.getID());\n    //schedule the nextAttemptNumber\n    if (failedAttempts.size() \u003e 0 || reschedule) {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_RESCHEDULE));\n    } else {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_SCHEDULE));\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
          "extendedDetails": {
            "oldValue": "[avataar-Avataar]",
            "newValue": "[avataar-Avataar, reschedule-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-6513. MR job got hanged forever when one NM unstable for some time. (Varun Saxena via wangda)\n",
          "commitDate": "14/04/16 11:00 AM",
          "commitName": "8b2880c0b62102fc5c8b6962752f72cb2c416a01",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "02/10/15 8:04 AM",
          "commitNameOld": "439f43ad3defbac907eda2d139a793f153544430",
          "commitAuthorOld": "rohithsharmaks",
          "daysBetweenCommits": 195.12,
          "commitsBetweenForRepo": 1304,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,12 @@\n-  private void addAndScheduleAttempt(Avataar avataar) {\n+  private void addAndScheduleAttempt(Avataar avataar, boolean reschedule) {\n     TaskAttempt attempt \u003d addAttempt(avataar);\n     inProgressAttempts.add(attempt.getID());\n     //schedule the nextAttemptNumber\n-    if (failedAttempts.size() \u003e 0) {\n+    if (failedAttempts.size() \u003e 0 || reschedule) {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n           TaskAttemptEventType.TA_RESCHEDULE));\n     } else {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n           TaskAttemptEventType.TA_SCHEDULE));\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void addAndScheduleAttempt(Avataar avataar, boolean reschedule) {\n    TaskAttempt attempt \u003d addAttempt(avataar);\n    inProgressAttempts.add(attempt.getID());\n    //schedule the nextAttemptNumber\n    if (failedAttempts.size() \u003e 0 || reschedule) {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_RESCHEDULE));\n    } else {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_SCHEDULE));\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "6a1c41111edcdc58c846fc50e53554fbba230171": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5079. Changes job recovery to restore state directly from job history, instaed of simulating state machine events. Contributed by Jason Lowe and Robert Parker.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1466767 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/04/13 9:52 PM",
      "commitName": "6a1c41111edcdc58c846fc50e53554fbba230171",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "27/02/13 1:03 PM",
      "commitNameOld": "979fb054f8e7141116718645d19ec7ba00455a63",
      "commitAuthorOld": "Siddharth Seth",
      "daysBetweenCommits": 42.33,
      "commitsBetweenForRepo": 235,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,12 @@\n   private void addAndScheduleAttempt(Avataar avataar) {\n-    TaskAttempt attempt \u003d createAttempt();\n-    ((TaskAttemptImpl) attempt).setAvataar(avataar);\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Created attempt \" + attempt.getID());\n-    }\n-    switch (attempts.size()) {\n-      case 0:\n-        attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n-        break;\n-        \n-      case 1:\n-        Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n-            \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n-        newAttempts.putAll(attempts);\n-        attempts \u003d newAttempts;\n-        attempts.put(attempt.getID(), attempt);\n-        break;\n-\n-      default:\n-        attempts.put(attempt.getID(), attempt);\n-        break;\n-    }\n-\n-    // Update nextATtemptNumber\n-    if (taskAttemptsFromPreviousGeneration.isEmpty()) {\n-      ++nextAttemptNumber;\n-    } else {\n-      // There are still some TaskAttempts from previous generation, use them\n-      nextAttemptNumber \u003d\n-          taskAttemptsFromPreviousGeneration.remove(0).getAttemptId().getId();\n-    }\n-\n+    TaskAttempt attempt \u003d addAttempt(avataar);\n     inProgressAttempts.add(attempt.getID());\n     //schedule the nextAttemptNumber\n     if (failedAttempts.size() \u003e 0) {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n-        TaskAttemptEventType.TA_RESCHEDULE));\n+          TaskAttemptEventType.TA_RESCHEDULE));\n     } else {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n           TaskAttemptEventType.TA_SCHEDULE));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addAndScheduleAttempt(Avataar avataar) {\n    TaskAttempt attempt \u003d addAttempt(avataar);\n    inProgressAttempts.add(attempt.getID());\n    //schedule the nextAttemptNumber\n    if (failedAttempts.size() \u003e 0) {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_RESCHEDULE));\n    } else {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_SCHEDULE));\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
      "extendedDetails": {}
    },
    "c163dc2fce10c1f4f4def6f079069b1bae901fcd": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-4838. Add additional fields like Locality, Avataar to the JobHistory logs. Contributed by Zhijie Shen\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1439714 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/01/13 4:21 PM",
      "commitName": "c163dc2fce10c1f4f4def6f079069b1bae901fcd",
      "commitAuthor": "Siddharth Seth",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-4838. Add additional fields like Locality, Avataar to the JobHistory logs. Contributed by Zhijie Shen\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1439714 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/01/13 4:21 PM",
          "commitName": "c163dc2fce10c1f4f4def6f079069b1bae901fcd",
          "commitAuthor": "Siddharth Seth",
          "commitDateOld": "28/12/12 7:01 AM",
          "commitNameOld": "402eb1851341fce72c8e46266a2578bb67b5b684",
          "commitAuthorOld": "Robert Joseph Evans",
          "daysBetweenCommits": 31.39,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,43 @@\n-  private void addAndScheduleAttempt() {\n+  private void addAndScheduleAttempt(Avataar avataar) {\n     TaskAttempt attempt \u003d createAttempt();\n+    ((TaskAttemptImpl) attempt).setAvataar(avataar);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Created attempt \" + attempt.getID());\n     }\n     switch (attempts.size()) {\n       case 0:\n         attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n         break;\n         \n       case 1:\n         Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n             \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n         newAttempts.putAll(attempts);\n         attempts \u003d newAttempts;\n         attempts.put(attempt.getID(), attempt);\n         break;\n \n       default:\n         attempts.put(attempt.getID(), attempt);\n         break;\n     }\n \n     // Update nextATtemptNumber\n     if (taskAttemptsFromPreviousGeneration.isEmpty()) {\n       ++nextAttemptNumber;\n     } else {\n       // There are still some TaskAttempts from previous generation, use them\n       nextAttemptNumber \u003d\n           taskAttemptsFromPreviousGeneration.remove(0).getAttemptId().getId();\n     }\n \n     inProgressAttempts.add(attempt.getID());\n     //schedule the nextAttemptNumber\n     if (failedAttempts.size() \u003e 0) {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n         TaskAttemptEventType.TA_RESCHEDULE));\n     } else {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n           TaskAttemptEventType.TA_SCHEDULE));\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void addAndScheduleAttempt(Avataar avataar) {\n    TaskAttempt attempt \u003d createAttempt();\n    ((TaskAttemptImpl) attempt).setAvataar(avataar);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Created attempt \" + attempt.getID());\n    }\n    switch (attempts.size()) {\n      case 0:\n        attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n        break;\n        \n      case 1:\n        Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n            \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n        newAttempts.putAll(attempts);\n        attempts \u003d newAttempts;\n        attempts.put(attempt.getID(), attempt);\n        break;\n\n      default:\n        attempts.put(attempt.getID(), attempt);\n        break;\n    }\n\n    // Update nextATtemptNumber\n    if (taskAttemptsFromPreviousGeneration.isEmpty()) {\n      ++nextAttemptNumber;\n    } else {\n      // There are still some TaskAttempts from previous generation, use them\n      nextAttemptNumber \u003d\n          taskAttemptsFromPreviousGeneration.remove(0).getAttemptId().getId();\n    }\n\n    inProgressAttempts.add(attempt.getID());\n    //schedule the nextAttemptNumber\n    if (failedAttempts.size() \u003e 0) {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n        TaskAttemptEventType.TA_RESCHEDULE));\n    } else {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_SCHEDULE));\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[avataar-Avataar]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-4838. Add additional fields like Locality, Avataar to the JobHistory logs. Contributed by Zhijie Shen\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1439714 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/01/13 4:21 PM",
          "commitName": "c163dc2fce10c1f4f4def6f079069b1bae901fcd",
          "commitAuthor": "Siddharth Seth",
          "commitDateOld": "28/12/12 7:01 AM",
          "commitNameOld": "402eb1851341fce72c8e46266a2578bb67b5b684",
          "commitAuthorOld": "Robert Joseph Evans",
          "daysBetweenCommits": 31.39,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,43 @@\n-  private void addAndScheduleAttempt() {\n+  private void addAndScheduleAttempt(Avataar avataar) {\n     TaskAttempt attempt \u003d createAttempt();\n+    ((TaskAttemptImpl) attempt).setAvataar(avataar);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Created attempt \" + attempt.getID());\n     }\n     switch (attempts.size()) {\n       case 0:\n         attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n         break;\n         \n       case 1:\n         Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n             \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n         newAttempts.putAll(attempts);\n         attempts \u003d newAttempts;\n         attempts.put(attempt.getID(), attempt);\n         break;\n \n       default:\n         attempts.put(attempt.getID(), attempt);\n         break;\n     }\n \n     // Update nextATtemptNumber\n     if (taskAttemptsFromPreviousGeneration.isEmpty()) {\n       ++nextAttemptNumber;\n     } else {\n       // There are still some TaskAttempts from previous generation, use them\n       nextAttemptNumber \u003d\n           taskAttemptsFromPreviousGeneration.remove(0).getAttemptId().getId();\n     }\n \n     inProgressAttempts.add(attempt.getID());\n     //schedule the nextAttemptNumber\n     if (failedAttempts.size() \u003e 0) {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n         TaskAttemptEventType.TA_RESCHEDULE));\n     } else {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n           TaskAttemptEventType.TA_SCHEDULE));\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void addAndScheduleAttempt(Avataar avataar) {\n    TaskAttempt attempt \u003d createAttempt();\n    ((TaskAttemptImpl) attempt).setAvataar(avataar);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Created attempt \" + attempt.getID());\n    }\n    switch (attempts.size()) {\n      case 0:\n        attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n        break;\n        \n      case 1:\n        Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n            \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n        newAttempts.putAll(attempts);\n        attempts \u003d newAttempts;\n        attempts.put(attempt.getID(), attempt);\n        break;\n\n      default:\n        attempts.put(attempt.getID(), attempt);\n        break;\n    }\n\n    // Update nextATtemptNumber\n    if (taskAttemptsFromPreviousGeneration.isEmpty()) {\n      ++nextAttemptNumber;\n    } else {\n      // There are still some TaskAttempts from previous generation, use them\n      nextAttemptNumber \u003d\n          taskAttemptsFromPreviousGeneration.remove(0).getAttemptId().getId();\n    }\n\n    inProgressAttempts.add(attempt.getID());\n    //schedule the nextAttemptNumber\n    if (failedAttempts.size() \u003e 0) {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n        TaskAttemptEventType.TA_RESCHEDULE));\n    } else {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_SCHEDULE));\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "dd72ca35365b59fcfc5724d2c182ff77e013fc47": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4751. AM stuck in KILL_WAIT for days (vinodkv via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1408314 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/11/12 7:15 AM",
      "commitName": "dd72ca35365b59fcfc5724d2c182ff77e013fc47",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "31/10/12 7:57 AM",
      "commitNameOld": "aac5c149c7ca500b8eb810b7d4b561ff1e38ea02",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 12.01,
      "commitsBetweenForRepo": 62,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n   private void addAndScheduleAttempt() {\n     TaskAttempt attempt \u003d createAttempt();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Created attempt \" + attempt.getID());\n     }\n     switch (attempts.size()) {\n       case 0:\n         attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n         break;\n         \n       case 1:\n         Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n             \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n         newAttempts.putAll(attempts);\n         attempts \u003d newAttempts;\n         attempts.put(attempt.getID(), attempt);\n         break;\n \n       default:\n         attempts.put(attempt.getID(), attempt);\n         break;\n     }\n \n     // Update nextATtemptNumber\n     if (taskAttemptsFromPreviousGeneration.isEmpty()) {\n       ++nextAttemptNumber;\n     } else {\n       // There are still some TaskAttempts from previous generation, use them\n       nextAttemptNumber \u003d\n           taskAttemptsFromPreviousGeneration.remove(0).getAttemptId().getId();\n     }\n \n-    ++numberUncompletedAttempts;\n+    inProgressAttempts.add(attempt.getID());\n     //schedule the nextAttemptNumber\n-    if (failedAttempts \u003e 0) {\n+    if (failedAttempts.size() \u003e 0) {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n         TaskAttemptEventType.TA_RESCHEDULE));\n     } else {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n           TaskAttemptEventType.TA_SCHEDULE));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addAndScheduleAttempt() {\n    TaskAttempt attempt \u003d createAttempt();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Created attempt \" + attempt.getID());\n    }\n    switch (attempts.size()) {\n      case 0:\n        attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n        break;\n        \n      case 1:\n        Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n            \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n        newAttempts.putAll(attempts);\n        attempts \u003d newAttempts;\n        attempts.put(attempt.getID(), attempt);\n        break;\n\n      default:\n        attempts.put(attempt.getID(), attempt);\n        break;\n    }\n\n    // Update nextATtemptNumber\n    if (taskAttemptsFromPreviousGeneration.isEmpty()) {\n      ++nextAttemptNumber;\n    } else {\n      // There are still some TaskAttempts from previous generation, use them\n      nextAttemptNumber \u003d\n          taskAttemptsFromPreviousGeneration.remove(0).getAttemptId().getId();\n    }\n\n    inProgressAttempts.add(attempt.getID());\n    //schedule the nextAttemptNumber\n    if (failedAttempts.size() \u003e 0) {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n        TaskAttemptEventType.TA_RESCHEDULE));\n    } else {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_SCHEDULE));\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
      "extendedDetails": {}
    },
    "0515b3322f9d94f1743504085967b29efe1dd7fe": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3846. Addressed MR AM hanging issues during AM restart and then the recovery. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1243752 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/02/12 4:06 PM",
      "commitName": "0515b3322f9d94f1743504085967b29efe1dd7fe",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "06/02/12 2:01 PM",
      "commitNameOld": "28a2eb9d722bb8cbbeee87a1c43b4dc4ef4467ce",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 7.09,
      "commitsBetweenForRepo": 63,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,42 @@\n   private void addAndScheduleAttempt() {\n     TaskAttempt attempt \u003d createAttempt();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Created attempt \" + attempt.getID());\n     }\n     switch (attempts.size()) {\n       case 0:\n         attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n         break;\n         \n       case 1:\n         Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n             \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n         newAttempts.putAll(attempts);\n         attempts \u003d newAttempts;\n         attempts.put(attempt.getID(), attempt);\n         break;\n \n       default:\n         attempts.put(attempt.getID(), attempt);\n         break;\n     }\n-    ++nextAttemptNumber;\n+\n+    // Update nextATtemptNumber\n+    if (taskAttemptsFromPreviousGeneration.isEmpty()) {\n+      ++nextAttemptNumber;\n+    } else {\n+      // There are still some TaskAttempts from previous generation, use them\n+      nextAttemptNumber \u003d\n+          taskAttemptsFromPreviousGeneration.remove(0).getAttemptId().getId();\n+    }\n+\n     ++numberUncompletedAttempts;\n     //schedule the nextAttemptNumber\n     if (failedAttempts \u003e 0) {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n         TaskAttemptEventType.TA_RESCHEDULE));\n     } else {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n           TaskAttemptEventType.TA_SCHEDULE));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addAndScheduleAttempt() {\n    TaskAttempt attempt \u003d createAttempt();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Created attempt \" + attempt.getID());\n    }\n    switch (attempts.size()) {\n      case 0:\n        attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n        break;\n        \n      case 1:\n        Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n            \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n        newAttempts.putAll(attempts);\n        attempts \u003d newAttempts;\n        attempts.put(attempt.getID(), attempt);\n        break;\n\n      default:\n        attempts.put(attempt.getID(), attempt);\n        break;\n    }\n\n    // Update nextATtemptNumber\n    if (taskAttemptsFromPreviousGeneration.isEmpty()) {\n      ++nextAttemptNumber;\n    } else {\n      // There are still some TaskAttempts from previous generation, use them\n      nextAttemptNumber \u003d\n          taskAttemptsFromPreviousGeneration.remove(0).getAttemptId().getId();\n    }\n\n    ++numberUncompletedAttempts;\n    //schedule the nextAttemptNumber\n    if (failedAttempts \u003e 0) {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n        TaskAttemptEventType.TA_RESCHEDULE));\n    } else {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_SCHEDULE));\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
      "extendedDetails": {}
    },
    "28a2eb9d722bb8cbbeee87a1c43b4dc4ef4467ce": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3810. Performance tweaks - reduced logging in AM and defined hascode/equals for ResourceRequest \u0026 Priority. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/02/12 2:01 PM",
      "commitName": "28a2eb9d722bb8cbbeee87a1c43b4dc4ef4467ce",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "03/02/12 4:04 PM",
      "commitNameOld": "94242c93857a06fb9c56ee571a47d6ca18f00f48",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 2.91,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,33 @@\n   private void addAndScheduleAttempt() {\n     TaskAttempt attempt \u003d createAttempt();\n-    LOG.info(\"Created attempt \" + attempt.getID());\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Created attempt \" + attempt.getID());\n+    }\n     switch (attempts.size()) {\n       case 0:\n         attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n         break;\n         \n       case 1:\n         Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n             \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n         newAttempts.putAll(attempts);\n         attempts \u003d newAttempts;\n         attempts.put(attempt.getID(), attempt);\n         break;\n \n       default:\n         attempts.put(attempt.getID(), attempt);\n         break;\n     }\n     ++nextAttemptNumber;\n     ++numberUncompletedAttempts;\n     //schedule the nextAttemptNumber\n     if (failedAttempts \u003e 0) {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n         TaskAttemptEventType.TA_RESCHEDULE));\n     } else {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n           TaskAttemptEventType.TA_SCHEDULE));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addAndScheduleAttempt() {\n    TaskAttempt attempt \u003d createAttempt();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Created attempt \" + attempt.getID());\n    }\n    switch (attempts.size()) {\n      case 0:\n        attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n        break;\n        \n      case 1:\n        Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n            \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n        newAttempts.putAll(attempts);\n        attempts \u003d newAttempts;\n        attempts.put(attempt.getID(), attempt);\n        break;\n\n      default:\n        attempts.put(attempt.getID(), attempt);\n        break;\n    }\n    ++nextAttemptNumber;\n    ++numberUncompletedAttempts;\n    //schedule the nextAttemptNumber\n    if (failedAttempts \u003e 0) {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n        TaskAttemptEventType.TA_RESCHEDULE));\n    } else {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_SCHEDULE));\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
      "extendedDetails": {}
    },
    "d6546fc0a444228c9d45b5bef89aeef120f98831": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3125. Modified TaskImpl to consider only non-failed, non-killed task-attempts for obtaining task\u0027s progress. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1182230 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/10/11 11:51 PM",
      "commitName": "d6546fc0a444228c9d45b5bef89aeef120f98831",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "13/09/11 11:12 AM",
      "commitNameOld": "53f921418d25cb232c7a0e1fa24c17bda729ac35",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 28.53,
      "commitsBetweenForRepo": 207,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   private void addAndScheduleAttempt() {\n     TaskAttempt attempt \u003d createAttempt();\n     LOG.info(\"Created attempt \" + attempt.getID());\n     switch (attempts.size()) {\n       case 0:\n         attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n         break;\n         \n       case 1:\n-        Map newAttempts\n+        Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n             \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n         newAttempts.putAll(attempts);\n         attempts \u003d newAttempts;\n         attempts.put(attempt.getID(), attempt);\n         break;\n \n       default:\n         attempts.put(attempt.getID(), attempt);\n         break;\n     }\n     ++nextAttemptNumber;\n     ++numberUncompletedAttempts;\n     //schedule the nextAttemptNumber\n     if (failedAttempts \u003e 0) {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n         TaskAttemptEventType.TA_RESCHEDULE));\n     } else {\n       eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n           TaskAttemptEventType.TA_SCHEDULE));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addAndScheduleAttempt() {\n    TaskAttempt attempt \u003d createAttempt();\n    LOG.info(\"Created attempt \" + attempt.getID());\n    switch (attempts.size()) {\n      case 0:\n        attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n        break;\n        \n      case 1:\n        Map\u003cTaskAttemptId, TaskAttempt\u003e newAttempts\n            \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n        newAttempts.putAll(attempts);\n        attempts \u003d newAttempts;\n        attempts.put(attempt.getID(), attempt);\n        break;\n\n      default:\n        attempts.put(attempt.getID(), attempt);\n        break;\n    }\n    ++nextAttemptNumber;\n    ++numberUncompletedAttempts;\n    //schedule the nextAttemptNumber\n    if (failedAttempts \u003e 0) {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n        TaskAttemptEventType.TA_RESCHEDULE));\n    } else {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_SCHEDULE));\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void addAndScheduleAttempt() {\n    TaskAttempt attempt \u003d createAttempt();\n    LOG.info(\"Created attempt \" + attempt.getID());\n    switch (attempts.size()) {\n      case 0:\n        attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n        break;\n        \n      case 1:\n        Map newAttempts\n            \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n        newAttempts.putAll(attempts);\n        attempts \u003d newAttempts;\n        attempts.put(attempt.getID(), attempt);\n        break;\n\n      default:\n        attempts.put(attempt.getID(), attempt);\n        break;\n    }\n    ++nextAttemptNumber;\n    ++numberUncompletedAttempts;\n    //schedule the nextAttemptNumber\n    if (failedAttempts \u003e 0) {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n        TaskAttemptEventType.TA_RESCHEDULE));\n    } else {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_SCHEDULE));\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,31 @@\n+  private void addAndScheduleAttempt() {\n+    TaskAttempt attempt \u003d createAttempt();\n+    LOG.info(\"Created attempt \" + attempt.getID());\n+    switch (attempts.size()) {\n+      case 0:\n+        attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n+        break;\n+        \n+      case 1:\n+        Map newAttempts\n+            \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n+        newAttempts.putAll(attempts);\n+        attempts \u003d newAttempts;\n+        attempts.put(attempt.getID(), attempt);\n+        break;\n+\n+      default:\n+        attempts.put(attempt.getID(), attempt);\n+        break;\n+    }\n+    ++nextAttemptNumber;\n+    ++numberUncompletedAttempts;\n+    //schedule the nextAttemptNumber\n+    if (failedAttempts \u003e 0) {\n+      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n+        TaskAttemptEventType.TA_RESCHEDULE));\n+    } else {\n+      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n+          TaskAttemptEventType.TA_SCHEDULE));\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void addAndScheduleAttempt() {\n    TaskAttempt attempt \u003d createAttempt();\n    LOG.info(\"Created attempt \" + attempt.getID());\n    switch (attempts.size()) {\n      case 0:\n        attempts \u003d Collections.singletonMap(attempt.getID(), attempt);\n        break;\n        \n      case 1:\n        Map newAttempts\n            \u003d new LinkedHashMap\u003cTaskAttemptId, TaskAttempt\u003e(maxAttempts);\n        newAttempts.putAll(attempts);\n        attempts \u003d newAttempts;\n        attempts.put(attempt.getID(), attempt);\n        break;\n\n      default:\n        attempts.put(attempt.getID(), attempt);\n        break;\n    }\n    ++nextAttemptNumber;\n    ++numberUncompletedAttempts;\n    //schedule the nextAttemptNumber\n    if (failedAttempts \u003e 0) {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n        TaskAttemptEventType.TA_RESCHEDULE));\n    } else {\n      eventHandler.handle(new TaskAttemptEvent(attempt.getID(),\n          TaskAttemptEventType.TA_SCHEDULE));\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java"
    }
  }
}