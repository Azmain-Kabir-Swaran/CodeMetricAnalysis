{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "call",
  "functionId": "call",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 1164,
  "functionEndLine": 1168,
  "numCommitsSeen": 136,
  "timeTaken": 6067,
  "changeHistory": [
    "5d748bd056a32f2c6922514cd0c5b31d866a9919",
    "b6bfb2fcb2391d51b8de97c01c1290880779132e",
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11",
    "401db4fc65140979fe7665983e36905e886df971",
    "8808779db351fe444388d4acb3094766b5980718",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "bff5999d07e9416a22846c849487e509ede55040",
    "a42bb1cd915abe5dc33eda3c01e8c74c64f35748",
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f",
    "0ca41a8f35e4f05bb04805a2e0a617850707b4db",
    "f8904ad299bbcd109e3460f9b8ab9fbb9cebdad4",
    "17db74a1c1972392a5aba48a3e0334dcd6c76487"
  ],
  "changeHistoryShort": {
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": "Ybodychange",
    "b6bfb2fcb2391d51b8de97c01c1290880779132e": "Ybodychange",
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11": "Ybodychange",
    "401db4fc65140979fe7665983e36905e886df971": "Ybodychange",
    "8808779db351fe444388d4acb3094766b5980718": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "bff5999d07e9416a22846c849487e509ede55040": "Ybodychange",
    "a42bb1cd915abe5dc33eda3c01e8c74c64f35748": "Ybodychange",
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f": "Ybodychange",
    "0ca41a8f35e4f05bb04805a2e0a617850707b4db": "Ybodychange",
    "f8904ad299bbcd109e3460f9b8ab9fbb9cebdad4": "Ybodychange",
    "17db74a1c1972392a5aba48a3e0334dcd6c76487": "Yintroduced"
  },
  "changeHistoryDetails": {
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13702. Remove HTrace hooks from DFSClient to reduce CPU usage. Contributed by Todd Lipcon.\n",
      "commitDate": "02/07/18 3:11 AM",
      "commitName": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "02/07/18 3:02 AM",
      "commitNameOld": "6ba99741086170b83c38d3e7e715d9e8046a1e00",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,5 @@\n       public ByteBuffer call() throws Exception {\n         DFSClientFaultInjector.get().sleepBeforeHedgedGet();\n-        try (TraceScope ignored \u003d dfsClient.getTracer().\n-            newScope(\"hedgedRead\" + hedgedReadId, parentSpanId)) {\n-          actualGetFromOneDataNode(datanode, start, end, bb, corruptedBlocks);\n-          return bb;\n-        }\n+        actualGetFromOneDataNode(datanode, start, end, bb, corruptedBlocks);\n+        return bb;\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        DFSClientFaultInjector.get().sleepBeforeHedgedGet();\n        actualGetFromOneDataNode(datanode, start, end, bb, corruptedBlocks);\n        return bb;\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "b6bfb2fcb2391d51b8de97c01c1290880779132e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11738. Hedged pread takes more time when block moved from initial locations. Contributed by Vinayakumar B.\n",
      "commitDate": "21/08/17 1:45 PM",
      "commitName": "b6bfb2fcb2391d51b8de97c01c1290880779132e",
      "commitAuthor": "John Zhuge",
      "commitDateOld": "11/08/17 7:42 PM",
      "commitNameOld": "8b242f09a61a7536d2422546bfa6c2aaf1d57ed6",
      "commitAuthorOld": "John Zhuge",
      "daysBetweenCommits": 9.75,
      "commitsBetweenForRepo": 46,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,7 +1,8 @@\n       public ByteBuffer call() throws Exception {\n+        DFSClientFaultInjector.get().sleepBeforeHedgedGet();\n         try (TraceScope ignored \u003d dfsClient.getTracer().\n             newScope(\"hedgedRead\" + hedgedReadId, parentSpanId)) {\n           actualGetFromOneDataNode(datanode, start, end, bb, corruptedBlocks);\n           return bb;\n         }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        DFSClientFaultInjector.get().sleepBeforeHedgedGet();\n        try (TraceScope ignored \u003d dfsClient.getTracer().\n            newScope(\"hedgedRead\" + hedgedReadId, parentSpanId)) {\n          actualGetFromOneDataNode(datanode, start, end, bb, corruptedBlocks);\n          return bb;\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11708. Positional read will fail if replicas moved to different DNs after stream is opened. Contributed by Vinayakumar B.\n",
      "commitDate": "06/06/17 10:25 PM",
      "commitName": "70fc6746b326b9a913e8bebca5f5afaf01ab9e11",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "03/04/17 8:13 PM",
      "commitNameOld": "6eba79232f36b36e0196163adc8fe4219a6b6bf9",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 64.09,
      "commitsBetweenForRepo": 349,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,7 @@\n       public ByteBuffer call() throws Exception {\n         try (TraceScope ignored \u003d dfsClient.getTracer().\n             newScope(\"hedgedRead\" + hedgedReadId, parentSpanId)) {\n-          actualGetFromOneDataNode(datanode, block, start, end, bb,\n-              corruptedBlocks);\n+          actualGetFromOneDataNode(datanode, start, end, bb, corruptedBlocks);\n           return bb;\n         }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        try (TraceScope ignored \u003d dfsClient.getTracer().\n            newScope(\"hedgedRead\" + hedgedReadId, parentSpanId)) {\n          actualGetFromOneDataNode(datanode, start, end, bb, corruptedBlocks);\n          return bb;\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "401db4fc65140979fe7665983e36905e886df971": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
      "commitDate": "08/09/16 11:54 AM",
      "commitName": "401db4fc65140979fe7665983e36905e886df971",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "24/08/16 6:57 AM",
      "commitNameOld": "793447f79924c97c2b562d5e41fa85adf19673fe",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 15.21,
      "commitsBetweenForRepo": 83,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,8 @@\n       public ByteBuffer call() throws Exception {\n-        byte[] buf \u003d bb.array();\n-        int offset \u003d bb.position();\n         try (TraceScope ignored \u003d dfsClient.getTracer().\n             newScope(\"hedgedRead\" + hedgedReadId, parentSpanId)) {\n-          actualGetFromOneDataNode(datanode, block, start, end, buf,\n-              offset, corruptedBlocks);\n+          actualGetFromOneDataNode(datanode, block, start, end, bb,\n+              corruptedBlocks);\n           return bb;\n         }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        try (TraceScope ignored \u003d dfsClient.getTracer().\n            newScope(\"hedgedRead\" + hedgedReadId, parentSpanId)) {\n          actualGetFromOneDataNode(datanode, block, start, end, bb,\n              corruptedBlocks);\n          return bb;\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "8808779db351fe444388d4acb3094766b5980718": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9734. Refactoring of checksum failure report related codes. Contributed by Kai Zheng.\n\nChange-Id: Ie69a77e3498a360959f8e213c51fb2b17c28b64a\n",
      "commitDate": "25/02/16 9:55 AM",
      "commitName": "8808779db351fe444388d4acb3094766b5980718",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "22/01/16 9:46 AM",
      "commitNameOld": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 34.01,
      "commitsBetweenForRepo": 234,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,10 @@\n       public ByteBuffer call() throws Exception {\n         byte[] buf \u003d bb.array();\n         int offset \u003d bb.position();\n         try (TraceScope ignored \u003d dfsClient.getTracer().\n             newScope(\"hedgedRead\" + hedgedReadId, parentSpanId)) {\n           actualGetFromOneDataNode(datanode, block, start, end, buf,\n-              offset, corruptedBlockMap);\n+              offset, corruptedBlocks);\n           return bb;\n         }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        byte[] buf \u003d bb.array();\n        int offset \u003d bb.position();\n        try (TraceScope ignored \u003d dfsClient.getTracer().\n            newScope(\"hedgedRead\" + hedgedReadId, parentSpanId)) {\n          actualGetFromOneDataNode(datanode, block, start, end, buf,\n              offset, corruptedBlocks);\n          return bb;\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,10 @@\n       public ByteBuffer call() throws Exception {\n         byte[] buf \u003d bb.array();\n         int offset \u003d bb.position();\n-        TraceScope scope \u003d dfsClient.getTracer().\n-            newScope(\"hedgedRead\" + hedgedReadId, parentSpanId);\n-        try {\n+        try (TraceScope ignored \u003d dfsClient.getTracer().\n+            newScope(\"hedgedRead\" + hedgedReadId, parentSpanId)) {\n           actualGetFromOneDataNode(datanode, block, start, end, buf,\n               offset, corruptedBlockMap);\n           return bb;\n-        } finally {\n-          scope.close();\n         }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        byte[] buf \u003d bb.array();\n        int offset \u003d bb.position();\n        try (TraceScope ignored \u003d dfsClient.getTracer().\n            newScope(\"hedgedRead\" + hedgedReadId, parentSpanId)) {\n          actualGetFromOneDataNode(datanode, block, start, end, buf,\n              offset, corruptedBlockMap);\n          return bb;\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/09/15 11:08 AM",
      "commitNameOld": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.86,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n       public ByteBuffer call() throws Exception {\n         byte[] buf \u003d bb.array();\n         int offset \u003d bb.position();\n-        TraceScope scope \u003d\n-            Trace.startSpan(\"hedgedRead\" + hedgedReadId, parentSpan);\n+        TraceScope scope \u003d dfsClient.getTracer().\n+            newScope(\"hedgedRead\" + hedgedReadId, parentSpanId);\n         try {\n           actualGetFromOneDataNode(datanode, block, start, end, buf,\n               offset, corruptedBlockMap);\n           return bb;\n         } finally {\n           scope.close();\n         }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        byte[] buf \u003d bb.array();\n        int offset \u003d bb.position();\n        TraceScope scope \u003d dfsClient.getTracer().\n            newScope(\"hedgedRead\" + hedgedReadId, parentSpanId);\n        try {\n          actualGetFromOneDataNode(datanode, block, start, end, buf,\n              offset, corruptedBlockMap);\n          return bb;\n        } finally {\n          scope.close();\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        byte[] buf \u003d bb.array();\n        int offset \u003d bb.position();\n        TraceScope scope \u003d\n            Trace.startSpan(\"hedgedRead\" + hedgedReadId, parentSpan);\n        try {\n          actualGetFromOneDataNode(datanode, block, start, end, buf,\n              offset, corruptedBlockMap);\n          return bb;\n        } finally {\n          scope.close();\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "bff5999d07e9416a22846c849487e509ede55040": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
      "commitDate": "02/07/15 3:41 AM",
      "commitName": "bff5999d07e9416a22846c849487e509ede55040",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "04/06/15 10:51 AM",
      "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 27.7,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n       public ByteBuffer call() throws Exception {\n         byte[] buf \u003d bb.array();\n         int offset \u003d bb.position();\n         TraceScope scope \u003d\n             Trace.startSpan(\"hedgedRead\" + hedgedReadId, parentSpan);\n         try {\n-          actualGetFromOneDataNode(datanode, blockStartOffset, start, end, buf,\n+          actualGetFromOneDataNode(datanode, block, start, end, buf,\n               offset, corruptedBlockMap);\n           return bb;\n         } finally {\n           scope.close();\n         }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        byte[] buf \u003d bb.array();\n        int offset \u003d bb.position();\n        TraceScope scope \u003d\n            Trace.startSpan(\"hedgedRead\" + hedgedReadId, parentSpan);\n        try {\n          actualGetFromOneDataNode(datanode, block, start, end, buf,\n              offset, corruptedBlockMap);\n          return bb;\n        } finally {\n          scope.close();\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "a42bb1cd915abe5dc33eda3c01e8c74c64f35748": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8076. Code cleanup for DFSInputStream: use offset instead of LocatedBlock when possible. Contributed by Zhe Zhang.\n",
      "commitDate": "08/04/15 3:41 PM",
      "commitName": "a42bb1cd915abe5dc33eda3c01e8c74c64f35748",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "25/02/15 1:30 PM",
      "commitNameOld": "caa42adf208bfb5625d1b3ef665fbf334ffcccd9",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 42.05,
      "commitsBetweenForRepo": 367,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n       public ByteBuffer call() throws Exception {\n         byte[] buf \u003d bb.array();\n         int offset \u003d bb.position();\n         TraceScope scope \u003d\n             Trace.startSpan(\"hedgedRead\" + hedgedReadId, parentSpan);\n         try {\n-          actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n-              corruptedBlockMap);\n+          actualGetFromOneDataNode(datanode, blockStartOffset, start, end, buf,\n+              offset, corruptedBlockMap);\n           return bb;\n         } finally {\n           scope.close();\n         }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        byte[] buf \u003d bb.array();\n        int offset \u003d bb.position();\n        TraceScope scope \u003d\n            Trace.startSpan(\"hedgedRead\" + hedgedReadId, parentSpan);\n        try {\n          actualGetFromOneDataNode(datanode, blockStartOffset, start, end, buf,\n              offset, corruptedBlockMap);\n          return bb;\n        } finally {\n          scope.close();\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7055. Add tracing to DFSInputStream (cmccabe)\n",
      "commitDate": "03/10/14 1:35 PM",
      "commitName": "7f6ed7fe365166e8075359f1d0ad035fa876c70f",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "15/07/14 2:10 PM",
      "commitNameOld": "56c0bd4d37ab13b6cbcf860eda852da603ab2f62",
      "commitAuthorOld": "",
      "daysBetweenCommits": 79.98,
      "commitsBetweenForRepo": 820,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,7 +1,13 @@\n       public ByteBuffer call() throws Exception {\n         byte[] buf \u003d bb.array();\n         int offset \u003d bb.position();\n-        actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n-            corruptedBlockMap);\n-        return bb;\n+        TraceScope scope \u003d\n+            Trace.startSpan(\"hedgedRead\" + hedgedReadId, parentSpan);\n+        try {\n+          actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n+              corruptedBlockMap);\n+          return bb;\n+        } finally {\n+          scope.close();\n+        }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        byte[] buf \u003d bb.array();\n        int offset \u003d bb.position();\n        TraceScope scope \u003d\n            Trace.startSpan(\"hedgedRead\" + hedgedReadId, parentSpan);\n        try {\n          actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n              corruptedBlockMap);\n          return bb;\n        } finally {\n          scope.close();\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "0ca41a8f35e4f05bb04805a2e0a617850707b4db": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6591. while loop is executed tens of thousands of times in Hedged Read. Contributed by Liang Xie.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1606927 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/06/14 1:46 PM",
      "commitName": "0ca41a8f35e4f05bb04805a2e0a617850707b4db",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "28/04/14 1:20 PM",
      "commitNameOld": "71aa608b84afcbe19dc38ca7c43c6e750d5df97a",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 63.02,
      "commitsBetweenForRepo": 367,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,7 @@\n       public ByteBuffer call() throws Exception {\n-        try {\n-          byte[] buf \u003d bb.array();\n-          int offset \u003d bb.position();\n-          actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n-              corruptedBlockMap);\n-          return bb;\n-        } finally {\n-          latch.countDown();\n-        }\n+        byte[] buf \u003d bb.array();\n+        int offset \u003d bb.position();\n+        actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n+            corruptedBlockMap);\n+        return bb;\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        byte[] buf \u003d bb.array();\n        int offset \u003d bb.position();\n        actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n            corruptedBlockMap);\n        return bb;\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "f8904ad299bbcd109e3460f9b8ab9fbb9cebdad4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6231. DFSClient hangs infinitely if using hedged reads and all eligible datanodes die. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1586551 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/04/14 8:48 PM",
      "commitName": "f8904ad299bbcd109e3460f9b8ab9fbb9cebdad4",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "01/04/14 10:09 PM",
      "commitNameOld": "f93d99990a9a02ce693cd74466c2e5f127c1f560",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 8.94,
      "commitsBetweenForRepo": 63,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,11 @@\n       public ByteBuffer call() throws Exception {\n-        byte[] buf \u003d bb.array();\n-        int offset \u003d bb.position();\n-        actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n-            corruptedBlockMap);\n-        latch.countDown();\n-        return bb;\n+        try {\n+          byte[] buf \u003d bb.array();\n+          int offset \u003d bb.position();\n+          actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n+              corruptedBlockMap);\n+          return bb;\n+        } finally {\n+          latch.countDown();\n+        }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        try {\n          byte[] buf \u003d bb.array();\n          int offset \u003d bb.position();\n          actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n              corruptedBlockMap);\n          return bb;\n        } finally {\n          latch.countDown();\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "17db74a1c1972392a5aba48a3e0334dcd6c76487": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5776 Support \u0027hedged\u0027 reads in DFSClient\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1571466 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/02/14 2:34 PM",
      "commitName": "17db74a1c1972392a5aba48a3e0334dcd6c76487",
      "commitAuthor": "Michael Stack",
      "diff": "@@ -0,0 +1,8 @@\n+      public ByteBuffer call() throws Exception {\n+        byte[] buf \u003d bb.array();\n+        int offset \u003d bb.position();\n+        actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n+            corruptedBlockMap);\n+        latch.countDown();\n+        return bb;\n+      }\n\\ No newline at end of file\n",
      "actualSource": "      public ByteBuffer call() throws Exception {\n        byte[] buf \u003d bb.array();\n        int offset \u003d bb.position();\n        actualGetFromOneDataNode(datanode, block, start, end, buf, offset,\n            corruptedBlockMap);\n        latch.countDown();\n        return bb;\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}