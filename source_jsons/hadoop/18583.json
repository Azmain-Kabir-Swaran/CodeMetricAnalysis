{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "QuasiMonteCarlo.java",
  "functionName": "estimatePi",
  "functionId": "estimatePi___numMaps-int__numPoints-long__tmpDir-Path__conf-Configuration",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java",
  "functionStartLine": 249,
  "functionEndLine": 335,
  "numCommitsSeen": 11,
  "timeTaken": 6369,
  "changeHistory": [
    "908d8e914ef55aca69cb8e725e62bda29d1073cc",
    "09b8241e6e369ee00241efcc2e048e710dbcccd1",
    "bd69fb2d44403e930d1fc0868ed1dd2a49dd9659",
    "bb81a17e0b4fd7aa78604a6bb018ada8996f5674",
    "26447229ba2c3d43db978c1b3ce95613669182ee",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "908d8e914ef55aca69cb8e725e62bda29d1073cc": "Ybodychange",
    "09b8241e6e369ee00241efcc2e048e710dbcccd1": "Ybodychange",
    "bd69fb2d44403e930d1fc0868ed1dd2a49dd9659": "Ybodychange",
    "bb81a17e0b4fd7aa78604a6bb018ada8996f5674": "Ymultichange(Yparameterchange,Ybodychange)",
    "26447229ba2c3d43db978c1b3ce95613669182ee": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "908d8e914ef55aca69cb8e725e62bda29d1073cc": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6965. QuasiMonteCarlo should use Time.monotonicNow for measuring durations. Contributed by Chetna Chaudhari\n",
      "commitDate": "22/09/17 7:34 AM",
      "commitName": "908d8e914ef55aca69cb8e725e62bda29d1073cc",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "08/09/17 10:02 AM",
      "commitNameOld": "c35510a465cbda72c08239bcb5537375478bec3a",
      "commitAuthorOld": "Anu Engineer",
      "daysBetweenCommits": 13.9,
      "commitsBetweenForRepo": 156,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,87 @@\n   public static BigDecimal estimatePi(int numMaps, long numPoints,\n       Path tmpDir, Configuration conf\n       ) throws IOException, ClassNotFoundException, InterruptedException {\n     Job job \u003d Job.getInstance(conf);\n     //setup job conf\n     job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n     job.setJarByClass(QuasiMonteCarlo.class);\n \n     job.setInputFormatClass(SequenceFileInputFormat.class);\n \n     job.setOutputKeyClass(BooleanWritable.class);\n     job.setOutputValueClass(LongWritable.class);\n     job.setOutputFormatClass(SequenceFileOutputFormat.class);\n \n     job.setMapperClass(QmcMapper.class);\n \n     job.setReducerClass(QmcReducer.class);\n     job.setNumReduceTasks(1);\n \n     // turn off speculative execution, because DFS doesn\u0027t handle\n     // multiple writers to the same file.\n     job.setSpeculativeExecution(false);\n \n     //setup input/output directories\n     final Path inDir \u003d new Path(tmpDir, \"in\");\n     final Path outDir \u003d new Path(tmpDir, \"out\");\n     FileInputFormat.setInputPaths(job, inDir);\n     FileOutputFormat.setOutputPath(job, outDir);\n \n     final FileSystem fs \u003d FileSystem.get(conf);\n     if (fs.exists(tmpDir)) {\n       throw new IOException(\"Tmp directory \" + fs.makeQualified(tmpDir)\n           + \" already exists.  Please remove it first.\");\n     }\n     if (!fs.mkdirs(inDir)) {\n       throw new IOException(\"Cannot create input directory \" + inDir);\n     }\n \n     try {\n       //generate an input file for each map task\n       for(int i\u003d0; i \u003c numMaps; ++i) {\n         final Path file \u003d new Path(inDir, \"part\"+i);\n         final LongWritable offset \u003d new LongWritable(i * numPoints);\n         final LongWritable size \u003d new LongWritable(numPoints);\n         final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n             fs, conf, file,\n             LongWritable.class, LongWritable.class, CompressionType.NONE);\n         try {\n           writer.append(offset, size);\n         } finally {\n           writer.close();\n         }\n         System.out.println(\"Wrote input for Map #\"+i);\n       }\n   \n       //start a map/reduce job\n       System.out.println(\"Starting Job\");\n-      final long startTime \u003d System.currentTimeMillis();\n+      final long startTime \u003d Time.monotonicNow();\n       job.waitForCompletion(true);\n       if (!job.isSuccessful()) {\n         System.out.println(\"Job \" + job.getJobID() + \" failed!\");\n         System.exit(1);\n       }\n-      final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n+      final double duration \u003d (Time.monotonicNow() - startTime)/1000.0;\n       System.out.println(\"Job Finished in \" + duration + \" seconds\");\n \n       //read outputs\n       Path inFile \u003d new Path(outDir, \"reduce-out\");\n       LongWritable numInside \u003d new LongWritable();\n       LongWritable numOutside \u003d new LongWritable();\n       SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n       try {\n         reader.next(numInside, numOutside);\n       } finally {\n         reader.close();\n       }\n \n       //compute estimated value\n       final BigDecimal numTotal\n           \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n       return BigDecimal.valueOf(4).setScale(20)\n           .multiply(BigDecimal.valueOf(numInside.get()))\n           .divide(numTotal, RoundingMode.HALF_UP);\n     } finally {\n       fs.delete(tmpDir, true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BigDecimal estimatePi(int numMaps, long numPoints,\n      Path tmpDir, Configuration conf\n      ) throws IOException, ClassNotFoundException, InterruptedException {\n    Job job \u003d Job.getInstance(conf);\n    //setup job conf\n    job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n    job.setJarByClass(QuasiMonteCarlo.class);\n\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    job.setOutputKeyClass(BooleanWritable.class);\n    job.setOutputValueClass(LongWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    job.setMapperClass(QmcMapper.class);\n\n    job.setReducerClass(QmcReducer.class);\n    job.setNumReduceTasks(1);\n\n    // turn off speculative execution, because DFS doesn\u0027t handle\n    // multiple writers to the same file.\n    job.setSpeculativeExecution(false);\n\n    //setup input/output directories\n    final Path inDir \u003d new Path(tmpDir, \"in\");\n    final Path outDir \u003d new Path(tmpDir, \"out\");\n    FileInputFormat.setInputPaths(job, inDir);\n    FileOutputFormat.setOutputPath(job, outDir);\n\n    final FileSystem fs \u003d FileSystem.get(conf);\n    if (fs.exists(tmpDir)) {\n      throw new IOException(\"Tmp directory \" + fs.makeQualified(tmpDir)\n          + \" already exists.  Please remove it first.\");\n    }\n    if (!fs.mkdirs(inDir)) {\n      throw new IOException(\"Cannot create input directory \" + inDir);\n    }\n\n    try {\n      //generate an input file for each map task\n      for(int i\u003d0; i \u003c numMaps; ++i) {\n        final Path file \u003d new Path(inDir, \"part\"+i);\n        final LongWritable offset \u003d new LongWritable(i * numPoints);\n        final LongWritable size \u003d new LongWritable(numPoints);\n        final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n            fs, conf, file,\n            LongWritable.class, LongWritable.class, CompressionType.NONE);\n        try {\n          writer.append(offset, size);\n        } finally {\n          writer.close();\n        }\n        System.out.println(\"Wrote input for Map #\"+i);\n      }\n  \n      //start a map/reduce job\n      System.out.println(\"Starting Job\");\n      final long startTime \u003d Time.monotonicNow();\n      job.waitForCompletion(true);\n      if (!job.isSuccessful()) {\n        System.out.println(\"Job \" + job.getJobID() + \" failed!\");\n        System.exit(1);\n      }\n      final double duration \u003d (Time.monotonicNow() - startTime)/1000.0;\n      System.out.println(\"Job Finished in \" + duration + \" seconds\");\n\n      //read outputs\n      Path inFile \u003d new Path(outDir, \"reduce-out\");\n      LongWritable numInside \u003d new LongWritable();\n      LongWritable numOutside \u003d new LongWritable();\n      SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n      try {\n        reader.next(numInside, numOutside);\n      } finally {\n        reader.close();\n      }\n\n      //compute estimated value\n      final BigDecimal numTotal\n          \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n      return BigDecimal.valueOf(4).setScale(20)\n          .multiply(BigDecimal.valueOf(numInside.get()))\n          .divide(numTotal, RoundingMode.HALF_UP);\n    } finally {\n      fs.delete(tmpDir, true);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java",
      "extendedDetails": {}
    },
    "09b8241e6e369ee00241efcc2e048e710dbcccd1": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6383. Pi job (QuasiMonteCarlo) should not try to read the\nresults file if its job fails. Contributed by Harsh J.\n",
      "commitDate": "05/06/15 8:52 AM",
      "commitName": "09b8241e6e369ee00241efcc2e048e710dbcccd1",
      "commitAuthor": "Devaraj K",
      "commitDateOld": "03/03/15 1:06 AM",
      "commitNameOld": "9ae7f9eb7baeb244e1b95aabc93ad8124870b9a9",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 94.28,
      "commitsBetweenForRepo": 884,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,87 @@\n   public static BigDecimal estimatePi(int numMaps, long numPoints,\n       Path tmpDir, Configuration conf\n       ) throws IOException, ClassNotFoundException, InterruptedException {\n     Job job \u003d Job.getInstance(conf);\n     //setup job conf\n     job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n     job.setJarByClass(QuasiMonteCarlo.class);\n \n     job.setInputFormatClass(SequenceFileInputFormat.class);\n \n     job.setOutputKeyClass(BooleanWritable.class);\n     job.setOutputValueClass(LongWritable.class);\n     job.setOutputFormatClass(SequenceFileOutputFormat.class);\n \n     job.setMapperClass(QmcMapper.class);\n \n     job.setReducerClass(QmcReducer.class);\n     job.setNumReduceTasks(1);\n \n     // turn off speculative execution, because DFS doesn\u0027t handle\n     // multiple writers to the same file.\n     job.setSpeculativeExecution(false);\n \n     //setup input/output directories\n     final Path inDir \u003d new Path(tmpDir, \"in\");\n     final Path outDir \u003d new Path(tmpDir, \"out\");\n     FileInputFormat.setInputPaths(job, inDir);\n     FileOutputFormat.setOutputPath(job, outDir);\n \n     final FileSystem fs \u003d FileSystem.get(conf);\n     if (fs.exists(tmpDir)) {\n       throw new IOException(\"Tmp directory \" + fs.makeQualified(tmpDir)\n           + \" already exists.  Please remove it first.\");\n     }\n     if (!fs.mkdirs(inDir)) {\n       throw new IOException(\"Cannot create input directory \" + inDir);\n     }\n \n     try {\n       //generate an input file for each map task\n       for(int i\u003d0; i \u003c numMaps; ++i) {\n         final Path file \u003d new Path(inDir, \"part\"+i);\n         final LongWritable offset \u003d new LongWritable(i * numPoints);\n         final LongWritable size \u003d new LongWritable(numPoints);\n         final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n             fs, conf, file,\n             LongWritable.class, LongWritable.class, CompressionType.NONE);\n         try {\n           writer.append(offset, size);\n         } finally {\n           writer.close();\n         }\n         System.out.println(\"Wrote input for Map #\"+i);\n       }\n   \n       //start a map/reduce job\n       System.out.println(\"Starting Job\");\n       final long startTime \u003d System.currentTimeMillis();\n       job.waitForCompletion(true);\n+      if (!job.isSuccessful()) {\n+        System.out.println(\"Job \" + job.getJobID() + \" failed!\");\n+        System.exit(1);\n+      }\n       final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n       System.out.println(\"Job Finished in \" + duration + \" seconds\");\n \n       //read outputs\n       Path inFile \u003d new Path(outDir, \"reduce-out\");\n       LongWritable numInside \u003d new LongWritable();\n       LongWritable numOutside \u003d new LongWritable();\n       SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n       try {\n         reader.next(numInside, numOutside);\n       } finally {\n         reader.close();\n       }\n \n       //compute estimated value\n       final BigDecimal numTotal\n           \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n       return BigDecimal.valueOf(4).setScale(20)\n           .multiply(BigDecimal.valueOf(numInside.get()))\n           .divide(numTotal, RoundingMode.HALF_UP);\n     } finally {\n       fs.delete(tmpDir, true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BigDecimal estimatePi(int numMaps, long numPoints,\n      Path tmpDir, Configuration conf\n      ) throws IOException, ClassNotFoundException, InterruptedException {\n    Job job \u003d Job.getInstance(conf);\n    //setup job conf\n    job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n    job.setJarByClass(QuasiMonteCarlo.class);\n\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    job.setOutputKeyClass(BooleanWritable.class);\n    job.setOutputValueClass(LongWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    job.setMapperClass(QmcMapper.class);\n\n    job.setReducerClass(QmcReducer.class);\n    job.setNumReduceTasks(1);\n\n    // turn off speculative execution, because DFS doesn\u0027t handle\n    // multiple writers to the same file.\n    job.setSpeculativeExecution(false);\n\n    //setup input/output directories\n    final Path inDir \u003d new Path(tmpDir, \"in\");\n    final Path outDir \u003d new Path(tmpDir, \"out\");\n    FileInputFormat.setInputPaths(job, inDir);\n    FileOutputFormat.setOutputPath(job, outDir);\n\n    final FileSystem fs \u003d FileSystem.get(conf);\n    if (fs.exists(tmpDir)) {\n      throw new IOException(\"Tmp directory \" + fs.makeQualified(tmpDir)\n          + \" already exists.  Please remove it first.\");\n    }\n    if (!fs.mkdirs(inDir)) {\n      throw new IOException(\"Cannot create input directory \" + inDir);\n    }\n\n    try {\n      //generate an input file for each map task\n      for(int i\u003d0; i \u003c numMaps; ++i) {\n        final Path file \u003d new Path(inDir, \"part\"+i);\n        final LongWritable offset \u003d new LongWritable(i * numPoints);\n        final LongWritable size \u003d new LongWritable(numPoints);\n        final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n            fs, conf, file,\n            LongWritable.class, LongWritable.class, CompressionType.NONE);\n        try {\n          writer.append(offset, size);\n        } finally {\n          writer.close();\n        }\n        System.out.println(\"Wrote input for Map #\"+i);\n      }\n  \n      //start a map/reduce job\n      System.out.println(\"Starting Job\");\n      final long startTime \u003d System.currentTimeMillis();\n      job.waitForCompletion(true);\n      if (!job.isSuccessful()) {\n        System.out.println(\"Job \" + job.getJobID() + \" failed!\");\n        System.exit(1);\n      }\n      final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n      System.out.println(\"Job Finished in \" + duration + \" seconds\");\n\n      //read outputs\n      Path inFile \u003d new Path(outDir, \"reduce-out\");\n      LongWritable numInside \u003d new LongWritable();\n      LongWritable numOutside \u003d new LongWritable();\n      SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n      try {\n        reader.next(numInside, numOutside);\n      } finally {\n        reader.close();\n      }\n\n      //compute estimated value\n      final BigDecimal numTotal\n          \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n      return BigDecimal.valueOf(4).setScale(20)\n          .multiply(BigDecimal.valueOf(numInside.get()))\n          .divide(numTotal, RoundingMode.HALF_UP);\n    } finally {\n      fs.delete(tmpDir, true);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java",
      "extendedDetails": {}
    },
    "bd69fb2d44403e930d1fc0868ed1dd2a49dd9659": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5800. Use Job#getInstance instead of deprecated constructors. (aajisaka)\n",
      "commitDate": "03/02/15 2:30 PM",
      "commitName": "bd69fb2d44403e930d1fc0868ed1dd2a49dd9659",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "22/01/13 8:26 AM",
      "commitNameOld": "bb81a17e0b4fd7aa78604a6bb018ada8996f5674",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 742.25,
      "commitsBetweenForRepo": 5193,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,83 @@\n   public static BigDecimal estimatePi(int numMaps, long numPoints,\n       Path tmpDir, Configuration conf\n       ) throws IOException, ClassNotFoundException, InterruptedException {\n-    Job job \u003d new Job(conf);\n+    Job job \u003d Job.getInstance(conf);\n     //setup job conf\n     job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n     job.setJarByClass(QuasiMonteCarlo.class);\n \n     job.setInputFormatClass(SequenceFileInputFormat.class);\n \n     job.setOutputKeyClass(BooleanWritable.class);\n     job.setOutputValueClass(LongWritable.class);\n     job.setOutputFormatClass(SequenceFileOutputFormat.class);\n \n     job.setMapperClass(QmcMapper.class);\n \n     job.setReducerClass(QmcReducer.class);\n     job.setNumReduceTasks(1);\n \n     // turn off speculative execution, because DFS doesn\u0027t handle\n     // multiple writers to the same file.\n     job.setSpeculativeExecution(false);\n \n     //setup input/output directories\n     final Path inDir \u003d new Path(tmpDir, \"in\");\n     final Path outDir \u003d new Path(tmpDir, \"out\");\n     FileInputFormat.setInputPaths(job, inDir);\n     FileOutputFormat.setOutputPath(job, outDir);\n \n     final FileSystem fs \u003d FileSystem.get(conf);\n     if (fs.exists(tmpDir)) {\n       throw new IOException(\"Tmp directory \" + fs.makeQualified(tmpDir)\n           + \" already exists.  Please remove it first.\");\n     }\n     if (!fs.mkdirs(inDir)) {\n       throw new IOException(\"Cannot create input directory \" + inDir);\n     }\n \n     try {\n       //generate an input file for each map task\n       for(int i\u003d0; i \u003c numMaps; ++i) {\n         final Path file \u003d new Path(inDir, \"part\"+i);\n         final LongWritable offset \u003d new LongWritable(i * numPoints);\n         final LongWritable size \u003d new LongWritable(numPoints);\n         final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n             fs, conf, file,\n             LongWritable.class, LongWritable.class, CompressionType.NONE);\n         try {\n           writer.append(offset, size);\n         } finally {\n           writer.close();\n         }\n         System.out.println(\"Wrote input for Map #\"+i);\n       }\n   \n       //start a map/reduce job\n       System.out.println(\"Starting Job\");\n       final long startTime \u003d System.currentTimeMillis();\n       job.waitForCompletion(true);\n       final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n       System.out.println(\"Job Finished in \" + duration + \" seconds\");\n \n       //read outputs\n       Path inFile \u003d new Path(outDir, \"reduce-out\");\n       LongWritable numInside \u003d new LongWritable();\n       LongWritable numOutside \u003d new LongWritable();\n       SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n       try {\n         reader.next(numInside, numOutside);\n       } finally {\n         reader.close();\n       }\n \n       //compute estimated value\n       final BigDecimal numTotal\n           \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n       return BigDecimal.valueOf(4).setScale(20)\n           .multiply(BigDecimal.valueOf(numInside.get()))\n           .divide(numTotal, RoundingMode.HALF_UP);\n     } finally {\n       fs.delete(tmpDir, true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BigDecimal estimatePi(int numMaps, long numPoints,\n      Path tmpDir, Configuration conf\n      ) throws IOException, ClassNotFoundException, InterruptedException {\n    Job job \u003d Job.getInstance(conf);\n    //setup job conf\n    job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n    job.setJarByClass(QuasiMonteCarlo.class);\n\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    job.setOutputKeyClass(BooleanWritable.class);\n    job.setOutputValueClass(LongWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    job.setMapperClass(QmcMapper.class);\n\n    job.setReducerClass(QmcReducer.class);\n    job.setNumReduceTasks(1);\n\n    // turn off speculative execution, because DFS doesn\u0027t handle\n    // multiple writers to the same file.\n    job.setSpeculativeExecution(false);\n\n    //setup input/output directories\n    final Path inDir \u003d new Path(tmpDir, \"in\");\n    final Path outDir \u003d new Path(tmpDir, \"out\");\n    FileInputFormat.setInputPaths(job, inDir);\n    FileOutputFormat.setOutputPath(job, outDir);\n\n    final FileSystem fs \u003d FileSystem.get(conf);\n    if (fs.exists(tmpDir)) {\n      throw new IOException(\"Tmp directory \" + fs.makeQualified(tmpDir)\n          + \" already exists.  Please remove it first.\");\n    }\n    if (!fs.mkdirs(inDir)) {\n      throw new IOException(\"Cannot create input directory \" + inDir);\n    }\n\n    try {\n      //generate an input file for each map task\n      for(int i\u003d0; i \u003c numMaps; ++i) {\n        final Path file \u003d new Path(inDir, \"part\"+i);\n        final LongWritable offset \u003d new LongWritable(i * numPoints);\n        final LongWritable size \u003d new LongWritable(numPoints);\n        final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n            fs, conf, file,\n            LongWritable.class, LongWritable.class, CompressionType.NONE);\n        try {\n          writer.append(offset, size);\n        } finally {\n          writer.close();\n        }\n        System.out.println(\"Wrote input for Map #\"+i);\n      }\n  \n      //start a map/reduce job\n      System.out.println(\"Starting Job\");\n      final long startTime \u003d System.currentTimeMillis();\n      job.waitForCompletion(true);\n      final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n      System.out.println(\"Job Finished in \" + duration + \" seconds\");\n\n      //read outputs\n      Path inFile \u003d new Path(outDir, \"reduce-out\");\n      LongWritable numInside \u003d new LongWritable();\n      LongWritable numOutside \u003d new LongWritable();\n      SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n      try {\n        reader.next(numInside, numOutside);\n      } finally {\n        reader.close();\n      }\n\n      //compute estimated value\n      final BigDecimal numTotal\n          \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n      return BigDecimal.valueOf(4).setScale(20)\n          .multiply(BigDecimal.valueOf(numInside.get()))\n          .divide(numTotal, RoundingMode.HALF_UP);\n    } finally {\n      fs.delete(tmpDir, true);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java",
      "extendedDetails": {}
    },
    "bb81a17e0b4fd7aa78604a6bb018ada8996f5674": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-4949. Enable multiple pi jobs to run in parallel. (sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1437029 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/01/13 8:26 AM",
      "commitName": "bb81a17e0b4fd7aa78604a6bb018ada8996f5674",
      "commitAuthor": "Alejandro Abdelnur",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-4949. Enable multiple pi jobs to run in parallel. (sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1437029 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/01/13 8:26 AM",
          "commitName": "bb81a17e0b4fd7aa78604a6bb018ada8996f5674",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "18/11/11 5:24 PM",
          "commitNameOld": "26447229ba2c3d43db978c1b3ce95613669182ee",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 430.63,
          "commitsBetweenForRepo": 2612,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,83 +1,83 @@\n   public static BigDecimal estimatePi(int numMaps, long numPoints,\n-      Configuration conf\n+      Path tmpDir, Configuration conf\n       ) throws IOException, ClassNotFoundException, InterruptedException {\n     Job job \u003d new Job(conf);\n     //setup job conf\n     job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n     job.setJarByClass(QuasiMonteCarlo.class);\n \n     job.setInputFormatClass(SequenceFileInputFormat.class);\n \n     job.setOutputKeyClass(BooleanWritable.class);\n     job.setOutputValueClass(LongWritable.class);\n     job.setOutputFormatClass(SequenceFileOutputFormat.class);\n \n     job.setMapperClass(QmcMapper.class);\n \n     job.setReducerClass(QmcReducer.class);\n     job.setNumReduceTasks(1);\n \n     // turn off speculative execution, because DFS doesn\u0027t handle\n     // multiple writers to the same file.\n     job.setSpeculativeExecution(false);\n \n     //setup input/output directories\n-    final Path inDir \u003d new Path(TMP_DIR, \"in\");\n-    final Path outDir \u003d new Path(TMP_DIR, \"out\");\n+    final Path inDir \u003d new Path(tmpDir, \"in\");\n+    final Path outDir \u003d new Path(tmpDir, \"out\");\n     FileInputFormat.setInputPaths(job, inDir);\n     FileOutputFormat.setOutputPath(job, outDir);\n \n     final FileSystem fs \u003d FileSystem.get(conf);\n-    if (fs.exists(TMP_DIR)) {\n-      throw new IOException(\"Tmp directory \" + fs.makeQualified(TMP_DIR)\n+    if (fs.exists(tmpDir)) {\n+      throw new IOException(\"Tmp directory \" + fs.makeQualified(tmpDir)\n           + \" already exists.  Please remove it first.\");\n     }\n     if (!fs.mkdirs(inDir)) {\n       throw new IOException(\"Cannot create input directory \" + inDir);\n     }\n \n     try {\n       //generate an input file for each map task\n       for(int i\u003d0; i \u003c numMaps; ++i) {\n         final Path file \u003d new Path(inDir, \"part\"+i);\n         final LongWritable offset \u003d new LongWritable(i * numPoints);\n         final LongWritable size \u003d new LongWritable(numPoints);\n         final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n             fs, conf, file,\n             LongWritable.class, LongWritable.class, CompressionType.NONE);\n         try {\n           writer.append(offset, size);\n         } finally {\n           writer.close();\n         }\n         System.out.println(\"Wrote input for Map #\"+i);\n       }\n   \n       //start a map/reduce job\n       System.out.println(\"Starting Job\");\n       final long startTime \u003d System.currentTimeMillis();\n       job.waitForCompletion(true);\n       final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n       System.out.println(\"Job Finished in \" + duration + \" seconds\");\n \n       //read outputs\n       Path inFile \u003d new Path(outDir, \"reduce-out\");\n       LongWritable numInside \u003d new LongWritable();\n       LongWritable numOutside \u003d new LongWritable();\n       SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n       try {\n         reader.next(numInside, numOutside);\n       } finally {\n         reader.close();\n       }\n \n       //compute estimated value\n       final BigDecimal numTotal\n           \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n       return BigDecimal.valueOf(4).setScale(20)\n           .multiply(BigDecimal.valueOf(numInside.get()))\n           .divide(numTotal, RoundingMode.HALF_UP);\n     } finally {\n-      fs.delete(TMP_DIR, true);\n+      fs.delete(tmpDir, true);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BigDecimal estimatePi(int numMaps, long numPoints,\n      Path tmpDir, Configuration conf\n      ) throws IOException, ClassNotFoundException, InterruptedException {\n    Job job \u003d new Job(conf);\n    //setup job conf\n    job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n    job.setJarByClass(QuasiMonteCarlo.class);\n\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    job.setOutputKeyClass(BooleanWritable.class);\n    job.setOutputValueClass(LongWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    job.setMapperClass(QmcMapper.class);\n\n    job.setReducerClass(QmcReducer.class);\n    job.setNumReduceTasks(1);\n\n    // turn off speculative execution, because DFS doesn\u0027t handle\n    // multiple writers to the same file.\n    job.setSpeculativeExecution(false);\n\n    //setup input/output directories\n    final Path inDir \u003d new Path(tmpDir, \"in\");\n    final Path outDir \u003d new Path(tmpDir, \"out\");\n    FileInputFormat.setInputPaths(job, inDir);\n    FileOutputFormat.setOutputPath(job, outDir);\n\n    final FileSystem fs \u003d FileSystem.get(conf);\n    if (fs.exists(tmpDir)) {\n      throw new IOException(\"Tmp directory \" + fs.makeQualified(tmpDir)\n          + \" already exists.  Please remove it first.\");\n    }\n    if (!fs.mkdirs(inDir)) {\n      throw new IOException(\"Cannot create input directory \" + inDir);\n    }\n\n    try {\n      //generate an input file for each map task\n      for(int i\u003d0; i \u003c numMaps; ++i) {\n        final Path file \u003d new Path(inDir, \"part\"+i);\n        final LongWritable offset \u003d new LongWritable(i * numPoints);\n        final LongWritable size \u003d new LongWritable(numPoints);\n        final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n            fs, conf, file,\n            LongWritable.class, LongWritable.class, CompressionType.NONE);\n        try {\n          writer.append(offset, size);\n        } finally {\n          writer.close();\n        }\n        System.out.println(\"Wrote input for Map #\"+i);\n      }\n  \n      //start a map/reduce job\n      System.out.println(\"Starting Job\");\n      final long startTime \u003d System.currentTimeMillis();\n      job.waitForCompletion(true);\n      final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n      System.out.println(\"Job Finished in \" + duration + \" seconds\");\n\n      //read outputs\n      Path inFile \u003d new Path(outDir, \"reduce-out\");\n      LongWritable numInside \u003d new LongWritable();\n      LongWritable numOutside \u003d new LongWritable();\n      SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n      try {\n        reader.next(numInside, numOutside);\n      } finally {\n        reader.close();\n      }\n\n      //compute estimated value\n      final BigDecimal numTotal\n          \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n      return BigDecimal.valueOf(4).setScale(20)\n          .multiply(BigDecimal.valueOf(numInside.get()))\n          .divide(numTotal, RoundingMode.HALF_UP);\n    } finally {\n      fs.delete(tmpDir, true);\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java",
          "extendedDetails": {
            "oldValue": "[numMaps-int, numPoints-long, conf-Configuration]",
            "newValue": "[numMaps-int, numPoints-long, tmpDir-Path, conf-Configuration]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-4949. Enable multiple pi jobs to run in parallel. (sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1437029 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/01/13 8:26 AM",
          "commitName": "bb81a17e0b4fd7aa78604a6bb018ada8996f5674",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "18/11/11 5:24 PM",
          "commitNameOld": "26447229ba2c3d43db978c1b3ce95613669182ee",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 430.63,
          "commitsBetweenForRepo": 2612,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,83 +1,83 @@\n   public static BigDecimal estimatePi(int numMaps, long numPoints,\n-      Configuration conf\n+      Path tmpDir, Configuration conf\n       ) throws IOException, ClassNotFoundException, InterruptedException {\n     Job job \u003d new Job(conf);\n     //setup job conf\n     job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n     job.setJarByClass(QuasiMonteCarlo.class);\n \n     job.setInputFormatClass(SequenceFileInputFormat.class);\n \n     job.setOutputKeyClass(BooleanWritable.class);\n     job.setOutputValueClass(LongWritable.class);\n     job.setOutputFormatClass(SequenceFileOutputFormat.class);\n \n     job.setMapperClass(QmcMapper.class);\n \n     job.setReducerClass(QmcReducer.class);\n     job.setNumReduceTasks(1);\n \n     // turn off speculative execution, because DFS doesn\u0027t handle\n     // multiple writers to the same file.\n     job.setSpeculativeExecution(false);\n \n     //setup input/output directories\n-    final Path inDir \u003d new Path(TMP_DIR, \"in\");\n-    final Path outDir \u003d new Path(TMP_DIR, \"out\");\n+    final Path inDir \u003d new Path(tmpDir, \"in\");\n+    final Path outDir \u003d new Path(tmpDir, \"out\");\n     FileInputFormat.setInputPaths(job, inDir);\n     FileOutputFormat.setOutputPath(job, outDir);\n \n     final FileSystem fs \u003d FileSystem.get(conf);\n-    if (fs.exists(TMP_DIR)) {\n-      throw new IOException(\"Tmp directory \" + fs.makeQualified(TMP_DIR)\n+    if (fs.exists(tmpDir)) {\n+      throw new IOException(\"Tmp directory \" + fs.makeQualified(tmpDir)\n           + \" already exists.  Please remove it first.\");\n     }\n     if (!fs.mkdirs(inDir)) {\n       throw new IOException(\"Cannot create input directory \" + inDir);\n     }\n \n     try {\n       //generate an input file for each map task\n       for(int i\u003d0; i \u003c numMaps; ++i) {\n         final Path file \u003d new Path(inDir, \"part\"+i);\n         final LongWritable offset \u003d new LongWritable(i * numPoints);\n         final LongWritable size \u003d new LongWritable(numPoints);\n         final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n             fs, conf, file,\n             LongWritable.class, LongWritable.class, CompressionType.NONE);\n         try {\n           writer.append(offset, size);\n         } finally {\n           writer.close();\n         }\n         System.out.println(\"Wrote input for Map #\"+i);\n       }\n   \n       //start a map/reduce job\n       System.out.println(\"Starting Job\");\n       final long startTime \u003d System.currentTimeMillis();\n       job.waitForCompletion(true);\n       final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n       System.out.println(\"Job Finished in \" + duration + \" seconds\");\n \n       //read outputs\n       Path inFile \u003d new Path(outDir, \"reduce-out\");\n       LongWritable numInside \u003d new LongWritable();\n       LongWritable numOutside \u003d new LongWritable();\n       SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n       try {\n         reader.next(numInside, numOutside);\n       } finally {\n         reader.close();\n       }\n \n       //compute estimated value\n       final BigDecimal numTotal\n           \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n       return BigDecimal.valueOf(4).setScale(20)\n           .multiply(BigDecimal.valueOf(numInside.get()))\n           .divide(numTotal, RoundingMode.HALF_UP);\n     } finally {\n-      fs.delete(TMP_DIR, true);\n+      fs.delete(tmpDir, true);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BigDecimal estimatePi(int numMaps, long numPoints,\n      Path tmpDir, Configuration conf\n      ) throws IOException, ClassNotFoundException, InterruptedException {\n    Job job \u003d new Job(conf);\n    //setup job conf\n    job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n    job.setJarByClass(QuasiMonteCarlo.class);\n\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    job.setOutputKeyClass(BooleanWritable.class);\n    job.setOutputValueClass(LongWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    job.setMapperClass(QmcMapper.class);\n\n    job.setReducerClass(QmcReducer.class);\n    job.setNumReduceTasks(1);\n\n    // turn off speculative execution, because DFS doesn\u0027t handle\n    // multiple writers to the same file.\n    job.setSpeculativeExecution(false);\n\n    //setup input/output directories\n    final Path inDir \u003d new Path(tmpDir, \"in\");\n    final Path outDir \u003d new Path(tmpDir, \"out\");\n    FileInputFormat.setInputPaths(job, inDir);\n    FileOutputFormat.setOutputPath(job, outDir);\n\n    final FileSystem fs \u003d FileSystem.get(conf);\n    if (fs.exists(tmpDir)) {\n      throw new IOException(\"Tmp directory \" + fs.makeQualified(tmpDir)\n          + \" already exists.  Please remove it first.\");\n    }\n    if (!fs.mkdirs(inDir)) {\n      throw new IOException(\"Cannot create input directory \" + inDir);\n    }\n\n    try {\n      //generate an input file for each map task\n      for(int i\u003d0; i \u003c numMaps; ++i) {\n        final Path file \u003d new Path(inDir, \"part\"+i);\n        final LongWritable offset \u003d new LongWritable(i * numPoints);\n        final LongWritable size \u003d new LongWritable(numPoints);\n        final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n            fs, conf, file,\n            LongWritable.class, LongWritable.class, CompressionType.NONE);\n        try {\n          writer.append(offset, size);\n        } finally {\n          writer.close();\n        }\n        System.out.println(\"Wrote input for Map #\"+i);\n      }\n  \n      //start a map/reduce job\n      System.out.println(\"Starting Job\");\n      final long startTime \u003d System.currentTimeMillis();\n      job.waitForCompletion(true);\n      final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n      System.out.println(\"Job Finished in \" + duration + \" seconds\");\n\n      //read outputs\n      Path inFile \u003d new Path(outDir, \"reduce-out\");\n      LongWritable numInside \u003d new LongWritable();\n      LongWritable numOutside \u003d new LongWritable();\n      SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n      try {\n        reader.next(numInside, numOutside);\n      } finally {\n        reader.close();\n      }\n\n      //compute estimated value\n      final BigDecimal numTotal\n          \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n      return BigDecimal.valueOf(4).setScale(20)\n          .multiply(BigDecimal.valueOf(numInside.get()))\n          .divide(numTotal, RoundingMode.HALF_UP);\n    } finally {\n      fs.delete(tmpDir, true);\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java",
          "extendedDetails": {}
        }
      ]
    },
    "26447229ba2c3d43db978c1b3ce95613669182ee": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7590. Mavenize streaming and MR examples. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203941 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 5:24 PM",
      "commitName": "26447229ba2c3d43db978c1b3ce95613669182ee",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "18/11/11 1:04 AM",
      "commitNameOld": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.68,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static BigDecimal estimatePi(int numMaps, long numPoints,\n      Configuration conf\n      ) throws IOException, ClassNotFoundException, InterruptedException {\n    Job job \u003d new Job(conf);\n    //setup job conf\n    job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n    job.setJarByClass(QuasiMonteCarlo.class);\n\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    job.setOutputKeyClass(BooleanWritable.class);\n    job.setOutputValueClass(LongWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    job.setMapperClass(QmcMapper.class);\n\n    job.setReducerClass(QmcReducer.class);\n    job.setNumReduceTasks(1);\n\n    // turn off speculative execution, because DFS doesn\u0027t handle\n    // multiple writers to the same file.\n    job.setSpeculativeExecution(false);\n\n    //setup input/output directories\n    final Path inDir \u003d new Path(TMP_DIR, \"in\");\n    final Path outDir \u003d new Path(TMP_DIR, \"out\");\n    FileInputFormat.setInputPaths(job, inDir);\n    FileOutputFormat.setOutputPath(job, outDir);\n\n    final FileSystem fs \u003d FileSystem.get(conf);\n    if (fs.exists(TMP_DIR)) {\n      throw new IOException(\"Tmp directory \" + fs.makeQualified(TMP_DIR)\n          + \" already exists.  Please remove it first.\");\n    }\n    if (!fs.mkdirs(inDir)) {\n      throw new IOException(\"Cannot create input directory \" + inDir);\n    }\n\n    try {\n      //generate an input file for each map task\n      for(int i\u003d0; i \u003c numMaps; ++i) {\n        final Path file \u003d new Path(inDir, \"part\"+i);\n        final LongWritable offset \u003d new LongWritable(i * numPoints);\n        final LongWritable size \u003d new LongWritable(numPoints);\n        final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n            fs, conf, file,\n            LongWritable.class, LongWritable.class, CompressionType.NONE);\n        try {\n          writer.append(offset, size);\n        } finally {\n          writer.close();\n        }\n        System.out.println(\"Wrote input for Map #\"+i);\n      }\n  \n      //start a map/reduce job\n      System.out.println(\"Starting Job\");\n      final long startTime \u003d System.currentTimeMillis();\n      job.waitForCompletion(true);\n      final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n      System.out.println(\"Job Finished in \" + duration + \" seconds\");\n\n      //read outputs\n      Path inFile \u003d new Path(outDir, \"reduce-out\");\n      LongWritable numInside \u003d new LongWritable();\n      LongWritable numOutside \u003d new LongWritable();\n      SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n      try {\n        reader.next(numInside, numOutside);\n      } finally {\n        reader.close();\n      }\n\n      //compute estimated value\n      final BigDecimal numTotal\n          \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n      return BigDecimal.valueOf(4).setScale(20)\n          .multiply(BigDecimal.valueOf(numInside.get()))\n          .divide(numTotal, RoundingMode.HALF_UP);\n    } finally {\n      fs.delete(TMP_DIR, true);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/examples/org/apache/hadoop/examples/QuasiMonteCarlo.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static BigDecimal estimatePi(int numMaps, long numPoints,\n      Configuration conf\n      ) throws IOException, ClassNotFoundException, InterruptedException {\n    Job job \u003d new Job(conf);\n    //setup job conf\n    job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n    job.setJarByClass(QuasiMonteCarlo.class);\n\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    job.setOutputKeyClass(BooleanWritable.class);\n    job.setOutputValueClass(LongWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    job.setMapperClass(QmcMapper.class);\n\n    job.setReducerClass(QmcReducer.class);\n    job.setNumReduceTasks(1);\n\n    // turn off speculative execution, because DFS doesn\u0027t handle\n    // multiple writers to the same file.\n    job.setSpeculativeExecution(false);\n\n    //setup input/output directories\n    final Path inDir \u003d new Path(TMP_DIR, \"in\");\n    final Path outDir \u003d new Path(TMP_DIR, \"out\");\n    FileInputFormat.setInputPaths(job, inDir);\n    FileOutputFormat.setOutputPath(job, outDir);\n\n    final FileSystem fs \u003d FileSystem.get(conf);\n    if (fs.exists(TMP_DIR)) {\n      throw new IOException(\"Tmp directory \" + fs.makeQualified(TMP_DIR)\n          + \" already exists.  Please remove it first.\");\n    }\n    if (!fs.mkdirs(inDir)) {\n      throw new IOException(\"Cannot create input directory \" + inDir);\n    }\n\n    try {\n      //generate an input file for each map task\n      for(int i\u003d0; i \u003c numMaps; ++i) {\n        final Path file \u003d new Path(inDir, \"part\"+i);\n        final LongWritable offset \u003d new LongWritable(i * numPoints);\n        final LongWritable size \u003d new LongWritable(numPoints);\n        final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n            fs, conf, file,\n            LongWritable.class, LongWritable.class, CompressionType.NONE);\n        try {\n          writer.append(offset, size);\n        } finally {\n          writer.close();\n        }\n        System.out.println(\"Wrote input for Map #\"+i);\n      }\n  \n      //start a map/reduce job\n      System.out.println(\"Starting Job\");\n      final long startTime \u003d System.currentTimeMillis();\n      job.waitForCompletion(true);\n      final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n      System.out.println(\"Job Finished in \" + duration + \" seconds\");\n\n      //read outputs\n      Path inFile \u003d new Path(outDir, \"reduce-out\");\n      LongWritable numInside \u003d new LongWritable();\n      LongWritable numOutside \u003d new LongWritable();\n      SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n      try {\n        reader.next(numInside, numOutside);\n      } finally {\n        reader.close();\n      }\n\n      //compute estimated value\n      final BigDecimal numTotal\n          \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n      return BigDecimal.valueOf(4).setScale(20)\n          .multiply(BigDecimal.valueOf(numInside.get()))\n          .divide(numTotal, RoundingMode.HALF_UP);\n    } finally {\n      fs.delete(TMP_DIR, true);\n    }\n  }",
      "path": "hadoop-mapreduce-project/src/examples/org/apache/hadoop/examples/QuasiMonteCarlo.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/examples/org/apache/hadoop/examples/QuasiMonteCarlo.java",
        "newPath": "hadoop-mapreduce-project/src/examples/org/apache/hadoop/examples/QuasiMonteCarlo.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static BigDecimal estimatePi(int numMaps, long numPoints,\n      Configuration conf\n      ) throws IOException, ClassNotFoundException, InterruptedException {\n    Job job \u003d new Job(conf);\n    //setup job conf\n    job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n    job.setJarByClass(QuasiMonteCarlo.class);\n\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    job.setOutputKeyClass(BooleanWritable.class);\n    job.setOutputValueClass(LongWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    job.setMapperClass(QmcMapper.class);\n\n    job.setReducerClass(QmcReducer.class);\n    job.setNumReduceTasks(1);\n\n    // turn off speculative execution, because DFS doesn\u0027t handle\n    // multiple writers to the same file.\n    job.setSpeculativeExecution(false);\n\n    //setup input/output directories\n    final Path inDir \u003d new Path(TMP_DIR, \"in\");\n    final Path outDir \u003d new Path(TMP_DIR, \"out\");\n    FileInputFormat.setInputPaths(job, inDir);\n    FileOutputFormat.setOutputPath(job, outDir);\n\n    final FileSystem fs \u003d FileSystem.get(conf);\n    if (fs.exists(TMP_DIR)) {\n      throw new IOException(\"Tmp directory \" + fs.makeQualified(TMP_DIR)\n          + \" already exists.  Please remove it first.\");\n    }\n    if (!fs.mkdirs(inDir)) {\n      throw new IOException(\"Cannot create input directory \" + inDir);\n    }\n\n    try {\n      //generate an input file for each map task\n      for(int i\u003d0; i \u003c numMaps; ++i) {\n        final Path file \u003d new Path(inDir, \"part\"+i);\n        final LongWritable offset \u003d new LongWritable(i * numPoints);\n        final LongWritable size \u003d new LongWritable(numPoints);\n        final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n            fs, conf, file,\n            LongWritable.class, LongWritable.class, CompressionType.NONE);\n        try {\n          writer.append(offset, size);\n        } finally {\n          writer.close();\n        }\n        System.out.println(\"Wrote input for Map #\"+i);\n      }\n  \n      //start a map/reduce job\n      System.out.println(\"Starting Job\");\n      final long startTime \u003d System.currentTimeMillis();\n      job.waitForCompletion(true);\n      final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n      System.out.println(\"Job Finished in \" + duration + \" seconds\");\n\n      //read outputs\n      Path inFile \u003d new Path(outDir, \"reduce-out\");\n      LongWritable numInside \u003d new LongWritable();\n      LongWritable numOutside \u003d new LongWritable();\n      SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n      try {\n        reader.next(numInside, numOutside);\n      } finally {\n        reader.close();\n      }\n\n      //compute estimated value\n      final BigDecimal numTotal\n          \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n      return BigDecimal.valueOf(4).setScale(20)\n          .multiply(BigDecimal.valueOf(numInside.get()))\n          .divide(numTotal, RoundingMode.HALF_UP);\n    } finally {\n      fs.delete(TMP_DIR, true);\n    }\n  }",
      "path": "hadoop-mapreduce/src/examples/org/apache/hadoop/examples/QuasiMonteCarlo.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/examples/org/apache/hadoop/examples/QuasiMonteCarlo.java",
        "newPath": "hadoop-mapreduce/src/examples/org/apache/hadoop/examples/QuasiMonteCarlo.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,83 @@\n+  public static BigDecimal estimatePi(int numMaps, long numPoints,\n+      Configuration conf\n+      ) throws IOException, ClassNotFoundException, InterruptedException {\n+    Job job \u003d new Job(conf);\n+    //setup job conf\n+    job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n+    job.setJarByClass(QuasiMonteCarlo.class);\n+\n+    job.setInputFormatClass(SequenceFileInputFormat.class);\n+\n+    job.setOutputKeyClass(BooleanWritable.class);\n+    job.setOutputValueClass(LongWritable.class);\n+    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n+\n+    job.setMapperClass(QmcMapper.class);\n+\n+    job.setReducerClass(QmcReducer.class);\n+    job.setNumReduceTasks(1);\n+\n+    // turn off speculative execution, because DFS doesn\u0027t handle\n+    // multiple writers to the same file.\n+    job.setSpeculativeExecution(false);\n+\n+    //setup input/output directories\n+    final Path inDir \u003d new Path(TMP_DIR, \"in\");\n+    final Path outDir \u003d new Path(TMP_DIR, \"out\");\n+    FileInputFormat.setInputPaths(job, inDir);\n+    FileOutputFormat.setOutputPath(job, outDir);\n+\n+    final FileSystem fs \u003d FileSystem.get(conf);\n+    if (fs.exists(TMP_DIR)) {\n+      throw new IOException(\"Tmp directory \" + fs.makeQualified(TMP_DIR)\n+          + \" already exists.  Please remove it first.\");\n+    }\n+    if (!fs.mkdirs(inDir)) {\n+      throw new IOException(\"Cannot create input directory \" + inDir);\n+    }\n+\n+    try {\n+      //generate an input file for each map task\n+      for(int i\u003d0; i \u003c numMaps; ++i) {\n+        final Path file \u003d new Path(inDir, \"part\"+i);\n+        final LongWritable offset \u003d new LongWritable(i * numPoints);\n+        final LongWritable size \u003d new LongWritable(numPoints);\n+        final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n+            fs, conf, file,\n+            LongWritable.class, LongWritable.class, CompressionType.NONE);\n+        try {\n+          writer.append(offset, size);\n+        } finally {\n+          writer.close();\n+        }\n+        System.out.println(\"Wrote input for Map #\"+i);\n+      }\n+  \n+      //start a map/reduce job\n+      System.out.println(\"Starting Job\");\n+      final long startTime \u003d System.currentTimeMillis();\n+      job.waitForCompletion(true);\n+      final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n+      System.out.println(\"Job Finished in \" + duration + \" seconds\");\n+\n+      //read outputs\n+      Path inFile \u003d new Path(outDir, \"reduce-out\");\n+      LongWritable numInside \u003d new LongWritable();\n+      LongWritable numOutside \u003d new LongWritable();\n+      SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n+      try {\n+        reader.next(numInside, numOutside);\n+      } finally {\n+        reader.close();\n+      }\n+\n+      //compute estimated value\n+      final BigDecimal numTotal\n+          \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n+      return BigDecimal.valueOf(4).setScale(20)\n+          .multiply(BigDecimal.valueOf(numInside.get()))\n+          .divide(numTotal, RoundingMode.HALF_UP);\n+    } finally {\n+      fs.delete(TMP_DIR, true);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static BigDecimal estimatePi(int numMaps, long numPoints,\n      Configuration conf\n      ) throws IOException, ClassNotFoundException, InterruptedException {\n    Job job \u003d new Job(conf);\n    //setup job conf\n    job.setJobName(QuasiMonteCarlo.class.getSimpleName());\n    job.setJarByClass(QuasiMonteCarlo.class);\n\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    job.setOutputKeyClass(BooleanWritable.class);\n    job.setOutputValueClass(LongWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    job.setMapperClass(QmcMapper.class);\n\n    job.setReducerClass(QmcReducer.class);\n    job.setNumReduceTasks(1);\n\n    // turn off speculative execution, because DFS doesn\u0027t handle\n    // multiple writers to the same file.\n    job.setSpeculativeExecution(false);\n\n    //setup input/output directories\n    final Path inDir \u003d new Path(TMP_DIR, \"in\");\n    final Path outDir \u003d new Path(TMP_DIR, \"out\");\n    FileInputFormat.setInputPaths(job, inDir);\n    FileOutputFormat.setOutputPath(job, outDir);\n\n    final FileSystem fs \u003d FileSystem.get(conf);\n    if (fs.exists(TMP_DIR)) {\n      throw new IOException(\"Tmp directory \" + fs.makeQualified(TMP_DIR)\n          + \" already exists.  Please remove it first.\");\n    }\n    if (!fs.mkdirs(inDir)) {\n      throw new IOException(\"Cannot create input directory \" + inDir);\n    }\n\n    try {\n      //generate an input file for each map task\n      for(int i\u003d0; i \u003c numMaps; ++i) {\n        final Path file \u003d new Path(inDir, \"part\"+i);\n        final LongWritable offset \u003d new LongWritable(i * numPoints);\n        final LongWritable size \u003d new LongWritable(numPoints);\n        final SequenceFile.Writer writer \u003d SequenceFile.createWriter(\n            fs, conf, file,\n            LongWritable.class, LongWritable.class, CompressionType.NONE);\n        try {\n          writer.append(offset, size);\n        } finally {\n          writer.close();\n        }\n        System.out.println(\"Wrote input for Map #\"+i);\n      }\n  \n      //start a map/reduce job\n      System.out.println(\"Starting Job\");\n      final long startTime \u003d System.currentTimeMillis();\n      job.waitForCompletion(true);\n      final double duration \u003d (System.currentTimeMillis() - startTime)/1000.0;\n      System.out.println(\"Job Finished in \" + duration + \" seconds\");\n\n      //read outputs\n      Path inFile \u003d new Path(outDir, \"reduce-out\");\n      LongWritable numInside \u003d new LongWritable();\n      LongWritable numOutside \u003d new LongWritable();\n      SequenceFile.Reader reader \u003d new SequenceFile.Reader(fs, inFile, conf);\n      try {\n        reader.next(numInside, numOutside);\n      } finally {\n        reader.close();\n      }\n\n      //compute estimated value\n      final BigDecimal numTotal\n          \u003d BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\n      return BigDecimal.valueOf(4).setScale(20)\n          .multiply(BigDecimal.valueOf(numInside.get()))\n          .divide(numTotal, RoundingMode.HALF_UP);\n    } finally {\n      fs.delete(TMP_DIR, true);\n    }\n  }",
      "path": "mapreduce/src/examples/org/apache/hadoop/examples/QuasiMonteCarlo.java"
    }
  }
}