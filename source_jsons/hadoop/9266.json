{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSNamesystem.java",
  "functionName": "getNodeUsage",
  "functionId": "getNodeUsage",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
  "functionStartLine": 6565,
  "functionEndLine": 6610,
  "numCommitsSeen": 873,
  "timeTaken": 43109,
  "changeHistory": [
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "22be604fa3e4054a8f329296be7334c1531b874f",
    "0424056a77002f4a2334ee2eb240fbc67b676471",
    "6374ee0db445e0a1c3462c19ddee345df740cfb3",
    "ed70fb1608e6c81314da83daddadd756394fb87e"
  ],
  "changeHistoryShort": {
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "22be604fa3e4054a8f329296be7334c1531b874f": "Ybodychange",
    "0424056a77002f4a2334ee2eb240fbc67b676471": "Ybodychange",
    "6374ee0db445e0a1c3462c19ddee345df740cfb3": "Ybodychange",
    "ed70fb1608e6c81314da83daddadd756394fb87e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "14/10/16 1:21 PM",
      "commitNameOld": "adb96e109f1ab4a2c3d469e716c084d0a891b951",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 3.18,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n   public String getNodeUsage() {\n     float median \u003d 0;\n     float max \u003d 0;\n     float min \u003d 0;\n     float dev \u003d 0;\n \n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d\n         new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (Iterator\u003cDatanodeDescriptor\u003e it \u003d live.iterator(); it.hasNext();) {\n       DatanodeDescriptor node \u003d it.next();\n-      if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n+      if (!node.isInService()) {\n         it.remove();\n       }\n     }\n \n     if (live.size() \u003e 0) {\n       float totalDfsUsed \u003d 0;\n       float[] usages \u003d new float[live.size()];\n       int i \u003d 0;\n       for (DatanodeDescriptor dn : live) {\n         usages[i++] \u003d dn.getDfsUsedPercent();\n         totalDfsUsed +\u003d dn.getDfsUsedPercent();\n       }\n       totalDfsUsed /\u003d live.size();\n       Arrays.sort(usages);\n       median \u003d usages[usages.length / 2];\n       max \u003d usages[usages.length - 1];\n       min \u003d usages[0];\n \n       for (i \u003d 0; i \u003c usages.length; i++) {\n         dev +\u003d (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);\n       }\n       dev \u003d (float) Math.sqrt(dev / usages.length);\n     }\n \n     final Map\u003cString, Object\u003e innerInfo \u003d new HashMap\u003cString, Object\u003e();\n     innerInfo.put(\"min\", StringUtils.format(\"%.2f%%\", min));\n     innerInfo.put(\"median\", StringUtils.format(\"%.2f%%\", median));\n     innerInfo.put(\"max\", StringUtils.format(\"%.2f%%\", max));\n     innerInfo.put(\"stdDev\", StringUtils.format(\"%.2f%%\", dev));\n     info.put(\"nodeUsage\", innerInfo);\n \n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getNodeUsage() {\n    float median \u003d 0;\n    float max \u003d 0;\n    float min \u003d 0;\n    float dev \u003d 0;\n\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d\n        new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (Iterator\u003cDatanodeDescriptor\u003e it \u003d live.iterator(); it.hasNext();) {\n      DatanodeDescriptor node \u003d it.next();\n      if (!node.isInService()) {\n        it.remove();\n      }\n    }\n\n    if (live.size() \u003e 0) {\n      float totalDfsUsed \u003d 0;\n      float[] usages \u003d new float[live.size()];\n      int i \u003d 0;\n      for (DatanodeDescriptor dn : live) {\n        usages[i++] \u003d dn.getDfsUsedPercent();\n        totalDfsUsed +\u003d dn.getDfsUsedPercent();\n      }\n      totalDfsUsed /\u003d live.size();\n      Arrays.sort(usages);\n      median \u003d usages[usages.length / 2];\n      max \u003d usages[usages.length - 1];\n      min \u003d usages[0];\n\n      for (i \u003d 0; i \u003c usages.length; i++) {\n        dev +\u003d (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);\n      }\n      dev \u003d (float) Math.sqrt(dev / usages.length);\n    }\n\n    final Map\u003cString, Object\u003e innerInfo \u003d new HashMap\u003cString, Object\u003e();\n    innerInfo.put(\"min\", StringUtils.format(\"%.2f%%\", min));\n    innerInfo.put(\"median\", StringUtils.format(\"%.2f%%\", median));\n    innerInfo.put(\"max\", StringUtils.format(\"%.2f%%\", max));\n    innerInfo.put(\"stdDev\", StringUtils.format(\"%.2f%%\", dev));\n    info.put(\"nodeUsage\", innerInfo);\n\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "22be604fa3e4054a8f329296be7334c1531b874f": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-10534. NameNode WebUI should display DataNode usage rate with a certain percentile. Contributed by Kai Sasaki.\"\n\nThis reverts commit 0424056a77002f4a2334ee2eb240fbc67b676471.\n",
      "commitDate": "24/06/16 9:36 AM",
      "commitName": "22be604fa3e4054a8f329296be7334c1531b874f",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "24/06/16 8:58 AM",
      "commitNameOld": "0424056a77002f4a2334ee2eb240fbc67b676471",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,46 @@\n   public String getNodeUsage() {\n     float median \u003d 0;\n     float max \u003d 0;\n     float min \u003d 0;\n     float dev \u003d 0;\n-    float percentile \u003d 0;\n \n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d\n         new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (Iterator\u003cDatanodeDescriptor\u003e it \u003d live.iterator(); it.hasNext();) {\n       DatanodeDescriptor node \u003d it.next();\n       if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         it.remove();\n       }\n     }\n \n     if (live.size() \u003e 0) {\n       float totalDfsUsed \u003d 0;\n       float[] usages \u003d new float[live.size()];\n       int i \u003d 0;\n       for (DatanodeDescriptor dn : live) {\n         usages[i++] \u003d dn.getDfsUsedPercent();\n         totalDfsUsed +\u003d dn.getDfsUsedPercent();\n       }\n       totalDfsUsed /\u003d live.size();\n       Arrays.sort(usages);\n       median \u003d usages[usages.length / 2];\n       max \u003d usages[usages.length - 1];\n       min \u003d usages[0];\n-      percentile \u003d usages[(int)((usages.length - 1) * percentileFactor)];\n \n       for (i \u003d 0; i \u003c usages.length; i++) {\n         dev +\u003d (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);\n       }\n       dev \u003d (float) Math.sqrt(dev / usages.length);\n     }\n \n     final Map\u003cString, Object\u003e innerInfo \u003d new HashMap\u003cString, Object\u003e();\n     innerInfo.put(\"min\", StringUtils.format(\"%.2f%%\", min));\n     innerInfo.put(\"median\", StringUtils.format(\"%.2f%%\", median));\n     innerInfo.put(\"max\", StringUtils.format(\"%.2f%%\", max));\n     innerInfo.put(\"stdDev\", StringUtils.format(\"%.2f%%\", dev));\n-    final Map\u003cString, Object\u003e percentileInfo \u003d new HashMap\u003cString, Object\u003e();\n-    percentileInfo.put(\"name\", StringUtils.format(\"%dth percentile\",\n-        (int)(percentileFactor * 100)));\n-    percentileInfo.put(\"value\", StringUtils.format(\"%.2f%%\", percentile));\n-    innerInfo.put(\"percentile\", percentileInfo);\n     info.put(\"nodeUsage\", innerInfo);\n \n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getNodeUsage() {\n    float median \u003d 0;\n    float max \u003d 0;\n    float min \u003d 0;\n    float dev \u003d 0;\n\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d\n        new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (Iterator\u003cDatanodeDescriptor\u003e it \u003d live.iterator(); it.hasNext();) {\n      DatanodeDescriptor node \u003d it.next();\n      if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        it.remove();\n      }\n    }\n\n    if (live.size() \u003e 0) {\n      float totalDfsUsed \u003d 0;\n      float[] usages \u003d new float[live.size()];\n      int i \u003d 0;\n      for (DatanodeDescriptor dn : live) {\n        usages[i++] \u003d dn.getDfsUsedPercent();\n        totalDfsUsed +\u003d dn.getDfsUsedPercent();\n      }\n      totalDfsUsed /\u003d live.size();\n      Arrays.sort(usages);\n      median \u003d usages[usages.length / 2];\n      max \u003d usages[usages.length - 1];\n      min \u003d usages[0];\n\n      for (i \u003d 0; i \u003c usages.length; i++) {\n        dev +\u003d (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);\n      }\n      dev \u003d (float) Math.sqrt(dev / usages.length);\n    }\n\n    final Map\u003cString, Object\u003e innerInfo \u003d new HashMap\u003cString, Object\u003e();\n    innerInfo.put(\"min\", StringUtils.format(\"%.2f%%\", min));\n    innerInfo.put(\"median\", StringUtils.format(\"%.2f%%\", median));\n    innerInfo.put(\"max\", StringUtils.format(\"%.2f%%\", max));\n    innerInfo.put(\"stdDev\", StringUtils.format(\"%.2f%%\", dev));\n    info.put(\"nodeUsage\", innerInfo);\n\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "0424056a77002f4a2334ee2eb240fbc67b676471": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10534. NameNode WebUI should display DataNode usage rate with a certain percentile. Contributed by Kai Sasaki.\n",
      "commitDate": "24/06/16 8:58 AM",
      "commitName": "0424056a77002f4a2334ee2eb240fbc67b676471",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "08/06/16 1:44 PM",
      "commitNameOld": "ae047655f4355288406cd5396fb4e3ea7c307b14",
      "commitAuthorOld": "Ravi Prakash",
      "daysBetweenCommits": 15.8,
      "commitsBetweenForRepo": 139,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,53 @@\n   public String getNodeUsage() {\n     float median \u003d 0;\n     float max \u003d 0;\n     float min \u003d 0;\n     float dev \u003d 0;\n+    float percentile \u003d 0;\n \n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d\n         new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (Iterator\u003cDatanodeDescriptor\u003e it \u003d live.iterator(); it.hasNext();) {\n       DatanodeDescriptor node \u003d it.next();\n       if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         it.remove();\n       }\n     }\n \n     if (live.size() \u003e 0) {\n       float totalDfsUsed \u003d 0;\n       float[] usages \u003d new float[live.size()];\n       int i \u003d 0;\n       for (DatanodeDescriptor dn : live) {\n         usages[i++] \u003d dn.getDfsUsedPercent();\n         totalDfsUsed +\u003d dn.getDfsUsedPercent();\n       }\n       totalDfsUsed /\u003d live.size();\n       Arrays.sort(usages);\n       median \u003d usages[usages.length / 2];\n       max \u003d usages[usages.length - 1];\n       min \u003d usages[0];\n+      percentile \u003d usages[(int)((usages.length - 1) * percentileFactor)];\n \n       for (i \u003d 0; i \u003c usages.length; i++) {\n         dev +\u003d (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);\n       }\n       dev \u003d (float) Math.sqrt(dev / usages.length);\n     }\n \n     final Map\u003cString, Object\u003e innerInfo \u003d new HashMap\u003cString, Object\u003e();\n     innerInfo.put(\"min\", StringUtils.format(\"%.2f%%\", min));\n     innerInfo.put(\"median\", StringUtils.format(\"%.2f%%\", median));\n     innerInfo.put(\"max\", StringUtils.format(\"%.2f%%\", max));\n     innerInfo.put(\"stdDev\", StringUtils.format(\"%.2f%%\", dev));\n+    final Map\u003cString, Object\u003e percentileInfo \u003d new HashMap\u003cString, Object\u003e();\n+    percentileInfo.put(\"name\", StringUtils.format(\"%dth percentile\",\n+        (int)(percentileFactor * 100)));\n+    percentileInfo.put(\"value\", StringUtils.format(\"%.2f%%\", percentile));\n+    innerInfo.put(\"percentile\", percentileInfo);\n     info.put(\"nodeUsage\", innerInfo);\n \n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getNodeUsage() {\n    float median \u003d 0;\n    float max \u003d 0;\n    float min \u003d 0;\n    float dev \u003d 0;\n    float percentile \u003d 0;\n\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d\n        new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (Iterator\u003cDatanodeDescriptor\u003e it \u003d live.iterator(); it.hasNext();) {\n      DatanodeDescriptor node \u003d it.next();\n      if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        it.remove();\n      }\n    }\n\n    if (live.size() \u003e 0) {\n      float totalDfsUsed \u003d 0;\n      float[] usages \u003d new float[live.size()];\n      int i \u003d 0;\n      for (DatanodeDescriptor dn : live) {\n        usages[i++] \u003d dn.getDfsUsedPercent();\n        totalDfsUsed +\u003d dn.getDfsUsedPercent();\n      }\n      totalDfsUsed /\u003d live.size();\n      Arrays.sort(usages);\n      median \u003d usages[usages.length / 2];\n      max \u003d usages[usages.length - 1];\n      min \u003d usages[0];\n      percentile \u003d usages[(int)((usages.length - 1) * percentileFactor)];\n\n      for (i \u003d 0; i \u003c usages.length; i++) {\n        dev +\u003d (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);\n      }\n      dev \u003d (float) Math.sqrt(dev / usages.length);\n    }\n\n    final Map\u003cString, Object\u003e innerInfo \u003d new HashMap\u003cString, Object\u003e();\n    innerInfo.put(\"min\", StringUtils.format(\"%.2f%%\", min));\n    innerInfo.put(\"median\", StringUtils.format(\"%.2f%%\", median));\n    innerInfo.put(\"max\", StringUtils.format(\"%.2f%%\", max));\n    innerInfo.put(\"stdDev\", StringUtils.format(\"%.2f%%\", dev));\n    final Map\u003cString, Object\u003e percentileInfo \u003d new HashMap\u003cString, Object\u003e();\n    percentileInfo.put(\"name\", StringUtils.format(\"%dth percentile\",\n        (int)(percentileFactor * 100)));\n    percentileInfo.put(\"value\", StringUtils.format(\"%.2f%%\", percentile));\n    innerInfo.put(\"percentile\", percentileInfo);\n    info.put(\"nodeUsage\", innerInfo);\n\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "6374ee0db445e0a1c3462c19ddee345df740cfb3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8670. Better to exclude decommissioned nodes for namenode NodeUsage JMX (Contributed by J.Andreina)\n",
      "commitDate": "29/07/15 2:17 AM",
      "commitName": "6374ee0db445e0a1c3462c19ddee345df740cfb3",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "29/07/15 12:48 AM",
      "commitNameOld": "2a1d656196cf9750fa482cb10893684e8a2ce7c3",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,46 @@\n   public String getNodeUsage() {\n     float median \u003d 0;\n     float max \u003d 0;\n     float min \u003d 0;\n     float dev \u003d 0;\n \n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d\n         new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n+    for (Iterator\u003cDatanodeDescriptor\u003e it \u003d live.iterator(); it.hasNext();) {\n+      DatanodeDescriptor node \u003d it.next();\n+      if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n+        it.remove();\n+      }\n+    }\n \n     if (live.size() \u003e 0) {\n       float totalDfsUsed \u003d 0;\n       float[] usages \u003d new float[live.size()];\n       int i \u003d 0;\n       for (DatanodeDescriptor dn : live) {\n         usages[i++] \u003d dn.getDfsUsedPercent();\n         totalDfsUsed +\u003d dn.getDfsUsedPercent();\n       }\n       totalDfsUsed /\u003d live.size();\n       Arrays.sort(usages);\n       median \u003d usages[usages.length / 2];\n       max \u003d usages[usages.length - 1];\n       min \u003d usages[0];\n \n       for (i \u003d 0; i \u003c usages.length; i++) {\n         dev +\u003d (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);\n       }\n       dev \u003d (float) Math.sqrt(dev / usages.length);\n     }\n \n     final Map\u003cString, Object\u003e innerInfo \u003d new HashMap\u003cString, Object\u003e();\n     innerInfo.put(\"min\", StringUtils.format(\"%.2f%%\", min));\n     innerInfo.put(\"median\", StringUtils.format(\"%.2f%%\", median));\n     innerInfo.put(\"max\", StringUtils.format(\"%.2f%%\", max));\n     innerInfo.put(\"stdDev\", StringUtils.format(\"%.2f%%\", dev));\n     info.put(\"nodeUsage\", innerInfo);\n \n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getNodeUsage() {\n    float median \u003d 0;\n    float max \u003d 0;\n    float min \u003d 0;\n    float dev \u003d 0;\n\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d\n        new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (Iterator\u003cDatanodeDescriptor\u003e it \u003d live.iterator(); it.hasNext();) {\n      DatanodeDescriptor node \u003d it.next();\n      if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        it.remove();\n      }\n    }\n\n    if (live.size() \u003e 0) {\n      float totalDfsUsed \u003d 0;\n      float[] usages \u003d new float[live.size()];\n      int i \u003d 0;\n      for (DatanodeDescriptor dn : live) {\n        usages[i++] \u003d dn.getDfsUsedPercent();\n        totalDfsUsed +\u003d dn.getDfsUsedPercent();\n      }\n      totalDfsUsed /\u003d live.size();\n      Arrays.sort(usages);\n      median \u003d usages[usages.length / 2];\n      max \u003d usages[usages.length - 1];\n      min \u003d usages[0];\n\n      for (i \u003d 0; i \u003c usages.length; i++) {\n        dev +\u003d (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);\n      }\n      dev \u003d (float) Math.sqrt(dev / usages.length);\n    }\n\n    final Map\u003cString, Object\u003e innerInfo \u003d new HashMap\u003cString, Object\u003e();\n    innerInfo.put(\"min\", StringUtils.format(\"%.2f%%\", min));\n    innerInfo.put(\"median\", StringUtils.format(\"%.2f%%\", median));\n    innerInfo.put(\"max\", StringUtils.format(\"%.2f%%\", max));\n    innerInfo.put(\"stdDev\", StringUtils.format(\"%.2f%%\", dev));\n    info.put(\"nodeUsage\", innerInfo);\n\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ed70fb1608e6c81314da83daddadd756394fb87e": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4860. Add additional attributes to JMX beans. Contributed by Trevor Lorimer\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1500139 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/07/13 2:36 PM",
      "commitName": "ed70fb1608e6c81314da83daddadd756394fb87e",
      "commitAuthor": "Konstantin Boudnik",
      "diff": "@@ -0,0 +1,40 @@\n+  public String getNodeUsage() {\n+    float median \u003d 0;\n+    float max \u003d 0;\n+    float min \u003d 0;\n+    float dev \u003d 0;\n+\n+    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d\n+        new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n+    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n+\n+    if (live.size() \u003e 0) {\n+      float totalDfsUsed \u003d 0;\n+      float[] usages \u003d new float[live.size()];\n+      int i \u003d 0;\n+      for (DatanodeDescriptor dn : live) {\n+        usages[i++] \u003d dn.getDfsUsedPercent();\n+        totalDfsUsed +\u003d dn.getDfsUsedPercent();\n+      }\n+      totalDfsUsed /\u003d live.size();\n+      Arrays.sort(usages);\n+      median \u003d usages[usages.length / 2];\n+      max \u003d usages[usages.length - 1];\n+      min \u003d usages[0];\n+\n+      for (i \u003d 0; i \u003c usages.length; i++) {\n+        dev +\u003d (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);\n+      }\n+      dev \u003d (float) Math.sqrt(dev / usages.length);\n+    }\n+\n+    final Map\u003cString, Object\u003e innerInfo \u003d new HashMap\u003cString, Object\u003e();\n+    innerInfo.put(\"min\", StringUtils.format(\"%.2f%%\", min));\n+    innerInfo.put(\"median\", StringUtils.format(\"%.2f%%\", median));\n+    innerInfo.put(\"max\", StringUtils.format(\"%.2f%%\", max));\n+    innerInfo.put(\"stdDev\", StringUtils.format(\"%.2f%%\", dev));\n+    info.put(\"nodeUsage\", innerInfo);\n+\n+    return JSON.toString(info);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public String getNodeUsage() {\n    float median \u003d 0;\n    float max \u003d 0;\n    float min \u003d 0;\n    float dev \u003d 0;\n\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d\n        new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n\n    if (live.size() \u003e 0) {\n      float totalDfsUsed \u003d 0;\n      float[] usages \u003d new float[live.size()];\n      int i \u003d 0;\n      for (DatanodeDescriptor dn : live) {\n        usages[i++] \u003d dn.getDfsUsedPercent();\n        totalDfsUsed +\u003d dn.getDfsUsedPercent();\n      }\n      totalDfsUsed /\u003d live.size();\n      Arrays.sort(usages);\n      median \u003d usages[usages.length / 2];\n      max \u003d usages[usages.length - 1];\n      min \u003d usages[0];\n\n      for (i \u003d 0; i \u003c usages.length; i++) {\n        dev +\u003d (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);\n      }\n      dev \u003d (float) Math.sqrt(dev / usages.length);\n    }\n\n    final Map\u003cString, Object\u003e innerInfo \u003d new HashMap\u003cString, Object\u003e();\n    innerInfo.put(\"min\", StringUtils.format(\"%.2f%%\", min));\n    innerInfo.put(\"median\", StringUtils.format(\"%.2f%%\", median));\n    innerInfo.put(\"max\", StringUtils.format(\"%.2f%%\", max));\n    innerInfo.put(\"stdDev\", StringUtils.format(\"%.2f%%\", dev));\n    info.put(\"nodeUsage\", innerInfo);\n\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}