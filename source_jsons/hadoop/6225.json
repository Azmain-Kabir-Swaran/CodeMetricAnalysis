{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BackupImage.java",
  "functionName": "convergeJournalSpool",
  "functionId": "convergeJournalSpool",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
  "functionStartLine": 237,
  "functionEndLine": 245,
  "numCommitsSeen": 30,
  "timeTaken": 4665,
  "changeHistory": [
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void convergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n\n    while (!tryConvergeJournalSpool()) {\n      ;\n    }\n    assert bnState \u003d\u003d BNState.IN_SYNC;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void convergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n\n    while (!tryConvergeJournalSpool()) {\n      ;\n    }\n    assert bnState \u003d\u003d BNState.IN_SYNC;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java"
      }
    },
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/07/11 9:28 AM",
      "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "27/07/11 8:19 PM",
      "commitNameOld": "ffbe9e5972bf3eee9037e2602c1330e0dc744646",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.55,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,9 @@\n   void convergeJournalSpool() throws IOException {\n-    Iterator\u003cStorageDirectory\u003e itEdits\n-      \u003d storage.dirIterator(NameNodeDirType.EDITS);\n-    if(! itEdits.hasNext())\n-      throw new IOException(\"Could not locate checkpoint directories\");\n-    StorageDirectory sdEdits \u003d itEdits.next();\n-    int numEdits \u003d 0;\n-    File jSpoolFile \u003d getJSpoolFile(sdEdits);\n-    long startTime \u003d now();\n-    if(jSpoolFile.exists()) {\n-      // load edits.new\n-      EditLogFileInputStream edits \u003d new EditLogFileInputStream(jSpoolFile);\n-      BufferedInputStream bin \u003d new BufferedInputStream(edits);\n-      DataInputStream in \u003d new DataInputStream(bin);\n-      FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n-      int logVersion \u003d logLoader.readLogVersion(in);\n-      Checksum checksum \u003d null;\n-      if (LayoutVersion.supports(Feature.EDITS_CHESKUM, logVersion)) {\n-        checksum \u003d FSEditLog.getChecksum();\n-        in \u003d new DataInputStream(new CheckedInputStream(bin, checksum));\n-      }\n-      numEdits +\u003d logLoader.loadEditRecords(logVersion, in, checksum, false);\n+    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n+        \"bad state: %s\", bnState);\n \n-      // first time reached the end of spool\n-      jsState \u003d JSpoolState.WAIT;\n-      numEdits +\u003d logLoader.loadEditRecords(logVersion,\n-                                            in, checksum, true);\n-      getFSNamesystem().dir.updateCountForINodeWithQuota();\n-      edits.close();\n+    while (!tryConvergeJournalSpool()) {\n+      ;\n     }\n-\n-    FSImage.LOG.info(\"Edits file \" + jSpoolFile.getCanonicalPath()\n-        + \" of size \" + jSpoolFile.length() + \" edits # \" + numEdits\n-        + \" loaded in \" + (now()-startTime)/1000 + \" seconds.\");\n-\n-    // rename spool edits.new to edits making it in sync with the active node\n-    // subsequent journal records will go directly to edits\n-    editLog.revertFileStreams(STORAGE_JSPOOL_DIR + \"/\" + STORAGE_JSPOOL_FILE);\n-\n-    // write version file\n-    resetVersion(false, storage.getImageDigest());\n-\n-    // wake up journal writer\n-    synchronized(this) {\n-      jsState \u003d JSpoolState.OFF;\n-      notifyAll();\n-    }\n-\n-    // Rename lastcheckpoint.tmp to previous.checkpoint\n-    for (Iterator\u003cStorageDirectory\u003e it \u003d storage.dirIterator(); it.hasNext();) {\n-      StorageDirectory sd \u003d it.next();\n-      storage.moveLastCheckpoint(sd);\n-    }\n+    assert bnState \u003d\u003d BNState.IN_SYNC;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void convergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n\n    while (!tryConvergeJournalSpool()) {\n      ;\n    }\n    assert bnState \u003d\u003d BNState.IN_SYNC;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,54 @@\n+  void convergeJournalSpool() throws IOException {\n+    Iterator\u003cStorageDirectory\u003e itEdits\n+      \u003d storage.dirIterator(NameNodeDirType.EDITS);\n+    if(! itEdits.hasNext())\n+      throw new IOException(\"Could not locate checkpoint directories\");\n+    StorageDirectory sdEdits \u003d itEdits.next();\n+    int numEdits \u003d 0;\n+    File jSpoolFile \u003d getJSpoolFile(sdEdits);\n+    long startTime \u003d now();\n+    if(jSpoolFile.exists()) {\n+      // load edits.new\n+      EditLogFileInputStream edits \u003d new EditLogFileInputStream(jSpoolFile);\n+      BufferedInputStream bin \u003d new BufferedInputStream(edits);\n+      DataInputStream in \u003d new DataInputStream(bin);\n+      FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n+      int logVersion \u003d logLoader.readLogVersion(in);\n+      Checksum checksum \u003d null;\n+      if (LayoutVersion.supports(Feature.EDITS_CHESKUM, logVersion)) {\n+        checksum \u003d FSEditLog.getChecksum();\n+        in \u003d new DataInputStream(new CheckedInputStream(bin, checksum));\n+      }\n+      numEdits +\u003d logLoader.loadEditRecords(logVersion, in, checksum, false);\n+\n+      // first time reached the end of spool\n+      jsState \u003d JSpoolState.WAIT;\n+      numEdits +\u003d logLoader.loadEditRecords(logVersion,\n+                                            in, checksum, true);\n+      getFSNamesystem().dir.updateCountForINodeWithQuota();\n+      edits.close();\n+    }\n+\n+    FSImage.LOG.info(\"Edits file \" + jSpoolFile.getCanonicalPath()\n+        + \" of size \" + jSpoolFile.length() + \" edits # \" + numEdits\n+        + \" loaded in \" + (now()-startTime)/1000 + \" seconds.\");\n+\n+    // rename spool edits.new to edits making it in sync with the active node\n+    // subsequent journal records will go directly to edits\n+    editLog.revertFileStreams(STORAGE_JSPOOL_DIR + \"/\" + STORAGE_JSPOOL_FILE);\n+\n+    // write version file\n+    resetVersion(false, storage.getImageDigest());\n+\n+    // wake up journal writer\n+    synchronized(this) {\n+      jsState \u003d JSpoolState.OFF;\n+      notifyAll();\n+    }\n+\n+    // Rename lastcheckpoint.tmp to previous.checkpoint\n+    for (Iterator\u003cStorageDirectory\u003e it \u003d storage.dirIterator(); it.hasNext();) {\n+      StorageDirectory sd \u003d it.next();\n+      storage.moveLastCheckpoint(sd);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void convergeJournalSpool() throws IOException {\n    Iterator\u003cStorageDirectory\u003e itEdits\n      \u003d storage.dirIterator(NameNodeDirType.EDITS);\n    if(! itEdits.hasNext())\n      throw new IOException(\"Could not locate checkpoint directories\");\n    StorageDirectory sdEdits \u003d itEdits.next();\n    int numEdits \u003d 0;\n    File jSpoolFile \u003d getJSpoolFile(sdEdits);\n    long startTime \u003d now();\n    if(jSpoolFile.exists()) {\n      // load edits.new\n      EditLogFileInputStream edits \u003d new EditLogFileInputStream(jSpoolFile);\n      BufferedInputStream bin \u003d new BufferedInputStream(edits);\n      DataInputStream in \u003d new DataInputStream(bin);\n      FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n      int logVersion \u003d logLoader.readLogVersion(in);\n      Checksum checksum \u003d null;\n      if (LayoutVersion.supports(Feature.EDITS_CHESKUM, logVersion)) {\n        checksum \u003d FSEditLog.getChecksum();\n        in \u003d new DataInputStream(new CheckedInputStream(bin, checksum));\n      }\n      numEdits +\u003d logLoader.loadEditRecords(logVersion, in, checksum, false);\n\n      // first time reached the end of spool\n      jsState \u003d JSpoolState.WAIT;\n      numEdits +\u003d logLoader.loadEditRecords(logVersion,\n                                            in, checksum, true);\n      getFSNamesystem().dir.updateCountForINodeWithQuota();\n      edits.close();\n    }\n\n    FSImage.LOG.info(\"Edits file \" + jSpoolFile.getCanonicalPath()\n        + \" of size \" + jSpoolFile.length() + \" edits # \" + numEdits\n        + \" loaded in \" + (now()-startTime)/1000 + \" seconds.\");\n\n    // rename spool edits.new to edits making it in sync with the active node\n    // subsequent journal records will go directly to edits\n    editLog.revertFileStreams(STORAGE_JSPOOL_DIR + \"/\" + STORAGE_JSPOOL_FILE);\n\n    // write version file\n    resetVersion(false, storage.getImageDigest());\n\n    // wake up journal writer\n    synchronized(this) {\n      jsState \u003d JSpoolState.OFF;\n      notifyAll();\n    }\n\n    // Rename lastcheckpoint.tmp to previous.checkpoint\n    for (Iterator\u003cStorageDirectory\u003e it \u003d storage.dirIterator(); it.hasNext();) {\n      StorageDirectory sd \u003d it.next();\n      storage.moveLastCheckpoint(sd);\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java"
    }
  }
}