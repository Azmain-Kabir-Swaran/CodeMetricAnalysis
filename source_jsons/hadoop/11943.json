{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "getInitialVolumeFailureInfos",
  "functionId": "getInitialVolumeFailureInfos___dataLocations-Collection__StorageLocation____storage-DataStorage",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 415,
  "functionEndLine": 435,
  "numCommitsSeen": 197,
  "timeTaken": 6133,
  "changeHistory": [
    "04a024b683cdaac8723fee5d606bc6da939e2fb9",
    "96b12662ea76e3ded4ef13944fc8df206cfb4613",
    "9729b244de50322c2cc889c97c2ffb2b4675cf77"
  ],
  "changeHistoryShort": {
    "04a024b683cdaac8723fee5d606bc6da939e2fb9": "Ybodychange",
    "96b12662ea76e3ded4ef13944fc8df206cfb4613": "Ybodychange",
    "9729b244de50322c2cc889c97c2ffb2b4675cf77": "Yintroduced"
  },
  "changeHistoryDetails": {
    "04a024b683cdaac8723fee5d606bc6da939e2fb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11147. Remove confusing log output in FsDatasetImpl#getInitialVolumeFailureInfos. Contributed by Chen Liang.\n",
      "commitDate": "16/11/16 2:47 PM",
      "commitName": "04a024b683cdaac8723fee5d606bc6da939e2fb9",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "31/10/16 12:39 PM",
      "commitNameOld": "a9d68d2e8ed8c9efc96ec7c5057a3435560311fd",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 16.13,
      "commitsBetweenForRepo": 157,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,21 @@\n   private static List\u003cVolumeFailureInfo\u003e getInitialVolumeFailureInfos(\n       Collection\u003cStorageLocation\u003e dataLocations, DataStorage storage) {\n     Set\u003cStorageLocation\u003e failedLocationSet \u003d Sets.newHashSetWithExpectedSize(\n         dataLocations.size());\n     for (StorageLocation sl: dataLocations) {\n-      LOG.info(\"Adding to failedLocationSet \" + sl);\n       failedLocationSet.add(sl);\n     }\n     for (Iterator\u003cStorage.StorageDirectory\u003e it \u003d storage.dirIterator();\n          it.hasNext(); ) {\n       Storage.StorageDirectory sd \u003d it.next();\n       failedLocationSet.remove(sd.getStorageLocation());\n-      LOG.info(\"Removing from failedLocationSet \" + sd.getStorageLocation());\n     }\n     List\u003cVolumeFailureInfo\u003e volumeFailureInfos \u003d Lists.newArrayListWithCapacity(\n         failedLocationSet.size());\n     long failureDate \u003d Time.now();\n     for (StorageLocation failedStorageLocation: failedLocationSet) {\n       volumeFailureInfos.add(new VolumeFailureInfo(failedStorageLocation,\n           failureDate));\n     }\n     return volumeFailureInfos;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static List\u003cVolumeFailureInfo\u003e getInitialVolumeFailureInfos(\n      Collection\u003cStorageLocation\u003e dataLocations, DataStorage storage) {\n    Set\u003cStorageLocation\u003e failedLocationSet \u003d Sets.newHashSetWithExpectedSize(\n        dataLocations.size());\n    for (StorageLocation sl: dataLocations) {\n      failedLocationSet.add(sl);\n    }\n    for (Iterator\u003cStorage.StorageDirectory\u003e it \u003d storage.dirIterator();\n         it.hasNext(); ) {\n      Storage.StorageDirectory sd \u003d it.next();\n      failedLocationSet.remove(sd.getStorageLocation());\n    }\n    List\u003cVolumeFailureInfo\u003e volumeFailureInfos \u003d Lists.newArrayListWithCapacity(\n        failedLocationSet.size());\n    long failureDate \u003d Time.now();\n    for (StorageLocation failedStorageLocation: failedLocationSet) {\n      volumeFailureInfos.add(new VolumeFailureInfo(failedStorageLocation,\n          failureDate));\n    }\n    return volumeFailureInfos;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "96b12662ea76e3ded4ef13944fc8df206cfb4613": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10637. Modifications to remove the assumption that FsVolumes are backed by java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "10/10/16 3:30 PM",
      "commitName": "96b12662ea76e3ded4ef13944fc8df206cfb4613",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "30/09/16 11:11 PM",
      "commitNameOld": "fe9ebe20ab113567f0777c11cb48ce0d3ce587a8",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 9.68,
      "commitsBetweenForRepo": 64,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,23 @@\n   private static List\u003cVolumeFailureInfo\u003e getInitialVolumeFailureInfos(\n       Collection\u003cStorageLocation\u003e dataLocations, DataStorage storage) {\n-    Set\u003cString\u003e failedLocationSet \u003d Sets.newHashSetWithExpectedSize(\n+    Set\u003cStorageLocation\u003e failedLocationSet \u003d Sets.newHashSetWithExpectedSize(\n         dataLocations.size());\n     for (StorageLocation sl: dataLocations) {\n-      failedLocationSet.add(sl.getFile().getAbsolutePath());\n+      LOG.info(\"Adding to failedLocationSet \" + sl);\n+      failedLocationSet.add(sl);\n     }\n     for (Iterator\u003cStorage.StorageDirectory\u003e it \u003d storage.dirIterator();\n          it.hasNext(); ) {\n       Storage.StorageDirectory sd \u003d it.next();\n-      failedLocationSet.remove(sd.getRoot().getAbsolutePath());\n+      failedLocationSet.remove(sd.getStorageLocation());\n+      LOG.info(\"Removing from failedLocationSet \" + sd.getStorageLocation());\n     }\n     List\u003cVolumeFailureInfo\u003e volumeFailureInfos \u003d Lists.newArrayListWithCapacity(\n         failedLocationSet.size());\n     long failureDate \u003d Time.now();\n-    for (String failedStorageLocation: failedLocationSet) {\n+    for (StorageLocation failedStorageLocation: failedLocationSet) {\n       volumeFailureInfos.add(new VolumeFailureInfo(failedStorageLocation,\n           failureDate));\n     }\n     return volumeFailureInfos;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static List\u003cVolumeFailureInfo\u003e getInitialVolumeFailureInfos(\n      Collection\u003cStorageLocation\u003e dataLocations, DataStorage storage) {\n    Set\u003cStorageLocation\u003e failedLocationSet \u003d Sets.newHashSetWithExpectedSize(\n        dataLocations.size());\n    for (StorageLocation sl: dataLocations) {\n      LOG.info(\"Adding to failedLocationSet \" + sl);\n      failedLocationSet.add(sl);\n    }\n    for (Iterator\u003cStorage.StorageDirectory\u003e it \u003d storage.dirIterator();\n         it.hasNext(); ) {\n      Storage.StorageDirectory sd \u003d it.next();\n      failedLocationSet.remove(sd.getStorageLocation());\n      LOG.info(\"Removing from failedLocationSet \" + sd.getStorageLocation());\n    }\n    List\u003cVolumeFailureInfo\u003e volumeFailureInfos \u003d Lists.newArrayListWithCapacity(\n        failedLocationSet.size());\n    long failureDate \u003d Time.now();\n    for (StorageLocation failedStorageLocation: failedLocationSet) {\n      volumeFailureInfos.add(new VolumeFailureInfo(failedStorageLocation,\n          failureDate));\n    }\n    return volumeFailureInfos;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "9729b244de50322c2cc889c97c2ffb2b4675cf77": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7604. Track and display failed DataNode storage locations in NameNode. Contributed by Chris Nauroth.\n",
      "commitDate": "16/02/15 2:43 PM",
      "commitName": "9729b244de50322c2cc889c97c2ffb2b4675cf77",
      "commitAuthor": "cnauroth",
      "diff": "@@ -0,0 +1,21 @@\n+  private static List\u003cVolumeFailureInfo\u003e getInitialVolumeFailureInfos(\n+      Collection\u003cStorageLocation\u003e dataLocations, DataStorage storage) {\n+    Set\u003cString\u003e failedLocationSet \u003d Sets.newHashSetWithExpectedSize(\n+        dataLocations.size());\n+    for (StorageLocation sl: dataLocations) {\n+      failedLocationSet.add(sl.getFile().getAbsolutePath());\n+    }\n+    for (Iterator\u003cStorage.StorageDirectory\u003e it \u003d storage.dirIterator();\n+         it.hasNext(); ) {\n+      Storage.StorageDirectory sd \u003d it.next();\n+      failedLocationSet.remove(sd.getRoot().getAbsolutePath());\n+    }\n+    List\u003cVolumeFailureInfo\u003e volumeFailureInfos \u003d Lists.newArrayListWithCapacity(\n+        failedLocationSet.size());\n+    long failureDate \u003d Time.now();\n+    for (String failedStorageLocation: failedLocationSet) {\n+      volumeFailureInfos.add(new VolumeFailureInfo(failedStorageLocation,\n+          failureDate));\n+    }\n+    return volumeFailureInfos;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static List\u003cVolumeFailureInfo\u003e getInitialVolumeFailureInfos(\n      Collection\u003cStorageLocation\u003e dataLocations, DataStorage storage) {\n    Set\u003cString\u003e failedLocationSet \u003d Sets.newHashSetWithExpectedSize(\n        dataLocations.size());\n    for (StorageLocation sl: dataLocations) {\n      failedLocationSet.add(sl.getFile().getAbsolutePath());\n    }\n    for (Iterator\u003cStorage.StorageDirectory\u003e it \u003d storage.dirIterator();\n         it.hasNext(); ) {\n      Storage.StorageDirectory sd \u003d it.next();\n      failedLocationSet.remove(sd.getRoot().getAbsolutePath());\n    }\n    List\u003cVolumeFailureInfo\u003e volumeFailureInfos \u003d Lists.newArrayListWithCapacity(\n        failedLocationSet.size());\n    long failureDate \u003d Time.now();\n    for (String failedStorageLocation: failedLocationSet) {\n      volumeFailureInfos.add(new VolumeFailureInfo(failedStorageLocation,\n          failureDate));\n    }\n    return volumeFailureInfos;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java"
    }
  }
}