{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSOutputStream.java",
  "functionName": "writeChunkPrepare",
  "functionId": "writeChunkPrepare___buflen-int__ckoff-int__cklen-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
  "functionStartLine": 464,
  "functionEndLine": 487,
  "numCommitsSeen": 29,
  "timeTaken": 1164,
  "changeHistory": [
    "b5af9be72c72734d668f817c99d889031922a951"
  ],
  "changeHistoryShort": {
    "b5af9be72c72734d668f817c99d889031922a951": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b5af9be72c72734d668f817c99d889031922a951": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8668. Erasure Coding: revisit buffer used for encoding and decoding. Contributed by Sammi Chen\n",
      "commitDate": "12/08/16 10:52 PM",
      "commitName": "b5af9be72c72734d668f817c99d889031922a951",
      "commitAuthor": "Kai Zheng",
      "diff": "@@ -0,0 +1,24 @@\n+  private synchronized void writeChunkPrepare(int buflen,\n+      int ckoff, int cklen) throws IOException {\n+    dfsClient.checkOpen();\n+    checkClosed();\n+\n+    if (buflen \u003e bytesPerChecksum) {\n+      throw new IOException(\"writeChunk() buffer size is \" + buflen +\n+                            \" is larger than supported  bytesPerChecksum \" +\n+                            bytesPerChecksum);\n+    }\n+    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n+      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n+                            getChecksumSize() + \" but found to be \" + cklen);\n+    }\n+\n+    if (currentPacket \u003d\u003d null) {\n+      currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n+          .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n+      DFSClient.LOG.debug(\"WriteChunk allocating new packet seqno\u003d{},\"\n+              + \" src\u003d{}, packetSize\u003d{}, chunksPerPacket\u003d{}, bytesCurBlock\u003d{}\",\n+          currentPacket.getSeqno(), src, packetSize, chunksPerPacket,\n+          getStreamer().getBytesCurBlock() + \", \" + this);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void writeChunkPrepare(int buflen,\n      int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (buflen \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + buflen +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n          .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n      DFSClient.LOG.debug(\"WriteChunk allocating new packet seqno\u003d{},\"\n              + \" src\u003d{}, packetSize\u003d{}, chunksPerPacket\u003d{}, bytesCurBlock\u003d{}\",\n          currentPacket.getSeqno(), src, packetSize, chunksPerPacket,\n          getStreamer().getBytesCurBlock() + \", \" + this);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}