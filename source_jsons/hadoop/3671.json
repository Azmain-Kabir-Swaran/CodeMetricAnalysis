{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSPacket.java",
  "functionName": "writeTo",
  "functionId": "writeTo___stm-DataOutputStream",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSPacket.java",
  "functionStartLine": 158,
  "functionEndLine": 202,
  "numCommitsSeen": 99,
  "timeTaken": 4206,
  "changeHistory": [
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "952640fa4cbdc23fe8781e5627c2e8eab565c535",
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb",
    "98a692fd6361365db4afb9523a5d83ee32774112",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50"
  ],
  "changeHistoryShort": {
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "952640fa4cbdc23fe8781e5627c2e8eab565c535": "Ymultichange(Ymovefromfile,Ybodychange)",
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb": "Ymultichange(Ymodifierchange,Ybodychange)",
    "98a692fd6361365db4afb9523a5d83ee32774112": "Ybodychange",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": "Yintroduced"
  },
  "changeHistoryDetails": {
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  synchronized void writeTo(DataOutputStream stm) throws IOException {\n    checkBuffer();\n\n    final int dataLen \u003d dataPos - dataStart;\n    final int checksumLen \u003d checksumPos - checksumStart;\n    final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n\n    PacketHeader header \u003d new PacketHeader(\n        pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n\n    if (checksumPos !\u003d dataStart) {\n      // Move the checksum to cover the gap. This can happen for the last\n      // packet or during an hflush/hsync call.\n      System.arraycopy(buf, checksumStart, buf,\n          dataStart - checksumLen , checksumLen);\n      checksumPos \u003d dataStart;\n      checksumStart \u003d checksumPos - checksumLen;\n    }\n\n    final int headerStart \u003d checksumStart - header.getSerializedSize();\n    assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n    assert headerStart \u003e\u003d 0;\n    assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n\n    // Copy the header data into the buffer immediately preceding the checksum\n    // data.\n    System.arraycopy(header.getBytes(), 0, buf, headerStart,\n        header.getSerializedSize());\n\n    // corrupt the data for testing.\n    if (DFSClientFaultInjector.get().corruptPacket()) {\n      buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n    }\n\n    // Write the now contiguous full packet to the output stream.\n    stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n\n    // undo corruption.\n    if (DFSClientFaultInjector.get().uncorruptPacket()) {\n      buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSPacket.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSPacket.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSPacket.java"
      }
    },
    "952640fa4cbdc23fe8781e5627c2e8eab565c535": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-7855. Separate class Packet from DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "05/03/15 10:58 AM",
      "commitName": "952640fa4cbdc23fe8781e5627c2e8eab565c535",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7855. Separate class Packet from DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "05/03/15 10:58 AM",
          "commitName": "952640fa4cbdc23fe8781e5627c2e8eab565c535",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "05/03/15 10:54 AM",
          "commitNameOld": "138c9cadee32da4d17be9835461bde646825d8d5",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,42 @@\n-    synchronized void writeTo(DataOutputStream stm) throws IOException {\n-      checkBuffer();\n+  synchronized void writeTo(DataOutputStream stm) throws IOException {\n+    checkBuffer();\n \n-      final int dataLen \u003d dataPos - dataStart;\n-      final int checksumLen \u003d checksumPos - checksumStart;\n-      final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n+    final int dataLen \u003d dataPos - dataStart;\n+    final int checksumLen \u003d checksumPos - checksumStart;\n+    final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n \n-      PacketHeader header \u003d new PacketHeader(\n+    PacketHeader header \u003d new PacketHeader(\n         pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n-      \n-      if (checksumPos !\u003d dataStart) {\n-        // Move the checksum to cover the gap. This can happen for the last\n-        // packet or during an hflush/hsync call.\n-        System.arraycopy(buf, checksumStart, buf, \n-                         dataStart - checksumLen , checksumLen); \n-        checksumPos \u003d dataStart;\n-        checksumStart \u003d checksumPos - checksumLen;\n-      }\n-      \n-      final int headerStart \u003d checksumStart - header.getSerializedSize();\n-      assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n-      assert checksumPos \u003d\u003d dataStart;\n-      assert headerStart \u003e\u003d 0;\n-      assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n-      \n-      // Copy the header data into the buffer immediately preceding the checksum\n-      // data.\n-      System.arraycopy(header.getBytes(), 0, buf, headerStart,\n-          header.getSerializedSize());\n-      \n-      // corrupt the data for testing.\n-      if (DFSClientFaultInjector.get().corruptPacket()) {\n-        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n-      }\n \n-      // Write the now contiguous full packet to the output stream.\n-      stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n+    if (checksumPos !\u003d dataStart) {\n+      // Move the checksum to cover the gap. This can happen for the last\n+      // packet or during an hflush/hsync call.\n+      System.arraycopy(buf, checksumStart, buf,\n+          dataStart - checksumLen , checksumLen);\n+      checksumPos \u003d dataStart;\n+      checksumStart \u003d checksumPos - checksumLen;\n+    }\n \n-      // undo corruption.\n-      if (DFSClientFaultInjector.get().uncorruptPacket()) {\n-        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n-      }\n-    }\n\\ No newline at end of file\n+    final int headerStart \u003d checksumStart - header.getSerializedSize();\n+    assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n+    assert headerStart \u003e\u003d 0;\n+    assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n+\n+    // Copy the header data into the buffer immediately preceding the checksum\n+    // data.\n+    System.arraycopy(header.getBytes(), 0, buf, headerStart,\n+        header.getSerializedSize());\n+\n+    // corrupt the data for testing.\n+    if (DFSClientFaultInjector.get().corruptPacket()) {\n+      buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n+    }\n+\n+    // Write the now contiguous full packet to the output stream.\n+    stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n+\n+    // undo corruption.\n+    if (DFSClientFaultInjector.get().uncorruptPacket()) {\n+      buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized void writeTo(DataOutputStream stm) throws IOException {\n    checkBuffer();\n\n    final int dataLen \u003d dataPos - dataStart;\n    final int checksumLen \u003d checksumPos - checksumStart;\n    final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n\n    PacketHeader header \u003d new PacketHeader(\n        pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n\n    if (checksumPos !\u003d dataStart) {\n      // Move the checksum to cover the gap. This can happen for the last\n      // packet or during an hflush/hsync call.\n      System.arraycopy(buf, checksumStart, buf,\n          dataStart - checksumLen , checksumLen);\n      checksumPos \u003d dataStart;\n      checksumStart \u003d checksumPos - checksumLen;\n    }\n\n    final int headerStart \u003d checksumStart - header.getSerializedSize();\n    assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n    assert headerStart \u003e\u003d 0;\n    assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n\n    // Copy the header data into the buffer immediately preceding the checksum\n    // data.\n    System.arraycopy(header.getBytes(), 0, buf, headerStart,\n        header.getSerializedSize());\n\n    // corrupt the data for testing.\n    if (DFSClientFaultInjector.get().corruptPacket()) {\n      buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n    }\n\n    // Write the now contiguous full packet to the output stream.\n    stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n\n    // undo corruption.\n    if (DFSClientFaultInjector.get().uncorruptPacket()) {\n      buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSPacket.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSPacket.java",
            "oldMethodName": "writeTo",
            "newMethodName": "writeTo"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7855. Separate class Packet from DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "05/03/15 10:58 AM",
          "commitName": "952640fa4cbdc23fe8781e5627c2e8eab565c535",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "05/03/15 10:54 AM",
          "commitNameOld": "138c9cadee32da4d17be9835461bde646825d8d5",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,42 @@\n-    synchronized void writeTo(DataOutputStream stm) throws IOException {\n-      checkBuffer();\n+  synchronized void writeTo(DataOutputStream stm) throws IOException {\n+    checkBuffer();\n \n-      final int dataLen \u003d dataPos - dataStart;\n-      final int checksumLen \u003d checksumPos - checksumStart;\n-      final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n+    final int dataLen \u003d dataPos - dataStart;\n+    final int checksumLen \u003d checksumPos - checksumStart;\n+    final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n \n-      PacketHeader header \u003d new PacketHeader(\n+    PacketHeader header \u003d new PacketHeader(\n         pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n-      \n-      if (checksumPos !\u003d dataStart) {\n-        // Move the checksum to cover the gap. This can happen for the last\n-        // packet or during an hflush/hsync call.\n-        System.arraycopy(buf, checksumStart, buf, \n-                         dataStart - checksumLen , checksumLen); \n-        checksumPos \u003d dataStart;\n-        checksumStart \u003d checksumPos - checksumLen;\n-      }\n-      \n-      final int headerStart \u003d checksumStart - header.getSerializedSize();\n-      assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n-      assert checksumPos \u003d\u003d dataStart;\n-      assert headerStart \u003e\u003d 0;\n-      assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n-      \n-      // Copy the header data into the buffer immediately preceding the checksum\n-      // data.\n-      System.arraycopy(header.getBytes(), 0, buf, headerStart,\n-          header.getSerializedSize());\n-      \n-      // corrupt the data for testing.\n-      if (DFSClientFaultInjector.get().corruptPacket()) {\n-        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n-      }\n \n-      // Write the now contiguous full packet to the output stream.\n-      stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n+    if (checksumPos !\u003d dataStart) {\n+      // Move the checksum to cover the gap. This can happen for the last\n+      // packet or during an hflush/hsync call.\n+      System.arraycopy(buf, checksumStart, buf,\n+          dataStart - checksumLen , checksumLen);\n+      checksumPos \u003d dataStart;\n+      checksumStart \u003d checksumPos - checksumLen;\n+    }\n \n-      // undo corruption.\n-      if (DFSClientFaultInjector.get().uncorruptPacket()) {\n-        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n-      }\n-    }\n\\ No newline at end of file\n+    final int headerStart \u003d checksumStart - header.getSerializedSize();\n+    assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n+    assert headerStart \u003e\u003d 0;\n+    assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n+\n+    // Copy the header data into the buffer immediately preceding the checksum\n+    // data.\n+    System.arraycopy(header.getBytes(), 0, buf, headerStart,\n+        header.getSerializedSize());\n+\n+    // corrupt the data for testing.\n+    if (DFSClientFaultInjector.get().corruptPacket()) {\n+      buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n+    }\n+\n+    // Write the now contiguous full packet to the output stream.\n+    stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n+\n+    // undo corruption.\n+    if (DFSClientFaultInjector.get().uncorruptPacket()) {\n+      buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized void writeTo(DataOutputStream stm) throws IOException {\n    checkBuffer();\n\n    final int dataLen \u003d dataPos - dataStart;\n    final int checksumLen \u003d checksumPos - checksumStart;\n    final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n\n    PacketHeader header \u003d new PacketHeader(\n        pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n\n    if (checksumPos !\u003d dataStart) {\n      // Move the checksum to cover the gap. This can happen for the last\n      // packet or during an hflush/hsync call.\n      System.arraycopy(buf, checksumStart, buf,\n          dataStart - checksumLen , checksumLen);\n      checksumPos \u003d dataStart;\n      checksumStart \u003d checksumPos - checksumLen;\n    }\n\n    final int headerStart \u003d checksumStart - header.getSerializedSize();\n    assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n    assert headerStart \u003e\u003d 0;\n    assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n\n    // Copy the header data into the buffer immediately preceding the checksum\n    // data.\n    System.arraycopy(header.getBytes(), 0, buf, headerStart,\n        header.getSerializedSize());\n\n    // corrupt the data for testing.\n    if (DFSClientFaultInjector.get().corruptPacket()) {\n      buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n    }\n\n    // Write the now contiguous full packet to the output stream.\n    stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n\n    // undo corruption.\n    if (DFSClientFaultInjector.get().uncorruptPacket()) {\n      buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSPacket.java",
          "extendedDetails": {}
        }
      ]
    },
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-7358. Clients may get stuck waiting when using ByteArrayManager.\n",
      "commitDate": "13/11/14 12:28 PM",
      "commitName": "394ba94c5d2801fbc5d95c7872eeeede28eed1eb",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7358. Clients may get stuck waiting when using ByteArrayManager.\n",
          "commitDate": "13/11/14 12:28 PM",
          "commitName": "394ba94c5d2801fbc5d95c7872eeeede28eed1eb",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "05/11/14 10:51 AM",
          "commitNameOld": "56257fab1d5a7f66bebd9149c7df0436c0a57adb",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 8.07,
          "commitsBetweenForRepo": 93,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,43 @@\n-    void writeTo(DataOutputStream stm) throws IOException {\n+    synchronized void writeTo(DataOutputStream stm) throws IOException {\n+      checkBuffer();\n+\n       final int dataLen \u003d dataPos - dataStart;\n       final int checksumLen \u003d checksumPos - checksumStart;\n       final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n \n       PacketHeader header \u003d new PacketHeader(\n         pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n       \n       if (checksumPos !\u003d dataStart) {\n         // Move the checksum to cover the gap. This can happen for the last\n         // packet or during an hflush/hsync call.\n         System.arraycopy(buf, checksumStart, buf, \n                          dataStart - checksumLen , checksumLen); \n         checksumPos \u003d dataStart;\n         checksumStart \u003d checksumPos - checksumLen;\n       }\n       \n       final int headerStart \u003d checksumStart - header.getSerializedSize();\n       assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n       assert checksumPos \u003d\u003d dataStart;\n       assert headerStart \u003e\u003d 0;\n       assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n       \n       // Copy the header data into the buffer immediately preceding the checksum\n       // data.\n       System.arraycopy(header.getBytes(), 0, buf, headerStart,\n           header.getSerializedSize());\n       \n       // corrupt the data for testing.\n       if (DFSClientFaultInjector.get().corruptPacket()) {\n         buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n       }\n \n       // Write the now contiguous full packet to the output stream.\n       stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n \n       // undo corruption.\n       if (DFSClientFaultInjector.get().uncorruptPacket()) {\n         buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    synchronized void writeTo(DataOutputStream stm) throws IOException {\n      checkBuffer();\n\n      final int dataLen \u003d dataPos - dataStart;\n      final int checksumLen \u003d checksumPos - checksumStart;\n      final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n\n      PacketHeader header \u003d new PacketHeader(\n        pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n      \n      if (checksumPos !\u003d dataStart) {\n        // Move the checksum to cover the gap. This can happen for the last\n        // packet or during an hflush/hsync call.\n        System.arraycopy(buf, checksumStart, buf, \n                         dataStart - checksumLen , checksumLen); \n        checksumPos \u003d dataStart;\n        checksumStart \u003d checksumPos - checksumLen;\n      }\n      \n      final int headerStart \u003d checksumStart - header.getSerializedSize();\n      assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n      assert checksumPos \u003d\u003d dataStart;\n      assert headerStart \u003e\u003d 0;\n      assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n      \n      // Copy the header data into the buffer immediately preceding the checksum\n      // data.\n      System.arraycopy(header.getBytes(), 0, buf, headerStart,\n          header.getSerializedSize());\n      \n      // corrupt the data for testing.\n      if (DFSClientFaultInjector.get().corruptPacket()) {\n        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n      }\n\n      // Write the now contiguous full packet to the output stream.\n      stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n\n      // undo corruption.\n      if (DFSClientFaultInjector.get().uncorruptPacket()) {\n        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[synchronized]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7358. Clients may get stuck waiting when using ByteArrayManager.\n",
          "commitDate": "13/11/14 12:28 PM",
          "commitName": "394ba94c5d2801fbc5d95c7872eeeede28eed1eb",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "05/11/14 10:51 AM",
          "commitNameOld": "56257fab1d5a7f66bebd9149c7df0436c0a57adb",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 8.07,
          "commitsBetweenForRepo": 93,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,43 @@\n-    void writeTo(DataOutputStream stm) throws IOException {\n+    synchronized void writeTo(DataOutputStream stm) throws IOException {\n+      checkBuffer();\n+\n       final int dataLen \u003d dataPos - dataStart;\n       final int checksumLen \u003d checksumPos - checksumStart;\n       final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n \n       PacketHeader header \u003d new PacketHeader(\n         pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n       \n       if (checksumPos !\u003d dataStart) {\n         // Move the checksum to cover the gap. This can happen for the last\n         // packet or during an hflush/hsync call.\n         System.arraycopy(buf, checksumStart, buf, \n                          dataStart - checksumLen , checksumLen); \n         checksumPos \u003d dataStart;\n         checksumStart \u003d checksumPos - checksumLen;\n       }\n       \n       final int headerStart \u003d checksumStart - header.getSerializedSize();\n       assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n       assert checksumPos \u003d\u003d dataStart;\n       assert headerStart \u003e\u003d 0;\n       assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n       \n       // Copy the header data into the buffer immediately preceding the checksum\n       // data.\n       System.arraycopy(header.getBytes(), 0, buf, headerStart,\n           header.getSerializedSize());\n       \n       // corrupt the data for testing.\n       if (DFSClientFaultInjector.get().corruptPacket()) {\n         buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n       }\n \n       // Write the now contiguous full packet to the output stream.\n       stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n \n       // undo corruption.\n       if (DFSClientFaultInjector.get().uncorruptPacket()) {\n         buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    synchronized void writeTo(DataOutputStream stm) throws IOException {\n      checkBuffer();\n\n      final int dataLen \u003d dataPos - dataStart;\n      final int checksumLen \u003d checksumPos - checksumStart;\n      final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n\n      PacketHeader header \u003d new PacketHeader(\n        pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n      \n      if (checksumPos !\u003d dataStart) {\n        // Move the checksum to cover the gap. This can happen for the last\n        // packet or during an hflush/hsync call.\n        System.arraycopy(buf, checksumStart, buf, \n                         dataStart - checksumLen , checksumLen); \n        checksumPos \u003d dataStart;\n        checksumStart \u003d checksumPos - checksumLen;\n      }\n      \n      final int headerStart \u003d checksumStart - header.getSerializedSize();\n      assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n      assert checksumPos \u003d\u003d dataStart;\n      assert headerStart \u003e\u003d 0;\n      assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n      \n      // Copy the header data into the buffer immediately preceding the checksum\n      // data.\n      System.arraycopy(header.getBytes(), 0, buf, headerStart,\n          header.getSerializedSize());\n      \n      // corrupt the data for testing.\n      if (DFSClientFaultInjector.get().corruptPacket()) {\n        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n      }\n\n      // Write the now contiguous full packet to the output stream.\n      stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n\n      // undo corruption.\n      if (DFSClientFaultInjector.get().uncorruptPacket()) {\n        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "98a692fd6361365db4afb9523a5d83ee32774112": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3875. Issue handling checksum errors in write pipeline. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1484808 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/05/13 6:42 AM",
      "commitName": "98a692fd6361365db4afb9523a5d83ee32774112",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "26/04/13 4:50 PM",
      "commitNameOld": "60341dae1922c37acadc8629bff7443a6a0871cb",
      "commitAuthorOld": "",
      "daysBetweenCommits": 24.58,
      "commitsBetweenForRepo": 137,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,41 @@\n     void writeTo(DataOutputStream stm) throws IOException {\n       final int dataLen \u003d dataPos - dataStart;\n       final int checksumLen \u003d checksumPos - checksumStart;\n       final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n \n       PacketHeader header \u003d new PacketHeader(\n         pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n       \n       if (checksumPos !\u003d dataStart) {\n         // Move the checksum to cover the gap. This can happen for the last\n         // packet or during an hflush/hsync call.\n         System.arraycopy(buf, checksumStart, buf, \n                          dataStart - checksumLen , checksumLen); \n         checksumPos \u003d dataStart;\n         checksumStart \u003d checksumPos - checksumLen;\n       }\n       \n       final int headerStart \u003d checksumStart - header.getSerializedSize();\n       assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n       assert checksumPos \u003d\u003d dataStart;\n       assert headerStart \u003e\u003d 0;\n       assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n       \n       // Copy the header data into the buffer immediately preceding the checksum\n       // data.\n       System.arraycopy(header.getBytes(), 0, buf, headerStart,\n           header.getSerializedSize());\n       \n+      // corrupt the data for testing.\n+      if (DFSClientFaultInjector.get().corruptPacket()) {\n+        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n+      }\n+\n       // Write the now contiguous full packet to the output stream.\n       stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n+\n+      // undo corruption.\n+      if (DFSClientFaultInjector.get().uncorruptPacket()) {\n+        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n+      }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void writeTo(DataOutputStream stm) throws IOException {\n      final int dataLen \u003d dataPos - dataStart;\n      final int checksumLen \u003d checksumPos - checksumStart;\n      final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n\n      PacketHeader header \u003d new PacketHeader(\n        pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n      \n      if (checksumPos !\u003d dataStart) {\n        // Move the checksum to cover the gap. This can happen for the last\n        // packet or during an hflush/hsync call.\n        System.arraycopy(buf, checksumStart, buf, \n                         dataStart - checksumLen , checksumLen); \n        checksumPos \u003d dataStart;\n        checksumStart \u003d checksumPos - checksumLen;\n      }\n      \n      final int headerStart \u003d checksumStart - header.getSerializedSize();\n      assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n      assert checksumPos \u003d\u003d dataStart;\n      assert headerStart \u003e\u003d 0;\n      assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n      \n      // Copy the header data into the buffer immediately preceding the checksum\n      // data.\n      System.arraycopy(header.getBytes(), 0, buf, headerStart,\n          header.getSerializedSize());\n      \n      // corrupt the data for testing.\n      if (DFSClientFaultInjector.get().corruptPacket()) {\n        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n      }\n\n      // Write the now contiguous full packet to the output stream.\n      stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n\n      // undo corruption.\n      if (DFSClientFaultInjector.get().uncorruptPacket()) {\n        buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^\u003d 0xff;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3721. hsync support broke wire compatibility. Contributed by Todd Lipcon and Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1371495 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/12 2:31 PM",
      "commitName": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
      "commitAuthor": "Aaron Myers",
      "diff": "@@ -0,0 +1,31 @@\n+    void writeTo(DataOutputStream stm) throws IOException {\n+      final int dataLen \u003d dataPos - dataStart;\n+      final int checksumLen \u003d checksumPos - checksumStart;\n+      final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n+\n+      PacketHeader header \u003d new PacketHeader(\n+        pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n+      \n+      if (checksumPos !\u003d dataStart) {\n+        // Move the checksum to cover the gap. This can happen for the last\n+        // packet or during an hflush/hsync call.\n+        System.arraycopy(buf, checksumStart, buf, \n+                         dataStart - checksumLen , checksumLen); \n+        checksumPos \u003d dataStart;\n+        checksumStart \u003d checksumPos - checksumLen;\n+      }\n+      \n+      final int headerStart \u003d checksumStart - header.getSerializedSize();\n+      assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n+      assert checksumPos \u003d\u003d dataStart;\n+      assert headerStart \u003e\u003d 0;\n+      assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n+      \n+      // Copy the header data into the buffer immediately preceding the checksum\n+      // data.\n+      System.arraycopy(header.getBytes(), 0, buf, headerStart,\n+          header.getSerializedSize());\n+      \n+      // Write the now contiguous full packet to the output stream.\n+      stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    void writeTo(DataOutputStream stm) throws IOException {\n      final int dataLen \u003d dataPos - dataStart;\n      final int checksumLen \u003d checksumPos - checksumStart;\n      final int pktLen \u003d HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\n\n      PacketHeader header \u003d new PacketHeader(\n        pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\n      \n      if (checksumPos !\u003d dataStart) {\n        // Move the checksum to cover the gap. This can happen for the last\n        // packet or during an hflush/hsync call.\n        System.arraycopy(buf, checksumStart, buf, \n                         dataStart - checksumLen , checksumLen); \n        checksumPos \u003d dataStart;\n        checksumStart \u003d checksumPos - checksumLen;\n      }\n      \n      final int headerStart \u003d checksumStart - header.getSerializedSize();\n      assert checksumStart + 1 \u003e\u003d header.getSerializedSize();\n      assert checksumPos \u003d\u003d dataStart;\n      assert headerStart \u003e\u003d 0;\n      assert headerStart + header.getSerializedSize() \u003d\u003d checksumStart;\n      \n      // Copy the header data into the buffer immediately preceding the checksum\n      // data.\n      System.arraycopy(header.getBytes(), 0, buf, headerStart,\n          header.getSerializedSize());\n      \n      // Write the now contiguous full packet to the output stream.\n      stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}