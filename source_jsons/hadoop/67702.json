{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RetriableFileCopyCommand.java",
  "functionName": "copyToFile",
  "functionId": "copyToFile___targetPath-Path__targetFS-FileSystem__source-CopyListingFileStatus__sourceOffset-long__context-Mapper.Context__fileAttributes-EnumSet__FileAttribute____sourceChecksum-FileChecksum(modifiers-final)__sourceStatus-FileStatus",
  "sourceFilePath": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
  "functionStartLine": 192,
  "functionEndLine": 245,
  "numCommitsSeen": 87,
  "timeTaken": 5929,
  "changeHistory": [
    "c757cb61ebc9e69d9f6f143da91189b9f0517ee9",
    "b4adc8392c1314d6d6fbdd00f2afb306ef20a650",
    "0e6f8e4bc6642f90dc7b33848bfb1129ec20ee49",
    "a1a0281e12ea96476e75b076f76d5b5eb5254eea",
    "3671a5e16fbddbe5a0516289ce98e1305e02291c",
    "067d52b98c1d17a73b142bb53acc8aaa9c041f38",
    "718f0f92a9074cb9574bc9ae629b042e60626d34",
    "500dc615230f3010305a4318276bf7128d5cde18",
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67"
  ],
  "changeHistoryShort": {
    "c757cb61ebc9e69d9f6f143da91189b9f0517ee9": "Ymultichange(Yparameterchange,Ybodychange)",
    "b4adc8392c1314d6d6fbdd00f2afb306ef20a650": "Ybodychange",
    "0e6f8e4bc6642f90dc7b33848bfb1129ec20ee49": "Ybodychange",
    "a1a0281e12ea96476e75b076f76d5b5eb5254eea": "Ymultichange(Yparameterchange,Ybodychange)",
    "3671a5e16fbddbe5a0516289ce98e1305e02291c": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
    "067d52b98c1d17a73b142bb53acc8aaa9c041f38": "Ymultichange(Yparameterchange,Ybodychange)",
    "718f0f92a9074cb9574bc9ae629b042e60626d34": "Ybodychange",
    "500dc615230f3010305a4318276bf7128d5cde18": "Ybodychange",
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c757cb61ebc9e69d9f6f143da91189b9f0517ee9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-14254. Add a Distcp option to preserve Erasure Coding attributes. Contributed by Ayush Saxena.\n",
      "commitDate": "13/05/20 12:01 PM",
      "commitName": "c757cb61ebc9e69d9f6f143da91189b9f0517ee9",
      "commitAuthor": "Ayush Saxena",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-14254. Add a Distcp option to preserve Erasure Coding attributes. Contributed by Ayush Saxena.\n",
          "commitDate": "13/05/20 12:01 PM",
          "commitName": "c757cb61ebc9e69d9f6f143da91189b9f0517ee9",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "02/01/20 7:36 AM",
          "commitNameOld": "b6dc00f481189821e5d982083eba6d01f108b3de",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 132.14,
          "commitsBetweenForRepo": 454,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,30 +1,54 @@\n   private long copyToFile(Path targetPath, FileSystem targetFS,\n       CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n-      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n+      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum,\n+      FileStatus sourceStatus)\n       throws IOException {\n     FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n         FsPermission.getUMask(targetFS.getConf()));\n     int copyBufferSize \u003d context.getConfiguration().getInt(\n         DistCpOptionSwitch.COPY_BUFFER_SIZE.getConfigLabel(),\n         DistCpConstants.COPY_BUFFER_SIZE_DEFAULT);\n+    boolean preserveEC \u003d getFileAttributeSettings(context)\n+        .contains(DistCpOptions.FileAttribute.ERASURECODINGPOLICY);\n+\n+    ErasureCodingPolicy ecPolicy \u003d null;\n+    if (preserveEC \u0026\u0026 sourceStatus.isErasureCoded()\n+        \u0026\u0026 sourceStatus instanceof HdfsFileStatus\n+        \u0026\u0026 targetFS instanceof DistributedFileSystem) {\n+      ecPolicy \u003d ((HdfsFileStatus) sourceStatus).getErasureCodingPolicy();\n+    }\n     final OutputStream outStream;\n     if (action \u003d\u003d FileAction.OVERWRITE) {\n       // If there is an erasure coding policy set on the target directory,\n       // files will be written to the target directory using the same EC policy.\n       // The replication factor of the source file is ignored and not preserved.\n       final short repl \u003d getReplicationFactor(fileAttributes, source,\n           targetFS, targetPath);\n       final long blockSize \u003d getBlockSize(fileAttributes, source,\n           targetFS, targetPath);\n-      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n-          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n-          copyBufferSize, repl, blockSize, context,\n-          getChecksumOpt(fileAttributes, sourceChecksum));\n+      FSDataOutputStream out;\n+      ChecksumOpt checksumOpt \u003d getChecksumOpt(fileAttributes, sourceChecksum);\n+      if (!preserveEC || ecPolicy \u003d\u003d null) {\n+        out \u003d targetFS.create(targetPath, permission,\n+            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize,\n+            repl, blockSize, context, checksumOpt);\n+      } else {\n+        DistributedFileSystem dfs \u003d (DistributedFileSystem) targetFS;\n+        DistributedFileSystem.HdfsDataOutputStreamBuilder builder \u003d\n+            dfs.createFile(targetPath).permission(permission).create()\n+                .overwrite(true).bufferSize(copyBufferSize).replication(repl)\n+                .blockSize(blockSize).progress(context).recursive()\n+                .ecPolicyName(ecPolicy.getName());\n+        if (checksumOpt !\u003d null) {\n+          builder.checksumOpt(checksumOpt);\n+        }\n+        out \u003d builder.build();\n+      }\n       outStream \u003d new BufferedOutputStream(out);\n     } else {\n       outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n           copyBufferSize));\n     }\n     return copyBytes(source, sourceOffset, outStream, copyBufferSize,\n         context);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long copyToFile(Path targetPath, FileSystem targetFS,\n      CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum,\n      FileStatus sourceStatus)\n      throws IOException {\n    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n        FsPermission.getUMask(targetFS.getConf()));\n    int copyBufferSize \u003d context.getConfiguration().getInt(\n        DistCpOptionSwitch.COPY_BUFFER_SIZE.getConfigLabel(),\n        DistCpConstants.COPY_BUFFER_SIZE_DEFAULT);\n    boolean preserveEC \u003d getFileAttributeSettings(context)\n        .contains(DistCpOptions.FileAttribute.ERASURECODINGPOLICY);\n\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (preserveEC \u0026\u0026 sourceStatus.isErasureCoded()\n        \u0026\u0026 sourceStatus instanceof HdfsFileStatus\n        \u0026\u0026 targetFS instanceof DistributedFileSystem) {\n      ecPolicy \u003d ((HdfsFileStatus) sourceStatus).getErasureCodingPolicy();\n    }\n    final OutputStream outStream;\n    if (action \u003d\u003d FileAction.OVERWRITE) {\n      // If there is an erasure coding policy set on the target directory,\n      // files will be written to the target directory using the same EC policy.\n      // The replication factor of the source file is ignored and not preserved.\n      final short repl \u003d getReplicationFactor(fileAttributes, source,\n          targetFS, targetPath);\n      final long blockSize \u003d getBlockSize(fileAttributes, source,\n          targetFS, targetPath);\n      FSDataOutputStream out;\n      ChecksumOpt checksumOpt \u003d getChecksumOpt(fileAttributes, sourceChecksum);\n      if (!preserveEC || ecPolicy \u003d\u003d null) {\n        out \u003d targetFS.create(targetPath, permission,\n            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize,\n            repl, blockSize, context, checksumOpt);\n      } else {\n        DistributedFileSystem dfs \u003d (DistributedFileSystem) targetFS;\n        DistributedFileSystem.HdfsDataOutputStreamBuilder builder \u003d\n            dfs.createFile(targetPath).permission(permission).create()\n                .overwrite(true).bufferSize(copyBufferSize).replication(repl)\n                .blockSize(blockSize).progress(context).recursive()\n                .ecPolicyName(ecPolicy.getName());\n        if (checksumOpt !\u003d null) {\n          builder.checksumOpt(checksumOpt);\n        }\n        out \u003d builder.build();\n      }\n      outStream \u003d new BufferedOutputStream(out);\n    } else {\n      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n          copyBufferSize));\n    }\n    return copyBytes(source, sourceOffset, outStream, copyBufferSize,\n        context);\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
          "extendedDetails": {
            "oldValue": "[targetPath-Path, targetFS-FileSystem, source-CopyListingFileStatus, sourceOffset-long, context-Mapper.Context, fileAttributes-EnumSet\u003cFileAttribute\u003e, sourceChecksum-FileChecksum(modifiers-final)]",
            "newValue": "[targetPath-Path, targetFS-FileSystem, source-CopyListingFileStatus, sourceOffset-long, context-Mapper.Context, fileAttributes-EnumSet\u003cFileAttribute\u003e, sourceChecksum-FileChecksum(modifiers-final), sourceStatus-FileStatus]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-14254. Add a Distcp option to preserve Erasure Coding attributes. Contributed by Ayush Saxena.\n",
          "commitDate": "13/05/20 12:01 PM",
          "commitName": "c757cb61ebc9e69d9f6f143da91189b9f0517ee9",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "02/01/20 7:36 AM",
          "commitNameOld": "b6dc00f481189821e5d982083eba6d01f108b3de",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 132.14,
          "commitsBetweenForRepo": 454,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,30 +1,54 @@\n   private long copyToFile(Path targetPath, FileSystem targetFS,\n       CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n-      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n+      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum,\n+      FileStatus sourceStatus)\n       throws IOException {\n     FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n         FsPermission.getUMask(targetFS.getConf()));\n     int copyBufferSize \u003d context.getConfiguration().getInt(\n         DistCpOptionSwitch.COPY_BUFFER_SIZE.getConfigLabel(),\n         DistCpConstants.COPY_BUFFER_SIZE_DEFAULT);\n+    boolean preserveEC \u003d getFileAttributeSettings(context)\n+        .contains(DistCpOptions.FileAttribute.ERASURECODINGPOLICY);\n+\n+    ErasureCodingPolicy ecPolicy \u003d null;\n+    if (preserveEC \u0026\u0026 sourceStatus.isErasureCoded()\n+        \u0026\u0026 sourceStatus instanceof HdfsFileStatus\n+        \u0026\u0026 targetFS instanceof DistributedFileSystem) {\n+      ecPolicy \u003d ((HdfsFileStatus) sourceStatus).getErasureCodingPolicy();\n+    }\n     final OutputStream outStream;\n     if (action \u003d\u003d FileAction.OVERWRITE) {\n       // If there is an erasure coding policy set on the target directory,\n       // files will be written to the target directory using the same EC policy.\n       // The replication factor of the source file is ignored and not preserved.\n       final short repl \u003d getReplicationFactor(fileAttributes, source,\n           targetFS, targetPath);\n       final long blockSize \u003d getBlockSize(fileAttributes, source,\n           targetFS, targetPath);\n-      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n-          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n-          copyBufferSize, repl, blockSize, context,\n-          getChecksumOpt(fileAttributes, sourceChecksum));\n+      FSDataOutputStream out;\n+      ChecksumOpt checksumOpt \u003d getChecksumOpt(fileAttributes, sourceChecksum);\n+      if (!preserveEC || ecPolicy \u003d\u003d null) {\n+        out \u003d targetFS.create(targetPath, permission,\n+            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize,\n+            repl, blockSize, context, checksumOpt);\n+      } else {\n+        DistributedFileSystem dfs \u003d (DistributedFileSystem) targetFS;\n+        DistributedFileSystem.HdfsDataOutputStreamBuilder builder \u003d\n+            dfs.createFile(targetPath).permission(permission).create()\n+                .overwrite(true).bufferSize(copyBufferSize).replication(repl)\n+                .blockSize(blockSize).progress(context).recursive()\n+                .ecPolicyName(ecPolicy.getName());\n+        if (checksumOpt !\u003d null) {\n+          builder.checksumOpt(checksumOpt);\n+        }\n+        out \u003d builder.build();\n+      }\n       outStream \u003d new BufferedOutputStream(out);\n     } else {\n       outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n           copyBufferSize));\n     }\n     return copyBytes(source, sourceOffset, outStream, copyBufferSize,\n         context);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long copyToFile(Path targetPath, FileSystem targetFS,\n      CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum,\n      FileStatus sourceStatus)\n      throws IOException {\n    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n        FsPermission.getUMask(targetFS.getConf()));\n    int copyBufferSize \u003d context.getConfiguration().getInt(\n        DistCpOptionSwitch.COPY_BUFFER_SIZE.getConfigLabel(),\n        DistCpConstants.COPY_BUFFER_SIZE_DEFAULT);\n    boolean preserveEC \u003d getFileAttributeSettings(context)\n        .contains(DistCpOptions.FileAttribute.ERASURECODINGPOLICY);\n\n    ErasureCodingPolicy ecPolicy \u003d null;\n    if (preserveEC \u0026\u0026 sourceStatus.isErasureCoded()\n        \u0026\u0026 sourceStatus instanceof HdfsFileStatus\n        \u0026\u0026 targetFS instanceof DistributedFileSystem) {\n      ecPolicy \u003d ((HdfsFileStatus) sourceStatus).getErasureCodingPolicy();\n    }\n    final OutputStream outStream;\n    if (action \u003d\u003d FileAction.OVERWRITE) {\n      // If there is an erasure coding policy set on the target directory,\n      // files will be written to the target directory using the same EC policy.\n      // The replication factor of the source file is ignored and not preserved.\n      final short repl \u003d getReplicationFactor(fileAttributes, source,\n          targetFS, targetPath);\n      final long blockSize \u003d getBlockSize(fileAttributes, source,\n          targetFS, targetPath);\n      FSDataOutputStream out;\n      ChecksumOpt checksumOpt \u003d getChecksumOpt(fileAttributes, sourceChecksum);\n      if (!preserveEC || ecPolicy \u003d\u003d null) {\n        out \u003d targetFS.create(targetPath, permission,\n            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize,\n            repl, blockSize, context, checksumOpt);\n      } else {\n        DistributedFileSystem dfs \u003d (DistributedFileSystem) targetFS;\n        DistributedFileSystem.HdfsDataOutputStreamBuilder builder \u003d\n            dfs.createFile(targetPath).permission(permission).create()\n                .overwrite(true).bufferSize(copyBufferSize).replication(repl)\n                .blockSize(blockSize).progress(context).recursive()\n                .ecPolicyName(ecPolicy.getName());\n        if (checksumOpt !\u003d null) {\n          builder.checksumOpt(checksumOpt);\n        }\n        out \u003d builder.build();\n      }\n      outStream \u003d new BufferedOutputStream(out);\n    } else {\n      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n          copyBufferSize));\n    }\n    return copyBytes(source, sourceOffset, outStream, copyBufferSize,\n        context);\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
          "extendedDetails": {}
        }
      ]
    },
    "b4adc8392c1314d6d6fbdd00f2afb306ef20a650": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14407. DistCp - Introduce a configurable copy buffer size. (Omkar Aradhya K S via Yongjun Zhang)\n",
      "commitDate": "18/05/17 3:35 PM",
      "commitName": "b4adc8392c1314d6d6fbdd00f2afb306ef20a650",
      "commitAuthor": "Yongjun Zhang",
      "commitDateOld": "30/03/17 5:38 PM",
      "commitNameOld": "bf3fb585aaf2b179836e139c041fc87920a3c886",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 48.91,
      "commitsBetweenForRepo": 266,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,30 @@\n   private long copyToFile(Path targetPath, FileSystem targetFS,\n       CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n       EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n       throws IOException {\n     FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n         FsPermission.getUMask(targetFS.getConf()));\n+    int copyBufferSize \u003d context.getConfiguration().getInt(\n+        DistCpOptionSwitch.COPY_BUFFER_SIZE.getConfigLabel(),\n+        DistCpConstants.COPY_BUFFER_SIZE_DEFAULT);\n     final OutputStream outStream;\n     if (action \u003d\u003d FileAction.OVERWRITE) {\n       // If there is an erasure coding policy set on the target directory,\n       // files will be written to the target directory using the same EC policy.\n       // The replication factor of the source file is ignored and not preserved.\n       final short repl \u003d getReplicationFactor(fileAttributes, source,\n           targetFS, targetPath);\n       final long blockSize \u003d getBlockSize(fileAttributes, source,\n           targetFS, targetPath);\n       FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n           EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n-          BUFFER_SIZE, repl, blockSize, context,\n+          copyBufferSize, repl, blockSize, context,\n           getChecksumOpt(fileAttributes, sourceChecksum));\n       outStream \u003d new BufferedOutputStream(out);\n     } else {\n       outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n-          BUFFER_SIZE));\n+          copyBufferSize));\n     }\n-    return copyBytes(source, sourceOffset, outStream, BUFFER_SIZE,\n+    return copyBytes(source, sourceOffset, outStream, copyBufferSize,\n         context);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long copyToFile(Path targetPath, FileSystem targetFS,\n      CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n      throws IOException {\n    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n        FsPermission.getUMask(targetFS.getConf()));\n    int copyBufferSize \u003d context.getConfiguration().getInt(\n        DistCpOptionSwitch.COPY_BUFFER_SIZE.getConfigLabel(),\n        DistCpConstants.COPY_BUFFER_SIZE_DEFAULT);\n    final OutputStream outStream;\n    if (action \u003d\u003d FileAction.OVERWRITE) {\n      // If there is an erasure coding policy set on the target directory,\n      // files will be written to the target directory using the same EC policy.\n      // The replication factor of the source file is ignored and not preserved.\n      final short repl \u003d getReplicationFactor(fileAttributes, source,\n          targetFS, targetPath);\n      final long blockSize \u003d getBlockSize(fileAttributes, source,\n          targetFS, targetPath);\n      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n          copyBufferSize, repl, blockSize, context,\n          getChecksumOpt(fileAttributes, sourceChecksum));\n      outStream \u003d new BufferedOutputStream(out);\n    } else {\n      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n          copyBufferSize));\n    }\n    return copyBytes(source, sourceOffset, outStream, copyBufferSize,\n        context);\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
      "extendedDetails": {}
    },
    "0e6f8e4bc6642f90dc7b33848bfb1129ec20ee49": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10971. Distcp should not copy replication factor if source file is erasure coded. Contributed by Manoj Govindassamy.\n",
      "commitDate": "28/03/17 10:14 PM",
      "commitName": "0e6f8e4bc6642f90dc7b33848bfb1129ec20ee49",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "19/01/17 3:25 AM",
      "commitNameOld": "ed33ce11dd8de36fb79e103d8491d077cd4aaf77",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 68.74,
      "commitsBetweenForRepo": 371,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,27 @@\n   private long copyToFile(Path targetPath, FileSystem targetFS,\n       CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n       EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n       throws IOException {\n     FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n         FsPermission.getUMask(targetFS.getConf()));\n     final OutputStream outStream;\n     if (action \u003d\u003d FileAction.OVERWRITE) {\n+      // If there is an erasure coding policy set on the target directory,\n+      // files will be written to the target directory using the same EC policy.\n+      // The replication factor of the source file is ignored and not preserved.\n       final short repl \u003d getReplicationFactor(fileAttributes, source,\n           targetFS, targetPath);\n       final long blockSize \u003d getBlockSize(fileAttributes, source,\n           targetFS, targetPath);\n       FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n           EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n           BUFFER_SIZE, repl, blockSize, context,\n           getChecksumOpt(fileAttributes, sourceChecksum));\n       outStream \u003d new BufferedOutputStream(out);\n     } else {\n       outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n           BUFFER_SIZE));\n     }\n     return copyBytes(source, sourceOffset, outStream, BUFFER_SIZE,\n         context);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long copyToFile(Path targetPath, FileSystem targetFS,\n      CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n      throws IOException {\n    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n        FsPermission.getUMask(targetFS.getConf()));\n    final OutputStream outStream;\n    if (action \u003d\u003d FileAction.OVERWRITE) {\n      // If there is an erasure coding policy set on the target directory,\n      // files will be written to the target directory using the same EC policy.\n      // The replication factor of the source file is ignored and not preserved.\n      final short repl \u003d getReplicationFactor(fileAttributes, source,\n          targetFS, targetPath);\n      final long blockSize \u003d getBlockSize(fileAttributes, source,\n          targetFS, targetPath);\n      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n          BUFFER_SIZE, repl, blockSize, context,\n          getChecksumOpt(fileAttributes, sourceChecksum));\n      outStream \u003d new BufferedOutputStream(out);\n    } else {\n      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n          BUFFER_SIZE));\n    }\n    return copyBytes(source, sourceOffset, outStream, BUFFER_SIZE,\n        context);\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
      "extendedDetails": {}
    },
    "a1a0281e12ea96476e75b076f76d5b5eb5254eea": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-13626. Remove distcp dependency on FileStatus serialization\n",
      "commitDate": "24/10/16 12:46 PM",
      "commitName": "a1a0281e12ea96476e75b076f76d5b5eb5254eea",
      "commitAuthor": "Chris Douglas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-13626. Remove distcp dependency on FileStatus serialization\n",
          "commitDate": "24/10/16 12:46 PM",
          "commitName": "a1a0281e12ea96476e75b076f76d5b5eb5254eea",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "11/01/16 9:46 AM",
          "commitNameOld": "95f32015ad9273420299130a9f10acdbafe63556",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 287.08,
          "commitsBetweenForRepo": 2025,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,24 @@\n   private long copyToFile(Path targetPath, FileSystem targetFS,\n-      FileStatus sourceFileStatus, long sourceOffset, Mapper.Context context,\n+      CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n       EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n       throws IOException {\n     FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n         FsPermission.getUMask(targetFS.getConf()));\n     final OutputStream outStream;\n     if (action \u003d\u003d FileAction.OVERWRITE) {\n-      final short repl \u003d getReplicationFactor(fileAttributes, sourceFileStatus,\n+      final short repl \u003d getReplicationFactor(fileAttributes, source,\n           targetFS, targetPath);\n-      final long blockSize \u003d getBlockSize(fileAttributes, sourceFileStatus,\n+      final long blockSize \u003d getBlockSize(fileAttributes, source,\n           targetFS, targetPath);\n       FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n           EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n           BUFFER_SIZE, repl, blockSize, context,\n           getChecksumOpt(fileAttributes, sourceChecksum));\n       outStream \u003d new BufferedOutputStream(out);\n     } else {\n       outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n           BUFFER_SIZE));\n     }\n-    return copyBytes(sourceFileStatus, sourceOffset, outStream, BUFFER_SIZE,\n+    return copyBytes(source, sourceOffset, outStream, BUFFER_SIZE,\n         context);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long copyToFile(Path targetPath, FileSystem targetFS,\n      CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n      throws IOException {\n    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n        FsPermission.getUMask(targetFS.getConf()));\n    final OutputStream outStream;\n    if (action \u003d\u003d FileAction.OVERWRITE) {\n      final short repl \u003d getReplicationFactor(fileAttributes, source,\n          targetFS, targetPath);\n      final long blockSize \u003d getBlockSize(fileAttributes, source,\n          targetFS, targetPath);\n      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n          BUFFER_SIZE, repl, blockSize, context,\n          getChecksumOpt(fileAttributes, sourceChecksum));\n      outStream \u003d new BufferedOutputStream(out);\n    } else {\n      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n          BUFFER_SIZE));\n    }\n    return copyBytes(source, sourceOffset, outStream, BUFFER_SIZE,\n        context);\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
          "extendedDetails": {
            "oldValue": "[targetPath-Path, targetFS-FileSystem, sourceFileStatus-FileStatus, sourceOffset-long, context-Mapper.Context, fileAttributes-EnumSet\u003cFileAttribute\u003e, sourceChecksum-FileChecksum(modifiers-final)]",
            "newValue": "[targetPath-Path, targetFS-FileSystem, source-CopyListingFileStatus, sourceOffset-long, context-Mapper.Context, fileAttributes-EnumSet\u003cFileAttribute\u003e, sourceChecksum-FileChecksum(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-13626. Remove distcp dependency on FileStatus serialization\n",
          "commitDate": "24/10/16 12:46 PM",
          "commitName": "a1a0281e12ea96476e75b076f76d5b5eb5254eea",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "11/01/16 9:46 AM",
          "commitNameOld": "95f32015ad9273420299130a9f10acdbafe63556",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 287.08,
          "commitsBetweenForRepo": 2025,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,24 @@\n   private long copyToFile(Path targetPath, FileSystem targetFS,\n-      FileStatus sourceFileStatus, long sourceOffset, Mapper.Context context,\n+      CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n       EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n       throws IOException {\n     FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n         FsPermission.getUMask(targetFS.getConf()));\n     final OutputStream outStream;\n     if (action \u003d\u003d FileAction.OVERWRITE) {\n-      final short repl \u003d getReplicationFactor(fileAttributes, sourceFileStatus,\n+      final short repl \u003d getReplicationFactor(fileAttributes, source,\n           targetFS, targetPath);\n-      final long blockSize \u003d getBlockSize(fileAttributes, sourceFileStatus,\n+      final long blockSize \u003d getBlockSize(fileAttributes, source,\n           targetFS, targetPath);\n       FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n           EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n           BUFFER_SIZE, repl, blockSize, context,\n           getChecksumOpt(fileAttributes, sourceChecksum));\n       outStream \u003d new BufferedOutputStream(out);\n     } else {\n       outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n           BUFFER_SIZE));\n     }\n-    return copyBytes(sourceFileStatus, sourceOffset, outStream, BUFFER_SIZE,\n+    return copyBytes(source, sourceOffset, outStream, BUFFER_SIZE,\n         context);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long copyToFile(Path targetPath, FileSystem targetFS,\n      CopyListingFileStatus source, long sourceOffset, Mapper.Context context,\n      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n      throws IOException {\n    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n        FsPermission.getUMask(targetFS.getConf()));\n    final OutputStream outStream;\n    if (action \u003d\u003d FileAction.OVERWRITE) {\n      final short repl \u003d getReplicationFactor(fileAttributes, source,\n          targetFS, targetPath);\n      final long blockSize \u003d getBlockSize(fileAttributes, source,\n          targetFS, targetPath);\n      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n          BUFFER_SIZE, repl, blockSize, context,\n          getChecksumOpt(fileAttributes, sourceChecksum));\n      outStream \u003d new BufferedOutputStream(out);\n    } else {\n      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n          BUFFER_SIZE));\n    }\n    return copyBytes(source, sourceOffset, outStream, BUFFER_SIZE,\n        context);\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
          "extendedDetails": {}
        }
      ]
    },
    "3671a5e16fbddbe5a0516289ce98e1305e02291c": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-5899. Support incremental data copy in DistCp. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596931 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/05/14 11:17 AM",
      "commitName": "3671a5e16fbddbe5a0516289ce98e1305e02291c",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "MAPREDUCE-5899. Support incremental data copy in DistCp. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596931 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/05/14 11:17 AM",
          "commitName": "3671a5e16fbddbe5a0516289ce98e1305e02291c",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "30/01/14 3:53 PM",
          "commitNameOld": "067d52b98c1d17a73b142bb53acc8aaa9c041f38",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 111.77,
          "commitsBetweenForRepo": 836,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,24 @@\n-  private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n-      FileStatus sourceFileStatus, Mapper.Context context,\n+  private long copyToFile(Path targetPath, FileSystem targetFS,\n+      FileStatus sourceFileStatus, long sourceOffset, Mapper.Context context,\n       EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n       throws IOException {\n     FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n         FsPermission.getUMask(targetFS.getConf()));\n-    OutputStream outStream \u003d new BufferedOutputStream(\n-        targetFS.create(tmpTargetPath, permission,\n-            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), BUFFER_SIZE,\n-            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS,\n-                tmpTargetPath),\n-            getBlockSize(fileAttributes, sourceFileStatus, targetFS,\n-                tmpTargetPath),\n-            context, getChecksumOpt(fileAttributes, sourceChecksum)));\n-    return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, context);\n+    final OutputStream outStream;\n+    if (action \u003d\u003d FileAction.OVERWRITE) {\n+      final short repl \u003d getReplicationFactor(fileAttributes, sourceFileStatus,\n+          targetFS, targetPath);\n+      final long blockSize \u003d getBlockSize(fileAttributes, sourceFileStatus,\n+          targetFS, targetPath);\n+      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n+          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n+          BUFFER_SIZE, repl, blockSize, context,\n+          getChecksumOpt(fileAttributes, sourceChecksum));\n+      outStream \u003d new BufferedOutputStream(out);\n+    } else {\n+      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n+          BUFFER_SIZE));\n+    }\n+    return copyBytes(sourceFileStatus, sourceOffset, outStream, BUFFER_SIZE,\n+        context);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long copyToFile(Path targetPath, FileSystem targetFS,\n      FileStatus sourceFileStatus, long sourceOffset, Mapper.Context context,\n      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n      throws IOException {\n    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n        FsPermission.getUMask(targetFS.getConf()));\n    final OutputStream outStream;\n    if (action \u003d\u003d FileAction.OVERWRITE) {\n      final short repl \u003d getReplicationFactor(fileAttributes, sourceFileStatus,\n          targetFS, targetPath);\n      final long blockSize \u003d getBlockSize(fileAttributes, sourceFileStatus,\n          targetFS, targetPath);\n      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n          BUFFER_SIZE, repl, blockSize, context,\n          getChecksumOpt(fileAttributes, sourceChecksum));\n      outStream \u003d new BufferedOutputStream(out);\n    } else {\n      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n          BUFFER_SIZE));\n    }\n    return copyBytes(sourceFileStatus, sourceOffset, outStream, BUFFER_SIZE,\n        context);\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
          "extendedDetails": {
            "oldValue": "copyToTmpFile",
            "newValue": "copyToFile"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-5899. Support incremental data copy in DistCp. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596931 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/05/14 11:17 AM",
          "commitName": "3671a5e16fbddbe5a0516289ce98e1305e02291c",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "30/01/14 3:53 PM",
          "commitNameOld": "067d52b98c1d17a73b142bb53acc8aaa9c041f38",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 111.77,
          "commitsBetweenForRepo": 836,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,24 @@\n-  private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n-      FileStatus sourceFileStatus, Mapper.Context context,\n+  private long copyToFile(Path targetPath, FileSystem targetFS,\n+      FileStatus sourceFileStatus, long sourceOffset, Mapper.Context context,\n       EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n       throws IOException {\n     FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n         FsPermission.getUMask(targetFS.getConf()));\n-    OutputStream outStream \u003d new BufferedOutputStream(\n-        targetFS.create(tmpTargetPath, permission,\n-            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), BUFFER_SIZE,\n-            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS,\n-                tmpTargetPath),\n-            getBlockSize(fileAttributes, sourceFileStatus, targetFS,\n-                tmpTargetPath),\n-            context, getChecksumOpt(fileAttributes, sourceChecksum)));\n-    return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, context);\n+    final OutputStream outStream;\n+    if (action \u003d\u003d FileAction.OVERWRITE) {\n+      final short repl \u003d getReplicationFactor(fileAttributes, sourceFileStatus,\n+          targetFS, targetPath);\n+      final long blockSize \u003d getBlockSize(fileAttributes, sourceFileStatus,\n+          targetFS, targetPath);\n+      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n+          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n+          BUFFER_SIZE, repl, blockSize, context,\n+          getChecksumOpt(fileAttributes, sourceChecksum));\n+      outStream \u003d new BufferedOutputStream(out);\n+    } else {\n+      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n+          BUFFER_SIZE));\n+    }\n+    return copyBytes(sourceFileStatus, sourceOffset, outStream, BUFFER_SIZE,\n+        context);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long copyToFile(Path targetPath, FileSystem targetFS,\n      FileStatus sourceFileStatus, long sourceOffset, Mapper.Context context,\n      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n      throws IOException {\n    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n        FsPermission.getUMask(targetFS.getConf()));\n    final OutputStream outStream;\n    if (action \u003d\u003d FileAction.OVERWRITE) {\n      final short repl \u003d getReplicationFactor(fileAttributes, sourceFileStatus,\n          targetFS, targetPath);\n      final long blockSize \u003d getBlockSize(fileAttributes, sourceFileStatus,\n          targetFS, targetPath);\n      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n          BUFFER_SIZE, repl, blockSize, context,\n          getChecksumOpt(fileAttributes, sourceChecksum));\n      outStream \u003d new BufferedOutputStream(out);\n    } else {\n      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n          BUFFER_SIZE));\n    }\n    return copyBytes(sourceFileStatus, sourceOffset, outStream, BUFFER_SIZE,\n        context);\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
          "extendedDetails": {
            "oldValue": "[tmpTargetPath-Path, targetFS-FileSystem, sourceFileStatus-FileStatus, context-Mapper.Context, fileAttributes-EnumSet\u003cFileAttribute\u003e, sourceChecksum-FileChecksum(modifiers-final)]",
            "newValue": "[targetPath-Path, targetFS-FileSystem, sourceFileStatus-FileStatus, sourceOffset-long, context-Mapper.Context, fileAttributes-EnumSet\u003cFileAttribute\u003e, sourceChecksum-FileChecksum(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-5899. Support incremental data copy in DistCp. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596931 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/05/14 11:17 AM",
          "commitName": "3671a5e16fbddbe5a0516289ce98e1305e02291c",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "30/01/14 3:53 PM",
          "commitNameOld": "067d52b98c1d17a73b142bb53acc8aaa9c041f38",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 111.77,
          "commitsBetweenForRepo": 836,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,24 @@\n-  private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n-      FileStatus sourceFileStatus, Mapper.Context context,\n+  private long copyToFile(Path targetPath, FileSystem targetFS,\n+      FileStatus sourceFileStatus, long sourceOffset, Mapper.Context context,\n       EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n       throws IOException {\n     FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n         FsPermission.getUMask(targetFS.getConf()));\n-    OutputStream outStream \u003d new BufferedOutputStream(\n-        targetFS.create(tmpTargetPath, permission,\n-            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), BUFFER_SIZE,\n-            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS,\n-                tmpTargetPath),\n-            getBlockSize(fileAttributes, sourceFileStatus, targetFS,\n-                tmpTargetPath),\n-            context, getChecksumOpt(fileAttributes, sourceChecksum)));\n-    return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, context);\n+    final OutputStream outStream;\n+    if (action \u003d\u003d FileAction.OVERWRITE) {\n+      final short repl \u003d getReplicationFactor(fileAttributes, sourceFileStatus,\n+          targetFS, targetPath);\n+      final long blockSize \u003d getBlockSize(fileAttributes, sourceFileStatus,\n+          targetFS, targetPath);\n+      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n+          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n+          BUFFER_SIZE, repl, blockSize, context,\n+          getChecksumOpt(fileAttributes, sourceChecksum));\n+      outStream \u003d new BufferedOutputStream(out);\n+    } else {\n+      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n+          BUFFER_SIZE));\n+    }\n+    return copyBytes(sourceFileStatus, sourceOffset, outStream, BUFFER_SIZE,\n+        context);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long copyToFile(Path targetPath, FileSystem targetFS,\n      FileStatus sourceFileStatus, long sourceOffset, Mapper.Context context,\n      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n      throws IOException {\n    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n        FsPermission.getUMask(targetFS.getConf()));\n    final OutputStream outStream;\n    if (action \u003d\u003d FileAction.OVERWRITE) {\n      final short repl \u003d getReplicationFactor(fileAttributes, sourceFileStatus,\n          targetFS, targetPath);\n      final long blockSize \u003d getBlockSize(fileAttributes, sourceFileStatus,\n          targetFS, targetPath);\n      FSDataOutputStream out \u003d targetFS.create(targetPath, permission,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n          BUFFER_SIZE, repl, blockSize, context,\n          getChecksumOpt(fileAttributes, sourceChecksum));\n      outStream \u003d new BufferedOutputStream(out);\n    } else {\n      outStream \u003d new BufferedOutputStream(targetFS.append(targetPath,\n          BUFFER_SIZE));\n    }\n    return copyBytes(sourceFileStatus, sourceOffset, outStream, BUFFER_SIZE,\n        context);\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
          "extendedDetails": {}
        }
      ]
    },
    "067d52b98c1d17a73b142bb53acc8aaa9c041f38": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-10295. Allow distcp to automatically identify the checksum type of source files and use it for the target. Contributed by Jing Zhao and Laurent Goujon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563019 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/01/14 3:53 PM",
      "commitName": "067d52b98c1d17a73b142bb53acc8aaa9c041f38",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-10295. Allow distcp to automatically identify the checksum type of source files and use it for the target. Contributed by Jing Zhao and Laurent Goujon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563019 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/01/14 3:53 PM",
          "commitName": "067d52b98c1d17a73b142bb53acc8aaa9c041f38",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "05/12/13 7:47 AM",
          "commitNameOld": "9ea61e44153b938309841b1499488360e9abd176",
          "commitAuthorOld": "Daryn Sharp",
          "daysBetweenCommits": 56.34,
          "commitsBetweenForRepo": 291,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,10 +1,16 @@\n   private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n-                             FileStatus sourceFileStatus, Mapper.Context context,\n-                             EnumSet\u003cFileAttribute\u003e fileAttributes)\n-                             throws IOException {\n-    OutputStream outStream \u003d new BufferedOutputStream(targetFS.create(\n-            tmpTargetPath, true, BUFFER_SIZE,\n-            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath),\n-            getBlockSize(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath), context));\n+      FileStatus sourceFileStatus, Mapper.Context context,\n+      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n+      throws IOException {\n+    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n+        FsPermission.getUMask(targetFS.getConf()));\n+    OutputStream outStream \u003d new BufferedOutputStream(\n+        targetFS.create(tmpTargetPath, permission,\n+            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), BUFFER_SIZE,\n+            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS,\n+                tmpTargetPath),\n+            getBlockSize(fileAttributes, sourceFileStatus, targetFS,\n+                tmpTargetPath),\n+            context, getChecksumOpt(fileAttributes, sourceChecksum)));\n     return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, context);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n      FileStatus sourceFileStatus, Mapper.Context context,\n      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n      throws IOException {\n    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n        FsPermission.getUMask(targetFS.getConf()));\n    OutputStream outStream \u003d new BufferedOutputStream(\n        targetFS.create(tmpTargetPath, permission,\n            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), BUFFER_SIZE,\n            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS,\n                tmpTargetPath),\n            getBlockSize(fileAttributes, sourceFileStatus, targetFS,\n                tmpTargetPath),\n            context, getChecksumOpt(fileAttributes, sourceChecksum)));\n    return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, context);\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
          "extendedDetails": {
            "oldValue": "[tmpTargetPath-Path, targetFS-FileSystem, sourceFileStatus-FileStatus, context-Mapper.Context, fileAttributes-EnumSet\u003cFileAttribute\u003e]",
            "newValue": "[tmpTargetPath-Path, targetFS-FileSystem, sourceFileStatus-FileStatus, context-Mapper.Context, fileAttributes-EnumSet\u003cFileAttribute\u003e, sourceChecksum-FileChecksum(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-10295. Allow distcp to automatically identify the checksum type of source files and use it for the target. Contributed by Jing Zhao and Laurent Goujon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563019 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/01/14 3:53 PM",
          "commitName": "067d52b98c1d17a73b142bb53acc8aaa9c041f38",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "05/12/13 7:47 AM",
          "commitNameOld": "9ea61e44153b938309841b1499488360e9abd176",
          "commitAuthorOld": "Daryn Sharp",
          "daysBetweenCommits": 56.34,
          "commitsBetweenForRepo": 291,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,10 +1,16 @@\n   private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n-                             FileStatus sourceFileStatus, Mapper.Context context,\n-                             EnumSet\u003cFileAttribute\u003e fileAttributes)\n-                             throws IOException {\n-    OutputStream outStream \u003d new BufferedOutputStream(targetFS.create(\n-            tmpTargetPath, true, BUFFER_SIZE,\n-            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath),\n-            getBlockSize(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath), context));\n+      FileStatus sourceFileStatus, Mapper.Context context,\n+      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n+      throws IOException {\n+    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n+        FsPermission.getUMask(targetFS.getConf()));\n+    OutputStream outStream \u003d new BufferedOutputStream(\n+        targetFS.create(tmpTargetPath, permission,\n+            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), BUFFER_SIZE,\n+            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS,\n+                tmpTargetPath),\n+            getBlockSize(fileAttributes, sourceFileStatus, targetFS,\n+                tmpTargetPath),\n+            context, getChecksumOpt(fileAttributes, sourceChecksum)));\n     return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, context);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n      FileStatus sourceFileStatus, Mapper.Context context,\n      EnumSet\u003cFileAttribute\u003e fileAttributes, final FileChecksum sourceChecksum)\n      throws IOException {\n    FsPermission permission \u003d FsPermission.getFileDefault().applyUMask(\n        FsPermission.getUMask(targetFS.getConf()));\n    OutputStream outStream \u003d new BufferedOutputStream(\n        targetFS.create(tmpTargetPath, permission,\n            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), BUFFER_SIZE,\n            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS,\n                tmpTargetPath),\n            getBlockSize(fileAttributes, sourceFileStatus, targetFS,\n                tmpTargetPath),\n            context, getChecksumOpt(fileAttributes, sourceChecksum)));\n    return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, context);\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
          "extendedDetails": {}
        }
      ]
    },
    "718f0f92a9074cb9574bc9ae629b042e60626d34": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5075. DistCp leaks input file handles since ThrottledInputStream does not close the wrapped InputStream.  Contributed by Chris Nauroth\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1458741 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/03/13 3:40 AM",
      "commitName": "718f0f92a9074cb9574bc9ae629b042e60626d34",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "05/09/12 11:54 AM",
      "commitNameOld": "e57843e02a846e5b3bb19c4bf9c3f61675d3e8ff",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 195.66,
      "commitsBetweenForRepo": 950,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,10 @@\n   private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n                              FileStatus sourceFileStatus, Mapper.Context context,\n                              EnumSet\u003cFileAttribute\u003e fileAttributes)\n                              throws IOException {\n     OutputStream outStream \u003d new BufferedOutputStream(targetFS.create(\n             tmpTargetPath, true, BUFFER_SIZE,\n             getReplicationFactor(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath),\n             getBlockSize(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath), context));\n-    return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, true, context);\n+    return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, context);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n                             FileStatus sourceFileStatus, Mapper.Context context,\n                             EnumSet\u003cFileAttribute\u003e fileAttributes)\n                             throws IOException {\n    OutputStream outStream \u003d new BufferedOutputStream(targetFS.create(\n            tmpTargetPath, true, BUFFER_SIZE,\n            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath),\n            getBlockSize(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath), context));\n    return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, context);\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
      "extendedDetails": {}
    },
    "500dc615230f3010305a4318276bf7128d5cde18": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-8305. distcp over viewfs is broken (John George via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1331440 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/04/12 7:39 AM",
      "commitName": "500dc615230f3010305a4318276bf7128d5cde18",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "25/01/12 10:36 PM",
      "commitNameOld": "d06948002fb0cabf72cc0d46bf2fa67d45370f67",
      "commitAuthorOld": "Mahadev Konar",
      "daysBetweenCommits": 92.34,
      "commitsBetweenForRepo": 735,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,10 @@\n   private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n                              FileStatus sourceFileStatus, Mapper.Context context,\n                              EnumSet\u003cFileAttribute\u003e fileAttributes)\n                              throws IOException {\n     OutputStream outStream \u003d new BufferedOutputStream(targetFS.create(\n             tmpTargetPath, true, BUFFER_SIZE,\n-            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS),\n-            getBlockSize(fileAttributes, sourceFileStatus, targetFS), context));\n+            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath),\n+            getBlockSize(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath), context));\n     return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, true, context);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n                             FileStatus sourceFileStatus, Mapper.Context context,\n                             EnumSet\u003cFileAttribute\u003e fileAttributes)\n                             throws IOException {\n    OutputStream outStream \u003d new BufferedOutputStream(targetFS.create(\n            tmpTargetPath, true, BUFFER_SIZE,\n            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath),\n            getBlockSize(fileAttributes, sourceFileStatus, targetFS, tmpTargetPath), context));\n    return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, true, context);\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java",
      "extendedDetails": {}
    },
    "d06948002fb0cabf72cc0d46bf2fa67d45370f67": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1236045 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/12 10:36 PM",
      "commitName": "d06948002fb0cabf72cc0d46bf2fa67d45370f67",
      "commitAuthor": "Mahadev Konar",
      "diff": "@@ -0,0 +1,10 @@\n+  private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n+                             FileStatus sourceFileStatus, Mapper.Context context,\n+                             EnumSet\u003cFileAttribute\u003e fileAttributes)\n+                             throws IOException {\n+    OutputStream outStream \u003d new BufferedOutputStream(targetFS.create(\n+            tmpTargetPath, true, BUFFER_SIZE,\n+            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS),\n+            getBlockSize(fileAttributes, sourceFileStatus, targetFS), context));\n+    return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, true, context);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private long copyToTmpFile(Path tmpTargetPath, FileSystem targetFS,\n                             FileStatus sourceFileStatus, Mapper.Context context,\n                             EnumSet\u003cFileAttribute\u003e fileAttributes)\n                             throws IOException {\n    OutputStream outStream \u003d new BufferedOutputStream(targetFS.create(\n            tmpTargetPath, true, BUFFER_SIZE,\n            getReplicationFactor(fileAttributes, sourceFileStatus, targetFS),\n            getBlockSize(fileAttributes, sourceFileStatus, targetFS), context));\n    return copyBytes(sourceFileStatus, outStream, BUFFER_SIZE, true, context);\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java"
    }
  }
}