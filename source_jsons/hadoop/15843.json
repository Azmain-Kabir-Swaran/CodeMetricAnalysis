{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSOperations.java",
  "functionName": "contentSummaryToJSON",
  "functionId": "contentSummaryToJSON___contentSummary-ContentSummary",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java",
  "functionStartLine": 276,
  "functionEndLine": 298,
  "numCommitsSeen": 44,
  "timeTaken": 1928,
  "changeHistory": [
    "17e8cf501b384af93726e4f2e6f5e28c6e3a8f65",
    "3ae775d74029b6ae82263739f598ceb25c597dcd",
    "3334306512b5dc932814fded31a89ba1ee97cd9f"
  ],
  "changeHistoryShort": {
    "17e8cf501b384af93726e4f2e6f5e28c6e3a8f65": "Ybodychange",
    "3ae775d74029b6ae82263739f598ceb25c597dcd": "Ybodychange",
    "3334306512b5dc932814fded31a89ba1ee97cd9f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "17e8cf501b384af93726e4f2e6f5e28c6e3a8f65": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14686. HttpFS: HttpFSFileSystem#getErasureCodingPolicy always returns null (#1192) Contributed by Siyao Meng.\n\n",
      "commitDate": "01/08/19 5:15 PM",
      "commitName": "17e8cf501b384af93726e4f2e6f5e28c6e3a8f65",
      "commitAuthor": "Siyao Meng",
      "commitDateOld": "30/07/19 4:01 PM",
      "commitNameOld": "3ae775d74029b6ae82263739f598ceb25c597dcd",
      "commitAuthorOld": "Chao Sun",
      "daysBetweenCommits": 2.05,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,23 @@\n   private static Map contentSummaryToJSON(ContentSummary contentSummary) {\n     Map json \u003d new LinkedHashMap();\n     json.put(HttpFSFileSystem.CONTENT_SUMMARY_DIRECTORY_COUNT_JSON,\n         contentSummary.getDirectoryCount());\n+    json.put(HttpFSFileSystem.CONTENT_SUMMARY_ECPOLICY_JSON,\n+        contentSummary.getErasureCodingPolicy());\n     json.put(HttpFSFileSystem.CONTENT_SUMMARY_FILE_COUNT_JSON,\n         contentSummary.getFileCount());\n     json.put(HttpFSFileSystem.CONTENT_SUMMARY_LENGTH_JSON,\n         contentSummary.getLength());\n     Map\u003cString, Object\u003e quotaUsageMap \u003d quotaUsageToMap(contentSummary);\n     for (Map.Entry\u003cString, Object\u003e e : quotaUsageMap.entrySet()) {\n       // For ContentSummary we don\u0027t need this since we already have\n       // separate count for file and directory.\n       if (!e.getKey().equals(\n           HttpFSFileSystem.QUOTA_USAGE_FILE_AND_DIRECTORY_COUNT_JSON)) {\n         json.put(e.getKey(), e.getValue());\n       }\n     }\n     Map response \u003d new LinkedHashMap();\n     response.put(HttpFSFileSystem.CONTENT_SUMMARY_JSON, json);\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static Map contentSummaryToJSON(ContentSummary contentSummary) {\n    Map json \u003d new LinkedHashMap();\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_DIRECTORY_COUNT_JSON,\n        contentSummary.getDirectoryCount());\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_ECPOLICY_JSON,\n        contentSummary.getErasureCodingPolicy());\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_FILE_COUNT_JSON,\n        contentSummary.getFileCount());\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_LENGTH_JSON,\n        contentSummary.getLength());\n    Map\u003cString, Object\u003e quotaUsageMap \u003d quotaUsageToMap(contentSummary);\n    for (Map.Entry\u003cString, Object\u003e e : quotaUsageMap.entrySet()) {\n      // For ContentSummary we don\u0027t need this since we already have\n      // separate count for file and directory.\n      if (!e.getKey().equals(\n          HttpFSFileSystem.QUOTA_USAGE_FILE_AND_DIRECTORY_COUNT_JSON)) {\n        json.put(e.getKey(), e.getValue());\n      }\n    }\n    Map response \u003d new LinkedHashMap();\n    response.put(HttpFSFileSystem.CONTENT_SUMMARY_JSON, json);\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java",
      "extendedDetails": {}
    },
    "3ae775d74029b6ae82263739f598ceb25c597dcd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14034. Support getQuotaUsage API in WebHDFS. Contributed by Chao Sun.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "30/07/19 4:01 PM",
      "commitName": "3ae775d74029b6ae82263739f598ceb25c597dcd",
      "commitAuthor": "Chao Sun",
      "commitDateOld": "11/10/18 3:01 PM",
      "commitNameOld": "6dcfef79afe97eda6d09fe2567bb4d4074223141",
      "commitAuthorOld": "Siyao Meng",
      "daysBetweenCommits": 292.04,
      "commitsBetweenForRepo": 2142,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,21 @@\n   private static Map contentSummaryToJSON(ContentSummary contentSummary) {\n     Map json \u003d new LinkedHashMap();\n-    json.put(HttpFSFileSystem.CONTENT_SUMMARY_DIRECTORY_COUNT_JSON, contentSummary.getDirectoryCount());\n-    json.put(HttpFSFileSystem.CONTENT_SUMMARY_FILE_COUNT_JSON, contentSummary.getFileCount());\n-    json.put(HttpFSFileSystem.CONTENT_SUMMARY_LENGTH_JSON, contentSummary.getLength());\n-    json.put(HttpFSFileSystem.CONTENT_SUMMARY_QUOTA_JSON, contentSummary.getQuota());\n-    json.put(HttpFSFileSystem.CONTENT_SUMMARY_SPACE_CONSUMED_JSON, contentSummary.getSpaceConsumed());\n-    json.put(HttpFSFileSystem.CONTENT_SUMMARY_SPACE_QUOTA_JSON, contentSummary.getSpaceQuota());\n+    json.put(HttpFSFileSystem.CONTENT_SUMMARY_DIRECTORY_COUNT_JSON,\n+        contentSummary.getDirectoryCount());\n+    json.put(HttpFSFileSystem.CONTENT_SUMMARY_FILE_COUNT_JSON,\n+        contentSummary.getFileCount());\n+    json.put(HttpFSFileSystem.CONTENT_SUMMARY_LENGTH_JSON,\n+        contentSummary.getLength());\n+    Map\u003cString, Object\u003e quotaUsageMap \u003d quotaUsageToMap(contentSummary);\n+    for (Map.Entry\u003cString, Object\u003e e : quotaUsageMap.entrySet()) {\n+      // For ContentSummary we don\u0027t need this since we already have\n+      // separate count for file and directory.\n+      if (!e.getKey().equals(\n+          HttpFSFileSystem.QUOTA_USAGE_FILE_AND_DIRECTORY_COUNT_JSON)) {\n+        json.put(e.getKey(), e.getValue());\n+      }\n+    }\n     Map response \u003d new LinkedHashMap();\n     response.put(HttpFSFileSystem.CONTENT_SUMMARY_JSON, json);\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static Map contentSummaryToJSON(ContentSummary contentSummary) {\n    Map json \u003d new LinkedHashMap();\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_DIRECTORY_COUNT_JSON,\n        contentSummary.getDirectoryCount());\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_FILE_COUNT_JSON,\n        contentSummary.getFileCount());\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_LENGTH_JSON,\n        contentSummary.getLength());\n    Map\u003cString, Object\u003e quotaUsageMap \u003d quotaUsageToMap(contentSummary);\n    for (Map.Entry\u003cString, Object\u003e e : quotaUsageMap.entrySet()) {\n      // For ContentSummary we don\u0027t need this since we already have\n      // separate count for file and directory.\n      if (!e.getKey().equals(\n          HttpFSFileSystem.QUOTA_USAGE_FILE_AND_DIRECTORY_COUNT_JSON)) {\n        json.put(e.getKey(), e.getValue());\n      }\n    }\n    Map response \u003d new LinkedHashMap();\n    response.put(HttpFSFileSystem.CONTENT_SUMMARY_JSON, json);\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java",
      "extendedDetails": {}
    },
    "3334306512b5dc932814fded31a89ba1ee97cd9f": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2178. Contributing Hoop to HDFS, replacement for HDFS proxy with read/write capabilities. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212060 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/12/11 11:25 AM",
      "commitName": "3334306512b5dc932814fded31a89ba1ee97cd9f",
      "commitAuthor": "Alejandro Abdelnur",
      "diff": "@@ -0,0 +1,12 @@\n+  private static Map contentSummaryToJSON(ContentSummary contentSummary) {\n+    Map json \u003d new LinkedHashMap();\n+    json.put(HttpFSFileSystem.CONTENT_SUMMARY_DIRECTORY_COUNT_JSON, contentSummary.getDirectoryCount());\n+    json.put(HttpFSFileSystem.CONTENT_SUMMARY_FILE_COUNT_JSON, contentSummary.getFileCount());\n+    json.put(HttpFSFileSystem.CONTENT_SUMMARY_LENGTH_JSON, contentSummary.getLength());\n+    json.put(HttpFSFileSystem.CONTENT_SUMMARY_QUOTA_JSON, contentSummary.getQuota());\n+    json.put(HttpFSFileSystem.CONTENT_SUMMARY_SPACE_CONSUMED_JSON, contentSummary.getSpaceConsumed());\n+    json.put(HttpFSFileSystem.CONTENT_SUMMARY_SPACE_QUOTA_JSON, contentSummary.getSpaceQuota());\n+    Map response \u003d new LinkedHashMap();\n+    response.put(HttpFSFileSystem.CONTENT_SUMMARY_JSON, json);\n+    return response;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static Map contentSummaryToJSON(ContentSummary contentSummary) {\n    Map json \u003d new LinkedHashMap();\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_DIRECTORY_COUNT_JSON, contentSummary.getDirectoryCount());\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_FILE_COUNT_JSON, contentSummary.getFileCount());\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_LENGTH_JSON, contentSummary.getLength());\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_QUOTA_JSON, contentSummary.getQuota());\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_SPACE_CONSUMED_JSON, contentSummary.getSpaceConsumed());\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_SPACE_QUOTA_JSON, contentSummary.getSpaceQuota());\n    Map response \u003d new LinkedHashMap();\n    response.put(HttpFSFileSystem.CONTENT_SUMMARY_JSON, json);\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java"
    }
  }
}