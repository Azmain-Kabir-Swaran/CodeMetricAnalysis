{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirErasureCodingOp.java",
  "functionName": "removeErasureCodingPolicyXAttr",
  "functionId": "removeErasureCodingPolicyXAttr___fsn-FSNamesystem(modifiers-final)__srcIIP-INodesInPath(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
  "functionStartLine": 314,
  "functionEndLine": 339,
  "numCommitsSeen": 40,
  "timeTaken": 2613,
  "changeHistory": [
    "27ffd43b6419c9ebe697536bcb6abb858ce791d2",
    "219f4c199e45f8ce7f41192493bf0dc8f1e5dc30",
    "e69231658dc4a79da936e6856017b5c4f6124ecb"
  ],
  "changeHistoryShort": {
    "27ffd43b6419c9ebe697536bcb6abb858ce791d2": "Ybodychange",
    "219f4c199e45f8ce7f41192493bf0dc8f1e5dc30": "Ybodychange",
    "e69231658dc4a79da936e6856017b5c4f6124ecb": "Yintroduced"
  },
  "changeHistoryDetails": {
    "27ffd43b6419c9ebe697536bcb6abb858ce791d2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12569. Unset EC policy logs empty payload in edit log. (Lei (Eddy) Xu)\n",
      "commitDate": "02/10/17 3:35 PM",
      "commitName": "27ffd43b6419c9ebe697536bcb6abb858ce791d2",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "14/09/17 6:43 PM",
      "commitNameOld": "08d996d3e9265efad737efad50cbc5b10a0202f8",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 17.87,
      "commitsBetweenForRepo": 123,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,26 @@\n   private static List\u003cXAttr\u003e removeErasureCodingPolicyXAttr(\n       final FSNamesystem fsn, final INodesInPath srcIIP) throws IOException {\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     assert fsd.hasWriteLock();\n     Preconditions.checkNotNull(srcIIP, \"INodes cannot be null\");\n     String src \u003d srcIIP.getPath();\n     final INode inode \u003d srcIIP.getLastINode();\n     if (inode \u003d\u003d null) {\n       throw new FileNotFoundException(\"Path not found: \" + srcIIP.getPath());\n     }\n     if (!inode.isDirectory()) {\n       throw new IOException(\"Cannot unset an erasure coding policy \" +\n           \"on a file \" + src);\n     }\n \n     // Check whether the directory has a specific erasure coding policy\n     // directly on itself.\n     final XAttr ecXAttr \u003d getErasureCodingPolicyXAttrForINode(fsn, inode);\n     if (ecXAttr \u003d\u003d null) {\n       return null;\n     }\n \n     final List\u003cXAttr\u003e xattrs \u003d Lists.newArrayListWithCapacity(1);\n     xattrs.add(ecXAttr);\n-    FSDirXAttrOp.unprotectedRemoveXAttrs(fsd, srcIIP, xattrs);\n-    return xattrs;\n+    return FSDirXAttrOp.unprotectedRemoveXAttrs(fsd, srcIIP, xattrs);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static List\u003cXAttr\u003e removeErasureCodingPolicyXAttr(\n      final FSNamesystem fsn, final INodesInPath srcIIP) throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    assert fsd.hasWriteLock();\n    Preconditions.checkNotNull(srcIIP, \"INodes cannot be null\");\n    String src \u003d srcIIP.getPath();\n    final INode inode \u003d srcIIP.getLastINode();\n    if (inode \u003d\u003d null) {\n      throw new FileNotFoundException(\"Path not found: \" + srcIIP.getPath());\n    }\n    if (!inode.isDirectory()) {\n      throw new IOException(\"Cannot unset an erasure coding policy \" +\n          \"on a file \" + src);\n    }\n\n    // Check whether the directory has a specific erasure coding policy\n    // directly on itself.\n    final XAttr ecXAttr \u003d getErasureCodingPolicyXAttrForINode(fsn, inode);\n    if (ecXAttr \u003d\u003d null) {\n      return null;\n    }\n\n    final List\u003cXAttr\u003e xattrs \u003d Lists.newArrayListWithCapacity(1);\n    xattrs.add(ecXAttr);\n    return FSDirXAttrOp.unprotectedRemoveXAttrs(fsd, srcIIP, xattrs);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
      "extendedDetails": {}
    },
    "219f4c199e45f8ce7f41192493bf0dc8f1e5dc30": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11904. Reuse iip in unprotectedRemoveXAttrs calls.\n",
      "commitDate": "01/06/17 2:13 PM",
      "commitName": "219f4c199e45f8ce7f41192493bf0dc8f1e5dc30",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "24/05/17 3:45 AM",
      "commitNameOld": "a62be38a5e5d3a61dfb59054b3f5fd5b1b7053b3",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 8.44,
      "commitsBetweenForRepo": 51,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   private static List\u003cXAttr\u003e removeErasureCodingPolicyXAttr(\n       final FSNamesystem fsn, final INodesInPath srcIIP) throws IOException {\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     assert fsd.hasWriteLock();\n     Preconditions.checkNotNull(srcIIP, \"INodes cannot be null\");\n     String src \u003d srcIIP.getPath();\n     final INode inode \u003d srcIIP.getLastINode();\n     if (inode \u003d\u003d null) {\n       throw new FileNotFoundException(\"Path not found: \" + srcIIP.getPath());\n     }\n     if (!inode.isDirectory()) {\n       throw new IOException(\"Cannot unset an erasure coding policy \" +\n           \"on a file \" + src);\n     }\n \n     // Check whether the directory has a specific erasure coding policy\n     // directly on itself.\n     final XAttr ecXAttr \u003d getErasureCodingPolicyXAttrForINode(fsn, inode);\n     if (ecXAttr \u003d\u003d null) {\n       return null;\n     }\n \n     final List\u003cXAttr\u003e xattrs \u003d Lists.newArrayListWithCapacity(1);\n     xattrs.add(ecXAttr);\n-    FSDirXAttrOp.unprotectedRemoveXAttrs(fsd, srcIIP.getPath(), xattrs);\n+    FSDirXAttrOp.unprotectedRemoveXAttrs(fsd, srcIIP, xattrs);\n     return xattrs;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static List\u003cXAttr\u003e removeErasureCodingPolicyXAttr(\n      final FSNamesystem fsn, final INodesInPath srcIIP) throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    assert fsd.hasWriteLock();\n    Preconditions.checkNotNull(srcIIP, \"INodes cannot be null\");\n    String src \u003d srcIIP.getPath();\n    final INode inode \u003d srcIIP.getLastINode();\n    if (inode \u003d\u003d null) {\n      throw new FileNotFoundException(\"Path not found: \" + srcIIP.getPath());\n    }\n    if (!inode.isDirectory()) {\n      throw new IOException(\"Cannot unset an erasure coding policy \" +\n          \"on a file \" + src);\n    }\n\n    // Check whether the directory has a specific erasure coding policy\n    // directly on itself.\n    final XAttr ecXAttr \u003d getErasureCodingPolicyXAttrForINode(fsn, inode);\n    if (ecXAttr \u003d\u003d null) {\n      return null;\n    }\n\n    final List\u003cXAttr\u003e xattrs \u003d Lists.newArrayListWithCapacity(1);\n    xattrs.add(ecXAttr);\n    FSDirXAttrOp.unprotectedRemoveXAttrs(fsd, srcIIP, xattrs);\n    return xattrs;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
      "extendedDetails": {}
    },
    "e69231658dc4a79da936e6856017b5c4f6124ecb": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11072. Add ability to unset and change directory EC policy. Contributed by Sammi Chen.\n",
      "commitDate": "10/01/17 11:32 AM",
      "commitName": "e69231658dc4a79da936e6856017b5c4f6124ecb",
      "commitAuthor": "Andrew Wang",
      "diff": "@@ -0,0 +1,27 @@\n+  private static List\u003cXAttr\u003e removeErasureCodingPolicyXAttr(\n+      final FSNamesystem fsn, final INodesInPath srcIIP) throws IOException {\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    assert fsd.hasWriteLock();\n+    Preconditions.checkNotNull(srcIIP, \"INodes cannot be null\");\n+    String src \u003d srcIIP.getPath();\n+    final INode inode \u003d srcIIP.getLastINode();\n+    if (inode \u003d\u003d null) {\n+      throw new FileNotFoundException(\"Path not found: \" + srcIIP.getPath());\n+    }\n+    if (!inode.isDirectory()) {\n+      throw new IOException(\"Cannot unset an erasure coding policy \" +\n+          \"on a file \" + src);\n+    }\n+\n+    // Check whether the directory has a specific erasure coding policy\n+    // directly on itself.\n+    final XAttr ecXAttr \u003d getErasureCodingPolicyXAttrForINode(fsn, inode);\n+    if (ecXAttr \u003d\u003d null) {\n+      return null;\n+    }\n+\n+    final List\u003cXAttr\u003e xattrs \u003d Lists.newArrayListWithCapacity(1);\n+    xattrs.add(ecXAttr);\n+    FSDirXAttrOp.unprotectedRemoveXAttrs(fsd, srcIIP.getPath(), xattrs);\n+    return xattrs;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static List\u003cXAttr\u003e removeErasureCodingPolicyXAttr(\n      final FSNamesystem fsn, final INodesInPath srcIIP) throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    assert fsd.hasWriteLock();\n    Preconditions.checkNotNull(srcIIP, \"INodes cannot be null\");\n    String src \u003d srcIIP.getPath();\n    final INode inode \u003d srcIIP.getLastINode();\n    if (inode \u003d\u003d null) {\n      throw new FileNotFoundException(\"Path not found: \" + srcIIP.getPath());\n    }\n    if (!inode.isDirectory()) {\n      throw new IOException(\"Cannot unset an erasure coding policy \" +\n          \"on a file \" + src);\n    }\n\n    // Check whether the directory has a specific erasure coding policy\n    // directly on itself.\n    final XAttr ecXAttr \u003d getErasureCodingPolicyXAttrForINode(fsn, inode);\n    if (ecXAttr \u003d\u003d null) {\n      return null;\n    }\n\n    final List\u003cXAttr\u003e xattrs \u003d Lists.newArrayListWithCapacity(1);\n    xattrs.add(ecXAttr);\n    FSDirXAttrOp.unprotectedRemoveXAttrs(fsd, srcIIP.getPath(), xattrs);\n    return xattrs;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java"
    }
  }
}