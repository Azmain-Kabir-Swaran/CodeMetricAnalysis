{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSImageFormat.java",
  "functionName": "save",
  "functionId": "save___newFile-File__compression-FSImageCompression",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
  "functionStartLine": 1239,
  "functionEndLine": 1325,
  "numCommitsSeen": 127,
  "timeTaken": 5643,
  "changeHistory": [
    "ec25c7f9c7e60c077d8c4143253c20445fcdaecf",
    "3a9571308e99cc374681bbc451a517d41a150aa0",
    "8a91109d16394310f2568717f103e6fff7cbddb0",
    "c304890c8c7782d835896859f5b7f60b96c306c0",
    "49d5cff49011cc0878665204e22b5c832bc914ce",
    "1e1e93040748231dc913190aec1e031c379d8271",
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "5dae97a584d30cef3e34141edfaca49c4ec57913",
    "185e0c7b4c056b88f606362c71e4a22aae7076e0",
    "571e9c623241106dad5521a870fb8daef3f2b00a",
    "97f58955a6045b373ab73653bf26ab5922b00cf3"
  ],
  "changeHistoryShort": {
    "ec25c7f9c7e60c077d8c4143253c20445fcdaecf": "Ybodychange",
    "3a9571308e99cc374681bbc451a517d41a150aa0": "Ybodychange",
    "8a91109d16394310f2568717f103e6fff7cbddb0": "Ybodychange",
    "c304890c8c7782d835896859f5b7f60b96c306c0": "Ybodychange",
    "49d5cff49011cc0878665204e22b5c832bc914ce": "Ybodychange",
    "1e1e93040748231dc913190aec1e031c379d8271": "Ybodychange",
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63": "Ybodychange",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "5dae97a584d30cef3e34141edfaca49c4ec57913": "Ybodychange",
    "185e0c7b4c056b88f606362c71e4a22aae7076e0": "Ybodychange",
    "571e9c623241106dad5521a870fb8daef3f2b00a": "Ybodychange",
    "97f58955a6045b373ab73653bf26ab5922b00cf3": "Yintroduced"
  },
  "changeHistoryDetails": {
    "ec25c7f9c7e60c077d8c4143253c20445fcdaecf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9677. Rename generationStampV1/generationStampV2 to legacyGenerationStamp/generationStamp. Contributed by Mingliang Liu.\n",
      "commitDate": "27/01/16 4:34 PM",
      "commitName": "ec25c7f9c7e60c077d8c4143253c20445fcdaecf",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/01/16 4:31 PM",
      "commitNameOld": "3a9571308e99cc374681bbc451a517d41a150aa0",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,87 @@\n     void save(File newFile, FSImageCompression compression) throws IOException {\n       checkNotSaved();\n \n       final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n       final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n       final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n           .getSpaceConsumed().getNameSpace();\n       String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n       Step step \u003d new Step(StepType.INODES, sdPath);\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n       Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n       long startTime \u003d monotonicNow();\n       //\n       // Write out data\n       //\n       MessageDigest digester \u003d MD5Hash.getDigester();\n       FileOutputStream fout \u003d new FileOutputStream(newFile);\n       DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n       DataOutputStream out \u003d new DataOutputStream(fos);\n       try {\n         out.writeInt(LAYOUT_VERSION);\n         LayoutFlags.write(out);\n         // We use the non-locked version of getNamespaceInfo here since\n         // the coordinating thread of saveNamespace already has read-locked\n         // the namespace for us. If we attempt to take another readlock\n         // from the actual saver thread, there\u0027s a potential of a\n         // fairness-related deadlock. See the comments on HDFS-2223.\n         out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n             .getNamespaceID());\n         out.writeLong(numINodes);\n         final BlockIdManager blockIdManager \u003d sourceNamesystem.getBlockManager()\n             .getBlockIdManager();\n-        out.writeLong(blockIdManager.getGenerationStampV1());\n-        out.writeLong(blockIdManager.getGenerationStampV2());\n+        out.writeLong(blockIdManager.getLegacyGenerationStamp());\n+        out.writeLong(blockIdManager.getGenerationStamp());\n         out.writeLong(blockIdManager.getGenerationStampAtblockIdSwitch());\n         out.writeLong(blockIdManager.getLastAllocatedContiguousBlockId());\n         out.writeLong(context.getTxId());\n         out.writeLong(sourceNamesystem.dir.getLastInodeId());\n \n \n         sourceNamesystem.getSnapshotManager().write(out);\n \n         // write compression info and set up compressed stream\n         out \u003d compression.writeHeaderAndWrapStream(fos);\n         LOG.info(\"Saving image file \" + newFile +\n                  \" using \" + compression);\n \n         // save the root\n         saveINode2Image(rootDir, out, false, referenceMap, counter);\n         // save the rest of the nodes\n         saveImage(rootDir, out, true, false, counter);\n         prog.endStep(Phase.SAVING_CHECKPOINT, step);\n         // Now that the step is finished, set counter equal to total to adjust\n         // for possible under-counting due to reference inodes.\n         prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n         // save files under construction\n         // TODO: for HDFS-5428, since we cannot break the compatibility of\n         // fsimage, we store part of the under-construction files that are only\n         // in snapshots in this \"under-construction-file\" section. As a\n         // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n         // paths, so that when loading fsimage we do not put them into the lease\n         // map. In the future, we can remove this hack when we can bump the\n         // layout version.\n         saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n \n         context.checkCancelled();\n         sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n         context.checkCancelled();\n         sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n         context.checkCancelled();\n         out.flush();\n         context.checkCancelled();\n         fout.getChannel().force(true);\n       } finally {\n         out.close();\n       }\n \n       saved \u003d true;\n       // set md5 of the saved image\n       savedDigest \u003d new MD5Hash(digester.digest());\n \n       LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n           + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n           + \" seconds.\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().getNameSpace();\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d monotonicNow();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        final BlockIdManager blockIdManager \u003d sourceNamesystem.getBlockManager()\n            .getBlockIdManager();\n        out.writeLong(blockIdManager.getLegacyGenerationStamp());\n        out.writeLong(blockIdManager.getGenerationStamp());\n        out.writeLong(blockIdManager.getGenerationStampAtblockIdSwitch());\n        out.writeLong(blockIdManager.getLastAllocatedContiguousBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.dir.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n          + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n          + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "3a9571308e99cc374681bbc451a517d41a150aa0": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9677. Rename generationStampV1/generationStampV2 to legacyGenerationStamp/generationStamp. Contributed by Mingliang Liu.\"\n\nThis reverts commit 8a91109d16394310f2568717f103e6fff7cbddb0.\n",
      "commitDate": "27/01/16 4:31 PM",
      "commitName": "3a9571308e99cc374681bbc451a517d41a150aa0",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/01/16 3:48 PM",
      "commitNameOld": "8a91109d16394310f2568717f103e6fff7cbddb0",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,87 @@\n     void save(File newFile, FSImageCompression compression) throws IOException {\n       checkNotSaved();\n \n       final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n       final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n       final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n           .getSpaceConsumed().getNameSpace();\n       String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n       Step step \u003d new Step(StepType.INODES, sdPath);\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n       Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n       long startTime \u003d monotonicNow();\n       //\n       // Write out data\n       //\n       MessageDigest digester \u003d MD5Hash.getDigester();\n       FileOutputStream fout \u003d new FileOutputStream(newFile);\n       DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n       DataOutputStream out \u003d new DataOutputStream(fos);\n       try {\n         out.writeInt(LAYOUT_VERSION);\n         LayoutFlags.write(out);\n         // We use the non-locked version of getNamespaceInfo here since\n         // the coordinating thread of saveNamespace already has read-locked\n         // the namespace for us. If we attempt to take another readlock\n         // from the actual saver thread, there\u0027s a potential of a\n         // fairness-related deadlock. See the comments on HDFS-2223.\n         out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n             .getNamespaceID());\n         out.writeLong(numINodes);\n         final BlockIdManager blockIdManager \u003d sourceNamesystem.getBlockManager()\n             .getBlockIdManager();\n-        out.writeLong(blockIdManager.getLegacyGenerationStamp());\n-        out.writeLong(blockIdManager.getGenerationStamp());\n+        out.writeLong(blockIdManager.getGenerationStampV1());\n+        out.writeLong(blockIdManager.getGenerationStampV2());\n         out.writeLong(blockIdManager.getGenerationStampAtblockIdSwitch());\n         out.writeLong(blockIdManager.getLastAllocatedContiguousBlockId());\n         out.writeLong(context.getTxId());\n         out.writeLong(sourceNamesystem.dir.getLastInodeId());\n \n \n         sourceNamesystem.getSnapshotManager().write(out);\n \n         // write compression info and set up compressed stream\n         out \u003d compression.writeHeaderAndWrapStream(fos);\n         LOG.info(\"Saving image file \" + newFile +\n                  \" using \" + compression);\n \n         // save the root\n         saveINode2Image(rootDir, out, false, referenceMap, counter);\n         // save the rest of the nodes\n         saveImage(rootDir, out, true, false, counter);\n         prog.endStep(Phase.SAVING_CHECKPOINT, step);\n         // Now that the step is finished, set counter equal to total to adjust\n         // for possible under-counting due to reference inodes.\n         prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n         // save files under construction\n         // TODO: for HDFS-5428, since we cannot break the compatibility of\n         // fsimage, we store part of the under-construction files that are only\n         // in snapshots in this \"under-construction-file\" section. As a\n         // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n         // paths, so that when loading fsimage we do not put them into the lease\n         // map. In the future, we can remove this hack when we can bump the\n         // layout version.\n         saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n \n         context.checkCancelled();\n         sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n         context.checkCancelled();\n         sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n         context.checkCancelled();\n         out.flush();\n         context.checkCancelled();\n         fout.getChannel().force(true);\n       } finally {\n         out.close();\n       }\n \n       saved \u003d true;\n       // set md5 of the saved image\n       savedDigest \u003d new MD5Hash(digester.digest());\n \n       LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n           + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n           + \" seconds.\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().getNameSpace();\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d monotonicNow();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        final BlockIdManager blockIdManager \u003d sourceNamesystem.getBlockManager()\n            .getBlockIdManager();\n        out.writeLong(blockIdManager.getGenerationStampV1());\n        out.writeLong(blockIdManager.getGenerationStampV2());\n        out.writeLong(blockIdManager.getGenerationStampAtblockIdSwitch());\n        out.writeLong(blockIdManager.getLastAllocatedContiguousBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.dir.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n          + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n          + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "8a91109d16394310f2568717f103e6fff7cbddb0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9677. Rename generationStampV1/generationStampV2 to legacyGenerationStamp/generationStamp. Contributed by Mingliang Liu.\n",
      "commitDate": "27/01/16 3:48 PM",
      "commitName": "8a91109d16394310f2568717f103e6fff7cbddb0",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "21/01/16 11:13 AM",
      "commitNameOld": "c304890c8c7782d835896859f5b7f60b96c306c0",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 6.19,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,87 @@\n     void save(File newFile, FSImageCompression compression) throws IOException {\n       checkNotSaved();\n \n       final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n       final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n       final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n           .getSpaceConsumed().getNameSpace();\n       String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n       Step step \u003d new Step(StepType.INODES, sdPath);\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n       Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n       long startTime \u003d monotonicNow();\n       //\n       // Write out data\n       //\n       MessageDigest digester \u003d MD5Hash.getDigester();\n       FileOutputStream fout \u003d new FileOutputStream(newFile);\n       DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n       DataOutputStream out \u003d new DataOutputStream(fos);\n       try {\n         out.writeInt(LAYOUT_VERSION);\n         LayoutFlags.write(out);\n         // We use the non-locked version of getNamespaceInfo here since\n         // the coordinating thread of saveNamespace already has read-locked\n         // the namespace for us. If we attempt to take another readlock\n         // from the actual saver thread, there\u0027s a potential of a\n         // fairness-related deadlock. See the comments on HDFS-2223.\n         out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n             .getNamespaceID());\n         out.writeLong(numINodes);\n         final BlockIdManager blockIdManager \u003d sourceNamesystem.getBlockManager()\n             .getBlockIdManager();\n-        out.writeLong(blockIdManager.getGenerationStampV1());\n-        out.writeLong(blockIdManager.getGenerationStampV2());\n+        out.writeLong(blockIdManager.getLegacyGenerationStamp());\n+        out.writeLong(blockIdManager.getGenerationStamp());\n         out.writeLong(blockIdManager.getGenerationStampAtblockIdSwitch());\n         out.writeLong(blockIdManager.getLastAllocatedContiguousBlockId());\n         out.writeLong(context.getTxId());\n         out.writeLong(sourceNamesystem.dir.getLastInodeId());\n \n \n         sourceNamesystem.getSnapshotManager().write(out);\n \n         // write compression info and set up compressed stream\n         out \u003d compression.writeHeaderAndWrapStream(fos);\n         LOG.info(\"Saving image file \" + newFile +\n                  \" using \" + compression);\n \n         // save the root\n         saveINode2Image(rootDir, out, false, referenceMap, counter);\n         // save the rest of the nodes\n         saveImage(rootDir, out, true, false, counter);\n         prog.endStep(Phase.SAVING_CHECKPOINT, step);\n         // Now that the step is finished, set counter equal to total to adjust\n         // for possible under-counting due to reference inodes.\n         prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n         // save files under construction\n         // TODO: for HDFS-5428, since we cannot break the compatibility of\n         // fsimage, we store part of the under-construction files that are only\n         // in snapshots in this \"under-construction-file\" section. As a\n         // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n         // paths, so that when loading fsimage we do not put them into the lease\n         // map. In the future, we can remove this hack when we can bump the\n         // layout version.\n         saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n \n         context.checkCancelled();\n         sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n         context.checkCancelled();\n         sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n         context.checkCancelled();\n         out.flush();\n         context.checkCancelled();\n         fout.getChannel().force(true);\n       } finally {\n         out.close();\n       }\n \n       saved \u003d true;\n       // set md5 of the saved image\n       savedDigest \u003d new MD5Hash(digester.digest());\n \n       LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n           + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n           + \" seconds.\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().getNameSpace();\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d monotonicNow();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        final BlockIdManager blockIdManager \u003d sourceNamesystem.getBlockManager()\n            .getBlockIdManager();\n        out.writeLong(blockIdManager.getLegacyGenerationStamp());\n        out.writeLong(blockIdManager.getGenerationStamp());\n        out.writeLong(blockIdManager.getGenerationStampAtblockIdSwitch());\n        out.writeLong(blockIdManager.getLastAllocatedContiguousBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.dir.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n          + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n          + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "c304890c8c7782d835896859f5b7f60b96c306c0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9542. Move BlockIdManager from FSNamesystem to BlockManager. Contributed by Jing Zhao.\n",
      "commitDate": "21/01/16 11:13 AM",
      "commitName": "c304890c8c7782d835896859f5b7f60b96c306c0",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/01/16 9:22 AM",
      "commitNameOld": "25051c3bd08efc12333a6acb51782cc7800403a4",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 9.08,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,87 @@\n     void save(File newFile, FSImageCompression compression) throws IOException {\n       checkNotSaved();\n \n       final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n       final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n       final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n           .getSpaceConsumed().getNameSpace();\n       String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n       Step step \u003d new Step(StepType.INODES, sdPath);\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n       Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n       long startTime \u003d monotonicNow();\n       //\n       // Write out data\n       //\n       MessageDigest digester \u003d MD5Hash.getDigester();\n       FileOutputStream fout \u003d new FileOutputStream(newFile);\n       DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n       DataOutputStream out \u003d new DataOutputStream(fos);\n       try {\n         out.writeInt(LAYOUT_VERSION);\n         LayoutFlags.write(out);\n         // We use the non-locked version of getNamespaceInfo here since\n         // the coordinating thread of saveNamespace already has read-locked\n         // the namespace for us. If we attempt to take another readlock\n         // from the actual saver thread, there\u0027s a potential of a\n         // fairness-related deadlock. See the comments on HDFS-2223.\n         out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n             .getNamespaceID());\n         out.writeLong(numINodes);\n-        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n-        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n-        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n-        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedContiguousBlockId());\n+        final BlockIdManager blockIdManager \u003d sourceNamesystem.getBlockManager()\n+            .getBlockIdManager();\n+        out.writeLong(blockIdManager.getGenerationStampV1());\n+        out.writeLong(blockIdManager.getGenerationStampV2());\n+        out.writeLong(blockIdManager.getGenerationStampAtblockIdSwitch());\n+        out.writeLong(blockIdManager.getLastAllocatedContiguousBlockId());\n         out.writeLong(context.getTxId());\n         out.writeLong(sourceNamesystem.dir.getLastInodeId());\n \n \n         sourceNamesystem.getSnapshotManager().write(out);\n \n         // write compression info and set up compressed stream\n         out \u003d compression.writeHeaderAndWrapStream(fos);\n         LOG.info(\"Saving image file \" + newFile +\n                  \" using \" + compression);\n \n         // save the root\n         saveINode2Image(rootDir, out, false, referenceMap, counter);\n         // save the rest of the nodes\n         saveImage(rootDir, out, true, false, counter);\n         prog.endStep(Phase.SAVING_CHECKPOINT, step);\n         // Now that the step is finished, set counter equal to total to adjust\n         // for possible under-counting due to reference inodes.\n         prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n         // save files under construction\n         // TODO: for HDFS-5428, since we cannot break the compatibility of\n         // fsimage, we store part of the under-construction files that are only\n         // in snapshots in this \"under-construction-file\" section. As a\n         // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n         // paths, so that when loading fsimage we do not put them into the lease\n         // map. In the future, we can remove this hack when we can bump the\n         // layout version.\n         saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n \n         context.checkCancelled();\n         sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n         context.checkCancelled();\n         sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n         context.checkCancelled();\n         out.flush();\n         context.checkCancelled();\n         fout.getChannel().force(true);\n       } finally {\n         out.close();\n       }\n \n       saved \u003d true;\n       // set md5 of the saved image\n       savedDigest \u003d new MD5Hash(digester.digest());\n \n       LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n           + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n           + \" seconds.\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().getNameSpace();\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d monotonicNow();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        final BlockIdManager blockIdManager \u003d sourceNamesystem.getBlockManager()\n            .getBlockIdManager();\n        out.writeLong(blockIdManager.getGenerationStampV1());\n        out.writeLong(blockIdManager.getGenerationStampV2());\n        out.writeLong(blockIdManager.getGenerationStampAtblockIdSwitch());\n        out.writeLong(blockIdManager.getLastAllocatedContiguousBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.dir.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n          + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n          + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "49d5cff49011cc0878665204e22b5c832bc914ce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8559. Erasure Coding: fix non-protobuf fsimage for striped blocks. (Jing Zhao via yliu)\n",
      "commitDate": "14/06/15 12:39 AM",
      "commitName": "49d5cff49011cc0878665204e22b5c832bc914ce",
      "commitAuthor": "yliu",
      "commitDateOld": "26/05/15 12:02 PM",
      "commitNameOld": "c9103e9cacc67a614940e32fa87c5dbc3daa60de",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 18.53,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,85 @@\n     void save(File newFile, FSImageCompression compression) throws IOException {\n       checkNotSaved();\n \n       final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n       final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n       final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n           .getSpaceConsumed().getNameSpace();\n       String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n       Step step \u003d new Step(StepType.INODES, sdPath);\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n       Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n       long startTime \u003d monotonicNow();\n       //\n       // Write out data\n       //\n       MessageDigest digester \u003d MD5Hash.getDigester();\n       FileOutputStream fout \u003d new FileOutputStream(newFile);\n       DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n       DataOutputStream out \u003d new DataOutputStream(fos);\n       try {\n         out.writeInt(LAYOUT_VERSION);\n         LayoutFlags.write(out);\n         // We use the non-locked version of getNamespaceInfo here since\n         // the coordinating thread of saveNamespace already has read-locked\n         // the namespace for us. If we attempt to take another readlock\n         // from the actual saver thread, there\u0027s a potential of a\n         // fairness-related deadlock. See the comments on HDFS-2223.\n         out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n             .getNamespaceID());\n         out.writeLong(numINodes);\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedContiguousBlockId());\n-        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedStripedBlockId());\n         out.writeLong(context.getTxId());\n         out.writeLong(sourceNamesystem.dir.getLastInodeId());\n \n \n         sourceNamesystem.getSnapshotManager().write(out);\n \n         // write compression info and set up compressed stream\n         out \u003d compression.writeHeaderAndWrapStream(fos);\n         LOG.info(\"Saving image file \" + newFile +\n                  \" using \" + compression);\n \n         // save the root\n         saveINode2Image(rootDir, out, false, referenceMap, counter);\n         // save the rest of the nodes\n         saveImage(rootDir, out, true, false, counter);\n         prog.endStep(Phase.SAVING_CHECKPOINT, step);\n         // Now that the step is finished, set counter equal to total to adjust\n         // for possible under-counting due to reference inodes.\n         prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n         // save files under construction\n         // TODO: for HDFS-5428, since we cannot break the compatibility of\n         // fsimage, we store part of the under-construction files that are only\n         // in snapshots in this \"under-construction-file\" section. As a\n         // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n         // paths, so that when loading fsimage we do not put them into the lease\n         // map. In the future, we can remove this hack when we can bump the\n         // layout version.\n         saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n \n         context.checkCancelled();\n         sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n         context.checkCancelled();\n         sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n         context.checkCancelled();\n         out.flush();\n         context.checkCancelled();\n         fout.getChannel().force(true);\n       } finally {\n         out.close();\n       }\n \n       saved \u003d true;\n       // set md5 of the saved image\n       savedDigest \u003d new MD5Hash(digester.digest());\n \n       LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n           + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n           + \" seconds.\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().getNameSpace();\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d monotonicNow();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedContiguousBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.dir.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n          + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n          + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "1e1e93040748231dc913190aec1e031c379d8271": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7837. Erasure Coding: allocate and persist striped blocks in NameNode. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:32 AM",
      "commitName": "1e1e93040748231dc913190aec1e031c379d8271",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:07 AM",
      "commitNameOld": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,86 @@\n     void save(File newFile, FSImageCompression compression) throws IOException {\n       checkNotSaved();\n \n       final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n       final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n       final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n           .getSpaceConsumed().getNameSpace();\n       String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n       Step step \u003d new Step(StepType.INODES, sdPath);\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n       Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n       long startTime \u003d monotonicNow();\n       //\n       // Write out data\n       //\n       MessageDigest digester \u003d MD5Hash.getDigester();\n       FileOutputStream fout \u003d new FileOutputStream(newFile);\n       DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n       DataOutputStream out \u003d new DataOutputStream(fos);\n       try {\n         out.writeInt(LAYOUT_VERSION);\n         LayoutFlags.write(out);\n         // We use the non-locked version of getNamespaceInfo here since\n         // the coordinating thread of saveNamespace already has read-locked\n         // the namespace for us. If we attempt to take another readlock\n         // from the actual saver thread, there\u0027s a potential of a\n         // fairness-related deadlock. See the comments on HDFS-2223.\n         out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n             .getNamespaceID());\n         out.writeLong(numINodes);\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n-        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedBlockId());\n+        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedContiguousBlockId());\n+        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedStripedBlockId());\n         out.writeLong(context.getTxId());\n         out.writeLong(sourceNamesystem.dir.getLastInodeId());\n \n \n         sourceNamesystem.getSnapshotManager().write(out);\n \n         // write compression info and set up compressed stream\n         out \u003d compression.writeHeaderAndWrapStream(fos);\n         LOG.info(\"Saving image file \" + newFile +\n                  \" using \" + compression);\n \n         // save the root\n         saveINode2Image(rootDir, out, false, referenceMap, counter);\n         // save the rest of the nodes\n         saveImage(rootDir, out, true, false, counter);\n         prog.endStep(Phase.SAVING_CHECKPOINT, step);\n         // Now that the step is finished, set counter equal to total to adjust\n         // for possible under-counting due to reference inodes.\n         prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n         // save files under construction\n         // TODO: for HDFS-5428, since we cannot break the compatibility of\n         // fsimage, we store part of the under-construction files that are only\n         // in snapshots in this \"under-construction-file\" section. As a\n         // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n         // paths, so that when loading fsimage we do not put them into the lease\n         // map. In the future, we can remove this hack when we can bump the\n         // layout version.\n         saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n \n         context.checkCancelled();\n         sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n         context.checkCancelled();\n         sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n         context.checkCancelled();\n         out.flush();\n         context.checkCancelled();\n         fout.getChannel().force(true);\n       } finally {\n         out.close();\n       }\n \n       saved \u003d true;\n       // set md5 of the saved image\n       savedDigest \u003d new MD5Hash(digester.digest());\n \n       LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n           + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n           + \" seconds.\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().getNameSpace();\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d monotonicNow();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedContiguousBlockId());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedStripedBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.dir.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n          + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n          + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6757. Simplify lease manager with INodeID. Contributed by Haohui Mai.\n",
      "commitDate": "08/05/15 11:04 PM",
      "commitName": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.54,
      "commitsBetweenForRepo": 129,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,85 @@\n     void save(File newFile, FSImageCompression compression) throws IOException {\n       checkNotSaved();\n \n       final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n       final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n       final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n           .getSpaceConsumed().getNameSpace();\n       String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n       Step step \u003d new Step(StepType.INODES, sdPath);\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n       Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n       long startTime \u003d monotonicNow();\n       //\n       // Write out data\n       //\n       MessageDigest digester \u003d MD5Hash.getDigester();\n       FileOutputStream fout \u003d new FileOutputStream(newFile);\n       DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n       DataOutputStream out \u003d new DataOutputStream(fos);\n       try {\n         out.writeInt(LAYOUT_VERSION);\n         LayoutFlags.write(out);\n         // We use the non-locked version of getNamespaceInfo here since\n         // the coordinating thread of saveNamespace already has read-locked\n         // the namespace for us. If we attempt to take another readlock\n         // from the actual saver thread, there\u0027s a potential of a\n         // fairness-related deadlock. See the comments on HDFS-2223.\n         out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n             .getNamespaceID());\n         out.writeLong(numINodes);\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedBlockId());\n         out.writeLong(context.getTxId());\n         out.writeLong(sourceNamesystem.dir.getLastInodeId());\n \n \n         sourceNamesystem.getSnapshotManager().write(out);\n \n         // write compression info and set up compressed stream\n         out \u003d compression.writeHeaderAndWrapStream(fos);\n         LOG.info(\"Saving image file \" + newFile +\n                  \" using \" + compression);\n \n         // save the root\n         saveINode2Image(rootDir, out, false, referenceMap, counter);\n         // save the rest of the nodes\n         saveImage(rootDir, out, true, false, counter);\n         prog.endStep(Phase.SAVING_CHECKPOINT, step);\n         // Now that the step is finished, set counter equal to total to adjust\n         // for possible under-counting due to reference inodes.\n         prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n         // save files under construction\n         // TODO: for HDFS-5428, since we cannot break the compatibility of\n         // fsimage, we store part of the under-construction files that are only\n         // in snapshots in this \"under-construction-file\" section. As a\n         // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n         // paths, so that when loading fsimage we do not put them into the lease\n         // map. In the future, we can remove this hack when we can bump the\n         // layout version.\n-        sourceNamesystem.saveFilesUnderConstruction(out, snapshotUCMap);\n+        saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n \n         context.checkCancelled();\n         sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n         context.checkCancelled();\n         sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n         context.checkCancelled();\n         out.flush();\n         context.checkCancelled();\n         fout.getChannel().force(true);\n       } finally {\n         out.close();\n       }\n \n       saved \u003d true;\n       // set md5 of the saved image\n       savedDigest \u003d new MD5Hash(digester.digest());\n \n       LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n           + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n           + \" seconds.\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().getNameSpace();\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d monotonicNow();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.dir.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        saveFilesUnderConstruction(sourceNamesystem, out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n          + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n          + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "13/02/15 9:01 PM",
      "commitNameOld": "f2231cebcddc80f0b753c4a7cb45ee4040846951",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 34.58,
      "commitsBetweenForRepo": 294,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,85 @@\n     void save(File newFile, FSImageCompression compression) throws IOException {\n       checkNotSaved();\n \n       final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n       final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n       final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n           .getSpaceConsumed().getNameSpace();\n       String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n       Step step \u003d new Step(StepType.INODES, sdPath);\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n       Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n-      long startTime \u003d now();\n+      long startTime \u003d monotonicNow();\n       //\n       // Write out data\n       //\n       MessageDigest digester \u003d MD5Hash.getDigester();\n       FileOutputStream fout \u003d new FileOutputStream(newFile);\n       DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n       DataOutputStream out \u003d new DataOutputStream(fos);\n       try {\n         out.writeInt(LAYOUT_VERSION);\n         LayoutFlags.write(out);\n         // We use the non-locked version of getNamespaceInfo here since\n         // the coordinating thread of saveNamespace already has read-locked\n         // the namespace for us. If we attempt to take another readlock\n         // from the actual saver thread, there\u0027s a potential of a\n         // fairness-related deadlock. See the comments on HDFS-2223.\n         out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n             .getNamespaceID());\n         out.writeLong(numINodes);\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedBlockId());\n         out.writeLong(context.getTxId());\n         out.writeLong(sourceNamesystem.dir.getLastInodeId());\n \n \n         sourceNamesystem.getSnapshotManager().write(out);\n \n         // write compression info and set up compressed stream\n         out \u003d compression.writeHeaderAndWrapStream(fos);\n         LOG.info(\"Saving image file \" + newFile +\n                  \" using \" + compression);\n \n         // save the root\n         saveINode2Image(rootDir, out, false, referenceMap, counter);\n         // save the rest of the nodes\n         saveImage(rootDir, out, true, false, counter);\n         prog.endStep(Phase.SAVING_CHECKPOINT, step);\n         // Now that the step is finished, set counter equal to total to adjust\n         // for possible under-counting due to reference inodes.\n         prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n         // save files under construction\n         // TODO: for HDFS-5428, since we cannot break the compatibility of\n         // fsimage, we store part of the under-construction files that are only\n         // in snapshots in this \"under-construction-file\" section. As a\n         // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n         // paths, so that when loading fsimage we do not put them into the lease\n         // map. In the future, we can remove this hack when we can bump the\n         // layout version.\n         sourceNamesystem.saveFilesUnderConstruction(out, snapshotUCMap);\n \n         context.checkCancelled();\n         sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n         context.checkCancelled();\n         sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n         context.checkCancelled();\n         out.flush();\n         context.checkCancelled();\n         fout.getChannel().force(true);\n       } finally {\n         out.close();\n       }\n \n       saved \u003d true;\n       // set md5 of the saved image\n       savedDigest \u003d new MD5Hash(digester.digest());\n \n-      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length() +\n-          \" bytes saved in \" + (now() - startTime)/1000 + \" seconds.\");\n+      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n+          + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n+          + \" seconds.\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().getNameSpace();\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d monotonicNow();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.dir.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        sourceNamesystem.saveFilesUnderConstruction(out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length()\n          + \" bytes saved in \" + (monotonicNow() - startTime) / 1000\n          + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "5dae97a584d30cef3e34141edfaca49c4ec57913": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
      "commitDate": "11/02/15 10:41 AM",
      "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "08/02/15 11:51 AM",
      "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.95,
      "commitsBetweenForRepo": 40,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,84 @@\n     void save(File newFile, FSImageCompression compression) throws IOException {\n       checkNotSaved();\n \n       final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n       final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n       final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n-          .getSpaceConsumed().get(Quota.NAMESPACE);\n+          .getSpaceConsumed().getNameSpace();\n       String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n       Step step \u003d new Step(StepType.INODES, sdPath);\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n       Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n       long startTime \u003d now();\n       //\n       // Write out data\n       //\n       MessageDigest digester \u003d MD5Hash.getDigester();\n       FileOutputStream fout \u003d new FileOutputStream(newFile);\n       DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n       DataOutputStream out \u003d new DataOutputStream(fos);\n       try {\n         out.writeInt(LAYOUT_VERSION);\n         LayoutFlags.write(out);\n         // We use the non-locked version of getNamespaceInfo here since\n         // the coordinating thread of saveNamespace already has read-locked\n         // the namespace for us. If we attempt to take another readlock\n         // from the actual saver thread, there\u0027s a potential of a\n         // fairness-related deadlock. See the comments on HDFS-2223.\n         out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n             .getNamespaceID());\n         out.writeLong(numINodes);\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedBlockId());\n         out.writeLong(context.getTxId());\n         out.writeLong(sourceNamesystem.dir.getLastInodeId());\n \n \n         sourceNamesystem.getSnapshotManager().write(out);\n \n         // write compression info and set up compressed stream\n         out \u003d compression.writeHeaderAndWrapStream(fos);\n         LOG.info(\"Saving image file \" + newFile +\n                  \" using \" + compression);\n \n         // save the root\n         saveINode2Image(rootDir, out, false, referenceMap, counter);\n         // save the rest of the nodes\n         saveImage(rootDir, out, true, false, counter);\n         prog.endStep(Phase.SAVING_CHECKPOINT, step);\n         // Now that the step is finished, set counter equal to total to adjust\n         // for possible under-counting due to reference inodes.\n         prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n         // save files under construction\n         // TODO: for HDFS-5428, since we cannot break the compatibility of\n         // fsimage, we store part of the under-construction files that are only\n         // in snapshots in this \"under-construction-file\" section. As a\n         // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n         // paths, so that when loading fsimage we do not put them into the lease\n         // map. In the future, we can remove this hack when we can bump the\n         // layout version.\n         sourceNamesystem.saveFilesUnderConstruction(out, snapshotUCMap);\n \n         context.checkCancelled();\n         sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n         context.checkCancelled();\n         sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n         context.checkCancelled();\n         out.flush();\n         context.checkCancelled();\n         fout.getChannel().force(true);\n       } finally {\n         out.close();\n       }\n \n       saved \u003d true;\n       // set md5 of the saved image\n       savedDigest \u003d new MD5Hash(digester.digest());\n \n       LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length() +\n           \" bytes saved in \" + (now() - startTime)/1000 + \" seconds.\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().getNameSpace();\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d now();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.dir.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        sourceNamesystem.saveFilesUnderConstruction(out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length() +\n          \" bytes saved in \" + (now() - startTime)/1000 + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "185e0c7b4c056b88f606362c71e4a22aae7076e0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7462. Consolidate implementation of mkdirs() into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "02/12/14 2:53 PM",
      "commitName": "185e0c7b4c056b88f606362c71e4a22aae7076e0",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "25/11/14 3:37 PM",
      "commitNameOld": "f43a20c529ac3f104add95b222de6580757b3763",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 6.97,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,84 @@\n     void save(File newFile, FSImageCompression compression) throws IOException {\n       checkNotSaved();\n \n       final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n       final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n       final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n           .getSpaceConsumed().get(Quota.NAMESPACE);\n       String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n       Step step \u003d new Step(StepType.INODES, sdPath);\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n       Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n       long startTime \u003d now();\n       //\n       // Write out data\n       //\n       MessageDigest digester \u003d MD5Hash.getDigester();\n       FileOutputStream fout \u003d new FileOutputStream(newFile);\n       DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n       DataOutputStream out \u003d new DataOutputStream(fos);\n       try {\n         out.writeInt(LAYOUT_VERSION);\n         LayoutFlags.write(out);\n         // We use the non-locked version of getNamespaceInfo here since\n         // the coordinating thread of saveNamespace already has read-locked\n         // the namespace for us. If we attempt to take another readlock\n         // from the actual saver thread, there\u0027s a potential of a\n         // fairness-related deadlock. See the comments on HDFS-2223.\n         out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n             .getNamespaceID());\n         out.writeLong(numINodes);\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n         out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedBlockId());\n         out.writeLong(context.getTxId());\n-        out.writeLong(sourceNamesystem.getLastInodeId());\n+        out.writeLong(sourceNamesystem.dir.getLastInodeId());\n \n \n         sourceNamesystem.getSnapshotManager().write(out);\n \n         // write compression info and set up compressed stream\n         out \u003d compression.writeHeaderAndWrapStream(fos);\n         LOG.info(\"Saving image file \" + newFile +\n                  \" using \" + compression);\n \n         // save the root\n         saveINode2Image(rootDir, out, false, referenceMap, counter);\n         // save the rest of the nodes\n         saveImage(rootDir, out, true, false, counter);\n         prog.endStep(Phase.SAVING_CHECKPOINT, step);\n         // Now that the step is finished, set counter equal to total to adjust\n         // for possible under-counting due to reference inodes.\n         prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n         // save files under construction\n         // TODO: for HDFS-5428, since we cannot break the compatibility of\n         // fsimage, we store part of the under-construction files that are only\n         // in snapshots in this \"under-construction-file\" section. As a\n         // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n         // paths, so that when loading fsimage we do not put them into the lease\n         // map. In the future, we can remove this hack when we can bump the\n         // layout version.\n         sourceNamesystem.saveFilesUnderConstruction(out, snapshotUCMap);\n \n         context.checkCancelled();\n         sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n         context.checkCancelled();\n         sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n         context.checkCancelled();\n         out.flush();\n         context.checkCancelled();\n         fout.getChannel().force(true);\n       } finally {\n         out.close();\n       }\n \n       saved \u003d true;\n       // set md5 of the saved image\n       savedDigest \u003d new MD5Hash(digester.digest());\n \n       LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length() +\n           \" bytes saved in \" + (now() - startTime)/1000 + \" seconds.\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().get(Quota.NAMESPACE);\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d now();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.dir.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        sourceNamesystem.saveFilesUnderConstruction(out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length() +\n          \" bytes saved in \" + (now() - startTime)/1000 + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "571e9c623241106dad5521a870fb8daef3f2b00a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7381. Decouple the management of block id and gen stamps from FSNamesystem. Contributed by Haohui Mai.\n",
      "commitDate": "11/11/14 12:42 PM",
      "commitName": "571e9c623241106dad5521a870fb8daef3f2b00a",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "15/10/14 10:27 AM",
      "commitNameOld": "18620649f96d9e378fb7ea40de216284a9d525c7",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 27.14,
      "commitsBetweenForRepo": 268,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,84 @@\n     void save(File newFile, FSImageCompression compression) throws IOException {\n       checkNotSaved();\n \n       final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n       final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n       final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n           .getSpaceConsumed().get(Quota.NAMESPACE);\n       String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n       Step step \u003d new Step(StepType.INODES, sdPath);\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n       Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n       long startTime \u003d now();\n       //\n       // Write out data\n       //\n       MessageDigest digester \u003d MD5Hash.getDigester();\n       FileOutputStream fout \u003d new FileOutputStream(newFile);\n       DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n       DataOutputStream out \u003d new DataOutputStream(fos);\n       try {\n         out.writeInt(LAYOUT_VERSION);\n         LayoutFlags.write(out);\n         // We use the non-locked version of getNamespaceInfo here since\n         // the coordinating thread of saveNamespace already has read-locked\n         // the namespace for us. If we attempt to take another readlock\n         // from the actual saver thread, there\u0027s a potential of a\n         // fairness-related deadlock. See the comments on HDFS-2223.\n         out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n             .getNamespaceID());\n         out.writeLong(numINodes);\n-        out.writeLong(sourceNamesystem.getGenerationStampV1());\n-        out.writeLong(sourceNamesystem.getGenerationStampV2());\n-        out.writeLong(sourceNamesystem.getGenerationStampAtblockIdSwitch());\n-        out.writeLong(sourceNamesystem.getLastAllocatedBlockId());\n+        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n+        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n+        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n+        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedBlockId());\n         out.writeLong(context.getTxId());\n         out.writeLong(sourceNamesystem.getLastInodeId());\n \n \n         sourceNamesystem.getSnapshotManager().write(out);\n \n         // write compression info and set up compressed stream\n         out \u003d compression.writeHeaderAndWrapStream(fos);\n         LOG.info(\"Saving image file \" + newFile +\n                  \" using \" + compression);\n \n         // save the root\n         saveINode2Image(rootDir, out, false, referenceMap, counter);\n         // save the rest of the nodes\n         saveImage(rootDir, out, true, false, counter);\n         prog.endStep(Phase.SAVING_CHECKPOINT, step);\n         // Now that the step is finished, set counter equal to total to adjust\n         // for possible under-counting due to reference inodes.\n         prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n         // save files under construction\n         // TODO: for HDFS-5428, since we cannot break the compatibility of\n         // fsimage, we store part of the under-construction files that are only\n         // in snapshots in this \"under-construction-file\" section. As a\n         // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n         // paths, so that when loading fsimage we do not put them into the lease\n         // map. In the future, we can remove this hack when we can bump the\n         // layout version.\n         sourceNamesystem.saveFilesUnderConstruction(out, snapshotUCMap);\n \n         context.checkCancelled();\n         sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n         context.checkCancelled();\n         sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n         context.checkCancelled();\n         out.flush();\n         context.checkCancelled();\n         fout.getChannel().force(true);\n       } finally {\n         out.close();\n       }\n \n       saved \u003d true;\n       // set md5 of the saved image\n       savedDigest \u003d new MD5Hash(digester.digest());\n \n       LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length() +\n           \" bytes saved in \" + (now() - startTime)/1000 + \" seconds.\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().get(Quota.NAMESPACE);\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d now();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV1());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampV2());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getGenerationStampAtblockIdSwitch());\n        out.writeLong(sourceNamesystem.getBlockIdManager().getLastAllocatedBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        sourceNamesystem.saveFilesUnderConstruction(out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length() +\n          \" bytes saved in \" + (now() - startTime)/1000 + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "97f58955a6045b373ab73653bf26ab5922b00cf3": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-6293. Issues with OIV processing PB-based fsimages. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594439 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/05/14 6:15 PM",
      "commitName": "97f58955a6045b373ab73653bf26ab5922b00cf3",
      "commitAuthor": "Kihwal Lee",
      "diff": "@@ -0,0 +1,84 @@\n+    void save(File newFile, FSImageCompression compression) throws IOException {\n+      checkNotSaved();\n+\n+      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n+      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n+      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n+          .getSpaceConsumed().get(Quota.NAMESPACE);\n+      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n+      Step step \u003d new Step(StepType.INODES, sdPath);\n+      StartupProgress prog \u003d NameNode.getStartupProgress();\n+      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n+      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n+      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n+      long startTime \u003d now();\n+      //\n+      // Write out data\n+      //\n+      MessageDigest digester \u003d MD5Hash.getDigester();\n+      FileOutputStream fout \u003d new FileOutputStream(newFile);\n+      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n+      DataOutputStream out \u003d new DataOutputStream(fos);\n+      try {\n+        out.writeInt(LAYOUT_VERSION);\n+        LayoutFlags.write(out);\n+        // We use the non-locked version of getNamespaceInfo here since\n+        // the coordinating thread of saveNamespace already has read-locked\n+        // the namespace for us. If we attempt to take another readlock\n+        // from the actual saver thread, there\u0027s a potential of a\n+        // fairness-related deadlock. See the comments on HDFS-2223.\n+        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n+            .getNamespaceID());\n+        out.writeLong(numINodes);\n+        out.writeLong(sourceNamesystem.getGenerationStampV1());\n+        out.writeLong(sourceNamesystem.getGenerationStampV2());\n+        out.writeLong(sourceNamesystem.getGenerationStampAtblockIdSwitch());\n+        out.writeLong(sourceNamesystem.getLastAllocatedBlockId());\n+        out.writeLong(context.getTxId());\n+        out.writeLong(sourceNamesystem.getLastInodeId());\n+\n+\n+        sourceNamesystem.getSnapshotManager().write(out);\n+\n+        // write compression info and set up compressed stream\n+        out \u003d compression.writeHeaderAndWrapStream(fos);\n+        LOG.info(\"Saving image file \" + newFile +\n+                 \" using \" + compression);\n+\n+        // save the root\n+        saveINode2Image(rootDir, out, false, referenceMap, counter);\n+        // save the rest of the nodes\n+        saveImage(rootDir, out, true, false, counter);\n+        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n+        // Now that the step is finished, set counter equal to total to adjust\n+        // for possible under-counting due to reference inodes.\n+        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n+        // save files under construction\n+        // TODO: for HDFS-5428, since we cannot break the compatibility of\n+        // fsimage, we store part of the under-construction files that are only\n+        // in snapshots in this \"under-construction-file\" section. As a\n+        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n+        // paths, so that when loading fsimage we do not put them into the lease\n+        // map. In the future, we can remove this hack when we can bump the\n+        // layout version.\n+        sourceNamesystem.saveFilesUnderConstruction(out, snapshotUCMap);\n+\n+        context.checkCancelled();\n+        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n+        context.checkCancelled();\n+        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n+        context.checkCancelled();\n+        out.flush();\n+        context.checkCancelled();\n+        fout.getChannel().force(true);\n+      } finally {\n+        out.close();\n+      }\n+\n+      saved \u003d true;\n+      // set md5 of the saved image\n+      savedDigest \u003d new MD5Hash(digester.digest());\n+\n+      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length() +\n+          \" bytes saved in \" + (now() - startTime)/1000 + \" seconds.\");\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    void save(File newFile, FSImageCompression compression) throws IOException {\n      checkNotSaved();\n\n      final FSNamesystem sourceNamesystem \u003d context.getSourceNamesystem();\n      final INodeDirectory rootDir \u003d sourceNamesystem.dir.rootDir;\n      final long numINodes \u003d rootDir.getDirectoryWithQuotaFeature()\n          .getSpaceConsumed().get(Quota.NAMESPACE);\n      String sdPath \u003d newFile.getParentFile().getParentFile().getAbsolutePath();\n      Step step \u003d new Step(StepType.INODES, sdPath);\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      prog.setTotal(Phase.SAVING_CHECKPOINT, step, numINodes);\n      Counter counter \u003d prog.getCounter(Phase.SAVING_CHECKPOINT, step);\n      long startTime \u003d now();\n      //\n      // Write out data\n      //\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      FileOutputStream fout \u003d new FileOutputStream(newFile);\n      DigestOutputStream fos \u003d new DigestOutputStream(fout, digester);\n      DataOutputStream out \u003d new DataOutputStream(fos);\n      try {\n        out.writeInt(LAYOUT_VERSION);\n        LayoutFlags.write(out);\n        // We use the non-locked version of getNamespaceInfo here since\n        // the coordinating thread of saveNamespace already has read-locked\n        // the namespace for us. If we attempt to take another readlock\n        // from the actual saver thread, there\u0027s a potential of a\n        // fairness-related deadlock. See the comments on HDFS-2223.\n        out.writeInt(sourceNamesystem.unprotectedGetNamespaceInfo()\n            .getNamespaceID());\n        out.writeLong(numINodes);\n        out.writeLong(sourceNamesystem.getGenerationStampV1());\n        out.writeLong(sourceNamesystem.getGenerationStampV2());\n        out.writeLong(sourceNamesystem.getGenerationStampAtblockIdSwitch());\n        out.writeLong(sourceNamesystem.getLastAllocatedBlockId());\n        out.writeLong(context.getTxId());\n        out.writeLong(sourceNamesystem.getLastInodeId());\n\n\n        sourceNamesystem.getSnapshotManager().write(out);\n\n        // write compression info and set up compressed stream\n        out \u003d compression.writeHeaderAndWrapStream(fos);\n        LOG.info(\"Saving image file \" + newFile +\n                 \" using \" + compression);\n\n        // save the root\n        saveINode2Image(rootDir, out, false, referenceMap, counter);\n        // save the rest of the nodes\n        saveImage(rootDir, out, true, false, counter);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n        // Now that the step is finished, set counter equal to total to adjust\n        // for possible under-counting due to reference inodes.\n        prog.setCount(Phase.SAVING_CHECKPOINT, step, numINodes);\n        // save files under construction\n        // TODO: for HDFS-5428, since we cannot break the compatibility of\n        // fsimage, we store part of the under-construction files that are only\n        // in snapshots in this \"under-construction-file\" section. As a\n        // temporary solution, we use \"/.reserved/.inodes/\u003cinodeid\u003e\" as their\n        // paths, so that when loading fsimage we do not put them into the lease\n        // map. In the future, we can remove this hack when we can bump the\n        // layout version.\n        sourceNamesystem.saveFilesUnderConstruction(out, snapshotUCMap);\n\n        context.checkCancelled();\n        sourceNamesystem.saveSecretManagerStateCompat(out, sdPath);\n        context.checkCancelled();\n        sourceNamesystem.getCacheManager().saveStateCompat(out, sdPath);\n        context.checkCancelled();\n        out.flush();\n        context.checkCancelled();\n        fout.getChannel().force(true);\n      } finally {\n        out.close();\n      }\n\n      saved \u003d true;\n      // set md5 of the saved image\n      savedDigest \u003d new MD5Hash(digester.digest());\n\n      LOG.info(\"Image file \" + newFile + \" of size \" + newFile.length() +\n          \" bytes saved in \" + (now() - startTime)/1000 + \" seconds.\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java"
    }
  }
}