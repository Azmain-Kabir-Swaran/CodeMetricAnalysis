{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HHXORErasureDecodingStep.java",
  "functionName": "doDecodeSingle",
  "functionId": "doDecodeSingle___inputs-ByteBuffer[][]__outputs-ByteBuffer[][]__erasedLocationToFix-int__bufSize-int__isDirect-boolean",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java",
  "functionStartLine": 123,
  "functionEndLine": 222,
  "numCommitsSeen": 3,
  "timeTaken": 995,
  "changeHistory": [
    "31ebccc96238136560f4210bdf6766fe18e0650c",
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743"
  ],
  "changeHistoryShort": {
    "31ebccc96238136560f4210bdf6766fe18e0650c": "Yexceptionschange",
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743": "Yintroduced"
  },
  "changeHistoryDetails": {
    "31ebccc96238136560f4210bdf6766fe18e0650c": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-12613. Native EC coder should implement release() as idempotent function. (Lei (Eddy) Xu)\n",
      "commitDate": "16/10/17 7:44 PM",
      "commitName": "31ebccc96238136560f4210bdf6766fe18e0650c",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "17/10/16 11:02 PM",
      "commitNameOld": "c023c748869063fb67d14ea996569c42578d1cea",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 363.86,
      "commitsBetweenForRepo": 2347,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,100 +1,100 @@\n   private void doDecodeSingle(ByteBuffer[][] inputs, ByteBuffer[][] outputs,\n                               int erasedLocationToFix, int bufSize,\n-                              boolean isDirect) {\n+                              boolean isDirect) throws IOException {\n     final int numDataUnits \u003d rsRawDecoder.getNumDataUnits();\n     final int numParityUnits \u003d rsRawDecoder.getNumParityUnits();\n     final int subPacketSize \u003d getSubPacketSize();\n \n     int[][] inputPositions \u003d new int[subPacketSize][inputs[0].length];\n     for (int i \u003d 0; i \u003c subPacketSize; ++i) {\n       for (int j \u003d 0; j \u003c inputs[i].length; ++j) {\n         if (inputs[i][j] !\u003d null) {\n           inputPositions[i][j] \u003d inputs[i][j].position();\n         }\n       }\n     }\n \n     ByteBuffer[] tempInputs \u003d new ByteBuffer[numDataUnits + numParityUnits];\n     for (int i \u003d 0; i \u003c tempInputs.length; ++i) {\n       tempInputs[i] \u003d inputs[1][i];\n     }\n \n     ByteBuffer[][] tmpOutputs \u003d new ByteBuffer[subPacketSize][numParityUnits];\n     for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n       for (int j \u003d 0; j \u003c erasedIndexes.length; ++j) {\n         tmpOutputs[i][j] \u003d outputs[i][j];\n       }\n \n       for (int m \u003d erasedIndexes.length; m \u003c numParityUnits; ++m) {\n         tmpOutputs[i][m] \u003d HHUtil.allocateByteBuffer(isDirect, bufSize);\n       }\n     }\n \n     // First consider the second subPacket\n     int[] erasedLocation \u003d new int[numParityUnits];\n     erasedLocation[0] \u003d erasedLocationToFix;\n \n     // assign the erased locations based on the locations not read for\n     // second subPacket but from decoding\n     for (int i \u003d 1; i \u003c numParityUnits; i++) {\n       erasedLocation[i] \u003d numDataUnits + i;\n       tempInputs[numDataUnits + i] \u003d null;\n     }\n \n     rsRawDecoder.decode(tempInputs, erasedLocation, tmpOutputs[1]);\n \n     int piggyBackParityIndex \u003d piggyBackFullIndex[erasedLocationToFix];\n     ByteBuffer piggyBack \u003d HHUtil.getPiggyBackForDecode(inputs, tmpOutputs,\n             piggyBackParityIndex, numDataUnits, numParityUnits, pbIndex);\n \n     // Second consider the first subPacket.\n     // get the value of the piggyback associated with the erased location\n     if (isDirect) {\n       // decode the erased value in the first subPacket by using the piggyback\n       int idxToWrite \u003d 0;\n       doDecodeByPiggyBack(inputs[0], tmpOutputs[0][idxToWrite], piggyBack,\n               erasedLocationToFix);\n     } else {\n       ByteBuffer buffer;\n       byte[][][] newInputs \u003d new byte[getSubPacketSize()][inputs[0].length][];\n       int[][] inputOffsets \u003d new int[getSubPacketSize()][inputs[0].length];\n       byte[][][] newOutputs \u003d new byte[getSubPacketSize()][numParityUnits][];\n       int[][] outOffsets \u003d new int[getSubPacketSize()][numParityUnits];\n \n       for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n         for (int j \u003d 0; j \u003c inputs[0].length; ++j) {\n           buffer \u003d inputs[i][j];\n           if (buffer !\u003d null) {\n             inputOffsets[i][j] \u003d buffer.arrayOffset() + buffer.position();\n             newInputs[i][j] \u003d buffer.array();\n           }\n         }\n       }\n \n       for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n         for (int j \u003d 0; j \u003c numParityUnits; ++j) {\n           buffer \u003d tmpOutputs[i][j];\n           if (buffer !\u003d null) {\n             outOffsets[i][j] \u003d buffer.arrayOffset() + buffer.position();\n             newOutputs[i][j] \u003d buffer.array();\n           }\n         }\n       }\n \n       byte[] newPiggyBack \u003d piggyBack.array();\n \n       // decode the erased value in the first subPacket by using the piggyback\n       int idxToWrite \u003d 0;\n       doDecodeByPiggyBack(newInputs[0], inputOffsets[0],\n               newOutputs[0][idxToWrite], outOffsets[0][idxToWrite],\n               newPiggyBack, erasedLocationToFix, bufSize);\n     }\n \n     for (int i \u003d 0; i \u003c subPacketSize; ++i) {\n       for (int j \u003d 0; j \u003c inputs[i].length; ++j) {\n         if (inputs[i][j] !\u003d null) {\n           inputs[i][j].position(inputPositions[i][j] + bufSize);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void doDecodeSingle(ByteBuffer[][] inputs, ByteBuffer[][] outputs,\n                              int erasedLocationToFix, int bufSize,\n                              boolean isDirect) throws IOException {\n    final int numDataUnits \u003d rsRawDecoder.getNumDataUnits();\n    final int numParityUnits \u003d rsRawDecoder.getNumParityUnits();\n    final int subPacketSize \u003d getSubPacketSize();\n\n    int[][] inputPositions \u003d new int[subPacketSize][inputs[0].length];\n    for (int i \u003d 0; i \u003c subPacketSize; ++i) {\n      for (int j \u003d 0; j \u003c inputs[i].length; ++j) {\n        if (inputs[i][j] !\u003d null) {\n          inputPositions[i][j] \u003d inputs[i][j].position();\n        }\n      }\n    }\n\n    ByteBuffer[] tempInputs \u003d new ByteBuffer[numDataUnits + numParityUnits];\n    for (int i \u003d 0; i \u003c tempInputs.length; ++i) {\n      tempInputs[i] \u003d inputs[1][i];\n    }\n\n    ByteBuffer[][] tmpOutputs \u003d new ByteBuffer[subPacketSize][numParityUnits];\n    for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n      for (int j \u003d 0; j \u003c erasedIndexes.length; ++j) {\n        tmpOutputs[i][j] \u003d outputs[i][j];\n      }\n\n      for (int m \u003d erasedIndexes.length; m \u003c numParityUnits; ++m) {\n        tmpOutputs[i][m] \u003d HHUtil.allocateByteBuffer(isDirect, bufSize);\n      }\n    }\n\n    // First consider the second subPacket\n    int[] erasedLocation \u003d new int[numParityUnits];\n    erasedLocation[0] \u003d erasedLocationToFix;\n\n    // assign the erased locations based on the locations not read for\n    // second subPacket but from decoding\n    for (int i \u003d 1; i \u003c numParityUnits; i++) {\n      erasedLocation[i] \u003d numDataUnits + i;\n      tempInputs[numDataUnits + i] \u003d null;\n    }\n\n    rsRawDecoder.decode(tempInputs, erasedLocation, tmpOutputs[1]);\n\n    int piggyBackParityIndex \u003d piggyBackFullIndex[erasedLocationToFix];\n    ByteBuffer piggyBack \u003d HHUtil.getPiggyBackForDecode(inputs, tmpOutputs,\n            piggyBackParityIndex, numDataUnits, numParityUnits, pbIndex);\n\n    // Second consider the first subPacket.\n    // get the value of the piggyback associated with the erased location\n    if (isDirect) {\n      // decode the erased value in the first subPacket by using the piggyback\n      int idxToWrite \u003d 0;\n      doDecodeByPiggyBack(inputs[0], tmpOutputs[0][idxToWrite], piggyBack,\n              erasedLocationToFix);\n    } else {\n      ByteBuffer buffer;\n      byte[][][] newInputs \u003d new byte[getSubPacketSize()][inputs[0].length][];\n      int[][] inputOffsets \u003d new int[getSubPacketSize()][inputs[0].length];\n      byte[][][] newOutputs \u003d new byte[getSubPacketSize()][numParityUnits][];\n      int[][] outOffsets \u003d new int[getSubPacketSize()][numParityUnits];\n\n      for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n        for (int j \u003d 0; j \u003c inputs[0].length; ++j) {\n          buffer \u003d inputs[i][j];\n          if (buffer !\u003d null) {\n            inputOffsets[i][j] \u003d buffer.arrayOffset() + buffer.position();\n            newInputs[i][j] \u003d buffer.array();\n          }\n        }\n      }\n\n      for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n        for (int j \u003d 0; j \u003c numParityUnits; ++j) {\n          buffer \u003d tmpOutputs[i][j];\n          if (buffer !\u003d null) {\n            outOffsets[i][j] \u003d buffer.arrayOffset() + buffer.position();\n            newOutputs[i][j] \u003d buffer.array();\n          }\n        }\n      }\n\n      byte[] newPiggyBack \u003d piggyBack.array();\n\n      // decode the erased value in the first subPacket by using the piggyback\n      int idxToWrite \u003d 0;\n      doDecodeByPiggyBack(newInputs[0], inputOffsets[0],\n              newOutputs[0][idxToWrite], outOffsets[0][idxToWrite],\n              newPiggyBack, erasedLocationToFix, bufSize);\n    }\n\n    for (int i \u003d 0; i \u003c subPacketSize; ++i) {\n      for (int j \u003d 0; j \u003c inputs[i].length; ++j) {\n        if (inputs[i][j] !\u003d null) {\n          inputs[i][j].position(inputPositions[i][j] + bufSize);\n        }\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[IOException]"
      }
    },
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-11828. Implement the Hitchhiker erasure coding algorithm. Contributed by Jack Liu Quan.\n\nChange-Id: If43475ccc2574df60949c947af562722db076251\n",
      "commitDate": "21/01/16 10:30 AM",
      "commitName": "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743",
      "commitAuthor": "Zhe Zhang",
      "diff": "@@ -0,0 +1,100 @@\n+  private void doDecodeSingle(ByteBuffer[][] inputs, ByteBuffer[][] outputs,\n+                              int erasedLocationToFix, int bufSize,\n+                              boolean isDirect) {\n+    final int numDataUnits \u003d rsRawDecoder.getNumDataUnits();\n+    final int numParityUnits \u003d rsRawDecoder.getNumParityUnits();\n+    final int subPacketSize \u003d getSubPacketSize();\n+\n+    int[][] inputPositions \u003d new int[subPacketSize][inputs[0].length];\n+    for (int i \u003d 0; i \u003c subPacketSize; ++i) {\n+      for (int j \u003d 0; j \u003c inputs[i].length; ++j) {\n+        if (inputs[i][j] !\u003d null) {\n+          inputPositions[i][j] \u003d inputs[i][j].position();\n+        }\n+      }\n+    }\n+\n+    ByteBuffer[] tempInputs \u003d new ByteBuffer[numDataUnits + numParityUnits];\n+    for (int i \u003d 0; i \u003c tempInputs.length; ++i) {\n+      tempInputs[i] \u003d inputs[1][i];\n+    }\n+\n+    ByteBuffer[][] tmpOutputs \u003d new ByteBuffer[subPacketSize][numParityUnits];\n+    for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n+      for (int j \u003d 0; j \u003c erasedIndexes.length; ++j) {\n+        tmpOutputs[i][j] \u003d outputs[i][j];\n+      }\n+\n+      for (int m \u003d erasedIndexes.length; m \u003c numParityUnits; ++m) {\n+        tmpOutputs[i][m] \u003d HHUtil.allocateByteBuffer(isDirect, bufSize);\n+      }\n+    }\n+\n+    // First consider the second subPacket\n+    int[] erasedLocation \u003d new int[numParityUnits];\n+    erasedLocation[0] \u003d erasedLocationToFix;\n+\n+    // assign the erased locations based on the locations not read for\n+    // second subPacket but from decoding\n+    for (int i \u003d 1; i \u003c numParityUnits; i++) {\n+      erasedLocation[i] \u003d numDataUnits + i;\n+      tempInputs[numDataUnits + i] \u003d null;\n+    }\n+\n+    rsRawDecoder.decode(tempInputs, erasedLocation, tmpOutputs[1]);\n+\n+    int piggyBackParityIndex \u003d piggyBackFullIndex[erasedLocationToFix];\n+    ByteBuffer piggyBack \u003d HHUtil.getPiggyBackForDecode(inputs, tmpOutputs,\n+            piggyBackParityIndex, numDataUnits, numParityUnits, pbIndex);\n+\n+    // Second consider the first subPacket.\n+    // get the value of the piggyback associated with the erased location\n+    if (isDirect) {\n+      // decode the erased value in the first subPacket by using the piggyback\n+      int idxToWrite \u003d 0;\n+      doDecodeByPiggyBack(inputs[0], tmpOutputs[0][idxToWrite], piggyBack,\n+              erasedLocationToFix);\n+    } else {\n+      ByteBuffer buffer;\n+      byte[][][] newInputs \u003d new byte[getSubPacketSize()][inputs[0].length][];\n+      int[][] inputOffsets \u003d new int[getSubPacketSize()][inputs[0].length];\n+      byte[][][] newOutputs \u003d new byte[getSubPacketSize()][numParityUnits][];\n+      int[][] outOffsets \u003d new int[getSubPacketSize()][numParityUnits];\n+\n+      for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n+        for (int j \u003d 0; j \u003c inputs[0].length; ++j) {\n+          buffer \u003d inputs[i][j];\n+          if (buffer !\u003d null) {\n+            inputOffsets[i][j] \u003d buffer.arrayOffset() + buffer.position();\n+            newInputs[i][j] \u003d buffer.array();\n+          }\n+        }\n+      }\n+\n+      for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n+        for (int j \u003d 0; j \u003c numParityUnits; ++j) {\n+          buffer \u003d tmpOutputs[i][j];\n+          if (buffer !\u003d null) {\n+            outOffsets[i][j] \u003d buffer.arrayOffset() + buffer.position();\n+            newOutputs[i][j] \u003d buffer.array();\n+          }\n+        }\n+      }\n+\n+      byte[] newPiggyBack \u003d piggyBack.array();\n+\n+      // decode the erased value in the first subPacket by using the piggyback\n+      int idxToWrite \u003d 0;\n+      doDecodeByPiggyBack(newInputs[0], inputOffsets[0],\n+              newOutputs[0][idxToWrite], outOffsets[0][idxToWrite],\n+              newPiggyBack, erasedLocationToFix, bufSize);\n+    }\n+\n+    for (int i \u003d 0; i \u003c subPacketSize; ++i) {\n+      for (int j \u003d 0; j \u003c inputs[i].length; ++j) {\n+        if (inputs[i][j] !\u003d null) {\n+          inputs[i][j].position(inputPositions[i][j] + bufSize);\n+        }\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void doDecodeSingle(ByteBuffer[][] inputs, ByteBuffer[][] outputs,\n                              int erasedLocationToFix, int bufSize,\n                              boolean isDirect) {\n    final int numDataUnits \u003d rsRawDecoder.getNumDataUnits();\n    final int numParityUnits \u003d rsRawDecoder.getNumParityUnits();\n    final int subPacketSize \u003d getSubPacketSize();\n\n    int[][] inputPositions \u003d new int[subPacketSize][inputs[0].length];\n    for (int i \u003d 0; i \u003c subPacketSize; ++i) {\n      for (int j \u003d 0; j \u003c inputs[i].length; ++j) {\n        if (inputs[i][j] !\u003d null) {\n          inputPositions[i][j] \u003d inputs[i][j].position();\n        }\n      }\n    }\n\n    ByteBuffer[] tempInputs \u003d new ByteBuffer[numDataUnits + numParityUnits];\n    for (int i \u003d 0; i \u003c tempInputs.length; ++i) {\n      tempInputs[i] \u003d inputs[1][i];\n    }\n\n    ByteBuffer[][] tmpOutputs \u003d new ByteBuffer[subPacketSize][numParityUnits];\n    for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n      for (int j \u003d 0; j \u003c erasedIndexes.length; ++j) {\n        tmpOutputs[i][j] \u003d outputs[i][j];\n      }\n\n      for (int m \u003d erasedIndexes.length; m \u003c numParityUnits; ++m) {\n        tmpOutputs[i][m] \u003d HHUtil.allocateByteBuffer(isDirect, bufSize);\n      }\n    }\n\n    // First consider the second subPacket\n    int[] erasedLocation \u003d new int[numParityUnits];\n    erasedLocation[0] \u003d erasedLocationToFix;\n\n    // assign the erased locations based on the locations not read for\n    // second subPacket but from decoding\n    for (int i \u003d 1; i \u003c numParityUnits; i++) {\n      erasedLocation[i] \u003d numDataUnits + i;\n      tempInputs[numDataUnits + i] \u003d null;\n    }\n\n    rsRawDecoder.decode(tempInputs, erasedLocation, tmpOutputs[1]);\n\n    int piggyBackParityIndex \u003d piggyBackFullIndex[erasedLocationToFix];\n    ByteBuffer piggyBack \u003d HHUtil.getPiggyBackForDecode(inputs, tmpOutputs,\n            piggyBackParityIndex, numDataUnits, numParityUnits, pbIndex);\n\n    // Second consider the first subPacket.\n    // get the value of the piggyback associated with the erased location\n    if (isDirect) {\n      // decode the erased value in the first subPacket by using the piggyback\n      int idxToWrite \u003d 0;\n      doDecodeByPiggyBack(inputs[0], tmpOutputs[0][idxToWrite], piggyBack,\n              erasedLocationToFix);\n    } else {\n      ByteBuffer buffer;\n      byte[][][] newInputs \u003d new byte[getSubPacketSize()][inputs[0].length][];\n      int[][] inputOffsets \u003d new int[getSubPacketSize()][inputs[0].length];\n      byte[][][] newOutputs \u003d new byte[getSubPacketSize()][numParityUnits][];\n      int[][] outOffsets \u003d new int[getSubPacketSize()][numParityUnits];\n\n      for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n        for (int j \u003d 0; j \u003c inputs[0].length; ++j) {\n          buffer \u003d inputs[i][j];\n          if (buffer !\u003d null) {\n            inputOffsets[i][j] \u003d buffer.arrayOffset() + buffer.position();\n            newInputs[i][j] \u003d buffer.array();\n          }\n        }\n      }\n\n      for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n        for (int j \u003d 0; j \u003c numParityUnits; ++j) {\n          buffer \u003d tmpOutputs[i][j];\n          if (buffer !\u003d null) {\n            outOffsets[i][j] \u003d buffer.arrayOffset() + buffer.position();\n            newOutputs[i][j] \u003d buffer.array();\n          }\n        }\n      }\n\n      byte[] newPiggyBack \u003d piggyBack.array();\n\n      // decode the erased value in the first subPacket by using the piggyback\n      int idxToWrite \u003d 0;\n      doDecodeByPiggyBack(newInputs[0], inputOffsets[0],\n              newOutputs[0][idxToWrite], outOffsets[0][idxToWrite],\n              newPiggyBack, erasedLocationToFix, bufSize);\n    }\n\n    for (int i \u003d 0; i \u003c subPacketSize; ++i) {\n      for (int j \u003d 0; j \u003c inputs[i].length; ++j) {\n        if (inputs[i][j] !\u003d null) {\n          inputs[i][j].position(inputPositions[i][j] + bufSize);\n        }\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java"
    }
  }
}