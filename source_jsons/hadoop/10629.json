{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockChecksumHelper.java",
  "functionName": "compute",
  "functionId": "compute",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
  "functionStartLine": 294,
  "functionEndLine": 315,
  "numCommitsSeen": 8,
  "timeTaken": 3155,
  "changeHistory": [
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
    "e6cb07520f935efde3e881de8f84ee7f6e0a746f",
    "d749cf65e1ab0e0daf5be86931507183f189e855",
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
    "a337ceb74e984991dbf976236d2e785cf5921b16",
    "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6"
  ],
  "changeHistoryShort": {
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf": "Ybodychange",
    "e6cb07520f935efde3e881de8f84ee7f6e0a746f": "Ybodychange",
    "d749cf65e1ab0e0daf5be86931507183f189e855": "Ybodychange",
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720": "Ybodychange",
    "a337ceb74e984991dbf976236d2e785cf5921b16": "Ybodychange",
    "e5ff0ea7ba087984262f1f27200ae5bb40d9b838": "Ybodychange",
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13056. Expose file-level composite CRCs in HDFS which are comparable across different instances/layouts. Contributed by Dennis Huo.\n",
      "commitDate": "10/04/18 9:31 PM",
      "commitName": "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "14/03/17 4:41 PM",
      "commitNameOld": "cc1292e73acd39c1f1023ad4841ffe30176f7daf",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 392.2,
      "commitsBetweenForRepo": 2722,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,58 @@\n     void compute() throws IOException {\n       assert datanodes.length \u003d\u003d blockIndices.length;\n \n       Map\u003cByte, LiveBlockInfo\u003e liveDns \u003d new HashMap\u003c\u003e(datanodes.length);\n       int blkIndxLen \u003d blockIndices.length;\n       int numDataUnits \u003d ecPolicy.getNumDataUnits();\n       // Prepare live datanode list. Missing data blocks will be reconstructed\n       // and recalculate checksum.\n       for (int idx \u003d 0; idx \u003c blkIndxLen; idx++) {\n         liveDns.put(blockIndices[idx],\n             new LiveBlockInfo(datanodes[idx], blockTokens[idx]));\n       }\n       long checksumLen \u003d 0;\n       for (int idx \u003d 0; idx \u003c numDataUnits \u0026\u0026 idx \u003c blkIndxLen; idx++) {\n+        // Before populating the blockChecksum at this index, record the byte\n+        // offset where it will begin.\n+        blockChecksumPositions[idx] \u003d blockChecksumBuf.getLength();\n         try {\n           ExtendedBlock block \u003d getInternalBlock(numDataUnits, idx);\n \n           LiveBlockInfo liveBlkInfo \u003d liveDns.get((byte) idx);\n           if (liveBlkInfo \u003d\u003d null) {\n             // reconstruct block and calculate checksum for missing node\n             recalculateChecksum(idx, block.getNumBytes());\n           } else {\n             try {\n               checksumBlock(block, idx, liveBlkInfo.getToken(),\n                   liveBlkInfo.getDn());\n             } catch (IOException ioe) {\n               LOG.warn(\"Exception while reading checksum\", ioe);\n               // reconstruct block and calculate checksum for the failed node\n               recalculateChecksum(idx, block.getNumBytes());\n             }\n           }\n           checksumLen +\u003d block.getNumBytes();\n           if (checksumLen \u003e\u003d requestedNumBytes) {\n             break; // done with the computation, simply return.\n           }\n         } catch (IOException e) {\n           LOG.warn(\"Failed to get the checksum\", e);\n         }\n       }\n \n-      MD5Hash md5out \u003d MD5Hash.digest(md5writer.getData());\n-      setOutBytes(md5out.getDigest());\n+      BlockChecksumType type \u003d getBlockChecksumOptions().getBlockChecksumType();\n+      switch (type) {\n+      case MD5CRC:\n+        MD5Hash md5out \u003d MD5Hash.digest(blockChecksumBuf.getData());\n+        setOutBytes(md5out.getDigest());\n+        break;\n+      case COMPOSITE_CRC:\n+        byte[] digest \u003d reassembleNonStripedCompositeCrc(checksumLen);\n+        setOutBytes(digest);\n+        break;\n+      default:\n+        throw new IOException(String.format(\n+            \"Unrecognized BlockChecksumType: %s\", type));\n+      }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void compute() throws IOException {\n      assert datanodes.length \u003d\u003d blockIndices.length;\n\n      Map\u003cByte, LiveBlockInfo\u003e liveDns \u003d new HashMap\u003c\u003e(datanodes.length);\n      int blkIndxLen \u003d blockIndices.length;\n      int numDataUnits \u003d ecPolicy.getNumDataUnits();\n      // Prepare live datanode list. Missing data blocks will be reconstructed\n      // and recalculate checksum.\n      for (int idx \u003d 0; idx \u003c blkIndxLen; idx++) {\n        liveDns.put(blockIndices[idx],\n            new LiveBlockInfo(datanodes[idx], blockTokens[idx]));\n      }\n      long checksumLen \u003d 0;\n      for (int idx \u003d 0; idx \u003c numDataUnits \u0026\u0026 idx \u003c blkIndxLen; idx++) {\n        // Before populating the blockChecksum at this index, record the byte\n        // offset where it will begin.\n        blockChecksumPositions[idx] \u003d blockChecksumBuf.getLength();\n        try {\n          ExtendedBlock block \u003d getInternalBlock(numDataUnits, idx);\n\n          LiveBlockInfo liveBlkInfo \u003d liveDns.get((byte) idx);\n          if (liveBlkInfo \u003d\u003d null) {\n            // reconstruct block and calculate checksum for missing node\n            recalculateChecksum(idx, block.getNumBytes());\n          } else {\n            try {\n              checksumBlock(block, idx, liveBlkInfo.getToken(),\n                  liveBlkInfo.getDn());\n            } catch (IOException ioe) {\n              LOG.warn(\"Exception while reading checksum\", ioe);\n              // reconstruct block and calculate checksum for the failed node\n              recalculateChecksum(idx, block.getNumBytes());\n            }\n          }\n          checksumLen +\u003d block.getNumBytes();\n          if (checksumLen \u003e\u003d requestedNumBytes) {\n            break; // done with the computation, simply return.\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Failed to get the checksum\", e);\n        }\n      }\n\n      BlockChecksumType type \u003d getBlockChecksumOptions().getBlockChecksumType();\n      switch (type) {\n      case MD5CRC:\n        MD5Hash md5out \u003d MD5Hash.digest(blockChecksumBuf.getData());\n        setOutBytes(md5out.getDigest());\n        break;\n      case COMPOSITE_CRC:\n        byte[] digest \u003d reassembleNonStripedCompositeCrc(checksumLen);\n        setOutBytes(digest);\n        break;\n      default:\n        throw new IOException(String.format(\n            \"Unrecognized BlockChecksumType: %s\", type));\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
      "extendedDetails": {}
    },
    "e6cb07520f935efde3e881de8f84ee7f6e0a746f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10460. Recompute block checksum for a particular range less than file size on the fly by reconstructing missed block. Contributed by Rakesh R\n",
      "commitDate": "24/06/16 2:39 AM",
      "commitName": "e6cb07520f935efde3e881de8f84ee7f6e0a746f",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "01/06/16 9:56 PM",
      "commitNameOld": "d749cf65e1ab0e0daf5be86931507183f189e855",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 22.2,
      "commitsBetweenForRepo": 141,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,43 @@\n     void compute() throws IOException {\n       assert datanodes.length \u003d\u003d blockIndices.length;\n \n       Map\u003cByte, LiveBlockInfo\u003e liveDns \u003d new HashMap\u003c\u003e(datanodes.length);\n       int blkIndxLen \u003d blockIndices.length;\n       int numDataUnits \u003d ecPolicy.getNumDataUnits();\n       // Prepare live datanode list. Missing data blocks will be reconstructed\n       // and recalculate checksum.\n       for (int idx \u003d 0; idx \u003c blkIndxLen; idx++) {\n         liveDns.put(blockIndices[idx],\n             new LiveBlockInfo(datanodes[idx], blockTokens[idx]));\n       }\n+      long checksumLen \u003d 0;\n       for (int idx \u003d 0; idx \u003c numDataUnits \u0026\u0026 idx \u003c blkIndxLen; idx++) {\n         try {\n+          ExtendedBlock block \u003d getInternalBlock(numDataUnits, idx);\n+\n           LiveBlockInfo liveBlkInfo \u003d liveDns.get((byte) idx);\n           if (liveBlkInfo \u003d\u003d null) {\n             // reconstruct block and calculate checksum for missing node\n-            recalculateChecksum(idx);\n+            recalculateChecksum(idx, block.getNumBytes());\n           } else {\n             try {\n-              ExtendedBlock block \u003d StripedBlockUtil.constructInternalBlock(\n-                  blockGroup, ecPolicy.getCellSize(), numDataUnits, idx);\n               checksumBlock(block, idx, liveBlkInfo.getToken(),\n                   liveBlkInfo.getDn());\n             } catch (IOException ioe) {\n               LOG.warn(\"Exception while reading checksum\", ioe);\n               // reconstruct block and calculate checksum for the failed node\n-              recalculateChecksum(idx);\n+              recalculateChecksum(idx, block.getNumBytes());\n             }\n           }\n+          checksumLen +\u003d block.getNumBytes();\n+          if (checksumLen \u003e\u003d requestedNumBytes) {\n+            break; // done with the computation, simply return.\n+          }\n         } catch (IOException e) {\n           LOG.warn(\"Failed to get the checksum\", e);\n         }\n       }\n \n       MD5Hash md5out \u003d MD5Hash.digest(md5writer.getData());\n       setOutBytes(md5out.getDigest());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void compute() throws IOException {\n      assert datanodes.length \u003d\u003d blockIndices.length;\n\n      Map\u003cByte, LiveBlockInfo\u003e liveDns \u003d new HashMap\u003c\u003e(datanodes.length);\n      int blkIndxLen \u003d blockIndices.length;\n      int numDataUnits \u003d ecPolicy.getNumDataUnits();\n      // Prepare live datanode list. Missing data blocks will be reconstructed\n      // and recalculate checksum.\n      for (int idx \u003d 0; idx \u003c blkIndxLen; idx++) {\n        liveDns.put(blockIndices[idx],\n            new LiveBlockInfo(datanodes[idx], blockTokens[idx]));\n      }\n      long checksumLen \u003d 0;\n      for (int idx \u003d 0; idx \u003c numDataUnits \u0026\u0026 idx \u003c blkIndxLen; idx++) {\n        try {\n          ExtendedBlock block \u003d getInternalBlock(numDataUnits, idx);\n\n          LiveBlockInfo liveBlkInfo \u003d liveDns.get((byte) idx);\n          if (liveBlkInfo \u003d\u003d null) {\n            // reconstruct block and calculate checksum for missing node\n            recalculateChecksum(idx, block.getNumBytes());\n          } else {\n            try {\n              checksumBlock(block, idx, liveBlkInfo.getToken(),\n                  liveBlkInfo.getDn());\n            } catch (IOException ioe) {\n              LOG.warn(\"Exception while reading checksum\", ioe);\n              // reconstruct block and calculate checksum for the failed node\n              recalculateChecksum(idx, block.getNumBytes());\n            }\n          }\n          checksumLen +\u003d block.getNumBytes();\n          if (checksumLen \u003e\u003d requestedNumBytes) {\n            break; // done with the computation, simply return.\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Failed to get the checksum\", e);\n        }\n      }\n\n      MD5Hash md5out \u003d MD5Hash.digest(md5writer.getData());\n      setOutBytes(md5out.getDigest());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
      "extendedDetails": {}
    },
    "d749cf65e1ab0e0daf5be86931507183f189e855": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9833. Erasure coding: recomputing block checksum on the fly by reconstructing the missed/corrupt block data. Contributed by Rakesh R.\n",
      "commitDate": "01/06/16 9:56 PM",
      "commitName": "d749cf65e1ab0e0daf5be86931507183f189e855",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "26/03/16 7:58 PM",
      "commitNameOld": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 67.08,
      "commitsBetweenForRepo": 433,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,38 @@\n     void compute() throws IOException {\n-      for (int idx \u003d 0; idx \u003c ecPolicy.getNumDataUnits(); idx++) {\n-        ExtendedBlock block \u003d\n-            StripedBlockUtil.constructInternalBlock(blockGroup,\n-            ecPolicy.getCellSize(), ecPolicy.getNumDataUnits(), idx);\n-        DatanodeInfo targetDatanode \u003d datanodes[idx];\n-        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d blockTokens[idx];\n-        checksumBlock(block, idx, blockToken, targetDatanode);\n+      assert datanodes.length \u003d\u003d blockIndices.length;\n+\n+      Map\u003cByte, LiveBlockInfo\u003e liveDns \u003d new HashMap\u003c\u003e(datanodes.length);\n+      int blkIndxLen \u003d blockIndices.length;\n+      int numDataUnits \u003d ecPolicy.getNumDataUnits();\n+      // Prepare live datanode list. Missing data blocks will be reconstructed\n+      // and recalculate checksum.\n+      for (int idx \u003d 0; idx \u003c blkIndxLen; idx++) {\n+        liveDns.put(blockIndices[idx],\n+            new LiveBlockInfo(datanodes[idx], blockTokens[idx]));\n+      }\n+      for (int idx \u003d 0; idx \u003c numDataUnits \u0026\u0026 idx \u003c blkIndxLen; idx++) {\n+        try {\n+          LiveBlockInfo liveBlkInfo \u003d liveDns.get((byte) idx);\n+          if (liveBlkInfo \u003d\u003d null) {\n+            // reconstruct block and calculate checksum for missing node\n+            recalculateChecksum(idx);\n+          } else {\n+            try {\n+              ExtendedBlock block \u003d StripedBlockUtil.constructInternalBlock(\n+                  blockGroup, ecPolicy.getCellSize(), numDataUnits, idx);\n+              checksumBlock(block, idx, liveBlkInfo.getToken(),\n+                  liveBlkInfo.getDn());\n+            } catch (IOException ioe) {\n+              LOG.warn(\"Exception while reading checksum\", ioe);\n+              // reconstruct block and calculate checksum for the failed node\n+              recalculateChecksum(idx);\n+            }\n+          }\n+        } catch (IOException e) {\n+          LOG.warn(\"Failed to get the checksum\", e);\n+        }\n       }\n \n       MD5Hash md5out \u003d MD5Hash.digest(md5writer.getData());\n       setOutBytes(md5out.getDigest());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void compute() throws IOException {\n      assert datanodes.length \u003d\u003d blockIndices.length;\n\n      Map\u003cByte, LiveBlockInfo\u003e liveDns \u003d new HashMap\u003c\u003e(datanodes.length);\n      int blkIndxLen \u003d blockIndices.length;\n      int numDataUnits \u003d ecPolicy.getNumDataUnits();\n      // Prepare live datanode list. Missing data blocks will be reconstructed\n      // and recalculate checksum.\n      for (int idx \u003d 0; idx \u003c blkIndxLen; idx++) {\n        liveDns.put(blockIndices[idx],\n            new LiveBlockInfo(datanodes[idx], blockTokens[idx]));\n      }\n      for (int idx \u003d 0; idx \u003c numDataUnits \u0026\u0026 idx \u003c blkIndxLen; idx++) {\n        try {\n          LiveBlockInfo liveBlkInfo \u003d liveDns.get((byte) idx);\n          if (liveBlkInfo \u003d\u003d null) {\n            // reconstruct block and calculate checksum for missing node\n            recalculateChecksum(idx);\n          } else {\n            try {\n              ExtendedBlock block \u003d StripedBlockUtil.constructInternalBlock(\n                  blockGroup, ecPolicy.getCellSize(), numDataUnits, idx);\n              checksumBlock(block, idx, liveBlkInfo.getToken(),\n                  liveBlkInfo.getDn());\n            } catch (IOException ioe) {\n              LOG.warn(\"Exception while reading checksum\", ioe);\n              // reconstruct block and calculate checksum for the failed node\n              recalculateChecksum(idx);\n            }\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Failed to get the checksum\", e);\n        }\n      }\n\n      MD5Hash md5out \u003d MD5Hash.digest(md5writer.getData());\n      setOutBytes(md5out.getDigest());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
      "extendedDetails": {}
    },
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
      "commitDate": "26/03/16 7:58 PM",
      "commitName": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "26/03/16 9:20 AM",
      "commitNameOld": "a337ceb74e984991dbf976236d2e785cf5921b16",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.44,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,13 @@\n     void compute() throws IOException {\n-      try {\n-        readHeader();\n-\n-        MD5Hash md5out;\n-        if (isPartialBlk() \u0026\u0026 getCrcPerBlock() \u003e 0) {\n-          md5out \u003d checksumPartialBlock();\n-        } else {\n-          md5out \u003d checksumWholeBlock();\n-        }\n-        setOutBytes(md5out.getDigest());\n-\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"block\u003d\" + getBlock() + \", bytesPerCRC\u003d\" + getBytesPerCRC()\n-              + \", crcPerBlock\u003d\" + getCrcPerBlock() + \", md5out\u003d\" + md5out);\n-        }\n-      } finally {\n-        IOUtils.closeStream(getChecksumIn());\n-        IOUtils.closeStream(getMetadataIn());\n+      for (int idx \u003d 0; idx \u003c ecPolicy.getNumDataUnits(); idx++) {\n+        ExtendedBlock block \u003d\n+            StripedBlockUtil.constructInternalBlock(blockGroup,\n+            ecPolicy.getCellSize(), ecPolicy.getNumDataUnits(), idx);\n+        DatanodeInfo targetDatanode \u003d datanodes[idx];\n+        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d blockTokens[idx];\n+        checksumBlock(block, idx, blockToken, targetDatanode);\n       }\n+\n+      MD5Hash md5out \u003d MD5Hash.digest(md5writer.getData());\n+      setOutBytes(md5out.getDigest());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void compute() throws IOException {\n      for (int idx \u003d 0; idx \u003c ecPolicy.getNumDataUnits(); idx++) {\n        ExtendedBlock block \u003d\n            StripedBlockUtil.constructInternalBlock(blockGroup,\n            ecPolicy.getCellSize(), ecPolicy.getNumDataUnits(), idx);\n        DatanodeInfo targetDatanode \u003d datanodes[idx];\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d blockTokens[idx];\n        checksumBlock(block, idx, blockToken, targetDatanode);\n      }\n\n      MD5Hash md5out \u003d MD5Hash.digest(md5writer.getData());\n      setOutBytes(md5out.getDigest());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
      "extendedDetails": {}
    },
    "a337ceb74e984991dbf976236d2e785cf5921b16": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\"\n\nThis reverts commit e5ff0ea7ba087984262f1f27200ae5bb40d9b838.\n",
      "commitDate": "26/03/16 9:20 AM",
      "commitName": "a337ceb74e984991dbf976236d2e785cf5921b16",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "26/03/16 12:52 AM",
      "commitNameOld": "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.35,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,21 @@\n     void compute() throws IOException {\n-      for (int idx \u003d 0; idx \u003c ecPolicy.getNumDataUnits(); idx++) {\n-        ExtendedBlock block \u003d\n-            StripedBlockUtil.constructInternalBlock(blockGroup,\n-            ecPolicy.getCellSize(), ecPolicy.getNumDataUnits(), idx);\n-        DatanodeInfo targetDatanode \u003d datanodes[idx];\n-        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d blockTokens[idx];\n-        checksumBlock(block, idx, blockToken, targetDatanode);\n-      }\n+      try {\n+        readHeader();\n \n-      MD5Hash md5out \u003d MD5Hash.digest(md5writer.getData());\n-      setOutBytes(md5out.getDigest());\n+        MD5Hash md5out;\n+        if (isPartialBlk() \u0026\u0026 getCrcPerBlock() \u003e 0) {\n+          md5out \u003d checksumPartialBlock();\n+        } else {\n+          md5out \u003d checksumWholeBlock();\n+        }\n+        setOutBytes(md5out.getDigest());\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"block\u003d\" + getBlock() + \", bytesPerCRC\u003d\" + getBytesPerCRC()\n+              + \", crcPerBlock\u003d\" + getCrcPerBlock() + \", md5out\u003d\" + md5out);\n+        }\n+      } finally {\n+        IOUtils.closeStream(getChecksumIn());\n+        IOUtils.closeStream(getMetadataIn());\n+      }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void compute() throws IOException {\n      try {\n        readHeader();\n\n        MD5Hash md5out;\n        if (isPartialBlk() \u0026\u0026 getCrcPerBlock() \u003e 0) {\n          md5out \u003d checksumPartialBlock();\n        } else {\n          md5out \u003d checksumWholeBlock();\n        }\n        setOutBytes(md5out.getDigest());\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"block\u003d\" + getBlock() + \", bytesPerCRC\u003d\" + getBytesPerCRC()\n              + \", crcPerBlock\u003d\" + getCrcPerBlock() + \", md5out\u003d\" + md5out);\n        }\n      } finally {\n        IOUtils.closeStream(getChecksumIn());\n        IOUtils.closeStream(getMetadataIn());\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
      "extendedDetails": {}
    },
    "e5ff0ea7ba087984262f1f27200ae5bb40d9b838": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
      "commitDate": "26/03/16 12:52 AM",
      "commitName": "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "29/02/16 9:52 PM",
      "commitNameOld": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 25.08,
      "commitsBetweenForRepo": 134,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,13 @@\n     void compute() throws IOException {\n-      try {\n-        readHeader();\n-\n-        MD5Hash md5out;\n-        if (isPartialBlk() \u0026\u0026 getCrcPerBlock() \u003e 0) {\n-          md5out \u003d checksumPartialBlock();\n-        } else {\n-          md5out \u003d checksumWholeBlock();\n-        }\n-        setOutBytes(md5out.getDigest());\n-\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"block\u003d\" + getBlock() + \", bytesPerCRC\u003d\" + getBytesPerCRC()\n-              + \", crcPerBlock\u003d\" + getCrcPerBlock() + \", md5out\u003d\" + md5out);\n-        }\n-      } finally {\n-        IOUtils.closeStream(getChecksumIn());\n-        IOUtils.closeStream(getMetadataIn());\n+      for (int idx \u003d 0; idx \u003c ecPolicy.getNumDataUnits(); idx++) {\n+        ExtendedBlock block \u003d\n+            StripedBlockUtil.constructInternalBlock(blockGroup,\n+            ecPolicy.getCellSize(), ecPolicy.getNumDataUnits(), idx);\n+        DatanodeInfo targetDatanode \u003d datanodes[idx];\n+        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d blockTokens[idx];\n+        checksumBlock(block, idx, blockToken, targetDatanode);\n       }\n+\n+      MD5Hash md5out \u003d MD5Hash.digest(md5writer.getData());\n+      setOutBytes(md5out.getDigest());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void compute() throws IOException {\n      for (int idx \u003d 0; idx \u003c ecPolicy.getNumDataUnits(); idx++) {\n        ExtendedBlock block \u003d\n            StripedBlockUtil.constructInternalBlock(blockGroup,\n            ecPolicy.getCellSize(), ecPolicy.getNumDataUnits(), idx);\n        DatanodeInfo targetDatanode \u003d datanodes[idx];\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d blockTokens[idx];\n        checksumBlock(block, idx, blockToken, targetDatanode);\n      }\n\n      MD5Hash md5out \u003d MD5Hash.digest(md5writer.getData());\n      setOutBytes(md5out.getDigest());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
      "extendedDetails": {}
    },
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9733. Refactor DFSClient#getFileChecksum and DataXceiver#blockChecksum. Contributed by Kai Zheng\n",
      "commitDate": "29/02/16 9:52 PM",
      "commitName": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,21 @@\n+    void compute() throws IOException {\n+      try {\n+        readHeader();\n+\n+        MD5Hash md5out;\n+        if (isPartialBlk() \u0026\u0026 getCrcPerBlock() \u003e 0) {\n+          md5out \u003d checksumPartialBlock();\n+        } else {\n+          md5out \u003d checksumWholeBlock();\n+        }\n+        setOutBytes(md5out.getDigest());\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"block\u003d\" + getBlock() + \", bytesPerCRC\u003d\" + getBytesPerCRC()\n+              + \", crcPerBlock\u003d\" + getCrcPerBlock() + \", md5out\u003d\" + md5out);\n+        }\n+      } finally {\n+        IOUtils.closeStream(getChecksumIn());\n+        IOUtils.closeStream(getMetadataIn());\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    void compute() throws IOException {\n      try {\n        readHeader();\n\n        MD5Hash md5out;\n        if (isPartialBlk() \u0026\u0026 getCrcPerBlock() \u003e 0) {\n          md5out \u003d checksumPartialBlock();\n        } else {\n          md5out \u003d checksumWholeBlock();\n        }\n        setOutBytes(md5out.getDigest());\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"block\u003d\" + getBlock() + \", bytesPerCRC\u003d\" + getBytesPerCRC()\n              + \", crcPerBlock\u003d\" + getCrcPerBlock() + \", md5out\u003d\" + md5out);\n        }\n      } finally {\n        IOUtils.closeStream(getChecksumIn());\n        IOUtils.closeStream(getMetadataIn());\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java"
    }
  }
}