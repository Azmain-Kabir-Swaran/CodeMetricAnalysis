{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BackupImage.java",
  "functionName": "tryConvergeJournalSpool",
  "functionId": "tryConvergeJournalSpool",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
  "functionStartLine": 247,
  "functionEndLine": 323,
  "numCommitsSeen": 30,
  "timeTaken": 2708,
  "changeHistory": [
    "a4ceea60f57a32d531549e492aa5894dd34e0d0f",
    "d8bc523754181b4c1321bcfab886ebf228d9c98f",
    "706394d03992b394e9f907aff2155df493e4ea4e",
    "9a07ba8945407cd8f63169faf9e0faa4311d38c7",
    "06e84a1bca19bd01568a3095e33944d4d6387fd3",
    "bdc3720d5b67a1c8fc2dfb29be16e4155c0e7f15",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63"
  ],
  "changeHistoryShort": {
    "a4ceea60f57a32d531549e492aa5894dd34e0d0f": "Ybodychange",
    "d8bc523754181b4c1321bcfab886ebf228d9c98f": "Ybodychange",
    "706394d03992b394e9f907aff2155df493e4ea4e": "Ybodychange",
    "9a07ba8945407cd8f63169faf9e0faa4311d38c7": "Ybodychange",
    "06e84a1bca19bd01568a3095e33944d4d6387fd3": "Ybodychange",
    "bdc3720d5b67a1c8fc2dfb29be16e4155c0e7f15": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a4ceea60f57a32d531549e492aa5894dd34e0d0f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7753. Fix Multithreaded correctness Warnings in BackupImage. Contributed by Rakesh R and Konstantin Shvachko.",
      "commitDate": "11/02/15 12:45 AM",
      "commitName": "a4ceea60f57a32d531549e492aa5894dd34e0d0f",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "06/02/15 6:59 PM",
      "commitNameOld": "cfb829ecd5b91fc9adcf5406e788184cfd75300f",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 4.24,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,77 @@\n   private boolean tryConvergeJournalSpool() throws IOException {\n     Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n         \"bad state: %s\", bnState);\n     \n     // This section is unsynchronized so we can continue to apply\n     // ahead of where we\u0027re reading, concurrently. Since the state\n     // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n     // doesn\u0027t change, and curSegmentTxId only increases\n \n     while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n       long target \u003d editLog.getCurSegmentTxId();\n       LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n           + lastAppliedTxId + \" to \" + target);\n       FSImageTransactionalStorageInspector inspector \u003d\n         new FSImageTransactionalStorageInspector();\n       \n       storage.inspectStorageDirs(inspector);\n \n       editLog.recoverUnclosedStreams();\n       Iterable\u003cEditLogInputStream\u003e editStreamsAll \n         \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n       // remove inprogress\n       List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n       for (EditLogInputStream s : editStreamsAll) {\n         if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n           editStreams.add(s);\n         }\n       }\n-      loadEdits(editStreams, namesystem);\n+      loadEdits(editStreams, getNamesystem());\n     }\n     \n     // now, need to load the in-progress file\n     synchronized (this) {\n       if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n         LOG.debug(\"Logs rolled while catching up to current segment\");\n         return false; // drop lock and try again to load local logs\n       }\n       \n       EditLogInputStream stream \u003d null;\n       Collection\u003cEditLogInputStream\u003e editStreams\n         \u003d getEditLog().selectInputStreams(\n             getEditLog().getCurSegmentTxId(),\n             getEditLog().getCurSegmentTxId());\n       \n       for (EditLogInputStream s : editStreams) {\n         if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n           stream \u003d s;\n         }\n         break;\n       }\n       if (stream \u003d\u003d null) {\n         LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n                  + \". This indicates that there is an error in synchronization in BackupImage\");\n         return false;\n       }\n \n       try {\n         long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n         \n         LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n             + \" txns from in-progress stream \" + stream);\n         \n         FSEditLogLoader loader \u003d\n-            new FSEditLogLoader(namesystem, lastAppliedTxId);\n+            new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n         loader.loadFSEdits(stream, lastAppliedTxId + 1);\n         lastAppliedTxId \u003d loader.getLastAppliedTxId();\n         assert lastAppliedTxId \u003d\u003d getEditLog().getLastWrittenTxId();\n       } finally {\n         FSEditLog.closeAllStreams(editStreams);\n       }\n \n       LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n           lastAppliedTxId);\n       setState(BNState.IN_SYNC);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean tryConvergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n    \n    // This section is unsynchronized so we can continue to apply\n    // ahead of where we\u0027re reading, concurrently. Since the state\n    // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n    // doesn\u0027t change, and curSegmentTxId only increases\n\n    while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n      long target \u003d editLog.getCurSegmentTxId();\n      LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n          + lastAppliedTxId + \" to \" + target);\n      FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n      \n      storage.inspectStorageDirs(inspector);\n\n      editLog.recoverUnclosedStreams();\n      Iterable\u003cEditLogInputStream\u003e editStreamsAll \n        \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n      // remove inprogress\n      List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n      for (EditLogInputStream s : editStreamsAll) {\n        if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n          editStreams.add(s);\n        }\n      }\n      loadEdits(editStreams, getNamesystem());\n    }\n    \n    // now, need to load the in-progress file\n    synchronized (this) {\n      if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n        LOG.debug(\"Logs rolled while catching up to current segment\");\n        return false; // drop lock and try again to load local logs\n      }\n      \n      EditLogInputStream stream \u003d null;\n      Collection\u003cEditLogInputStream\u003e editStreams\n        \u003d getEditLog().selectInputStreams(\n            getEditLog().getCurSegmentTxId(),\n            getEditLog().getCurSegmentTxId());\n      \n      for (EditLogInputStream s : editStreams) {\n        if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n          stream \u003d s;\n        }\n        break;\n      }\n      if (stream \u003d\u003d null) {\n        LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n                 + \". This indicates that there is an error in synchronization in BackupImage\");\n        return false;\n      }\n\n      try {\n        long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n        \n        LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n            + \" txns from in-progress stream \" + stream);\n        \n        FSEditLogLoader loader \u003d\n            new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n        loader.loadFSEdits(stream, lastAppliedTxId + 1);\n        lastAppliedTxId \u003d loader.getLastAppliedTxId();\n        assert lastAppliedTxId \u003d\u003d getEditLog().getLastWrittenTxId();\n      } finally {\n        FSEditLog.closeAllStreams(editStreams);\n      }\n\n      LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n          lastAppliedTxId);\n      setState(BNState.IN_SYNC);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "d8bc523754181b4c1321bcfab886ebf228d9c98f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5753. Add new NN startup options for downgrade and rollback using upgrade marker.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1559907 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/01/14 6:38 PM",
      "commitName": "d8bc523754181b4c1321bcfab886ebf228d9c98f",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "20/02/13 12:02 PM",
      "commitNameOld": "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 334.28,
      "commitsBetweenForRepo": 2039,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,77 @@\n   private boolean tryConvergeJournalSpool() throws IOException {\n     Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n         \"bad state: %s\", bnState);\n     \n     // This section is unsynchronized so we can continue to apply\n     // ahead of where we\u0027re reading, concurrently. Since the state\n     // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n     // doesn\u0027t change, and curSegmentTxId only increases\n \n     while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n       long target \u003d editLog.getCurSegmentTxId();\n       LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n           + lastAppliedTxId + \" to \" + target);\n       FSImageTransactionalStorageInspector inspector \u003d\n         new FSImageTransactionalStorageInspector();\n       \n       storage.inspectStorageDirs(inspector);\n \n       editLog.recoverUnclosedStreams();\n       Iterable\u003cEditLogInputStream\u003e editStreamsAll \n         \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n       // remove inprogress\n       List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n       for (EditLogInputStream s : editStreamsAll) {\n         if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n           editStreams.add(s);\n         }\n       }\n-      loadEdits(editStreams, namesystem, null);\n+      loadEdits(editStreams, namesystem);\n     }\n     \n     // now, need to load the in-progress file\n     synchronized (this) {\n       if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n         LOG.debug(\"Logs rolled while catching up to current segment\");\n         return false; // drop lock and try again to load local logs\n       }\n       \n       EditLogInputStream stream \u003d null;\n       Collection\u003cEditLogInputStream\u003e editStreams\n         \u003d getEditLog().selectInputStreams(\n             getEditLog().getCurSegmentTxId(),\n             getEditLog().getCurSegmentTxId());\n       \n       for (EditLogInputStream s : editStreams) {\n         if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n           stream \u003d s;\n         }\n         break;\n       }\n       if (stream \u003d\u003d null) {\n         LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n                  + \". This indicates that there is an error in synchronization in BackupImage\");\n         return false;\n       }\n \n       try {\n         long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n         \n         LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n             + \" txns from in-progress stream \" + stream);\n         \n         FSEditLogLoader loader \u003d\n             new FSEditLogLoader(namesystem, lastAppliedTxId);\n-        loader.loadFSEdits(stream, lastAppliedTxId + 1, null);\n+        loader.loadFSEdits(stream, lastAppliedTxId + 1);\n         lastAppliedTxId \u003d loader.getLastAppliedTxId();\n         assert lastAppliedTxId \u003d\u003d getEditLog().getLastWrittenTxId();\n       } finally {\n         FSEditLog.closeAllStreams(editStreams);\n       }\n \n       LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n           lastAppliedTxId);\n       setState(BNState.IN_SYNC);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean tryConvergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n    \n    // This section is unsynchronized so we can continue to apply\n    // ahead of where we\u0027re reading, concurrently. Since the state\n    // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n    // doesn\u0027t change, and curSegmentTxId only increases\n\n    while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n      long target \u003d editLog.getCurSegmentTxId();\n      LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n          + lastAppliedTxId + \" to \" + target);\n      FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n      \n      storage.inspectStorageDirs(inspector);\n\n      editLog.recoverUnclosedStreams();\n      Iterable\u003cEditLogInputStream\u003e editStreamsAll \n        \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n      // remove inprogress\n      List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n      for (EditLogInputStream s : editStreamsAll) {\n        if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n          editStreams.add(s);\n        }\n      }\n      loadEdits(editStreams, namesystem);\n    }\n    \n    // now, need to load the in-progress file\n    synchronized (this) {\n      if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n        LOG.debug(\"Logs rolled while catching up to current segment\");\n        return false; // drop lock and try again to load local logs\n      }\n      \n      EditLogInputStream stream \u003d null;\n      Collection\u003cEditLogInputStream\u003e editStreams\n        \u003d getEditLog().selectInputStreams(\n            getEditLog().getCurSegmentTxId(),\n            getEditLog().getCurSegmentTxId());\n      \n      for (EditLogInputStream s : editStreams) {\n        if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n          stream \u003d s;\n        }\n        break;\n      }\n      if (stream \u003d\u003d null) {\n        LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n                 + \". This indicates that there is an error in synchronization in BackupImage\");\n        return false;\n      }\n\n      try {\n        long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n        \n        LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n            + \" txns from in-progress stream \" + stream);\n        \n        FSEditLogLoader loader \u003d\n            new FSEditLogLoader(namesystem, lastAppliedTxId);\n        loader.loadFSEdits(stream, lastAppliedTxId + 1);\n        lastAppliedTxId \u003d loader.getLastAppliedTxId();\n        assert lastAppliedTxId \u003d\u003d getEditLog().getLastWrittenTxId();\n      } finally {\n        FSEditLog.closeAllStreams(editStreams);\n      }\n\n      LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n          lastAppliedTxId);\n      setState(BNState.IN_SYNC);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "706394d03992b394e9f907aff2155df493e4ea4e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3004. Implement Recovery Mode. Contributed by Colin Patrick McCabe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1311394 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/04/12 12:39 PM",
      "commitName": "706394d03992b394e9f907aff2155df493e4ea4e",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "05/04/12 5:08 PM",
      "commitNameOld": "861c872541b614971c73a3ae46fa3846d729dbee",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 3.81,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,77 @@\n   private boolean tryConvergeJournalSpool() throws IOException {\n     Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n         \"bad state: %s\", bnState);\n     \n     // This section is unsynchronized so we can continue to apply\n     // ahead of where we\u0027re reading, concurrently. Since the state\n     // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n     // doesn\u0027t change, and curSegmentTxId only increases\n \n     while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n       long target \u003d editLog.getCurSegmentTxId();\n       LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n           + lastAppliedTxId + \" to \" + target);\n       FSImageTransactionalStorageInspector inspector \u003d\n         new FSImageTransactionalStorageInspector();\n       \n       storage.inspectStorageDirs(inspector);\n \n       editLog.recoverUnclosedStreams();\n       Iterable\u003cEditLogInputStream\u003e editStreamsAll \n         \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n       // remove inprogress\n       List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n       for (EditLogInputStream s : editStreamsAll) {\n         if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n           editStreams.add(s);\n         }\n       }\n-      loadEdits(editStreams, namesystem);\n+      loadEdits(editStreams, namesystem, null);\n     }\n     \n     // now, need to load the in-progress file\n     synchronized (this) {\n       if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n         LOG.debug(\"Logs rolled while catching up to current segment\");\n         return false; // drop lock and try again to load local logs\n       }\n       \n       EditLogInputStream stream \u003d null;\n       Collection\u003cEditLogInputStream\u003e editStreams\n         \u003d getEditLog().selectInputStreams(\n             getEditLog().getCurSegmentTxId(),\n             getEditLog().getCurSegmentTxId());\n       \n       for (EditLogInputStream s : editStreams) {\n         if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n           stream \u003d s;\n         }\n         break;\n       }\n       if (stream \u003d\u003d null) {\n         LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n                  + \". This indicates that there is an error in synchronization in BackupImage\");\n         return false;\n       }\n \n       try {\n         long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n         \n         LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n             + \" txns from in-progress stream \" + stream);\n         \n-        FSEditLogLoader loader \u003d new FSEditLogLoader(namesystem);\n-        long numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n-        lastAppliedTxId +\u003d numLoaded;\n-        assert numLoaded \u003d\u003d remainingTxns :\n-          \"expected to load \" + remainingTxns + \" but loaded \" +\n-          numLoaded + \" from \" + stream;\n+        FSEditLogLoader loader \u003d\n+            new FSEditLogLoader(namesystem, lastAppliedTxId);\n+        loader.loadFSEdits(stream, lastAppliedTxId + 1, null);\n+        lastAppliedTxId \u003d loader.getLastAppliedTxId();\n+        assert lastAppliedTxId \u003d\u003d getEditLog().getLastWrittenTxId();\n       } finally {\n         FSEditLog.closeAllStreams(editStreams);\n       }\n \n       LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n           lastAppliedTxId);\n       setState(BNState.IN_SYNC);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean tryConvergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n    \n    // This section is unsynchronized so we can continue to apply\n    // ahead of where we\u0027re reading, concurrently. Since the state\n    // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n    // doesn\u0027t change, and curSegmentTxId only increases\n\n    while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n      long target \u003d editLog.getCurSegmentTxId();\n      LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n          + lastAppliedTxId + \" to \" + target);\n      FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n      \n      storage.inspectStorageDirs(inspector);\n\n      editLog.recoverUnclosedStreams();\n      Iterable\u003cEditLogInputStream\u003e editStreamsAll \n        \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n      // remove inprogress\n      List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n      for (EditLogInputStream s : editStreamsAll) {\n        if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n          editStreams.add(s);\n        }\n      }\n      loadEdits(editStreams, namesystem, null);\n    }\n    \n    // now, need to load the in-progress file\n    synchronized (this) {\n      if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n        LOG.debug(\"Logs rolled while catching up to current segment\");\n        return false; // drop lock and try again to load local logs\n      }\n      \n      EditLogInputStream stream \u003d null;\n      Collection\u003cEditLogInputStream\u003e editStreams\n        \u003d getEditLog().selectInputStreams(\n            getEditLog().getCurSegmentTxId(),\n            getEditLog().getCurSegmentTxId());\n      \n      for (EditLogInputStream s : editStreams) {\n        if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n          stream \u003d s;\n        }\n        break;\n      }\n      if (stream \u003d\u003d null) {\n        LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n                 + \". This indicates that there is an error in synchronization in BackupImage\");\n        return false;\n      }\n\n      try {\n        long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n        \n        LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n            + \" txns from in-progress stream \" + stream);\n        \n        FSEditLogLoader loader \u003d\n            new FSEditLogLoader(namesystem, lastAppliedTxId);\n        loader.loadFSEdits(stream, lastAppliedTxId + 1, null);\n        lastAppliedTxId \u003d loader.getLastAppliedTxId();\n        assert lastAppliedTxId \u003d\u003d getEditLog().getLastWrittenTxId();\n      } finally {\n        FSEditLog.closeAllStreams(editStreams);\n      }\n\n      LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n          lastAppliedTxId);\n      setState(BNState.IN_SYNC);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "9a07ba8945407cd8f63169faf9e0faa4311d38c7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2709. Appropriately handle error conditions in EditLogTailer. Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1228390 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/01/12 12:44 PM",
      "commitName": "9a07ba8945407cd8f63169faf9e0faa4311d38c7",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "08/12/11 3:55 PM",
      "commitNameOld": "2481474bd9c50a23e4fd2eea67ac2dea11ca1f58",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 28.87,
      "commitsBetweenForRepo": 175,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,78 @@\n   private boolean tryConvergeJournalSpool() throws IOException {\n     Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n         \"bad state: %s\", bnState);\n     \n     // This section is unsynchronized so we can continue to apply\n     // ahead of where we\u0027re reading, concurrently. Since the state\n     // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n     // doesn\u0027t change, and curSegmentTxId only increases\n \n     while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n       long target \u003d editLog.getCurSegmentTxId();\n       LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n           + lastAppliedTxId + \" to \" + target);\n       FSImageTransactionalStorageInspector inspector \u003d\n         new FSImageTransactionalStorageInspector();\n       \n       storage.inspectStorageDirs(inspector);\n \n       editLog.recoverUnclosedStreams();\n       Iterable\u003cEditLogInputStream\u003e editStreamsAll \n         \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n       // remove inprogress\n       List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n       for (EditLogInputStream s : editStreamsAll) {\n         if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n           editStreams.add(s);\n         }\n       }\n       loadEdits(editStreams, namesystem);\n     }\n     \n     // now, need to load the in-progress file\n     synchronized (this) {\n       if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n         LOG.debug(\"Logs rolled while catching up to current segment\");\n         return false; // drop lock and try again to load local logs\n       }\n       \n       EditLogInputStream stream \u003d null;\n       Collection\u003cEditLogInputStream\u003e editStreams\n         \u003d getEditLog().selectInputStreams(\n             getEditLog().getCurSegmentTxId(),\n             getEditLog().getCurSegmentTxId());\n       \n       for (EditLogInputStream s : editStreams) {\n         if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n           stream \u003d s;\n         }\n         break;\n       }\n       if (stream \u003d\u003d null) {\n         LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n                  + \". This indicates that there is an error in synchronization in BackupImage\");\n         return false;\n       }\n \n       try {\n         long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n         \n         LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n             + \" txns from in-progress stream \" + stream);\n         \n         FSEditLogLoader loader \u003d new FSEditLogLoader(namesystem);\n-        int numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n+        long numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n         lastAppliedTxId +\u003d numLoaded;\n         assert numLoaded \u003d\u003d remainingTxns :\n           \"expected to load \" + remainingTxns + \" but loaded \" +\n           numLoaded + \" from \" + stream;\n       } finally {\n         FSEditLog.closeAllStreams(editStreams);\n       }\n \n       LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n           lastAppliedTxId);\n       setState(BNState.IN_SYNC);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean tryConvergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n    \n    // This section is unsynchronized so we can continue to apply\n    // ahead of where we\u0027re reading, concurrently. Since the state\n    // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n    // doesn\u0027t change, and curSegmentTxId only increases\n\n    while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n      long target \u003d editLog.getCurSegmentTxId();\n      LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n          + lastAppliedTxId + \" to \" + target);\n      FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n      \n      storage.inspectStorageDirs(inspector);\n\n      editLog.recoverUnclosedStreams();\n      Iterable\u003cEditLogInputStream\u003e editStreamsAll \n        \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n      // remove inprogress\n      List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n      for (EditLogInputStream s : editStreamsAll) {\n        if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n          editStreams.add(s);\n        }\n      }\n      loadEdits(editStreams, namesystem);\n    }\n    \n    // now, need to load the in-progress file\n    synchronized (this) {\n      if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n        LOG.debug(\"Logs rolled while catching up to current segment\");\n        return false; // drop lock and try again to load local logs\n      }\n      \n      EditLogInputStream stream \u003d null;\n      Collection\u003cEditLogInputStream\u003e editStreams\n        \u003d getEditLog().selectInputStreams(\n            getEditLog().getCurSegmentTxId(),\n            getEditLog().getCurSegmentTxId());\n      \n      for (EditLogInputStream s : editStreams) {\n        if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n          stream \u003d s;\n        }\n        break;\n      }\n      if (stream \u003d\u003d null) {\n        LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n                 + \". This indicates that there is an error in synchronization in BackupImage\");\n        return false;\n      }\n\n      try {\n        long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n        \n        LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n            + \" txns from in-progress stream \" + stream);\n        \n        FSEditLogLoader loader \u003d new FSEditLogLoader(namesystem);\n        long numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n        lastAppliedTxId +\u003d numLoaded;\n        assert numLoaded \u003d\u003d remainingTxns :\n          \"expected to load \" + remainingTxns + \" but loaded \" +\n          numLoaded + \" from \" + stream;\n      } finally {\n        FSEditLog.closeAllStreams(editStreams);\n      }\n\n      LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n          lastAppliedTxId);\n      setState(BNState.IN_SYNC);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "06e84a1bca19bd01568a3095e33944d4d6387fd3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2223. Untangle depencencies between NN components. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1166466 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/09/11 4:23 PM",
      "commitName": "06e84a1bca19bd01568a3095e33944d4d6387fd3",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "06/09/11 1:27 PM",
      "commitNameOld": "bdc3720d5b67a1c8fc2dfb29be16e4155c0e7f15",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 1.12,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,78 @@\n   private boolean tryConvergeJournalSpool() throws IOException {\n     Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n         \"bad state: %s\", bnState);\n     \n     // This section is unsynchronized so we can continue to apply\n     // ahead of where we\u0027re reading, concurrently. Since the state\n     // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n     // doesn\u0027t change, and curSegmentTxId only increases\n \n     while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n       long target \u003d editLog.getCurSegmentTxId();\n       LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n           + lastAppliedTxId + \" to \" + target);\n       FSImageTransactionalStorageInspector inspector \u003d\n         new FSImageTransactionalStorageInspector();\n       \n       storage.inspectStorageDirs(inspector);\n \n       editLog.recoverUnclosedStreams();\n       Iterable\u003cEditLogInputStream\u003e editStreamsAll \n         \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n       // remove inprogress\n       List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n       for (EditLogInputStream s : editStreamsAll) {\n         if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n           editStreams.add(s);\n         }\n       }\n-      loadEdits(editStreams);\n+      loadEdits(editStreams, namesystem);\n     }\n     \n     // now, need to load the in-progress file\n     synchronized (this) {\n       if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n         LOG.debug(\"Logs rolled while catching up to current segment\");\n         return false; // drop lock and try again to load local logs\n       }\n       \n       EditLogInputStream stream \u003d null;\n       Collection\u003cEditLogInputStream\u003e editStreams\n         \u003d getEditLog().selectInputStreams(\n             getEditLog().getCurSegmentTxId(),\n             getEditLog().getCurSegmentTxId());\n       \n       for (EditLogInputStream s : editStreams) {\n         if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n           stream \u003d s;\n         }\n         break;\n       }\n       if (stream \u003d\u003d null) {\n         LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n                  + \". This indicates that there is an error in synchronization in BackupImage\");\n         return false;\n       }\n \n       try {\n         long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n         \n         LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n             + \" txns from in-progress stream \" + stream);\n         \n         FSEditLogLoader loader \u003d new FSEditLogLoader(namesystem);\n         int numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n         lastAppliedTxId +\u003d numLoaded;\n         assert numLoaded \u003d\u003d remainingTxns :\n           \"expected to load \" + remainingTxns + \" but loaded \" +\n           numLoaded + \" from \" + stream;\n       } finally {\n         FSEditLog.closeAllStreams(editStreams);\n       }\n \n       LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n           lastAppliedTxId);\n       setState(BNState.IN_SYNC);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean tryConvergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n    \n    // This section is unsynchronized so we can continue to apply\n    // ahead of where we\u0027re reading, concurrently. Since the state\n    // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n    // doesn\u0027t change, and curSegmentTxId only increases\n\n    while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n      long target \u003d editLog.getCurSegmentTxId();\n      LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n          + lastAppliedTxId + \" to \" + target);\n      FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n      \n      storage.inspectStorageDirs(inspector);\n\n      editLog.recoverUnclosedStreams();\n      Iterable\u003cEditLogInputStream\u003e editStreamsAll \n        \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n      // remove inprogress\n      List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n      for (EditLogInputStream s : editStreamsAll) {\n        if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n          editStreams.add(s);\n        }\n      }\n      loadEdits(editStreams, namesystem);\n    }\n    \n    // now, need to load the in-progress file\n    synchronized (this) {\n      if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n        LOG.debug(\"Logs rolled while catching up to current segment\");\n        return false; // drop lock and try again to load local logs\n      }\n      \n      EditLogInputStream stream \u003d null;\n      Collection\u003cEditLogInputStream\u003e editStreams\n        \u003d getEditLog().selectInputStreams(\n            getEditLog().getCurSegmentTxId(),\n            getEditLog().getCurSegmentTxId());\n      \n      for (EditLogInputStream s : editStreams) {\n        if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n          stream \u003d s;\n        }\n        break;\n      }\n      if (stream \u003d\u003d null) {\n        LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n                 + \". This indicates that there is an error in synchronization in BackupImage\");\n        return false;\n      }\n\n      try {\n        long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n        \n        LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n            + \" txns from in-progress stream \" + stream);\n        \n        FSEditLogLoader loader \u003d new FSEditLogLoader(namesystem);\n        int numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n        lastAppliedTxId +\u003d numLoaded;\n        assert numLoaded \u003d\u003d remainingTxns :\n          \"expected to load \" + remainingTxns + \" but loaded \" +\n          numLoaded + \" from \" + stream;\n      } finally {\n        FSEditLog.closeAllStreams(editStreams);\n      }\n\n      LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n          lastAppliedTxId);\n      setState(BNState.IN_SYNC);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "bdc3720d5b67a1c8fc2dfb29be16e4155c0e7f15": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2018. Move all journal stream management code into one place. Contributed by Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165826 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/09/11 1:27 PM",
      "commitName": "bdc3720d5b67a1c8fc2dfb29be16e4155c0e7f15",
      "commitAuthor": "Jitendra Nath Pandey",
      "commitDateOld": "04/09/11 12:30 PM",
      "commitNameOld": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 2.04,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,78 @@\n   private boolean tryConvergeJournalSpool() throws IOException {\n     Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n         \"bad state: %s\", bnState);\n     \n     // This section is unsynchronized so we can continue to apply\n     // ahead of where we\u0027re reading, concurrently. Since the state\n     // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n     // doesn\u0027t change, and curSegmentTxId only increases\n \n     while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n       long target \u003d editLog.getCurSegmentTxId();\n       LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n           + lastAppliedTxId + \" to \" + target);\n       FSImageTransactionalStorageInspector inspector \u003d\n         new FSImageTransactionalStorageInspector();\n       \n       storage.inspectStorageDirs(inspector);\n-      LogLoadPlan logLoadPlan \u003d inspector.createLogLoadPlan(lastAppliedTxId,\n-          target - 1);\n-  \n-      logLoadPlan.doRecovery();\n-      loadEdits(logLoadPlan.getEditsFiles());\n+\n+      editLog.recoverUnclosedStreams();\n+      Iterable\u003cEditLogInputStream\u003e editStreamsAll \n+        \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n+      // remove inprogress\n+      List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n+      for (EditLogInputStream s : editStreamsAll) {\n+        if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n+          editStreams.add(s);\n+        }\n+      }\n+      loadEdits(editStreams);\n     }\n     \n     // now, need to load the in-progress file\n     synchronized (this) {\n       if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n         LOG.debug(\"Logs rolled while catching up to current segment\");\n         return false; // drop lock and try again to load local logs\n       }\n       \n-      EditLogInputStream stream \u003d getEditLog().getInProgressFileInputStream();\n+      EditLogInputStream stream \u003d null;\n+      Collection\u003cEditLogInputStream\u003e editStreams\n+        \u003d getEditLog().selectInputStreams(\n+            getEditLog().getCurSegmentTxId(),\n+            getEditLog().getCurSegmentTxId());\n+      \n+      for (EditLogInputStream s : editStreams) {\n+        if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n+          stream \u003d s;\n+        }\n+        break;\n+      }\n+      if (stream \u003d\u003d null) {\n+        LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n+                 + \". This indicates that there is an error in synchronization in BackupImage\");\n+        return false;\n+      }\n+\n       try {\n         long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n         \n         LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n             + \" txns from in-progress stream \" + stream);\n         \n         FSEditLogLoader loader \u003d new FSEditLogLoader(namesystem);\n         int numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n         lastAppliedTxId +\u003d numLoaded;\n         assert numLoaded \u003d\u003d remainingTxns :\n           \"expected to load \" + remainingTxns + \" but loaded \" +\n           numLoaded + \" from \" + stream;\n       } finally {\n-        IOUtils.closeStream(stream);\n+        FSEditLog.closeAllStreams(editStreams);\n       }\n \n       LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n           lastAppliedTxId);\n       setState(BNState.IN_SYNC);\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean tryConvergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n    \n    // This section is unsynchronized so we can continue to apply\n    // ahead of where we\u0027re reading, concurrently. Since the state\n    // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n    // doesn\u0027t change, and curSegmentTxId only increases\n\n    while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n      long target \u003d editLog.getCurSegmentTxId();\n      LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n          + lastAppliedTxId + \" to \" + target);\n      FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n      \n      storage.inspectStorageDirs(inspector);\n\n      editLog.recoverUnclosedStreams();\n      Iterable\u003cEditLogInputStream\u003e editStreamsAll \n        \u003d editLog.selectInputStreams(lastAppliedTxId, target - 1);\n      // remove inprogress\n      List\u003cEditLogInputStream\u003e editStreams \u003d Lists.newArrayList();\n      for (EditLogInputStream s : editStreamsAll) {\n        if (s.getFirstTxId() !\u003d editLog.getCurSegmentTxId()) {\n          editStreams.add(s);\n        }\n      }\n      loadEdits(editStreams);\n    }\n    \n    // now, need to load the in-progress file\n    synchronized (this) {\n      if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n        LOG.debug(\"Logs rolled while catching up to current segment\");\n        return false; // drop lock and try again to load local logs\n      }\n      \n      EditLogInputStream stream \u003d null;\n      Collection\u003cEditLogInputStream\u003e editStreams\n        \u003d getEditLog().selectInputStreams(\n            getEditLog().getCurSegmentTxId(),\n            getEditLog().getCurSegmentTxId());\n      \n      for (EditLogInputStream s : editStreams) {\n        if (s.getFirstTxId() \u003d\u003d getEditLog().getCurSegmentTxId()) {\n          stream \u003d s;\n        }\n        break;\n      }\n      if (stream \u003d\u003d null) {\n        LOG.warn(\"Unable to find stream starting with \" + editLog.getCurSegmentTxId()\n                 + \". This indicates that there is an error in synchronization in BackupImage\");\n        return false;\n      }\n\n      try {\n        long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n        \n        LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n            + \" txns from in-progress stream \" + stream);\n        \n        FSEditLogLoader loader \u003d new FSEditLogLoader(namesystem);\n        int numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n        lastAppliedTxId +\u003d numLoaded;\n        assert numLoaded \u003d\u003d remainingTxns :\n          \"expected to load \" + remainingTxns + \" but loaded \" +\n          numLoaded + \" from \" + stream;\n      } finally {\n        FSEditLog.closeAllStreams(editStreams);\n      }\n\n      LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n          lastAppliedTxId);\n      setState(BNState.IN_SYNC);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean tryConvergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n    \n    // This section is unsynchronized so we can continue to apply\n    // ahead of where we\u0027re reading, concurrently. Since the state\n    // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n    // doesn\u0027t change, and curSegmentTxId only increases\n\n    while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n      long target \u003d editLog.getCurSegmentTxId();\n      LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n          + lastAppliedTxId + \" to \" + target);\n      FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n      \n      storage.inspectStorageDirs(inspector);\n      LogLoadPlan logLoadPlan \u003d inspector.createLogLoadPlan(lastAppliedTxId,\n          target - 1);\n  \n      logLoadPlan.doRecovery();\n      loadEdits(logLoadPlan.getEditsFiles());\n    }\n    \n    // now, need to load the in-progress file\n    synchronized (this) {\n      if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n        LOG.debug(\"Logs rolled while catching up to current segment\");\n        return false; // drop lock and try again to load local logs\n      }\n      \n      EditLogInputStream stream \u003d getEditLog().getInProgressFileInputStream();\n      try {\n        long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n        \n        LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n            + \" txns from in-progress stream \" + stream);\n        \n        FSEditLogLoader loader \u003d new FSEditLogLoader(namesystem);\n        int numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n        lastAppliedTxId +\u003d numLoaded;\n        assert numLoaded \u003d\u003d remainingTxns :\n          \"expected to load \" + remainingTxns + \" but loaded \" +\n          numLoaded + \" from \" + stream;\n      } finally {\n        IOUtils.closeStream(stream);\n      }\n\n      LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n          lastAppliedTxId);\n      setState(BNState.IN_SYNC);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean tryConvergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n    \n    // This section is unsynchronized so we can continue to apply\n    // ahead of where we\u0027re reading, concurrently. Since the state\n    // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n    // doesn\u0027t change, and curSegmentTxId only increases\n\n    while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n      long target \u003d editLog.getCurSegmentTxId();\n      LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n          + lastAppliedTxId + \" to \" + target);\n      FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n      \n      storage.inspectStorageDirs(inspector);\n      LogLoadPlan logLoadPlan \u003d inspector.createLogLoadPlan(lastAppliedTxId,\n          target - 1);\n  \n      logLoadPlan.doRecovery();\n      loadEdits(logLoadPlan.getEditsFiles());\n    }\n    \n    // now, need to load the in-progress file\n    synchronized (this) {\n      if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n        LOG.debug(\"Logs rolled while catching up to current segment\");\n        return false; // drop lock and try again to load local logs\n      }\n      \n      EditLogInputStream stream \u003d getEditLog().getInProgressFileInputStream();\n      try {\n        long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n        \n        LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n            + \" txns from in-progress stream \" + stream);\n        \n        FSEditLogLoader loader \u003d new FSEditLogLoader(namesystem);\n        int numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n        lastAppliedTxId +\u003d numLoaded;\n        assert numLoaded \u003d\u003d remainingTxns :\n          \"expected to load \" + remainingTxns + \" but loaded \" +\n          numLoaded + \" from \" + stream;\n      } finally {\n        IOUtils.closeStream(stream);\n      }\n\n      LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n          lastAppliedTxId);\n      setState(BNState.IN_SYNC);\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java"
      }
    },
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/07/11 9:28 AM",
      "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,54 @@\n+  private boolean tryConvergeJournalSpool() throws IOException {\n+    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n+        \"bad state: %s\", bnState);\n+    \n+    // This section is unsynchronized so we can continue to apply\n+    // ahead of where we\u0027re reading, concurrently. Since the state\n+    // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n+    // doesn\u0027t change, and curSegmentTxId only increases\n+\n+    while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n+      long target \u003d editLog.getCurSegmentTxId();\n+      LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n+          + lastAppliedTxId + \" to \" + target);\n+      FSImageTransactionalStorageInspector inspector \u003d\n+        new FSImageTransactionalStorageInspector();\n+      \n+      storage.inspectStorageDirs(inspector);\n+      LogLoadPlan logLoadPlan \u003d inspector.createLogLoadPlan(lastAppliedTxId,\n+          target - 1);\n+  \n+      logLoadPlan.doRecovery();\n+      loadEdits(logLoadPlan.getEditsFiles());\n+    }\n+    \n+    // now, need to load the in-progress file\n+    synchronized (this) {\n+      if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n+        LOG.debug(\"Logs rolled while catching up to current segment\");\n+        return false; // drop lock and try again to load local logs\n+      }\n+      \n+      EditLogInputStream stream \u003d getEditLog().getInProgressFileInputStream();\n+      try {\n+        long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n+        \n+        LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n+            + \" txns from in-progress stream \" + stream);\n+        \n+        FSEditLogLoader loader \u003d new FSEditLogLoader(namesystem);\n+        int numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n+        lastAppliedTxId +\u003d numLoaded;\n+        assert numLoaded \u003d\u003d remainingTxns :\n+          \"expected to load \" + remainingTxns + \" but loaded \" +\n+          numLoaded + \" from \" + stream;\n+      } finally {\n+        IOUtils.closeStream(stream);\n+      }\n+\n+      LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n+          lastAppliedTxId);\n+      setState(BNState.IN_SYNC);\n+    }\n+    return true;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean tryConvergeJournalSpool() throws IOException {\n    Preconditions.checkState(bnState \u003d\u003d BNState.JOURNAL_ONLY,\n        \"bad state: %s\", bnState);\n    \n    // This section is unsynchronized so we can continue to apply\n    // ahead of where we\u0027re reading, concurrently. Since the state\n    // is JOURNAL_ONLY at this point, we know that lastAppliedTxId\n    // doesn\u0027t change, and curSegmentTxId only increases\n\n    while (lastAppliedTxId \u003c editLog.getCurSegmentTxId() - 1) {\n      long target \u003d editLog.getCurSegmentTxId();\n      LOG.info(\"Loading edits into backupnode to try to catch up from txid \"\n          + lastAppliedTxId + \" to \" + target);\n      FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n      \n      storage.inspectStorageDirs(inspector);\n      LogLoadPlan logLoadPlan \u003d inspector.createLogLoadPlan(lastAppliedTxId,\n          target - 1);\n  \n      logLoadPlan.doRecovery();\n      loadEdits(logLoadPlan.getEditsFiles());\n    }\n    \n    // now, need to load the in-progress file\n    synchronized (this) {\n      if (lastAppliedTxId !\u003d editLog.getCurSegmentTxId() - 1) {\n        LOG.debug(\"Logs rolled while catching up to current segment\");\n        return false; // drop lock and try again to load local logs\n      }\n      \n      EditLogInputStream stream \u003d getEditLog().getInProgressFileInputStream();\n      try {\n        long remainingTxns \u003d getEditLog().getLastWrittenTxId() - lastAppliedTxId;\n        \n        LOG.info(\"Going to finish converging with remaining \" + remainingTxns\n            + \" txns from in-progress stream \" + stream);\n        \n        FSEditLogLoader loader \u003d new FSEditLogLoader(namesystem);\n        int numLoaded \u003d loader.loadFSEdits(stream, lastAppliedTxId + 1);\n        lastAppliedTxId +\u003d numLoaded;\n        assert numLoaded \u003d\u003d remainingTxns :\n          \"expected to load \" + remainingTxns + \" but loaded \" +\n          numLoaded + \" from \" + stream;\n      } finally {\n        IOUtils.closeStream(stream);\n      }\n\n      LOG.info(\"Successfully synced BackupNode with NameNode at txnid \" +\n          lastAppliedTxId);\n      setState(BNState.IN_SYNC);\n    }\n    return true;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java"
    }
  }
}