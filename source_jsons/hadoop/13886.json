{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "processAndHandleReportedBlock",
  "functionId": "processAndHandleReportedBlock___storageInfo-DatanodeStorageInfo__block-Block__reportedState-ReplicaState__delHintNode-DatanodeDescriptor",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 4180,
  "functionEndLine": 4250,
  "numCommitsSeen": 617,
  "timeTaken": 16406,
  "changeHistory": [
    "7f8685f4760f1358bb30927a7da9a5041e8c39e1",
    "a7f085d6bf499edf23e650a4f7211c53a442da0e",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
    "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "ba9371492036983a9899398907ab41fe548f29b3",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4"
  ],
  "changeHistoryShort": {
    "7f8685f4760f1358bb30927a7da9a5041e8c39e1": "Ymultichange(Yreturntypechange,Ybodychange)",
    "a7f085d6bf499edf23e650a4f7211c53a442da0e": "Ybodychange",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": "Ybodychange",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": "Ybodychange",
    "d311a38a6b32bbb210bd8748cfb65463e9c0740e": "Ybodychange",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "ba9371492036983a9899398907ab41fe548f29b3": "Ybodychange",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": "Ybodychange",
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "7f8685f4760f1358bb30927a7da9a5041e8c39e1": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-15187. CORRUPT replica mismatch between namenodes after failover. Contributed by Ayush Saxena.\n",
      "commitDate": "24/02/20 7:08 AM",
      "commitName": "7f8685f4760f1358bb30927a7da9a5041e8c39e1",
      "commitAuthor": "Ayush Saxena",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-15187. CORRUPT replica mismatch between namenodes after failover. Contributed by Ayush Saxena.\n",
          "commitDate": "24/02/20 7:08 AM",
          "commitName": "7f8685f4760f1358bb30927a7da9a5041e8c39e1",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "23/02/20 12:55 AM",
          "commitNameOld": "9eb7a8bdf8f3b1dc76efc22db9651474303d309e",
          "commitAuthorOld": "Ayush Saxena",
          "daysBetweenCommits": 1.26,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,70 +1,71 @@\n-  private void processAndHandleReportedBlock(\n+  private boolean processAndHandleReportedBlock(\n       DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n \n     final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n \n     LOG.debug(\"Reported block {} on {} size {} replicaState \u003d {}\",\n         block, node, block.getNumBytes(), reportedState);\n \n     if (shouldPostponeBlocksFromFuture \u0026\u0026\n         isGenStampInFuture(block)) {\n       queueReportedBlock(storageInfo, block, reportedState,\n           QUEUE_REASON_FUTURE_GENSTAMP);\n-      return;\n+      return false;\n     }\n \n     // find block by blockId\n     BlockInfo storedBlock \u003d getStoredBlock(block);\n     if(storedBlock \u003d\u003d null) {\n       // If blocksMap does not contain reported block id,\n       // the replica should be removed from the data-node.\n       blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n           \"belong to any file\", block, node, block.getNumBytes());\n       addToInvalidates(new Block(block), node);\n-      return;\n+      return true;\n     }\n \n     BlockUCState ucState \u003d storedBlock.getBlockUCState();\n     // Block is on the NN\n     LOG.debug(\"In memory blockUCState \u003d {}\", ucState);\n \n     // Ignore replicas already scheduled to be removed from the DN\n     if(invalidateBlocks.contains(node, block)) {\n-      return;\n+      return true;\n     }\n \n     BlockToMarkCorrupt c \u003d checkReplicaCorrupt(\n         block, reportedState, storedBlock, ucState, node);\n     if (c !\u003d null) {\n       if (shouldPostponeBlocksFromFuture) {\n         // If the block is an out-of-date generation stamp or state,\n         // but we\u0027re the standby, we shouldn\u0027t treat it as corrupt,\n         // but instead just queue it for later processing.\n         // TODO: Pretty confident this should be s/storedBlock/block below,\n         // since we should be postponing the info of the reported block, not\n         // the stored block. See HDFS-6289 for more context.\n         queueReportedBlock(storageInfo, storedBlock, reportedState,\n             QUEUE_REASON_CORRUPT_STATE);\n       } else {\n         markBlockAsCorrupt(c, storageInfo, node);\n       }\n-      return;\n+      return true;\n     }\n \n     if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {\n       addStoredBlockUnderConstruction(\n           new StatefulBlockInfo(storedBlock, new Block(block), reportedState),\n           storageInfo);\n-      return;\n+      return true;\n     }\n \n     // Add replica if appropriate. If the replica was previously corrupt\n     // but now okay, it might need to be updated.\n     if (reportedState \u003d\u003d ReplicaState.FINALIZED\n         \u0026\u0026 (storedBlock.findStorageInfo(storageInfo) \u003d\u003d -1 ||\n             corruptReplicas.isReplicaCorrupt(storedBlock, node))) {\n       addStoredBlock(storedBlock, block, storageInfo, delHintNode, true);\n     }\n+    return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    LOG.debug(\"Reported block {} on {} size {} replicaState \u003d {}\",\n        block, node, block.getNumBytes(), reportedState);\n\n    if (shouldPostponeBlocksFromFuture \u0026\u0026\n        isGenStampInFuture(block)) {\n      queueReportedBlock(storageInfo, block, reportedState,\n          QUEUE_REASON_FUTURE_GENSTAMP);\n      return false;\n    }\n\n    // find block by blockId\n    BlockInfo storedBlock \u003d getStoredBlock(block);\n    if(storedBlock \u003d\u003d null) {\n      // If blocksMap does not contain reported block id,\n      // the replica should be removed from the data-node.\n      blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n          \"belong to any file\", block, node, block.getNumBytes());\n      addToInvalidates(new Block(block), node);\n      return true;\n    }\n\n    BlockUCState ucState \u003d storedBlock.getBlockUCState();\n    // Block is on the NN\n    LOG.debug(\"In memory blockUCState \u003d {}\", ucState);\n\n    // Ignore replicas already scheduled to be removed from the DN\n    if(invalidateBlocks.contains(node, block)) {\n      return true;\n    }\n\n    BlockToMarkCorrupt c \u003d checkReplicaCorrupt(\n        block, reportedState, storedBlock, ucState, node);\n    if (c !\u003d null) {\n      if (shouldPostponeBlocksFromFuture) {\n        // If the block is an out-of-date generation stamp or state,\n        // but we\u0027re the standby, we shouldn\u0027t treat it as corrupt,\n        // but instead just queue it for later processing.\n        // TODO: Pretty confident this should be s/storedBlock/block below,\n        // since we should be postponing the info of the reported block, not\n        // the stored block. See HDFS-6289 for more context.\n        queueReportedBlock(storageInfo, storedBlock, reportedState,\n            QUEUE_REASON_CORRUPT_STATE);\n      } else {\n        markBlockAsCorrupt(c, storageInfo, node);\n      }\n      return true;\n    }\n\n    if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {\n      addStoredBlockUnderConstruction(\n          new StatefulBlockInfo(storedBlock, new Block(block), reportedState),\n          storageInfo);\n      return true;\n    }\n\n    // Add replica if appropriate. If the replica was previously corrupt\n    // but now okay, it might need to be updated.\n    if (reportedState \u003d\u003d ReplicaState.FINALIZED\n        \u0026\u0026 (storedBlock.findStorageInfo(storageInfo) \u003d\u003d -1 ||\n            corruptReplicas.isReplicaCorrupt(storedBlock, node))) {\n      addStoredBlock(storedBlock, block, storageInfo, delHintNode, true);\n    }\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "boolean"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-15187. CORRUPT replica mismatch between namenodes after failover. Contributed by Ayush Saxena.\n",
          "commitDate": "24/02/20 7:08 AM",
          "commitName": "7f8685f4760f1358bb30927a7da9a5041e8c39e1",
          "commitAuthor": "Ayush Saxena",
          "commitDateOld": "23/02/20 12:55 AM",
          "commitNameOld": "9eb7a8bdf8f3b1dc76efc22db9651474303d309e",
          "commitAuthorOld": "Ayush Saxena",
          "daysBetweenCommits": 1.26,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,70 +1,71 @@\n-  private void processAndHandleReportedBlock(\n+  private boolean processAndHandleReportedBlock(\n       DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n \n     final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n \n     LOG.debug(\"Reported block {} on {} size {} replicaState \u003d {}\",\n         block, node, block.getNumBytes(), reportedState);\n \n     if (shouldPostponeBlocksFromFuture \u0026\u0026\n         isGenStampInFuture(block)) {\n       queueReportedBlock(storageInfo, block, reportedState,\n           QUEUE_REASON_FUTURE_GENSTAMP);\n-      return;\n+      return false;\n     }\n \n     // find block by blockId\n     BlockInfo storedBlock \u003d getStoredBlock(block);\n     if(storedBlock \u003d\u003d null) {\n       // If blocksMap does not contain reported block id,\n       // the replica should be removed from the data-node.\n       blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n           \"belong to any file\", block, node, block.getNumBytes());\n       addToInvalidates(new Block(block), node);\n-      return;\n+      return true;\n     }\n \n     BlockUCState ucState \u003d storedBlock.getBlockUCState();\n     // Block is on the NN\n     LOG.debug(\"In memory blockUCState \u003d {}\", ucState);\n \n     // Ignore replicas already scheduled to be removed from the DN\n     if(invalidateBlocks.contains(node, block)) {\n-      return;\n+      return true;\n     }\n \n     BlockToMarkCorrupt c \u003d checkReplicaCorrupt(\n         block, reportedState, storedBlock, ucState, node);\n     if (c !\u003d null) {\n       if (shouldPostponeBlocksFromFuture) {\n         // If the block is an out-of-date generation stamp or state,\n         // but we\u0027re the standby, we shouldn\u0027t treat it as corrupt,\n         // but instead just queue it for later processing.\n         // TODO: Pretty confident this should be s/storedBlock/block below,\n         // since we should be postponing the info of the reported block, not\n         // the stored block. See HDFS-6289 for more context.\n         queueReportedBlock(storageInfo, storedBlock, reportedState,\n             QUEUE_REASON_CORRUPT_STATE);\n       } else {\n         markBlockAsCorrupt(c, storageInfo, node);\n       }\n-      return;\n+      return true;\n     }\n \n     if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {\n       addStoredBlockUnderConstruction(\n           new StatefulBlockInfo(storedBlock, new Block(block), reportedState),\n           storageInfo);\n-      return;\n+      return true;\n     }\n \n     // Add replica if appropriate. If the replica was previously corrupt\n     // but now okay, it might need to be updated.\n     if (reportedState \u003d\u003d ReplicaState.FINALIZED\n         \u0026\u0026 (storedBlock.findStorageInfo(storageInfo) \u003d\u003d -1 ||\n             corruptReplicas.isReplicaCorrupt(storedBlock, node))) {\n       addStoredBlock(storedBlock, block, storageInfo, delHintNode, true);\n     }\n+    return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    LOG.debug(\"Reported block {} on {} size {} replicaState \u003d {}\",\n        block, node, block.getNumBytes(), reportedState);\n\n    if (shouldPostponeBlocksFromFuture \u0026\u0026\n        isGenStampInFuture(block)) {\n      queueReportedBlock(storageInfo, block, reportedState,\n          QUEUE_REASON_FUTURE_GENSTAMP);\n      return false;\n    }\n\n    // find block by blockId\n    BlockInfo storedBlock \u003d getStoredBlock(block);\n    if(storedBlock \u003d\u003d null) {\n      // If blocksMap does not contain reported block id,\n      // the replica should be removed from the data-node.\n      blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n          \"belong to any file\", block, node, block.getNumBytes());\n      addToInvalidates(new Block(block), node);\n      return true;\n    }\n\n    BlockUCState ucState \u003d storedBlock.getBlockUCState();\n    // Block is on the NN\n    LOG.debug(\"In memory blockUCState \u003d {}\", ucState);\n\n    // Ignore replicas already scheduled to be removed from the DN\n    if(invalidateBlocks.contains(node, block)) {\n      return true;\n    }\n\n    BlockToMarkCorrupt c \u003d checkReplicaCorrupt(\n        block, reportedState, storedBlock, ucState, node);\n    if (c !\u003d null) {\n      if (shouldPostponeBlocksFromFuture) {\n        // If the block is an out-of-date generation stamp or state,\n        // but we\u0027re the standby, we shouldn\u0027t treat it as corrupt,\n        // but instead just queue it for later processing.\n        // TODO: Pretty confident this should be s/storedBlock/block below,\n        // since we should be postponing the info of the reported block, not\n        // the stored block. See HDFS-6289 for more context.\n        queueReportedBlock(storageInfo, storedBlock, reportedState,\n            QUEUE_REASON_CORRUPT_STATE);\n      } else {\n        markBlockAsCorrupt(c, storageInfo, node);\n      }\n      return true;\n    }\n\n    if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {\n      addStoredBlockUnderConstruction(\n          new StatefulBlockInfo(storedBlock, new Block(block), reportedState),\n          storageInfo);\n      return true;\n    }\n\n    // Add replica if appropriate. If the replica was previously corrupt\n    // but now okay, it might need to be updated.\n    if (reportedState \u003d\u003d ReplicaState.FINALIZED\n        \u0026\u0026 (storedBlock.findStorageInfo(storageInfo) \u003d\u003d -1 ||\n            corruptReplicas.isReplicaCorrupt(storedBlock, node))) {\n      addStoredBlock(storedBlock, block, storageInfo, delHintNode, true);\n    }\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "a7f085d6bf499edf23e650a4f7211c53a442da0e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11832. Switch leftover logs to slf4j format in BlockManager.java. Contributed by Hui Xu and Chen Liang.\n",
      "commitDate": "29/05/17 1:30 AM",
      "commitName": "a7f085d6bf499edf23e650a4f7211c53a442da0e",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "25/05/17 7:35 AM",
      "commitNameOld": "2e41f8803dd46d1bab16c1b206c71be72ea260a1",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 3.75,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,70 @@\n   private void processAndHandleReportedBlock(\n       DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n \n     final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n \n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"Reported block \" + block\n-          + \" on \" + node + \" size \" + block.getNumBytes()\n-          + \" replicaState \u003d \" + reportedState);\n-    }\n+    LOG.debug(\"Reported block {} on {} size {} replicaState \u003d {}\",\n+        block, node, block.getNumBytes(), reportedState);\n \n     if (shouldPostponeBlocksFromFuture \u0026\u0026\n         isGenStampInFuture(block)) {\n       queueReportedBlock(storageInfo, block, reportedState,\n           QUEUE_REASON_FUTURE_GENSTAMP);\n       return;\n     }\n \n     // find block by blockId\n     BlockInfo storedBlock \u003d getStoredBlock(block);\n     if(storedBlock \u003d\u003d null) {\n       // If blocksMap does not contain reported block id,\n       // the replica should be removed from the data-node.\n       blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n           \"belong to any file\", block, node, block.getNumBytes());\n       addToInvalidates(new Block(block), node);\n       return;\n     }\n \n     BlockUCState ucState \u003d storedBlock.getBlockUCState();\n     // Block is on the NN\n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"In memory blockUCState \u003d \" + ucState);\n-    }\n+    LOG.debug(\"In memory blockUCState \u003d {}\", ucState);\n \n     // Ignore replicas already scheduled to be removed from the DN\n     if(invalidateBlocks.contains(node, block)) {\n       return;\n     }\n \n     BlockToMarkCorrupt c \u003d checkReplicaCorrupt(\n         block, reportedState, storedBlock, ucState, node);\n     if (c !\u003d null) {\n       if (shouldPostponeBlocksFromFuture) {\n         // If the block is an out-of-date generation stamp or state,\n         // but we\u0027re the standby, we shouldn\u0027t treat it as corrupt,\n         // but instead just queue it for later processing.\n         // TODO: Pretty confident this should be s/storedBlock/block below,\n         // since we should be postponing the info of the reported block, not\n         // the stored block. See HDFS-6289 for more context.\n         queueReportedBlock(storageInfo, storedBlock, reportedState,\n             QUEUE_REASON_CORRUPT_STATE);\n       } else {\n         markBlockAsCorrupt(c, storageInfo, node);\n       }\n       return;\n     }\n \n     if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {\n       addStoredBlockUnderConstruction(\n           new StatefulBlockInfo(storedBlock, new Block(block), reportedState),\n           storageInfo);\n       return;\n     }\n \n     // Add replica if appropriate. If the replica was previously corrupt\n     // but now okay, it might need to be updated.\n     if (reportedState \u003d\u003d ReplicaState.FINALIZED\n         \u0026\u0026 (storedBlock.findStorageInfo(storageInfo) \u003d\u003d -1 ||\n             corruptReplicas.isReplicaCorrupt(storedBlock, node))) {\n       addStoredBlock(storedBlock, block, storageInfo, delHintNode, true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    LOG.debug(\"Reported block {} on {} size {} replicaState \u003d {}\",\n        block, node, block.getNumBytes(), reportedState);\n\n    if (shouldPostponeBlocksFromFuture \u0026\u0026\n        isGenStampInFuture(block)) {\n      queueReportedBlock(storageInfo, block, reportedState,\n          QUEUE_REASON_FUTURE_GENSTAMP);\n      return;\n    }\n\n    // find block by blockId\n    BlockInfo storedBlock \u003d getStoredBlock(block);\n    if(storedBlock \u003d\u003d null) {\n      // If blocksMap does not contain reported block id,\n      // the replica should be removed from the data-node.\n      blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n          \"belong to any file\", block, node, block.getNumBytes());\n      addToInvalidates(new Block(block), node);\n      return;\n    }\n\n    BlockUCState ucState \u003d storedBlock.getBlockUCState();\n    // Block is on the NN\n    LOG.debug(\"In memory blockUCState \u003d {}\", ucState);\n\n    // Ignore replicas already scheduled to be removed from the DN\n    if(invalidateBlocks.contains(node, block)) {\n      return;\n    }\n\n    BlockToMarkCorrupt c \u003d checkReplicaCorrupt(\n        block, reportedState, storedBlock, ucState, node);\n    if (c !\u003d null) {\n      if (shouldPostponeBlocksFromFuture) {\n        // If the block is an out-of-date generation stamp or state,\n        // but we\u0027re the standby, we shouldn\u0027t treat it as corrupt,\n        // but instead just queue it for later processing.\n        // TODO: Pretty confident this should be s/storedBlock/block below,\n        // since we should be postponing the info of the reported block, not\n        // the stored block. See HDFS-6289 for more context.\n        queueReportedBlock(storageInfo, storedBlock, reportedState,\n            QUEUE_REASON_CORRUPT_STATE);\n      } else {\n        markBlockAsCorrupt(c, storageInfo, node);\n      }\n      return;\n    }\n\n    if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {\n      addStoredBlockUnderConstruction(\n          new StatefulBlockInfo(storedBlock, new Block(block), reportedState),\n          storageInfo);\n      return;\n    }\n\n    // Add replica if appropriate. If the replica was previously corrupt\n    // but now okay, it might need to be updated.\n    if (reportedState \u003d\u003d ReplicaState.FINALIZED\n        \u0026\u0026 (storedBlock.findStorageInfo(storageInfo) \u003d\u003d -1 ||\n            corruptReplicas.isReplicaCorrupt(storedBlock, node))) {\n      addStoredBlock(storedBlock, block, storageInfo, delHintNode, true);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9260. Improve the performance and GC friendliness of NameNode startup and full block reports (Staffan Friberg via cmccabe)\n",
      "commitDate": "02/02/16 11:23 AM",
      "commitName": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "31/01/16 11:54 PM",
      "commitNameOld": "e418bd1fb0568ce7ae22f588fea2dd9c95567383",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 1.48,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,75 @@\n   private void processAndHandleReportedBlock(\n       DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n-    // blockReceived reports a finalized block\n-    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n-    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n-    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n-    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n+\n     final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n \n-    processReportedBlock(storageInfo, block, reportedState, toAdd, toInvalidate,\n-        toCorrupt, toUC);\n-    // the block is only in one of the to-do lists\n-    // if it is in none then data-node already has it\n-    assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n-      : \"The block should be only in one of the lists.\";\n+    if(LOG.isDebugEnabled()) {\n+      LOG.debug(\"Reported block \" + block\n+          + \" on \" + node + \" size \" + block.getNumBytes()\n+          + \" replicaState \u003d \" + reportedState);\n+    }\n \n-    for (StatefulBlockInfo b : toUC) { \n-      addStoredBlockUnderConstruction(b, storageInfo);\n+    if (shouldPostponeBlocksFromFuture \u0026\u0026\n+        isGenStampInFuture(block)) {\n+      queueReportedBlock(storageInfo, block, reportedState,\n+          QUEUE_REASON_FUTURE_GENSTAMP);\n+      return;\n     }\n-    long numBlocksLogged \u003d 0;\n-    for (BlockInfoToAdd b : toAdd) {\n-      addStoredBlock(b.stored, b.reported, storageInfo, delHintNode,\n-          numBlocksLogged \u003c maxNumBlocksToLog);\n-      numBlocksLogged++;\n-    }\n-    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n-      blockLog.debug(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n-          maxNumBlocksToLog, numBlocksLogged);\n-    }\n-    for (Block b : toInvalidate) {\n+\n+    // find block by blockId\n+    BlockInfo storedBlock \u003d getStoredBlock(block);\n+    if(storedBlock \u003d\u003d null) {\n+      // If blocksMap does not contain reported block id,\n+      // the replica should be removed from the data-node.\n       blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n-          \"belong to any file\", b, node, b.getNumBytes());\n-      addToInvalidates(b, node);\n+          \"belong to any file\", block, node, block.getNumBytes());\n+      addToInvalidates(new Block(block), node);\n+      return;\n     }\n-    for (BlockToMarkCorrupt b : toCorrupt) {\n-      markBlockAsCorrupt(b, storageInfo, node);\n+\n+    BlockUCState ucState \u003d storedBlock.getBlockUCState();\n+    // Block is on the NN\n+    if(LOG.isDebugEnabled()) {\n+      LOG.debug(\"In memory blockUCState \u003d \" + ucState);\n+    }\n+\n+    // Ignore replicas already scheduled to be removed from the DN\n+    if(invalidateBlocks.contains(node, block)) {\n+      return;\n+    }\n+\n+    BlockToMarkCorrupt c \u003d checkReplicaCorrupt(\n+        block, reportedState, storedBlock, ucState, node);\n+    if (c !\u003d null) {\n+      if (shouldPostponeBlocksFromFuture) {\n+        // If the block is an out-of-date generation stamp or state,\n+        // but we\u0027re the standby, we shouldn\u0027t treat it as corrupt,\n+        // but instead just queue it for later processing.\n+        // TODO: Pretty confident this should be s/storedBlock/block below,\n+        // since we should be postponing the info of the reported block, not\n+        // the stored block. See HDFS-6289 for more context.\n+        queueReportedBlock(storageInfo, storedBlock, reportedState,\n+            QUEUE_REASON_CORRUPT_STATE);\n+      } else {\n+        markBlockAsCorrupt(c, storageInfo, node);\n+      }\n+      return;\n+    }\n+\n+    if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {\n+      addStoredBlockUnderConstruction(\n+          new StatefulBlockInfo(storedBlock, new Block(block), reportedState),\n+          storageInfo);\n+      return;\n+    }\n+\n+    // Add replica if appropriate. If the replica was previously corrupt\n+    // but now okay, it might need to be updated.\n+    if (reportedState \u003d\u003d ReplicaState.FINALIZED\n+        \u0026\u0026 (storedBlock.findStorageInfo(storageInfo) \u003d\u003d -1 ||\n+            corruptReplicas.isReplicaCorrupt(storedBlock, node))) {\n+      addStoredBlock(storedBlock, block, storageInfo, delHintNode, true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Reported block \" + block\n          + \" on \" + node + \" size \" + block.getNumBytes()\n          + \" replicaState \u003d \" + reportedState);\n    }\n\n    if (shouldPostponeBlocksFromFuture \u0026\u0026\n        isGenStampInFuture(block)) {\n      queueReportedBlock(storageInfo, block, reportedState,\n          QUEUE_REASON_FUTURE_GENSTAMP);\n      return;\n    }\n\n    // find block by blockId\n    BlockInfo storedBlock \u003d getStoredBlock(block);\n    if(storedBlock \u003d\u003d null) {\n      // If blocksMap does not contain reported block id,\n      // the replica should be removed from the data-node.\n      blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n          \"belong to any file\", block, node, block.getNumBytes());\n      addToInvalidates(new Block(block), node);\n      return;\n    }\n\n    BlockUCState ucState \u003d storedBlock.getBlockUCState();\n    // Block is on the NN\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"In memory blockUCState \u003d \" + ucState);\n    }\n\n    // Ignore replicas already scheduled to be removed from the DN\n    if(invalidateBlocks.contains(node, block)) {\n      return;\n    }\n\n    BlockToMarkCorrupt c \u003d checkReplicaCorrupt(\n        block, reportedState, storedBlock, ucState, node);\n    if (c !\u003d null) {\n      if (shouldPostponeBlocksFromFuture) {\n        // If the block is an out-of-date generation stamp or state,\n        // but we\u0027re the standby, we shouldn\u0027t treat it as corrupt,\n        // but instead just queue it for later processing.\n        // TODO: Pretty confident this should be s/storedBlock/block below,\n        // since we should be postponing the info of the reported block, not\n        // the stored block. See HDFS-6289 for more context.\n        queueReportedBlock(storageInfo, storedBlock, reportedState,\n            QUEUE_REASON_CORRUPT_STATE);\n      } else {\n        markBlockAsCorrupt(c, storageInfo, node);\n      }\n      return;\n    }\n\n    if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {\n      addStoredBlockUnderConstruction(\n          new StatefulBlockInfo(storedBlock, new Block(block), reportedState),\n          storageInfo);\n      return;\n    }\n\n    // Add replica if appropriate. If the replica was previously corrupt\n    // but now okay, it might need to be updated.\n    if (reportedState \u003d\u003d ReplicaState.FINALIZED\n        \u0026\u0026 (storedBlock.findStorageInfo(storageInfo) \u003d\u003d -1 ||\n            corruptReplicas.isReplicaCorrupt(storedBlock, node))) {\n      addStoredBlock(storedBlock, block, storageInfo, delHintNode, true);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\"\n\nThis reverts commit de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "31/07/15 4:15 PM",
      "commitNameOld": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 5.75,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,39 @@\n   private void processAndHandleReportedBlock(\n       DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n     // blockReceived reports a finalized block\n-    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n-    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n-    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n-    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n+    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n+    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n+    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n+    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n \n-    processReportedBlock(storageInfo, block, reportedState, toAdd, toInvalidate,\n-        toCorrupt, toUC);\n+    processReportedBlock(storageInfo, block, reportedState,\n+                              toAdd, toInvalidate, toCorrupt, toUC);\n     // the block is only in one of the to-do lists\n     // if it is in none then data-node already has it\n     assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n-        : \"The block should be only in one of the lists.\";\n+      : \"The block should be only in one of the lists.\";\n \n-    for (StatefulBlockInfo b : toUC) {\n+    for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     long numBlocksLogged \u003d 0;\n-    for (BlockInfoToAdd b : toAdd) {\n-      addStoredBlock(b.getStored(), b.getReported(), storageInfo, delHintNode,\n-          numBlocksLogged \u003c maxNumBlocksToLog);\n+    for (BlockInfo b : toAdd) {\n+      addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.debug(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n           maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n           \"belong to any file\", b, node, b.getNumBytes());\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n    // blockReceived reports a finalized block\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    processReportedBlock(storageInfo, block, reportedState,\n                              toAdd, toInvalidate, toCorrupt, toUC);\n    // the block is only in one of the to-do lists\n    // if it is in none then data-node already has it\n    assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n      : \"The block should be only in one of the lists.\";\n\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    long numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.debug(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n          maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "d311a38a6b32bbb210bd8748cfb65463e9c0740e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6860. BlockStateChange logs are too noisy. Contributed by Chang Li and Xiaoyu Yao.\n",
      "commitDate": "31/07/15 4:15 PM",
      "commitName": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "29/07/15 12:48 AM",
      "commitNameOld": "2a1d656196cf9750fa482cb10893684e8a2ce7c3",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 2.64,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   private void processAndHandleReportedBlock(\n       DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n     // blockReceived reports a finalized block\n     Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n     final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n \n     processReportedBlock(storageInfo, block, reportedState, toAdd, toInvalidate,\n         toCorrupt, toUC);\n     // the block is only in one of the to-do lists\n     // if it is in none then data-node already has it\n     assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n         : \"The block should be only in one of the lists.\";\n \n     for (StatefulBlockInfo b : toUC) {\n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     long numBlocksLogged \u003d 0;\n     for (BlockInfoToAdd b : toAdd) {\n       addStoredBlock(b.getStored(), b.getReported(), storageInfo, delHintNode,\n           numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n-      blockLog.info(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n+      blockLog.debug(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n           maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n-      blockLog.info(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n+      blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n           \"belong to any file\", b, node, b.getNumBytes());\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n    // blockReceived reports a finalized block\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    processReportedBlock(storageInfo, block, reportedState, toAdd, toInvalidate,\n        toCorrupt, toUC);\n    // the block is only in one of the to-do lists\n    // if it is in none then data-node already has it\n    assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n        : \"The block should be only in one of the lists.\";\n\n    for (StatefulBlockInfo b : toUC) {\n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    long numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.getStored(), b.getReported(), storageInfo, delHintNode,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.debug(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n          maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      blockLog.debug(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\n",
      "commitDate": "26/06/15 10:49 AM",
      "commitName": "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/06/15 2:42 PM",
      "commitNameOld": "afe9ea3c12e1f5a71922400eadb642960bc87ca1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,40 @@\n   private void processAndHandleReportedBlock(\n       DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n     // blockReceived reports a finalized block\n-    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n-    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n-    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n-    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n+    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n+    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n+    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n+    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n     final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n \n-    processReportedBlock(storageInfo, block, reportedState,\n-                              toAdd, toInvalidate, toCorrupt, toUC);\n+    processReportedBlock(storageInfo, block, reportedState, toAdd, toInvalidate,\n+        toCorrupt, toUC);\n     // the block is only in one of the to-do lists\n     // if it is in none then data-node already has it\n     assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n-      : \"The block should be only in one of the lists.\";\n+        : \"The block should be only in one of the lists.\";\n \n-    for (StatefulBlockInfo b : toUC) { \n+    for (StatefulBlockInfo b : toUC) {\n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     long numBlocksLogged \u003d 0;\n-    for (BlockInfo b : toAdd) {\n-      addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n+    for (BlockInfoToAdd b : toAdd) {\n+      addStoredBlock(b.getStored(), b.getReported(), storageInfo, delHintNode,\n+          numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n           maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       blockLog.info(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n           \"belong to any file\", b, node, b.getNumBytes());\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n    // blockReceived reports a finalized block\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    processReportedBlock(storageInfo, block, reportedState, toAdd, toInvalidate,\n        toCorrupt, toUC);\n    // the block is only in one of the to-do lists\n    // if it is in none then data-node already has it\n    assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n        : \"The block should be only in one of the lists.\";\n\n    for (StatefulBlockInfo b : toUC) {\n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    long numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.getStored(), b.getReported(), storageInfo, delHintNode,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n          maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      blockLog.info(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "19/05/15 11:05 AM",
      "commitNameOld": "8860e352c394372e4eb3ebdf82ea899567f34e4e",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 8.19,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,39 @@\n   private void processAndHandleReportedBlock(\n       DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n     // blockReceived reports a finalized block\n-    Collection\u003cBlockInfoContiguous\u003e toAdd \u003d new LinkedList\u003cBlockInfoContiguous\u003e();\n+    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n \n     processReportedBlock(storageInfo, block, reportedState,\n                               toAdd, toInvalidate, toCorrupt, toUC);\n     // the block is only in one of the to-do lists\n     // if it is in none then data-node already has it\n     assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n       : \"The block should be only in one of the lists.\";\n \n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     long numBlocksLogged \u003d 0;\n-    for (BlockInfoContiguous b : toAdd) {\n+    for (BlockInfo b : toAdd) {\n       addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n           maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       blockLog.info(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n           \"belong to any file\", b, node, b.getNumBytes());\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n    // blockReceived reports a finalized block\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    processReportedBlock(storageInfo, block, reportedState,\n                              toAdd, toInvalidate, toCorrupt, toUC);\n    // the block is only in one of the to-do lists\n    // if it is in none then data-node already has it\n    assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n      : \"The block should be only in one of the lists.\";\n\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    long numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n          maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      blockLog.info(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "ba9371492036983a9899398907ab41fe548f29b3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7716. Erasure Coding: extend BlockInfo to handle EC info. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "ba9371492036983a9899398907ab41fe548f29b3",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:03 AM",
      "commitNameOld": "0c1da5a0300f015a7e39f2b40a73fb06c65a78c8",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,40 @@\n   private void processAndHandleReportedBlock(\n       DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n     // blockReceived reports a finalized block\n-    Collection\u003cBlockInfoContiguous\u003e toAdd \u003d new LinkedList\u003cBlockInfoContiguous\u003e();\n+    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n \n-    processReportedBlock(storageInfo, block, reportedState,\n-                              toAdd, toInvalidate, toCorrupt, toUC);\n+    processReportedBlock(storageInfo, block, reportedState, toAdd, toInvalidate,\n+        toCorrupt, toUC);\n     // the block is only in one of the to-do lists\n     // if it is in none then data-node already has it\n     assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n       : \"The block should be only in one of the lists.\";\n \n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     long numBlocksLogged \u003d 0;\n-    for (BlockInfoContiguous b : toAdd) {\n-      addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n+    for (BlockInfoToAdd b : toAdd) {\n+      addStoredBlock(b.stored, b.reported, storageInfo, delHintNode,\n+          numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n           maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       blockLog.info(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n           \"belong to any file\", b, node, b.getNumBytes());\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n    // blockReceived reports a finalized block\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    processReportedBlock(storageInfo, block, reportedState, toAdd, toInvalidate,\n        toCorrupt, toUC);\n    // the block is only in one of the to-do lists\n    // if it is in none then data-node already has it\n    assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n      : \"The block should be only in one of the lists.\";\n\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    long numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.stored, b.reported, storageInfo, delHintNode,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n          maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      blockLog.info(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "04/02/15 11:31 AM",
      "commitNameOld": "9175105eeaecf0a1d60b57989b73ce45cee4689b",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.01,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,39 @@\n   private void processAndHandleReportedBlock(\n       DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n     // blockReceived reports a finalized block\n-    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n+    Collection\u003cBlockInfoContiguous\u003e toAdd \u003d new LinkedList\u003cBlockInfoContiguous\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n \n     processReportedBlock(storageInfo, block, reportedState,\n                               toAdd, toInvalidate, toCorrupt, toUC);\n     // the block is only in one of the to-do lists\n     // if it is in none then data-node already has it\n     assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n       : \"The block should be only in one of the lists.\";\n \n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     long numBlocksLogged \u003d 0;\n-    for (BlockInfo b : toAdd) {\n+    for (BlockInfoContiguous b : toAdd) {\n       addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n           maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       blockLog.info(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n           \"belong to any file\", b, node, b.getNumBytes());\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n    // blockReceived reports a finalized block\n    Collection\u003cBlockInfoContiguous\u003e toAdd \u003d new LinkedList\u003cBlockInfoContiguous\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    processReportedBlock(storageInfo, block, reportedState,\n                              toAdd, toInvalidate, toCorrupt, toUC);\n    // the block is only in one of the to-do lists\n    // if it is in none then data-node already has it\n    assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n      : \"The block should be only in one of the lists.\";\n\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    long numBlocksLogged \u003d 0;\n    for (BlockInfoContiguous b : toAdd) {\n      addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n          maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      blockLog.info(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7712. Switch blockStateChangeLog to use slf4j.\n",
      "commitDate": "03/02/15 3:01 PM",
      "commitName": "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/01/15 11:33 AM",
      "commitNameOld": "951b3608a8cb1d9063b9be9c740b524c137b816f",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.14,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,39 @@\n   private void processAndHandleReportedBlock(\n       DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n     // blockReceived reports a finalized block\n     Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n \n     processReportedBlock(storageInfo, block, reportedState,\n                               toAdd, toInvalidate, toCorrupt, toUC);\n     // the block is only in one of the to-do lists\n     // if it is in none then data-node already has it\n     assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n       : \"The block should be only in one of the lists.\";\n \n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     long numBlocksLogged \u003d 0;\n     for (BlockInfo b : toAdd) {\n       addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n-      blockLog.info(\"BLOCK* addBlock: logged info for \" + maxNumBlocksToLog\n-          + \" of \" + numBlocksLogged + \" reported.\");\n+      blockLog.info(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n+          maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n-      blockLog.info(\"BLOCK* addBlock: block \"\n-          + b + \" on \" + node + \" size \" + b.getNumBytes()\n-          + \" does not belong to any file\");\n+      blockLog.info(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n+          \"belong to any file\", b, node, b.getNumBytes());\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n    // blockReceived reports a finalized block\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    processReportedBlock(storageInfo, block, reportedState,\n                              toAdd, toInvalidate, toCorrupt, toUC);\n    // the block is only in one of the to-do lists\n    // if it is in none then data-node already has it\n    assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n      : \"The block should be only in one of the lists.\";\n\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    long numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* addBlock: logged info for {} of {} reported.\",\n          maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      blockLog.info(\"BLOCK* addBlock: block {} on node {} size {} does not \" +\n          \"belong to any file\", b, node, b.getNumBytes());\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/14 9:58 AM",
      "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "01/08/14 9:58 AM",
          "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "31/07/14 6:05 PM",
          "commitNameOld": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.66,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,38 +1,40 @@\n-  private void processAndHandleReportedBlock(DatanodeDescriptor node,\n-      String storageID, Block block,\n+  private void processAndHandleReportedBlock(\n+      DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n     // blockReceived reports a finalized block\n     Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n-    processReportedBlock(node, storageID, block, reportedState,\n+    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n+\n+    processReportedBlock(storageInfo, block, reportedState,\n                               toAdd, toInvalidate, toCorrupt, toUC);\n     // the block is only in one of the to-do lists\n     // if it is in none then data-node already has it\n     assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n       : \"The block should be only in one of the lists.\";\n \n     for (StatefulBlockInfo b : toUC) { \n-      addStoredBlockUnderConstruction(b, node, storageID);\n+      addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     long numBlocksLogged \u003d 0;\n     for (BlockInfo b : toAdd) {\n-      addStoredBlock(b, node, storageID, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n+      addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* addBlock: logged info for \" + maxNumBlocksToLog\n           + \" of \" + numBlocksLogged + \" reported.\");\n     }\n     for (Block b : toInvalidate) {\n       blockLog.info(\"BLOCK* addBlock: block \"\n           + b + \" on \" + node + \" size \" + b.getNumBytes()\n           + \" does not belong to any file\");\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n-      markBlockAsCorrupt(b, node, storageID);\n+      markBlockAsCorrupt(b, storageInfo, node);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n    // blockReceived reports a finalized block\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    processReportedBlock(storageInfo, block, reportedState,\n                              toAdd, toInvalidate, toCorrupt, toUC);\n    // the block is only in one of the to-do lists\n    // if it is in none then data-node already has it\n    assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n      : \"The block should be only in one of the lists.\";\n\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    long numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* addBlock: logged info for \" + maxNumBlocksToLog\n          + \" of \" + numBlocksLogged + \" reported.\");\n    }\n    for (Block b : toInvalidate) {\n      blockLog.info(\"BLOCK* addBlock: block \"\n          + b + \" on \" + node + \" size \" + b.getNumBytes()\n          + \" does not belong to any file\");\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[node-DatanodeDescriptor, storageID-String, block-Block, reportedState-ReplicaState, delHintNode-DatanodeDescriptor]",
            "newValue": "[storageInfo-DatanodeStorageInfo, block-Block, reportedState-ReplicaState, delHintNode-DatanodeDescriptor]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "01/08/14 9:58 AM",
          "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "31/07/14 6:05 PM",
          "commitNameOld": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.66,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,38 +1,40 @@\n-  private void processAndHandleReportedBlock(DatanodeDescriptor node,\n-      String storageID, Block block,\n+  private void processAndHandleReportedBlock(\n+      DatanodeStorageInfo storageInfo, Block block,\n       ReplicaState reportedState, DatanodeDescriptor delHintNode)\n       throws IOException {\n     // blockReceived reports a finalized block\n     Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n-    processReportedBlock(node, storageID, block, reportedState,\n+    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n+\n+    processReportedBlock(storageInfo, block, reportedState,\n                               toAdd, toInvalidate, toCorrupt, toUC);\n     // the block is only in one of the to-do lists\n     // if it is in none then data-node already has it\n     assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n       : \"The block should be only in one of the lists.\";\n \n     for (StatefulBlockInfo b : toUC) { \n-      addStoredBlockUnderConstruction(b, node, storageID);\n+      addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     long numBlocksLogged \u003d 0;\n     for (BlockInfo b : toAdd) {\n-      addStoredBlock(b, node, storageID, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n+      addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* addBlock: logged info for \" + maxNumBlocksToLog\n           + \" of \" + numBlocksLogged + \" reported.\");\n     }\n     for (Block b : toInvalidate) {\n       blockLog.info(\"BLOCK* addBlock: block \"\n           + b + \" on \" + node + \" size \" + b.getNumBytes()\n           + \" does not belong to any file\");\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n-      markBlockAsCorrupt(b, node, storageID);\n+      markBlockAsCorrupt(b, storageInfo, node);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processAndHandleReportedBlock(\n      DatanodeStorageInfo storageInfo, Block block,\n      ReplicaState reportedState, DatanodeDescriptor delHintNode)\n      throws IOException {\n    // blockReceived reports a finalized block\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    final DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n\n    processReportedBlock(storageInfo, block, reportedState,\n                              toAdd, toInvalidate, toCorrupt, toUC);\n    // the block is only in one of the to-do lists\n    // if it is in none then data-node already has it\n    assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() \u003c\u003d 1\n      : \"The block should be only in one of the lists.\";\n\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    long numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* addBlock: logged info for \" + maxNumBlocksToLog\n          + \" of \" + numBlocksLogged + \" reported.\");\n    }\n    for (Block b : toInvalidate) {\n      blockLog.info(\"BLOCK* addBlock: block \"\n          + b + \" on \" + node + \" size \" + b.getNumBytes()\n          + \" does not belong to any file\");\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}