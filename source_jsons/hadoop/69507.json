{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LoadJob.java",
  "functionName": "setup",
  "functionId": "setup___ctxt-Context",
  "sourceFilePath": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
  "functionStartLine": 283,
  "functionEndLine": 374,
  "numCommitsSeen": 11,
  "timeTaken": 4532,
  "changeHistory": [
    "dcf84707ab50662add112bd6b01c0bfd63374853",
    "3edc40e3777a4cf226fff1be0bdc0ac4c2f49f34",
    "5795fcfd9904431ec075fdce7ab8559ff50eccd2",
    "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "dcf84707ab50662add112bd6b01c0bfd63374853": "Yfilerename",
    "3edc40e3777a4cf226fff1be0bdc0ac4c2f49f34": "Ybodychange",
    "5795fcfd9904431ec075fdce7ab8559ff50eccd2": "Ybodychange",
    "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "dcf84707ab50662add112bd6b01c0bfd63374853": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3543. Mavenize Gridmix. (tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1339629 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/05/12 8:06 AM",
      "commitName": "dcf84707ab50662add112bd6b01c0bfd63374853",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "17/05/12 7:20 AM",
      "commitNameOld": "e1f09365ca0bee093f849fcf2e546dd6e2c0a965",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected void setup(Context ctxt) \n    throws IOException, InterruptedException {\n      final Configuration conf \u003d ctxt.getConfiguration();\n      final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n      final int maps \u003d split.getMapCount();\n      final long[] reduceBytes \u003d split.getOutputBytes();\n      final long[] reduceRecords \u003d split.getOutputRecords();\n\n      long totalRecords \u003d 0L;\n      final int nReduces \u003d ctxt.getNumReduceTasks();\n      if (nReduces \u003e 0) {\n        // enable gridmix map output record for compression\n        boolean emulateMapOutputCompression \u003d \n          CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n          \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n        float compressionRatio \u003d 1.0f;\n        if (emulateMapOutputCompression) {\n          compressionRatio \u003d \n            CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n          LOG.info(\"GridMix is configured to use a compression ratio of \" \n                   + compressionRatio + \" for the map output data.\");\n          key.setCompressibility(true, compressionRatio);\n          val.setCompressibility(true, compressionRatio);\n        }\n        \n        int idx \u003d 0;\n        int id \u003d split.getId();\n        for (int i \u003d 0; i \u003c nReduces; ++i) {\n          final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n          if (i \u003d\u003d id) {\n            spec.bytes_out \u003d split.getReduceBytes(idx);\n            spec.rec_out \u003d split.getReduceRecords(idx);\n            spec.setResourceUsageSpecification(\n                   split.getReduceResourceUsageMetrics(idx));\n            ++idx;\n            id +\u003d maps;\n          }\n          \n          // set the map output bytes such that the final reduce input bytes \n          // match the expected value obtained from the original job\n          long mapOutputBytes \u003d reduceBytes[i];\n          if (emulateMapOutputCompression) {\n            mapOutputBytes /\u003d compressionRatio;\n          }\n          reduces.add(new IntermediateRecordFactory(\n              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                   5*1024),\n              i, reduceRecords[i], spec, conf));\n          totalRecords +\u003d reduceRecords[i];\n        }\n      } else {\n        long mapOutputBytes \u003d reduceBytes[0];\n        \n        // enable gridmix job output compression\n        boolean emulateJobOutputCompression \u003d \n          CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n          \u0026\u0026 conf.getBoolean(FileOutputFormat.COMPRESS, false);\n\n        if (emulateJobOutputCompression) {\n          float compressionRatio \u003d \n            CompressionEmulationUtil.getJobOutputCompressionEmulationRatio(conf);\n          LOG.info(\"GridMix is configured to use a compression ratio of \" \n                   + compressionRatio + \" for the job output data.\");\n          key.setCompressibility(true, compressionRatio);\n          val.setCompressibility(true, compressionRatio);\n\n          // set the output size accordingly\n          mapOutputBytes /\u003d compressionRatio;\n        }\n        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                         conf, 5*1024));\n        totalRecords \u003d reduceRecords[0];\n      }\n      final long splitRecords \u003d split.getInputRecords();\n      int missingRecSize \u003d \n        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n      final long inputRecords \u003d \n        (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n        ? Math.max(1, split.getLength() / missingRecSize)\n        : splitRecords;\n      ratio \u003d totalRecords / (1.0 * inputRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(ctxt, \n                      split.getMapResourceUsageMetrics());\n      matcher.setDaemon(true);\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(ctxt, matcher);\n      reporter.setDaemon(true);\n      reporter.start();\n    }",
      "path": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
        "newPath": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
      }
    },
    "3edc40e3777a4cf226fff1be0bdc0ac4c2f49f34": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4100. [Gridmix] Bug fixed in compression emulation feature for map only jobs. (amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1327816 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/04/12 9:26 PM",
      "commitName": "3edc40e3777a4cf226fff1be0bdc0ac4c2f49f34",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "05/03/12 5:44 AM",
      "commitNameOld": "231e39462dbfe60f66710e0425dbf16069382dbe",
      "commitAuthorOld": "Ravi Gummadi",
      "daysBetweenCommits": 44.61,
      "commitsBetweenForRepo": 333,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,92 @@\n     protected void setup(Context ctxt) \n     throws IOException, InterruptedException {\n       final Configuration conf \u003d ctxt.getConfiguration();\n       final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n       final int maps \u003d split.getMapCount();\n       final long[] reduceBytes \u003d split.getOutputBytes();\n       final long[] reduceRecords \u003d split.getOutputRecords();\n \n-      // enable gridmix map output record for compression\n-      final boolean emulateMapOutputCompression \u003d \n-        CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n-        \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n-      float compressionRatio \u003d 1.0f;\n-      if (emulateMapOutputCompression) {\n-        compressionRatio \u003d \n-          CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n-        LOG.info(\"GridMix is configured to use a compression ratio of \" \n-                 + compressionRatio + \" for the map output data.\");\n-        key.setCompressibility(true, compressionRatio);\n-        val.setCompressibility(true, compressionRatio);\n-      }\n-      \n       long totalRecords \u003d 0L;\n       final int nReduces \u003d ctxt.getNumReduceTasks();\n       if (nReduces \u003e 0) {\n+        // enable gridmix map output record for compression\n+        boolean emulateMapOutputCompression \u003d \n+          CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n+          \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n+        float compressionRatio \u003d 1.0f;\n+        if (emulateMapOutputCompression) {\n+          compressionRatio \u003d \n+            CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n+          LOG.info(\"GridMix is configured to use a compression ratio of \" \n+                   + compressionRatio + \" for the map output data.\");\n+          key.setCompressibility(true, compressionRatio);\n+          val.setCompressibility(true, compressionRatio);\n+        }\n+        \n         int idx \u003d 0;\n         int id \u003d split.getId();\n         for (int i \u003d 0; i \u003c nReduces; ++i) {\n           final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n           if (i \u003d\u003d id) {\n             spec.bytes_out \u003d split.getReduceBytes(idx);\n             spec.rec_out \u003d split.getReduceRecords(idx);\n             spec.setResourceUsageSpecification(\n                    split.getReduceResourceUsageMetrics(idx));\n             ++idx;\n             id +\u003d maps;\n           }\n           \n           // set the map output bytes such that the final reduce input bytes \n           // match the expected value obtained from the original job\n           long mapOutputBytes \u003d reduceBytes[i];\n           if (emulateMapOutputCompression) {\n             mapOutputBytes /\u003d compressionRatio;\n           }\n           reduces.add(new IntermediateRecordFactory(\n               new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                    5*1024),\n               i, reduceRecords[i], spec, conf));\n           totalRecords +\u003d reduceRecords[i];\n         }\n       } else {\n         long mapOutputBytes \u003d reduceBytes[0];\n-        if (emulateMapOutputCompression) {\n+        \n+        // enable gridmix job output compression\n+        boolean emulateJobOutputCompression \u003d \n+          CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n+          \u0026\u0026 conf.getBoolean(FileOutputFormat.COMPRESS, false);\n+\n+        if (emulateJobOutputCompression) {\n+          float compressionRatio \u003d \n+            CompressionEmulationUtil.getJobOutputCompressionEmulationRatio(conf);\n+          LOG.info(\"GridMix is configured to use a compression ratio of \" \n+                   + compressionRatio + \" for the job output data.\");\n+          key.setCompressibility(true, compressionRatio);\n+          val.setCompressibility(true, compressionRatio);\n+\n+          // set the output size accordingly\n           mapOutputBytes /\u003d compressionRatio;\n         }\n         reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                          conf, 5*1024));\n         totalRecords \u003d reduceRecords[0];\n       }\n       final long splitRecords \u003d split.getInputRecords();\n       int missingRecSize \u003d \n         conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n       final long inputRecords \u003d \n         (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n         ? Math.max(1, split.getLength() / missingRecSize)\n         : splitRecords;\n       ratio \u003d totalRecords / (1.0 * inputRecords);\n       acc \u003d 0.0;\n       \n       matcher \u003d new ResourceUsageMatcherRunner(ctxt, \n                       split.getMapResourceUsageMetrics());\n       matcher.setDaemon(true);\n       \n       // start the status reporter thread\n       reporter \u003d new StatusReporter(ctxt, matcher);\n       reporter.setDaemon(true);\n       reporter.start();\n     }\n\\ No newline at end of file\n",
      "actualSource": "    protected void setup(Context ctxt) \n    throws IOException, InterruptedException {\n      final Configuration conf \u003d ctxt.getConfiguration();\n      final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n      final int maps \u003d split.getMapCount();\n      final long[] reduceBytes \u003d split.getOutputBytes();\n      final long[] reduceRecords \u003d split.getOutputRecords();\n\n      long totalRecords \u003d 0L;\n      final int nReduces \u003d ctxt.getNumReduceTasks();\n      if (nReduces \u003e 0) {\n        // enable gridmix map output record for compression\n        boolean emulateMapOutputCompression \u003d \n          CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n          \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n        float compressionRatio \u003d 1.0f;\n        if (emulateMapOutputCompression) {\n          compressionRatio \u003d \n            CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n          LOG.info(\"GridMix is configured to use a compression ratio of \" \n                   + compressionRatio + \" for the map output data.\");\n          key.setCompressibility(true, compressionRatio);\n          val.setCompressibility(true, compressionRatio);\n        }\n        \n        int idx \u003d 0;\n        int id \u003d split.getId();\n        for (int i \u003d 0; i \u003c nReduces; ++i) {\n          final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n          if (i \u003d\u003d id) {\n            spec.bytes_out \u003d split.getReduceBytes(idx);\n            spec.rec_out \u003d split.getReduceRecords(idx);\n            spec.setResourceUsageSpecification(\n                   split.getReduceResourceUsageMetrics(idx));\n            ++idx;\n            id +\u003d maps;\n          }\n          \n          // set the map output bytes such that the final reduce input bytes \n          // match the expected value obtained from the original job\n          long mapOutputBytes \u003d reduceBytes[i];\n          if (emulateMapOutputCompression) {\n            mapOutputBytes /\u003d compressionRatio;\n          }\n          reduces.add(new IntermediateRecordFactory(\n              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                   5*1024),\n              i, reduceRecords[i], spec, conf));\n          totalRecords +\u003d reduceRecords[i];\n        }\n      } else {\n        long mapOutputBytes \u003d reduceBytes[0];\n        \n        // enable gridmix job output compression\n        boolean emulateJobOutputCompression \u003d \n          CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n          \u0026\u0026 conf.getBoolean(FileOutputFormat.COMPRESS, false);\n\n        if (emulateJobOutputCompression) {\n          float compressionRatio \u003d \n            CompressionEmulationUtil.getJobOutputCompressionEmulationRatio(conf);\n          LOG.info(\"GridMix is configured to use a compression ratio of \" \n                   + compressionRatio + \" for the job output data.\");\n          key.setCompressibility(true, compressionRatio);\n          val.setCompressibility(true, compressionRatio);\n\n          // set the output size accordingly\n          mapOutputBytes /\u003d compressionRatio;\n        }\n        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                         conf, 5*1024));\n        totalRecords \u003d reduceRecords[0];\n      }\n      final long splitRecords \u003d split.getInputRecords();\n      int missingRecSize \u003d \n        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n      final long inputRecords \u003d \n        (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n        ? Math.max(1, split.getLength() / missingRecSize)\n        : splitRecords;\n      ratio \u003d totalRecords / (1.0 * inputRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(ctxt, \n                      split.getMapResourceUsageMetrics());\n      matcher.setDaemon(true);\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(ctxt, matcher);\n      reporter.setDaemon(true);\n      reporter.start();\n    }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {}
    },
    "5795fcfd9904431ec075fdce7ab8559ff50eccd2": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3058. Fixed MR YarnChild to report failure when task throws an error and thus prevent a hanging task and job. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1187654 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/10/11 11:14 PM",
      "commitName": "5795fcfd9904431ec075fdce7ab8559ff50eccd2",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "06/10/11 9:59 PM",
      "commitNameOld": "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd",
      "commitAuthorOld": "Amar Kamat",
      "daysBetweenCommits": 15.05,
      "commitsBetweenForRepo": 122,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,78 @@\n     protected void setup(Context ctxt) \n     throws IOException, InterruptedException {\n       final Configuration conf \u003d ctxt.getConfiguration();\n       final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n       final int maps \u003d split.getMapCount();\n       final long[] reduceBytes \u003d split.getOutputBytes();\n       final long[] reduceRecords \u003d split.getOutputRecords();\n \n       // enable gridmix map output record for compression\n       final boolean emulateMapOutputCompression \u003d \n         CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n         \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n       float compressionRatio \u003d 1.0f;\n       if (emulateMapOutputCompression) {\n         compressionRatio \u003d \n           CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n         LOG.info(\"GridMix is configured to use a compression ratio of \" \n                  + compressionRatio + \" for the map output data.\");\n         key.setCompressibility(true, compressionRatio);\n         val.setCompressibility(true, compressionRatio);\n       }\n       \n       long totalRecords \u003d 0L;\n       final int nReduces \u003d ctxt.getNumReduceTasks();\n       if (nReduces \u003e 0) {\n         int idx \u003d 0;\n         int id \u003d split.getId();\n         for (int i \u003d 0; i \u003c nReduces; ++i) {\n           final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n           if (i \u003d\u003d id) {\n             spec.bytes_out \u003d split.getReduceBytes(idx);\n             spec.rec_out \u003d split.getReduceRecords(idx);\n             spec.setResourceUsageSpecification(\n                    split.getReduceResourceUsageMetrics(idx));\n             ++idx;\n             id +\u003d maps;\n           }\n           \n           // set the map output bytes such that the final reduce input bytes \n           // match the expected value obtained from the original job\n           long mapOutputBytes \u003d reduceBytes[i];\n           if (emulateMapOutputCompression) {\n             mapOutputBytes /\u003d compressionRatio;\n           }\n           reduces.add(new IntermediateRecordFactory(\n               new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                    5*1024),\n               i, reduceRecords[i], spec, conf));\n           totalRecords +\u003d reduceRecords[i];\n         }\n       } else {\n         long mapOutputBytes \u003d reduceBytes[0];\n         if (emulateMapOutputCompression) {\n           mapOutputBytes /\u003d compressionRatio;\n         }\n         reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                          conf, 5*1024));\n         totalRecords \u003d reduceRecords[0];\n       }\n       final long splitRecords \u003d split.getInputRecords();\n       int missingRecSize \u003d \n         conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n       final long inputRecords \u003d \n         (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n         ? Math.max(1, split.getLength() / missingRecSize)\n         : splitRecords;\n       ratio \u003d totalRecords / (1.0 * inputRecords);\n       acc \u003d 0.0;\n       \n       matcher \u003d new ResourceUsageMatcherRunner(ctxt, \n                       split.getMapResourceUsageMetrics());\n+      matcher.setDaemon(true);\n       \n       // start the status reporter thread\n       reporter \u003d new StatusReporter(ctxt, matcher);\n+      reporter.setDaemon(true);\n       reporter.start();\n     }\n\\ No newline at end of file\n",
      "actualSource": "    protected void setup(Context ctxt) \n    throws IOException, InterruptedException {\n      final Configuration conf \u003d ctxt.getConfiguration();\n      final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n      final int maps \u003d split.getMapCount();\n      final long[] reduceBytes \u003d split.getOutputBytes();\n      final long[] reduceRecords \u003d split.getOutputRecords();\n\n      // enable gridmix map output record for compression\n      final boolean emulateMapOutputCompression \u003d \n        CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n        \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n      float compressionRatio \u003d 1.0f;\n      if (emulateMapOutputCompression) {\n        compressionRatio \u003d \n          CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the map output data.\");\n        key.setCompressibility(true, compressionRatio);\n        val.setCompressibility(true, compressionRatio);\n      }\n      \n      long totalRecords \u003d 0L;\n      final int nReduces \u003d ctxt.getNumReduceTasks();\n      if (nReduces \u003e 0) {\n        int idx \u003d 0;\n        int id \u003d split.getId();\n        for (int i \u003d 0; i \u003c nReduces; ++i) {\n          final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n          if (i \u003d\u003d id) {\n            spec.bytes_out \u003d split.getReduceBytes(idx);\n            spec.rec_out \u003d split.getReduceRecords(idx);\n            spec.setResourceUsageSpecification(\n                   split.getReduceResourceUsageMetrics(idx));\n            ++idx;\n            id +\u003d maps;\n          }\n          \n          // set the map output bytes such that the final reduce input bytes \n          // match the expected value obtained from the original job\n          long mapOutputBytes \u003d reduceBytes[i];\n          if (emulateMapOutputCompression) {\n            mapOutputBytes /\u003d compressionRatio;\n          }\n          reduces.add(new IntermediateRecordFactory(\n              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                   5*1024),\n              i, reduceRecords[i], spec, conf));\n          totalRecords +\u003d reduceRecords[i];\n        }\n      } else {\n        long mapOutputBytes \u003d reduceBytes[0];\n        if (emulateMapOutputCompression) {\n          mapOutputBytes /\u003d compressionRatio;\n        }\n        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                         conf, 5*1024));\n        totalRecords \u003d reduceRecords[0];\n      }\n      final long splitRecords \u003d split.getInputRecords();\n      int missingRecSize \u003d \n        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n      final long inputRecords \u003d \n        (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n        ? Math.max(1, split.getLength() / missingRecSize)\n        : splitRecords;\n      ratio \u003d totalRecords / (1.0 * inputRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(ctxt, \n                      split.getMapResourceUsageMetrics());\n      matcher.setDaemon(true);\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(ctxt, matcher);\n      reporter.setDaemon(true);\n      reporter.start();\n    }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {}
    },
    "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3008. Improvements to cumulative CPU emulation for short running tasks in Gridmix. (amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179933 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/10/11 9:59 PM",
      "commitName": "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 43.2,
      "commitsBetweenForRepo": 284,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,76 @@\n     protected void setup(Context ctxt) \n     throws IOException, InterruptedException {\n       final Configuration conf \u003d ctxt.getConfiguration();\n       final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n       final int maps \u003d split.getMapCount();\n       final long[] reduceBytes \u003d split.getOutputBytes();\n       final long[] reduceRecords \u003d split.getOutputRecords();\n \n       // enable gridmix map output record for compression\n       final boolean emulateMapOutputCompression \u003d \n         CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n         \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n       float compressionRatio \u003d 1.0f;\n       if (emulateMapOutputCompression) {\n         compressionRatio \u003d \n           CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n         LOG.info(\"GridMix is configured to use a compression ratio of \" \n                  + compressionRatio + \" for the map output data.\");\n         key.setCompressibility(true, compressionRatio);\n         val.setCompressibility(true, compressionRatio);\n       }\n       \n       long totalRecords \u003d 0L;\n       final int nReduces \u003d ctxt.getNumReduceTasks();\n       if (nReduces \u003e 0) {\n         int idx \u003d 0;\n         int id \u003d split.getId();\n         for (int i \u003d 0; i \u003c nReduces; ++i) {\n           final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n           if (i \u003d\u003d id) {\n             spec.bytes_out \u003d split.getReduceBytes(idx);\n             spec.rec_out \u003d split.getReduceRecords(idx);\n             spec.setResourceUsageSpecification(\n                    split.getReduceResourceUsageMetrics(idx));\n             ++idx;\n             id +\u003d maps;\n           }\n           \n           // set the map output bytes such that the final reduce input bytes \n           // match the expected value obtained from the original job\n           long mapOutputBytes \u003d reduceBytes[i];\n           if (emulateMapOutputCompression) {\n             mapOutputBytes /\u003d compressionRatio;\n           }\n           reduces.add(new IntermediateRecordFactory(\n               new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                    5*1024),\n               i, reduceRecords[i], spec, conf));\n           totalRecords +\u003d reduceRecords[i];\n         }\n       } else {\n         long mapOutputBytes \u003d reduceBytes[0];\n         if (emulateMapOutputCompression) {\n           mapOutputBytes /\u003d compressionRatio;\n         }\n         reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                          conf, 5*1024));\n         totalRecords \u003d reduceRecords[0];\n       }\n       final long splitRecords \u003d split.getInputRecords();\n       int missingRecSize \u003d \n         conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n       final long inputRecords \u003d \n         (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n         ? Math.max(1, split.getLength() / missingRecSize)\n         : splitRecords;\n       ratio \u003d totalRecords / (1.0 * inputRecords);\n       acc \u003d 0.0;\n       \n       matcher \u003d new ResourceUsageMatcherRunner(ctxt, \n                       split.getMapResourceUsageMetrics());\n       \n       // start the status reporter thread\n-      reporter \u003d new StatusReporter(ctxt);\n+      reporter \u003d new StatusReporter(ctxt, matcher);\n       reporter.start();\n     }\n\\ No newline at end of file\n",
      "actualSource": "    protected void setup(Context ctxt) \n    throws IOException, InterruptedException {\n      final Configuration conf \u003d ctxt.getConfiguration();\n      final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n      final int maps \u003d split.getMapCount();\n      final long[] reduceBytes \u003d split.getOutputBytes();\n      final long[] reduceRecords \u003d split.getOutputRecords();\n\n      // enable gridmix map output record for compression\n      final boolean emulateMapOutputCompression \u003d \n        CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n        \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n      float compressionRatio \u003d 1.0f;\n      if (emulateMapOutputCompression) {\n        compressionRatio \u003d \n          CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the map output data.\");\n        key.setCompressibility(true, compressionRatio);\n        val.setCompressibility(true, compressionRatio);\n      }\n      \n      long totalRecords \u003d 0L;\n      final int nReduces \u003d ctxt.getNumReduceTasks();\n      if (nReduces \u003e 0) {\n        int idx \u003d 0;\n        int id \u003d split.getId();\n        for (int i \u003d 0; i \u003c nReduces; ++i) {\n          final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n          if (i \u003d\u003d id) {\n            spec.bytes_out \u003d split.getReduceBytes(idx);\n            spec.rec_out \u003d split.getReduceRecords(idx);\n            spec.setResourceUsageSpecification(\n                   split.getReduceResourceUsageMetrics(idx));\n            ++idx;\n            id +\u003d maps;\n          }\n          \n          // set the map output bytes such that the final reduce input bytes \n          // match the expected value obtained from the original job\n          long mapOutputBytes \u003d reduceBytes[i];\n          if (emulateMapOutputCompression) {\n            mapOutputBytes /\u003d compressionRatio;\n          }\n          reduces.add(new IntermediateRecordFactory(\n              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                   5*1024),\n              i, reduceRecords[i], spec, conf));\n          totalRecords +\u003d reduceRecords[i];\n        }\n      } else {\n        long mapOutputBytes \u003d reduceBytes[0];\n        if (emulateMapOutputCompression) {\n          mapOutputBytes /\u003d compressionRatio;\n        }\n        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                         conf, 5*1024));\n        totalRecords \u003d reduceRecords[0];\n      }\n      final long splitRecords \u003d split.getInputRecords();\n      int missingRecSize \u003d \n        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n      final long inputRecords \u003d \n        (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n        ? Math.max(1, split.getLength() / missingRecSize)\n        : splitRecords;\n      ratio \u003d totalRecords / (1.0 * inputRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(ctxt, \n                      split.getMapResourceUsageMetrics());\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(ctxt, matcher);\n      reporter.start();\n    }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected void setup(Context ctxt) \n    throws IOException, InterruptedException {\n      final Configuration conf \u003d ctxt.getConfiguration();\n      final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n      final int maps \u003d split.getMapCount();\n      final long[] reduceBytes \u003d split.getOutputBytes();\n      final long[] reduceRecords \u003d split.getOutputRecords();\n\n      // enable gridmix map output record for compression\n      final boolean emulateMapOutputCompression \u003d \n        CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n        \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n      float compressionRatio \u003d 1.0f;\n      if (emulateMapOutputCompression) {\n        compressionRatio \u003d \n          CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the map output data.\");\n        key.setCompressibility(true, compressionRatio);\n        val.setCompressibility(true, compressionRatio);\n      }\n      \n      long totalRecords \u003d 0L;\n      final int nReduces \u003d ctxt.getNumReduceTasks();\n      if (nReduces \u003e 0) {\n        int idx \u003d 0;\n        int id \u003d split.getId();\n        for (int i \u003d 0; i \u003c nReduces; ++i) {\n          final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n          if (i \u003d\u003d id) {\n            spec.bytes_out \u003d split.getReduceBytes(idx);\n            spec.rec_out \u003d split.getReduceRecords(idx);\n            spec.setResourceUsageSpecification(\n                   split.getReduceResourceUsageMetrics(idx));\n            ++idx;\n            id +\u003d maps;\n          }\n          \n          // set the map output bytes such that the final reduce input bytes \n          // match the expected value obtained from the original job\n          long mapOutputBytes \u003d reduceBytes[i];\n          if (emulateMapOutputCompression) {\n            mapOutputBytes /\u003d compressionRatio;\n          }\n          reduces.add(new IntermediateRecordFactory(\n              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                   5*1024),\n              i, reduceRecords[i], spec, conf));\n          totalRecords +\u003d reduceRecords[i];\n        }\n      } else {\n        long mapOutputBytes \u003d reduceBytes[0];\n        if (emulateMapOutputCompression) {\n          mapOutputBytes /\u003d compressionRatio;\n        }\n        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                         conf, 5*1024));\n        totalRecords \u003d reduceRecords[0];\n      }\n      final long splitRecords \u003d split.getInputRecords();\n      int missingRecSize \u003d \n        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n      final long inputRecords \u003d \n        (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n        ? Math.max(1, split.getLength() / missingRecSize)\n        : splitRecords;\n      ratio \u003d totalRecords / (1.0 * inputRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(ctxt, \n                      split.getMapResourceUsageMetrics());\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(ctxt);\n      reporter.start();\n    }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
        "newPath": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected void setup(Context ctxt) \n    throws IOException, InterruptedException {\n      final Configuration conf \u003d ctxt.getConfiguration();\n      final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n      final int maps \u003d split.getMapCount();\n      final long[] reduceBytes \u003d split.getOutputBytes();\n      final long[] reduceRecords \u003d split.getOutputRecords();\n\n      // enable gridmix map output record for compression\n      final boolean emulateMapOutputCompression \u003d \n        CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n        \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n      float compressionRatio \u003d 1.0f;\n      if (emulateMapOutputCompression) {\n        compressionRatio \u003d \n          CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the map output data.\");\n        key.setCompressibility(true, compressionRatio);\n        val.setCompressibility(true, compressionRatio);\n      }\n      \n      long totalRecords \u003d 0L;\n      final int nReduces \u003d ctxt.getNumReduceTasks();\n      if (nReduces \u003e 0) {\n        int idx \u003d 0;\n        int id \u003d split.getId();\n        for (int i \u003d 0; i \u003c nReduces; ++i) {\n          final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n          if (i \u003d\u003d id) {\n            spec.bytes_out \u003d split.getReduceBytes(idx);\n            spec.rec_out \u003d split.getReduceRecords(idx);\n            spec.setResourceUsageSpecification(\n                   split.getReduceResourceUsageMetrics(idx));\n            ++idx;\n            id +\u003d maps;\n          }\n          \n          // set the map output bytes such that the final reduce input bytes \n          // match the expected value obtained from the original job\n          long mapOutputBytes \u003d reduceBytes[i];\n          if (emulateMapOutputCompression) {\n            mapOutputBytes /\u003d compressionRatio;\n          }\n          reduces.add(new IntermediateRecordFactory(\n              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                   5*1024),\n              i, reduceRecords[i], spec, conf));\n          totalRecords +\u003d reduceRecords[i];\n        }\n      } else {\n        long mapOutputBytes \u003d reduceBytes[0];\n        if (emulateMapOutputCompression) {\n          mapOutputBytes /\u003d compressionRatio;\n        }\n        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                         conf, 5*1024));\n        totalRecords \u003d reduceRecords[0];\n      }\n      final long splitRecords \u003d split.getInputRecords();\n      int missingRecSize \u003d \n        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n      final long inputRecords \u003d \n        (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n        ? Math.max(1, split.getLength() / missingRecSize)\n        : splitRecords;\n      ratio \u003d totalRecords / (1.0 * inputRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(ctxt, \n                      split.getMapResourceUsageMetrics());\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(ctxt);\n      reporter.start();\n    }",
      "path": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
        "newPath": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
      }
    },
    "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2106. [Gridmix] Cumulative CPU usage emulation in Gridmix. (amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1135396 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/06/11 12:44 AM",
      "commitName": "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 1.41,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,76 @@\n     protected void setup(Context ctxt) \n     throws IOException, InterruptedException {\n       final Configuration conf \u003d ctxt.getConfiguration();\n       final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n       final int maps \u003d split.getMapCount();\n       final long[] reduceBytes \u003d split.getOutputBytes();\n       final long[] reduceRecords \u003d split.getOutputRecords();\n \n       // enable gridmix map output record for compression\n       final boolean emulateMapOutputCompression \u003d \n         CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n         \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n       float compressionRatio \u003d 1.0f;\n       if (emulateMapOutputCompression) {\n         compressionRatio \u003d \n           CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n         LOG.info(\"GridMix is configured to use a compression ratio of \" \n                  + compressionRatio + \" for the map output data.\");\n         key.setCompressibility(true, compressionRatio);\n         val.setCompressibility(true, compressionRatio);\n       }\n       \n       long totalRecords \u003d 0L;\n       final int nReduces \u003d ctxt.getNumReduceTasks();\n       if (nReduces \u003e 0) {\n         int idx \u003d 0;\n         int id \u003d split.getId();\n         for (int i \u003d 0; i \u003c nReduces; ++i) {\n           final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n           if (i \u003d\u003d id) {\n             spec.bytes_out \u003d split.getReduceBytes(idx);\n             spec.rec_out \u003d split.getReduceRecords(idx);\n+            spec.setResourceUsageSpecification(\n+                   split.getReduceResourceUsageMetrics(idx));\n             ++idx;\n             id +\u003d maps;\n           }\n           \n           // set the map output bytes such that the final reduce input bytes \n           // match the expected value obtained from the original job\n           long mapOutputBytes \u003d reduceBytes[i];\n           if (emulateMapOutputCompression) {\n             mapOutputBytes /\u003d compressionRatio;\n           }\n           reduces.add(new IntermediateRecordFactory(\n               new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                    5*1024),\n               i, reduceRecords[i], spec, conf));\n           totalRecords +\u003d reduceRecords[i];\n         }\n       } else {\n         long mapOutputBytes \u003d reduceBytes[0];\n         if (emulateMapOutputCompression) {\n           mapOutputBytes /\u003d compressionRatio;\n         }\n         reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                          conf, 5*1024));\n         totalRecords \u003d reduceRecords[0];\n       }\n       final long splitRecords \u003d split.getInputRecords();\n       int missingRecSize \u003d \n         conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n       final long inputRecords \u003d \n         (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n         ? Math.max(1, split.getLength() / missingRecSize)\n         : splitRecords;\n       ratio \u003d totalRecords / (1.0 * inputRecords);\n       acc \u003d 0.0;\n+      \n+      matcher \u003d new ResourceUsageMatcherRunner(ctxt, \n+                      split.getMapResourceUsageMetrics());\n+      \n+      // start the status reporter thread\n+      reporter \u003d new StatusReporter(ctxt);\n+      reporter.start();\n     }\n\\ No newline at end of file\n",
      "actualSource": "    protected void setup(Context ctxt) \n    throws IOException, InterruptedException {\n      final Configuration conf \u003d ctxt.getConfiguration();\n      final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n      final int maps \u003d split.getMapCount();\n      final long[] reduceBytes \u003d split.getOutputBytes();\n      final long[] reduceRecords \u003d split.getOutputRecords();\n\n      // enable gridmix map output record for compression\n      final boolean emulateMapOutputCompression \u003d \n        CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n        \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n      float compressionRatio \u003d 1.0f;\n      if (emulateMapOutputCompression) {\n        compressionRatio \u003d \n          CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the map output data.\");\n        key.setCompressibility(true, compressionRatio);\n        val.setCompressibility(true, compressionRatio);\n      }\n      \n      long totalRecords \u003d 0L;\n      final int nReduces \u003d ctxt.getNumReduceTasks();\n      if (nReduces \u003e 0) {\n        int idx \u003d 0;\n        int id \u003d split.getId();\n        for (int i \u003d 0; i \u003c nReduces; ++i) {\n          final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n          if (i \u003d\u003d id) {\n            spec.bytes_out \u003d split.getReduceBytes(idx);\n            spec.rec_out \u003d split.getReduceRecords(idx);\n            spec.setResourceUsageSpecification(\n                   split.getReduceResourceUsageMetrics(idx));\n            ++idx;\n            id +\u003d maps;\n          }\n          \n          // set the map output bytes such that the final reduce input bytes \n          // match the expected value obtained from the original job\n          long mapOutputBytes \u003d reduceBytes[i];\n          if (emulateMapOutputCompression) {\n            mapOutputBytes /\u003d compressionRatio;\n          }\n          reduces.add(new IntermediateRecordFactory(\n              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                   5*1024),\n              i, reduceRecords[i], spec, conf));\n          totalRecords +\u003d reduceRecords[i];\n        }\n      } else {\n        long mapOutputBytes \u003d reduceBytes[0];\n        if (emulateMapOutputCompression) {\n          mapOutputBytes /\u003d compressionRatio;\n        }\n        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                         conf, 5*1024));\n        totalRecords \u003d reduceRecords[0];\n      }\n      final long splitRecords \u003d split.getInputRecords();\n      int missingRecSize \u003d \n        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n      final long inputRecords \u003d \n        (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n        ? Math.max(1, split.getLength() / missingRecSize)\n        : splitRecords;\n      ratio \u003d totalRecords / (1.0 * inputRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(ctxt, \n                      split.getMapResourceUsageMetrics());\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(ctxt);\n      reporter.start();\n    }",
      "path": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,67 @@\n+    protected void setup(Context ctxt) \n+    throws IOException, InterruptedException {\n+      final Configuration conf \u003d ctxt.getConfiguration();\n+      final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n+      final int maps \u003d split.getMapCount();\n+      final long[] reduceBytes \u003d split.getOutputBytes();\n+      final long[] reduceRecords \u003d split.getOutputRecords();\n+\n+      // enable gridmix map output record for compression\n+      final boolean emulateMapOutputCompression \u003d \n+        CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n+        \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n+      float compressionRatio \u003d 1.0f;\n+      if (emulateMapOutputCompression) {\n+        compressionRatio \u003d \n+          CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n+        LOG.info(\"GridMix is configured to use a compression ratio of \" \n+                 + compressionRatio + \" for the map output data.\");\n+        key.setCompressibility(true, compressionRatio);\n+        val.setCompressibility(true, compressionRatio);\n+      }\n+      \n+      long totalRecords \u003d 0L;\n+      final int nReduces \u003d ctxt.getNumReduceTasks();\n+      if (nReduces \u003e 0) {\n+        int idx \u003d 0;\n+        int id \u003d split.getId();\n+        for (int i \u003d 0; i \u003c nReduces; ++i) {\n+          final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n+          if (i \u003d\u003d id) {\n+            spec.bytes_out \u003d split.getReduceBytes(idx);\n+            spec.rec_out \u003d split.getReduceRecords(idx);\n+            ++idx;\n+            id +\u003d maps;\n+          }\n+          \n+          // set the map output bytes such that the final reduce input bytes \n+          // match the expected value obtained from the original job\n+          long mapOutputBytes \u003d reduceBytes[i];\n+          if (emulateMapOutputCompression) {\n+            mapOutputBytes /\u003d compressionRatio;\n+          }\n+          reduces.add(new IntermediateRecordFactory(\n+              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n+                                   5*1024),\n+              i, reduceRecords[i], spec, conf));\n+          totalRecords +\u003d reduceRecords[i];\n+        }\n+      } else {\n+        long mapOutputBytes \u003d reduceBytes[0];\n+        if (emulateMapOutputCompression) {\n+          mapOutputBytes /\u003d compressionRatio;\n+        }\n+        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n+                                         conf, 5*1024));\n+        totalRecords \u003d reduceRecords[0];\n+      }\n+      final long splitRecords \u003d split.getInputRecords();\n+      int missingRecSize \u003d \n+        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n+      final long inputRecords \u003d \n+        (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n+        ? Math.max(1, split.getLength() / missingRecSize)\n+        : splitRecords;\n+      ratio \u003d totalRecords / (1.0 * inputRecords);\n+      acc \u003d 0.0;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    protected void setup(Context ctxt) \n    throws IOException, InterruptedException {\n      final Configuration conf \u003d ctxt.getConfiguration();\n      final LoadSplit split \u003d (LoadSplit) ctxt.getInputSplit();\n      final int maps \u003d split.getMapCount();\n      final long[] reduceBytes \u003d split.getOutputBytes();\n      final long[] reduceRecords \u003d split.getOutputRecords();\n\n      // enable gridmix map output record for compression\n      final boolean emulateMapOutputCompression \u003d \n        CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n        \u0026\u0026 conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\n      float compressionRatio \u003d 1.0f;\n      if (emulateMapOutputCompression) {\n        compressionRatio \u003d \n          CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the map output data.\");\n        key.setCompressibility(true, compressionRatio);\n        val.setCompressibility(true, compressionRatio);\n      }\n      \n      long totalRecords \u003d 0L;\n      final int nReduces \u003d ctxt.getNumReduceTasks();\n      if (nReduces \u003e 0) {\n        int idx \u003d 0;\n        int id \u003d split.getId();\n        for (int i \u003d 0; i \u003c nReduces; ++i) {\n          final GridmixKey.Spec spec \u003d new GridmixKey.Spec();\n          if (i \u003d\u003d id) {\n            spec.bytes_out \u003d split.getReduceBytes(idx);\n            spec.rec_out \u003d split.getReduceRecords(idx);\n            ++idx;\n            id +\u003d maps;\n          }\n          \n          // set the map output bytes such that the final reduce input bytes \n          // match the expected value obtained from the original job\n          long mapOutputBytes \u003d reduceBytes[i];\n          if (emulateMapOutputCompression) {\n            mapOutputBytes /\u003d compressionRatio;\n          }\n          reduces.add(new IntermediateRecordFactory(\n              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, \n                                   5*1024),\n              i, reduceRecords[i], spec, conf));\n          totalRecords +\u003d reduceRecords[i];\n        }\n      } else {\n        long mapOutputBytes \u003d reduceBytes[0];\n        if (emulateMapOutputCompression) {\n          mapOutputBytes /\u003d compressionRatio;\n        }\n        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],\n                                         conf, 5*1024));\n        totalRecords \u003d reduceRecords[0];\n      }\n      final long splitRecords \u003d split.getInputRecords();\n      int missingRecSize \u003d \n        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);\n      final long inputRecords \u003d \n        (splitRecords \u003c\u003d 0 \u0026\u0026 split.getLength() \u003e\u003d 0)\n        ? Math.max(1, split.getLength() / missingRecSize)\n        : splitRecords;\n      ratio \u003d totalRecords / (1.0 * inputRecords);\n      acc \u003d 0.0;\n    }",
      "path": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
    }
  }
}