{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReaderRemote.java",
  "functionName": "readNextPacket",
  "functionId": "readNextPacket",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
  "functionStartLine": 185,
  "functionEndLine": 240,
  "numCommitsSeen": 45,
  "timeTaken": 3846,
  "changeHistory": [
    "8b281bce85474501868d68f8d5590a6086abb7b7",
    "f308561f1d885491b88db73ac63003202056d661",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "826ae1c26d31f87d88efc920b271bec7eec2e17a",
    "837e17b2eac1471d93e2eff395272063b265fee7",
    "239b2742d0e80d13c970fd062af4930e672fe903",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "40fe96546fcd68696076db67053f30d38a39a0d5"
  ],
  "changeHistoryShort": {
    "8b281bce85474501868d68f8d5590a6086abb7b7": "Ymovefromfile",
    "f308561f1d885491b88db73ac63003202056d661": "Yfilerename",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "826ae1c26d31f87d88efc920b271bec7eec2e17a": "Yfilerename",
    "837e17b2eac1471d93e2eff395272063b265fee7": "Ybodychange",
    "239b2742d0e80d13c970fd062af4930e672fe903": "Ybodychange",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ybodychange",
    "40fe96546fcd68696076db67053f30d38a39a0d5": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8b281bce85474501868d68f8d5590a6086abb7b7": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-10548. Remove the long deprecated BlockReaderRemote. Contributed by Kai Zheng\n",
      "commitDate": "02/07/16 8:56 PM",
      "commitName": "8b281bce85474501868d68f8d5590a6086abb7b7",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "01/07/16 3:53 PM",
      "commitNameOld": "0a5def155eff4564b5dc7685e7460952f51bbd24",
      "commitAuthorOld": "Ray Chiang",
      "daysBetweenCommits": 1.21,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void readNextPacket() throws IOException {\n    //Read packet headers.\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader curHeader \u003d packetReceiver.getHeader();\n    curDataSlice \u003d packetReceiver.getDataSlice();\n    assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n\n    LOG.trace(\"DFSClient readNextPacket got header {}\", curHeader);\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n      throw new IOException(\"BlockReader: error in packet header \" +\n          curHeader);\n    }\n\n    if (curHeader.getDataLen() \u003e 0) {\n      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen \u003d chunks * checksumSize;\n\n      assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n          \"checksum slice capacity\u003d\" +\n              packetReceiver.getChecksumSlice().capacity() +\n              \" checksumsLen\u003d\" + checksumsLen;\n\n      lastSeqNo \u003d curHeader.getSeqno();\n      if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n        // N.B.: the checksum error offset reported here is actually\n        // relative to the start of the block, not the start of the file.\n        // This is slightly misleading, but preserves the behavior from\n        // the older BlockReader.\n        checksum.verifyChunkedSums(curDataSlice,\n            packetReceiver.getChecksumSlice(),\n            filename, curHeader.getOffsetInBlock());\n      }\n      bytesNeededToFinish -\u003d curHeader.getDataLen();\n    }\n\n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we\u0027ve now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish \u003c\u003d 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote2.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
        "oldMethodName": "readNextPacket",
        "newMethodName": "readNextPacket"
      }
    },
    "f308561f1d885491b88db73ac63003202056d661": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
      "commitDate": "25/04/16 12:01 PM",
      "commitName": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "25/04/16 9:38 AM",
      "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void readNextPacket() throws IOException {\n    //Read packet headers.\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader curHeader \u003d packetReceiver.getHeader();\n    curDataSlice \u003d packetReceiver.getDataSlice();\n    assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n\n    LOG.trace(\"DFSClient readNextPacket got header {}\", curHeader);\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n      throw new IOException(\"BlockReader: error in packet header \" +\n          curHeader);\n    }\n\n    if (curHeader.getDataLen() \u003e 0) {\n      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen \u003d chunks * checksumSize;\n\n      assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n          \"checksum slice capacity\u003d\" +\n              packetReceiver.getChecksumSlice().capacity() +\n              \" checksumsLen\u003d\" + checksumsLen;\n\n      lastSeqNo \u003d curHeader.getSeqno();\n      if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n        // N.B.: the checksum error offset reported here is actually\n        // relative to the start of the block, not the start of the file.\n        // This is slightly misleading, but preserves the behavior from\n        // the older BlockReader.\n        checksum.verifyChunkedSums(curDataSlice,\n            packetReceiver.getChecksumSlice(),\n            filename, curHeader.getOffsetInBlock());\n      }\n      bytesNeededToFinish -\u003d curHeader.getDataLen();\n    }\n\n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we\u0027ve now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish \u003c\u003d 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote2.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote2.java"
      }
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,55 @@\n   private void readNextPacket() throws IOException {\n     //Read packet headers.\n     packetReceiver.receiveNextPacket(in);\n \n     PacketHeader curHeader \u003d packetReceiver.getHeader();\n     curDataSlice \u003d packetReceiver.getDataSlice();\n     assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n     \n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n-    }\n+    LOG.trace(\"DFSClient readNextPacket got header {}\", curHeader);\n \n     // Sanity check the lengths\n     if (!curHeader.sanityCheck(lastSeqNo)) {\n          throw new IOException(\"BlockReader: error in packet header \" +\n                                curHeader);\n     }\n     \n     if (curHeader.getDataLen() \u003e 0) {\n       int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n       int checksumsLen \u003d chunks * checksumSize;\n \n       assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n         \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n           \" checksumsLen\u003d\" + checksumsLen;\n       \n       lastSeqNo \u003d curHeader.getSeqno();\n       if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n         // N.B.: the checksum error offset reported here is actually\n         // relative to the start of the block, not the start of the file.\n         // This is slightly misleading, but preserves the behavior from\n         // the older BlockReader.\n         checksum.verifyChunkedSums(curDataSlice,\n             packetReceiver.getChecksumSlice(),\n             filename, curHeader.getOffsetInBlock());\n       }\n       bytesNeededToFinish -\u003d curHeader.getDataLen();\n     }    \n     \n     // First packet will include some data prior to the first byte\n     // the user requested. Skip it.\n     if (curHeader.getOffsetInBlock() \u003c startOffset) {\n       int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n       curDataSlice.position(newPos);\n     }\n \n     // If we\u0027ve now satisfied the whole client read, read one last packet\n     // header, which should be empty\n     if (bytesNeededToFinish \u003c\u003d 0) {\n       readTrailingEmptyPacket();\n       if (verifyChecksum) {\n         sendReadResult(Status.CHECKSUM_OK);\n       } else {\n         sendReadResult(Status.SUCCESS);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void readNextPacket() throws IOException {\n    //Read packet headers.\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader curHeader \u003d packetReceiver.getHeader();\n    curDataSlice \u003d packetReceiver.getDataSlice();\n    assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n    \n    LOG.trace(\"DFSClient readNextPacket got header {}\", curHeader);\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n         throw new IOException(\"BlockReader: error in packet header \" +\n                               curHeader);\n    }\n    \n    if (curHeader.getDataLen() \u003e 0) {\n      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen \u003d chunks * checksumSize;\n\n      assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n        \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n          \" checksumsLen\u003d\" + checksumsLen;\n      \n      lastSeqNo \u003d curHeader.getSeqno();\n      if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n        // N.B.: the checksum error offset reported here is actually\n        // relative to the start of the block, not the start of the file.\n        // This is slightly misleading, but preserves the behavior from\n        // the older BlockReader.\n        checksum.verifyChunkedSums(curDataSlice,\n            packetReceiver.getChecksumSlice(),\n            filename, curHeader.getOffsetInBlock());\n      }\n      bytesNeededToFinish -\u003d curHeader.getDataLen();\n    }    \n    \n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we\u0027ve now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish \u003c\u003d 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,57 @@\n   private void readNextPacket() throws IOException {\n     //Read packet headers.\n     packetReceiver.receiveNextPacket(in);\n \n     PacketHeader curHeader \u003d packetReceiver.getHeader();\n     curDataSlice \u003d packetReceiver.getDataSlice();\n     assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n     \n-    LOG.trace(\"DFSClient readNextPacket got header {}\", curHeader);\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n+    }\n \n     // Sanity check the lengths\n     if (!curHeader.sanityCheck(lastSeqNo)) {\n          throw new IOException(\"BlockReader: error in packet header \" +\n                                curHeader);\n     }\n     \n     if (curHeader.getDataLen() \u003e 0) {\n       int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n       int checksumsLen \u003d chunks * checksumSize;\n \n       assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n         \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n           \" checksumsLen\u003d\" + checksumsLen;\n       \n       lastSeqNo \u003d curHeader.getSeqno();\n       if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n         // N.B.: the checksum error offset reported here is actually\n         // relative to the start of the block, not the start of the file.\n         // This is slightly misleading, but preserves the behavior from\n         // the older BlockReader.\n         checksum.verifyChunkedSums(curDataSlice,\n             packetReceiver.getChecksumSlice(),\n             filename, curHeader.getOffsetInBlock());\n       }\n       bytesNeededToFinish -\u003d curHeader.getDataLen();\n     }    \n     \n     // First packet will include some data prior to the first byte\n     // the user requested. Skip it.\n     if (curHeader.getOffsetInBlock() \u003c startOffset) {\n       int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n       curDataSlice.position(newPos);\n     }\n \n     // If we\u0027ve now satisfied the whole client read, read one last packet\n     // header, which should be empty\n     if (bytesNeededToFinish \u003c\u003d 0) {\n       readTrailingEmptyPacket();\n       if (verifyChecksum) {\n         sendReadResult(Status.CHECKSUM_OK);\n       } else {\n         sendReadResult(Status.SUCCESS);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void readNextPacket() throws IOException {\n    //Read packet headers.\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader curHeader \u003d packetReceiver.getHeader();\n    curDataSlice \u003d packetReceiver.getDataSlice();\n    assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n    }\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n         throw new IOException(\"BlockReader: error in packet header \" +\n                               curHeader);\n    }\n    \n    if (curHeader.getDataLen() \u003e 0) {\n      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen \u003d chunks * checksumSize;\n\n      assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n        \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n          \" checksumsLen\u003d\" + checksumsLen;\n      \n      lastSeqNo \u003d curHeader.getSeqno();\n      if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n        // N.B.: the checksum error offset reported here is actually\n        // relative to the start of the block, not the start of the file.\n        // This is slightly misleading, but preserves the behavior from\n        // the older BlockReader.\n        checksum.verifyChunkedSums(curDataSlice,\n            packetReceiver.getChecksumSlice(),\n            filename, curHeader.getOffsetInBlock());\n      }\n      bytesNeededToFinish -\u003d curHeader.getDataLen();\n    }    \n    \n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we\u0027ve now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish \u003c\u003d 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,55 @@\n   private void readNextPacket() throws IOException {\n     //Read packet headers.\n     packetReceiver.receiveNextPacket(in);\n \n     PacketHeader curHeader \u003d packetReceiver.getHeader();\n     curDataSlice \u003d packetReceiver.getDataSlice();\n     assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n     \n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n-    }\n+    LOG.trace(\"DFSClient readNextPacket got header {}\", curHeader);\n \n     // Sanity check the lengths\n     if (!curHeader.sanityCheck(lastSeqNo)) {\n          throw new IOException(\"BlockReader: error in packet header \" +\n                                curHeader);\n     }\n     \n     if (curHeader.getDataLen() \u003e 0) {\n       int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n       int checksumsLen \u003d chunks * checksumSize;\n \n       assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n         \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n           \" checksumsLen\u003d\" + checksumsLen;\n       \n       lastSeqNo \u003d curHeader.getSeqno();\n       if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n         // N.B.: the checksum error offset reported here is actually\n         // relative to the start of the block, not the start of the file.\n         // This is slightly misleading, but preserves the behavior from\n         // the older BlockReader.\n         checksum.verifyChunkedSums(curDataSlice,\n             packetReceiver.getChecksumSlice(),\n             filename, curHeader.getOffsetInBlock());\n       }\n       bytesNeededToFinish -\u003d curHeader.getDataLen();\n     }    \n     \n     // First packet will include some data prior to the first byte\n     // the user requested. Skip it.\n     if (curHeader.getOffsetInBlock() \u003c startOffset) {\n       int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n       curDataSlice.position(newPos);\n     }\n \n     // If we\u0027ve now satisfied the whole client read, read one last packet\n     // header, which should be empty\n     if (bytesNeededToFinish \u003c\u003d 0) {\n       readTrailingEmptyPacket();\n       if (verifyChecksum) {\n         sendReadResult(Status.CHECKSUM_OK);\n       } else {\n         sendReadResult(Status.SUCCESS);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void readNextPacket() throws IOException {\n    //Read packet headers.\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader curHeader \u003d packetReceiver.getHeader();\n    curDataSlice \u003d packetReceiver.getDataSlice();\n    assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n    \n    LOG.trace(\"DFSClient readNextPacket got header {}\", curHeader);\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n         throw new IOException(\"BlockReader: error in packet header \" +\n                               curHeader);\n    }\n    \n    if (curHeader.getDataLen() \u003e 0) {\n      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen \u003d chunks * checksumSize;\n\n      assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n        \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n          \" checksumsLen\u003d\" + checksumsLen;\n      \n      lastSeqNo \u003d curHeader.getSeqno();\n      if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n        // N.B.: the checksum error offset reported here is actually\n        // relative to the start of the block, not the start of the file.\n        // This is slightly misleading, but preserves the behavior from\n        // the older BlockReader.\n        checksum.verifyChunkedSums(curDataSlice,\n            packetReceiver.getChecksumSlice(),\n            filename, curHeader.getOffsetInBlock());\n      }\n      bytesNeededToFinish -\u003d curHeader.getDataLen();\n    }    \n    \n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we\u0027ve now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish \u003c\u003d 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java",
      "extendedDetails": {}
    },
    "826ae1c26d31f87d88efc920b271bec7eec2e17a": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8990. Move RemoteBlockReader to hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "31/08/15 1:54 PM",
      "commitName": "826ae1c26d31f87d88efc920b271bec7eec2e17a",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "31/08/15 11:48 AM",
      "commitNameOld": "caa04de149030691b7bc952b534c6128db217ed2",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.09,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void readNextPacket() throws IOException {\n    //Read packet headers.\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader curHeader \u003d packetReceiver.getHeader();\n    curDataSlice \u003d packetReceiver.getDataSlice();\n    assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n    }\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n         throw new IOException(\"BlockReader: error in packet header \" +\n                               curHeader);\n    }\n    \n    if (curHeader.getDataLen() \u003e 0) {\n      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen \u003d chunks * checksumSize;\n\n      assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n        \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n          \" checksumsLen\u003d\" + checksumsLen;\n      \n      lastSeqNo \u003d curHeader.getSeqno();\n      if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n        // N.B.: the checksum error offset reported here is actually\n        // relative to the start of the block, not the start of the file.\n        // This is slightly misleading, but preserves the behavior from\n        // the older BlockReader.\n        checksum.verifyChunkedSums(curDataSlice,\n            packetReceiver.getChecksumSlice(),\n            filename, curHeader.getOffsetInBlock());\n      }\n      bytesNeededToFinish -\u003d curHeader.getDataLen();\n    }    \n    \n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we\u0027ve now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish \u003c\u003d 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java"
      }
    },
    "837e17b2eac1471d93e2eff395272063b265fee7": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 6:39 PM",
      "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/01/13 12:44 PM",
      "commitNameOld": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.25,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   private void readNextPacket() throws IOException {\n     //Read packet headers.\n-    packetReceiver.receiveNextPacket(peer.getInputStreamChannel());\n+    packetReceiver.receiveNextPacket(in);\n \n     PacketHeader curHeader \u003d packetReceiver.getHeader();\n     curDataSlice \u003d packetReceiver.getDataSlice();\n     assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n     \n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n     }\n \n     // Sanity check the lengths\n     if (!curHeader.sanityCheck(lastSeqNo)) {\n          throw new IOException(\"BlockReader: error in packet header \" +\n                                curHeader);\n     }\n     \n     if (curHeader.getDataLen() \u003e 0) {\n       int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n       int checksumsLen \u003d chunks * checksumSize;\n \n       assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n         \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n           \" checksumsLen\u003d\" + checksumsLen;\n       \n       lastSeqNo \u003d curHeader.getSeqno();\n       if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n         // N.B.: the checksum error offset reported here is actually\n         // relative to the start of the block, not the start of the file.\n         // This is slightly misleading, but preserves the behavior from\n         // the older BlockReader.\n         checksum.verifyChunkedSums(curDataSlice,\n             packetReceiver.getChecksumSlice(),\n             filename, curHeader.getOffsetInBlock());\n       }\n       bytesNeededToFinish -\u003d curHeader.getDataLen();\n     }    \n     \n     // First packet will include some data prior to the first byte\n     // the user requested. Skip it.\n     if (curHeader.getOffsetInBlock() \u003c startOffset) {\n       int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n       curDataSlice.position(newPos);\n     }\n \n     // If we\u0027ve now satisfied the whole client read, read one last packet\n     // header, which should be empty\n     if (bytesNeededToFinish \u003c\u003d 0) {\n       readTrailingEmptyPacket();\n       if (verifyChecksum) {\n         sendReadResult(Status.CHECKSUM_OK);\n       } else {\n         sendReadResult(Status.SUCCESS);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void readNextPacket() throws IOException {\n    //Read packet headers.\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader curHeader \u003d packetReceiver.getHeader();\n    curDataSlice \u003d packetReceiver.getDataSlice();\n    assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n    }\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n         throw new IOException(\"BlockReader: error in packet header \" +\n                               curHeader);\n    }\n    \n    if (curHeader.getDataLen() \u003e 0) {\n      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen \u003d chunks * checksumSize;\n\n      assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n        \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n          \" checksumsLen\u003d\" + checksumsLen;\n      \n      lastSeqNo \u003d curHeader.getSeqno();\n      if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n        // N.B.: the checksum error offset reported here is actually\n        // relative to the start of the block, not the start of the file.\n        // This is slightly misleading, but preserves the behavior from\n        // the older BlockReader.\n        checksum.verifyChunkedSums(curDataSlice,\n            packetReceiver.getChecksumSlice(),\n            filename, curHeader.getOffsetInBlock());\n      }\n      bytesNeededToFinish -\u003d curHeader.getDataLen();\n    }    \n    \n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we\u0027ve now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish \u003c\u003d 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java",
      "extendedDetails": {}
    },
    "239b2742d0e80d13c970fd062af4930e672fe903": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 12:44 PM",
      "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "03/01/13 10:59 PM",
      "commitNameOld": "32052a1e3a8007b5348dc42415861aeb859ebc5a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.57,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   private void readNextPacket() throws IOException {\n     //Read packet headers.\n-    packetReceiver.receiveNextPacket(in);\n+    packetReceiver.receiveNextPacket(peer.getInputStreamChannel());\n \n     PacketHeader curHeader \u003d packetReceiver.getHeader();\n     curDataSlice \u003d packetReceiver.getDataSlice();\n     assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n     \n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n     }\n \n     // Sanity check the lengths\n     if (!curHeader.sanityCheck(lastSeqNo)) {\n          throw new IOException(\"BlockReader: error in packet header \" +\n                                curHeader);\n     }\n     \n     if (curHeader.getDataLen() \u003e 0) {\n       int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n       int checksumsLen \u003d chunks * checksumSize;\n \n       assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n         \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n           \" checksumsLen\u003d\" + checksumsLen;\n       \n       lastSeqNo \u003d curHeader.getSeqno();\n       if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n         // N.B.: the checksum error offset reported here is actually\n         // relative to the start of the block, not the start of the file.\n         // This is slightly misleading, but preserves the behavior from\n         // the older BlockReader.\n         checksum.verifyChunkedSums(curDataSlice,\n             packetReceiver.getChecksumSlice(),\n             filename, curHeader.getOffsetInBlock());\n       }\n       bytesNeededToFinish -\u003d curHeader.getDataLen();\n     }    \n     \n     // First packet will include some data prior to the first byte\n     // the user requested. Skip it.\n     if (curHeader.getOffsetInBlock() \u003c startOffset) {\n       int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n       curDataSlice.position(newPos);\n     }\n \n     // If we\u0027ve now satisfied the whole client read, read one last packet\n     // header, which should be empty\n     if (bytesNeededToFinish \u003c\u003d 0) {\n       readTrailingEmptyPacket();\n       if (verifyChecksum) {\n         sendReadResult(Status.CHECKSUM_OK);\n       } else {\n         sendReadResult(Status.SUCCESS);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void readNextPacket() throws IOException {\n    //Read packet headers.\n    packetReceiver.receiveNextPacket(peer.getInputStreamChannel());\n\n    PacketHeader curHeader \u003d packetReceiver.getHeader();\n    curDataSlice \u003d packetReceiver.getDataSlice();\n    assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n    }\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n         throw new IOException(\"BlockReader: error in packet header \" +\n                               curHeader);\n    }\n    \n    if (curHeader.getDataLen() \u003e 0) {\n      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen \u003d chunks * checksumSize;\n\n      assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n        \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n          \" checksumsLen\u003d\" + checksumsLen;\n      \n      lastSeqNo \u003d curHeader.getSeqno();\n      if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n        // N.B.: the checksum error offset reported here is actually\n        // relative to the start of the block, not the start of the file.\n        // This is slightly misleading, but preserves the behavior from\n        // the older BlockReader.\n        checksum.verifyChunkedSums(curDataSlice,\n            packetReceiver.getChecksumSlice(),\n            filename, curHeader.getOffsetInBlock());\n      }\n      bytesNeededToFinish -\u003d curHeader.getDataLen();\n    }    \n    \n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we\u0027ve now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish \u003c\u003d 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java",
      "extendedDetails": {}
    },
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3721. hsync support broke wire compatibility. Contributed by Todd Lipcon and Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1371495 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/12 2:31 PM",
      "commitName": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "07/08/12 9:40 AM",
      "commitNameOld": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 2.2,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,57 @@\n   private void readNextPacket() throws IOException {\n-    Preconditions.checkState(curHeader \u003d\u003d null || !curHeader.isLastPacketInBlock());\n-    \n     //Read packet headers.\n-    readPacketHeader();\n+    packetReceiver.receiveNextPacket(in);\n \n+    PacketHeader curHeader \u003d packetReceiver.getHeader();\n+    curDataSlice \u003d packetReceiver.getDataSlice();\n+    assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n+    \n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n     }\n \n     // Sanity check the lengths\n     if (!curHeader.sanityCheck(lastSeqNo)) {\n          throw new IOException(\"BlockReader: error in packet header \" +\n                                curHeader);\n     }\n     \n     if (curHeader.getDataLen() \u003e 0) {\n       int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n       int checksumsLen \u003d chunks * checksumSize;\n-      int bufsize \u003d checksumsLen + curHeader.getDataLen();\n+\n+      assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n+        \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n+          \" checksumsLen\u003d\" + checksumsLen;\n       \n-      resetPacketBuffer(checksumsLen, curHeader.getDataLen());\n-  \n       lastSeqNo \u003d curHeader.getSeqno();\n-      if (bufsize \u003e 0) {\n-        readChannelFully(in, curPacketBuf);\n-        curPacketBuf.flip();\n-        if (verifyChecksum) {\n-          verifyPacketChecksums();\n-        }\n+      if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n+        // N.B.: the checksum error offset reported here is actually\n+        // relative to the start of the block, not the start of the file.\n+        // This is slightly misleading, but preserves the behavior from\n+        // the older BlockReader.\n+        checksum.verifyChunkedSums(curDataSlice,\n+            packetReceiver.getChecksumSlice(),\n+            filename, curHeader.getOffsetInBlock());\n       }\n       bytesNeededToFinish -\u003d curHeader.getDataLen();\n     }    \n     \n     // First packet will include some data prior to the first byte\n     // the user requested. Skip it.\n     if (curHeader.getOffsetInBlock() \u003c startOffset) {\n       int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n       curDataSlice.position(newPos);\n     }\n \n     // If we\u0027ve now satisfied the whole client read, read one last packet\n     // header, which should be empty\n     if (bytesNeededToFinish \u003c\u003d 0) {\n       readTrailingEmptyPacket();\n       if (verifyChecksum) {\n         sendReadResult(Status.CHECKSUM_OK);\n       } else {\n         sendReadResult(Status.SUCCESS);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void readNextPacket() throws IOException {\n    //Read packet headers.\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader curHeader \u003d packetReceiver.getHeader();\n    curDataSlice \u003d packetReceiver.getDataSlice();\n    assert curDataSlice.capacity() \u003d\u003d curHeader.getDataLen();\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n    }\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n         throw new IOException(\"BlockReader: error in packet header \" +\n                               curHeader);\n    }\n    \n    if (curHeader.getDataLen() \u003e 0) {\n      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen \u003d chunks * checksumSize;\n\n      assert packetReceiver.getChecksumSlice().capacity() \u003d\u003d checksumsLen :\n        \"checksum slice capacity\u003d\" + packetReceiver.getChecksumSlice().capacity() + \n          \" checksumsLen\u003d\" + checksumsLen;\n      \n      lastSeqNo \u003d curHeader.getSeqno();\n      if (verifyChecksum \u0026\u0026 curDataSlice.remaining() \u003e 0) {\n        // N.B.: the checksum error offset reported here is actually\n        // relative to the start of the block, not the start of the file.\n        // This is slightly misleading, but preserves the behavior from\n        // the older BlockReader.\n        checksum.verifyChunkedSums(curDataSlice,\n            packetReceiver.getChecksumSlice(),\n            filename, curHeader.getOffsetInBlock());\n      }\n      bytesNeededToFinish -\u003d curHeader.getDataLen();\n    }    \n    \n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we\u0027ve now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish \u003c\u003d 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "03/05/12 2:57 PM",
      "commitNameOld": "03181022ab238b2d4f59932eb8eadbe7cb52a669",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 95.78,
      "commitsBetweenForRepo": 490,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,52 @@\n   private void readNextPacket() throws IOException {\n     Preconditions.checkState(curHeader \u003d\u003d null || !curHeader.isLastPacketInBlock());\n     \n     //Read packet headers.\n     readPacketHeader();\n \n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n     }\n \n     // Sanity check the lengths\n     if (!curHeader.sanityCheck(lastSeqNo)) {\n          throw new IOException(\"BlockReader: error in packet header \" +\n                                curHeader);\n     }\n     \n     if (curHeader.getDataLen() \u003e 0) {\n       int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n       int checksumsLen \u003d chunks * checksumSize;\n       int bufsize \u003d checksumsLen + curHeader.getDataLen();\n       \n       resetPacketBuffer(checksumsLen, curHeader.getDataLen());\n   \n       lastSeqNo \u003d curHeader.getSeqno();\n       if (bufsize \u003e 0) {\n         readChannelFully(in, curPacketBuf);\n         curPacketBuf.flip();\n         if (verifyChecksum) {\n           verifyPacketChecksums();\n         }\n       }\n       bytesNeededToFinish -\u003d curHeader.getDataLen();\n     }    \n     \n     // First packet will include some data prior to the first byte\n     // the user requested. Skip it.\n     if (curHeader.getOffsetInBlock() \u003c startOffset) {\n       int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n       curDataSlice.position(newPos);\n     }\n \n     // If we\u0027ve now satisfied the whole client read, read one last packet\n     // header, which should be empty\n     if (bytesNeededToFinish \u003c\u003d 0) {\n       readTrailingEmptyPacket();\n       if (verifyChecksum) {\n-        sendReadResult(dnSock, Status.CHECKSUM_OK);\n+        sendReadResult(Status.CHECKSUM_OK);\n       } else {\n-        sendReadResult(dnSock, Status.SUCCESS);\n+        sendReadResult(Status.SUCCESS);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void readNextPacket() throws IOException {\n    Preconditions.checkState(curHeader \u003d\u003d null || !curHeader.isLastPacketInBlock());\n    \n    //Read packet headers.\n    readPacketHeader();\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n    }\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n         throw new IOException(\"BlockReader: error in packet header \" +\n                               curHeader);\n    }\n    \n    if (curHeader.getDataLen() \u003e 0) {\n      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen \u003d chunks * checksumSize;\n      int bufsize \u003d checksumsLen + curHeader.getDataLen();\n      \n      resetPacketBuffer(checksumsLen, curHeader.getDataLen());\n  \n      lastSeqNo \u003d curHeader.getSeqno();\n      if (bufsize \u003e 0) {\n        readChannelFully(in, curPacketBuf);\n        curPacketBuf.flip();\n        if (verifyChecksum) {\n          verifyPacketChecksums();\n        }\n      }\n      bytesNeededToFinish -\u003d curHeader.getDataLen();\n    }    \n    \n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we\u0027ve now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish \u003c\u003d 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(Status.SUCCESS);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java",
      "extendedDetails": {}
    },
    "40fe96546fcd68696076db67053f30d38a39a0d5": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2129. Simplify BlockReader to not inherit from FSInputChecker. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196976 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/11/11 11:54 PM",
      "commitName": "40fe96546fcd68696076db67053f30d38a39a0d5",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,52 @@\n+  private void readNextPacket() throws IOException {\n+    Preconditions.checkState(curHeader \u003d\u003d null || !curHeader.isLastPacketInBlock());\n+    \n+    //Read packet headers.\n+    readPacketHeader();\n+\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n+    }\n+\n+    // Sanity check the lengths\n+    if (!curHeader.sanityCheck(lastSeqNo)) {\n+         throw new IOException(\"BlockReader: error in packet header \" +\n+                               curHeader);\n+    }\n+    \n+    if (curHeader.getDataLen() \u003e 0) {\n+      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n+      int checksumsLen \u003d chunks * checksumSize;\n+      int bufsize \u003d checksumsLen + curHeader.getDataLen();\n+      \n+      resetPacketBuffer(checksumsLen, curHeader.getDataLen());\n+  \n+      lastSeqNo \u003d curHeader.getSeqno();\n+      if (bufsize \u003e 0) {\n+        readChannelFully(in, curPacketBuf);\n+        curPacketBuf.flip();\n+        if (verifyChecksum) {\n+          verifyPacketChecksums();\n+        }\n+      }\n+      bytesNeededToFinish -\u003d curHeader.getDataLen();\n+    }    \n+    \n+    // First packet will include some data prior to the first byte\n+    // the user requested. Skip it.\n+    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n+      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n+      curDataSlice.position(newPos);\n+    }\n+\n+    // If we\u0027ve now satisfied the whole client read, read one last packet\n+    // header, which should be empty\n+    if (bytesNeededToFinish \u003c\u003d 0) {\n+      readTrailingEmptyPacket();\n+      if (verifyChecksum) {\n+        sendReadResult(dnSock, Status.CHECKSUM_OK);\n+      } else {\n+        sendReadResult(dnSock, Status.SUCCESS);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void readNextPacket() throws IOException {\n    Preconditions.checkState(curHeader \u003d\u003d null || !curHeader.isLastPacketInBlock());\n    \n    //Read packet headers.\n    readPacketHeader();\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DFSClient readNextPacket got header \" + curHeader);\n    }\n\n    // Sanity check the lengths\n    if (!curHeader.sanityCheck(lastSeqNo)) {\n         throw new IOException(\"BlockReader: error in packet header \" +\n                               curHeader);\n    }\n    \n    if (curHeader.getDataLen() \u003e 0) {\n      int chunks \u003d 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\n      int checksumsLen \u003d chunks * checksumSize;\n      int bufsize \u003d checksumsLen + curHeader.getDataLen();\n      \n      resetPacketBuffer(checksumsLen, curHeader.getDataLen());\n  \n      lastSeqNo \u003d curHeader.getSeqno();\n      if (bufsize \u003e 0) {\n        readChannelFully(in, curPacketBuf);\n        curPacketBuf.flip();\n        if (verifyChecksum) {\n          verifyPacketChecksums();\n        }\n      }\n      bytesNeededToFinish -\u003d curHeader.getDataLen();\n    }    \n    \n    // First packet will include some data prior to the first byte\n    // the user requested. Skip it.\n    if (curHeader.getOffsetInBlock() \u003c startOffset) {\n      int newPos \u003d (int) (startOffset - curHeader.getOffsetInBlock());\n      curDataSlice.position(newPos);\n    }\n\n    // If we\u0027ve now satisfied the whole client read, read one last packet\n    // header, which should be empty\n    if (bytesNeededToFinish \u003c\u003d 0) {\n      readTrailingEmptyPacket();\n      if (verifyChecksum) {\n        sendReadResult(dnSock, Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(dnSock, Status.SUCCESS);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java"
    }
  }
}