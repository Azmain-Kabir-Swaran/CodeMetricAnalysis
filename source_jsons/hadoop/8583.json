{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSImageFormatPBINode.java",
  "functionName": "loadINodeDirectorySectionInParallel",
  "functionId": "loadINodeDirectorySectionInParallel___service-ExecutorService__sections-ArrayList__FileSummary.Section____compressionCodec-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
  "functionStartLine": 218,
  "functionEndLine": 261,
  "numCommitsSeen": 63,
  "timeTaken": 1232,
  "changeHistory": [
    "b67812ea2111fa11bdd76096b923c93e1bdf2923"
  ],
  "changeHistoryShort": {
    "b67812ea2111fa11bdd76096b923c93e1bdf2923": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b67812ea2111fa11bdd76096b923c93e1bdf2923": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-14617. Improve fsimage load time by writing sub-sections to the fsimage index (#1028). Contributed by  Stephen O\u0027Donnell.\n\nReviewed-by: He Xiaoqiao \u003chexiaoqiao@apache.org\u003e",
      "commitDate": "22/08/19 5:09 PM",
      "commitName": "b67812ea2111fa11bdd76096b923c93e1bdf2923",
      "commitAuthor": "Stephen O\u0027Donnell",
      "diff": "@@ -0,0 +1,44 @@\n+    void loadINodeDirectorySectionInParallel(ExecutorService service,\n+        ArrayList\u003cFileSummary.Section\u003e sections, String compressionCodec)\n+        throws IOException {\n+      LOG.info(\"Loading the INodeDirectory section in parallel with {} sub-\" +\n+              \"sections\", sections.size());\n+      CountDownLatch latch \u003d new CountDownLatch(sections.size());\n+      final CopyOnWriteArrayList\u003cIOException\u003e exceptions \u003d\n+          new CopyOnWriteArrayList\u003c\u003e();\n+      for (FileSummary.Section s : sections) {\n+        service.submit(() -\u003e {\n+          InputStream ins \u003d null;\n+          try {\n+            ins \u003d parent.getInputStreamForSection(s,\n+                compressionCodec);\n+            loadINodeDirectorySection(ins);\n+          } catch (Exception e) {\n+            LOG.error(\"An exception occurred loading INodeDirectories in \" +\n+                \"parallel\", e);\n+            exceptions.add(new IOException(e));\n+          } finally {\n+            latch.countDown();\n+            try {\n+              if (ins !\u003d null) {\n+                ins.close();\n+              }\n+            } catch (IOException ioe) {\n+              LOG.warn(\"Failed to close the input stream, ignoring\", ioe);\n+            }\n+          }\n+        });\n+      }\n+      try {\n+        latch.await();\n+      } catch (InterruptedException e) {\n+        LOG.error(\"Interrupted waiting for countdown latch\", e);\n+        throw new IOException(e);\n+      }\n+      if (exceptions.size() !\u003d 0) {\n+        LOG.error(\"{} exceptions occurred loading INodeDirectories\",\n+            exceptions.size());\n+        throw exceptions.get(0);\n+      }\n+      LOG.info(\"Completed loading all INodeDirectory sub-sections\");\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    void loadINodeDirectorySectionInParallel(ExecutorService service,\n        ArrayList\u003cFileSummary.Section\u003e sections, String compressionCodec)\n        throws IOException {\n      LOG.info(\"Loading the INodeDirectory section in parallel with {} sub-\" +\n              \"sections\", sections.size());\n      CountDownLatch latch \u003d new CountDownLatch(sections.size());\n      final CopyOnWriteArrayList\u003cIOException\u003e exceptions \u003d\n          new CopyOnWriteArrayList\u003c\u003e();\n      for (FileSummary.Section s : sections) {\n        service.submit(() -\u003e {\n          InputStream ins \u003d null;\n          try {\n            ins \u003d parent.getInputStreamForSection(s,\n                compressionCodec);\n            loadINodeDirectorySection(ins);\n          } catch (Exception e) {\n            LOG.error(\"An exception occurred loading INodeDirectories in \" +\n                \"parallel\", e);\n            exceptions.add(new IOException(e));\n          } finally {\n            latch.countDown();\n            try {\n              if (ins !\u003d null) {\n                ins.close();\n              }\n            } catch (IOException ioe) {\n              LOG.warn(\"Failed to close the input stream, ignoring\", ioe);\n            }\n          }\n        });\n      }\n      try {\n        latch.await();\n      } catch (InterruptedException e) {\n        LOG.error(\"Interrupted waiting for countdown latch\", e);\n        throw new IOException(e);\n      }\n      if (exceptions.size() !\u003d 0) {\n        LOG.error(\"{} exceptions occurred loading INodeDirectories\",\n            exceptions.size());\n        throw exceptions.get(0);\n      }\n      LOG.info(\"Completed loading all INodeDirectory sub-sections\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java"
    }
  }
}