{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Journal.java",
  "functionName": "journal",
  "functionId": "journal___reqInfo-RequestInfo__segmentTxId-long__firstTxnId-long__numTxns-int__records-byte[]",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
  "functionStartLine": 393,
  "functionEndLine": 473,
  "numCommitsSeen": 71,
  "timeTaken": 4991,
  "changeHistory": [
    "151c8ddbe4c05fcb5f251fa4450edc452f6c735a",
    "6beb25ab7e4f5454dba0315a296081e61753f301",
    "098ec2b11ff3f677eb823f75b147a1ac8dbf959e",
    "94cf7ab9d28a885181afeb2c181dfe857d158254",
    "53c38cc89ab979ec47557dcfa7affbad20578c0a",
    "a6ed4894b518351bf1b3290e725a475570a21296",
    "988639640024933448f822b48119dc832ebd2af3",
    "2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b",
    "81192e4e415d359ca832eff50d6f64c3da7acb73",
    "959afc0fd3bdd4fa366fbec97ffa6b96d4528e53",
    "60c20e559b8036410e2d9081b9c60d1e04e56253",
    "ca4582222e89114e4c61d38fbf973a66d2867abf",
    "cae8116a146cb27d40e4e41cece9a17945bc7f9c",
    "13daca1ef6aa4a24ff9a840397dda1bbddb16e37",
    "8021d9199f278345aca6211f318145342ad036f4",
    "1e68d4726b225fb4a62eb8d79a3160dd03059ccb",
    "42cdc1b0835abb4a331d40f30f2c210143b747bc",
    "f765fdb65701e61887daedb2b369af4be12cb432",
    "74d4573a23db5586c6e47ff2277aa7c35237da34"
  ],
  "changeHistoryShort": {
    "151c8ddbe4c05fcb5f251fa4450edc452f6c735a": "Ybodychange",
    "6beb25ab7e4f5454dba0315a296081e61753f301": "Ybodychange",
    "098ec2b11ff3f677eb823f75b147a1ac8dbf959e": "Ybodychange",
    "94cf7ab9d28a885181afeb2c181dfe857d158254": "Ybodychange",
    "53c38cc89ab979ec47557dcfa7affbad20578c0a": "Ybodychange",
    "a6ed4894b518351bf1b3290e725a475570a21296": "Ybodychange",
    "988639640024933448f822b48119dc832ebd2af3": "Ybodychange",
    "2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b": "Ybodychange",
    "81192e4e415d359ca832eff50d6f64c3da7acb73": "Ybodychange",
    "959afc0fd3bdd4fa366fbec97ffa6b96d4528e53": "Ybodychange",
    "60c20e559b8036410e2d9081b9c60d1e04e56253": "Ybodychange",
    "ca4582222e89114e4c61d38fbf973a66d2867abf": "Ybodychange",
    "cae8116a146cb27d40e4e41cece9a17945bc7f9c": "Ybodychange",
    "13daca1ef6aa4a24ff9a840397dda1bbddb16e37": "Ybodychange",
    "8021d9199f278345aca6211f318145342ad036f4": "Ybodychange",
    "1e68d4726b225fb4a62eb8d79a3160dd03059ccb": "Ybodychange",
    "42cdc1b0835abb4a331d40f30f2c210143b747bc": "Ymultichange(Yparameterchange,Ybodychange)",
    "f765fdb65701e61887daedb2b369af4be12cb432": "Ybodychange",
    "74d4573a23db5586c6e47ff2277aa7c35237da34": "Yintroduced"
  },
  "changeHistoryDetails": {
    "151c8ddbe4c05fcb5f251fa4450edc452f6c735a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13608. [SBN read] Edit Tail Fast Path Part 2: Add ability for JournalNode to serve edits via RPC. Contributed by Erik Krogen.\n",
      "commitDate": "24/12/18 9:33 AM",
      "commitName": "151c8ddbe4c05fcb5f251fa4450edc452f6c735a",
      "commitAuthor": "Erik Krogen",
      "commitDateOld": "06/09/18 2:48 PM",
      "commitNameOld": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 108.82,
      "commitsBetweenForRepo": 926,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,81 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n     // If numTxns is 0, it\u0027s actually a fake send which aims at updating\n     // committedTxId only. So we can return early.\n     if (numTxns \u003d\u003d 0) {\n       return;\n     }\n \n     checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\" + \" ; journal id: \" + journalId);\n \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n               + \" but current segment is \" + curSegmentTxId\n               + \" ; journal id: \" + journalId);\n       abortCurSegment();\n       throw e;\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId\n             + \" ; journal id: \" + journalId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId +\n           \" ; journal id: \" + journalId);\n     }\n+    if (cache !\u003d null) {\n+      cache.storeEdits(records, firstTxnId, lastTxnId, curSegmentLayoutVersion);\n+    }\n \n     // If the edit has already been marked as committed, we know\n     // it has been fsynced on a quorum of other nodes, and we are\n     // \"catching up\" with the rest. Hence we do not need to fsync.\n     boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n     boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     StopWatch sw \u003d new StopWatch();\n     sw.start();\n     curSegment.flush(shouldFsync);\n     sw.stop();\n \n     long nanoSeconds \u003d sw.now();\n     metrics.addSync(\n         TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n     long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n         nanoSeconds, TimeUnit.NANOSECONDS);\n \n     if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n       LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n                \" took \" + milliSeconds + \"ms\" + \" ; journal id: \" + journalId);\n     }\n \n     if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     \n     updateHighestWrittenTxId(lastTxnId);\n     nextTxId \u003d lastTxnId + 1;\n     lastJournalTimestamp \u003d Time.now();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    // If numTxns is 0, it\u0027s actually a fake send which aims at updating\n    // committedTxId only. So we can return early.\n    if (numTxns \u003d\u003d 0) {\n      return;\n    }\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\" + \" ; journal id: \" + journalId);\n\n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n              + \" but current segment is \" + curSegmentTxId\n              + \" ; journal id: \" + journalId);\n      abortCurSegment();\n      throw e;\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId\n            + \" ; journal id: \" + journalId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId +\n          \" ; journal id: \" + journalId);\n    }\n    if (cache !\u003d null) {\n      cache.storeEdits(records, firstTxnId, lastTxnId, curSegmentLayoutVersion);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    StopWatch sw \u003d new StopWatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n\n    long nanoSeconds \u003d sw.now();\n    metrics.addSync(\n        TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n    long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n        nanoSeconds, TimeUnit.NANOSECONDS);\n\n    if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n      LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n               \" took \" + milliSeconds + \"ms\" + \" ; journal id: \" + journalId);\n    }\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    \n    updateHighestWrittenTxId(lastTxnId);\n    nextTxId \u003d lastTxnId + 1;\n    lastJournalTimestamp \u003d Time.now();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "6beb25ab7e4f5454dba0315a296081e61753f301": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13544. Improve logging for JournalNode in federated cluster.\n",
      "commitDate": "14/05/18 10:12 AM",
      "commitName": "6beb25ab7e4f5454dba0315a296081e61753f301",
      "commitAuthor": "Hanisha Koneru",
      "commitDateOld": "13/10/17 2:22 PM",
      "commitNameOld": "8dd1eeb94fef59feaf19182dd8f1fcf1389c7f34",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 212.83,
      "commitsBetweenForRepo": 2051,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,78 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n     // If numTxns is 0, it\u0027s actually a fake send which aims at updating\n     // committedTxId only. So we can return early.\n     if (numTxns \u003d\u003d 0) {\n       return;\n     }\n \n     checkSync(curSegment !\u003d null,\n-        \"Can\u0027t write, no segment open\");\n+        \"Can\u0027t write, no segment open\" + \" ; journal id: \" + journalId);\n \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n-          + \" but current segment is \" + curSegmentTxId);\n+              + \" but current segment is \" + curSegmentTxId\n+              + \" ; journal id: \" + journalId);\n       abortCurSegment();\n       throw e;\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n-        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n+        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId\n+            + \" ; journal id: \" + journalId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n+      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId +\n+          \" ; journal id: \" + journalId);\n     }\n \n     // If the edit has already been marked as committed, we know\n     // it has been fsynced on a quorum of other nodes, and we are\n     // \"catching up\" with the rest. Hence we do not need to fsync.\n     boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n     boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     StopWatch sw \u003d new StopWatch();\n     sw.start();\n     curSegment.flush(shouldFsync);\n     sw.stop();\n \n     long nanoSeconds \u003d sw.now();\n     metrics.addSync(\n         TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n     long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n         nanoSeconds, TimeUnit.NANOSECONDS);\n \n     if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n       LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n-               \" took \" + milliSeconds + \"ms\");\n+               \" took \" + milliSeconds + \"ms\" + \" ; journal id: \" + journalId);\n     }\n \n     if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     \n     updateHighestWrittenTxId(lastTxnId);\n     nextTxId \u003d lastTxnId + 1;\n     lastJournalTimestamp \u003d Time.now();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    // If numTxns is 0, it\u0027s actually a fake send which aims at updating\n    // committedTxId only. So we can return early.\n    if (numTxns \u003d\u003d 0) {\n      return;\n    }\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\" + \" ; journal id: \" + journalId);\n\n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n              + \" but current segment is \" + curSegmentTxId\n              + \" ; journal id: \" + journalId);\n      abortCurSegment();\n      throw e;\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId\n            + \" ; journal id: \" + journalId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId +\n          \" ; journal id: \" + journalId);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    StopWatch sw \u003d new StopWatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n\n    long nanoSeconds \u003d sw.now();\n    metrics.addSync(\n        TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n    long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n        nanoSeconds, TimeUnit.NANOSECONDS);\n\n    if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n      LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n               \" took \" + milliSeconds + \"ms\" + \" ; journal id: \" + journalId);\n    }\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    \n    updateHighestWrittenTxId(lastTxnId);\n    nextTxId \u003d lastTxnId + 1;\n    lastJournalTimestamp \u003d Time.now();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "098ec2b11ff3f677eb823f75b147a1ac8dbf959e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10519. Add a configuration option to enable in-progress edit log tailing. Contributed by Jiayi Zhou.\n",
      "commitDate": "27/07/16 5:55 PM",
      "commitName": "098ec2b11ff3f677eb823f75b147a1ac8dbf959e",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "14/09/15 3:22 PM",
      "commitNameOld": "53bad4eb008ec553dcdbe01e7ae975dcecde6590",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 317.11,
      "commitsBetweenForRepo": 2253,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,75 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n+    // If numTxns is 0, it\u0027s actually a fake send which aims at updating\n+    // committedTxId only. So we can return early.\n+    if (numTxns \u003d\u003d 0) {\n+      return;\n+    }\n+\n     checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n-    \n+\n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n       abortCurSegment();\n       throw e;\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n \n     // If the edit has already been marked as committed, we know\n     // it has been fsynced on a quorum of other nodes, and we are\n     // \"catching up\" with the rest. Hence we do not need to fsync.\n     boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n     boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     StopWatch sw \u003d new StopWatch();\n     sw.start();\n     curSegment.flush(shouldFsync);\n     sw.stop();\n \n     long nanoSeconds \u003d sw.now();\n     metrics.addSync(\n         TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n     long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n         nanoSeconds, TimeUnit.NANOSECONDS);\n \n     if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n       LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n                \" took \" + milliSeconds + \"ms\");\n     }\n \n     if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     \n     updateHighestWrittenTxId(lastTxnId);\n     nextTxId \u003d lastTxnId + 1;\n     lastJournalTimestamp \u003d Time.now();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    // If numTxns is 0, it\u0027s actually a fake send which aims at updating\n    // committedTxId only. So we can return early.\n    if (numTxns \u003d\u003d 0) {\n      return;\n    }\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n\n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n      abortCurSegment();\n      throw e;\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    StopWatch sw \u003d new StopWatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n\n    long nanoSeconds \u003d sw.now();\n    metrics.addSync(\n        TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n    long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n        nanoSeconds, TimeUnit.NANOSECONDS);\n\n    if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n      LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n               \" took \" + milliSeconds + \"ms\");\n    }\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    \n    updateHighestWrittenTxId(lastTxnId);\n    nextTxId \u003d lastTxnId + 1;\n    lastJournalTimestamp \u003d Time.now();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "94cf7ab9d28a885181afeb2c181dfe857d158254": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8929. Add a metric to expose the timestamp of the last journal (Contributed by surendra singh lilhore)\n",
      "commitDate": "08/09/15 10:33 PM",
      "commitName": "94cf7ab9d28a885181afeb2c181dfe857d158254",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "03/09/15 11:22 AM",
      "commitNameOld": "53c38cc89ab979ec47557dcfa7affbad20578c0a",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 5.47,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,69 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n     checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n       abortCurSegment();\n       throw e;\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n \n     // If the edit has already been marked as committed, we know\n     // it has been fsynced on a quorum of other nodes, and we are\n     // \"catching up\" with the rest. Hence we do not need to fsync.\n     boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n     boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     StopWatch sw \u003d new StopWatch();\n     sw.start();\n     curSegment.flush(shouldFsync);\n     sw.stop();\n \n     long nanoSeconds \u003d sw.now();\n     metrics.addSync(\n         TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n     long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n         nanoSeconds, TimeUnit.NANOSECONDS);\n \n     if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n       LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n                \" took \" + milliSeconds + \"ms\");\n     }\n \n     if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     \n     updateHighestWrittenTxId(lastTxnId);\n     nextTxId \u003d lastTxnId + 1;\n+    lastJournalTimestamp \u003d Time.now();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n      abortCurSegment();\n      throw e;\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    StopWatch sw \u003d new StopWatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n\n    long nanoSeconds \u003d sw.now();\n    metrics.addSync(\n        TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n    long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n        nanoSeconds, TimeUnit.NANOSECONDS);\n\n    if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n      LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n               \" took \" + milliSeconds + \"ms\");\n    }\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    \n    updateHighestWrittenTxId(lastTxnId);\n    nextTxId \u003d lastTxnId + 1;\n    lastJournalTimestamp \u003d Time.now();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "53c38cc89ab979ec47557dcfa7affbad20578c0a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8964. When validating the edit log, do not read at or beyond the file offset that is being written (Zhe Zhang via Colin P. McCabe)\n",
      "commitDate": "03/09/15 11:22 AM",
      "commitName": "53c38cc89ab979ec47557dcfa7affbad20578c0a",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 124.06,
      "commitsBetweenForRepo": 888,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,68 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n     checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n       abortCurSegment();\n       throw e;\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n \n     // If the edit has already been marked as committed, we know\n     // it has been fsynced on a quorum of other nodes, and we are\n     // \"catching up\" with the rest. Hence we do not need to fsync.\n     boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n     boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     StopWatch sw \u003d new StopWatch();\n     sw.start();\n     curSegment.flush(shouldFsync);\n     sw.stop();\n \n     long nanoSeconds \u003d sw.now();\n     metrics.addSync(\n         TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n     long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n         nanoSeconds, TimeUnit.NANOSECONDS);\n \n     if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n       LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n                \" took \" + milliSeconds + \"ms\");\n     }\n \n     if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     \n-    highestWrittenTxId \u003d lastTxnId;\n+    updateHighestWrittenTxId(lastTxnId);\n     nextTxId \u003d lastTxnId + 1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n      abortCurSegment();\n      throw e;\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    StopWatch sw \u003d new StopWatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n\n    long nanoSeconds \u003d sw.now();\n    metrics.addSync(\n        TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n    long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n        nanoSeconds, TimeUnit.NANOSECONDS);\n\n    if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n      LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n               \" took \" + milliSeconds + \"ms\");\n    }\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    \n    updateHighestWrittenTxId(lastTxnId);\n    nextTxId \u003d lastTxnId + 1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "a6ed4894b518351bf1b3290e725a475570a21296": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11032. Replace use of Guava\u0027s Stopwatch with Hadoop\u0027s StopWatch. (ozawa)\n",
      "commitDate": "07/01/15 9:51 PM",
      "commitName": "a6ed4894b518351bf1b3290e725a475570a21296",
      "commitAuthor": "Tsuyoshi Ozawa",
      "commitDateOld": "07/01/15 9:35 PM",
      "commitNameOld": "988639640024933448f822b48119dc832ebd2af3",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,68 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n     checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n       abortCurSegment();\n       throw e;\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n \n     // If the edit has already been marked as committed, we know\n     // it has been fsynced on a quorum of other nodes, and we are\n     // \"catching up\" with the rest. Hence we do not need to fsync.\n     boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n     boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n-    Stopwatch sw \u003d new Stopwatch();\n+    StopWatch sw \u003d new StopWatch();\n     sw.start();\n     curSegment.flush(shouldFsync);\n     sw.stop();\n-    \n-    metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n-    if (sw.elapsedTime(TimeUnit.MILLISECONDS) \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n+\n+    long nanoSeconds \u003d sw.now();\n+    metrics.addSync(\n+        TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n+    long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n+        nanoSeconds, TimeUnit.NANOSECONDS);\n+\n+    if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n       LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n-               \" took \" + sw.elapsedTime(TimeUnit.MILLISECONDS) + \"ms\");\n+               \" took \" + milliSeconds + \"ms\");\n     }\n \n     if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     \n     highestWrittenTxId \u003d lastTxnId;\n     nextTxId \u003d lastTxnId + 1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n      abortCurSegment();\n      throw e;\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    StopWatch sw \u003d new StopWatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n\n    long nanoSeconds \u003d sw.now();\n    metrics.addSync(\n        TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n    long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n        nanoSeconds, TimeUnit.NANOSECONDS);\n\n    if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n      LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n               \" took \" + milliSeconds + \"ms\");\n    }\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    \n    highestWrittenTxId \u003d lastTxnId;\n    nextTxId \u003d lastTxnId + 1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "988639640024933448f822b48119dc832ebd2af3": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"Replace use of Guava\u0027s Stopwatch with Hadoop\u0027s StopWatch. (ozawa)\" because of missing JIRA\u0027s number.\n\nThis reverts commit 2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b.\n",
      "commitDate": "07/01/15 9:35 PM",
      "commitName": "988639640024933448f822b48119dc832ebd2af3",
      "commitAuthor": "Tsuyoshi Ozawa",
      "commitDateOld": "07/01/15 9:21 PM",
      "commitNameOld": "2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,63 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n     checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n       abortCurSegment();\n       throw e;\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n \n     // If the edit has already been marked as committed, we know\n     // it has been fsynced on a quorum of other nodes, and we are\n     // \"catching up\" with the rest. Hence we do not need to fsync.\n     boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n     boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n-    StopWatch sw \u003d new StopWatch();\n+    Stopwatch sw \u003d new Stopwatch();\n     sw.start();\n     curSegment.flush(shouldFsync);\n     sw.stop();\n-\n-    long nanoSeconds \u003d sw.now();\n-    metrics.addSync(\n-        TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n-    long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n-        nanoSeconds, TimeUnit.NANOSECONDS);\n-\n-    if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n+    \n+    metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n+    if (sw.elapsedTime(TimeUnit.MILLISECONDS) \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n       LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n-               \" took \" + milliSeconds + \"ms\");\n+               \" took \" + sw.elapsedTime(TimeUnit.MILLISECONDS) + \"ms\");\n     }\n \n     if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     \n     highestWrittenTxId \u003d lastTxnId;\n     nextTxId \u003d lastTxnId + 1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n      abortCurSegment();\n      throw e;\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    Stopwatch sw \u003d new Stopwatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n    \n    metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n    if (sw.elapsedTime(TimeUnit.MILLISECONDS) \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n      LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n               \" took \" + sw.elapsedTime(TimeUnit.MILLISECONDS) + \"ms\");\n    }\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    \n    highestWrittenTxId \u003d lastTxnId;\n    nextTxId \u003d lastTxnId + 1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b": {
      "type": "Ybodychange",
      "commitMessage": "Replace use of Guava\u0027s Stopwatch with Hadoop\u0027s StopWatch. (ozawa)\n",
      "commitDate": "07/01/15 9:21 PM",
      "commitName": "2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b",
      "commitAuthor": "Tsuyoshi Ozawa",
      "commitDateOld": "25/09/14 5:15 PM",
      "commitNameOld": "e9c37de485f8d4dcb04afb0d4cb887cc09d317c9",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 104.21,
      "commitsBetweenForRepo": 800,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,68 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n     checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n       abortCurSegment();\n       throw e;\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n \n     // If the edit has already been marked as committed, we know\n     // it has been fsynced on a quorum of other nodes, and we are\n     // \"catching up\" with the rest. Hence we do not need to fsync.\n     boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n     boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n-    Stopwatch sw \u003d new Stopwatch();\n+    StopWatch sw \u003d new StopWatch();\n     sw.start();\n     curSegment.flush(shouldFsync);\n     sw.stop();\n-    \n-    metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n-    if (sw.elapsedTime(TimeUnit.MILLISECONDS) \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n+\n+    long nanoSeconds \u003d sw.now();\n+    metrics.addSync(\n+        TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n+    long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n+        nanoSeconds, TimeUnit.NANOSECONDS);\n+\n+    if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n       LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n-               \" took \" + sw.elapsedTime(TimeUnit.MILLISECONDS) + \"ms\");\n+               \" took \" + milliSeconds + \"ms\");\n     }\n \n     if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     \n     highestWrittenTxId \u003d lastTxnId;\n     nextTxId \u003d lastTxnId + 1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n      abortCurSegment();\n      throw e;\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    StopWatch sw \u003d new StopWatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n\n    long nanoSeconds \u003d sw.now();\n    metrics.addSync(\n        TimeUnit.MICROSECONDS.convert(nanoSeconds, TimeUnit.NANOSECONDS));\n    long milliSeconds \u003d TimeUnit.MILLISECONDS.convert(\n        nanoSeconds, TimeUnit.NANOSECONDS);\n\n    if (milliSeconds \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n      LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n               \" took \" + milliSeconds + \"ms\");\n    }\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    \n    highestWrittenTxId \u003d lastTxnId;\n    nextTxId \u003d lastTxnId + 1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "81192e4e415d359ca832eff50d6f64c3da7acb73": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4621. Additional logging to help diagnose slow QJM syncs. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1461777 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/03/13 11:42 AM",
      "commitName": "81192e4e415d359ca832eff50d6f64c3da7acb73",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "11/03/13 4:13 PM",
      "commitNameOld": "9daa958c5a69d5577e742fad94167e713abf688f",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 15.81,
      "commitsBetweenForRepo": 100,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,63 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n     checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n       abortCurSegment();\n       throw e;\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n \n     // If the edit has already been marked as committed, we know\n     // it has been fsynced on a quorum of other nodes, and we are\n     // \"catching up\" with the rest. Hence we do not need to fsync.\n     boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n     boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     Stopwatch sw \u003d new Stopwatch();\n     sw.start();\n     curSegment.flush(shouldFsync);\n     sw.stop();\n     \n     metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n+    if (sw.elapsedTime(TimeUnit.MILLISECONDS) \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n+      LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n+               \" took \" + sw.elapsedTime(TimeUnit.MILLISECONDS) + \"ms\");\n+    }\n \n     if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     \n     highestWrittenTxId \u003d lastTxnId;\n     nextTxId \u003d lastTxnId + 1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n      abortCurSegment();\n      throw e;\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    Stopwatch sw \u003d new Stopwatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n    \n    metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n    if (sw.elapsedTime(TimeUnit.MILLISECONDS) \u003e WARN_SYNC_MILLIS_THRESHOLD) {\n      LOG.warn(\"Sync of transaction range \" + firstTxnId + \"-\" + lastTxnId +\n               \" took \" + sw.elapsedTime(TimeUnit.MILLISECONDS) + \"ms\");\n    }\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    \n    highestWrittenTxId \u003d lastTxnId;\n    nextTxId \u003d lastTxnId + 1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "959afc0fd3bdd4fa366fbec97ffa6b96d4528e53": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3901. QJM: send \u0027heartbeat\u0027 messages to JNs even when they are out-of-sync. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1383137 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/09/12 3:30 PM",
      "commitName": "959afc0fd3bdd4fa366fbec97ffa6b96d4528e53",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "10/09/12 11:53 AM",
      "commitNameOld": "60c20e559b8036410e2d9081b9c60d1e04e56253",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.15,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,59 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n     checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n       abortCurSegment();\n       throw e;\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n \n     // If the edit has already been marked as committed, we know\n     // it has been fsynced on a quorum of other nodes, and we are\n     // \"catching up\" with the rest. Hence we do not need to fsync.\n     boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n     boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     Stopwatch sw \u003d new Stopwatch();\n     sw.start();\n     curSegment.flush(shouldFsync);\n     sw.stop();\n     \n     metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n \n     if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n-      metrics.currentLagTxns.set(committedTxnId.get() - lastTxnId);\n-    } else {\n-      metrics.currentLagTxns.set(0L);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n-    metrics.lastWrittenTxId.set(lastTxnId);\n     \n-    nextTxId +\u003d numTxns;\n+    highestWrittenTxId \u003d lastTxnId;\n+    nextTxId \u003d lastTxnId + 1;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n      abortCurSegment();\n      throw e;\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    Stopwatch sw \u003d new Stopwatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n    \n    metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    \n    highestWrittenTxId \u003d lastTxnId;\n    nextTxId \u003d lastTxnId + 1;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "60c20e559b8036410e2d9081b9c60d1e04e56253": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3900. QJM: avoid validating log segments on log rolls. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1383041 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/09/12 11:53 AM",
      "commitName": "60c20e559b8036410e2d9081b9c60d1e04e56253",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "10/09/12 11:51 AM",
      "commitNameOld": "ca4582222e89114e4c61d38fbf973a66d2867abf",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,62 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n     checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n-      curSegment.abort();\n-      curSegment \u003d null;\n-      throw new IllegalStateException(\n+      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n+      abortCurSegment();\n+      throw e;\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n \n     // If the edit has already been marked as committed, we know\n     // it has been fsynced on a quorum of other nodes, and we are\n     // \"catching up\" with the rest. Hence we do not need to fsync.\n     boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n     boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     Stopwatch sw \u003d new Stopwatch();\n     sw.start();\n     curSegment.flush(shouldFsync);\n     sw.stop();\n     \n     metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n \n     if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n       metrics.currentLagTxns.set(committedTxnId.get() - lastTxnId);\n     } else {\n       metrics.currentLagTxns.set(0L);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     metrics.lastWrittenTxId.set(lastTxnId);\n     \n     nextTxId +\u003d numTxns;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      JournalOutOfSyncException e \u003d new JournalOutOfSyncException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n      abortCurSegment();\n      throw e;\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    Stopwatch sw \u003d new Stopwatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n    \n    metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n      metrics.currentLagTxns.set(committedTxnId.get() - lastTxnId);\n    } else {\n      metrics.currentLagTxns.set(0L);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    metrics.lastWrittenTxId.set(lastTxnId);\n    \n    nextTxId +\u003d numTxns;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "ca4582222e89114e4c61d38fbf973a66d2867abf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3885. QJM: optimize log sync when JN is lagging behind. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1383039 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/09/12 11:51 AM",
      "commitName": "ca4582222e89114e4c61d38fbf973a66d2867abf",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "06/09/12 2:42 PM",
      "commitNameOld": "df801074c929d5414b92cc9fc0cc8a2794e02751",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 3.88,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,62 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n \n     checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       curSegment.abort();\n       curSegment \u003d null;\n       throw new IllegalStateException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n     }\n       \n     checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n+\n+    // If the edit has already been marked as committed, we know\n+    // it has been fsynced on a quorum of other nodes, and we are\n+    // \"catching up\" with the rest. Hence we do not need to fsync.\n+    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n+    boolean shouldFsync \u003d !isLagging;\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     Stopwatch sw \u003d new Stopwatch();\n     sw.start();\n-    curSegment.flush();\n+    curSegment.flush(shouldFsync);\n     sw.stop();\n     \n     metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n-    \n-    if (committedTxnId.get() \u003e lastTxnId) {\n+\n+    if (isLagging) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n       metrics.currentLagTxns.set(committedTxnId.get() - lastTxnId);\n     } else {\n       metrics.currentLagTxns.set(0L);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     metrics.lastWrittenTxId.set(lastTxnId);\n     \n     nextTxId +\u003d numTxns;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      curSegment.abort();\n      curSegment \u003d null;\n      throw new IllegalStateException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n\n    // If the edit has already been marked as committed, we know\n    // it has been fsynced on a quorum of other nodes, and we are\n    // \"catching up\" with the rest. Hence we do not need to fsync.\n    boolean isLagging \u003d lastTxnId \u003c\u003d committedTxnId.get();\n    boolean shouldFsync \u003d !isLagging;\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    Stopwatch sw \u003d new Stopwatch();\n    sw.start();\n    curSegment.flush(shouldFsync);\n    sw.stop();\n    \n    metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n\n    if (isLagging) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n      metrics.currentLagTxns.set(committedTxnId.get() - lastTxnId);\n    } else {\n      metrics.currentLagTxns.set(0L);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    metrics.lastWrittenTxId.set(lastTxnId);\n    \n    nextTxId +\u003d numTxns;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "cae8116a146cb27d40e4e41cece9a17945bc7f9c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3726. If a logger misses an RPC, don\u0027t retry that logger until next segment. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1381482 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/09/12 12:03 AM",
      "commitName": "cae8116a146cb27d40e4e41cece9a17945bc7f9c",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "04/09/12 9:30 PM",
      "commitNameOld": "13daca1ef6aa4a24ff9a840397dda1bbddb16e37",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 1.11,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,56 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n-    \n-    // TODO: if a JN goes down and comes back up, then it will throw\n-    // this exception on every edit. We should instead send back\n-    // a response indicating the log needs to be rolled, which would\n-    // mark the logger on the client side as \"pending\" -- and have the\n-    // NN code look for this condition and trigger a roll when it happens.\n-    // That way the node can catch back up and rejoin\n-    Preconditions.checkState(curSegment !\u003d null,\n+\n+    checkSync(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       curSegment.abort();\n       curSegment \u003d null;\n       throw new IllegalStateException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n     }\n       \n-    Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n+    checkSync(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     Stopwatch sw \u003d new Stopwatch();\n     sw.start();\n     curSegment.flush();\n     sw.stop();\n     \n     metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n     \n     if (committedTxnId.get() \u003e lastTxnId) {\n       // This batch of edits has already been committed on a quorum of other\n       // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n       metrics.batchesWrittenWhileLagging.incr(1);\n       metrics.currentLagTxns.set(committedTxnId.get() - lastTxnId);\n     } else {\n       metrics.currentLagTxns.set(0L);\n     }\n     \n     metrics.batchesWritten.incr(1);\n     metrics.bytesWritten.incr(records.length);\n     metrics.txnsWritten.incr(numTxns);\n     metrics.lastWrittenTxId.set(lastTxnId);\n     \n     nextTxId +\u003d numTxns;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n\n    checkSync(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      curSegment.abort();\n      curSegment \u003d null;\n      throw new IllegalStateException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n    }\n      \n    checkSync(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    Stopwatch sw \u003d new Stopwatch();\n    sw.start();\n    curSegment.flush();\n    sw.stop();\n    \n    metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n    \n    if (committedTxnId.get() \u003e lastTxnId) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n      metrics.currentLagTxns.set(committedTxnId.get() - lastTxnId);\n    } else {\n      metrics.currentLagTxns.set(0L);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    metrics.lastWrittenTxId.set(lastTxnId);\n    \n    nextTxId +\u003d numTxns;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "13daca1ef6aa4a24ff9a840397dda1bbddb16e37": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3870. Add metrics to JournalNode. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1380980 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/12 9:30 PM",
      "commitName": "13daca1ef6aa4a24ff9a840397dda1bbddb16e37",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "04/09/12 9:27 PM",
      "commitNameOld": "f6b7f067c34e1fc3c050453f00f2d81274cd32b4",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,62 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkFormatted();\n     checkWriteRequest(reqInfo);\n     \n     // TODO: if a JN goes down and comes back up, then it will throw\n     // this exception on every edit. We should instead send back\n     // a response indicating the log needs to be rolled, which would\n     // mark the logger on the client side as \"pending\" -- and have the\n     // NN code look for this condition and trigger a roll when it happens.\n     // That way the node can catch back up and rejoin\n     Preconditions.checkState(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       curSegment.abort();\n       curSegment \u003d null;\n       throw new IllegalStateException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n     }\n       \n     Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n+    long lastTxnId \u003d firstTxnId + numTxns - 1;\n     if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n+      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n     }\n     \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n+    Stopwatch sw \u003d new Stopwatch();\n+    sw.start();\n     curSegment.flush();\n+    sw.stop();\n+    \n+    metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n+    \n+    if (committedTxnId.get() \u003e lastTxnId) {\n+      // This batch of edits has already been committed on a quorum of other\n+      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n+      metrics.batchesWrittenWhileLagging.incr(1);\n+      metrics.currentLagTxns.set(committedTxnId.get() - lastTxnId);\n+    } else {\n+      metrics.currentLagTxns.set(0L);\n+    }\n+    \n+    metrics.batchesWritten.incr(1);\n+    metrics.bytesWritten.incr(records.length);\n+    metrics.txnsWritten.incr(numTxns);\n+    metrics.lastWrittenTxId.set(lastTxnId);\n+    \n     nextTxId +\u003d numTxns;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n    \n    // TODO: if a JN goes down and comes back up, then it will throw\n    // this exception on every edit. We should instead send back\n    // a response indicating the log needs to be rolled, which would\n    // mark the logger on the client side as \"pending\" -- and have the\n    // NN code look for this condition and trigger a roll when it happens.\n    // That way the node can catch back up and rejoin\n    Preconditions.checkState(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      curSegment.abort();\n      curSegment \u003d null;\n      throw new IllegalStateException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n    }\n      \n    Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    long lastTxnId \u003d firstTxnId + numTxns - 1;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + lastTxnId);\n    }\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    Stopwatch sw \u003d new Stopwatch();\n    sw.start();\n    curSegment.flush();\n    sw.stop();\n    \n    metrics.addSync(sw.elapsedTime(TimeUnit.MICROSECONDS));\n    \n    if (committedTxnId.get() \u003e lastTxnId) {\n      // This batch of edits has already been committed on a quorum of other\n      // nodes. So, we are in \"catch up\" mode. This gets its own metric.\n      metrics.batchesWrittenWhileLagging.incr(1);\n      metrics.currentLagTxns.set(committedTxnId.get() - lastTxnId);\n    } else {\n      metrics.currentLagTxns.set(0L);\n    }\n    \n    metrics.batchesWritten.incr(1);\n    metrics.bytesWritten.incr(records.length);\n    metrics.txnsWritten.incr(numTxns);\n    metrics.lastWrittenTxId.set(lastTxnId);\n    \n    nextTxId +\u003d numTxns;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "8021d9199f278345aca6211f318145342ad036f4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3863. Track last \"committed\" txid in QJM. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1380976 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/12 9:13 PM",
      "commitName": "8021d9199f278345aca6211f318145342ad036f4",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "27/08/12 12:55 PM",
      "commitNameOld": "1e68d4726b225fb4a62eb8d79a3160dd03059ccb",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 8.35,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,41 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n-    checkWriteRequest(reqInfo);\n     checkFormatted();\n+    checkWriteRequest(reqInfo);\n     \n     // TODO: if a JN goes down and comes back up, then it will throw\n     // this exception on every edit. We should instead send back\n     // a response indicating the log needs to be rolled, which would\n     // mark the logger on the client side as \"pending\" -- and have the\n     // NN code look for this condition and trigger a roll when it happens.\n     // That way the node can catch back up and rejoin\n     Preconditions.checkState(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       curSegment.abort();\n       curSegment \u003d null;\n       throw new IllegalStateException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n     }\n       \n     Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n     }\n+    \n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     curSegment.flush();\n     nextTxId +\u003d numTxns;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkFormatted();\n    checkWriteRequest(reqInfo);\n    \n    // TODO: if a JN goes down and comes back up, then it will throw\n    // this exception on every edit. We should instead send back\n    // a response indicating the log needs to be rolled, which would\n    // mark the logger on the client side as \"pending\" -- and have the\n    // NN code look for this condition and trigger a roll when it happens.\n    // That way the node can catch back up and rejoin\n    Preconditions.checkState(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      curSegment.abort();\n      curSegment \u003d null;\n      throw new IllegalStateException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n    }\n      \n    Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n    }\n    \n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    curSegment.flush();\n    nextTxId +\u003d numTxns;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "1e68d4726b225fb4a62eb8d79a3160dd03059ccb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3845. Fixes for edge cases in QJM recovery protocol. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1377809 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/08/12 12:55 PM",
      "commitName": "1e68d4726b225fb4a62eb8d79a3160dd03059ccb",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "15/08/12 11:58 AM",
      "commitNameOld": "42cdc1b0835abb4a331d40f30f2c210143b747bc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 12.04,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   synchronized void journal(RequestInfo reqInfo,\n       long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n-    checkRequest(reqInfo);\n+    checkWriteRequest(reqInfo);\n     checkFormatted();\n     \n     // TODO: if a JN goes down and comes back up, then it will throw\n     // this exception on every edit. We should instead send back\n     // a response indicating the log needs to be rolled, which would\n     // mark the logger on the client side as \"pending\" -- and have the\n     // NN code look for this condition and trigger a roll when it happens.\n     // That way the node can catch back up and rejoin\n     Preconditions.checkState(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     \n     if (curSegmentTxId !\u003d segmentTxId) {\n       // Sanity check: it is possible that the writer will fail IPCs\n       // on both the finalize() and then the start() of the next segment.\n       // This could cause us to continue writing to an old segment\n       // instead of rolling to a new one, which breaks one of the\n       // invariants in the design. If it happens, abort the segment\n       // and throw an exception.\n       curSegment.abort();\n       curSegment \u003d null;\n       throw new IllegalStateException(\n           \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n           + \" but current segment is \" + curSegmentTxId);\n     }\n       \n     Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n     }\n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     curSegment.flush();\n     nextTxId +\u003d numTxns;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkWriteRequest(reqInfo);\n    checkFormatted();\n    \n    // TODO: if a JN goes down and comes back up, then it will throw\n    // this exception on every edit. We should instead send back\n    // a response indicating the log needs to be rolled, which would\n    // mark the logger on the client side as \"pending\" -- and have the\n    // NN code look for this condition and trigger a roll when it happens.\n    // That way the node can catch back up and rejoin\n    Preconditions.checkState(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      curSegment.abort();\n      curSegment \u003d null;\n      throw new IllegalStateException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n    }\n      \n    Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n    }\n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    curSegment.flush();\n    nextTxId +\u003d numTxns;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "42cdc1b0835abb4a331d40f30f2c210143b747bc": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-3797. QJM: add segment txid as a parameter to journal() RPC. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1373571 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/08/12 11:58 AM",
      "commitName": "42cdc1b0835abb4a331d40f30f2c210143b747bc",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-3797. QJM: add segment txid as a parameter to journal() RPC. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1373571 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "15/08/12 11:58 AM",
          "commitName": "42cdc1b0835abb4a331d40f30f2c210143b747bc",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "14/08/12 5:57 PM",
          "commitNameOld": "c95a1674b61ef2a6963dc64604986ef90a8c636d",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.75,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,40 @@\n-  synchronized void journal(RequestInfo reqInfo, long firstTxnId,\n+  synchronized void journal(RequestInfo reqInfo,\n+      long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkRequest(reqInfo);\n     checkFormatted();\n     \n     // TODO: if a JN goes down and comes back up, then it will throw\n     // this exception on every edit. We should instead send back\n     // a response indicating the log needs to be rolled, which would\n     // mark the logger on the client side as \"pending\" -- and have the\n     // NN code look for this condition and trigger a roll when it happens.\n     // That way the node can catch back up and rejoin\n     Preconditions.checkState(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n+    \n+    if (curSegmentTxId !\u003d segmentTxId) {\n+      // Sanity check: it is possible that the writer will fail IPCs\n+      // on both the finalize() and then the start() of the next segment.\n+      // This could cause us to continue writing to an old segment\n+      // instead of rolling to a new one, which breaks one of the\n+      // invariants in the design. If it happens, abort the segment\n+      // and throw an exception.\n+      curSegment.abort();\n+      curSegment \u003d null;\n+      throw new IllegalStateException(\n+          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n+          + \" but current segment is \" + curSegmentTxId);\n+    }\n+      \n     Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n     }\n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     curSegment.flush();\n     nextTxId +\u003d numTxns;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkRequest(reqInfo);\n    checkFormatted();\n    \n    // TODO: if a JN goes down and comes back up, then it will throw\n    // this exception on every edit. We should instead send back\n    // a response indicating the log needs to be rolled, which would\n    // mark the logger on the client side as \"pending\" -- and have the\n    // NN code look for this condition and trigger a roll when it happens.\n    // That way the node can catch back up and rejoin\n    Preconditions.checkState(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      curSegment.abort();\n      curSegment \u003d null;\n      throw new IllegalStateException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n    }\n      \n    Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n    }\n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    curSegment.flush();\n    nextTxId +\u003d numTxns;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
          "extendedDetails": {
            "oldValue": "[reqInfo-RequestInfo, firstTxnId-long, numTxns-int, records-byte[]]",
            "newValue": "[reqInfo-RequestInfo, segmentTxId-long, firstTxnId-long, numTxns-int, records-byte[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3797. QJM: add segment txid as a parameter to journal() RPC. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1373571 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "15/08/12 11:58 AM",
          "commitName": "42cdc1b0835abb4a331d40f30f2c210143b747bc",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "14/08/12 5:57 PM",
          "commitNameOld": "c95a1674b61ef2a6963dc64604986ef90a8c636d",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.75,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,40 @@\n-  synchronized void journal(RequestInfo reqInfo, long firstTxnId,\n+  synchronized void journal(RequestInfo reqInfo,\n+      long segmentTxId, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkRequest(reqInfo);\n     checkFormatted();\n     \n     // TODO: if a JN goes down and comes back up, then it will throw\n     // this exception on every edit. We should instead send back\n     // a response indicating the log needs to be rolled, which would\n     // mark the logger on the client side as \"pending\" -- and have the\n     // NN code look for this condition and trigger a roll when it happens.\n     // That way the node can catch back up and rejoin\n     Preconditions.checkState(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n+    \n+    if (curSegmentTxId !\u003d segmentTxId) {\n+      // Sanity check: it is possible that the writer will fail IPCs\n+      // on both the finalize() and then the start() of the next segment.\n+      // This could cause us to continue writing to an old segment\n+      // instead of rolling to a new one, which breaks one of the\n+      // invariants in the design. If it happens, abort the segment\n+      // and throw an exception.\n+      curSegment.abort();\n+      curSegment \u003d null;\n+      throw new IllegalStateException(\n+          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n+          + \" but current segment is \" + curSegmentTxId);\n+    }\n+      \n     Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n     }\n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     curSegment.flush();\n     nextTxId +\u003d numTxns;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized void journal(RequestInfo reqInfo,\n      long segmentTxId, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkRequest(reqInfo);\n    checkFormatted();\n    \n    // TODO: if a JN goes down and comes back up, then it will throw\n    // this exception on every edit. We should instead send back\n    // a response indicating the log needs to be rolled, which would\n    // mark the logger on the client side as \"pending\" -- and have the\n    // NN code look for this condition and trigger a roll when it happens.\n    // That way the node can catch back up and rejoin\n    Preconditions.checkState(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    \n    if (curSegmentTxId !\u003d segmentTxId) {\n      // Sanity check: it is possible that the writer will fail IPCs\n      // on both the finalize() and then the start() of the next segment.\n      // This could cause us to continue writing to an old segment\n      // instead of rolling to a new one, which breaks one of the\n      // invariants in the design. If it happens, abort the segment\n      // and throw an exception.\n      curSegment.abort();\n      curSegment \u003d null;\n      throw new IllegalStateException(\n          \"Writer out of sync: it thinks it is writing segment \" + segmentTxId\n          + \" but current segment is \" + curSegmentTxId);\n    }\n      \n    Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n    }\n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    curSegment.flush();\n    nextTxId +\u003d numTxns;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
          "extendedDetails": {}
        }
      ]
    },
    "f765fdb65701e61887daedb2b369af4be12cb432": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3793. Implement genericized format() in QJM. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1373177 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/08/12 5:48 PM",
      "commitName": "f765fdb65701e61887daedb2b369af4be12cb432",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "25/07/12 2:47 PM",
      "commitNameOld": "b17018e4b821ec860144d8bd38bc1fcb0d7eeaa5",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 20.13,
      "commitsBetweenForRepo": 82,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   synchronized void journal(RequestInfo reqInfo, long firstTxnId,\n       int numTxns, byte[] records) throws IOException {\n     checkRequest(reqInfo);\n+    checkFormatted();\n     \n     // TODO: if a JN goes down and comes back up, then it will throw\n     // this exception on every edit. We should instead send back\n     // a response indicating the log needs to be rolled, which would\n     // mark the logger on the client side as \"pending\" -- and have the\n     // NN code look for this condition and trigger a roll when it happens.\n     // That way the node can catch back up and rejoin\n     Preconditions.checkState(curSegment !\u003d null,\n         \"Can\u0027t write, no segment open\");\n     Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n         \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n     \n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n     }\n     curSegment.writeRaw(records, 0, records.length);\n     curSegment.setReadyToFlush();\n     curSegment.flush();\n     nextTxId +\u003d numTxns;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkRequest(reqInfo);\n    checkFormatted();\n    \n    // TODO: if a JN goes down and comes back up, then it will throw\n    // this exception on every edit. We should instead send back\n    // a response indicating the log needs to be rolled, which would\n    // mark the logger on the client side as \"pending\" -- and have the\n    // NN code look for this condition and trigger a roll when it happens.\n    // That way the node can catch back up and rejoin\n    Preconditions.checkState(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n    }\n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    curSegment.flush();\n    nextTxId +\u003d numTxns;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "74d4573a23db5586c6e47ff2277aa7c35237da34": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3077. Quorum-based protocol for reading and writing edit logs. Contributed by Todd Lipcon based on initial work from Brandon Li and Hari Mankude.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1363596 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/07/12 5:25 PM",
      "commitName": "74d4573a23db5586c6e47ff2277aa7c35237da34",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,23 @@\n+  synchronized void journal(RequestInfo reqInfo, long firstTxnId,\n+      int numTxns, byte[] records) throws IOException {\n+    checkRequest(reqInfo);\n+    \n+    // TODO: if a JN goes down and comes back up, then it will throw\n+    // this exception on every edit. We should instead send back\n+    // a response indicating the log needs to be rolled, which would\n+    // mark the logger on the client side as \"pending\" -- and have the\n+    // NN code look for this condition and trigger a roll when it happens.\n+    // That way the node can catch back up and rejoin\n+    Preconditions.checkState(curSegment !\u003d null,\n+        \"Can\u0027t write, no segment open\");\n+    Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n+        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n+    \n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n+    }\n+    curSegment.writeRaw(records, 0, records.length);\n+    curSegment.setReadyToFlush();\n+    curSegment.flush();\n+    nextTxId +\u003d numTxns;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void journal(RequestInfo reqInfo, long firstTxnId,\n      int numTxns, byte[] records) throws IOException {\n    checkRequest(reqInfo);\n    \n    // TODO: if a JN goes down and comes back up, then it will throw\n    // this exception on every edit. We should instead send back\n    // a response indicating the log needs to be rolled, which would\n    // mark the logger on the client side as \"pending\" -- and have the\n    // NN code look for this condition and trigger a roll when it happens.\n    // That way the node can catch back up and rejoin\n    Preconditions.checkState(curSegment !\u003d null,\n        \"Can\u0027t write, no segment open\");\n    Preconditions.checkState(nextTxId \u003d\u003d firstTxnId,\n        \"Can\u0027t write txid \" + firstTxnId + \" expecting nextTxId\u003d\" + nextTxId);\n    \n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Writing txid \" + firstTxnId + \"-\" + (firstTxnId + numTxns - 1));\n    }\n    curSegment.writeRaw(records, 0, records.length);\n    curSegment.setReadyToFlush();\n    curSegment.flush();\n    nextTxId +\u003d numTxns;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java"
    }
  }
}