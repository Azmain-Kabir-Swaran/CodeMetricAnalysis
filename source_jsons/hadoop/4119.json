{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DistributedFileSystem.java",
  "functionName": "build",
  "functionId": "build",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
  "functionStartLine": 3501,
  "functionEndLine": 3521,
  "numCommitsSeen": 96,
  "timeTaken": 5361,
  "changeHistory": [
    "0d7a5ac5f526801367a9ec963e6d72783b637d55",
    "9586b0e24fce29c278134658e68b8c47cd9d8c51",
    "6460df21a09a7fcc29eceb8dc3859d6298da6882",
    "5e7cfdca7b73a88bf3c3f1e5eb794a24218cce52",
    "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405",
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
    "332a997e10cca88d9ab3aa8252102366b628eaec"
  ],
  "changeHistoryShort": {
    "0d7a5ac5f526801367a9ec963e6d72783b637d55": "Ybodychange",
    "9586b0e24fce29c278134658e68b8c47cd9d8c51": "Ybodychange",
    "6460df21a09a7fcc29eceb8dc3859d6298da6882": "Ymultichange(Yreturntypechange,Ybodychange)",
    "5e7cfdca7b73a88bf3c3f1e5eb794a24218cce52": "Ybodychange",
    "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405": "Ybodychange",
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1": "Ybodychange",
    "332a997e10cca88d9ab3aa8252102366b628eaec": "Yintroduced"
  },
  "changeHistoryDetails": {
    "0d7a5ac5f526801367a9ec963e6d72783b637d55": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13209. DistributedFileSystem.create should allow an option to provide StoragePolicy. Contributed by Ayush Saxena.\n",
      "commitDate": "14/02/19 8:43 AM",
      "commitName": "0d7a5ac5f526801367a9ec963e6d72783b637d55",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "05/02/19 3:51 AM",
      "commitNameOld": "f365957c6326f88734bc0a5d01cfb7eac713db20",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 9.2,
      "commitsBetweenForRepo": 68,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,21 @@\n     public FSDataOutputStream build() throws IOException {\n       if (getFlags().contains(CreateFlag.CREATE) ||\n           getFlags().contains(CreateFlag.OVERWRITE)) {\n         if (isRecursive()) {\n           return dfs.create(getPath(), getPermission(), getFlags(),\n               getBufferSize(), getReplication(), getBlockSize(),\n               getProgress(), getChecksumOpt(), getFavoredNodes(),\n-              getEcPolicyName());\n+              getEcPolicyName(), getStoragePolicyName());\n         } else {\n           return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n               getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n-              getChecksumOpt(), getFavoredNodes(), getEcPolicyName());\n+              getChecksumOpt(), getFavoredNodes(), getEcPolicyName(),\n+              getStoragePolicyName());\n         }\n       } else if (getFlags().contains(CreateFlag.APPEND)) {\n         return dfs.append(getPath(), getFlags(), getBufferSize(), getProgress(),\n             getFavoredNodes());\n       }\n       throw new HadoopIllegalArgumentException(\n           \"Must specify either create or append\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public FSDataOutputStream build() throws IOException {\n      if (getFlags().contains(CreateFlag.CREATE) ||\n          getFlags().contains(CreateFlag.OVERWRITE)) {\n        if (isRecursive()) {\n          return dfs.create(getPath(), getPermission(), getFlags(),\n              getBufferSize(), getReplication(), getBlockSize(),\n              getProgress(), getChecksumOpt(), getFavoredNodes(),\n              getEcPolicyName(), getStoragePolicyName());\n        } else {\n          return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n              getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n              getChecksumOpt(), getFavoredNodes(), getEcPolicyName(),\n              getStoragePolicyName());\n        }\n      } else if (getFlags().contains(CreateFlag.APPEND)) {\n        return dfs.append(getPath(), getFlags(), getBufferSize(), getProgress(),\n            getFavoredNodes());\n      }\n      throw new HadoopIllegalArgumentException(\n          \"Must specify either create or append\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
      "extendedDetails": {}
    },
    "9586b0e24fce29c278134658e68b8c47cd9d8c51": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14397. Pull up the builder pattern to FileSystem and add AbstractContractCreateTest for it. (Lei (Eddy) Xu)\n",
      "commitDate": "31/07/17 8:12 PM",
      "commitName": "9586b0e24fce29c278134658e68b8c47cd9d8c51",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "28/06/17 10:54 AM",
      "commitNameOld": "f99b6d19de77c6e730fed8444f8848a7e63d6130",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 33.39,
      "commitsBetweenForRepo": 160,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,20 @@\n     public FSDataOutputStream build() throws IOException {\n-      if (getFlags().contains(CreateFlag.CREATE)) {\n+      if (getFlags().contains(CreateFlag.CREATE) ||\n+          getFlags().contains(CreateFlag.OVERWRITE)) {\n         if (isRecursive()) {\n           return dfs.create(getPath(), getPermission(), getFlags(),\n               getBufferSize(), getReplication(), getBlockSize(),\n               getProgress(), getChecksumOpt(), getFavoredNodes(),\n               getEcPolicyName());\n         } else {\n           return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n               getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n               getChecksumOpt(), getFavoredNodes(), getEcPolicyName());\n         }\n       } else if (getFlags().contains(CreateFlag.APPEND)) {\n         return dfs.append(getPath(), getFlags(), getBufferSize(), getProgress(),\n             getFavoredNodes());\n       }\n       throw new HadoopIllegalArgumentException(\n           \"Must specify either create or append\");\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public FSDataOutputStream build() throws IOException {\n      if (getFlags().contains(CreateFlag.CREATE) ||\n          getFlags().contains(CreateFlag.OVERWRITE)) {\n        if (isRecursive()) {\n          return dfs.create(getPath(), getPermission(), getFlags(),\n              getBufferSize(), getReplication(), getBlockSize(),\n              getProgress(), getChecksumOpt(), getFavoredNodes(),\n              getEcPolicyName());\n        } else {\n          return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n              getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n              getChecksumOpt(), getFavoredNodes(), getEcPolicyName());\n        }\n      } else if (getFlags().contains(CreateFlag.APPEND)) {\n        return dfs.append(getPath(), getFlags(), getBufferSize(), getProgress(),\n            getFavoredNodes());\n      }\n      throw new HadoopIllegalArgumentException(\n          \"Must specify either create or append\");\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
      "extendedDetails": {}
    },
    "6460df21a09a7fcc29eceb8dc3859d6298da6882": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HADOOP-14395. Provide Builder pattern for DistributedFileSystem.append. Contributed by Lei (Eddy) Xu.\n",
      "commitDate": "16/06/17 5:24 PM",
      "commitName": "6460df21a09a7fcc29eceb8dc3859d6298da6882",
      "commitAuthor": "Lei Xu",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HADOOP-14395. Provide Builder pattern for DistributedFileSystem.append. Contributed by Lei (Eddy) Xu.\n",
          "commitDate": "16/06/17 5:24 PM",
          "commitName": "6460df21a09a7fcc29eceb8dc3859d6298da6882",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "15/06/17 2:46 PM",
          "commitNameOld": "fb68980959f95f0d89e86f91909867724ad01791",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 1.11,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,19 @@\n-    public HdfsDataOutputStream build() throws IOException {\n-      if (isRecursive()) {\n-        return dfs.create(getPath(), getPermission(), getFlags(),\n-            getBufferSize(), getReplication(), getBlockSize(),\n-            getProgress(), getChecksumOpt(), getFavoredNodes(),\n-            getEcPolicyName());\n-      } else {\n-        return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n-            getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n-            getChecksumOpt(), getFavoredNodes(), getEcPolicyName());\n+    public FSDataOutputStream build() throws IOException {\n+      if (getFlags().contains(CreateFlag.CREATE)) {\n+        if (isRecursive()) {\n+          return dfs.create(getPath(), getPermission(), getFlags(),\n+              getBufferSize(), getReplication(), getBlockSize(),\n+              getProgress(), getChecksumOpt(), getFavoredNodes(),\n+              getEcPolicyName());\n+        } else {\n+          return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n+              getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n+              getChecksumOpt(), getFavoredNodes(), getEcPolicyName());\n+        }\n+      } else if (getFlags().contains(CreateFlag.APPEND)) {\n+        return dfs.append(getPath(), getFlags(), getBufferSize(), getProgress(),\n+            getFavoredNodes());\n       }\n+      throw new HadoopIllegalArgumentException(\n+          \"Must specify either create or append\");\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public FSDataOutputStream build() throws IOException {\n      if (getFlags().contains(CreateFlag.CREATE)) {\n        if (isRecursive()) {\n          return dfs.create(getPath(), getPermission(), getFlags(),\n              getBufferSize(), getReplication(), getBlockSize(),\n              getProgress(), getChecksumOpt(), getFavoredNodes(),\n              getEcPolicyName());\n        } else {\n          return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n              getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n              getChecksumOpt(), getFavoredNodes(), getEcPolicyName());\n        }\n      } else if (getFlags().contains(CreateFlag.APPEND)) {\n        return dfs.append(getPath(), getFlags(), getBufferSize(), getProgress(),\n            getFavoredNodes());\n      }\n      throw new HadoopIllegalArgumentException(\n          \"Must specify either create or append\");\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
          "extendedDetails": {
            "oldValue": "HdfsDataOutputStream",
            "newValue": "FSDataOutputStream"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-14395. Provide Builder pattern for DistributedFileSystem.append. Contributed by Lei (Eddy) Xu.\n",
          "commitDate": "16/06/17 5:24 PM",
          "commitName": "6460df21a09a7fcc29eceb8dc3859d6298da6882",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "15/06/17 2:46 PM",
          "commitNameOld": "fb68980959f95f0d89e86f91909867724ad01791",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 1.11,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,19 @@\n-    public HdfsDataOutputStream build() throws IOException {\n-      if (isRecursive()) {\n-        return dfs.create(getPath(), getPermission(), getFlags(),\n-            getBufferSize(), getReplication(), getBlockSize(),\n-            getProgress(), getChecksumOpt(), getFavoredNodes(),\n-            getEcPolicyName());\n-      } else {\n-        return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n-            getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n-            getChecksumOpt(), getFavoredNodes(), getEcPolicyName());\n+    public FSDataOutputStream build() throws IOException {\n+      if (getFlags().contains(CreateFlag.CREATE)) {\n+        if (isRecursive()) {\n+          return dfs.create(getPath(), getPermission(), getFlags(),\n+              getBufferSize(), getReplication(), getBlockSize(),\n+              getProgress(), getChecksumOpt(), getFavoredNodes(),\n+              getEcPolicyName());\n+        } else {\n+          return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n+              getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n+              getChecksumOpt(), getFavoredNodes(), getEcPolicyName());\n+        }\n+      } else if (getFlags().contains(CreateFlag.APPEND)) {\n+        return dfs.append(getPath(), getFlags(), getBufferSize(), getProgress(),\n+            getFavoredNodes());\n       }\n+      throw new HadoopIllegalArgumentException(\n+          \"Must specify either create or append\");\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public FSDataOutputStream build() throws IOException {\n      if (getFlags().contains(CreateFlag.CREATE)) {\n        if (isRecursive()) {\n          return dfs.create(getPath(), getPermission(), getFlags(),\n              getBufferSize(), getReplication(), getBlockSize(),\n              getProgress(), getChecksumOpt(), getFavoredNodes(),\n              getEcPolicyName());\n        } else {\n          return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n              getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n              getChecksumOpt(), getFavoredNodes(), getEcPolicyName());\n        }\n      } else if (getFlags().contains(CreateFlag.APPEND)) {\n        return dfs.append(getPath(), getFlags(), getBufferSize(), getProgress(),\n            getFavoredNodes());\n      }\n      throw new HadoopIllegalArgumentException(\n          \"Must specify either create or append\");\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "5e7cfdca7b73a88bf3c3f1e5eb794a24218cce52": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14394. Provide Builder pattern for DistributedFileSystem.create. (lei)\n",
      "commitDate": "15/06/17 10:59 AM",
      "commitName": "5e7cfdca7b73a88bf3c3f1e5eb794a24218cce52",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "14/06/17 10:44 AM",
      "commitNameOld": "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 1.01,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,12 @@\n     public HdfsDataOutputStream build() throws IOException {\n-      Preconditions.checkState(\n-          !(shouldReplicate() \u0026\u0026 (!StringUtils.isEmpty(getEcPolicyName()))),\n-          \"shouldReplicate and ecPolicyName are \" +\n-              \"exclusive parameters. Set both is not allowed!\");\n-\n-      EnumSet\u003cCreateFlag\u003e createFlags \u003d getFlags();\n-      if (shouldReplicate()) {\n-        createFlags.add(CreateFlag.SHOULD_REPLICATE);\n+      if (isRecursive()) {\n+        return dfs.create(getPath(), getPermission(), getFlags(),\n+            getBufferSize(), getReplication(), getBlockSize(),\n+            getProgress(), getChecksumOpt(), getFavoredNodes(),\n+            getEcPolicyName());\n+      } else {\n+        return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n+            getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n+            getChecksumOpt(), getFavoredNodes(), getEcPolicyName());\n       }\n-      return dfs.create(getPath(), getPermission(), createFlags,\n-          getBufferSize(), getReplication(), getBlockSize(),\n-          getProgress(), getChecksumOpt(), getFavoredNodes(),\n-          getEcPolicyName());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public HdfsDataOutputStream build() throws IOException {\n      if (isRecursive()) {\n        return dfs.create(getPath(), getPermission(), getFlags(),\n            getBufferSize(), getReplication(), getBlockSize(),\n            getProgress(), getChecksumOpt(), getFavoredNodes(),\n            getEcPolicyName());\n      } else {\n        return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n            getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n            getChecksumOpt(), getFavoredNodes(), getEcPolicyName());\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
      "extendedDetails": {}
    },
    "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11643. Add shouldReplicate option to create builder. Contributed by SammiChen.\n",
      "commitDate": "04/05/17 11:39 AM",
      "commitName": "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/04/17 10:18 PM",
      "commitNameOld": "cb672a45a0bbd8950b9b5e304c2e03f516945903",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 6.56,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,6 +1,15 @@\n     public HdfsDataOutputStream build() throws IOException {\n-      return dfs.create(getPath(), getPermission(), getFlags(),\n+      Preconditions.checkState(\n+          !(shouldReplicate() \u0026\u0026 (!StringUtils.isEmpty(getEcPolicyName()))),\n+          \"shouldReplicate and ecPolicyName are \" +\n+              \"exclusive parameters. Set both is not allowed!\");\n+\n+      EnumSet\u003cCreateFlag\u003e createFlags \u003d getFlags();\n+      if (shouldReplicate()) {\n+        createFlags.add(CreateFlag.SHOULD_REPLICATE);\n+      }\n+      return dfs.create(getPath(), getPermission(), createFlags,\n           getBufferSize(), getReplication(), getBlockSize(),\n           getProgress(), getChecksumOpt(), getFavoredNodes(),\n           getEcPolicyName());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public HdfsDataOutputStream build() throws IOException {\n      Preconditions.checkState(\n          !(shouldReplicate() \u0026\u0026 (!StringUtils.isEmpty(getEcPolicyName()))),\n          \"shouldReplicate and ecPolicyName are \" +\n              \"exclusive parameters. Set both is not allowed!\");\n\n      EnumSet\u003cCreateFlag\u003e createFlags \u003d getFlags();\n      if (shouldReplicate()) {\n        createFlags.add(CreateFlag.SHOULD_REPLICATE);\n      }\n      return dfs.create(getPath(), getPermission(), createFlags,\n          getBufferSize(), getReplication(), getBlockSize(),\n          getProgress(), getChecksumOpt(), getFavoredNodes(),\n          getEcPolicyName());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
      "extendedDetails": {}
    },
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10996. Ability to specify per-file EC policy at create time. Contributed by SammiChen.\n",
      "commitDate": "12/04/17 12:27 PM",
      "commitName": "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "04/04/17 1:38 PM",
      "commitNameOld": "18432130a7f580f206adf023507678c534487f2e",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 7.95,
      "commitsBetweenForRepo": 62,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,6 @@\n     public HdfsDataOutputStream build() throws IOException {\n       return dfs.create(getPath(), getPermission(), getFlags(),\n           getBufferSize(), getReplication(), getBlockSize(),\n-          getProgress(), getChecksumOpt(), getFavoredNodes());\n+          getProgress(), getChecksumOpt(), getFavoredNodes(),\n+          getEcPolicyName());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public HdfsDataOutputStream build() throws IOException {\n      return dfs.create(getPath(), getPermission(), getFlags(),\n          getBufferSize(), getReplication(), getBlockSize(),\n          getProgress(), getChecksumOpt(), getFavoredNodes(),\n          getEcPolicyName());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
      "extendedDetails": {}
    },
    "332a997e10cca88d9ab3aa8252102366b628eaec": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11170. Add builder-based create API to FileSystem. Contributed by SammiChen and Wei Zhou.\n",
      "commitDate": "24/03/17 12:56 PM",
      "commitName": "332a997e10cca88d9ab3aa8252102366b628eaec",
      "commitAuthor": "Andrew Wang",
      "diff": "@@ -0,0 +1,5 @@\n+    public HdfsDataOutputStream build() throws IOException {\n+      return dfs.create(getPath(), getPermission(), getFlags(),\n+          getBufferSize(), getReplication(), getBlockSize(),\n+          getProgress(), getChecksumOpt(), getFavoredNodes());\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public HdfsDataOutputStream build() throws IOException {\n      return dfs.create(getPath(), getPermission(), getFlags(),\n          getBufferSize(), getReplication(), getBlockSize(),\n          getProgress(), getChecksumOpt(), getFavoredNodes());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java"
    }
  }
}