{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "TableMapping.java",
  "functionName": "load",
  "functionId": "load",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/TableMapping.java",
  "functionStartLine": 92,
  "functionEndLine": 124,
  "numCommitsSeen": 9,
  "timeTaken": 1729,
  "changeHistory": [
    "7a3188d054481b9bd563e337901e93476303ce7f",
    "770b5eb2db686275df445be9280e76cc3710ffdc",
    "5b9fcedb4d116d91d70aaad6cbf59093eeee36df",
    "64741f46352f25743bfb77f804a06970d355a177",
    "1ff0359aa08bcb432b3bdfb5c91160cc4a303ea7"
  ],
  "changeHistoryShort": {
    "7a3188d054481b9bd563e337901e93476303ce7f": "Ybodychange",
    "770b5eb2db686275df445be9280e76cc3710ffdc": "Ybodychange",
    "5b9fcedb4d116d91d70aaad6cbf59093eeee36df": "Ybodychange",
    "64741f46352f25743bfb77f804a06970d355a177": "Ymultichange(Yreturntypechange,Ymodifierchange,Ybodychange)",
    "1ff0359aa08bcb432b3bdfb5c91160cc4a303ea7": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7a3188d054481b9bd563e337901e93476303ce7f": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16282. Avoid FileStream to improve performance. Contributed by Ayush Saxena.\n",
      "commitDate": "02/05/19 12:58 PM",
      "commitName": "7a3188d054481b9bd563e337901e93476303ce7f",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "27/06/18 10:37 PM",
      "commitNameOld": "2b2399d623539ab68e71a38fa9fbfc9a405bddb8",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 308.6,
      "commitsBetweenForRepo": 2368,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,33 @@\n     private Map\u003cString, String\u003e load() {\n       Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n   \n       String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n       if (StringUtils.isBlank(filename)) {\n         LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n         return null;\n       }\n   \n \n       try (BufferedReader reader \u003d\n                new BufferedReader(new InputStreamReader(\n-                   new FileInputStream(filename), StandardCharsets.UTF_8))) {\n+              Files.newInputStream(Paths.get(filename)),\n+              StandardCharsets.UTF_8))) {\n         String line \u003d reader.readLine();\n         while (line !\u003d null) {\n           line \u003d line.trim();\n           if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n             String[] columns \u003d line.split(\"\\\\s+\");\n             if (columns.length \u003d\u003d 2) {\n               loadMap.put(columns[0], columns[1]);\n             } else {\n               LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n             }\n           }\n           line \u003d reader.readLine();\n         }\n       } catch (Exception e) {\n         LOG.warn(filename + \" cannot be read.\", e);\n         return null;\n       }\n       return loadMap;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private Map\u003cString, String\u003e load() {\n      Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n  \n      String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n      if (StringUtils.isBlank(filename)) {\n        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n        return null;\n      }\n  \n\n      try (BufferedReader reader \u003d\n               new BufferedReader(new InputStreamReader(\n              Files.newInputStream(Paths.get(filename)),\n              StandardCharsets.UTF_8))) {\n        String line \u003d reader.readLine();\n        while (line !\u003d null) {\n          line \u003d line.trim();\n          if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n            String[] columns \u003d line.split(\"\\\\s+\");\n            if (columns.length \u003d\u003d 2) {\n              loadMap.put(columns[0], columns[1]);\n            } else {\n              LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n            }\n          }\n          line \u003d reader.readLine();\n        }\n      } catch (Exception e) {\n        LOG.warn(filename + \" cannot be read.\", e);\n        return null;\n      }\n      return loadMap;\n    }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/TableMapping.java",
      "extendedDetails": {}
    },
    "770b5eb2db686275df445be9280e76cc3710ffdc": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13444. Replace org.apache.commons.io.Charsets with java.nio.charset.StandardCharsets. Contributed by Vincent Poon.\n",
      "commitDate": "01/08/16 1:35 AM",
      "commitName": "770b5eb2db686275df445be9280e76cc3710ffdc",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "11/12/14 4:42 PM",
      "commitNameOld": "5b9fcedb4d116d91d70aaad6cbf59093eeee36df",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 598.33,
      "commitsBetweenForRepo": 4564,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,32 @@\n     private Map\u003cString, String\u003e load() {\n       Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n   \n       String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n       if (StringUtils.isBlank(filename)) {\n         LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n         return null;\n       }\n   \n \n       try (BufferedReader reader \u003d\n                new BufferedReader(new InputStreamReader(\n-                   new FileInputStream(filename), Charsets.UTF_8))) {\n+                   new FileInputStream(filename), StandardCharsets.UTF_8))) {\n         String line \u003d reader.readLine();\n         while (line !\u003d null) {\n           line \u003d line.trim();\n           if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n             String[] columns \u003d line.split(\"\\\\s+\");\n             if (columns.length \u003d\u003d 2) {\n               loadMap.put(columns[0], columns[1]);\n             } else {\n               LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n             }\n           }\n           line \u003d reader.readLine();\n         }\n       } catch (Exception e) {\n         LOG.warn(filename + \" cannot be read.\", e);\n         return null;\n       }\n       return loadMap;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private Map\u003cString, String\u003e load() {\n      Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n  \n      String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n      if (StringUtils.isBlank(filename)) {\n        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n        return null;\n      }\n  \n\n      try (BufferedReader reader \u003d\n               new BufferedReader(new InputStreamReader(\n                   new FileInputStream(filename), StandardCharsets.UTF_8))) {\n        String line \u003d reader.readLine();\n        while (line !\u003d null) {\n          line \u003d line.trim();\n          if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n            String[] columns \u003d line.split(\"\\\\s+\");\n            if (columns.length \u003d\u003d 2) {\n              loadMap.put(columns[0], columns[1]);\n            } else {\n              LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n            }\n          }\n          line \u003d reader.readLine();\n        }\n      } catch (Exception e) {\n        LOG.warn(filename + \" cannot be read.\", e);\n        return null;\n      }\n      return loadMap;\n    }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/TableMapping.java",
      "extendedDetails": {}
    },
    "5b9fcedb4d116d91d70aaad6cbf59093eeee36df": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11389. Clean up byte to string encoding issues in hadoop-common. Contributed by Haohui Mai.\n",
      "commitDate": "11/12/14 4:42 PM",
      "commitName": "5b9fcedb4d116d91d70aaad6cbf59093eeee36df",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/13 8:24 AM",
      "commitNameOld": "8a66e493ba03f710b353638647013401d18f413c",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 441.39,
      "commitsBetweenForRepo": 3399,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,32 @@\n     private Map\u003cString, String\u003e load() {\n       Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n   \n       String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n       if (StringUtils.isBlank(filename)) {\n         LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n         return null;\n       }\n   \n-      BufferedReader reader \u003d null;\n-      try {\n-        reader \u003d new BufferedReader(new FileReader(filename));\n+\n+      try (BufferedReader reader \u003d\n+               new BufferedReader(new InputStreamReader(\n+                   new FileInputStream(filename), Charsets.UTF_8))) {\n         String line \u003d reader.readLine();\n         while (line !\u003d null) {\n           line \u003d line.trim();\n           if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n             String[] columns \u003d line.split(\"\\\\s+\");\n             if (columns.length \u003d\u003d 2) {\n               loadMap.put(columns[0], columns[1]);\n             } else {\n               LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n             }\n           }\n           line \u003d reader.readLine();\n         }\n       } catch (Exception e) {\n         LOG.warn(filename + \" cannot be read.\", e);\n         return null;\n-      } finally {\n-        if (reader !\u003d null) {\n-          try {\n-            reader.close();\n-          } catch (IOException e) {\n-            LOG.warn(filename + \" cannot be read.\", e);\n-            return null;\n-          }\n-        }\n       }\n       return loadMap;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private Map\u003cString, String\u003e load() {\n      Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n  \n      String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n      if (StringUtils.isBlank(filename)) {\n        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n        return null;\n      }\n  \n\n      try (BufferedReader reader \u003d\n               new BufferedReader(new InputStreamReader(\n                   new FileInputStream(filename), Charsets.UTF_8))) {\n        String line \u003d reader.readLine();\n        while (line !\u003d null) {\n          line \u003d line.trim();\n          if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n            String[] columns \u003d line.split(\"\\\\s+\");\n            if (columns.length \u003d\u003d 2) {\n              loadMap.put(columns[0], columns[1]);\n            } else {\n              LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n            }\n          }\n          line \u003d reader.readLine();\n        }\n      } catch (Exception e) {\n        LOG.warn(filename + \" cannot be read.\", e);\n        return null;\n      }\n      return loadMap;\n    }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/TableMapping.java",
      "extendedDetails": {}
    },
    "64741f46352f25743bfb77f804a06970d355a177": {
      "type": "Ymultichange(Yreturntypechange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-4521. Invalid network toploogies should not be cached. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1457878 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/03/13 10:20 AM",
      "commitName": "64741f46352f25743bfb77f804a06970d355a177",
      "commitAuthor": "Aaron Myers",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-4521. Invalid network toploogies should not be cached. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1457878 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/03/13 10:20 AM",
          "commitName": "64741f46352f25743bfb77f804a06970d355a177",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "24/08/12 6:03 PM",
          "commitNameOld": "deead78e35b0cb81af875b5a8032cbd06c9a2dae",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 205.68,
          "commitsBetweenForRepo": 1009,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,40 @@\n-    private synchronized void load() {\n-      map.clear();\n+    private Map\u003cString, String\u003e load() {\n+      Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n   \n       String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n       if (StringUtils.isBlank(filename)) {\n-        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \"\n-            + NetworkTopology.DEFAULT_RACK + \" will be returned.\");\n-        return;\n+        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n+        return null;\n       }\n   \n       BufferedReader reader \u003d null;\n       try {\n         reader \u003d new BufferedReader(new FileReader(filename));\n         String line \u003d reader.readLine();\n         while (line !\u003d null) {\n           line \u003d line.trim();\n           if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n             String[] columns \u003d line.split(\"\\\\s+\");\n             if (columns.length \u003d\u003d 2) {\n-              map.put(columns[0], columns[1]);\n+              loadMap.put(columns[0], columns[1]);\n             } else {\n               LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n             }\n           }\n           line \u003d reader.readLine();\n         }\n       } catch (Exception e) {\n-        LOG.warn(filename + \" cannot be read. \" + NetworkTopology.DEFAULT_RACK\n-            + \" will be returned.\", e);\n-        map.clear();\n+        LOG.warn(filename + \" cannot be read.\", e);\n+        return null;\n       } finally {\n         if (reader !\u003d null) {\n           try {\n             reader.close();\n           } catch (IOException e) {\n-            LOG.warn(filename + \" cannot be read. \"\n-                + NetworkTopology.DEFAULT_RACK + \" will be returned.\", e);\n-            map.clear();\n+            LOG.warn(filename + \" cannot be read.\", e);\n+            return null;\n           }\n         }\n       }\n+      return loadMap;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private Map\u003cString, String\u003e load() {\n      Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n  \n      String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n      if (StringUtils.isBlank(filename)) {\n        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n        return null;\n      }\n  \n      BufferedReader reader \u003d null;\n      try {\n        reader \u003d new BufferedReader(new FileReader(filename));\n        String line \u003d reader.readLine();\n        while (line !\u003d null) {\n          line \u003d line.trim();\n          if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n            String[] columns \u003d line.split(\"\\\\s+\");\n            if (columns.length \u003d\u003d 2) {\n              loadMap.put(columns[0], columns[1]);\n            } else {\n              LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n            }\n          }\n          line \u003d reader.readLine();\n        }\n      } catch (Exception e) {\n        LOG.warn(filename + \" cannot be read.\", e);\n        return null;\n      } finally {\n        if (reader !\u003d null) {\n          try {\n            reader.close();\n          } catch (IOException e) {\n            LOG.warn(filename + \" cannot be read.\", e);\n            return null;\n          }\n        }\n      }\n      return loadMap;\n    }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/TableMapping.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "Map\u003cString,String\u003e"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-4521. Invalid network toploogies should not be cached. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1457878 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/03/13 10:20 AM",
          "commitName": "64741f46352f25743bfb77f804a06970d355a177",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "24/08/12 6:03 PM",
          "commitNameOld": "deead78e35b0cb81af875b5a8032cbd06c9a2dae",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 205.68,
          "commitsBetweenForRepo": 1009,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,40 @@\n-    private synchronized void load() {\n-      map.clear();\n+    private Map\u003cString, String\u003e load() {\n+      Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n   \n       String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n       if (StringUtils.isBlank(filename)) {\n-        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \"\n-            + NetworkTopology.DEFAULT_RACK + \" will be returned.\");\n-        return;\n+        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n+        return null;\n       }\n   \n       BufferedReader reader \u003d null;\n       try {\n         reader \u003d new BufferedReader(new FileReader(filename));\n         String line \u003d reader.readLine();\n         while (line !\u003d null) {\n           line \u003d line.trim();\n           if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n             String[] columns \u003d line.split(\"\\\\s+\");\n             if (columns.length \u003d\u003d 2) {\n-              map.put(columns[0], columns[1]);\n+              loadMap.put(columns[0], columns[1]);\n             } else {\n               LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n             }\n           }\n           line \u003d reader.readLine();\n         }\n       } catch (Exception e) {\n-        LOG.warn(filename + \" cannot be read. \" + NetworkTopology.DEFAULT_RACK\n-            + \" will be returned.\", e);\n-        map.clear();\n+        LOG.warn(filename + \" cannot be read.\", e);\n+        return null;\n       } finally {\n         if (reader !\u003d null) {\n           try {\n             reader.close();\n           } catch (IOException e) {\n-            LOG.warn(filename + \" cannot be read. \"\n-                + NetworkTopology.DEFAULT_RACK + \" will be returned.\", e);\n-            map.clear();\n+            LOG.warn(filename + \" cannot be read.\", e);\n+            return null;\n           }\n         }\n       }\n+      return loadMap;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private Map\u003cString, String\u003e load() {\n      Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n  \n      String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n      if (StringUtils.isBlank(filename)) {\n        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n        return null;\n      }\n  \n      BufferedReader reader \u003d null;\n      try {\n        reader \u003d new BufferedReader(new FileReader(filename));\n        String line \u003d reader.readLine();\n        while (line !\u003d null) {\n          line \u003d line.trim();\n          if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n            String[] columns \u003d line.split(\"\\\\s+\");\n            if (columns.length \u003d\u003d 2) {\n              loadMap.put(columns[0], columns[1]);\n            } else {\n              LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n            }\n          }\n          line \u003d reader.readLine();\n        }\n      } catch (Exception e) {\n        LOG.warn(filename + \" cannot be read.\", e);\n        return null;\n      } finally {\n        if (reader !\u003d null) {\n          try {\n            reader.close();\n          } catch (IOException e) {\n            LOG.warn(filename + \" cannot be read.\", e);\n            return null;\n          }\n        }\n      }\n      return loadMap;\n    }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/TableMapping.java",
          "extendedDetails": {
            "oldValue": "[private, synchronized]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4521. Invalid network toploogies should not be cached. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1457878 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/03/13 10:20 AM",
          "commitName": "64741f46352f25743bfb77f804a06970d355a177",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "24/08/12 6:03 PM",
          "commitNameOld": "deead78e35b0cb81af875b5a8032cbd06c9a2dae",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 205.68,
          "commitsBetweenForRepo": 1009,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,40 @@\n-    private synchronized void load() {\n-      map.clear();\n+    private Map\u003cString, String\u003e load() {\n+      Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n   \n       String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n       if (StringUtils.isBlank(filename)) {\n-        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \"\n-            + NetworkTopology.DEFAULT_RACK + \" will be returned.\");\n-        return;\n+        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n+        return null;\n       }\n   \n       BufferedReader reader \u003d null;\n       try {\n         reader \u003d new BufferedReader(new FileReader(filename));\n         String line \u003d reader.readLine();\n         while (line !\u003d null) {\n           line \u003d line.trim();\n           if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n             String[] columns \u003d line.split(\"\\\\s+\");\n             if (columns.length \u003d\u003d 2) {\n-              map.put(columns[0], columns[1]);\n+              loadMap.put(columns[0], columns[1]);\n             } else {\n               LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n             }\n           }\n           line \u003d reader.readLine();\n         }\n       } catch (Exception e) {\n-        LOG.warn(filename + \" cannot be read. \" + NetworkTopology.DEFAULT_RACK\n-            + \" will be returned.\", e);\n-        map.clear();\n+        LOG.warn(filename + \" cannot be read.\", e);\n+        return null;\n       } finally {\n         if (reader !\u003d null) {\n           try {\n             reader.close();\n           } catch (IOException e) {\n-            LOG.warn(filename + \" cannot be read. \"\n-                + NetworkTopology.DEFAULT_RACK + \" will be returned.\", e);\n-            map.clear();\n+            LOG.warn(filename + \" cannot be read.\", e);\n+            return null;\n           }\n         }\n       }\n+      return loadMap;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private Map\u003cString, String\u003e load() {\n      Map\u003cString, String\u003e loadMap \u003d new HashMap\u003cString, String\u003e();\n  \n      String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n      if (StringUtils.isBlank(filename)) {\n        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \");\n        return null;\n      }\n  \n      BufferedReader reader \u003d null;\n      try {\n        reader \u003d new BufferedReader(new FileReader(filename));\n        String line \u003d reader.readLine();\n        while (line !\u003d null) {\n          line \u003d line.trim();\n          if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n            String[] columns \u003d line.split(\"\\\\s+\");\n            if (columns.length \u003d\u003d 2) {\n              loadMap.put(columns[0], columns[1]);\n            } else {\n              LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n            }\n          }\n          line \u003d reader.readLine();\n        }\n      } catch (Exception e) {\n        LOG.warn(filename + \" cannot be read.\", e);\n        return null;\n      } finally {\n        if (reader !\u003d null) {\n          try {\n            reader.close();\n          } catch (IOException e) {\n            LOG.warn(filename + \" cannot be read.\", e);\n            return null;\n          }\n        }\n      }\n      return loadMap;\n    }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/TableMapping.java",
          "extendedDetails": {}
        }
      ]
    },
    "1ff0359aa08bcb432b3bdfb5c91160cc4a303ea7": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7030. Add TableMapping topology implementation to read host to rack mapping from a file. Contributed by Patrick Angeles and tomwhite.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1304597 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/03/12 2:03 PM",
      "commitName": "1ff0359aa08bcb432b3bdfb5c91160cc4a303ea7",
      "commitAuthor": "Thomas White",
      "diff": "@@ -0,0 +1,42 @@\n+    private synchronized void load() {\n+      map.clear();\n+  \n+      String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n+      if (StringUtils.isBlank(filename)) {\n+        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \"\n+            + NetworkTopology.DEFAULT_RACK + \" will be returned.\");\n+        return;\n+      }\n+  \n+      BufferedReader reader \u003d null;\n+      try {\n+        reader \u003d new BufferedReader(new FileReader(filename));\n+        String line \u003d reader.readLine();\n+        while (line !\u003d null) {\n+          line \u003d line.trim();\n+          if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n+            String[] columns \u003d line.split(\"\\\\s+\");\n+            if (columns.length \u003d\u003d 2) {\n+              map.put(columns[0], columns[1]);\n+            } else {\n+              LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n+            }\n+          }\n+          line \u003d reader.readLine();\n+        }\n+      } catch (Exception e) {\n+        LOG.warn(filename + \" cannot be read. \" + NetworkTopology.DEFAULT_RACK\n+            + \" will be returned.\", e);\n+        map.clear();\n+      } finally {\n+        if (reader !\u003d null) {\n+          try {\n+            reader.close();\n+          } catch (IOException e) {\n+            LOG.warn(filename + \" cannot be read. \"\n+                + NetworkTopology.DEFAULT_RACK + \" will be returned.\", e);\n+            map.clear();\n+          }\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private synchronized void load() {\n      map.clear();\n  \n      String filename \u003d getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);\n      if (StringUtils.isBlank(filename)) {\n        LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + \" not configured. \"\n            + NetworkTopology.DEFAULT_RACK + \" will be returned.\");\n        return;\n      }\n  \n      BufferedReader reader \u003d null;\n      try {\n        reader \u003d new BufferedReader(new FileReader(filename));\n        String line \u003d reader.readLine();\n        while (line !\u003d null) {\n          line \u003d line.trim();\n          if (line.length() !\u003d 0 \u0026\u0026 line.charAt(0) !\u003d \u0027#\u0027) {\n            String[] columns \u003d line.split(\"\\\\s+\");\n            if (columns.length \u003d\u003d 2) {\n              map.put(columns[0], columns[1]);\n            } else {\n              LOG.warn(\"Line does not have two columns. Ignoring. \" + line);\n            }\n          }\n          line \u003d reader.readLine();\n        }\n      } catch (Exception e) {\n        LOG.warn(filename + \" cannot be read. \" + NetworkTopology.DEFAULT_RACK\n            + \" will be returned.\", e);\n        map.clear();\n      } finally {\n        if (reader !\u003d null) {\n          try {\n            reader.close();\n          } catch (IOException e) {\n            LOG.warn(filename + \" cannot be read. \"\n                + NetworkTopology.DEFAULT_RACK + \" will be returned.\", e);\n            map.clear();\n          }\n        }\n      }\n    }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/TableMapping.java"
    }
  }
}