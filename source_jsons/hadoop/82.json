{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WriteCtx.java",
  "functionName": "writeData",
  "functionId": "writeData___fos-HdfsDataOutputStream",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
  "functionStartLine": 256,
  "functionEndLine": 283,
  "numCommitsSeen": 12,
  "timeTaken": 1070,
  "changeHistory": [
    "6e4562b844dfbbbdc0074323900eb69ee2a3e9c2",
    "d71d40a63d198991077d5babd70be5e9787a53f1",
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53"
  ],
  "changeHistoryShort": {
    "6e4562b844dfbbbdc0074323900eb69ee2a3e9c2": "Ybodychange",
    "d71d40a63d198991077d5babd70be5e9787a53f1": "Ybodychange",
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6e4562b844dfbbbdc0074323900eb69ee2a3e9c2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9245. Fix findbugs warnings in hdfs-nfs/WriteCtx. Contributed by Mingliang Liu.\n",
      "commitDate": "10/11/15 4:35 PM",
      "commitName": "6e4562b844dfbbbdc0074323900eb69ee2a3e9c2",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "28/09/15 6:45 PM",
      "commitNameOld": "151fca5032719e561226ef278e002739073c23ec",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 42.95,
      "commitsBetweenForRepo": 349,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   public void writeData(HdfsDataOutputStream fos) throws IOException {\n     Preconditions.checkState(fos !\u003d null);\n \n     ByteBuffer dataBuffer;\n     try {\n       dataBuffer \u003d getData();\n     } catch (Exception e1) {\n-      LOG.error(\"Failed to get request data offset:\" + offset + \" count:\"\n-          + count + \" error:\" + e1);\n+      LOG.error(\"Failed to get request data offset:\" + getPlainOffset() + \" \" +\n+          \"count:\" + count + \" error:\" + e1);\n       throw new IOException(\"Can\u0027t get WriteCtx.data\");\n     }\n \n     byte[] data \u003d dataBuffer.array();\n     int position \u003d dataBuffer.position();\n     int limit \u003d dataBuffer.limit();\n     Preconditions.checkState(limit - position \u003d\u003d count);\n     // Modified write has a valid original count\n     if (position !\u003d 0) {\n       if (limit !\u003d getOriginalCount()) {\n         throw new IOException(\"Modified write has differnt original size.\"\n             + \"buff position:\" + position + \" buff limit:\" + limit + \". \"\n             + toString());\n       }\n     }\n     \n     // Now write data\n     fos.write(data, position, count);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void writeData(HdfsDataOutputStream fos) throws IOException {\n    Preconditions.checkState(fos !\u003d null);\n\n    ByteBuffer dataBuffer;\n    try {\n      dataBuffer \u003d getData();\n    } catch (Exception e1) {\n      LOG.error(\"Failed to get request data offset:\" + getPlainOffset() + \" \" +\n          \"count:\" + count + \" error:\" + e1);\n      throw new IOException(\"Can\u0027t get WriteCtx.data\");\n    }\n\n    byte[] data \u003d dataBuffer.array();\n    int position \u003d dataBuffer.position();\n    int limit \u003d dataBuffer.limit();\n    Preconditions.checkState(limit - position \u003d\u003d count);\n    // Modified write has a valid original count\n    if (position !\u003d 0) {\n      if (limit !\u003d getOriginalCount()) {\n        throw new IOException(\"Modified write has differnt original size.\"\n            + \"buff position:\" + position + \" buff limit:\" + limit + \". \"\n            + toString());\n      }\n    }\n    \n    // Now write data\n    fos.write(data, position, count);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
      "extendedDetails": {}
    },
    "d71d40a63d198991077d5babd70be5e9787a53f1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7180. NFSv3 gateway frequently gets stuck due to GC. Contributed by Brandon Li\n",
      "commitDate": "22/10/14 9:27 PM",
      "commitName": "d71d40a63d198991077d5babd70be5e9787a53f1",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "21/10/14 10:20 AM",
      "commitNameOld": "b6f9d5538cf2b425652687e99503f3d566b2056a",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 1.46,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   public void writeData(HdfsDataOutputStream fos) throws IOException {\n     Preconditions.checkState(fos !\u003d null);\n \n-    ByteBuffer dataBuffer \u003d null;\n+    ByteBuffer dataBuffer;\n     try {\n       dataBuffer \u003d getData();\n     } catch (Exception e1) {\n       LOG.error(\"Failed to get request data offset:\" + offset + \" count:\"\n           + count + \" error:\" + e1);\n       throw new IOException(\"Can\u0027t get WriteCtx.data\");\n     }\n \n     byte[] data \u003d dataBuffer.array();\n     int position \u003d dataBuffer.position();\n     int limit \u003d dataBuffer.limit();\n     Preconditions.checkState(limit - position \u003d\u003d count);\n     // Modified write has a valid original count\n     if (position !\u003d 0) {\n       if (limit !\u003d getOriginalCount()) {\n         throw new IOException(\"Modified write has differnt original size.\"\n             + \"buff position:\" + position + \" buff limit:\" + limit + \". \"\n             + toString());\n       }\n     }\n     \n     // Now write data\n     fos.write(data, position, count);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void writeData(HdfsDataOutputStream fos) throws IOException {\n    Preconditions.checkState(fos !\u003d null);\n\n    ByteBuffer dataBuffer;\n    try {\n      dataBuffer \u003d getData();\n    } catch (Exception e1) {\n      LOG.error(\"Failed to get request data offset:\" + offset + \" count:\"\n          + count + \" error:\" + e1);\n      throw new IOException(\"Can\u0027t get WriteCtx.data\");\n    }\n\n    byte[] data \u003d dataBuffer.array();\n    int position \u003d dataBuffer.position();\n    int limit \u003d dataBuffer.limit();\n    Preconditions.checkState(limit - position \u003d\u003d count);\n    // Modified write has a valid original count\n    if (position !\u003d 0) {\n      if (limit !\u003d getOriginalCount()) {\n        throw new IOException(\"Modified write has differnt original size.\"\n            + \"buff position:\" + position + \" buff limit:\" + limit + \". \"\n            + toString());\n      }\n    }\n    \n    // Now write data\n    fos.write(data, position, count);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
      "extendedDetails": {}
    },
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5259. Support client which combines appended data with old data before sends it to NFS server. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529730 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/10/13 7:57 PM",
      "commitName": "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
      "commitAuthor": "Brandon Li",
      "diff": "@@ -0,0 +1,28 @@\n+  public void writeData(HdfsDataOutputStream fos) throws IOException {\n+    Preconditions.checkState(fos !\u003d null);\n+\n+    ByteBuffer dataBuffer \u003d null;\n+    try {\n+      dataBuffer \u003d getData();\n+    } catch (Exception e1) {\n+      LOG.error(\"Failed to get request data offset:\" + offset + \" count:\"\n+          + count + \" error:\" + e1);\n+      throw new IOException(\"Can\u0027t get WriteCtx.data\");\n+    }\n+\n+    byte[] data \u003d dataBuffer.array();\n+    int position \u003d dataBuffer.position();\n+    int limit \u003d dataBuffer.limit();\n+    Preconditions.checkState(limit - position \u003d\u003d count);\n+    // Modified write has a valid original count\n+    if (position !\u003d 0) {\n+      if (limit !\u003d getOriginalCount()) {\n+        throw new IOException(\"Modified write has differnt original size.\"\n+            + \"buff position:\" + position + \" buff limit:\" + limit + \". \"\n+            + toString());\n+      }\n+    }\n+    \n+    // Now write data\n+    fos.write(data, position, count);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void writeData(HdfsDataOutputStream fos) throws IOException {\n    Preconditions.checkState(fos !\u003d null);\n\n    ByteBuffer dataBuffer \u003d null;\n    try {\n      dataBuffer \u003d getData();\n    } catch (Exception e1) {\n      LOG.error(\"Failed to get request data offset:\" + offset + \" count:\"\n          + count + \" error:\" + e1);\n      throw new IOException(\"Can\u0027t get WriteCtx.data\");\n    }\n\n    byte[] data \u003d dataBuffer.array();\n    int position \u003d dataBuffer.position();\n    int limit \u003d dataBuffer.limit();\n    Preconditions.checkState(limit - position \u003d\u003d count);\n    // Modified write has a valid original count\n    if (position !\u003d 0) {\n      if (limit !\u003d getOriginalCount()) {\n        throw new IOException(\"Modified write has differnt original size.\"\n            + \"buff position:\" + position + \" buff limit:\" + limit + \". \"\n            + toString());\n      }\n    }\n    \n    // Now write data\n    fos.write(data, position, count);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java"
    }
  }
}