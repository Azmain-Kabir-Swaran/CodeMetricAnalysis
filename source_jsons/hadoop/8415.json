{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirDeleteOp.java",
  "functionName": "delete",
  "functionId": "delete___fsn-FSNamesystem__pc-FSPermissionChecker__src-String__recursive-boolean__logRetryCache-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
  "functionStartLine": 94,
  "functionEndLine": 117,
  "numCommitsSeen": 749,
  "timeTaken": 13747,
  "changeHistory": [
    "67f2c491fe3cd400605fb6082fd3504bc5e97037",
    "84a1321f6aa0af6895564a7c47f8f264656f0294",
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
    "3565c9af17ab05bf9e7f68b71b6c6850df772bb9",
    "869393643de23dcb010cc33091c8eb398de0fd6c",
    "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2",
    "bdbe53c676dd4ff135ea2f64d3b9193fe43d7c8e",
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
    "4a3161182905afaf450a60d02528161ed1f97471",
    "76a621ffd2d66bf012a554f4400091a92a5b473e",
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d",
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
    "0689363343a281a6f7f6f395227668bddc2663eb",
    "4da6de1ca3a1aaca6c80b16318340cfdcd3cea07",
    "8c7a7e619699386f9e6991842558d78aa0c8053d"
  ],
  "changeHistoryShort": {
    "67f2c491fe3cd400605fb6082fd3504bc5e97037": "Ybodychange",
    "84a1321f6aa0af6895564a7c47f8f264656f0294": "Ymultichange(Yparameterchange,Ybodychange)",
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04": "Ybodychange",
    "3565c9af17ab05bf9e7f68b71b6c6850df772bb9": "Ybodychange",
    "869393643de23dcb010cc33091c8eb398de0fd6c": "Ybodychange",
    "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2": "Ybodychange",
    "bdbe53c676dd4ff135ea2f64d3b9193fe43d7c8e": "Ybodychange",
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": "Ymultichange(Yparameterchange,Ybodychange)",
    "4a3161182905afaf450a60d02528161ed1f97471": "Ybodychange",
    "76a621ffd2d66bf012a554f4400091a92a5b473e": "Ybodychange",
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d": "Ybodychange",
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "0689363343a281a6f7f6f395227668bddc2663eb": "Ybodychange",
    "4da6de1ca3a1aaca6c80b16318340cfdcd3cea07": "Ybodychange",
    "8c7a7e619699386f9e6991842558d78aa0c8053d": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "67f2c491fe3cd400605fb6082fd3504bc5e97037": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14802. The feature of protect directories should be used in RenameOp (#1669)\n\n",
      "commitDate": "15/11/19 12:35 PM",
      "commitName": "67f2c491fe3cd400605fb6082fd3504bc5e97037",
      "commitAuthor": "Hui Fei",
      "commitDateOld": "22/02/18 11:32 AM",
      "commitNameOld": "84a1321f6aa0af6895564a7c47f8f264656f0294",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 631.04,
      "commitsBetweenForRepo": 5337,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   static BlocksMapUpdateInfo delete(\n       FSNamesystem fsn, FSPermissionChecker pc, String src, boolean recursive,\n       boolean logRetryCache) throws IOException {\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n \n     if (FSDirectory.isExactReservedName(src)) {\n       throw new InvalidPathException(src);\n     }\n \n     final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n     if (fsd.isPermissionEnabled()) {\n       fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                           FsAction.ALL, true);\n     }\n     if (fsd.isNonEmptyDirectory(iip)) {\n       if (!recursive) {\n         throw new PathIsNotEmptyDirectoryException(\n             iip.getPath() + \" is non empty\");\n       }\n-      checkProtectedDescendants(fsd, iip);\n+      DFSUtil.checkProtectedDescendants(fsd, iip);\n     }\n \n     return deleteInternal(fsn, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, FSPermissionChecker pc, String src, boolean recursive,\n      boolean logRetryCache) throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n\n    if (FSDirectory.isExactReservedName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n    if (fsd.isNonEmptyDirectory(iip)) {\n      if (!recursive) {\n        throw new PathIsNotEmptyDirectoryException(\n            iip.getPath() + \" is non empty\");\n      }\n      DFSUtil.checkProtectedDescendants(fsd, iip);\n    }\n\n    return deleteInternal(fsn, iip, logRetryCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "84a1321f6aa0af6895564a7c47f8f264656f0294": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
      "commitDate": "22/02/18 11:32 AM",
      "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
      "commitAuthor": "Xiaoyu Yao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
          "commitDate": "22/02/18 11:32 AM",
          "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
          "commitAuthor": "Xiaoyu Yao",
          "commitDateOld": "24/10/16 3:14 PM",
          "commitNameOld": "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 485.89,
          "commitsBetweenForRepo": 3146,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,24 @@\n   static BlocksMapUpdateInfo delete(\n-      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n-      throws IOException {\n+      FSNamesystem fsn, FSPermissionChecker pc, String src, boolean recursive,\n+      boolean logRetryCache) throws IOException {\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n-    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n \n     if (FSDirectory.isExactReservedName(src)) {\n       throw new InvalidPathException(src);\n     }\n \n     final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n     if (fsd.isPermissionEnabled()) {\n       fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                           FsAction.ALL, true);\n     }\n     if (fsd.isNonEmptyDirectory(iip)) {\n       if (!recursive) {\n         throw new PathIsNotEmptyDirectoryException(\n             iip.getPath() + \" is non empty\");\n       }\n       checkProtectedDescendants(fsd, iip);\n     }\n \n     return deleteInternal(fsn, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, FSPermissionChecker pc, String src, boolean recursive,\n      boolean logRetryCache) throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n\n    if (FSDirectory.isExactReservedName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n    if (fsd.isNonEmptyDirectory(iip)) {\n      if (!recursive) {\n        throw new PathIsNotEmptyDirectoryException(\n            iip.getPath() + \" is non empty\");\n      }\n      checkProtectedDescendants(fsd, iip);\n    }\n\n    return deleteInternal(fsn, iip, logRetryCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "[fsn-FSNamesystem, src-String, recursive-boolean, logRetryCache-boolean]",
            "newValue": "[fsn-FSNamesystem, pc-FSPermissionChecker, src-String, recursive-boolean, logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
          "commitDate": "22/02/18 11:32 AM",
          "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
          "commitAuthor": "Xiaoyu Yao",
          "commitDateOld": "24/10/16 3:14 PM",
          "commitNameOld": "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 485.89,
          "commitsBetweenForRepo": 3146,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,24 @@\n   static BlocksMapUpdateInfo delete(\n-      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n-      throws IOException {\n+      FSNamesystem fsn, FSPermissionChecker pc, String src, boolean recursive,\n+      boolean logRetryCache) throws IOException {\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n-    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n \n     if (FSDirectory.isExactReservedName(src)) {\n       throw new InvalidPathException(src);\n     }\n \n     final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n     if (fsd.isPermissionEnabled()) {\n       fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                           FsAction.ALL, true);\n     }\n     if (fsd.isNonEmptyDirectory(iip)) {\n       if (!recursive) {\n         throw new PathIsNotEmptyDirectoryException(\n             iip.getPath() + \" is non empty\");\n       }\n       checkProtectedDescendants(fsd, iip);\n     }\n \n     return deleteInternal(fsn, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, FSPermissionChecker pc, String src, boolean recursive,\n      boolean logRetryCache) throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n\n    if (FSDirectory.isExactReservedName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n    if (fsd.isNonEmptyDirectory(iip)) {\n      if (!recursive) {\n        throw new PathIsNotEmptyDirectoryException(\n            iip.getPath() + \" is non empty\");\n      }\n      checkProtectedDescendants(fsd, iip);\n    }\n\n    return deleteInternal(fsn, iip, logRetryCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10997. Reduce number of path resolving methods. Contributed by Daryn Sharp.\n",
      "commitDate": "24/10/16 3:14 PM",
      "commitName": "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "07/10/16 12:15 PM",
      "commitNameOld": "3565c9af17ab05bf9e7f68b71b6c6850df772bb9",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 17.12,
      "commitsBetweenForRepo": 111,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   static BlocksMapUpdateInfo delete(\n       FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n \n     if (FSDirectory.isExactReservedName(src)) {\n       throw new InvalidPathException(src);\n     }\n \n-    final INodesInPath iip \u003d fsd.resolvePathForWrite(pc, src, false);\n+    final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n     if (fsd.isPermissionEnabled()) {\n       fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                           FsAction.ALL, true);\n     }\n     if (fsd.isNonEmptyDirectory(iip)) {\n       if (!recursive) {\n         throw new PathIsNotEmptyDirectoryException(\n             iip.getPath() + \" is non empty\");\n       }\n       checkProtectedDescendants(fsd, iip);\n     }\n \n     return deleteInternal(fsn, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n\n    if (FSDirectory.isExactReservedName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n    if (fsd.isNonEmptyDirectory(iip)) {\n      if (!recursive) {\n        throw new PathIsNotEmptyDirectoryException(\n            iip.getPath() + \" is non empty\");\n      }\n      checkProtectedDescendants(fsd, iip);\n    }\n\n    return deleteInternal(fsn, iip, logRetryCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "3565c9af17ab05bf9e7f68b71b6c6850df772bb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10979. Pass IIP for FSDirDeleteOp methods. Contributed by Daryn Sharp.\n",
      "commitDate": "07/10/16 12:15 PM",
      "commitName": "3565c9af17ab05bf9e7f68b71b6c6850df772bb9",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "04/10/16 1:05 PM",
      "commitNameOld": "44f48ee96ee6b2a3909911c37bfddb0c963d5ffc",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 2.97,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,25 @@\n   static BlocksMapUpdateInfo delete(\n       FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n \n-    final INodesInPath iip \u003d fsd.resolvePathForWrite(pc, src, false);\n-    src \u003d iip.getPath();\n-    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n-      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n+    if (FSDirectory.isExactReservedName(src)) {\n+      throw new InvalidPathException(src);\n     }\n+\n+    final INodesInPath iip \u003d fsd.resolvePathForWrite(pc, src, false);\n     if (fsd.isPermissionEnabled()) {\n       fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                           FsAction.ALL, true);\n     }\n-    if (recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n-      checkProtectedDescendants(fsd, src);\n+    if (fsd.isNonEmptyDirectory(iip)) {\n+      if (!recursive) {\n+        throw new PathIsNotEmptyDirectoryException(\n+            iip.getPath() + \" is non empty\");\n+      }\n+      checkProtectedDescendants(fsd, iip);\n     }\n \n-    return deleteInternal(fsn, src, iip, logRetryCache);\n+    return deleteInternal(fsn, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n\n    if (FSDirectory.isExactReservedName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    final INodesInPath iip \u003d fsd.resolvePathForWrite(pc, src, false);\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n    if (fsd.isNonEmptyDirectory(iip)) {\n      if (!recursive) {\n        throw new PathIsNotEmptyDirectoryException(\n            iip.getPath() + \" is non empty\");\n      }\n      checkProtectedDescendants(fsd, iip);\n    }\n\n    return deleteInternal(fsn, iip, logRetryCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "869393643de23dcb010cc33091c8eb398de0fd6c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10745. Directly resolve paths into INodesInPath. Contributed by Daryn Sharp.\n",
      "commitDate": "17/08/16 1:53 PM",
      "commitName": "869393643de23dcb010cc33091c8eb398de0fd6c",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "15/08/16 2:45 PM",
      "commitNameOld": "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 1.96,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   static BlocksMapUpdateInfo delete(\n       FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n \n-    src \u003d fsd.resolvePath(pc, src);\n-    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n+    final INodesInPath iip \u003d fsd.resolvePathForWrite(pc, src, false);\n+    src \u003d iip.getPath();\n     if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n       throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n     }\n     if (fsd.isPermissionEnabled()) {\n       fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                           FsAction.ALL, true);\n     }\n     if (recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n       checkProtectedDescendants(fsd, src);\n     }\n \n     return deleteInternal(fsn, src, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n\n    final INodesInPath iip \u003d fsd.resolvePathForWrite(pc, src, false);\n    src \u003d iip.getPath();\n    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n    }\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n    if (recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n      checkProtectedDescendants(fsd, src);\n    }\n\n    return deleteInternal(fsn, src, iip, logRetryCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10744. Internally optimize path component resolution. Contributed by Daryn Sharp.\n",
      "commitDate": "15/08/16 2:45 PM",
      "commitName": "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "01/12/15 4:09 PM",
      "commitNameOld": "a49cc74b4c72195dee1dfb6f9548e5e411dff553",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 257.9,
      "commitsBetweenForRepo": 1769,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,21 @@\n   static BlocksMapUpdateInfo delete(\n       FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n \n-    src \u003d fsd.resolvePath(pc, src, pathComponents);\n+    src \u003d fsd.resolvePath(pc, src);\n     final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n     if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n       throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n     }\n     if (fsd.isPermissionEnabled()) {\n       fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                           FsAction.ALL, true);\n     }\n     if (recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n-      checkProtectedDescendants(fsd, fsd.normalizePath(src));\n+      checkProtectedDescendants(fsd, src);\n     }\n \n     return deleteInternal(fsn, src, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n\n    src \u003d fsd.resolvePath(pc, src);\n    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n    }\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n    if (recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n      checkProtectedDescendants(fsd, src);\n    }\n\n    return deleteInternal(fsn, src, iip, logRetryCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "bdbe53c676dd4ff135ea2f64d3b9193fe43d7c8e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8983. NameNode support for protected directories. (Contributed by Arpit Agarwal)\n",
      "commitDate": "29/08/15 9:52 AM",
      "commitName": "bdbe53c676dd4ff135ea2f64d3b9193fe43d7c8e",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/08/15 12:09 AM",
      "commitNameOld": "745d04be59accf80feda0ad38efcc74ba362f2ca",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 7.4,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,22 @@\n   static BlocksMapUpdateInfo delete(\n       FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n \n     src \u003d fsd.resolvePath(pc, src, pathComponents);\n     final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n     if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n       throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n     }\n     if (fsd.isPermissionEnabled()) {\n       fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                           FsAction.ALL, true);\n     }\n+    if (recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n+      checkProtectedDescendants(fsd, fsd.normalizePath(src));\n+    }\n \n     return deleteInternal(fsn, src, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n\n    src \u003d fsd.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n    }\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n    if (recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n      checkProtectedDescendants(fsd, fsd.normalizePath(src));\n    }\n\n    return deleteInternal(fsn, src, iip, logRetryCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "17/01/15 12:56 PM",
      "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,19 @@\n-  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n-    if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n+  static BlocksMapUpdateInfo delete(\n+      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n+      throws IOException {\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n+\n+    src \u003d fsd.resolvePath(pc, src, pathComponents);\n+    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n+    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n+      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n     }\n-    final long filesRemoved;\n-    writeLock();\n-    try {\n-      if (!deleteAllowed(iip, iip.getPath()) ) {\n-        filesRemoved \u003d -1;\n-      } else {\n-        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n-        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n-            removedINodes, mtime);\n-        namesystem.removeSnapshottableDirs(snapshottableDirs);\n-      }\n-    } finally {\n-      writeUnlock();\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n+                          FsAction.ALL, true);\n     }\n-    return filesRemoved;\n+\n+    return deleteInternal(fsn, src, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n\n    src \u003d fsd.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n    }\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n\n    return deleteInternal(fsn, src, iip, logRetryCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
            "oldMethodName": "delete",
            "newMethodName": "delete"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,19 @@\n-  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n-    if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n+  static BlocksMapUpdateInfo delete(\n+      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n+      throws IOException {\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n+\n+    src \u003d fsd.resolvePath(pc, src, pathComponents);\n+    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n+    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n+      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n     }\n-    final long filesRemoved;\n-    writeLock();\n-    try {\n-      if (!deleteAllowed(iip, iip.getPath()) ) {\n-        filesRemoved \u003d -1;\n-      } else {\n-        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n-        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n-            removedINodes, mtime);\n-        namesystem.removeSnapshottableDirs(snapshottableDirs);\n-      }\n-    } finally {\n-      writeUnlock();\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n+                          FsAction.ALL, true);\n     }\n-    return filesRemoved;\n+\n+    return deleteInternal(fsn, src, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n\n    src \u003d fsd.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n    }\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n\n    return deleteInternal(fsn, src, iip, logRetryCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "long",
            "newValue": "BlocksMapUpdateInfo"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,19 @@\n-  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n-    if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n+  static BlocksMapUpdateInfo delete(\n+      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n+      throws IOException {\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n+\n+    src \u003d fsd.resolvePath(pc, src, pathComponents);\n+    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n+    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n+      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n     }\n-    final long filesRemoved;\n-    writeLock();\n-    try {\n-      if (!deleteAllowed(iip, iip.getPath()) ) {\n-        filesRemoved \u003d -1;\n-      } else {\n-        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n-        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n-            removedINodes, mtime);\n-        namesystem.removeSnapshottableDirs(snapshottableDirs);\n-      }\n-    } finally {\n-      writeUnlock();\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n+                          FsAction.ALL, true);\n     }\n-    return filesRemoved;\n+\n+    return deleteInternal(fsn, src, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n\n    src \u003d fsd.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n    }\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n\n    return deleteInternal(fsn, src, iip, logRetryCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,19 @@\n-  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n-    if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n+  static BlocksMapUpdateInfo delete(\n+      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n+      throws IOException {\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n+\n+    src \u003d fsd.resolvePath(pc, src, pathComponents);\n+    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n+    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n+      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n     }\n-    final long filesRemoved;\n-    writeLock();\n-    try {\n-      if (!deleteAllowed(iip, iip.getPath()) ) {\n-        filesRemoved \u003d -1;\n-      } else {\n-        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n-        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n-            removedINodes, mtime);\n-        namesystem.removeSnapshottableDirs(snapshottableDirs);\n-      }\n-    } finally {\n-      writeUnlock();\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n+                          FsAction.ALL, true);\n     }\n-    return filesRemoved;\n+\n+    return deleteInternal(fsn, src, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n\n    src \u003d fsd.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n    }\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n\n    return deleteInternal(fsn, src, iip, logRetryCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,19 @@\n-  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n-              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n-    if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n+  static BlocksMapUpdateInfo delete(\n+      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n+      throws IOException {\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n+\n+    src \u003d fsd.resolvePath(pc, src, pathComponents);\n+    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n+    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n+      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n     }\n-    final long filesRemoved;\n-    writeLock();\n-    try {\n-      if (!deleteAllowed(iip, iip.getPath()) ) {\n-        filesRemoved \u003d -1;\n-      } else {\n-        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n-        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n-            removedINodes, mtime);\n-        namesystem.removeSnapshottableDirs(snapshottableDirs);\n-      }\n-    } finally {\n-      writeUnlock();\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n+                          FsAction.ALL, true);\n     }\n-    return filesRemoved;\n+\n+    return deleteInternal(fsn, src, iip, logRetryCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo delete(\n      FSNamesystem fsn, String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n\n    src \u003d fsd.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip \u003d fsd.getINodesInPath4Write(src, false);\n    if (!recursive \u0026\u0026 fsd.isNonEmptyDirectory(iip)) {\n      throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n    }\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n                          FsAction.ALL, true);\n    }\n\n    return deleteInternal(fsn, src, iip, logRetryCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "[iip-INodesInPath, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, mtime-long]",
            "newValue": "[fsn-FSNamesystem, src-String, recursive-boolean, logRetryCache-boolean]"
          }
        }
      ]
    },
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
      "commitDate": "12/12/14 3:13 PM",
      "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
          "commitDate": "12/12/14 3:13 PM",
          "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "09/12/14 11:37 AM",
          "commitNameOld": "5776a41da08af653206bb94d7c76c9c4dcce059a",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 3.15,
          "commitsBetweenForRepo": 33,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,22 @@\n-  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n+  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n               List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     final long filesRemoved;\n     writeLock();\n     try {\n-      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n-          normalizePath(src), false);\n-      if (!deleteAllowed(inodesInPath, src) ) {\n+      if (!deleteAllowed(iip, iip.getPath()) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n-        FSDirSnapshotOp.checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n+        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n+        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n             removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    final long filesRemoved;\n    writeLock();\n    try {\n      if (!deleteAllowed(iip, iip.getPath()) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[src-String, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, mtime-long]",
            "newValue": "[iip-INodesInPath, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, mtime-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
          "commitDate": "12/12/14 3:13 PM",
          "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "09/12/14 11:37 AM",
          "commitNameOld": "5776a41da08af653206bb94d7c76c9c4dcce059a",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 3.15,
          "commitsBetweenForRepo": 33,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,22 @@\n-  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n+  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n               List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n     }\n     final long filesRemoved;\n     writeLock();\n     try {\n-      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n-          normalizePath(src), false);\n-      if (!deleteAllowed(inodesInPath, src) ) {\n+      if (!deleteAllowed(iip, iip.getPath()) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n-        FSDirSnapshotOp.checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n-        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n+        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n+        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n             removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long delete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + iip.getPath());\n    }\n    final long filesRemoved;\n    writeLock();\n    try {\n      if (!deleteAllowed(iip, iip.getPath()) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n        FSDirSnapshotOp.checkSnapshot(iip.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(iip, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    },
    "4a3161182905afaf450a60d02528161ed1f97471": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7440. Consolidate snapshot related operations in a single class. Contributed by Haohui Mai.\n",
      "commitDate": "25/11/14 9:04 PM",
      "commitName": "4a3161182905afaf450a60d02528161ed1f97471",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/11/14 3:42 PM",
      "commitNameOld": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.22,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n               List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n-        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n+        FSDirSnapshotOp.checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n        FSDirSnapshotOp.checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "76a621ffd2d66bf012a554f4400091a92a5b473e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6609. Use DirectorySnapshottableFeature to represent a snapshottable directory. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1608631 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/07/14 5:08 PM",
      "commitName": "76a621ffd2d66bf012a554f4400091a92a5b473e",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "23/06/14 11:56 PM",
      "commitNameOld": "08986fdbed5a15bcdc57d142922911759b97e9d1",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 13.72,
      "commitsBetweenForRepo": 88,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,24 @@\n   long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n               List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n-        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n-            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n+        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectory\u003e snapshottableDirs \u003d new ArrayList\u003cINodeDirectory\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6480. Move waitForReady() from FSDirectory to FSNamesystem. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603705 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/06/14 9:13 PM",
      "commitName": "a4e0ff5e052abad498595ee198b49c5310c9ec0d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "18/06/14 12:57 PM",
      "commitNameOld": "4cf94aaf809c77b3b7dc925faa39a72d53e4246e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.35,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,25 @@\n   long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n               List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n-    waitForReady();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     return filesRemoved;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-6315. Decouple recording edit logs from FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601960 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/06/14 10:22 AM",
      "commitName": "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6315. Decouple recording edit logs from FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601960 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/06/14 10:22 AM",
          "commitName": "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/05/14 6:57 AM",
          "commitNameOld": "ac23a55547716df29b3e25c98a113399e184d9d1",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 21.14,
          "commitsBetweenForRepo": 105,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,26 @@\n-  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n+  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n+              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n-    long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n-            removedINodes, now);\n+            removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n-    if (filesRemoved \u003c 0) {\n-      return false;\n-    }\n-    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n-    incrDeletedFileCount(filesRemoved);\n-    // Blocks/INodes will be handled later by the caller of this method\n-    getFSNamesystem().removePathAndBlocks(src, null, null);\n-    return true;\n+    return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[src-String, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, logRetryCache-boolean]",
            "newValue": "[src-String, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, mtime-long]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-6315. Decouple recording edit logs from FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601960 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/06/14 10:22 AM",
          "commitName": "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/05/14 6:57 AM",
          "commitNameOld": "ac23a55547716df29b3e25c98a113399e184d9d1",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 21.14,
          "commitsBetweenForRepo": 105,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,26 @@\n-  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n+  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n+              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n-    long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n-            removedINodes, now);\n+            removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n-    if (filesRemoved \u003c 0) {\n-      return false;\n-    }\n-    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n-    incrDeletedFileCount(filesRemoved);\n-    // Blocks/INodes will be handled later by the caller of this method\n-    getFSNamesystem().removePathAndBlocks(src, null, null);\n-    return true;\n+    return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "boolean",
            "newValue": "long"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6315. Decouple recording edit logs from FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601960 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/06/14 10:22 AM",
          "commitName": "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/05/14 6:57 AM",
          "commitNameOld": "ac23a55547716df29b3e25c98a113399e184d9d1",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 21.14,
          "commitsBetweenForRepo": 105,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,26 @@\n-  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n+  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n+              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n-    long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n-            removedINodes, now);\n+            removedINodes, mtime);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n-    if (filesRemoved \u003c 0) {\n-      return false;\n-    }\n-    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n-    incrDeletedFileCount(filesRemoved);\n-    // Blocks/INodes will be handled later by the caller of this method\n-    getFSNamesystem().removePathAndBlocks(src, null, null);\n-    return true;\n+    return filesRemoved;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long delete(String src, BlocksMapUpdateInfo collectedBlocks,\n              List\u003cINode\u003e removedINodes, long mtime) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, mtime);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    return filesRemoved;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    },
    "0689363343a281a6f7f6f395227668bddc2663eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6304. Consolidate the logic of path resolution in FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1591411 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/04/14 10:44 AM",
      "commitName": "0689363343a281a6f7f6f395227668bddc2663eb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/04/14 7:05 PM",
      "commitNameOld": "10a037cccb00c9f791da394bf2dc05985fb80612",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 5.65,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n       List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n     long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n-      final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n+      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, now);\n         namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     if (filesRemoved \u003c 0) {\n       return false;\n     }\n     fsImage.getEditLog().logDelete(src, now, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n     // Blocks/INodes will be handled later by the caller of this method\n     getFSNamesystem().removePathAndBlocks(src, null, null);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    long now \u003d now();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, now);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    if (filesRemoved \u003c 0) {\n      return false;\n    }\n    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n    // Blocks/INodes will be handled later by the caller of this method\n    getFSNamesystem().removePathAndBlocks(src, null, null);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "4da6de1ca3a1aaca6c80b16318340cfdcd3cea07": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5982. Need to update snapshot manager when applying editlog for deleting a snapshottable directory. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1570395 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/02/14 4:15 PM",
      "commitName": "4da6de1ca3a1aaca6c80b16318340cfdcd3cea07",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/02/14 2:54 PM",
      "commitNameOld": "fc14360b0340a33c0e1eb34967d4dcd772533418",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 8.06,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,34 @@\n   boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n       List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n     long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n-        // Before removing the node, first check if the targetNode is for a\n-        // snapshottable dir with snapshots, or its descendants have\n-        // snapshottable dir with snapshots\n-        final INode targetNode \u003d inodesInPath.getLastINode();\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n-        checkSnapshot(targetNode, snapshottableDirs);\n+        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, now);\n-        if (snapshottableDirs.size() \u003e 0) {\n-          // There are some snapshottable directories without snapshots to be\n-          // deleted. Need to update the SnapshotManager.\n-          namesystem.removeSnapshottableDirs(snapshottableDirs);\n-        }\n+        namesystem.removeSnapshottableDirs(snapshottableDirs);\n       }\n     } finally {\n       writeUnlock();\n     }\n     if (filesRemoved \u003c 0) {\n       return false;\n     }\n     fsImage.getEditLog().logDelete(src, now, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n     // Blocks/INodes will be handled later by the caller of this method\n     getFSNamesystem().removePathAndBlocks(src, null, null);\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    long now \u003d now();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(inodesInPath.getLastINode(), snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, now);\n        namesystem.removeSnapshottableDirs(snapshottableDirs);\n      }\n    } finally {\n      writeUnlock();\n    }\n    if (filesRemoved \u003c 0) {\n      return false;\n    }\n    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n    // Blocks/INodes will be handled later by the caller of this method\n    getFSNamesystem().removePathAndBlocks(src, null, null);\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "8c7a7e619699386f9e6991842558d78aa0c8053d": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/07/13 12:51 AM",
      "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/07/13 12:51 AM",
          "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "22/07/13 11:22 AM",
          "commitNameOld": "11c073134afc878619c37c95935d6a3098a21f17",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 7.56,
          "commitsBetweenForRepo": 56,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,42 @@\n   boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes) throws IOException {\n+      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n     long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         // Before removing the node, first check if the targetNode is for a\n         // snapshottable dir with snapshots, or its descendants have\n         // snapshottable dir with snapshots\n         final INode targetNode \u003d inodesInPath.getLastINode();\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(targetNode, snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, now);\n         if (snapshottableDirs.size() \u003e 0) {\n           // There are some snapshottable directories without snapshots to be\n           // deleted. Need to update the SnapshotManager.\n           namesystem.removeSnapshottableDirs(snapshottableDirs);\n         }\n       }\n     } finally {\n       writeUnlock();\n     }\n     if (filesRemoved \u003c 0) {\n       return false;\n     }\n-    fsImage.getEditLog().logDelete(src, now);\n+    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n     // Blocks/INodes will be handled later by the caller of this method\n     getFSNamesystem().removePathAndBlocks(src, null, null);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    long now \u003d now();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        // Before removing the node, first check if the targetNode is for a\n        // snapshottable dir with snapshots, or its descendants have\n        // snapshottable dir with snapshots\n        final INode targetNode \u003d inodesInPath.getLastINode();\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(targetNode, snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, now);\n        if (snapshottableDirs.size() \u003e 0) {\n          // There are some snapshottable directories without snapshots to be\n          // deleted. Need to update the SnapshotManager.\n          namesystem.removeSnapshottableDirs(snapshottableDirs);\n        }\n      }\n    } finally {\n      writeUnlock();\n    }\n    if (filesRemoved \u003c 0) {\n      return false;\n    }\n    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n    // Blocks/INodes will be handled later by the caller of this method\n    getFSNamesystem().removePathAndBlocks(src, null, null);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[src-String, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e]",
            "newValue": "[src-String, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e, logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/07/13 12:51 AM",
          "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "22/07/13 11:22 AM",
          "commitNameOld": "11c073134afc878619c37c95935d6a3098a21f17",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 7.56,
          "commitsBetweenForRepo": 56,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,42 @@\n   boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n-      List\u003cINode\u003e removedINodes) throws IOException {\n+      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n     }\n     waitForReady();\n     long now \u003d now();\n     final long filesRemoved;\n     writeLock();\n     try {\n       final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n           normalizePath(src), false);\n       if (!deleteAllowed(inodesInPath, src) ) {\n         filesRemoved \u003d -1;\n       } else {\n         // Before removing the node, first check if the targetNode is for a\n         // snapshottable dir with snapshots, or its descendants have\n         // snapshottable dir with snapshots\n         final INode targetNode \u003d inodesInPath.getLastINode();\n         List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n             new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n         checkSnapshot(targetNode, snapshottableDirs);\n         filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n             removedINodes, now);\n         if (snapshottableDirs.size() \u003e 0) {\n           // There are some snapshottable directories without snapshots to be\n           // deleted. Need to update the SnapshotManager.\n           namesystem.removeSnapshottableDirs(snapshottableDirs);\n         }\n       }\n     } finally {\n       writeUnlock();\n     }\n     if (filesRemoved \u003c 0) {\n       return false;\n     }\n-    fsImage.getEditLog().logDelete(src, now);\n+    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n     // Blocks/INodes will be handled later by the caller of this method\n     getFSNamesystem().removePathAndBlocks(src, null, null);\n     return true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean delete(String src, BlocksMapUpdateInfo collectedBlocks,\n      List\u003cINode\u003e removedINodes, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.delete: \" + src);\n    }\n    waitForReady();\n    long now \u003d now();\n    final long filesRemoved;\n    writeLock();\n    try {\n      final INodesInPath inodesInPath \u003d rootDir.getINodesInPath4Write(\n          normalizePath(src), false);\n      if (!deleteAllowed(inodesInPath, src) ) {\n        filesRemoved \u003d -1;\n      } else {\n        // Before removing the node, first check if the targetNode is for a\n        // snapshottable dir with snapshots, or its descendants have\n        // snapshottable dir with snapshots\n        final INode targetNode \u003d inodesInPath.getLastINode();\n        List\u003cINodeDirectorySnapshottable\u003e snapshottableDirs \u003d \n            new ArrayList\u003cINodeDirectorySnapshottable\u003e();\n        checkSnapshot(targetNode, snapshottableDirs);\n        filesRemoved \u003d unprotectedDelete(inodesInPath, collectedBlocks,\n            removedINodes, now);\n        if (snapshottableDirs.size() \u003e 0) {\n          // There are some snapshottable directories without snapshots to be\n          // deleted. Need to update the SnapshotManager.\n          namesystem.removeSnapshottableDirs(snapshottableDirs);\n        }\n      }\n    } finally {\n      writeUnlock();\n    }\n    if (filesRemoved \u003c 0) {\n      return false;\n    }\n    fsImage.getEditLog().logDelete(src, now, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n    // Blocks/INodes will be handled later by the caller of this method\n    getFSNamesystem().removePathAndBlocks(src, null, null);\n    return true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}