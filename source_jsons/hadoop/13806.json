{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "getBlocksWithLocations",
  "functionId": "getBlocksWithLocations___datanode-DatanodeID(modifiers-final)__size-long(modifiers-final)__minBlockSize-long(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 1625,
  "functionEndLine": 1668,
  "numCommitsSeen": 904,
  "timeTaken": 19927,
  "changeHistory": [
    "88fba00caa8c8e26f70deb9be5b534e7482620a1",
    "8dfcd95d580bb090af7f40af0a57061518c18c8c",
    "67523ffcf491f4f2db5335899c00a174d0caaa9b",
    "c304890c8c7782d835896859f5b7f60b96c306c0",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "ba9371492036983a9899398907ab41fe548f29b3",
    "470c87dbc6c24dd3b370f1ad9e7ab1f6dabd2080",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
    "db71de2e11cfa56a254ef4c92fea5ef4f8c19100",
    "be7dd8333a7e56e732171db0781786987de03195",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee",
    "371f4a59059322000a40eb4bdf5386b96b626ece",
    "969a263188f7015261719fe45fa1505121ebb80e"
  ],
  "changeHistoryShort": {
    "88fba00caa8c8e26f70deb9be5b534e7482620a1": "Ymultichange(Yparameterchange,Ybodychange)",
    "8dfcd95d580bb090af7f40af0a57061518c18c8c": "Ybodychange",
    "67523ffcf491f4f2db5335899c00a174d0caaa9b": "Ybodychange",
    "c304890c8c7782d835896859f5b7f60b96c306c0": "Ymodifierchange",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": "Ybodychange",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "ba9371492036983a9899398907ab41fe548f29b3": "Ybodychange",
    "470c87dbc6c24dd3b370f1ad9e7ab1f6dabd2080": "Ybodychange",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": "Ybodychange",
    "db71de2e11cfa56a254ef4c92fea5ef4f8c19100": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee": "Ymodifierchange",
    "371f4a59059322000a40eb4bdf5386b96b626ece": "Ybodychange",
    "969a263188f7015261719fe45fa1505121ebb80e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "88fba00caa8c8e26f70deb9be5b534e7482620a1": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13222. Update getBlocks method to take minBlockSize in RPC calls.  Contributed by Bharat Viswanadham\n",
      "commitDate": "07/03/18 11:27 AM",
      "commitName": "88fba00caa8c8e26f70deb9be5b534e7482620a1",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13222. Update getBlocks method to take minBlockSize in RPC calls.  Contributed by Bharat Viswanadham\n",
          "commitDate": "07/03/18 11:27 AM",
          "commitName": "88fba00caa8c8e26f70deb9be5b534e7482620a1",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "02/01/18 2:59 PM",
          "commitNameOld": "42a1c98597e6dba2e371510a6b2b6b1fb94e4090",
          "commitAuthorOld": "Manoj Govindassamy",
          "daysBetweenCommits": 63.85,
          "commitsBetweenForRepo": 399,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n   public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n-      final long size) throws UnregisteredNodeException {\n+      final long size, final long minBlockSize) throws\n+      UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n           \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     // starting from a random block\n     int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n     Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator(startBlock);\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n-      if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n+      if (curBlock.getNumBytes() \u003c minBlockSize) {\n         continue;\n       }\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n-        if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n+        if (curBlock.getNumBytes() \u003c minBlockSize) {\n           continue;\n         }\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size, final long minBlockSize) throws\n      UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    // starting from a random block\n    int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator(startBlock);\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      if (curBlock.getNumBytes() \u003c minBlockSize) {\n        continue;\n      }\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        if (curBlock.getNumBytes() \u003c minBlockSize) {\n          continue;\n        }\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[datanode-DatanodeID(modifiers-final), size-long(modifiers-final)]",
            "newValue": "[datanode-DatanodeID(modifiers-final), size-long(modifiers-final), minBlockSize-long(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13222. Update getBlocks method to take minBlockSize in RPC calls.  Contributed by Bharat Viswanadham\n",
          "commitDate": "07/03/18 11:27 AM",
          "commitName": "88fba00caa8c8e26f70deb9be5b534e7482620a1",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "02/01/18 2:59 PM",
          "commitNameOld": "42a1c98597e6dba2e371510a6b2b6b1fb94e4090",
          "commitAuthorOld": "Manoj Govindassamy",
          "daysBetweenCommits": 63.85,
          "commitsBetweenForRepo": 399,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n   public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n-      final long size) throws UnregisteredNodeException {\n+      final long size, final long minBlockSize) throws\n+      UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n           \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     // starting from a random block\n     int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n     Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator(startBlock);\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n-      if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n+      if (curBlock.getNumBytes() \u003c minBlockSize) {\n         continue;\n       }\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n-        if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n+        if (curBlock.getNumBytes() \u003c minBlockSize) {\n           continue;\n         }\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size, final long minBlockSize) throws\n      UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    // starting from a random block\n    int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator(startBlock);\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      if (curBlock.getNumBytes() \u003c minBlockSize) {\n        continue;\n      }\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        if (curBlock.getNumBytes() \u003c minBlockSize) {\n          continue;\n        }\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "8dfcd95d580bb090af7f40af0a57061518c18c8c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11634. Optimize BlockIterator when interating starts in the middle. Contributed by Konstantin V Shvachko.",
      "commitDate": "17/04/17 4:56 PM",
      "commitName": "8dfcd95d580bb090af7f40af0a57061518c18c8c",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "16/03/17 3:07 PM",
      "commitNameOld": "4812518b23cac496ab5cdad5258773bcd9728770",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 32.08,
      "commitsBetweenForRepo": 179,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,43 @@\n   public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n           \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n-    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     // starting from a random block\n     int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n-    // skip blocks\n-    for(int i\u003d0; i\u003cstartBlock; i++) {\n-      iter.next();\n-    }\n+    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator(startBlock);\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n         continue;\n       }\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n           continue;\n         }\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    // starting from a random block\n    int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator(startBlock);\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n        continue;\n      }\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n          continue;\n        }\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "67523ffcf491f4f2db5335899c00a174d0caaa9b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9412. getBlocks occupies FSLock and takes too long to complete. Contributed by He Tianyi.\n",
      "commitDate": "17/04/16 6:28 PM",
      "commitName": "67523ffcf491f4f2db5335899c00a174d0caaa9b",
      "commitAuthor": "Walter Su",
      "commitDateOld": "06/04/16 10:42 AM",
      "commitNameOld": "221b3a8722f84f8e9ad0a98eea38a12cc4ad2f24",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 11.32,
      "commitsBetweenForRepo": 70,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,47 @@\n   public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n           \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     // starting from a random block\n     int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n+      if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n+        continue;\n+      }\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n+        if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n+          continue;\n+        }\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    // starting from a random block\n    int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n        continue;\n      }\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        if (curBlock.getNumBytes() \u003c getBlocksMinBlockSize) {\n          continue;\n        }\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "c304890c8c7782d835896859f5b7f60b96c306c0": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-9542. Move BlockIdManager from FSNamesystem to BlockManager. Contributed by Jing Zhao.\n",
      "commitDate": "21/01/16 11:13 AM",
      "commitName": "c304890c8c7782d835896859f5b7f60b96c306c0",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "06/01/16 9:57 PM",
      "commitNameOld": "34cd7cd76505d01ec251e30837c94ab03319a0c1",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 14.55,
      "commitsBetweenForRepo": 109,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n-  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n+  public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n           \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     // starting from a random block\n     int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    // starting from a random block\n    int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[public]"
      }
    },
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\"\n\nThis reverts commit de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "31/07/15 4:15 PM",
      "commitNameOld": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 5.75,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n           \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     // starting from a random block\n     int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n-    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003c\u003e();\n+    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    // starting from a random block\n    int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\n",
      "commitDate": "26/06/15 10:49 AM",
      "commitName": "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/06/15 2:42 PM",
      "commitNameOld": "afe9ea3c12e1f5a71922400eadb642960bc87ca1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n           \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     // starting from a random block\n     int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n-    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n+    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003c\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    // starting from a random block\n    int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003c\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "19/05/15 11:05 AM",
      "commitNameOld": "8860e352c394372e4eb3ebdf82ea899567f34e4e",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 8.19,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n           \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n-    Iterator\u003cBlockInfoContiguous\u003e iter \u003d node.getBlockIterator();\n+    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     // starting from a random block\n     int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n-    BlockInfoContiguous curBlock;\n+    BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    // starting from a random block\n    int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "ba9371492036983a9899398907ab41fe548f29b3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7716. Erasure Coding: extend BlockInfo to handle EC info. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "ba9371492036983a9899398907ab41fe548f29b3",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:03 AM",
      "commitNameOld": "0c1da5a0300f015a7e39f2b40a73fb06c65a78c8",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n           \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n-    Iterator\u003cBlockInfoContiguous\u003e iter \u003d node.getBlockIterator();\n+    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     // starting from a random block\n     int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n-    BlockInfoContiguous curBlock;\n+    BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    // starting from a random block\n    int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "470c87dbc6c24dd3b370f1ad9e7ab1f6dabd2080": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11970. Replace uses of ThreadLocal\u003cRandom\u003e with JDK7 ThreadLocalRandom (Sean Busbey via Colin P. McCabe)\n",
      "commitDate": "19/05/15 10:50 AM",
      "commitName": "470c87dbc6c24dd3b370f1ad9e7ab1f6dabd2080",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "13/05/15 2:29 PM",
      "commitNameOld": "281d47a96937bc329b1b4051ffcb8f5fcac98354",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 5.85,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,41 @@\n   private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n           \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     Iterator\u003cBlockInfoContiguous\u003e iter \u003d node.getBlockIterator();\n-    int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n+    // starting from a random block\n+    int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfoContiguous curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfoContiguous\u003e iter \u003d node.getBlockIterator();\n    // starting from a random block\n    int startBlock \u003d ThreadLocalRandom.current().nextInt(numBlocks);\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfoContiguous curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "04/02/15 11:31 AM",
      "commitNameOld": "9175105eeaecf0a1d60b57989b73ce45cee4689b",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.01,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n           \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n-    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n+    Iterator\u003cBlockInfoContiguous\u003e iter \u003d node.getBlockIterator();\n     int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n-    BlockInfo curBlock;\n+    BlockInfoContiguous curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfoContiguous\u003e iter \u003d node.getBlockIterator();\n    int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfoContiguous curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7712. Switch blockStateChangeLog to use slf4j.\n",
      "commitDate": "03/02/15 3:01 PM",
      "commitName": "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/01/15 11:33 AM",
      "commitNameOld": "951b3608a8cb1d9063b9be9c740b524c137b816f",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.14,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n-      blockLog.warn(\"BLOCK* getBlocks: \"\n-          + \"Asking for blocks from an unrecorded node \" + datanode);\n+      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n+          \" unrecorded node {}\", datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: Asking for blocks from an\" +\n          \" unrecorded node {}\", datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "db71de2e11cfa56a254ef4c92fea5ef4f8c19100": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4080. Add a separate logger for block state change logs to enable turning off those logs. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1407566 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/11/12 10:07 AM",
      "commitName": "db71de2e11cfa56a254ef4c92fea5ef4f8c19100",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "06/11/12 11:27 AM",
      "commitNameOld": "54b70db347c2ebf577919f2c42f171c6801e9ba1",
      "commitAuthorOld": "Daryn Sharp",
      "daysBetweenCommits": 2.94,
      "commitsBetweenForRepo": 28,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n-      NameNode.stateChangeLog.warn(\"BLOCK* getBlocks: \"\n+      blockLog.warn(\"BLOCK* getBlocks: \"\n           + \"Asking for blocks from an unrecorded node \" + datanode);\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      blockLog.warn(\"BLOCK* getBlocks: \"\n          + \"Asking for blocks from an unrecorded node \" + datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "31/03/12 12:58 PM",
      "commitNameOld": "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.09,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       NameNode.stateChangeLog.warn(\"BLOCK* getBlocks: \"\n-          + \"Asking for blocks from an unrecorded node \" + datanode.getName());\n+          + \"Asking for blocks from an unrecorded node \" + datanode);\n       throw new HadoopIllegalArgumentException(\n-          \"Datanode \" + datanode.getName() + \" not found.\");\n+          \"Datanode \" + datanode + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      NameNode.stateChangeLog.warn(\"BLOCK* getBlocks: \"\n          + \"Asking for blocks from an unrecorded node \" + datanode);\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      NameNode.stateChangeLog.warn(\"BLOCK* getBlocks: \"\n          + \"Asking for blocks from an unrecorded node \" + datanode.getName());\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode.getName() + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      NameNode.stateChangeLog.warn(\"BLOCK* getBlocks: \"\n          + \"Asking for blocks from an unrecorded node \" + datanode.getName());\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode.getName() + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-2239. Reduce access levels of the fields and methods in FSNamesystem.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1155998 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/11 6:50 PM",
      "commitName": "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/08/11 3:06 AM",
      "commitNameOld": "371f4a59059322000a40eb4bdf5386b96b626ece",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.66,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n-  public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n+  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n       NameNode.stateChangeLog.warn(\"BLOCK* getBlocks: \"\n           + \"Asking for blocks from an unrecorded node \" + datanode.getName());\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode.getName() + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      NameNode.stateChangeLog.warn(\"BLOCK* getBlocks: \"\n          + \"Asking for blocks from an unrecorded node \" + datanode.getName());\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode.getName() + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "[public]",
        "newValue": "[private]"
      }
    },
    "371f4a59059322000a40eb4bdf5386b96b626ece": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2228. Move block and datanode code from FSNamesystem to BlockManager and DatanodeManager.  (szetszwo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154899 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/08/11 3:06 AM",
      "commitName": "371f4a59059322000a40eb4bdf5386b96b626ece",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "04/08/11 3:55 PM",
      "commitNameOld": "7fac946ac983e31613fd62836c8ac9c4a579210a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.47,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n       final long size) throws UnregisteredNodeException {\n     final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n     if (node \u003d\u003d null) {\n-      NameNode.stateChangeLog.warn(\"BLOCK* NameSystem.getBlocks: \"\n+      NameNode.stateChangeLog.warn(\"BLOCK* getBlocks: \"\n           + \"Asking for blocks from an unrecorded node \" + datanode.getName());\n       throw new HadoopIllegalArgumentException(\n           \"Datanode \" + datanode.getName() + \" not found.\");\n     }\n \n     int numBlocks \u003d node.numBlocks();\n     if(numBlocks \u003d\u003d 0) {\n       return new BlocksWithLocations(new BlockWithLocations[0]);\n     }\n     Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n     int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n     // skip blocks\n     for(int i\u003d0; i\u003cstartBlock; i++) {\n       iter.next();\n     }\n     List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n     long totalSize \u003d 0;\n     BlockInfo curBlock;\n     while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n       curBlock \u003d iter.next();\n       if(!curBlock.isComplete())  continue;\n       totalSize +\u003d addBlock(curBlock, results);\n     }\n     if(totalSize\u003csize) {\n       iter \u003d node.getBlockIterator(); // start from the beginning\n       for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n         curBlock \u003d iter.next();\n         if(!curBlock.isComplete())  continue;\n         totalSize +\u003d addBlock(curBlock, results);\n       }\n     }\n \n     return new BlocksWithLocations(\n         results.toArray(new BlockWithLocations[results.size()]));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      NameNode.stateChangeLog.warn(\"BLOCK* getBlocks: \"\n          + \"Asking for blocks from an unrecorded node \" + datanode.getName());\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode.getName() + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "969a263188f7015261719fe45fa1505121ebb80e": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2191.  Move datanodeMap from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151339 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/11 10:46 PM",
      "commitName": "969a263188f7015261719fe45fa1505121ebb80e",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,40 @@\n+  public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n+      final long size) throws UnregisteredNodeException {\n+    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n+    if (node \u003d\u003d null) {\n+      NameNode.stateChangeLog.warn(\"BLOCK* NameSystem.getBlocks: \"\n+          + \"Asking for blocks from an unrecorded node \" + datanode.getName());\n+      throw new HadoopIllegalArgumentException(\n+          \"Datanode \" + datanode.getName() + \" not found.\");\n+    }\n+\n+    int numBlocks \u003d node.numBlocks();\n+    if(numBlocks \u003d\u003d 0) {\n+      return new BlocksWithLocations(new BlockWithLocations[0]);\n+    }\n+    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n+    int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n+    // skip blocks\n+    for(int i\u003d0; i\u003cstartBlock; i++) {\n+      iter.next();\n+    }\n+    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n+    long totalSize \u003d 0;\n+    BlockInfo curBlock;\n+    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n+      curBlock \u003d iter.next();\n+      if(!curBlock.isComplete())  continue;\n+      totalSize +\u003d addBlock(curBlock, results);\n+    }\n+    if(totalSize\u003csize) {\n+      iter \u003d node.getBlockIterator(); // start from the beginning\n+      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n+        curBlock \u003d iter.next();\n+        if(!curBlock.isComplete())  continue;\n+        totalSize +\u003d addBlock(curBlock, results);\n+      }\n+    }\n+\n+    return new BlocksWithLocations(\n+        results.toArray(new BlockWithLocations[results.size()]));\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,\n      final long size) throws UnregisteredNodeException {\n    final DatanodeDescriptor node \u003d getDatanodeManager().getDatanode(datanode);\n    if (node \u003d\u003d null) {\n      NameNode.stateChangeLog.warn(\"BLOCK* NameSystem.getBlocks: \"\n          + \"Asking for blocks from an unrecorded node \" + datanode.getName());\n      throw new HadoopIllegalArgumentException(\n          \"Datanode \" + datanode.getName() + \" not found.\");\n    }\n\n    int numBlocks \u003d node.numBlocks();\n    if(numBlocks \u003d\u003d 0) {\n      return new BlocksWithLocations(new BlockWithLocations[0]);\n    }\n    Iterator\u003cBlockInfo\u003e iter \u003d node.getBlockIterator();\n    int startBlock \u003d DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block\n    // skip blocks\n    for(int i\u003d0; i\u003cstartBlock; i++) {\n      iter.next();\n    }\n    List\u003cBlockWithLocations\u003e results \u003d new ArrayList\u003cBlockWithLocations\u003e();\n    long totalSize \u003d 0;\n    BlockInfo curBlock;\n    while(totalSize\u003csize \u0026\u0026 iter.hasNext()) {\n      curBlock \u003d iter.next();\n      if(!curBlock.isComplete())  continue;\n      totalSize +\u003d addBlock(curBlock, results);\n    }\n    if(totalSize\u003csize) {\n      iter \u003d node.getBlockIterator(); // start from the beginning\n      for(int i\u003d0; i\u003cstartBlock\u0026\u0026totalSize\u003csize; i++) {\n        curBlock \u003d iter.next();\n        if(!curBlock.isComplete())  continue;\n        totalSize +\u003d addBlock(curBlock, results);\n      }\n    }\n\n    return new BlocksWithLocations(\n        results.toArray(new BlockWithLocations[results.size()]));\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    }
  }
}