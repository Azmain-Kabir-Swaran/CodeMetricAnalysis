{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StripeReader.java",
  "functionName": "readDataForDecoding",
  "functionId": "readDataForDecoding",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
  "functionStartLine": 187,
  "functionEndLine": 198,
  "numCommitsSeen": 19,
  "timeTaken": 1202,
  "changeHistory": [
    "734d54c1a8950446e68098f62d8964e02ecc2890"
  ],
  "changeHistoryShort": {
    "734d54c1a8950446e68098f62d8964e02ecc2890": "Ymovefromfile"
  },
  "changeHistoryDetails": {
    "734d54c1a8950446e68098f62d8964e02ecc2890": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
      "commitDate": "21/09/16 6:34 AM",
      "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "20/09/16 12:03 AM",
      "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 1.27,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,12 @@\n-    private void readDataForDecoding() throws IOException {\n-      prepareDecodeInputs();\n-      for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n-        Preconditions.checkNotNull(alignedStripe.chunks[i]);\n-        if (alignedStripe.chunks[i].state \u003d\u003d StripingChunk.REQUESTED) {\n-          if (!readChunk(targetBlocks[i], i)) {\n-            alignedStripe.missingChunksNum++;\n-          }\n+  private void readDataForDecoding() throws IOException {\n+    prepareDecodeInputs();\n+    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n+      Preconditions.checkNotNull(alignedStripe.chunks[i]);\n+      if (alignedStripe.chunks[i].state \u003d\u003d StripingChunk.REQUESTED) {\n+        if (!readChunk(targetBlocks[i], i)) {\n+          alignedStripe.missingChunksNum++;\n         }\n       }\n-      checkMissingBlocks();\n-    }\n\\ No newline at end of file\n+    }\n+    checkMissingBlocks();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void readDataForDecoding() throws IOException {\n    prepareDecodeInputs();\n    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n      Preconditions.checkNotNull(alignedStripe.chunks[i]);\n      if (alignedStripe.chunks[i].state \u003d\u003d StripingChunk.REQUESTED) {\n        if (!readChunk(targetBlocks[i], i)) {\n          alignedStripe.missingChunksNum++;\n        }\n      }\n    }\n    checkMissingBlocks();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
        "oldMethodName": "readDataForDecoding",
        "newMethodName": "readDataForDecoding"
      }
    }
  }
}