{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "fetchLocatedBlocksAndGetLastBlockLength",
  "functionId": "fetchLocatedBlocksAndGetLastBlockLength___refresh-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 327,
  "functionEndLine": 353,
  "numCommitsSeen": 221,
  "timeTaken": 6369,
  "changeHistory": [
    "c36014165c212b26d75268ee3659aa2cadcff349",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "bff5999d07e9416a22846c849487e509ede55040",
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
    "2efea952139b30dd1c881eed0b443ffa72be6dce",
    "bdee397e95e98ece071345822e2e4d3f690f09c3",
    "f26d2adbf98890cfe350c17241f5049b89a11e3c",
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
    "fb95fce24056c0b0aa5b77683c684fe1b68c4f76",
    "d28b98242854ff7f9d615e1c9d6a5b7584ce2498"
  ],
  "changeHistoryShort": {
    "c36014165c212b26d75268ee3659aa2cadcff349": "Ybodychange",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "bff5999d07e9416a22846c849487e509ede55040": "Ymultichange(Yparameterchange,Ybodychange)",
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f": "Ybodychange",
    "2efea952139b30dd1c881eed0b443ffa72be6dce": "Ybodychange",
    "bdee397e95e98ece071345822e2e4d3f690f09c3": "Ybodychange",
    "f26d2adbf98890cfe350c17241f5049b89a11e3c": "Ybodychange",
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6": "Ybodychange",
    "fb95fce24056c0b0aa5b77683c684fe1b68c4f76": "Ybodychange",
    "d28b98242854ff7f9d615e1c9d6a5b7584ce2498": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c36014165c212b26d75268ee3659aa2cadcff349": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14384. When lastLocatedBlock token expire, it will take 1~3s second to refetch it. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "06/11/19 5:58 AM",
      "commitName": "c36014165c212b26d75268ee3659aa2cadcff349",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "27/09/19 12:25 PM",
      "commitNameOld": "13b427fc05da7352fadd7214adfa09c326bba238",
      "commitAuthorOld": "Sahil Takiar",
      "daysBetweenCommits": 39.77,
      "commitsBetweenForRepo": 204,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,26 @@\n   private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n       throws IOException {\n     LocatedBlocks newInfo \u003d locatedBlocks;\n     if (locatedBlocks \u003d\u003d null || refresh) {\n       newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n     }\n     DFSClient.LOG.debug(\"newInfo \u003d {}\", newInfo);\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (!oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n-    long lastBlockBeingWrittenLength \u003d 0;\n-    if (!locatedBlocks.isLastBlockComplete()) {\n-      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n-      if (last !\u003d null) {\n-        if (last.getLocations().length \u003d\u003d 0) {\n-          if (last.getBlockSize() \u003d\u003d 0) {\n-            // if the length is zero, then no data has been written to\n-            // datanode. So no need to wait for the locations.\n-            return 0;\n-          }\n-          return -1;\n-        }\n-        final long len \u003d readBlockLength(last);\n-        last.getBlock().setNumBytes(len);\n-        lastBlockBeingWrittenLength \u003d len;\n-      }\n-    }\n-\n+    long lastBlkBeingWrittenLength \u003d getLastBlockLength();\n     fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n \n-    return lastBlockBeingWrittenLength;\n+    return lastBlkBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n      throws IOException {\n    LocatedBlocks newInfo \u003d locatedBlocks;\n    if (locatedBlocks \u003d\u003d null || refresh) {\n      newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    }\n    DFSClient.LOG.debug(\"newInfo \u003d {}\", newInfo);\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (!oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlkBeingWrittenLength \u003d getLastBlockLength();\n    fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n\n    return lastBlkBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,43 @@\n   private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n       throws IOException {\n     LocatedBlocks newInfo \u003d locatedBlocks;\n     if (locatedBlocks \u003d\u003d null || refresh) {\n       newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n     }\n-    if (DFSClient.LOG.isDebugEnabled()) {\n-      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n-    }\n+    DFSClient.LOG.debug(\"newInfo \u003d {}\", newInfo);\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n     long lastBlockBeingWrittenLength \u003d 0;\n     if (!locatedBlocks.isLastBlockComplete()) {\n       final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n       if (last !\u003d null) {\n         if (last.getLocations().length \u003d\u003d 0) {\n           if (last.getBlockSize() \u003d\u003d 0) {\n             // if the length is zero, then no data has been written to\n             // datanode. So no need to wait for the locations.\n             return 0;\n           }\n           return -1;\n         }\n         final long len \u003d readBlockLength(last);\n         last.getBlock().setNumBytes(len);\n         lastBlockBeingWrittenLength \u003d len; \n       }\n     }\n \n     fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n \n     return lastBlockBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n      throws IOException {\n    LocatedBlocks newInfo \u003d locatedBlocks;\n    if (locatedBlocks \u003d\u003d null || refresh) {\n      newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    }\n    DFSClient.LOG.debug(\"newInfo \u003d {}\", newInfo);\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          if (last.getBlockSize() \u003d\u003d 0) {\n            // if the length is zero, then no data has been written to\n            // datanode. So no need to wait for the locations.\n            return 0;\n          }\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n\n    return lastBlockBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,45 @@\n   private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n       throws IOException {\n     LocatedBlocks newInfo \u003d locatedBlocks;\n     if (locatedBlocks \u003d\u003d null || refresh) {\n       newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n     }\n-    DFSClient.LOG.debug(\"newInfo \u003d {}\", newInfo);\n+    if (DFSClient.LOG.isDebugEnabled()) {\n+      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n+    }\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n     long lastBlockBeingWrittenLength \u003d 0;\n     if (!locatedBlocks.isLastBlockComplete()) {\n       final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n       if (last !\u003d null) {\n         if (last.getLocations().length \u003d\u003d 0) {\n           if (last.getBlockSize() \u003d\u003d 0) {\n             // if the length is zero, then no data has been written to\n             // datanode. So no need to wait for the locations.\n             return 0;\n           }\n           return -1;\n         }\n         final long len \u003d readBlockLength(last);\n         last.getBlock().setNumBytes(len);\n         lastBlockBeingWrittenLength \u003d len; \n       }\n     }\n \n     fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n \n     return lastBlockBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n      throws IOException {\n    LocatedBlocks newInfo \u003d locatedBlocks;\n    if (locatedBlocks \u003d\u003d null || refresh) {\n      newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    }\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          if (last.getBlockSize() \u003d\u003d 0) {\n            // if the length is zero, then no data has been written to\n            // datanode. So no need to wait for the locations.\n            return 0;\n          }\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n\n    return lastBlockBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,43 @@\n   private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n       throws IOException {\n     LocatedBlocks newInfo \u003d locatedBlocks;\n     if (locatedBlocks \u003d\u003d null || refresh) {\n       newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n     }\n-    if (DFSClient.LOG.isDebugEnabled()) {\n-      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n-    }\n+    DFSClient.LOG.debug(\"newInfo \u003d {}\", newInfo);\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n     long lastBlockBeingWrittenLength \u003d 0;\n     if (!locatedBlocks.isLastBlockComplete()) {\n       final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n       if (last !\u003d null) {\n         if (last.getLocations().length \u003d\u003d 0) {\n           if (last.getBlockSize() \u003d\u003d 0) {\n             // if the length is zero, then no data has been written to\n             // datanode. So no need to wait for the locations.\n             return 0;\n           }\n           return -1;\n         }\n         final long len \u003d readBlockLength(last);\n         last.getBlock().setNumBytes(len);\n         lastBlockBeingWrittenLength \u003d len; \n       }\n     }\n \n     fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n \n     return lastBlockBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n      throws IOException {\n    LocatedBlocks newInfo \u003d locatedBlocks;\n    if (locatedBlocks \u003d\u003d null || refresh) {\n      newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    }\n    DFSClient.LOG.debug(\"newInfo \u003d {}\", newInfo);\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          if (last.getBlockSize() \u003d\u003d 0) {\n            // if the length is zero, then no data has been written to\n            // datanode. So no need to wait for the locations.\n            return 0;\n          }\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n\n    return lastBlockBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n      throws IOException {\n    LocatedBlocks newInfo \u003d locatedBlocks;\n    if (locatedBlocks \u003d\u003d null || refresh) {\n      newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    }\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          if (last.getBlockSize() \u003d\u003d 0) {\n            // if the length is zero, then no data has been written to\n            // datanode. So no need to wait for the locations.\n            return 0;\n          }\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n\n    return lastBlockBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "bff5999d07e9416a22846c849487e509ede55040": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
      "commitDate": "02/07/15 3:41 AM",
      "commitName": "bff5999d07e9416a22846c849487e509ede55040",
      "commitAuthor": "Vinayakumar B",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
          "commitDate": "02/07/15 3:41 AM",
          "commitName": "bff5999d07e9416a22846c849487e509ede55040",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "04/06/15 10:51 AM",
          "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 27.7,
          "commitsBetweenForRepo": 196,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,45 @@\n-  private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n-    final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n+  private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n+      throws IOException {\n+    LocatedBlocks newInfo \u003d locatedBlocks;\n+    if (locatedBlocks \u003d\u003d null || refresh) {\n+      newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n+    }\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n     }\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n     long lastBlockBeingWrittenLength \u003d 0;\n     if (!locatedBlocks.isLastBlockComplete()) {\n       final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n       if (last !\u003d null) {\n         if (last.getLocations().length \u003d\u003d 0) {\n           if (last.getBlockSize() \u003d\u003d 0) {\n             // if the length is zero, then no data has been written to\n             // datanode. So no need to wait for the locations.\n             return 0;\n           }\n           return -1;\n         }\n         final long len \u003d readBlockLength(last);\n         last.getBlock().setNumBytes(len);\n         lastBlockBeingWrittenLength \u003d len; \n       }\n     }\n \n     fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n \n     return lastBlockBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n      throws IOException {\n    LocatedBlocks newInfo \u003d locatedBlocks;\n    if (locatedBlocks \u003d\u003d null || refresh) {\n      newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    }\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          if (last.getBlockSize() \u003d\u003d 0) {\n            // if the length is zero, then no data has been written to\n            // datanode. So no need to wait for the locations.\n            return 0;\n          }\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n\n    return lastBlockBeingWrittenLength;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[refresh-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
          "commitDate": "02/07/15 3:41 AM",
          "commitName": "bff5999d07e9416a22846c849487e509ede55040",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "04/06/15 10:51 AM",
          "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 27.7,
          "commitsBetweenForRepo": 196,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,45 @@\n-  private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n-    final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n+  private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n+      throws IOException {\n+    LocatedBlocks newInfo \u003d locatedBlocks;\n+    if (locatedBlocks \u003d\u003d null || refresh) {\n+      newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n+    }\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n     }\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n     long lastBlockBeingWrittenLength \u003d 0;\n     if (!locatedBlocks.isLastBlockComplete()) {\n       final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n       if (last !\u003d null) {\n         if (last.getLocations().length \u003d\u003d 0) {\n           if (last.getBlockSize() \u003d\u003d 0) {\n             // if the length is zero, then no data has been written to\n             // datanode. So no need to wait for the locations.\n             return 0;\n           }\n           return -1;\n         }\n         final long len \u003d readBlockLength(last);\n         last.getBlock().setNumBytes(len);\n         lastBlockBeingWrittenLength \u003d len; \n       }\n     }\n \n     fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n \n     return lastBlockBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength(boolean refresh)\n      throws IOException {\n    LocatedBlocks newInfo \u003d locatedBlocks;\n    if (locatedBlocks \u003d\u003d null || refresh) {\n      newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    }\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          if (last.getBlockSize() \u003d\u003d 0) {\n            // if the length is zero, then no data has been written to\n            // datanode. So no need to wait for the locations.\n            return 0;\n          }\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n\n    return lastBlockBeingWrittenLength;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6735. A minor optimization to avoid pread() be blocked by read() inside the same DFSInputStream (Lars Hofhansl via stack)\n",
      "commitDate": "02/12/14 8:57 PM",
      "commitName": "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
      "commitAuthor": "stack",
      "commitDateOld": "05/11/14 9:00 PM",
      "commitNameOld": "80d7d183cd4052d6e6d412ff6588d26471c85d6d",
      "commitAuthorOld": "Milan Desai",
      "daysBetweenCommits": 27.0,
      "commitsBetweenForRepo": 189,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,41 @@\n   private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n     final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n     }\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n     long lastBlockBeingWrittenLength \u003d 0;\n     if (!locatedBlocks.isLastBlockComplete()) {\n       final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n       if (last !\u003d null) {\n         if (last.getLocations().length \u003d\u003d 0) {\n           if (last.getBlockSize() \u003d\u003d 0) {\n             // if the length is zero, then no data has been written to\n             // datanode. So no need to wait for the locations.\n             return 0;\n           }\n           return -1;\n         }\n         final long len \u003d readBlockLength(last);\n         last.getBlock().setNumBytes(len);\n         lastBlockBeingWrittenLength \u003d len; \n       }\n     }\n \n     fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n \n-    currentNode \u003d null;\n     return lastBlockBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n    final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          if (last.getBlockSize() \u003d\u003d 0) {\n            // if the length is zero, then no data has been written to\n            // datanode. So no need to wait for the locations.\n            return 0;\n          }\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n\n    return lastBlockBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "2efea952139b30dd1c881eed0b443ffa72be6dce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6391. Get the Key/IV from the NameNode for encrypted files in DFSClient. Contributed by Charles Lamb and Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1606220 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/06/14 1:43 PM",
      "commitName": "2efea952139b30dd1c881eed0b443ffa72be6dce",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "05/06/14 3:10 AM",
      "commitNameOld": "bdee397e95e98ece071345822e2e4d3f690f09c3",
      "commitAuthorOld": "Charles Lamb",
      "daysBetweenCommits": 22.44,
      "commitsBetweenForRepo": 107,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n   private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n     final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n     }\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n     long lastBlockBeingWrittenLength \u003d 0;\n     if (!locatedBlocks.isLastBlockComplete()) {\n       final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n       if (last !\u003d null) {\n         if (last.getLocations().length \u003d\u003d 0) {\n           if (last.getBlockSize() \u003d\u003d 0) {\n             // if the length is zero, then no data has been written to\n             // datanode. So no need to wait for the locations.\n             return 0;\n           }\n           return -1;\n         }\n         final long len \u003d readBlockLength(last);\n         last.getBlock().setNumBytes(len);\n         lastBlockBeingWrittenLength \u003d len; \n       }\n     }\n \n-    key \u003d locatedBlocks.getKey();\n-    iv \u003d locatedBlocks.getIv();\n+    fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n+\n     currentNode \u003d null;\n     return lastBlockBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n    final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          if (last.getBlockSize() \u003d\u003d 0) {\n            // if the length is zero, then no data has been written to\n            // datanode. So no need to wait for the locations.\n            return 0;\n          }\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    fileEncryptionInfo \u003d locatedBlocks.getFileEncryptionInfo();\n\n    currentNode \u003d null;\n    return lastBlockBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "bdee397e95e98ece071345822e2e4d3f690f09c3": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-6392. Wire crypto streams for encrypted files in DFSClient. (clamb and yliu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1600582 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/06/14 3:10 AM",
      "commitName": "bdee397e95e98ece071345822e2e4d3f690f09c3",
      "commitAuthor": "Charles Lamb",
      "commitDateOld": "28/04/14 1:20 PM",
      "commitNameOld": "71aa608b84afcbe19dc38ca7c43c6e750d5df97a",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 37.58,
      "commitsBetweenForRepo": 189,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,42 @@\n   private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n     final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n     }\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n     long lastBlockBeingWrittenLength \u003d 0;\n     if (!locatedBlocks.isLastBlockComplete()) {\n       final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n       if (last !\u003d null) {\n         if (last.getLocations().length \u003d\u003d 0) {\n           if (last.getBlockSize() \u003d\u003d 0) {\n             // if the length is zero, then no data has been written to\n             // datanode. So no need to wait for the locations.\n             return 0;\n           }\n           return -1;\n         }\n         final long len \u003d readBlockLength(last);\n         last.getBlock().setNumBytes(len);\n         lastBlockBeingWrittenLength \u003d len; \n       }\n     }\n \n+    key \u003d locatedBlocks.getKey();\n+    iv \u003d locatedBlocks.getIv();\n     currentNode \u003d null;\n     return lastBlockBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n    final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          if (last.getBlockSize() \u003d\u003d 0) {\n            // if the length is zero, then no data has been written to\n            // datanode. So no need to wait for the locations.\n            return 0;\n          }\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    key \u003d locatedBlocks.getKey();\n    iv \u003d locatedBlocks.getIv();\n    currentNode \u003d null;\n    return lastBlockBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "f26d2adbf98890cfe350c17241f5049b89a11e3c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4516. Client crash after block allocation and NN switch before lease recovery for the same file can cause readers to fail forever. Contributed by Vinay.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543829 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/11/13 6:43 AM",
      "commitName": "f26d2adbf98890cfe350c17241f5049b89a11e3c",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "16/10/13 1:45 PM",
      "commitNameOld": "f28f5ed62861a4c87256eddb8a8ab64e05696192",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 34.75,
      "commitsBetweenForRepo": 192,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,40 @@\n   private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n     final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n     }\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n     long lastBlockBeingWrittenLength \u003d 0;\n     if (!locatedBlocks.isLastBlockComplete()) {\n       final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n       if (last !\u003d null) {\n         if (last.getLocations().length \u003d\u003d 0) {\n+          if (last.getBlockSize() \u003d\u003d 0) {\n+            // if the length is zero, then no data has been written to\n+            // datanode. So no need to wait for the locations.\n+            return 0;\n+          }\n           return -1;\n         }\n         final long len \u003d readBlockLength(last);\n         last.getBlock().setNumBytes(len);\n         lastBlockBeingWrittenLength \u003d len; \n       }\n     }\n \n     currentNode \u003d null;\n     return lastBlockBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n    final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          if (last.getBlockSize() \u003d\u003d 0) {\n            // if the length is zero, then no data has been written to\n            // datanode. So no need to wait for the locations.\n            return 0;\n          }\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    currentNode \u003d null;\n    return lastBlockBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4914. Use DFSClient.Conf instead of Configuration.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1494854 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/06/13 9:43 PM",
      "commitName": "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "10/05/13 10:58 AM",
      "commitNameOld": "4ed1fc58c0683bbcd5c4c211ea162ed37bf7dc4f",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 40.45,
      "commitsBetweenForRepo": 278,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,35 @@\n   private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n-    LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0, prefetchSize);\n+    final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n     }\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n     long lastBlockBeingWrittenLength \u003d 0;\n     if (!locatedBlocks.isLastBlockComplete()) {\n       final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n       if (last !\u003d null) {\n         if (last.getLocations().length \u003d\u003d 0) {\n           return -1;\n         }\n         final long len \u003d readBlockLength(last);\n         last.getBlock().setNumBytes(len);\n         lastBlockBeingWrittenLength \u003d len; \n       }\n     }\n \n     currentNode \u003d null;\n     return lastBlockBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n    final LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    currentNode \u003d null;\n    return lastBlockBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "fb95fce24056c0b0aa5b77683c684fe1b68c4f76": {
      "type": "Ybodychange",
      "commitMessage": "Fix issue with NN/DN re-registration.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1358347 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/07/12 11:59 AM",
      "commitName": "fb95fce24056c0b0aa5b77683c684fe1b68c4f76",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "07/05/12 9:42 AM",
      "commitNameOld": "eca2c850a57f61e40dff734b10a2e057c5bd8cc3",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 60.1,
      "commitsBetweenForRepo": 293,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,35 @@\n   private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n-    LocatedBlocks newInfo \u003d DFSClient.callGetBlockLocations(dfsClient.namenode, src, 0, prefetchSize);\n+    LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0, prefetchSize);\n     if (DFSClient.LOG.isDebugEnabled()) {\n       DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n     }\n     if (newInfo \u003d\u003d null) {\n       throw new IOException(\"Cannot open filename \" + src);\n     }\n \n     if (locatedBlocks !\u003d null) {\n       Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n       Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n       while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n         if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n           throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n         }\n       }\n     }\n     locatedBlocks \u003d newInfo;\n     long lastBlockBeingWrittenLength \u003d 0;\n     if (!locatedBlocks.isLastBlockComplete()) {\n       final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n       if (last !\u003d null) {\n         if (last.getLocations().length \u003d\u003d 0) {\n           return -1;\n         }\n         final long len \u003d readBlockLength(last);\n         last.getBlock().setNumBytes(len);\n         lastBlockBeingWrittenLength \u003d len; \n       }\n     }\n \n     currentNode \u003d null;\n     return lastBlockBeingWrittenLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n    LocatedBlocks newInfo \u003d dfsClient.getLocatedBlocks(src, 0, prefetchSize);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    currentNode \u003d null;\n    return lastBlockBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "d28b98242854ff7f9d615e1c9d6a5b7584ce2498": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3222. DFSInputStream#openInfo should not silently get the length as 0 when locations length is zero for last partial block. Contributed by Uma Maheswara Rao G.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1331061 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/04/12 1:18 PM",
      "commitName": "d28b98242854ff7f9d615e1c9d6a5b7584ce2498",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,35 @@\n+  private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n+    LocatedBlocks newInfo \u003d DFSClient.callGetBlockLocations(dfsClient.namenode, src, 0, prefetchSize);\n+    if (DFSClient.LOG.isDebugEnabled()) {\n+      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n+    }\n+    if (newInfo \u003d\u003d null) {\n+      throw new IOException(\"Cannot open filename \" + src);\n+    }\n+\n+    if (locatedBlocks !\u003d null) {\n+      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n+      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n+      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n+        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n+          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n+        }\n+      }\n+    }\n+    locatedBlocks \u003d newInfo;\n+    long lastBlockBeingWrittenLength \u003d 0;\n+    if (!locatedBlocks.isLastBlockComplete()) {\n+      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n+      if (last !\u003d null) {\n+        if (last.getLocations().length \u003d\u003d 0) {\n+          return -1;\n+        }\n+        final long len \u003d readBlockLength(last);\n+        last.getBlock().setNumBytes(len);\n+        lastBlockBeingWrittenLength \u003d len; \n+      }\n+    }\n+\n+    currentNode \u003d null;\n+    return lastBlockBeingWrittenLength;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {\n    LocatedBlocks newInfo \u003d DFSClient.callGetBlockLocations(dfsClient.namenode, src, 0, prefetchSize);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    long lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        if (last.getLocations().length \u003d\u003d 0) {\n          return -1;\n        }\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    currentNode \u003d null;\n    return lastBlockBeingWrittenLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}