{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PBHelper.java",
  "functionName": "convert",
  "functionId": "convert___blk-BlockWithLocations",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
  "functionStartLine": 192,
  "functionEndLine": 205,
  "numCommitsSeen": 195,
  "timeTaken": 7326,
  "changeHistory": [
    "c09dc258a8f64fab852bf6f26187163480dbee3c",
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
    "673280df24f0228bf01777035ceeab8807da8c40",
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
    "907fb15ee8c150e5ecc0560b7374441c57a84122",
    "eb9f1b670726e1af03f2e940ce2696b880964972",
    "7428aeca8666aeaf5f6682efbdb5349f44d1753e",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9"
  ],
  "changeHistoryShort": {
    "c09dc258a8f64fab852bf6f26187163480dbee3c": "Ybodychange",
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ybodychange",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": "Ybodychange",
    "673280df24f0228bf01777035ceeab8807da8c40": "Ybodychange",
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d": "Ybodychange",
    "907fb15ee8c150e5ecc0560b7374441c57a84122": "Ybodychange",
    "eb9f1b670726e1af03f2e940ce2696b880964972": "Ybodychange",
    "7428aeca8666aeaf5f6682efbdb5349f44d1753e": "Ybodychange",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c09dc258a8f64fab852bf6f26187163480dbee3c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8882. Erasure Coding: Use datablocks, parityblocks and cell size from ErasureCodingPolicy. Contributed by Vinayakumar B.\n\nChange-Id: Ic56da0b426f47c63dac440aef6f5fc8554f6cf13\n",
      "commitDate": "23/09/15 1:34 PM",
      "commitName": "c09dc258a8f64fab852bf6f26187163480dbee3c",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "22/09/15 1:27 PM",
      "commitNameOld": "1080c3730068177ddd10dc313890ac1f5dc58f1a",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,14 @@\n   public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n     BlockWithLocationsProto.Builder builder \u003d BlockWithLocationsProto\n         .newBuilder().setBlock(PBHelperClient.convert(blk.getBlock()))\n         .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n         .addAllStorageUuids(Arrays.asList(blk.getStorageIDs()))\n         .addAllStorageTypes(PBHelperClient.convertStorageTypes(blk.getStorageTypes()));\n     if (blk instanceof StripedBlockWithLocations) {\n       StripedBlockWithLocations sblk \u003d (StripedBlockWithLocations) blk;\n       builder.setIndices(PBHelperClient.getByteString(sblk.getIndices()));\n       builder.setDataBlockNum(sblk.getDataBlockNum());\n+      builder.setCellSize(sblk.getCellSize());\n     }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n    BlockWithLocationsProto.Builder builder \u003d BlockWithLocationsProto\n        .newBuilder().setBlock(PBHelperClient.convert(blk.getBlock()))\n        .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n        .addAllStorageUuids(Arrays.asList(blk.getStorageIDs()))\n        .addAllStorageTypes(PBHelperClient.convertStorageTypes(blk.getStorageTypes()));\n    if (blk instanceof StripedBlockWithLocations) {\n      StripedBlockWithLocations sblk \u003d (StripedBlockWithLocations) blk;\n      builder.setIndices(PBHelperClient.getByteString(sblk.getIndices()));\n      builder.setDataBlockNum(sblk.getDataBlockNum());\n      builder.setCellSize(sblk.getCellSize());\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/09/15 6:08 PM",
      "commitNameOld": "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 2.03,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,8 @@\n   public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n     return BlockWithLocationsProto.newBuilder()\n-        .setBlock(convert(blk.getBlock()))\n+        .setBlock(PBHelperClient.convert(blk.getBlock()))\n         .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n         .addAllStorageUuids(Arrays.asList(blk.getStorageIDs()))\n         .addAllStorageTypes(PBHelperClient.convertStorageTypes(blk.getStorageTypes()))\n         .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n    return BlockWithLocationsProto.newBuilder()\n        .setBlock(PBHelperClient.convert(blk.getBlock()))\n        .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n        .addAllStorageUuids(Arrays.asList(blk.getStorageIDs()))\n        .addAllStorageTypes(PBHelperClient.convertStorageTypes(blk.getStorageTypes()))\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8934. Move ShortCircuitShm to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "22/08/15 1:31 PM",
      "commitName": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "14/07/15 12:42 PM",
      "commitNameOld": "979c9ca2ca89e99dc7165abfa29c78d66de43d9a",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 39.03,
      "commitsBetweenForRepo": 217,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,8 @@\n   public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n     return BlockWithLocationsProto.newBuilder()\n         .setBlock(convert(blk.getBlock()))\n         .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n         .addAllStorageUuids(Arrays.asList(blk.getStorageIDs()))\n-        .addAllStorageTypes(convertStorageTypes(blk.getStorageTypes()))\n+        .addAllStorageTypes(PBHelperClient.convertStorageTypes(blk.getStorageTypes()))\n         .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n    return BlockWithLocationsProto.newBuilder()\n        .setBlock(convert(blk.getBlock()))\n        .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n        .addAllStorageUuids(Arrays.asList(blk.getStorageIDs()))\n        .addAllStorageTypes(PBHelperClient.convertStorageTypes(blk.getStorageTypes()))\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "673280df24f0228bf01777035ceeab8807da8c40": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7621. Erasure Coding: update the Balancer/Mover data migration logic. Contributed by Walter Su.\n",
      "commitDate": "03/06/15 11:51 AM",
      "commitName": "673280df24f0228bf01777035ceeab8807da8c40",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 12:07 PM",
      "commitNameOld": "9a18598e2da8e699ed852ffa30fd7f503902190c",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 7.99,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,13 @@\n   public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n-    return BlockWithLocationsProto.newBuilder()\n-        .setBlock(convert(blk.getBlock()))\n+    BlockWithLocationsProto.Builder builder \u003d BlockWithLocationsProto\n+        .newBuilder().setBlock(convert(blk.getBlock()))\n         .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n         .addAllStorageUuids(Arrays.asList(blk.getStorageIDs()))\n-        .addAllStorageTypes(convertStorageTypes(blk.getStorageTypes()))\n-        .build();\n+        .addAllStorageTypes(convertStorageTypes(blk.getStorageTypes()));\n+    if (blk instanceof StripedBlockWithLocations) {\n+      StripedBlockWithLocations sblk \u003d (StripedBlockWithLocations) blk;\n+      builder.setIndices(getByteString(sblk.getIndices()));\n+      builder.setDataBlockNum(sblk.getDataBlockNum());\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n    BlockWithLocationsProto.Builder builder \u003d BlockWithLocationsProto\n        .newBuilder().setBlock(convert(blk.getBlock()))\n        .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n        .addAllStorageUuids(Arrays.asList(blk.getStorageIDs()))\n        .addAllStorageTypes(convertStorageTypes(blk.getStorageTypes()));\n    if (blk instanceof StripedBlockWithLocations) {\n      StripedBlockWithLocations sblk \u003d (StripedBlockWithLocations) blk;\n      builder.setIndices(getByteString(sblk.getIndices()));\n      builder.setDataBlockNum(sblk.getDataBlockNum());\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6685. Balancer should preserve storage type of replicas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615015 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/07/14 6:05 PM",
      "commitName": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/07/14 10:49 AM",
      "commitNameOld": "535fe14dedbf919442ec03ac573315c7a16a6dbe",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 1.3,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,6 +1,8 @@\n   public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n     return BlockWithLocationsProto.newBuilder()\n         .setBlock(convert(blk.getBlock()))\n         .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n-        .addAllStorageUuids(Arrays.asList(blk.getStorageIDs())).build();\n+        .addAllStorageUuids(Arrays.asList(blk.getStorageIDs()))\n+        .addAllStorageTypes(convertStorageTypes(blk.getStorageTypes()))\n+        .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n    return BlockWithLocationsProto.newBuilder()\n        .setBlock(convert(blk.getBlock()))\n        .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n        .addAllStorageUuids(Arrays.asList(blk.getStorageIDs()))\n        .addAllStorageTypes(convertStorageTypes(blk.getStorageTypes()))\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "907fb15ee8c150e5ecc0560b7374441c57a84122": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5480. Update Balancer for HDFS-2832. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1540547 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/11/13 12:59 PM",
      "commitName": "907fb15ee8c150e5ecc0560b7374441c57a84122",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "07/11/13 5:44 PM",
      "commitNameOld": "6b0611ed24c26ec9428223ae43fc6186374e5148",
      "commitAuthorOld": "",
      "daysBetweenCommits": 2.8,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,6 @@\n   public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n     return BlockWithLocationsProto.newBuilder()\n         .setBlock(convert(blk.getBlock()))\n+        .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n         .addAllStorageUuids(Arrays.asList(blk.getStorageIDs())).build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n    return BlockWithLocationsProto.newBuilder()\n        .setBlock(convert(blk.getBlock()))\n        .addAllDatanodeUuids(Arrays.asList(blk.getDatanodeUuids()))\n        .addAllStorageUuids(Arrays.asList(blk.getStorageIDs())).build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "eb9f1b670726e1af03f2e940ce2696b880964972": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5232. Protocol changes to transmit StorageUuid.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1525153 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/09/13 3:06 PM",
      "commitName": "eb9f1b670726e1af03f2e940ce2696b880964972",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "18/09/13 8:12 AM",
      "commitNameOld": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.29,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,5 @@\n   public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n     return BlockWithLocationsProto.newBuilder()\n         .setBlock(convert(blk.getBlock()))\n-        .addAllStorageIDs(Arrays.asList(blk.getStorageIDs())).build();\n+        .addAllStorageUuids(Arrays.asList(blk.getStorageIDs())).build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n    return BlockWithLocationsProto.newBuilder()\n        .setBlock(convert(blk.getBlock()))\n        .addAllStorageUuids(Arrays.asList(blk.getStorageIDs())).build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "7428aeca8666aeaf5f6682efbdb5349f44d1753e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3418. Rename BlockWithLocationsProto datanodeIDs field to storageIDs. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1338830 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/05/12 12:03 PM",
      "commitName": "7428aeca8666aeaf5f6682efbdb5349f44d1753e",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "11/04/12 8:36 PM",
      "commitNameOld": "7f427646dfe80f9a4dfac0a979709f367e74a7e7",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 33.64,
      "commitsBetweenForRepo": 219,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,5 @@\n   public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n     return BlockWithLocationsProto.newBuilder()\n         .setBlock(convert(blk.getBlock()))\n-        .addAllDatanodeIDs(Arrays.asList(blk.getDatanodes())).build();\n+        .addAllStorageIDs(Arrays.asList(blk.getStorageIDs())).build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n    return BlockWithLocationsProto.newBuilder()\n        .setBlock(convert(blk.getBlock()))\n        .addAllStorageIDs(Arrays.asList(blk.getStorageIDs())).build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2618. Implement protobuf service for NamenodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210719 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/12/11 4:25 PM",
      "commitName": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
      "commitAuthor": "Suresh Srinivas",
      "diff": "@@ -0,0 +1,5 @@\n+  public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n+    return BlockWithLocationsProto.newBuilder()\n+        .setBlock(convert(blk.getBlock()))\n+        .addAllDatanodeIDs(Arrays.asList(blk.getDatanodes())).build();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocationsProto convert(BlockWithLocations blk) {\n    return BlockWithLocationsProto.newBuilder()\n        .setBlock(convert(blk.getBlock()))\n        .addAllDatanodeIDs(Arrays.asList(blk.getDatanodes())).build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java"
    }
  }
}