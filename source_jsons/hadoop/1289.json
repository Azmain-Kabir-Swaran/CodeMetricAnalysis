{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeInfo.java",
  "functionName": "dumpDatanode",
  "functionId": "dumpDatanode",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
  "functionStartLine": 431,
  "functionEndLine": 475,
  "numCommitsSeen": 44,
  "timeTaken": 6637,
  "changeHistory": [
    "fb8932a727f757b2e9c1c61a18145878d0eb77bd",
    "a2774debf71b809f9cd9202c0e75a41a8dd191d0",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
    "0113e4528deda7563b62a29745fbf209ab31b81a",
    "5c97db07fb306842f49d73a67a90cecec19a7833",
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
    "e28edbffe15e9d176d14ea2af8d9460d807b3fc4",
    "e505b7e704ff83893a40190695977ce1393f6248",
    "be7dd8333a7e56e732171db0781786987de03195",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "fb8932a727f757b2e9c1c61a18145878d0eb77bd": "Ybodychange",
    "a2774debf71b809f9cd9202c0e75a41a8dd191d0": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501": "Ybodychange",
    "0113e4528deda7563b62a29745fbf209ab31b81a": "Ybodychange",
    "5c97db07fb306842f49d73a67a90cecec19a7833": "Yfilerename",
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0": "Ybodychange",
    "e28edbffe15e9d176d14ea2af8d9460d807b3fc4": "Ybodychange",
    "e505b7e704ff83893a40190695977ce1393f6248": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "fb8932a727f757b2e9c1c61a18145878d0eb77bd": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16029. Consecutive StringBuilder.append can be reused. Contributed by Ayush Saxena.\n",
      "commitDate": "11/01/19 10:54 AM",
      "commitName": "fb8932a727f757b2e9c1c61a18145878d0eb77bd",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "13/03/18 4:39 PM",
      "commitNameOld": "9714fc1dd48edb1c40d96d69ae82ed3b0fab7748",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 303.8,
      "commitsBetweenForRepo": 2929,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   public String dumpDatanode() {\n     StringBuilder buffer \u003d new StringBuilder();\n     long c \u003d getCapacity();\n     long r \u003d getRemaining();\n     long u \u003d getDfsUsed();\n     float usedPercent \u003d getDfsUsedPercent();\n     long cc \u003d getCacheCapacity();\n     long cr \u003d getCacheRemaining();\n     long cu \u003d getCacheUsed();\n     float cacheUsedPercent \u003d getCacheUsedPercent();\n     buffer.append(getName());\n     if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n       buffer.append(\" \").append(location);\n     }\n     if (upgradeDomain !\u003d null) {\n       buffer.append(\" \").append(upgradeDomain);\n     }\n     if (isDecommissioned()) {\n       buffer.append(\" DD\");\n     } else if (isDecommissionInProgress()) {\n       buffer.append(\" DP\");\n     } else if (isInMaintenance()) {\n       buffer.append(\" IM\");\n     } else if (isEnteringMaintenance()) {\n       buffer.append(\" EM\");\n     } else {\n       buffer.append(\" IN\");\n     }\n     buffer.append(\" \").append(c).append(\"(\").append(StringUtils.byteDesc(c))\n-        .append(\")\");\n-    buffer.append(\" \").append(u).append(\"(\").append(StringUtils.byteDesc(u))\n-        .append(\")\");\n-    buffer.append(\" \").append(percent2String(usedPercent));\n-    buffer.append(\" \").append(r).append(\"(\").append(StringUtils.byteDesc(r))\n-        .append(\")\");\n-    buffer.append(\" \").append(cc).append(\"(\").append(StringUtils.byteDesc(cc))\n-        .append(\")\");\n-    buffer.append(\" \").append(cu).append(\"(\").append(StringUtils.byteDesc(cu))\n-        .append(\")\");\n-    buffer.append(\" \").append(percent2String(cacheUsedPercent));\n-    buffer.append(\" \").append(cr).append(\"(\").append(StringUtils.byteDesc(cr))\n-        .append(\")\");\n-    buffer.append(\" \").append(new Date(lastUpdate));\n+        .append(\")\")\n+        .append(\" \").append(u).append(\"(\").append(StringUtils.byteDesc(u))\n+        .append(\")\")\n+        .append(\" \").append(percent2String(usedPercent))\n+        .append(\" \").append(r).append(\"(\").append(StringUtils.byteDesc(r))\n+        .append(\")\")\n+        .append(\" \").append(cc).append(\"(\").append(StringUtils.byteDesc(cc))\n+        .append(\")\")\n+        .append(\" \").append(cu).append(\"(\").append(StringUtils.byteDesc(cu))\n+        .append(\")\")\n+        .append(\" \").append(percent2String(cacheUsedPercent))\n+        .append(\" \").append(cr).append(\"(\").append(StringUtils.byteDesc(cr))\n+        .append(\")\")\n+        .append(\" \").append(new Date(lastUpdate));\n     return buffer.toString();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    float usedPercent \u003d getDfsUsedPercent();\n    long cc \u003d getCacheCapacity();\n    long cr \u003d getCacheRemaining();\n    long cu \u003d getCacheUsed();\n    float cacheUsedPercent \u003d getCacheUsedPercent();\n    buffer.append(getName());\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \").append(location);\n    }\n    if (upgradeDomain !\u003d null) {\n      buffer.append(\" \").append(upgradeDomain);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else if (isInMaintenance()) {\n      buffer.append(\" IM\");\n    } else if (isEnteringMaintenance()) {\n      buffer.append(\" EM\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \").append(c).append(\"(\").append(StringUtils.byteDesc(c))\n        .append(\")\")\n        .append(\" \").append(u).append(\"(\").append(StringUtils.byteDesc(u))\n        .append(\")\")\n        .append(\" \").append(percent2String(usedPercent))\n        .append(\" \").append(r).append(\"(\").append(StringUtils.byteDesc(r))\n        .append(\")\")\n        .append(\" \").append(cc).append(\"(\").append(StringUtils.byteDesc(cc))\n        .append(\")\")\n        .append(\" \").append(cu).append(\"(\").append(StringUtils.byteDesc(cu))\n        .append(\")\")\n        .append(\" \").append(percent2String(cacheUsedPercent))\n        .append(\" \").append(cr).append(\"(\").append(StringUtils.byteDesc(cr))\n        .append(\")\")\n        .append(\" \").append(new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {}
    },
    "a2774debf71b809f9cd9202c0e75a41a8dd191d0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9389. Add maintenance states to AdminStates. (Ming Ma via lei)\n",
      "commitDate": "12/05/16 3:42 PM",
      "commitName": "a2774debf71b809f9cd9202c0e75a41a8dd191d0",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 222.17,
      "commitsBetweenForRepo": 1465,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,45 @@\n   public String dumpDatanode() {\n     StringBuilder buffer \u003d new StringBuilder();\n     long c \u003d getCapacity();\n     long r \u003d getRemaining();\n     long u \u003d getDfsUsed();\n     float usedPercent \u003d getDfsUsedPercent();\n     long cc \u003d getCacheCapacity();\n     long cr \u003d getCacheRemaining();\n     long cu \u003d getCacheUsed();\n     float cacheUsedPercent \u003d getCacheUsedPercent();\n     buffer.append(getName());\n     if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n       buffer.append(\" \").append(location);\n     }\n     if (upgradeDomain !\u003d null) {\n       buffer.append(\" \").append(upgradeDomain);\n     }\n     if (isDecommissioned()) {\n       buffer.append(\" DD\");\n     } else if (isDecommissionInProgress()) {\n       buffer.append(\" DP\");\n+    } else if (isInMaintenance()) {\n+      buffer.append(\" IM\");\n+    } else if (isEnteringMaintenance()) {\n+      buffer.append(\" EM\");\n     } else {\n       buffer.append(\" IN\");\n     }\n     buffer.append(\" \").append(c).append(\"(\").append(StringUtils.byteDesc(c))\n         .append(\")\");\n     buffer.append(\" \").append(u).append(\"(\").append(StringUtils.byteDesc(u))\n         .append(\")\");\n     buffer.append(\" \").append(percent2String(usedPercent));\n     buffer.append(\" \").append(r).append(\"(\").append(StringUtils.byteDesc(r))\n         .append(\")\");\n     buffer.append(\" \").append(cc).append(\"(\").append(StringUtils.byteDesc(cc))\n         .append(\")\");\n     buffer.append(\" \").append(cu).append(\"(\").append(StringUtils.byteDesc(cu))\n         .append(\")\");\n     buffer.append(\" \").append(percent2String(cacheUsedPercent));\n     buffer.append(\" \").append(cr).append(\"(\").append(StringUtils.byteDesc(cr))\n         .append(\")\");\n     buffer.append(\" \").append(new Date(lastUpdate));\n     return buffer.toString();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    float usedPercent \u003d getDfsUsedPercent();\n    long cc \u003d getCacheCapacity();\n    long cr \u003d getCacheRemaining();\n    long cu \u003d getCacheUsed();\n    float cacheUsedPercent \u003d getCacheUsedPercent();\n    buffer.append(getName());\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \").append(location);\n    }\n    if (upgradeDomain !\u003d null) {\n      buffer.append(\" \").append(upgradeDomain);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else if (isInMaintenance()) {\n      buffer.append(\" IM\");\n    } else if (isEnteringMaintenance()) {\n      buffer.append(\" EM\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \").append(c).append(\"(\").append(StringUtils.byteDesc(c))\n        .append(\")\");\n    buffer.append(\" \").append(u).append(\"(\").append(StringUtils.byteDesc(u))\n        .append(\")\");\n    buffer.append(\" \").append(percent2String(usedPercent));\n    buffer.append(\" \").append(r).append(\"(\").append(StringUtils.byteDesc(r))\n        .append(\")\");\n    buffer.append(\" \").append(cc).append(\"(\").append(StringUtils.byteDesc(cc))\n        .append(\")\");\n    buffer.append(\" \").append(cu).append(\"(\").append(StringUtils.byteDesc(cu))\n        .append(\")\");\n    buffer.append(\" \").append(percent2String(cacheUsedPercent));\n    buffer.append(\" \").append(cr).append(\"(\").append(StringUtils.byteDesc(cr))\n        .append(\")\");\n    buffer.append(\" \").append(new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/09/15 6:08 PM",
      "commitNameOld": "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 13.73,
      "commitsBetweenForRepo": 101,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,41 @@\n   public String dumpDatanode() {\n     StringBuilder buffer \u003d new StringBuilder();\n     long c \u003d getCapacity();\n     long r \u003d getRemaining();\n     long u \u003d getDfsUsed();\n     float usedPercent \u003d getDfsUsedPercent();\n     long cc \u003d getCacheCapacity();\n     long cr \u003d getCacheRemaining();\n     long cu \u003d getCacheUsed();\n     float cacheUsedPercent \u003d getCacheUsedPercent();\n     buffer.append(getName());\n     if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n-      buffer.append(\" \"+location);\n+      buffer.append(\" \").append(location);\n     }\n     if (upgradeDomain !\u003d null) {\n-      buffer.append(\" \" + upgradeDomain);\n+      buffer.append(\" \").append(upgradeDomain);\n     }\n     if (isDecommissioned()) {\n       buffer.append(\" DD\");\n     } else if (isDecommissionInProgress()) {\n       buffer.append(\" DP\");\n     } else {\n       buffer.append(\" IN\");\n     }\n-    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n-    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n-    buffer.append(\" \" + percent2String(usedPercent));\n-    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n-    buffer.append(\" \" + cc + \"(\" + StringUtils.byteDesc(cc)+\")\");\n-    buffer.append(\" \" + cu + \"(\" + StringUtils.byteDesc(cu)+\")\");\n-    buffer.append(\" \" + percent2String(cacheUsedPercent));\n-    buffer.append(\" \" + cr + \"(\" + StringUtils.byteDesc(cr)+\")\");\n-    buffer.append(\" \" + new Date(lastUpdate));\n+    buffer.append(\" \").append(c).append(\"(\").append(StringUtils.byteDesc(c))\n+        .append(\")\");\n+    buffer.append(\" \").append(u).append(\"(\").append(StringUtils.byteDesc(u))\n+        .append(\")\");\n+    buffer.append(\" \").append(percent2String(usedPercent));\n+    buffer.append(\" \").append(r).append(\"(\").append(StringUtils.byteDesc(r))\n+        .append(\")\");\n+    buffer.append(\" \").append(cc).append(\"(\").append(StringUtils.byteDesc(cc))\n+        .append(\")\");\n+    buffer.append(\" \").append(cu).append(\"(\").append(StringUtils.byteDesc(cu))\n+        .append(\")\");\n+    buffer.append(\" \").append(percent2String(cacheUsedPercent));\n+    buffer.append(\" \").append(cr).append(\"(\").append(StringUtils.byteDesc(cr))\n+        .append(\")\");\n+    buffer.append(\" \").append(new Date(lastUpdate));\n     return buffer.toString();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    float usedPercent \u003d getDfsUsedPercent();\n    long cc \u003d getCacheCapacity();\n    long cr \u003d getCacheRemaining();\n    long cu \u003d getCacheUsed();\n    float cacheUsedPercent \u003d getCacheUsedPercent();\n    buffer.append(getName());\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \").append(location);\n    }\n    if (upgradeDomain !\u003d null) {\n      buffer.append(\" \").append(upgradeDomain);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \").append(c).append(\"(\").append(StringUtils.byteDesc(c))\n        .append(\")\");\n    buffer.append(\" \").append(u).append(\"(\").append(StringUtils.byteDesc(u))\n        .append(\")\");\n    buffer.append(\" \").append(percent2String(usedPercent));\n    buffer.append(\" \").append(r).append(\"(\").append(StringUtils.byteDesc(r))\n        .append(\")\");\n    buffer.append(\" \").append(cc).append(\"(\").append(StringUtils.byteDesc(cc))\n        .append(\")\");\n    buffer.append(\" \").append(cu).append(\"(\").append(StringUtils.byteDesc(cu))\n        .append(\")\");\n    buffer.append(\" \").append(percent2String(cacheUsedPercent));\n    buffer.append(\" \").append(cr).append(\"(\").append(StringUtils.byteDesc(cr))\n        .append(\")\");\n    buffer.append(\" \").append(new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {}
    },
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9004. Add upgrade domain to DatanodeInfo. Contributed by Ming Ma (via Lei (Eddy) Xu).\n\nChange-Id: I887c66578eebd61acc34b94f18da6e6851c609f4\n",
      "commitDate": "19/09/15 6:08 PM",
      "commitName": "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "09/09/15 1:20 AM",
      "commitNameOld": "0113e4528deda7563b62a29745fbf209ab31b81a",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 10.7,
      "commitsBetweenForRepo": 80,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,35 @@\n   public String dumpDatanode() {\n     StringBuilder buffer \u003d new StringBuilder();\n     long c \u003d getCapacity();\n     long r \u003d getRemaining();\n     long u \u003d getDfsUsed();\n     float usedPercent \u003d getDfsUsedPercent();\n     long cc \u003d getCacheCapacity();\n     long cr \u003d getCacheRemaining();\n     long cu \u003d getCacheUsed();\n     float cacheUsedPercent \u003d getCacheUsedPercent();\n     buffer.append(getName());\n     if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n       buffer.append(\" \"+location);\n     }\n+    if (upgradeDomain !\u003d null) {\n+      buffer.append(\" \" + upgradeDomain);\n+    }\n     if (isDecommissioned()) {\n       buffer.append(\" DD\");\n     } else if (isDecommissionInProgress()) {\n       buffer.append(\" DP\");\n     } else {\n       buffer.append(\" IN\");\n     }\n     buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n     buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n     buffer.append(\" \" + percent2String(usedPercent));\n     buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n     buffer.append(\" \" + cc + \"(\" + StringUtils.byteDesc(cc)+\")\");\n     buffer.append(\" \" + cu + \"(\" + StringUtils.byteDesc(cu)+\")\");\n     buffer.append(\" \" + percent2String(cacheUsedPercent));\n     buffer.append(\" \" + cr + \"(\" + StringUtils.byteDesc(cr)+\")\");\n     buffer.append(\" \" + new Date(lastUpdate));\n     return buffer.toString();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    float usedPercent \u003d getDfsUsedPercent();\n    long cc \u003d getCacheCapacity();\n    long cr \u003d getCacheRemaining();\n    long cu \u003d getCacheUsed();\n    float cacheUsedPercent \u003d getCacheUsedPercent();\n    buffer.append(getName());\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \"+location);\n    }\n    if (upgradeDomain !\u003d null) {\n      buffer.append(\" \" + upgradeDomain);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n    buffer.append(\" \" + percent2String(usedPercent));\n    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n    buffer.append(\" \" + cc + \"(\" + StringUtils.byteDesc(cc)+\")\");\n    buffer.append(\" \" + cu + \"(\" + StringUtils.byteDesc(cu)+\")\");\n    buffer.append(\" \" + percent2String(cacheUsedPercent));\n    buffer.append(\" \" + cr + \"(\" + StringUtils.byteDesc(cr)+\")\");\n    buffer.append(\" \" + new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {}
    },
    "0113e4528deda7563b62a29745fbf209ab31b81a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9033. dfsadmin -metasave prints \"NaN\" for cache used%. Contributed by Brahma Reddy Battula.\n",
      "commitDate": "09/09/15 1:20 AM",
      "commitName": "0113e4528deda7563b62a29745fbf209ab31b81a",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "20/04/15 12:36 AM",
      "commitNameOld": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 142.03,
      "commitsBetweenForRepo": 1038,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,32 @@\n   public String dumpDatanode() {\n     StringBuilder buffer \u003d new StringBuilder();\n     long c \u003d getCapacity();\n     long r \u003d getRemaining();\n     long u \u003d getDfsUsed();\n+    float usedPercent \u003d getDfsUsedPercent();\n     long cc \u003d getCacheCapacity();\n     long cr \u003d getCacheRemaining();\n     long cu \u003d getCacheUsed();\n+    float cacheUsedPercent \u003d getCacheUsedPercent();\n     buffer.append(getName());\n     if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n       buffer.append(\" \"+location);\n     }\n     if (isDecommissioned()) {\n       buffer.append(\" DD\");\n     } else if (isDecommissionInProgress()) {\n       buffer.append(\" DP\");\n     } else {\n       buffer.append(\" IN\");\n     }\n     buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n     buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n-    buffer.append(\" \" + percent2String(u/(double)c));\n+    buffer.append(\" \" + percent2String(usedPercent));\n     buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n     buffer.append(\" \" + cc + \"(\" + StringUtils.byteDesc(cc)+\")\");\n     buffer.append(\" \" + cu + \"(\" + StringUtils.byteDesc(cu)+\")\");\n-    buffer.append(\" \" + percent2String(cu/(double)cc));\n+    buffer.append(\" \" + percent2String(cacheUsedPercent));\n     buffer.append(\" \" + cr + \"(\" + StringUtils.byteDesc(cr)+\")\");\n     buffer.append(\" \" + new Date(lastUpdate));\n     return buffer.toString();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    float usedPercent \u003d getDfsUsedPercent();\n    long cc \u003d getCacheCapacity();\n    long cr \u003d getCacheRemaining();\n    long cu \u003d getCacheUsed();\n    float cacheUsedPercent \u003d getCacheUsedPercent();\n    buffer.append(getName());\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \"+location);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n    buffer.append(\" \" + percent2String(usedPercent));\n    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n    buffer.append(\" \" + cc + \"(\" + StringUtils.byteDesc(cc)+\")\");\n    buffer.append(\" \" + cu + \"(\" + StringUtils.byteDesc(cu)+\")\");\n    buffer.append(\" \" + percent2String(cacheUsedPercent));\n    buffer.append(\" \" + cr + \"(\" + StringUtils.byteDesc(cr)+\")\");\n    buffer.append(\" \" + new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {}
    },
    "5c97db07fb306842f49d73a67a90cecec19a7833": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8169. Move LocatedBlocks and related classes to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "20/04/15 12:36 AM",
      "commitName": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/04/15 4:09 PM",
      "commitNameOld": "8511d80804de052618168a456a475ee0f7aa6d8c",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.35,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    long cc \u003d getCacheCapacity();\n    long cr \u003d getCacheRemaining();\n    long cu \u003d getCacheUsed();\n    buffer.append(getName());\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \"+location);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n    buffer.append(\" \" + percent2String(u/(double)c));\n    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n    buffer.append(\" \" + cc + \"(\" + StringUtils.byteDesc(cc)+\")\");\n    buffer.append(\" \" + cu + \"(\" + StringUtils.byteDesc(cu)+\")\");\n    buffer.append(\" \" + percent2String(cu/(double)cc));\n    buffer.append(\" \" + cr + \"(\" + StringUtils.byteDesc(cr)+\")\");\n    buffer.append(\" \" + new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java"
      }
    },
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5141. Add cache status information to datanode heartbeat. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1519101 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/08/13 3:15 PM",
      "commitName": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "21/07/13 10:28 PM",
      "commitNameOld": "febcf4d66969521d33762eec3c9a884ab77f8162",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 39.7,
      "commitsBetweenForRepo": 158,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,30 @@\n   public String dumpDatanode() {\n     StringBuilder buffer \u003d new StringBuilder();\n     long c \u003d getCapacity();\n     long r \u003d getRemaining();\n     long u \u003d getDfsUsed();\n+    long cc \u003d getCacheCapacity();\n+    long cr \u003d getCacheRemaining();\n+    long cu \u003d getCacheUsed();\n     buffer.append(getName());\n     if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n       buffer.append(\" \"+location);\n     }\n     if (isDecommissioned()) {\n       buffer.append(\" DD\");\n     } else if (isDecommissionInProgress()) {\n       buffer.append(\" DP\");\n     } else {\n       buffer.append(\" IN\");\n     }\n     buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n     buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n     buffer.append(\" \" + percent2String(u/(double)c));\n     buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n+    buffer.append(\" \" + cc + \"(\" + StringUtils.byteDesc(cc)+\")\");\n+    buffer.append(\" \" + cu + \"(\" + StringUtils.byteDesc(cu)+\")\");\n+    buffer.append(\" \" + percent2String(cu/(double)cc));\n+    buffer.append(\" \" + cr + \"(\" + StringUtils.byteDesc(cr)+\")\");\n     buffer.append(\" \" + new Date(lastUpdate));\n     return buffer.toString();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    long cc \u003d getCacheCapacity();\n    long cr \u003d getCacheRemaining();\n    long cu \u003d getCacheUsed();\n    buffer.append(getName());\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \"+location);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n    buffer.append(\" \" + percent2String(u/(double)c));\n    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n    buffer.append(\" \" + cc + \"(\" + StringUtils.byteDesc(cc)+\")\");\n    buffer.append(\" \" + cu + \"(\" + StringUtils.byteDesc(cu)+\")\");\n    buffer.append(\" \" + percent2String(cu/(double)cc));\n    buffer.append(\" \" + cr + \"(\" + StringUtils.byteDesc(cr)+\")\");\n    buffer.append(\" \" + new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {}
    },
    "e28edbffe15e9d176d14ea2af8d9460d807b3fc4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4468.  Use the new StringUtils methods added by HADOOP-9252 and fix TestHDFSCLI and TestQuota.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1442824 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/02/13 5:13 PM",
      "commitName": "e28edbffe15e9d176d14ea2af8d9460d807b3fc4",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "17/10/12 2:34 PM",
      "commitNameOld": "4d5600f6c714732d16bed29f0bc210eb72901545",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 111.15,
      "commitsBetweenForRepo": 514,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public String dumpDatanode() {\n     StringBuilder buffer \u003d new StringBuilder();\n     long c \u003d getCapacity();\n     long r \u003d getRemaining();\n     long u \u003d getDfsUsed();\n     buffer.append(getName());\n     if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n       buffer.append(\" \"+location);\n     }\n     if (isDecommissioned()) {\n       buffer.append(\" DD\");\n     } else if (isDecommissionInProgress()) {\n       buffer.append(\" DP\");\n     } else {\n       buffer.append(\" IN\");\n     }\n     buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n     buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n-    buffer.append(\" \" + StringUtils.limitDecimalTo2(((1.0*u)/c)*100)+\"%\");\n+    buffer.append(\" \" + percent2String(u/(double)c));\n     buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n     buffer.append(\" \" + new Date(lastUpdate));\n     return buffer.toString();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    buffer.append(getName());\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \"+location);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n    buffer.append(\" \" + percent2String(u/(double)c));\n    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n    buffer.append(\" \" + new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {}
    },
    "e505b7e704ff83893a40190695977ce1393f6248": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3208. Bogus entries in hosts files are incorrectly displayed in the report. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1310138 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/04/12 5:20 PM",
      "commitName": "e505b7e704ff83893a40190695977ce1393f6248",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "05/04/12 5:10 PM",
      "commitNameOld": "112c32415175f637a2791f2207c20393fc9ba740",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public String dumpDatanode() {\n     StringBuilder buffer \u003d new StringBuilder();\n     long c \u003d getCapacity();\n     long r \u003d getRemaining();\n     long u \u003d getDfsUsed();\n-    buffer.append(ipAddr);\n+    buffer.append(getName());\n     if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n       buffer.append(\" \"+location);\n     }\n     if (isDecommissioned()) {\n       buffer.append(\" DD\");\n     } else if (isDecommissionInProgress()) {\n       buffer.append(\" DP\");\n     } else {\n       buffer.append(\" IN\");\n     }\n     buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n     buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n     buffer.append(\" \" + StringUtils.limitDecimalTo2(((1.0*u)/c)*100)+\"%\");\n     buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n     buffer.append(\" \" + new Date(lastUpdate));\n     return buffer.toString();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    buffer.append(getName());\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \"+location);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n    buffer.append(\" \" + StringUtils.limitDecimalTo2(((1.0*u)/c)*100)+\"%\");\n    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n    buffer.append(\" \" + new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "31/03/12 12:58 PM",
      "commitNameOld": "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.09,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public String dumpDatanode() {\n     StringBuilder buffer \u003d new StringBuilder();\n     long c \u003d getCapacity();\n     long r \u003d getRemaining();\n     long u \u003d getDfsUsed();\n-    buffer.append(name);\n+    buffer.append(ipAddr);\n     if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n       buffer.append(\" \"+location);\n     }\n     if (isDecommissioned()) {\n       buffer.append(\" DD\");\n     } else if (isDecommissionInProgress()) {\n       buffer.append(\" DP\");\n     } else {\n       buffer.append(\" IN\");\n     }\n     buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n     buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n     buffer.append(\" \" + StringUtils.limitDecimalTo2(((1.0*u)/c)*100)+\"%\");\n     buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n     buffer.append(\" \" + new Date(lastUpdate));\n     return buffer.toString();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    buffer.append(ipAddr);\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \"+location);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n    buffer.append(\" \" + StringUtils.limitDecimalTo2(((1.0*u)/c)*100)+\"%\");\n    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n    buffer.append(\" \" + new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    buffer.append(name);\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \"+location);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n    buffer.append(\" \" + StringUtils.limitDecimalTo2(((1.0*u)/c)*100)+\"%\");\n    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n    buffer.append(\" \" + new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    buffer.append(name);\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \"+location);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n    buffer.append(\" \" + StringUtils.limitDecimalTo2(((1.0*u)/c)*100)+\"%\");\n    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n    buffer.append(\" \" + new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,23 @@\n+  public String dumpDatanode() {\n+    StringBuilder buffer \u003d new StringBuilder();\n+    long c \u003d getCapacity();\n+    long r \u003d getRemaining();\n+    long u \u003d getDfsUsed();\n+    buffer.append(name);\n+    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n+      buffer.append(\" \"+location);\n+    }\n+    if (isDecommissioned()) {\n+      buffer.append(\" DD\");\n+    } else if (isDecommissionInProgress()) {\n+      buffer.append(\" DP\");\n+    } else {\n+      buffer.append(\" IN\");\n+    }\n+    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n+    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n+    buffer.append(\" \" + StringUtils.limitDecimalTo2(((1.0*u)/c)*100)+\"%\");\n+    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n+    buffer.append(\" \" + new Date(lastUpdate));\n+    return buffer.toString();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public String dumpDatanode() {\n    StringBuilder buffer \u003d new StringBuilder();\n    long c \u003d getCapacity();\n    long r \u003d getRemaining();\n    long u \u003d getDfsUsed();\n    buffer.append(name);\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\n      buffer.append(\" \"+location);\n    }\n    if (isDecommissioned()) {\n      buffer.append(\" DD\");\n    } else if (isDecommissionInProgress()) {\n      buffer.append(\" DP\");\n    } else {\n      buffer.append(\" IN\");\n    }\n    buffer.append(\" \" + c + \"(\" + StringUtils.byteDesc(c)+\")\");\n    buffer.append(\" \" + u + \"(\" + StringUtils.byteDesc(u)+\")\");\n    buffer.append(\" \" + StringUtils.limitDecimalTo2(((1.0*u)/c)*100)+\"%\");\n    buffer.append(\" \" + r + \"(\" + StringUtils.byteDesc(r)+\")\");\n    buffer.append(\" \" + new Date(lastUpdate));\n    return buffer.toString();\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java"
    }
  }
}