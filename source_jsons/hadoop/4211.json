{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StripedBlockUtil.java",
  "functionName": "copyFrom",
  "functionId": "copyFrom___src-ByteBuffer",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
  "functionStartLine": 915,
  "functionEndLine": 925,
  "numCommitsSeen": 26,
  "timeTaken": 1431,
  "changeHistory": [
    "734d54c1a8950446e68098f62d8964e02ecc2890",
    "401db4fc65140979fe7665983e36905e886df971"
  ],
  "changeHistoryShort": {
    "734d54c1a8950446e68098f62d8964e02ecc2890": "Ymodifierchange",
    "401db4fc65140979fe7665983e36905e886df971": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "734d54c1a8950446e68098f62d8964e02ecc2890": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
      "commitDate": "21/09/16 6:34 AM",
      "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "08/09/16 11:54 AM",
      "commitNameOld": "401db4fc65140979fe7665983e36905e886df971",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 12.78,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,11 @@\n-    void copyFrom(ByteBuffer src) {\n+    public void copyFrom(ByteBuffer src) {\n       ByteBuffer tmp;\n       int len;\n       for (ByteBuffer slice : slices) {\n         len \u003d slice.remaining();\n         tmp \u003d src.duplicate();\n         tmp.limit(tmp.position() + len);\n         slice.put(tmp);\n         src.position(src.position() + len);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void copyFrom(ByteBuffer src) {\n      ByteBuffer tmp;\n      int len;\n      for (ByteBuffer slice : slices) {\n        len \u003d slice.remaining();\n        tmp \u003d src.duplicate();\n        tmp.limit(tmp.position() + len);\n        slice.put(tmp);\n        src.position(src.position() + len);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[public]"
      }
    },
    "401db4fc65140979fe7665983e36905e886df971": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
      "commitDate": "08/09/16 11:54 AM",
      "commitName": "401db4fc65140979fe7665983e36905e886df971",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
          "commitDate": "08/09/16 11:54 AM",
          "commitName": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "06/04/16 10:50 PM",
          "commitNameOld": "3c18a53cbd2efabb2ad108d63a0b0b558424115f",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 154.54,
          "commitsBetweenForRepo": 1132,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,11 @@\n-    void copyFrom(byte[] src) {\n-      int srcPos \u003d 0;\n-      for (int j \u003d 0; j \u003c offsetsInBuf.size(); j++) {\n-        System.arraycopy(src, srcPos, buf, offsetsInBuf.get(j),\n-            lengthsInBuf.get(j));\n-        srcPos +\u003d lengthsInBuf.get(j);\n+    void copyFrom(ByteBuffer src) {\n+      ByteBuffer tmp;\n+      int len;\n+      for (ByteBuffer slice : slices) {\n+        len \u003d slice.remaining();\n+        tmp \u003d src.duplicate();\n+        tmp.limit(tmp.position() + len);\n+        slice.put(tmp);\n+        src.position(src.position() + len);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    void copyFrom(ByteBuffer src) {\n      ByteBuffer tmp;\n      int len;\n      for (ByteBuffer slice : slices) {\n        len \u003d slice.remaining();\n        tmp \u003d src.duplicate();\n        tmp.limit(tmp.position() + len);\n        slice.put(tmp);\n        src.position(src.position() + len);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
          "extendedDetails": {
            "oldValue": "[src-byte[]]",
            "newValue": "[src-ByteBuffer]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
          "commitDate": "08/09/16 11:54 AM",
          "commitName": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "06/04/16 10:50 PM",
          "commitNameOld": "3c18a53cbd2efabb2ad108d63a0b0b558424115f",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 154.54,
          "commitsBetweenForRepo": 1132,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,11 @@\n-    void copyFrom(byte[] src) {\n-      int srcPos \u003d 0;\n-      for (int j \u003d 0; j \u003c offsetsInBuf.size(); j++) {\n-        System.arraycopy(src, srcPos, buf, offsetsInBuf.get(j),\n-            lengthsInBuf.get(j));\n-        srcPos +\u003d lengthsInBuf.get(j);\n+    void copyFrom(ByteBuffer src) {\n+      ByteBuffer tmp;\n+      int len;\n+      for (ByteBuffer slice : slices) {\n+        len \u003d slice.remaining();\n+        tmp \u003d src.duplicate();\n+        tmp.limit(tmp.position() + len);\n+        slice.put(tmp);\n+        src.position(src.position() + len);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    void copyFrom(ByteBuffer src) {\n      ByteBuffer tmp;\n      int len;\n      for (ByteBuffer slice : slices) {\n        len \u003d slice.remaining();\n        tmp \u003d src.duplicate();\n        tmp.limit(tmp.position() + len);\n        slice.put(tmp);\n        src.position(src.position() + len);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}