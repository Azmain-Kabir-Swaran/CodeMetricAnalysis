{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataNodeDiskMetrics.java",
  "functionName": "detectAndUpdateDiskOutliers",
  "functionId": "detectAndUpdateDiskOutliers___metadataOpStats-Map__String,Double____readIoStats-Map__String,Double____writeIoStats-Map__String,Double__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java",
  "functionStartLine": 128,
  "functionEndLine": 156,
  "numCommitsSeen": 5,
  "timeTaken": 2374,
  "changeHistory": [
    "41e18feda3f5ff924c87c4bed5b5cbbaecb19ae1",
    "28cdc5a8dc37ade1f45bda3aede589ee8593945e",
    "e7c8da614c37e36fb8081234f4c639d6054f6082",
    "b3ec531f400dd0a6506dc71233d38ae57b764a43"
  ],
  "changeHistoryShort": {
    "41e18feda3f5ff924c87c4bed5b5cbbaecb19ae1": "Ybodychange",
    "28cdc5a8dc37ade1f45bda3aede589ee8593945e": "Ybodychange",
    "e7c8da614c37e36fb8081234f4c639d6054f6082": "Ybodychange",
    "b3ec531f400dd0a6506dc71233d38ae57b764a43": "Yintroduced"
  },
  "changeHistoryDetails": {
    "41e18feda3f5ff924c87c4bed5b5cbbaecb19ae1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14235. Handle ArrayIndexOutOfBoundsException in DataNodeDiskMetrics#slowDiskDetectionDaemon. Contributed by Ranith Sardar.\n",
      "commitDate": "20/02/19 3:26 AM",
      "commitName": "41e18feda3f5ff924c87c4bed5b5cbbaecb19ae1",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "30/03/17 10:41 PM",
      "commitNameOld": "28cdc5a8dc37ade1f45bda3aede589ee8593945e",
      "commitAuthorOld": "Hanisha Koneru",
      "daysBetweenCommits": 691.24,
      "commitsBetweenForRepo": 5538,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,29 @@\n   private void detectAndUpdateDiskOutliers(Map\u003cString, Double\u003e metadataOpStats,\n       Map\u003cString, Double\u003e readIoStats, Map\u003cString, Double\u003e writeIoStats) {\n     Map\u003cString, Map\u003cDiskOp, Double\u003e\u003e diskStats \u003d Maps.newHashMap();\n \n     // Get MetadataOp Outliers\n     Map\u003cString, Double\u003e metadataOpOutliers \u003d slowDiskDetector\n         .getOutliers(metadataOpStats);\n     for (Map.Entry\u003cString, Double\u003e entry : metadataOpOutliers.entrySet()) {\n       addDiskStat(diskStats, entry.getKey(), DiskOp.METADATA, entry.getValue());\n     }\n \n     // Get ReadIo Outliers\n     Map\u003cString, Double\u003e readIoOutliers \u003d slowDiskDetector\n         .getOutliers(readIoStats);\n     for (Map.Entry\u003cString, Double\u003e entry : readIoOutliers.entrySet()) {\n       addDiskStat(diskStats, entry.getKey(), DiskOp.READ, entry.getValue());\n     }\n \n     // Get WriteIo Outliers\n     Map\u003cString, Double\u003e writeIoOutliers \u003d slowDiskDetector\n         .getOutliers(writeIoStats);\n     for (Map.Entry\u003cString, Double\u003e entry : writeIoOutliers.entrySet()) {\n       addDiskStat(diskStats, entry.getKey(), DiskOp.WRITE, entry.getValue());\n     }\n-\n-    diskOutliersStats \u003d diskStats;\n-    LOG.debug(\"Updated disk outliers.\");\n+    if (overrideStatus) {\n+      diskOutliersStats \u003d diskStats;\n+      LOG.debug(\"Updated disk outliers.\");\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void detectAndUpdateDiskOutliers(Map\u003cString, Double\u003e metadataOpStats,\n      Map\u003cString, Double\u003e readIoStats, Map\u003cString, Double\u003e writeIoStats) {\n    Map\u003cString, Map\u003cDiskOp, Double\u003e\u003e diskStats \u003d Maps.newHashMap();\n\n    // Get MetadataOp Outliers\n    Map\u003cString, Double\u003e metadataOpOutliers \u003d slowDiskDetector\n        .getOutliers(metadataOpStats);\n    for (Map.Entry\u003cString, Double\u003e entry : metadataOpOutliers.entrySet()) {\n      addDiskStat(diskStats, entry.getKey(), DiskOp.METADATA, entry.getValue());\n    }\n\n    // Get ReadIo Outliers\n    Map\u003cString, Double\u003e readIoOutliers \u003d slowDiskDetector\n        .getOutliers(readIoStats);\n    for (Map.Entry\u003cString, Double\u003e entry : readIoOutliers.entrySet()) {\n      addDiskStat(diskStats, entry.getKey(), DiskOp.READ, entry.getValue());\n    }\n\n    // Get WriteIo Outliers\n    Map\u003cString, Double\u003e writeIoOutliers \u003d slowDiskDetector\n        .getOutliers(writeIoStats);\n    for (Map.Entry\u003cString, Double\u003e entry : writeIoOutliers.entrySet()) {\n      addDiskStat(diskStats, entry.getKey(), DiskOp.WRITE, entry.getValue());\n    }\n    if (overrideStatus) {\n      diskOutliersStats \u003d diskStats;\n      LOG.debug(\"Updated disk outliers.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java",
      "extendedDetails": {}
    },
    "28cdc5a8dc37ade1f45bda3aede589ee8593945e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11551. Handle SlowDiskReport from DataNode at the NameNode. Contributed by Hanisha Koneru.\n",
      "commitDate": "30/03/17 10:41 PM",
      "commitName": "28cdc5a8dc37ade1f45bda3aede589ee8593945e",
      "commitAuthor": "Hanisha Koneru",
      "commitDateOld": "20/03/17 9:54 PM",
      "commitNameOld": "e7c8da614c37e36fb8081234f4c639d6054f6082",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 10.03,
      "commitsBetweenForRepo": 73,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,28 @@\n   private void detectAndUpdateDiskOutliers(Map\u003cString, Double\u003e metadataOpStats,\n       Map\u003cString, Double\u003e readIoStats, Map\u003cString, Double\u003e writeIoStats) {\n-    Set\u003cString\u003e diskOutliersSet \u003d Sets.newHashSet();\n+    Map\u003cString, Map\u003cDiskOp, Double\u003e\u003e diskStats \u003d Maps.newHashMap();\n \n     // Get MetadataOp Outliers\n     Map\u003cString, Double\u003e metadataOpOutliers \u003d slowDiskDetector\n         .getOutliers(metadataOpStats);\n-    if (!metadataOpOutliers.isEmpty()) {\n-      diskOutliersSet.addAll(metadataOpOutliers.keySet());\n+    for (Map.Entry\u003cString, Double\u003e entry : metadataOpOutliers.entrySet()) {\n+      addDiskStat(diskStats, entry.getKey(), DiskOp.METADATA, entry.getValue());\n     }\n \n     // Get ReadIo Outliers\n     Map\u003cString, Double\u003e readIoOutliers \u003d slowDiskDetector\n         .getOutliers(readIoStats);\n-    if (!readIoOutliers.isEmpty()) {\n-      diskOutliersSet.addAll(readIoOutliers.keySet());\n+    for (Map.Entry\u003cString, Double\u003e entry : readIoOutliers.entrySet()) {\n+      addDiskStat(diskStats, entry.getKey(), DiskOp.READ, entry.getValue());\n     }\n \n     // Get WriteIo Outliers\n     Map\u003cString, Double\u003e writeIoOutliers \u003d slowDiskDetector\n         .getOutliers(writeIoStats);\n-    if (!readIoOutliers.isEmpty()) {\n-      diskOutliersSet.addAll(writeIoOutliers.keySet());\n-    }\n-\n-    Map\u003cString, Map\u003cDiskOp, Double\u003e\u003e diskStats \u003d\n-        Maps.newHashMap();\n-    for (String disk : diskOutliersSet) {\n-      Map\u003cDiskOp, Double\u003e diskStat \u003d Maps.newHashMap();\n-      diskStat.put(DiskOp.METADATA, metadataOpStats.get(disk));\n-      diskStat.put(DiskOp.READ, readIoStats.get(disk));\n-      diskStat.put(DiskOp.WRITE, writeIoStats.get(disk));\n-      diskStats.put(disk, diskStat);\n+    for (Map.Entry\u003cString, Double\u003e entry : writeIoOutliers.entrySet()) {\n+      addDiskStat(diskStats, entry.getKey(), DiskOp.WRITE, entry.getValue());\n     }\n \n     diskOutliersStats \u003d diskStats;\n     LOG.debug(\"Updated disk outliers.\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void detectAndUpdateDiskOutliers(Map\u003cString, Double\u003e metadataOpStats,\n      Map\u003cString, Double\u003e readIoStats, Map\u003cString, Double\u003e writeIoStats) {\n    Map\u003cString, Map\u003cDiskOp, Double\u003e\u003e diskStats \u003d Maps.newHashMap();\n\n    // Get MetadataOp Outliers\n    Map\u003cString, Double\u003e metadataOpOutliers \u003d slowDiskDetector\n        .getOutliers(metadataOpStats);\n    for (Map.Entry\u003cString, Double\u003e entry : metadataOpOutliers.entrySet()) {\n      addDiskStat(diskStats, entry.getKey(), DiskOp.METADATA, entry.getValue());\n    }\n\n    // Get ReadIo Outliers\n    Map\u003cString, Double\u003e readIoOutliers \u003d slowDiskDetector\n        .getOutliers(readIoStats);\n    for (Map.Entry\u003cString, Double\u003e entry : readIoOutliers.entrySet()) {\n      addDiskStat(diskStats, entry.getKey(), DiskOp.READ, entry.getValue());\n    }\n\n    // Get WriteIo Outliers\n    Map\u003cString, Double\u003e writeIoOutliers \u003d slowDiskDetector\n        .getOutliers(writeIoStats);\n    for (Map.Entry\u003cString, Double\u003e entry : writeIoOutliers.entrySet()) {\n      addDiskStat(diskStats, entry.getKey(), DiskOp.WRITE, entry.getValue());\n    }\n\n    diskOutliersStats \u003d diskStats;\n    LOG.debug(\"Updated disk outliers.\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java",
      "extendedDetails": {}
    },
    "e7c8da614c37e36fb8081234f4c639d6054f6082": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11545. Propagate DataNode\u0027s slow disks info to the NameNode via Heartbeats. Contributed by Hanisha Koneru.\n",
      "commitDate": "20/03/17 9:54 PM",
      "commitName": "e7c8da614c37e36fb8081234f4c639d6054f6082",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "17/03/17 3:42 PM",
      "commitNameOld": "7f8e928400db954cfe37a3bd35762e1b310dcb8e",
      "commitAuthorOld": "Hanisha Koneru",
      "daysBetweenCommits": 3.26,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   private void detectAndUpdateDiskOutliers(Map\u003cString, Double\u003e metadataOpStats,\n       Map\u003cString, Double\u003e readIoStats, Map\u003cString, Double\u003e writeIoStats) {\n     Set\u003cString\u003e diskOutliersSet \u003d Sets.newHashSet();\n \n     // Get MetadataOp Outliers\n     Map\u003cString, Double\u003e metadataOpOutliers \u003d slowDiskDetector\n         .getOutliers(metadataOpStats);\n     if (!metadataOpOutliers.isEmpty()) {\n       diskOutliersSet.addAll(metadataOpOutliers.keySet());\n     }\n \n     // Get ReadIo Outliers\n     Map\u003cString, Double\u003e readIoOutliers \u003d slowDiskDetector\n         .getOutliers(readIoStats);\n     if (!readIoOutliers.isEmpty()) {\n       diskOutliersSet.addAll(readIoOutliers.keySet());\n     }\n \n     // Get WriteIo Outliers\n     Map\u003cString, Double\u003e writeIoOutliers \u003d slowDiskDetector\n         .getOutliers(writeIoStats);\n     if (!readIoOutliers.isEmpty()) {\n       diskOutliersSet.addAll(writeIoOutliers.keySet());\n     }\n \n-    Map\u003cString, Map\u003cDiskOutlierDetectionOp, Double\u003e\u003e diskStats \u003d\n+    Map\u003cString, Map\u003cDiskOp, Double\u003e\u003e diskStats \u003d\n         Maps.newHashMap();\n     for (String disk : diskOutliersSet) {\n-      Map\u003cDiskOutlierDetectionOp, Double\u003e diskStat \u003d Maps.newHashMap();\n-      diskStat.put(DiskOutlierDetectionOp.METADATA, metadataOpStats.get(disk));\n-      diskStat.put(DiskOutlierDetectionOp.READ, readIoStats.get(disk));\n-      diskStat.put(DiskOutlierDetectionOp.WRITE, writeIoStats.get(disk));\n+      Map\u003cDiskOp, Double\u003e diskStat \u003d Maps.newHashMap();\n+      diskStat.put(DiskOp.METADATA, metadataOpStats.get(disk));\n+      diskStat.put(DiskOp.READ, readIoStats.get(disk));\n+      diskStat.put(DiskOp.WRITE, writeIoStats.get(disk));\n       diskStats.put(disk, diskStat);\n     }\n \n     diskOutliersStats \u003d diskStats;\n     LOG.debug(\"Updated disk outliers.\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void detectAndUpdateDiskOutliers(Map\u003cString, Double\u003e metadataOpStats,\n      Map\u003cString, Double\u003e readIoStats, Map\u003cString, Double\u003e writeIoStats) {\n    Set\u003cString\u003e diskOutliersSet \u003d Sets.newHashSet();\n\n    // Get MetadataOp Outliers\n    Map\u003cString, Double\u003e metadataOpOutliers \u003d slowDiskDetector\n        .getOutliers(metadataOpStats);\n    if (!metadataOpOutliers.isEmpty()) {\n      diskOutliersSet.addAll(metadataOpOutliers.keySet());\n    }\n\n    // Get ReadIo Outliers\n    Map\u003cString, Double\u003e readIoOutliers \u003d slowDiskDetector\n        .getOutliers(readIoStats);\n    if (!readIoOutliers.isEmpty()) {\n      diskOutliersSet.addAll(readIoOutliers.keySet());\n    }\n\n    // Get WriteIo Outliers\n    Map\u003cString, Double\u003e writeIoOutliers \u003d slowDiskDetector\n        .getOutliers(writeIoStats);\n    if (!readIoOutliers.isEmpty()) {\n      diskOutliersSet.addAll(writeIoOutliers.keySet());\n    }\n\n    Map\u003cString, Map\u003cDiskOp, Double\u003e\u003e diskStats \u003d\n        Maps.newHashMap();\n    for (String disk : diskOutliersSet) {\n      Map\u003cDiskOp, Double\u003e diskStat \u003d Maps.newHashMap();\n      diskStat.put(DiskOp.METADATA, metadataOpStats.get(disk));\n      diskStat.put(DiskOp.READ, readIoStats.get(disk));\n      diskStat.put(DiskOp.WRITE, writeIoStats.get(disk));\n      diskStats.put(disk, diskStat);\n    }\n\n    diskOutliersStats \u003d diskStats;\n    LOG.debug(\"Updated disk outliers.\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java",
      "extendedDetails": {}
    },
    "b3ec531f400dd0a6506dc71233d38ae57b764a43": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11461. DataNode Disk Outlier Detection. Contributed by Hanisha Koneru.\n",
      "commitDate": "02/03/17 12:45 PM",
      "commitName": "b3ec531f400dd0a6506dc71233d38ae57b764a43",
      "commitAuthor": "Arpit Agarwal",
      "diff": "@@ -0,0 +1,38 @@\n+  private void detectAndUpdateDiskOutliers(Map\u003cString, Double\u003e metadataOpStats,\n+      Map\u003cString, Double\u003e readIoStats, Map\u003cString, Double\u003e writeIoStats) {\n+    Set\u003cString\u003e diskOutliersSet \u003d Sets.newHashSet();\n+\n+    // Get MetadataOp Outliers\n+    Map\u003cString, Double\u003e metadataOpOutliers \u003d slowDiskDetector\n+        .getOutliers(metadataOpStats);\n+    if (!metadataOpOutliers.isEmpty()) {\n+      diskOutliersSet.addAll(metadataOpOutliers.keySet());\n+    }\n+\n+    // Get ReadIo Outliers\n+    Map\u003cString, Double\u003e readIoOutliers \u003d slowDiskDetector\n+        .getOutliers(readIoStats);\n+    if (!readIoOutliers.isEmpty()) {\n+      diskOutliersSet.addAll(readIoOutliers.keySet());\n+    }\n+\n+    // Get WriteIo Outliers\n+    Map\u003cString, Double\u003e writeIoOutliers \u003d slowDiskDetector\n+        .getOutliers(writeIoStats);\n+    if (!readIoOutliers.isEmpty()) {\n+      diskOutliersSet.addAll(writeIoOutliers.keySet());\n+    }\n+\n+    Map\u003cString, Map\u003cDiskOutlierDetectionOp, Double\u003e\u003e diskStats \u003d\n+        Maps.newHashMap();\n+    for (String disk : diskOutliersSet) {\n+      Map\u003cDiskOutlierDetectionOp, Double\u003e diskStat \u003d Maps.newHashMap();\n+      diskStat.put(DiskOutlierDetectionOp.METADATA, metadataOpStats.get(disk));\n+      diskStat.put(DiskOutlierDetectionOp.READ, readIoStats.get(disk));\n+      diskStat.put(DiskOutlierDetectionOp.WRITE, writeIoStats.get(disk));\n+      diskStats.put(disk, diskStat);\n+    }\n+\n+    diskOutliersStats \u003d diskStats;\n+    LOG.debug(\"Updated disk outliers.\");\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void detectAndUpdateDiskOutliers(Map\u003cString, Double\u003e metadataOpStats,\n      Map\u003cString, Double\u003e readIoStats, Map\u003cString, Double\u003e writeIoStats) {\n    Set\u003cString\u003e diskOutliersSet \u003d Sets.newHashSet();\n\n    // Get MetadataOp Outliers\n    Map\u003cString, Double\u003e metadataOpOutliers \u003d slowDiskDetector\n        .getOutliers(metadataOpStats);\n    if (!metadataOpOutliers.isEmpty()) {\n      diskOutliersSet.addAll(metadataOpOutliers.keySet());\n    }\n\n    // Get ReadIo Outliers\n    Map\u003cString, Double\u003e readIoOutliers \u003d slowDiskDetector\n        .getOutliers(readIoStats);\n    if (!readIoOutliers.isEmpty()) {\n      diskOutliersSet.addAll(readIoOutliers.keySet());\n    }\n\n    // Get WriteIo Outliers\n    Map\u003cString, Double\u003e writeIoOutliers \u003d slowDiskDetector\n        .getOutliers(writeIoStats);\n    if (!readIoOutliers.isEmpty()) {\n      diskOutliersSet.addAll(writeIoOutliers.keySet());\n    }\n\n    Map\u003cString, Map\u003cDiskOutlierDetectionOp, Double\u003e\u003e diskStats \u003d\n        Maps.newHashMap();\n    for (String disk : diskOutliersSet) {\n      Map\u003cDiskOutlierDetectionOp, Double\u003e diskStat \u003d Maps.newHashMap();\n      diskStat.put(DiskOutlierDetectionOp.METADATA, metadataOpStats.get(disk));\n      diskStat.put(DiskOutlierDetectionOp.READ, readIoStats.get(disk));\n      diskStat.put(DiskOutlierDetectionOp.WRITE, writeIoStats.get(disk));\n      diskStats.put(disk, diskStat);\n    }\n\n    diskOutliersStats \u003d diskStats;\n    LOG.debug(\"Updated disk outliers.\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java"
    }
  }
}