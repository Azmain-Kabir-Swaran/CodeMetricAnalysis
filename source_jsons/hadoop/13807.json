{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "removeBlocksAssociatedTo",
  "functionId": "removeBlocksAssociatedTo___node-DatanodeDescriptor(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 1672,
  "functionEndLine": 1692,
  "numCommitsSeen": 515,
  "timeTaken": 18834,
  "changeHistory": [
    "546b95f4843f3cbbbdf72d90d202cad551696082",
    "51b671ef1844069888f976cd16f66c88f9bbc7de",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
    "abf833a7b228fff2bca4f69cd9df99d532380038",
    "b7923a356e9f111619375b94d12749d634069347",
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e",
    "e5d6fba47d9c6f4d984ca8295fcf5dee388e2241",
    "4551da302d94cffea0313eac79479ab6f9b7cb34",
    "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15",
    "31c91706f7d17da006ef2d6c541f8dd092fae077",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "513f17d115564e49124bb744cecf36d16a144ffc",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "7fac946ac983e31613fd62836c8ac9c4a579210a",
    "c3f6575ca44e8ad803d0b46991472465b595cdeb"
  ],
  "changeHistoryShort": {
    "546b95f4843f3cbbbdf72d90d202cad551696082": "Ybodychange",
    "51b671ef1844069888f976cd16f66c88f9bbc7de": "Ybodychange",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": "Ybodychange",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": "Ybodychange",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": "Ybodychange",
    "abf833a7b228fff2bca4f69cd9df99d532380038": "Ybodychange",
    "b7923a356e9f111619375b94d12749d634069347": "Ybodychange",
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e": "Ybodychange",
    "e5d6fba47d9c6f4d984ca8295fcf5dee388e2241": "Ybodychange",
    "4551da302d94cffea0313eac79479ab6f9b7cb34": "Ybodychange",
    "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15": "Ybodychange",
    "31c91706f7d17da006ef2d6c541f8dd092fae077": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "513f17d115564e49124bb744cecf36d16a144ffc": "Ybodychange",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "7fac946ac983e31613fd62836c8ac9c4a579210a": "Ymultichange(Yrename,Ymodifierchange,Ybodychange)",
    "c3f6575ca44e8ad803d0b46991472465b595cdeb": "Yintroduced"
  },
  "changeHistoryDetails": {
    "546b95f4843f3cbbbdf72d90d202cad551696082": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11673. [READ] Handle failures of Datanode with PROVIDED storage\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "546b95f4843f3cbbbdf72d90d202cad551696082",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "d65df0f27395792c6e25f5e03b6ba1765e2ba925",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,21 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n+    providedStorageMap.removeDatanode(node);\n     for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n       final Iterator\u003cBlockInfo\u003e it \u003d storage.getBlockIterator();\n       //add the BlockInfos to a new collection as the\n       //returned iterator is not modifiable.\n       Collection\u003cBlockInfo\u003e toRemove \u003d new ArrayList\u003c\u003e();\n       while (it.hasNext()) {\n         toRemove.add(it.next());\n       }\n \n       for (BlockInfo b : toRemove) {\n         removeStoredBlock(b, node);\n       }\n     }\n     // Remove all pending DN messages referencing this DN.\n     pendingDNMessages.removeAllMessagesForDatanode(node);\n \n     node.resetBlocks();\n     invalidateBlocks.remove(node);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    providedStorageMap.removeDatanode(node);\n    for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n      final Iterator\u003cBlockInfo\u003e it \u003d storage.getBlockIterator();\n      //add the BlockInfos to a new collection as the\n      //returned iterator is not modifiable.\n      Collection\u003cBlockInfo\u003e toRemove \u003d new ArrayList\u003c\u003e();\n      while (it.hasNext()) {\n        toRemove.add(it.next());\n      }\n\n      for (BlockInfo b : toRemove) {\n        removeStoredBlock(b, node);\n      }\n    }\n    // Remove all pending DN messages referencing this DN.\n    pendingDNMessages.removeAllMessagesForDatanode(node);\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "51b671ef1844069888f976cd16f66c88f9bbc7de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11681. DatanodeStorageInfo#getBlockIterator() should return an iterator to an unmodifiable set Contributed by Virajith Jalaparti\n",
      "commitDate": "10/05/17 10:25 PM",
      "commitName": "51b671ef1844069888f976cd16f66c88f9bbc7de",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "10/05/17 12:15 PM",
      "commitNameOld": "ad1e3e4d9f105fac246ce1bdae80e92e013b8ba5",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.42,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,20 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n     for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n       final Iterator\u003cBlockInfo\u003e it \u003d storage.getBlockIterator();\n+      //add the BlockInfos to a new collection as the\n+      //returned iterator is not modifiable.\n+      Collection\u003cBlockInfo\u003e toRemove \u003d new ArrayList\u003c\u003e();\n       while (it.hasNext()) {\n-        BlockInfo block \u003d it.next();\n-        // DatanodeStorageInfo must be removed using the iterator to avoid\n-        // ConcurrentModificationException in the underlying storage\n-        it.remove();\n-        removeStoredBlock(block, node);\n+        toRemove.add(it.next());\n+      }\n+\n+      for (BlockInfo b : toRemove) {\n+        removeStoredBlock(b, node);\n       }\n     }\n     // Remove all pending DN messages referencing this DN.\n     pendingDNMessages.removeAllMessagesForDatanode(node);\n \n     node.resetBlocks();\n     invalidateBlocks.remove(node);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n      final Iterator\u003cBlockInfo\u003e it \u003d storage.getBlockIterator();\n      //add the BlockInfos to a new collection as the\n      //returned iterator is not modifiable.\n      Collection\u003cBlockInfo\u003e toRemove \u003d new ArrayList\u003c\u003e();\n      while (it.hasNext()) {\n        toRemove.add(it.next());\n      }\n\n      for (BlockInfo b : toRemove) {\n        removeStoredBlock(b, node);\n      }\n    }\n    // Remove all pending DN messages referencing this DN.\n    pendingDNMessages.removeAllMessagesForDatanode(node);\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9260. Improve the performance and GC friendliness of NameNode startup and full block reports (Staffan Friberg via cmccabe)\n",
      "commitDate": "02/02/16 11:23 AM",
      "commitName": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "31/01/16 11:54 PM",
      "commitNameOld": "e418bd1fb0568ce7ae22f588fea2dd9c95567383",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 1.48,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,17 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n-    final Iterator\u003cBlockInfo\u003e it \u003d node.getBlockIterator();\n-    while(it.hasNext()) {\n-      removeStoredBlock(it.next(), node);\n+    for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n+      final Iterator\u003cBlockInfo\u003e it \u003d storage.getBlockIterator();\n+      while (it.hasNext()) {\n+        BlockInfo block \u003d it.next();\n+        // DatanodeStorageInfo must be removed using the iterator to avoid\n+        // ConcurrentModificationException in the underlying storage\n+        it.remove();\n+        removeStoredBlock(block, node);\n+      }\n     }\n     // Remove all pending DN messages referencing this DN.\n     pendingDNMessages.removeAllMessagesForDatanode(node);\n \n     node.resetBlocks();\n     invalidateBlocks.remove(node);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n      final Iterator\u003cBlockInfo\u003e it \u003d storage.getBlockIterator();\n      while (it.hasNext()) {\n        BlockInfo block \u003d it.next();\n        // DatanodeStorageInfo must be removed using the iterator to avoid\n        // ConcurrentModificationException in the underlying storage\n        it.remove();\n        removeStoredBlock(block, node);\n      }\n    }\n    // Remove all pending DN messages referencing this DN.\n    pendingDNMessages.removeAllMessagesForDatanode(node);\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\"\n\nThis reverts commit de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "31/07/15 4:15 PM",
      "commitNameOld": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 5.75,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,11 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n-    final Iterator\u003cBlockInfo\u003e it \u003d node.getBlockIterator();\n+    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n     // Remove all pending DN messages referencing this DN.\n     pendingDNMessages.removeAllMessagesForDatanode(node);\n \n     node.resetBlocks();\n     invalidateBlocks.remove(node);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n    // Remove all pending DN messages referencing this DN.\n    pendingDNMessages.removeAllMessagesForDatanode(node);\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\n",
      "commitDate": "26/06/15 10:49 AM",
      "commitName": "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/06/15 2:42 PM",
      "commitNameOld": "afe9ea3c12e1f5a71922400eadb642960bc87ca1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,11 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n-    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n+    final Iterator\u003cBlockInfo\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n     // Remove all pending DN messages referencing this DN.\n     pendingDNMessages.removeAllMessagesForDatanode(node);\n \n     node.resetBlocks();\n     invalidateBlocks.remove(node);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003cBlockInfo\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n    // Remove all pending DN messages referencing this DN.\n    pendingDNMessages.removeAllMessagesForDatanode(node);\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "abf833a7b228fff2bca4f69cd9df99d532380038": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7907. Erasure Coding: track invalid, corrupt, and under-recovery striped blocks in NameNode. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:43 AM",
      "commitName": "abf833a7b228fff2bca4f69cd9df99d532380038",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:43 AM",
      "commitNameOld": "ea2e60fbcc79c65ec571224bd3f57c262a5d9114",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,11 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n-    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n+    final Iterator\u003cBlockInfo\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n     // Remove all pending DN messages referencing this DN.\n     pendingDNMessages.removeAllMessagesForDatanode(node);\n \n     node.resetBlocks();\n     invalidateBlocks.remove(node);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003cBlockInfo\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n    // Remove all pending DN messages referencing this DN.\n    pendingDNMessages.removeAllMessagesForDatanode(node);\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "b7923a356e9f111619375b94d12749d634069347": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6425. Large postponedMisreplicatedBlocks has impact on blockReport latency. Contributed by Ming Ma.\n",
      "commitDate": "16/12/14 8:30 AM",
      "commitName": "b7923a356e9f111619375b94d12749d634069347",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "10/12/14 11:44 PM",
      "commitNameOld": "390642acf35f3d599271617d30ba26c2f6406fc1",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 5.37,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,11 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n     final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n     // Remove all pending DN messages referencing this DN.\n     pendingDNMessages.removeAllMessagesForDatanode(node);\n \n     node.resetBlocks();\n     invalidateBlocks.remove(node);\n-    \n-    // If the DN hasn\u0027t block-reported since the most recent\n-    // failover, then we may have been holding up on processing\n-    // over-replicated blocks because of it. But we can now\n-    // process those blocks.\n-    boolean stale \u003d false;\n-    for(DatanodeStorageInfo storage : node.getStorageInfos()) {\n-      if (storage.areBlockContentsStale()) {\n-        stale \u003d true;\n-        break;\n-      }\n-    }\n-    if (stale) {\n-      rescanPostponedMisreplicatedBlocks();\n-    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n    // Remove all pending DN messages referencing this DN.\n    pendingDNMessages.removeAllMessagesForDatanode(node);\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6362. InvalidateBlocks is inconsistent in usage of DatanodeUuid and StorageID. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595056 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/05/14 2:30 PM",
      "commitName": "9a0fcae5bc9e481201e101c3c98e23b6e827774e",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "13/05/14 11:22 AM",
      "commitNameOld": "8e5b5165c14486af6d5d73e7b4e591d4787ad8f2",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.13,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n     final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n     // Remove all pending DN messages referencing this DN.\n     pendingDNMessages.removeAllMessagesForDatanode(node);\n \n     node.resetBlocks();\n-    invalidateBlocks.remove(node.getDatanodeUuid());\n+    invalidateBlocks.remove(node);\n     \n     // If the DN hasn\u0027t block-reported since the most recent\n     // failover, then we may have been holding up on processing\n     // over-replicated blocks because of it. But we can now\n     // process those blocks.\n     boolean stale \u003d false;\n     for(DatanodeStorageInfo storage : node.getStorageInfos()) {\n       if (storage.areBlockContentsStale()) {\n         stale \u003d true;\n         break;\n       }\n     }\n     if (stale) {\n       rescanPostponedMisreplicatedBlocks();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n    // Remove all pending DN messages referencing this DN.\n    pendingDNMessages.removeAllMessagesForDatanode(node);\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node);\n    \n    // If the DN hasn\u0027t block-reported since the most recent\n    // failover, then we may have been holding up on processing\n    // over-replicated blocks because of it. But we can now\n    // process those blocks.\n    boolean stale \u003d false;\n    for(DatanodeStorageInfo storage : node.getStorageInfos()) {\n      if (storage.areBlockContentsStale()) {\n        stale \u003d true;\n        break;\n      }\n    }\n    if (stale) {\n      rescanPostponedMisreplicatedBlocks();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "e5d6fba47d9c6f4d984ca8295fcf5dee388e2241": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6289. HA failover can fail if there are pending DN messages for DNs which no longer exist. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1591413 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/04/14 10:46 AM",
      "commitName": "e5d6fba47d9c6f4d984ca8295fcf5dee388e2241",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/04/14 12:24 AM",
      "commitNameOld": "24d1cf9ac681fadaf2a3614a24b06327d5d5f53e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 6.43,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,26 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n     final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n+    // Remove all pending DN messages referencing this DN.\n+    pendingDNMessages.removeAllMessagesForDatanode(node);\n \n     node.resetBlocks();\n     invalidateBlocks.remove(node.getDatanodeUuid());\n     \n     // If the DN hasn\u0027t block-reported since the most recent\n     // failover, then we may have been holding up on processing\n     // over-replicated blocks because of it. But we can now\n     // process those blocks.\n     boolean stale \u003d false;\n     for(DatanodeStorageInfo storage : node.getStorageInfos()) {\n       if (storage.areBlockContentsStale()) {\n         stale \u003d true;\n         break;\n       }\n     }\n     if (stale) {\n       rescanPostponedMisreplicatedBlocks();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n    // Remove all pending DN messages referencing this DN.\n    pendingDNMessages.removeAllMessagesForDatanode(node);\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node.getDatanodeUuid());\n    \n    // If the DN hasn\u0027t block-reported since the most recent\n    // failover, then we may have been holding up on processing\n    // over-replicated blocks because of it. But we can now\n    // process those blocks.\n    boolean stale \u003d false;\n    for(DatanodeStorageInfo storage : node.getStorageInfos()) {\n      if (storage.areBlockContentsStale()) {\n        stale \u003d true;\n        break;\n      }\n    }\n    if (stale) {\n      rescanPostponedMisreplicatedBlocks();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4551da302d94cffea0313eac79479ab6f9b7cb34": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5233. Use Datanode UUID to identify Datanodes.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1525407 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/09/13 11:03 AM",
      "commitName": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "18/09/13 8:12 AM",
      "commitNameOld": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.12,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n     final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n \n     node.resetBlocks();\n-    invalidateBlocks.remove(node.getStorageID());\n+    invalidateBlocks.remove(node.getDatanodeUuid());\n     \n     // If the DN hasn\u0027t block-reported since the most recent\n     // failover, then we may have been holding up on processing\n     // over-replicated blocks because of it. But we can now\n     // process those blocks.\n     boolean stale \u003d false;\n     for(DatanodeStorageInfo storage : node.getStorageInfos()) {\n       if (storage.areBlockContentsStale()) {\n         stale \u003d true;\n         break;\n       }\n     }\n     if (stale) {\n       rescanPostponedMisreplicatedBlocks();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node.getDatanodeUuid());\n    \n    // If the DN hasn\u0027t block-reported since the most recent\n    // failover, then we may have been holding up on processing\n    // over-replicated blocks because of it. But we can now\n    // process those blocks.\n    boolean stale \u003d false;\n    for(DatanodeStorageInfo storage : node.getStorageInfos()) {\n      if (storage.areBlockContentsStale()) {\n        stale \u003d true;\n        break;\n      }\n    }\n    if (stale) {\n      rescanPostponedMisreplicatedBlocks();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5134. Move blockContentsStale, heartbeatedSinceFailover and firstBlockReport from DatanodeDescriptor to DatanodeStorageInfo; and fix a synchronization problem in DatanodeStorageInfo.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1520938 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/09/13 3:53 PM",
      "commitName": "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "03/09/13 7:03 AM",
      "commitNameOld": "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.37,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,24 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n     final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n \n     node.resetBlocks();\n     invalidateBlocks.remove(node.getStorageID());\n     \n     // If the DN hasn\u0027t block-reported since the most recent\n     // failover, then we may have been holding up on processing\n     // over-replicated blocks because of it. But we can now\n     // process those blocks.\n-    if (node.areBlockContentsStale()) {\n+    boolean stale \u003d false;\n+    for(DatanodeStorageInfo storage : node.getStorageInfos()) {\n+      if (storage.areBlockContentsStale()) {\n+        stale \u003d true;\n+        break;\n+      }\n+    }\n+    if (stale) {\n       rescanPostponedMisreplicatedBlocks();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node.getStorageID());\n    \n    // If the DN hasn\u0027t block-reported since the most recent\n    // failover, then we may have been holding up on processing\n    // over-replicated blocks because of it. But we can now\n    // process those blocks.\n    boolean stale \u003d false;\n    for(DatanodeStorageInfo storage : node.getStorageInfos()) {\n      if (storage.areBlockContentsStale()) {\n        stale \u003d true;\n        break;\n      }\n    }\n    if (stale) {\n      rescanPostponedMisreplicatedBlocks();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "31c91706f7d17da006ef2d6c541f8dd092fae077": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1972. Fencing mechanism for block invalidations and replications. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1221608 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/11 8:32 PM",
      "commitName": "31c91706f7d17da006ef2d6c541f8dd092fae077",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "20/12/11 7:03 PM",
      "commitNameOld": "36d1c49486587c2dbb193e8538b1d4510c462fa6",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,17 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n     final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n \n     node.resetBlocks();\n     invalidateBlocks.remove(node.getStorageID());\n+    \n+    // If the DN hasn\u0027t block-reported since the most recent\n+    // failover, then we may have been holding up on processing\n+    // over-replicated blocks because of it. But we can now\n+    // process those blocks.\n+    if (node.areBlockContentsStale()) {\n+      rescanPostponedMisreplicatedBlocks();\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node.getStorageID());\n    \n    // If the DN hasn\u0027t block-reported since the most recent\n    // failover, then we may have been holding up on processing\n    // over-replicated blocks because of it. But we can now\n    // process those blocks.\n    if (node.areBlockContentsStale()) {\n      rescanPostponedMisreplicatedBlocks();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node.getStorageID());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "513f17d115564e49124bb744cecf36d16a144ffc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2273.  Refactor BlockManager.recentInvalidateSets to a new class.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1160475 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/08/11 3:28 PM",
      "commitName": "513f17d115564e49124bb744cecf36d16a144ffc",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "19/08/11 10:36 AM",
      "commitNameOld": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 3.2,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,9 @@\n   void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n     final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n \n     node.resetBlocks();\n-    removeFromInvalidates(node.getStorageID());\n+    invalidateBlocks.remove(node.getStorageID());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n\n    node.resetBlocks();\n    invalidateBlocks.remove(node.getStorageID());\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n\n    node.resetBlocks();\n    removeFromInvalidates(node.getStorageID());\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "7fac946ac983e31613fd62836c8ac9c4a579210a": {
      "type": "Ymultichange(Yrename,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-2108. Move datanode heartbeat handling from namenode package to blockmanagement package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154042 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/08/11 3:55 PM",
      "commitName": "7fac946ac983e31613fd62836c8ac9c4a579210a",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-2108. Move datanode heartbeat handling from namenode package to blockmanagement package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154042 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/08/11 3:55 PM",
          "commitName": "7fac946ac983e31613fd62836c8ac9c4a579210a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/08/11 6:57 AM",
          "commitNameOld": "d68e38b78d9687987c4de2046ce9aa0016685e98",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 3.37,
          "commitsBetweenForRepo": 18,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,9 @@\n-  public void removeDatanode(final DatanodeDescriptor node) {\n+  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n     final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n \n     node.resetBlocks();\n     removeFromInvalidates(node.getStorageID());\n-    datanodeManager.getNetworkTopology().remove(node);\n-\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"remove datanode \" + node.getName());\n-    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n\n    node.resetBlocks();\n    removeFromInvalidates(node.getStorageID());\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "removeDatanode",
            "newValue": "removeBlocksAssociatedTo"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2108. Move datanode heartbeat handling from namenode package to blockmanagement package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154042 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/08/11 3:55 PM",
          "commitName": "7fac946ac983e31613fd62836c8ac9c4a579210a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/08/11 6:57 AM",
          "commitNameOld": "d68e38b78d9687987c4de2046ce9aa0016685e98",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 3.37,
          "commitsBetweenForRepo": 18,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,9 @@\n-  public void removeDatanode(final DatanodeDescriptor node) {\n+  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n     final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n \n     node.resetBlocks();\n     removeFromInvalidates(node.getStorageID());\n-    datanodeManager.getNetworkTopology().remove(node);\n-\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"remove datanode \" + node.getName());\n-    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n\n    node.resetBlocks();\n    removeFromInvalidates(node.getStorageID());\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2108. Move datanode heartbeat handling from namenode package to blockmanagement package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154042 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/08/11 3:55 PM",
          "commitName": "7fac946ac983e31613fd62836c8ac9c4a579210a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/08/11 6:57 AM",
          "commitNameOld": "d68e38b78d9687987c4de2046ce9aa0016685e98",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 3.37,
          "commitsBetweenForRepo": 18,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,9 @@\n-  public void removeDatanode(final DatanodeDescriptor node) {\n+  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n     final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n     while(it.hasNext()) {\n       removeStoredBlock(it.next(), node);\n     }\n \n     node.resetBlocks();\n     removeFromInvalidates(node.getStorageID());\n-    datanodeManager.getNetworkTopology().remove(node);\n-\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"remove datanode \" + node.getName());\n-    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void removeBlocksAssociatedTo(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n\n    node.resetBlocks();\n    removeFromInvalidates(node.getStorageID());\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "c3f6575ca44e8ad803d0b46991472465b595cdeb": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2147. Move cluster network topology to block management and fix some javac warnings.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1148112 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/07/11 5:26 PM",
      "commitName": "c3f6575ca44e8ad803d0b46991472465b595cdeb",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,14 @@\n+  public void removeDatanode(final DatanodeDescriptor node) {\n+    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n+    while(it.hasNext()) {\n+      removeStoredBlock(it.next(), node);\n+    }\n+\n+    node.resetBlocks();\n+    removeFromInvalidates(node.getStorageID());\n+    datanodeManager.getNetworkTopology().remove(node);\n+\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"remove datanode \" + node.getName());\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeDatanode(final DatanodeDescriptor node) {\n    final Iterator\u003c? extends Block\u003e it \u003d node.getBlockIterator();\n    while(it.hasNext()) {\n      removeStoredBlock(it.next(), node);\n    }\n\n    node.resetBlocks();\n    removeFromInvalidates(node.getStorageID());\n    datanodeManager.getNetworkTopology().remove(node);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"remove datanode \" + node.getName());\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    }
  }
}