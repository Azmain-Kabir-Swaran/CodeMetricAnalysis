{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeProtocolServerSideTranslatorPB.java",
  "functionName": "blockReport",
  "functionId": "blockReport___controller-RpcController__request-BlockReportRequestProto",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
  "functionStartLine": 159,
  "functionEndLine": 194,
  "numCommitsSeen": 42,
  "timeTaken": 3817,
  "changeHistory": [
    "63ac2db59af2b50e74dc892cae1dbc4d2e061423",
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "50ee8f4e67a66aa77c5359182f61f3e951844db6",
    "d324164a51a43d72c02567248bd9f0f12b244a40",
    "9e108e61fb28244326d7cf4bb31d175eb75d2636",
    "28eadb7cd71e99d563fb5c41aec563ab11e293e5",
    "f88574acdefae2816236bf6180916be96c6a6874",
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e",
    "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c",
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
    "b5229fd19bfecc2e5249db652ad34ec08152334b",
    "3001a172c8868763f8e59e866e36f7f50dee62cc",
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
    "38a19bc293dec6221ae96e304fc6ab660d94e706"
  ],
  "changeHistoryShort": {
    "63ac2db59af2b50e74dc892cae1dbc4d2e061423": "Ybodychange",
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ybodychange",
    "50ee8f4e67a66aa77c5359182f61f3e951844db6": "Ybodychange",
    "d324164a51a43d72c02567248bd9f0f12b244a40": "Ybodychange",
    "9e108e61fb28244326d7cf4bb31d175eb75d2636": "Ybodychange",
    "28eadb7cd71e99d563fb5c41aec563ab11e293e5": "Ybodychange",
    "f88574acdefae2816236bf6180916be96c6a6874": "Ybodychange",
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e": "Ybodychange",
    "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c": "Ybodychange",
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c": "Ybodychange",
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256": "Ybodychange",
    "b5229fd19bfecc2e5249db652ad34ec08152334b": "Ybodychange",
    "3001a172c8868763f8e59e866e36f7f50dee62cc": "Ybodychange",
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e": "Ybodychange",
    "38a19bc293dec6221ae96e304fc6ab660d94e706": "Yintroduced"
  },
  "changeHistoryDetails": {
    "63ac2db59af2b50e74dc892cae1dbc4d2e061423": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10312. Large block reports may fail to decode at NameNode due to 64 MB protobuf maximum length restriction. Contributed by Chris Nauroth.\n",
      "commitDate": "20/04/16 1:39 PM",
      "commitName": "63ac2db59af2b50e74dc892cae1dbc4d2e061423",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "25/11/15 8:09 PM",
      "commitNameOld": "9f256d1d716a7e17606245fcfc619901a8fa299a",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 146.69,
      "commitsBetweenForRepo": 878,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,36 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd \u003d null;\n     StorageBlockReport[] report \u003d \n         new StorageBlockReport[request.getReportsCount()];\n     \n     int index \u003d 0;\n     for (StorageBlockReportProto s : request.getReportsList()) {\n       final BlockListAsLongs blocks;\n       if (s.hasNumberOfBlocks()) { // new style buffer based reports\n         int num \u003d (int)s.getNumberOfBlocks();\n         Preconditions.checkState(s.getBlocksCount() \u003d\u003d 0,\n             \"cannot send both blocks list and buffers\");\n-        blocks \u003d BlockListAsLongs.decodeBuffers(num, s.getBlocksBuffersList());\n+        blocks \u003d BlockListAsLongs.decodeBuffers(num, s.getBlocksBuffersList(),\n+            maxDataLength);\n       } else {\n-        blocks \u003d BlockListAsLongs.decodeLongs(s.getBlocksList());\n+        blocks \u003d BlockListAsLongs.decodeLongs(s.getBlocksList(), maxDataLength);\n       }\n       report[index++] \u003d new StorageBlockReport(PBHelperClient.convert(s.getStorage()),\n           blocks);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), report,\n           request.hasContext() ?\n               PBHelper.convert(request.getContext()) : null);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n     BlockReportResponseProto.Builder builder \u003d \n         BlockReportResponseProto.newBuilder();\n     if (cmd !\u003d null) {\n       builder.setCmd(PBHelper.convert(cmd));\n     }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd \u003d null;\n    StorageBlockReport[] report \u003d \n        new StorageBlockReport[request.getReportsCount()];\n    \n    int index \u003d 0;\n    for (StorageBlockReportProto s : request.getReportsList()) {\n      final BlockListAsLongs blocks;\n      if (s.hasNumberOfBlocks()) { // new style buffer based reports\n        int num \u003d (int)s.getNumberOfBlocks();\n        Preconditions.checkState(s.getBlocksCount() \u003d\u003d 0,\n            \"cannot send both blocks list and buffers\");\n        blocks \u003d BlockListAsLongs.decodeBuffers(num, s.getBlocksBuffersList(),\n            maxDataLength);\n      } else {\n        blocks \u003d BlockListAsLongs.decodeLongs(s.getBlocksList(), maxDataLength);\n      }\n      report[index++] \u003d new StorageBlockReport(PBHelperClient.convert(s.getStorage()),\n          blocks);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), report,\n          request.hasContext() ?\n              PBHelper.convert(request.getContext()) : null);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/08/15 2:38 PM",
      "commitNameOld": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 24.18,
      "commitsBetweenForRepo": 143,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,35 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd \u003d null;\n     StorageBlockReport[] report \u003d \n         new StorageBlockReport[request.getReportsCount()];\n     \n     int index \u003d 0;\n     for (StorageBlockReportProto s : request.getReportsList()) {\n       final BlockListAsLongs blocks;\n       if (s.hasNumberOfBlocks()) { // new style buffer based reports\n         int num \u003d (int)s.getNumberOfBlocks();\n         Preconditions.checkState(s.getBlocksCount() \u003d\u003d 0,\n             \"cannot send both blocks list and buffers\");\n         blocks \u003d BlockListAsLongs.decodeBuffers(num, s.getBlocksBuffersList());\n       } else {\n         blocks \u003d BlockListAsLongs.decodeLongs(s.getBlocksList());\n       }\n-      report[index++] \u003d new StorageBlockReport(PBHelper.convert(s.getStorage()),\n+      report[index++] \u003d new StorageBlockReport(PBHelperClient.convert(s.getStorage()),\n           blocks);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), report,\n           request.hasContext() ?\n               PBHelper.convert(request.getContext()) : null);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n     BlockReportResponseProto.Builder builder \u003d \n         BlockReportResponseProto.newBuilder();\n     if (cmd !\u003d null) {\n       builder.setCmd(PBHelper.convert(cmd));\n     }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd \u003d null;\n    StorageBlockReport[] report \u003d \n        new StorageBlockReport[request.getReportsCount()];\n    \n    int index \u003d 0;\n    for (StorageBlockReportProto s : request.getReportsList()) {\n      final BlockListAsLongs blocks;\n      if (s.hasNumberOfBlocks()) { // new style buffer based reports\n        int num \u003d (int)s.getNumberOfBlocks();\n        Preconditions.checkState(s.getBlocksCount() \u003d\u003d 0,\n            \"cannot send both blocks list and buffers\");\n        blocks \u003d BlockListAsLongs.decodeBuffers(num, s.getBlocksBuffersList());\n      } else {\n        blocks \u003d BlockListAsLongs.decodeLongs(s.getBlocksList());\n      }\n      report[index++] \u003d new StorageBlockReport(PBHelperClient.convert(s.getStorage()),\n          blocks);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), report,\n          request.hasContext() ?\n              PBHelper.convert(request.getContext()) : null);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "50ee8f4e67a66aa77c5359182f61f3e951844db6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7960. The full block report should prune zombie storages even if they\u0027re not empty. Contributed by Colin McCabe and Eddy Xu.\n",
      "commitDate": "23/03/15 10:00 PM",
      "commitName": "50ee8f4e67a66aa77c5359182f61f3e951844db6",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "13/03/15 12:23 PM",
      "commitNameOld": "d324164a51a43d72c02567248bd9f0f12b244a40",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 10.4,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,35 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd \u003d null;\n     StorageBlockReport[] report \u003d \n         new StorageBlockReport[request.getReportsCount()];\n     \n     int index \u003d 0;\n     for (StorageBlockReportProto s : request.getReportsList()) {\n       final BlockListAsLongs blocks;\n       if (s.hasNumberOfBlocks()) { // new style buffer based reports\n         int num \u003d (int)s.getNumberOfBlocks();\n         Preconditions.checkState(s.getBlocksCount() \u003d\u003d 0,\n             \"cannot send both blocks list and buffers\");\n         blocks \u003d BlockListAsLongs.decodeBuffers(num, s.getBlocksBuffersList());\n       } else {\n         blocks \u003d BlockListAsLongs.decodeLongs(s.getBlocksList());\n       }\n       report[index++] \u003d new StorageBlockReport(PBHelper.convert(s.getStorage()),\n           blocks);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n-          request.getBlockPoolId(), report);\n+          request.getBlockPoolId(), report,\n+          request.hasContext() ?\n+              PBHelper.convert(request.getContext()) : null);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n     BlockReportResponseProto.Builder builder \u003d \n         BlockReportResponseProto.newBuilder();\n     if (cmd !\u003d null) {\n       builder.setCmd(PBHelper.convert(cmd));\n     }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd \u003d null;\n    StorageBlockReport[] report \u003d \n        new StorageBlockReport[request.getReportsCount()];\n    \n    int index \u003d 0;\n    for (StorageBlockReportProto s : request.getReportsList()) {\n      final BlockListAsLongs blocks;\n      if (s.hasNumberOfBlocks()) { // new style buffer based reports\n        int num \u003d (int)s.getNumberOfBlocks();\n        Preconditions.checkState(s.getBlocksCount() \u003d\u003d 0,\n            \"cannot send both blocks list and buffers\");\n        blocks \u003d BlockListAsLongs.decodeBuffers(num, s.getBlocksBuffersList());\n      } else {\n        blocks \u003d BlockListAsLongs.decodeLongs(s.getBlocksList());\n      }\n      report[index++] \u003d new StorageBlockReport(PBHelper.convert(s.getStorage()),\n          blocks);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), report,\n          request.hasContext() ?\n              PBHelper.convert(request.getContext()) : null);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "d324164a51a43d72c02567248bd9f0f12b244a40": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7435. PB encoding of block reports is very inefficient. Contributed by Daryn Sharp.\n",
      "commitDate": "13/03/15 12:23 PM",
      "commitName": "d324164a51a43d72c02567248bd9f0f12b244a40",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "16/02/15 2:43 PM",
      "commitNameOld": "9729b244de50322c2cc889c97c2ffb2b4675cf77",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 24.86,
      "commitsBetweenForRepo": 203,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,33 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd \u003d null;\n     StorageBlockReport[] report \u003d \n         new StorageBlockReport[request.getReportsCount()];\n     \n     int index \u003d 0;\n     for (StorageBlockReportProto s : request.getReportsList()) {\n-      List\u003cLong\u003e blockIds \u003d s.getBlocksList();\n-      long[] blocks \u003d new long[blockIds.size()];\n-      for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n-        blocks[i] \u003d blockIds.get(i);\n+      final BlockListAsLongs blocks;\n+      if (s.hasNumberOfBlocks()) { // new style buffer based reports\n+        int num \u003d (int)s.getNumberOfBlocks();\n+        Preconditions.checkState(s.getBlocksCount() \u003d\u003d 0,\n+            \"cannot send both blocks list and buffers\");\n+        blocks \u003d BlockListAsLongs.decodeBuffers(num, s.getBlocksBuffersList());\n+      } else {\n+        blocks \u003d BlockListAsLongs.decodeLongs(s.getBlocksList());\n       }\n       report[index++] \u003d new StorageBlockReport(PBHelper.convert(s.getStorage()),\n           blocks);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), report);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n     BlockReportResponseProto.Builder builder \u003d \n         BlockReportResponseProto.newBuilder();\n     if (cmd !\u003d null) {\n       builder.setCmd(PBHelper.convert(cmd));\n     }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd \u003d null;\n    StorageBlockReport[] report \u003d \n        new StorageBlockReport[request.getReportsCount()];\n    \n    int index \u003d 0;\n    for (StorageBlockReportProto s : request.getReportsList()) {\n      final BlockListAsLongs blocks;\n      if (s.hasNumberOfBlocks()) { // new style buffer based reports\n        int num \u003d (int)s.getNumberOfBlocks();\n        Preconditions.checkState(s.getBlocksCount() \u003d\u003d 0,\n            \"cannot send both blocks list and buffers\");\n        blocks \u003d BlockListAsLongs.decodeBuffers(num, s.getBlocksBuffersList());\n      } else {\n        blocks \u003d BlockListAsLongs.decodeLongs(s.getBlocksList());\n      }\n      report[index++] \u003d new StorageBlockReport(PBHelper.convert(s.getStorage()),\n          blocks);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), report);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "9e108e61fb28244326d7cf4bb31d175eb75d2636": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3086. Change Datanode not to send storage list in registration.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303318 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/03/12 2:07 AM",
      "commitName": "9e108e61fb28244326d7cf4bb31d175eb75d2636",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "19/03/12 3:09 PM",
      "commitNameOld": "6326605acb5a5bf48d994278c9d3a39733679e81",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.46,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,29 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd \u003d null;\n     StorageBlockReport[] report \u003d \n         new StorageBlockReport[request.getReportsCount()];\n     \n     int index \u003d 0;\n     for (StorageBlockReportProto s : request.getReportsList()) {\n       List\u003cLong\u003e blockIds \u003d s.getBlocksList();\n       long[] blocks \u003d new long[blockIds.size()];\n       for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n         blocks[i] \u003d blockIds.get(i);\n       }\n-      report[index++] \u003d new StorageBlockReport(s.getStorageID(), blocks);\n+      report[index++] \u003d new StorageBlockReport(PBHelper.convert(s.getStorage()),\n+          blocks);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), report);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n     BlockReportResponseProto.Builder builder \u003d \n         BlockReportResponseProto.newBuilder();\n     if (cmd !\u003d null) {\n       builder.setCmd(PBHelper.convert(cmd));\n     }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd \u003d null;\n    StorageBlockReport[] report \u003d \n        new StorageBlockReport[request.getReportsCount()];\n    \n    int index \u003d 0;\n    for (StorageBlockReportProto s : request.getReportsList()) {\n      List\u003cLong\u003e blockIds \u003d s.getBlocksList();\n      long[] blocks \u003d new long[blockIds.size()];\n      for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n        blocks[i] \u003d blockIds.get(i);\n      }\n      report[index++] \u003d new StorageBlockReport(PBHelper.convert(s.getStorage()),\n          blocks);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), report);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "28eadb7cd71e99d563fb5c41aec563ab11e293e5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2899. Service protocol changes in DatanodeProtocol to add multiple storages. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241519 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/02/12 8:59 AM",
      "commitName": "28eadb7cd71e99d563fb5c41aec563ab11e293e5",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "04/02/12 5:39 PM",
      "commitNameOld": "f88574acdefae2816236bf6180916be96c6a6874",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 2.64,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,28 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd \u003d null;\n-    List\u003cLong\u003e blockIds \u003d request.getReports(0).getBlocksList();\n-    long[] blocks \u003d new long[blockIds.size()];\n-    for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n-      blocks[i] \u003d blockIds.get(i);\n+    StorageBlockReport[] report \u003d \n+        new StorageBlockReport[request.getReportsCount()];\n+    \n+    int index \u003d 0;\n+    for (StorageBlockReportProto s : request.getReportsList()) {\n+      List\u003cLong\u003e blockIds \u003d s.getBlocksList();\n+      long[] blocks \u003d new long[blockIds.size()];\n+      for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n+        blocks[i] \u003d blockIds.get(i);\n+      }\n+      report[index++] \u003d new StorageBlockReport(s.getStorageID(), blocks);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n-          request.getBlockPoolId(), blocks);\n+          request.getBlockPoolId(), report);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n     BlockReportResponseProto.Builder builder \u003d \n         BlockReportResponseProto.newBuilder();\n     if (cmd !\u003d null) {\n       builder.setCmd(PBHelper.convert(cmd));\n     }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd \u003d null;\n    StorageBlockReport[] report \u003d \n        new StorageBlockReport[request.getReportsCount()];\n    \n    int index \u003d 0;\n    for (StorageBlockReportProto s : request.getReportsList()) {\n      List\u003cLong\u003e blockIds \u003d s.getBlocksList();\n      long[] blocks \u003d new long[blockIds.size()];\n      for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n        blocks[i] \u003d blockIds.get(i);\n      }\n      report[index++] \u003d new StorageBlockReport(s.getStorageID(), blocks);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), report);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "f88574acdefae2816236bf6180916be96c6a6874": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2880. Protobuf chagnes in DatanodeProtocol to add multiple storages. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1240653 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/02/12 5:39 PM",
      "commitName": "f88574acdefae2816236bf6180916be96c6a6874",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 6:15 PM",
      "commitNameOld": "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 52.97,
      "commitsBetweenForRepo": 265,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd \u003d null;\n-    List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n+    List\u003cLong\u003e blockIds \u003d request.getReports(0).getBlocksList();\n     long[] blocks \u003d new long[blockIds.size()];\n     for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n       blocks[i] \u003d blockIds.get(i);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), blocks);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n     BlockReportResponseProto.Builder builder \u003d \n         BlockReportResponseProto.newBuilder();\n     if (cmd !\u003d null) {\n       builder.setCmd(PBHelper.convert(cmd));\n     }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd \u003d null;\n    List\u003cLong\u003e blockIds \u003d request.getReports(0).getBlocksList();\n    long[] blocks \u003d new long[blockIds.size()];\n    for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n      blocks[i] \u003d blockIds.get(i);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), blocks);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e": {
      "type": "Ybodychange",
      "commitMessage": "Merge trunk into HA branch.\n\nSeveral conflicts around introduction of protobuf translator for DatanodeProtocol - mostly trivial resolutions.\n\nNB: this does not successfully pass any tests since the HAStatus field needs\nto be integrated into the HeartbeatResponse Protobuf implementation.\nThat will be a separate commit for clearer history.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1214518 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/12/11 2:47 PM",
      "commitName": "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "13/12/11 11:02 AM",
      "commitNameOld": "a0fe4f476ae907c9c070af48a250739a4fb33362",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.16,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n-    DatanodeCommand cmd;\n+    DatanodeCommand cmd \u003d null;\n     List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n     long[] blocks \u003d new long[blockIds.size()];\n     for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n       blocks[i] \u003d blockIds.get(i);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), blocks);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n     BlockReportResponseProto.Builder builder \u003d \n         BlockReportResponseProto.newBuilder();\n     if (cmd !\u003d null) {\n       builder.setCmd(PBHelper.convert(cmd));\n     }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd \u003d null;\n    List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n    long[] blocks \u003d new long[blockIds.size()];\n    for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n      blocks[i] \u003d blockIds.get(i);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), blocks);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2661. Enable protobuf RPC for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1214033 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 6:15 PM",
      "commitName": "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c",
      "commitAuthor": "Jitendra Nath Pandey",
      "commitDateOld": "13/12/11 3:31 PM",
      "commitNameOld": "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.11,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n-    DatanodeCommand cmd;\n+    DatanodeCommand cmd \u003d null;\n     List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n     long[] blocks \u003d new long[blockIds.size()];\n     for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n       blocks[i] \u003d blockIds.get(i);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), blocks);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n     BlockReportResponseProto.Builder builder \u003d \n         BlockReportResponseProto.newBuilder();\n     if (cmd !\u003d null) {\n       builder.setCmd(PBHelper.convert(cmd));\n     }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd \u003d null;\n    List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n    long[] blocks \u003d new long[blockIds.size()];\n    for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n      blocks[i] \u003d blockIds.get(i);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), blocks);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Optional protobuf parameters are not handled correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213985 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:31 PM",
      "commitName": "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:27 PM",
      "commitNameOld": "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,21 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd;\n     List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n     long[] blocks \u003d new long[blockIds.size()];\n     for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n       blocks[i] \u003d blockIds.get(i);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), blocks);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n-    return BlockReportResponseProto.newBuilder().setCmd(PBHelper.convert(cmd))\n-        .build();\n+    BlockReportResponseProto.Builder builder \u003d \n+        BlockReportResponseProto.newBuilder();\n+    if (cmd !\u003d null) {\n+      builder.setCmd(PBHelper.convert(cmd));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd;\n    List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n    long[] blocks \u003d new long[blockIds.size()];\n    for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n      blocks[i] \u003d blockIds.get(i);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), blocks);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256": {
      "type": "Ybodychange",
      "commitMessage": "Reverting the patch r1213981\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213984 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:27 PM",
      "commitName": "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:22 PM",
      "commitNameOld": "b5229fd19bfecc2e5249db652ad34ec08152334b",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,17 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd;\n     List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n     long[] blocks \u003d new long[blockIds.size()];\n     for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n       blocks[i] \u003d blockIds.get(i);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), blocks);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n-    BlockReportResponseProto.Builder builder \u003d \n-        BlockReportResponseProto.newBuilder();\n-    if (cmd !\u003d null) {\n-      builder.setCmd(PBHelper.convert(cmd));\n-    }\n-    return builder.build();\n+    return BlockReportResponseProto.newBuilder().setCmd(PBHelper.convert(cmd))\n+        .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd;\n    List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n    long[] blocks \u003d new long[blockIds.size()];\n    for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n      blocks[i] \u003d blockIds.get(i);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), blocks);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    return BlockReportResponseProto.newBuilder().setCmd(PBHelper.convert(cmd))\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "b5229fd19bfecc2e5249db652ad34ec08152334b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Optional protobuf parameters are not handled correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213981 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:22 PM",
      "commitName": "b5229fd19bfecc2e5249db652ad34ec08152334b",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:17 PM",
      "commitNameOld": "3001a172c8868763f8e59e866e36f7f50dee62cc",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,21 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd;\n     List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n     long[] blocks \u003d new long[blockIds.size()];\n     for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n       blocks[i] \u003d blockIds.get(i);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), blocks);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n-    return BlockReportResponseProto.newBuilder().setCmd(PBHelper.convert(cmd))\n-        .build();\n+    BlockReportResponseProto.Builder builder \u003d \n+        BlockReportResponseProto.newBuilder();\n+    if (cmd !\u003d null) {\n+      builder.setCmd(PBHelper.convert(cmd));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd;\n    List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n    long[] blocks \u003d new long[blockIds.size()];\n    for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n      blocks[i] \u003d blockIds.get(i);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), blocks);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "3001a172c8868763f8e59e866e36f7f50dee62cc": {
      "type": "Ybodychange",
      "commitMessage": "Reverting r1213512 because it committed changes that were not part of the patch.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213980 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:17 PM",
      "commitName": "3001a172c8868763f8e59e866e36f7f50dee62cc",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "12/12/11 4:21 PM",
      "commitNameOld": "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.96,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,17 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd;\n     List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n     long[] blocks \u003d new long[blockIds.size()];\n     for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n       blocks[i] \u003d blockIds.get(i);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), blocks);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n-    BlockReportResponseProto.Builder builder \u003d \n-        BlockReportResponseProto.newBuilder();\n-    if (cmd !\u003d null) {\n-      builder.setCmd(PBHelper.convert(cmd));\n-    }\n-    return builder.build();\n+    return BlockReportResponseProto.newBuilder().setCmd(PBHelper.convert(cmd))\n+        .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd;\n    List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n    long[] blocks \u003d new long[blockIds.size()];\n    for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n      blocks[i] \u003d blockIds.get(i);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), blocks);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    return BlockReportResponseProto.newBuilder().setCmd(PBHelper.convert(cmd))\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Handle protobuf optional parameters correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213512 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/12/11 4:21 PM",
      "commitName": "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "09/12/11 12:02 PM",
      "commitNameOld": "38a19bc293dec6221ae96e304fc6ab660d94e706",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 3.18,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,21 @@\n   public BlockReportResponseProto blockReport(RpcController controller,\n       BlockReportRequestProto request) throws ServiceException {\n     DatanodeCommand cmd;\n     List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n     long[] blocks \u003d new long[blockIds.size()];\n     for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n       blocks[i] \u003d blockIds.get(i);\n     }\n     try {\n       cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n           request.getBlockPoolId(), blocks);\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n-    return BlockReportResponseProto.newBuilder().setCmd(PBHelper.convert(cmd))\n-        .build();\n+    BlockReportResponseProto.Builder builder \u003d \n+        BlockReportResponseProto.newBuilder();\n+    if (cmd !\u003d null) {\n+      builder.setCmd(PBHelper.convert(cmd));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd;\n    List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n    long[] blocks \u003d new long[blockIds.size()];\n    for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n      blocks[i] \u003d blockIds.get(i);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), blocks);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    BlockReportResponseProto.Builder builder \u003d \n        BlockReportResponseProto.newBuilder();\n    if (cmd !\u003d null) {\n      builder.setCmd(PBHelper.convert(cmd));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "38a19bc293dec6221ae96e304fc6ab660d94e706": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/12/11 12:02 PM",
      "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
      "commitAuthor": "Jitendra Nath Pandey",
      "diff": "@@ -0,0 +1,17 @@\n+  public BlockReportResponseProto blockReport(RpcController controller,\n+      BlockReportRequestProto request) throws ServiceException {\n+    DatanodeCommand cmd;\n+    List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n+    long[] blocks \u003d new long[blockIds.size()];\n+    for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n+      blocks[i] \u003d blockIds.get(i);\n+    }\n+    try {\n+      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n+          request.getBlockPoolId(), blocks);\n+    } catch (IOException e) {\n+      throw new ServiceException(e);\n+    }\n+    return BlockReportResponseProto.newBuilder().setCmd(PBHelper.convert(cmd))\n+        .build();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public BlockReportResponseProto blockReport(RpcController controller,\n      BlockReportRequestProto request) throws ServiceException {\n    DatanodeCommand cmd;\n    List\u003cLong\u003e blockIds \u003d request.getBlocksList();\n    long[] blocks \u003d new long[blockIds.size()];\n    for (int i \u003d 0; i \u003c blockIds.size(); i++) {\n      blocks[i] \u003d blockIds.get(i);\n    }\n    try {\n      cmd \u003d impl.blockReport(PBHelper.convert(request.getRegistration()),\n          request.getBlockPoolId(), blocks);\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n    return BlockReportResponseProto.newBuilder().setCmd(PBHelper.convert(cmd))\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java"
    }
  }
}